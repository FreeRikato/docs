[
  {
    "id": 258990545,
    "timestamp": "2026-02-05T01:42:46.785Z",
    "title": "Machine Learning Explained in 100 Seconds",
    "url": "https://www.youtube.com/watch?v=PeMlggyqz0Y",
    "text": "machine learning teach a computer how to\nmachine learning teach a computer how to perform a task without explicitly\nperform a task without explicitly\nperform a task without explicitly programming it to perform said task\nprogramming it to perform said task\nprogramming it to perform said task instead feed data into an algorithm to\ninstead feed data into an algorithm to\ninstead feed data into an algorithm to gradually improve outcomes with\ngradually improve outcomes with\ngradually improve outcomes with experience similar to how organic life\nexperience similar to how organic life\nexperience similar to how organic life learns the term was coined in 1959 by\nlearns the term was coined in 1959 by\nlearns the term was coined in 1959 by Arthur Samuel at IBM who is developing\nArthur Samuel at IBM who is developing\nArthur Samuel at IBM who is developing artificial intelligence that could play\nartificial intelligence that could play\nartificial intelligence that could play checkers half a century later and\ncheckers half a century later and\ncheckers half a century later and predictive models are embedded in many\npredictive models are embedded in many\npredictive models are embedded in many of the products we use every day which\nof the products we use every day which\nof the products we use every day which perform two fundamental jobs one is to\nperform two fundamental jobs one is to\nperform two fundamental jobs one is to classify data like is there another car\nclassify data like is there another car\nclassify data like is there another car on the road or does this patient have\non the road or does this patient have\non the road or does this patient have cancer the other is to make predictions\ncancer the other is to make predictions\ncancer the other is to make predictions about future outcomes like will the\nabout future outcomes like will the\nabout future outcomes like will the stock go up or which YouTube video do\nstock go up or which YouTube video do\nstock go up or which YouTube video do you want to watch next the first step in\nyou want to watch next the first step in\nyou want to watch next the first step in the process is to acquire and clean up\nthe process is to acquire and clean up\nthe process is to acquire and clean up data lots and lots of data the better\ndata lots and lots of data the better\ndata lots and lots of data the better the data represents the problem the\nthe data represents the problem the\nthe data represents the problem the better the results garbage in garbage\nbetter the results garbage in garbage\nbetter the results garbage in garbage out the data needs to have some kind of\nout the data needs to have some kind of\nout the data needs to have some kind of signal to be valuable to the algorithm\nsignal to be valuable to the algorithm\nsignal to be valuable to the algorithm for making predictions and data\nfor making predictions and data\nfor making predictions and data scientists perform a job called feature\nscientists perform a job called feature\nscientists perform a job called feature engineering to transform raw data into\nengineering to transform raw data into\nengineering to transform raw data into features that better represent the\nfeatures that better represent the\nfeatures that better represent the underlying problem the next step is to\nunderlying problem the next step is to\nunderlying problem the next step is to separate the data into a training set\nseparate the data into a training set\nseparate the data into a training set and testing set the training data is fed\nand testing set the training data is fed\nand testing set the training data is fed into an algorithm to build a model then\ninto an algorithm to build a model then\ninto an algorithm to build a model then the testing data is used to validate the\nthe testing data is used to validate the\nthe testing data is used to validate the accuracy or error of the model the next\naccuracy or error of the model the next\naccuracy or error of the model the next step is to choose an algorithm which\nstep is to choose an algorithm which\nstep is to choose an algorithm which might be a simple statistical model like\nmight be a simple statistical model like\nmight be a simple statistical model like linear or logistic regression or a\nlinear or logistic regression or a\nlinear or logistic regression or a decision tree that assigns different\ndecision tree that assigns different\ndecision tree that assigns different weights to features in the data or you\nweights to features in the data or you\nweights to features in the data or you might get fancy with a convolutional\nmight get fancy with a convolutional\nmight get fancy with a convolutional neural network which is an algorithm\nneural network which is an algorithm\nneural network which is an algorithm that also assigns weights to Features\nthat also assigns weights to Features\nthat also assigns weights to Features but also takes the input data and\nbut also takes the input data and\nbut also takes the input data and creates a additional features\ncreates a additional features\ncreates a additional features automatically and that's extremely\nautomatically and that's extremely\nautomatically and that's extremely useful for data sets that contain things\nuseful for data sets that contain things\nuseful for data sets that contain things like images or natural language where\nlike images or natural language where\nlike images or natural language where manual feature engineering is virtually\nmanual feature engineering is virtually\nmanual feature engineering is virtually impossible every one of these algorithms\nimpossible every one of these algorithms\nimpossible every one of these algorithms learns to get better by comparing its\nlearns to get better by comparing its\nlearns to get better by comparing its predictions to an error function if it's\npredictions to an error function if it's\npredictions to an error function if it's a classification problem like is this\na classification problem like is this\na classification problem like is this animal a cat or a dog the error function\nanimal a cat or a dog the error function\nanimal a cat or a dog the error function might be accuracy if it's a regression\nmight be accuracy if it's a regression\nmight be accuracy if it's a regression problem like how much will a loaf of\nproblem like how much will a loaf of\nproblem like how much will a loaf of bread cost next year then it might be\nbread cost next year then it might be\nbread cost next year then it might be mean absolute error python is the\nmean absolute error python is the\nmean absolute error python is the language of choice among data scientists\nlanguage of choice among data scientists\nlanguage of choice among data scientists but R and Julia are also popular options\nbut R and Julia are also popular options\nbut R and Julia are also popular options and there are many supporting Frameworks\nand there are many supporting Frameworks\nand there are many supporting Frameworks out there to make the process\nout there to make the process\nout there to make the process approachable the end result of the\napproachable the end result of the\napproachable the end result of the machine learning process is a model\nmachine learning process is a model\nmachine learning process is a model which is just a file that takes some\nwhich is just a file that takes some\nwhich is just a file that takes some input data in the same shape that it was\ninput data in the same shape that it was\ninput data in the same shape that it was trained on then spits out a prediction\ntrained on then spits out a prediction\ntrained on then spits out a prediction that tries to minimize the error that it\nthat tries to minimize the error that it\nthat tries to minimize the error that it was optimized for it can then be\nwas optimized for it can then be\nwas optimized for it can then be embedded on an actual device or deployed\nembedded on an actual device or deployed\nembedded on an actual device or deployed to the cloud to build a real world\nto the cloud to build a real world\nto the cloud to build a real world product this has been machine learning\nproduct this has been machine learning\nproduct this has been machine learning in 100 seconds like And subscribe if you\nin 100 seconds like And subscribe if you\nin 100 seconds like And subscribe if you want to see more short videos like this\nwant to see more short videos like this\nwant to see more short videos like this and leave a comment if you want to see\nand leave a comment if you want to see\nand leave a comment if you want to see more machine learning content on this\nmore machine learning content on this\nmore machine learning content on this channel thanks for watching and I will\nchannel thanks for watching and I will\nchannel thanks for watching and I will see you in the next one"
  },
  {
    "id": 258990551,
    "timestamp": "2026-02-05T01:42:46.830Z",
    "title": "Machine Learning for Everybody â€“ Full Course",
    "url": "https://www.youtube.com/watch?v=i_LwzRVP7bg",
    "text": "Kylie Ying has worked at many interesting places such as MIT, CERN, and Free Code Camp.\nShe's a physicist, engineer, and basically a genius. And now she's going to teach you\nabout machine learning in a way that is accessible to absolute beginners.\nWhat's up you guys? So welcome to Machine Learning for Everyone. If you are someone who\nis interested in machine learning and you think you are considered as everyone, then this video\nis for you. In this video, we'll talk about supervised and unsupervised learning models,\nwe'll go through maybe a little bit of the logic or math behind them, and then we'll also see how\nwe can program it on Google CoLab. If there are certain things that I have done, and you know,\nyou're somebody with more experience than me, please feel free to correct me in the comments\nand we can all as a community learn from this together. So with that, let's just dive right in.\nWithout wasting any time, let's just dive straight into the code and I will be teaching you guys\nconcepts as we go. So this here is the UCI machine learning repository. And basically,\nthey just have a ton of data sets that we can access. And I found this really cool one called\nthe magic gamma telescope data set. So in this data set, if you want to read all this information,\nto summarize what I what I think is going on, is there's this gamma telescope, and we have all\nthese high energy particles hitting the telescope. Now there's a camera, there's a detector that\nactually records certain patterns of you know, how this light hits the camera. And we can use\nproperties of those patterns in order to predict what type of particle caused that radiation. So\nwhether it was a gamma particle, or some other head, like hadron. Down here, these are all of\nthe attributes of those patterns that we collect in the camera. So you can see that there's, you\nknow, some length, width, size, asymmetry, etc. Now we're going to use all these properties to\nhelp us discriminate the patterns and whether or not they came from a gamma particle or hadron.\nSo in order to do this, we're going to come up here, go to the data folder. And you're going\nto click this magic zero for data, and we're going to download that. Now over here, I have a colab\nnotebook open. So you go to colab dot research dot google.com, you start a new notebook. And\nI'm just going to call this the magic data set. So actually, I'm going to call this for code camp\nmagic example. Okay. So with that, I'm going to first start with some imports. So I will import,\nyou know, I always import NumPy, I always import pandas. And I always import matplotlib.\nAnd then we'll import other things as we go. So yeah,\nwe run that in order to run the cell, you can either click this play button here, or you can\non my computer, it's just shift enter and that that will run the cell. And here, I'm just going\nto order I'm just going to, you know, let you guys know, okay, this is where I found the data set.\nSo I've copied and pasted this actually, but this is just where I found the data set.\nAnd in order to import that downloaded file that we we got from the computer, we're going to go\nover here to this folder thing. And I am literally just going to drag and drop that file into here.\nOkay. So in order to take a look at, you know, what does this file consist of,\ndo we have the labels? Do we not? I mean, we could open it on our computer, but we can also just do\npandas read CSV. And we can pass in the name of this file.\nAnd let's see what it returns. So it doesn't seem like we have the label. So let's go back to here.\nI'm just going to make the columns, the column labels, all of these attribute names over here.\nSo I'm just going to take these values and make that the column names.\nAll right, how do I do that? So basically, I will come back here, and I will create a list called\ncalls. And I will type in all of those things. With f size, f conk. And we also have f conk one.\nWe have f symmetry, f m three long, f m three trans, f alpha. Let's see, we have f dist and class.\nOkay, great. Now in order to label those as these columns down here in our data frame.\nSo basically, this command here just reads some CSV file that you pass in CSV has come about comma\nseparated values, and turns that into a pandas data frame object. So now if I pass in a names here,\nthen it basically assigns these labels to the columns of this data set. So I'm going to set\nthis data frame equal to DF. And then if we call the head is just like, give me the first five things,\ngive me the first five things. Now you'll see that we have labels for all of these. Okay.\nAll right, great. So one thing that you might notice is that over here, the class labels,\nwe have G and H. So if I actually go down here, and I do data frame class unique,\nyou'll see that I have either G's or H's, and these stand for gammas or hadrons.\nAnd our computer is not so good at understanding letters, right? Our computer is really good at\nunderstanding numbers. So what we're going to do is we're going to convert this to zero for G and\none for H. So here, I'm going to set this equal to this, whether or not that equals G. And then\nI'm just going to say as type int. So what this should do is convert this entire column,\nif it equals G, then this is true. So I guess that would be one. And then if it's H, it would\nbe false. So that would be zero, but I'm just converting G and H to one and zero, it doesn't\nreally matter. Like, if G is one and H is zero or vice versa. Let me just take a step back right\nnow and talk about this data set. So here I have some data frame, and I have all of these different\nvalues for each entry. Now this is a you know, each of these is one sample, it's one example,\nit's one item in our data set, it's one data point, all of these things are kind of the same\nthing when I mentioned, oh, this is one example, or this is one sample or whatever. Now, each of\nthese samples, they have, you know, one quality for each or one value for each of these labels\nup here, and then it has the class. Now what we're going to do in this specific example is try to\npredict for future, you know, samples, whether the class is G for gamma or H for hadron. And\nthat is something known as classification. Now, all of these up here, these are known as our features,\nand features are just things that we're going to pass into our model in order to help us predict\nthe label, which in this case is the class column. So for you know, sample zero, I have\n10 different features. So I have 10 different values that I can pass into some model.\nAnd I can spit out, you know, the class the label, and I know the true label here is G. So this is\nthis is actually supervised learning. All right. So before I move on, let me just give you a quick\nlittle crash course on what I just said. This is machine learning for everyone. Well, the first\nquestion is, what is machine learning? Well, machine learning is a sub domain of computer science\nthat focuses on certain algorithms, which might help a computer learn from data, without a\nprogrammer being there telling the computer exactly what to do. That's what we call explicit\nprogramming. So you might have heard of AI and ML and data science, what is the difference between\nall of these. So AI is artificial intelligence. And that's an area of computer science, where the\ngoal is to enable computers and machines to perform human like tasks and simulate human behavior.\nNow machine learning is a subset of AI that tries to solve one specific problem and make predictions\nusing certain data. And data science is a field that attempts to find patterns and draw insights\nfrom data. And that might mean we're using machine learning. So all of these fields kind of overlap,\nand all of them might use machine learning. So there are a few types of machine learning.\nThe first one is supervised learning. And in supervised learning, we're using labeled inputs.\nSo this means whatever input we get, we have a corresponding output label, in order to train\nmodels and to learn outputs of different new inputs that we might feed our model. So for example,\nI might have these pictures, okay, to a computer, all these pictures are are pixels, they're pixels\nwith a certain color. Now in supervised learning, all of these inputs have a label associated with\nthem, this is the output that we might want the computer to be able to predict. So for example,\nover here, this picture is a cat, this picture is a dog, and this picture is a lizard.\nNow there's also unsupervised learning. And in unsupervised learning, we use unlabeled data\nto learn about patterns in the data. So here are here are my input data points. Again, they're just\nimages, they're just pixels. Well, okay, let's say I have a bunch of these different pictures.\nAnd what I can do is I can feed all these to my computer. And I might not, you know,\nmy computer is not going to be able to say, Oh, this is a cat, dog and lizard in terms of,\nyou know, the output. But it might be able to cluster all these pictures, it might say,\nHey, all of these have something in common. All of these have something in common. And then these\ndown here have something in common, that's finding some sort of structure in our unlabeled data.\nAnd finally, we have reinforcement learning. And reinforcement learning. Well, they usually\nthere's an agent that is learning in some sort of interactive environment, based on rewards and\npenalties. So let's think of a dog, we can train our dog, but there's not necessarily, you know,\nany wrong or right output at any given moment, right? Well, let's pretend that dog is a computer.\nEssentially, what we're doing is we're giving rewards to our computer, and tell your computer,\nHey, this is probably something good that you want to keep doing. Well, computer agent terminology.\nBut in this class today, we'll be focusing on supervised learning and unsupervised learning\nand learning different models for each of those. Alright, so let's talk about supervised learning\nfirst. So this is kind of what a machine learning model looks like you have a bunch of inputs\nthat are going into some model. And then the model is spitting out an output, which is our prediction.\nSo all these inputs, this is what we call the feature vector. Now there are different types\nof features that we can have, we might have qualitative features. And qualitative means\ncategorical data, there's either a finite number of categories or groups. So one example of a\nqualitative feature might be gender. And in this case, there's only two here, it's for the sake of\nthe example, I know this might be a little bit outdated. Here we have a girl and a boy, there are\ntwo genders, there are two different categories. That's a piece of qualitative data. Another\nexample might be okay, we have, you know, a bunch of different nationalities, maybe a nationality or\na nation or a location, that might also be an example of categorical data. Now, in both of\nthese, there's no inherent order. It's not like, you know, we can rate us one and France to Japan\nthree, etc. Right? There's not really any inherent order built into either of these categorical\ndata sets. That's why we call this nominal data. Now, for nominal data, the way that we want\nto feed it into our computer is using something called one hot encoding. So let's say that, you\nknow, I have a data set, some of the items in our data, some of the inputs might be from the US,\nsome might be from India, then Canada, then France. Now, how do we get our computer to recognize that\nwe have to do something called one hot encoding. And basically, one hot encoding is saying, okay,\nwell, if it matches some category, make that a one. And if it doesn't just make that a zero.\nSo for example, if your input were from the US, you would you might have 1000. India, you know,\n0100. Canada, okay, well, the item representing Canada is one and then France, the item representing\nFrance is one. And then you can see that the rest are zeros, that's one hot encoding.\nNow, there are also a different type of qualitative feature. So here on the left,\nthere are different age groups, there's babies, toddlers, teenagers, young adults,\nadults, and so on, right. And on the right hand side, we might have different ratings. So maybe\nbad, not so good, mediocre, good, and then like, great. Now, these are known as ordinal pieces of\ndata, because they have some sort of inherent order, right? Like, being a toddler is a lot closer to\nbeing a baby than being an elderly person, right? Or good is closer to great than it is to really\nbad. So these have some sort of inherent ordering system. And so for these types of data sets,\nwe can actually just mark them from, you know, one to five, or we can just say, hey, for each of these,\nlet's give it a number. And this makes sense. Because, like, for example, the thing that I\njust said, how good is closer to great, then good is close to not good at all. Well, four is closer\nto five, then four is close to one. So this actually kind of makes sense. And it'll make sense for the\ncomputer as well. Alright, there are also quantitative pieces of data and quantitative\npieces of data are numerical valued pieces of data. So this could be discrete, which means,\nyou know, they might be integers, or it could be continuous, which means all real numbers.\nSo for example, the length of something is a quantitative piece of data, it's a quantitative\nfeature, the temperature of something is a quantitative feature. And then maybe how many\nEaster eggs I collected in my basket, this Easter egg hunt, that is an example of discrete quantitative\nfeature. Okay, so these are continuous. And this over here is the screen. So those are the things\nthat go into our feature vector, those are our features that we're feeding this model, because\nour computers are really, really good at understanding math, right at understanding numbers,\nthey're not so good at understanding things that humans might be able to understand.\nWell, what are the types of predictions that our model can output? So in supervised learning,\nthere are some different tasks, there's one classification, and basically classification,\njust saying, okay, predict discrete classes. And that might mean, you know, this is a hot dog,\nthis is a pizza, and this is ice cream. Okay, so there are three distinct classes and any other\npictures of hot dogs, pizza or ice cream, I can put under these labels. Hot dog, pizza, ice cream.\nHot dog, pizza, ice cream. This is something known as multi class classification. But there's also\nbinary classification. And binary classification, you might have hot dog, or not hot dog. So there's\nonly two categories that you're working with something that is something and something that's\nisn't binary classification. Okay, so yeah, other examples. So if something has positive or negative\nsentiment, that's binary classification. Maybe you're predicting your pictures of their cats or\ndogs. That's binary classification. Maybe, you know, you are writing an email filter, and you're\ntrying to figure out if an email spam or not spam. So that's also binary classification.\nNow for multi class classification, you might have, you know, cat, dog, lizard, dolphin, shark,\nrabbit, etc. We might have different types of fruits like orange, apple, pear, etc. And then\nmaybe different plant species. But multi class classification just means more than two. Okay,\nand binary means we're predicting between two things. There's also something called regression\nwhen we talk about supervised learning. And this just means we're trying to predict continuous\nvalues. So instead of just trying to predict different categories, we're trying to come up\nwith a number that you know, is on some sort of scale. So some examples. So some examples might\nbe the price of aetherium tomorrow, or it might be okay, what is going to be the temperature?\nOr it might be what is the price of this house? Right? So these things don't really fit into\ndiscrete classes. We're trying to predict a number that's as close to the true value as possible\nusing different features of our data set. So that's exactly what our model looks like in\nsupervised learning. Now let's talk about the model itself. How do we make this model learn?\nOr how can we tell whether or not it's even learning? So before we talk about the models,\nlet's talk about how can we actually like evaluate these models? Or how can we tell\nwhether something is a good model or bad model? So let's take a look at this data set. So this data\nset has this is from a diabetes, a Pima Indian diabetes data set. And here we have different\nnumber of pregnancies, different glucose levels, blood pressure, skin thickness, insulin, BMI,\nage, and then the outcome whether or not they have diabetes one for they do zero for they don't.\nSo here, all of these are quantitative features, right, because they're all on some scale.\nSo each row is a different sample in the data. So it's a different example, it's one person's data,\nand each row represents one person in this data set. Now this column, each column represents a\ndifferent feature. So this one here is some measure of blood pressure levels. And this one\nover here, as we mentioned is the output label. So this one is whether or not they have diabetes.\nAnd as I mentioned, this is what we would call a feature vector, because these are all of our\nfeatures in one sample. And this is what's known as the target, or the output for that feature\nvector. That's what we're trying to predict. And all of these together is our features matrix x.\nAnd over here, this is our labels or targets vector y. So I've condensed this to a chocolate\nbar to kind of talk about some of the other concepts in machine learning. So over here,\nwe have our x, our features matrix, and over here, this is our label y. So each row of this\nwill be fed into our model, right. And our model will make some sort of prediction. And what we do\nis we compare that prediction to the actual value of y that we have in our label data set, because\nthat's the whole point of supervised learning is we can compare what our model is outputting to,\noh, what is the truth, actually, and then we can go back and we can adjust some things. So the next\niteration, we get closer to what the true value is. So that whole process here, the tinkering that,\nokay, what's the difference? Where did we go wrong? That's what's known as training the model.\nAlright, so take this whole, you know, chunk right here, do we want to really put our entire\nchocolate bar into the model to train our model? Not really, right? Because if we did that, then\nhow do we know that our model can do well on new data that we haven't seen? Like, if I were to\ncreate a model to predict whether or not someone has diabetes, let's say that I just train all my\ndata, and I see that all my training data does well, I go to some hospital, I'm like, here's my\nmodel. I think you can use this to predict if somebody has diabetes. Do we think that would\nbe effective or not? Probably not, right? Because we haven't assessed how well our model can\ngeneralize. Okay, it might do well after you know, our model has seen this data over and over and\nover again. But what about new data? Can our model handle new data? Well, how do we how do we get our\nmodel to assess that? So we actually break up our whole data set that we have into three different\ntypes of data sets, we call it the training data set, the validation data set and the testing data\nset. And you know, you might have 60% here 20% and 20% or 80 10 and 10. It really depends on how\nmany statistics you have, I think either of those would be acceptable. So what we do is then we feed\nthe training data set into our model, we come up with, you know, this might be a vector of predictions\ncorresponding with each sample that we put into our model, we figure out, okay, what's the difference\nbetween our prediction and the true values, this is something known as loss, losses, you know,\nwhat's the difference here, in some numerical quantity, of course. And then we make adjustments,\nand that's what we call training. Okay. So then, once you know, we've made a bunch of adjustments,\nwe can put our validation set through this model. And the validation set is kind of used as a reality\ncheck during or after training to ensure that the model can handle unseen data still. So every\nsingle time after we train one iteration, we might stick the validation set in and see, hey, what's\nthe loss there. And then after our training is over, we can assess the validation set and ask,\nhey, what's the loss there. But one key difference here is that we don't have that training step,\nthis loss never gets fed back into the model, right, that feedback loop is not closed.\nAlright, so let's talk about loss really quickly. So here, I have four different types of models,\nI have some sort of data that's being fed into the model, and then some output. Okay, so this output\nhere is pretty far from you know, this truth that we want. And so this loss is going to be high. In\nmodel B, again, this is pretty far from what we want. So this loss is also going to be high,\nlet's give it 1.5. Now this one here, it's pretty close, I mean, maybe not almost, but pretty close\nto this one. So that might have a loss of 0.5. And then this one here is maybe further than this,\nbut still better than these two. So that loss might be 0.9. Okay, so which of these model\nperforms the best? Well, model C has a smallest loss, so it's probably model C. Okay, now let's\ntake model C. After you know, we've come up with these, all these models, and we've seen, okay, model\nC is probably the best model. We take model C, and we run our test set through this model. And this\ntest set is used as a final check to see how generalizable that chosen model is. So if I,\nyou know, finish training my diabetes data set, then I could run it through some chunk of the\ndata and I can say, oh, like, this is how we perform on data that it's never seen before at\nany point during the training process. Okay. And that loss, that's the final reported performance\nof my test set, or this would be the final reported performance of my model. Okay.\nSo let's talk about this thing called loss, because I think I kind of just glossed over it,\nright? So loss is the difference between your prediction and the actual, like, label.\nSo this would give a slightly higher loss than this. And this would even give a higher loss,\nbecause it's even more off. In computer science, we like formulas, right? We like formulaic ways\nof describing things. So here are some examples of loss functions and how we can actually come\nup with numbers. This here is known as L one loss. And basically, L one loss just takes the\nabsolute value of whatever your you know, real value is, whatever the real output label is,\nsubtracts the predicted value, and takes the absolute value of that. Okay. So the absolute\nvalue is a function that looks something like this. So the further off you are, the greater your losses,\nright in either direction. So if your real value is off from your predicted value by 10,\nthen your loss for that point would be 10. And then this sum here just means, hey,\nwe're taking all the points in our data set. And we're trying to figure out the sum of how far\neverything is. Now, we also have something called L two loss. So this loss function is quadratic,\nwhich means that if it's close, the penalty is very minimal. And if it's off by a lot,\nthen the penalty is much, much higher. Okay. And this instead of the absolute value, we just square\nthe the difference between the two. Now, there's also something called binary cross entropy loss.\nIt looks something like this. And this is for binary classification, this this might be the\nloss that we use. So this loss, you know, I'm not going to really go through it too much.\nBut you just need to know that loss decreases as the performance gets better. So there are some\nother measures of accurate or performance as well. So for example, accuracy, what is accuracy?\nSo let's say that these are pictures that I'm feeding my model, okay. And these predictions\nmight be apple, orange, orange, apple, okay, but the actual is apple, orange, apple, apple. So\nthree of them were correct. And one of them was incorrect. So the accuracy of this model is\nthree quarters or 75%. Alright, coming back to our colab notebook, I'm going to close this a little\nbit. Again, we've imported stuff up here. And we've already created our data frame right here. And\nthis is this is all of our data. This is what we're going to use to train our models. So down here,\nagain, if we now take a look at our data set, you'll see that our classes are now zeros and ones.\nSo now this is all numerical, which is good, because our computer can now understand that.\nOkay. And you know, it would probably be a good idea to maybe kind of plot, hey, do these things\nhave anything to do with the class. So here, I'm going to go through all the labels. So for label\nin the columns of this data frame. So this just gets me the list. Actually, we have the list,\nright? It's called so let's just use that might be less confusing of everything up to the last\nthing, which is the class. So I'm going to take all these 10 different features. And I'm going\nto plot them as a histogram. So and now I'm going to plot them as a histogram. So basically, if I\ntake that data frame, and I say, okay, for everything where the class is equal to one, so these are all\nof our gammas, remember, now, for that portion of the data frame, if I look at this label, so now\nthese, okay, what this part here is saying is, inside the data frame, get me everything where\nthe class is equal to one. So that's all all of these would fit into that category, right?\nAnd now let's just look at the label column. So the first label would be f length, which would\nbe this column. So this command here is getting me all the different values that belong to class one\nfor this specific label. And that's exactly what I'm going to put into the histogram. And now I'm\njust going to tell you know, matplotlib make the color blue, make this label this as you know, gamma\nset alpha, why do I keep doing that, alpha equal to 0.7. So that's just like the transparency.\nAnd then I'm going to set density equal to true, so that when we compare it to\nthe hadrons here, we'll have a baseline for comparing them. Okay, so the density being true\njust basically normalizes these distributions. So you know, if you have 200 in of one type,\nand then 50 of another type, well, if you drew the histograms, it would be hard to compare because\none of them would be a lot bigger than the other, right. But by normalizing them, we kind of are\ndistributing them over how many samples there are. Alright, and then I'm just going to put a title\non here and make that the label, the y label. So because it's density, the y label is probability.\nAnd the x label is just going to be the label.\nWhat is going on. And I'm going to include a legend and PLT dot show just means okay, display\nthe plot. So if I run that, just be up to the last item. So we want a list, right, not just the last\nitem. And now we can see that we're plotting all of these. So here we have the length. Oh, and I\nmade this gamma. So this should be hadron. Okay, so the gammas in blue, the hadrons are in red. So\nhere we can already see that, you know, maybe if the length is smaller, it's probably more likely\nto be gamma, right. And we can kind of you know, these all look somewhat similar. But here, okay,\nclearly, if there's more asymmetry, or if you know, this asymmetry measure is larger, then it's\nprobably hadron. Okay, oh, this one's a good one. So f alpha seems like hadrons are pretty evenly\ndistributed. Whereas if this is smaller, it looks like there's more gammas in that area.\nOkay, so this is kind of what the data that we're working with, we can kind of see what's going on.\nOkay, so the next thing that we're going to do here is we are going to create our train,\nour validation, and our test data sets. I'm going to set train valid and test to be equal to\nthis. So NumPy dot split, I'm just splitting up the data frame. And if I do this sample,\nwhere I'm sampling everything, this will basically shuffle my data. Now, if I I want to pass in where\nexactly I'm splitting my data set, so the first split is going to be maybe at 60%. So I'm going\nto say 0.6 times the length of this data frame. So and then cast that 10 integer, that's going\nto be the first place where you know, I cut it off, and that'll be my training data. Now, if I\nthen go to 0.8, this basically means everything between 60% and 80% of the length of the data\nset will go towards validation. And then, like everything from 80 to 100, I'm going to pass\nmy test data. So I can run that. And now, if we go up here, and we inspect this data, we'll see that\nthese columns seem to have values in like the 100s, whereas this one is 0.03. Right? So the scale of\nall these numbers is way off. And sometimes that will affect our results. So I'm going to run this\nis way off. And sometimes that will affect our results. So one thing that we would want to do\nis scale these so that they are, you know, so that it's now relative to maybe the mean and the\nstandard deviation of that specific column. I'm going to create a function called scale data set.\nAnd I'm going to pass in the data frame. And that's what I'll do for now. Okay, so the x values are\ngoing to be, you know, I take the data frame. And let's assume that the columns are going to be,\nyou know, that the label will always be the last thing in the data frame. So what I can do is say\ndata frame, dot columns all the way up to the last item, and get those values. Now for my y,\nwell, it's the last column. So I can just do this, I can just index into that last column,\nand then get those values. Now, in, so I'm actually going to import something known as\nthe standard scalar from sk learn. So if I come up here, I can go to sk learn dot pre processing.\nAnd I'm going to import standard scalar, I have to run that cell, I'm going to come back down here.\nAnd now I'm going to create a scalar and use that skip or so standard scalar.\nAnd with the scalar, what I can do is actually just fit and transform x. So here, I can say x\nis equal to scalar dot fit, fit, transform x. So what that's doing is saying, okay, take x and\nfit the standard scalar to x, and then transform all those values. And what would it be? And that's\ngoing to be our new x. Alright. And then I'm also going to just create, you know, the whole data as\none huge 2d NumPy array. And in order to do that, I'm going to call H stack. So H stack is saying,\nokay, take an array, and another array and horizontally stack them together. That's what\nthe H stands for. So by horizontally stacked them together, just like put them side by side,\nokay, not on top of each other. So what am I stacking? Well, I have to pass in something\nso that it can stack x and y. And now, okay, so NumPy is very particular about dimensions,\nright? So in this specific case, our x is a two dimensional object, but y is only a one dimensional\nthing, it's only a vector of values. So in order to now reshape it into a 2d item, we have to call\nNumPy dot reshape. And we can pass in the dimensions of its reshape. So if I pass in negative\none comma one, that just means okay, make this a 2d array, where the negative one just means infer\nwhat what this dimension value would be, which ends up being the length of y, this would be the\nsame as literally doing this. But the negative one is easier because we're making the computer\ndo the hard work. So if I stack that, I'm going to then return the data x and y. Okay. So one more\nthing is that if we go into our training data set, okay, again, this is our training data set.\nAnd we get the length of the training data set. But where the training data sets class is one,\nso remember that this is the gammas. And then if we print that, and we do the same thing, but zero,\nwe'll see that, you know, there's around 7000 of the gammas, but only around 4000 of the hadrons.\nSo that might actually become an issue. And instead, what we want to do is we want to oversample\nour our training data set. So that means that we want to increase the number of these values,\nso that these kind of match better. And surprise, surprise, there is something that we can import\nthat will help us do that. It's so I'm going to go to from in the learn dot oversampling. And I'm\ngoing to import this random oversampler, run that cell, and come back down here. So I will actually\nadd in this parameter called oversample, and set that to false for default. And if I do want to\noversample, then what I'm going to do, and by oversample, so if I do want to oversample,\nthen I'm going to create this ROS and set it equal to this random oversampler. And then for x and y,\nI'm just going to say, okay, just fit and resample x and y. And what that's doing is saying, okay,\ntake more of the less class. So take take the less class and keep sampling from there to increase\nthe size of our data set of that smaller class so that they now match. So if I do this, and I scale\ndata set, and I pass in the training data set where oversample is true. So this let's say this\nis train and then x train, y train. Oops, what's going on? These should be columns. So basically,\nwhat I'm doing now is I'm just saying, okay, what is the length of y train? Okay, now it's\n14,800, whatever. And now let's take a look at how many of these are type one. So actually,\nwe can just sum that up. And then we'll also see that if we instead switch the label and ask how\nmany of them are the other type, it's the same value. So now these have been evenly, you know,\nrebalanced. Okay, well, okay. So here, I'm just going to make this the validation data set. And\nthen the next one, I'm going to make this the test data set. Alright, and we're actually going to\nswitch oversample here to false. Now, the reason why I'm switching that to false is because my\nvalidation and my test sets are for the purpose of you know, if I have data that I haven't seen yet,\nhow does my sample perform on those? And I don't want to oversample for that right now. Like,\nI don't care about balancing those I'm, I want to know if I have a random set of data that's\nunlabeled, can I trust my model, right? So that's why I'm not oversampling. I run that. And again,\nwhat is going on? Oh, it's because we already have this train. So I have to go come up here and split\nthat data frame again. And now let's run these. Okay. So now we have our data properly formatted.\nAnd we're going to move on to different models now. And I'm going to tell you guys a little bit\nabout each of these models. And then I'm going to show you how we can do that in our code. So the\nfirst model that we're going to learn about is KNN or K nearest neighbors. Okay, so here, I've\nalready drawn a plot on the y axis, I have the number of kids that a family might have. And then\non the x axis, I have their income in terms of 1000s per year. So, you know, if if someone's\nmaking 40,000 a year, that's where this would be. And if somebody making 320, that's where that\nwould be somebody has zero kids, it'd be somewhere along this axis. Somebody has five, it'd be\nsomewhere over here. Okay. And now I have these plus signs and these minus signs on here. So what\nI'm going to represent here is the plus sign means that they own a car. And the minus sign is going\nto represent no car. Okay. So your initial thought should be okay, I think this is binary\nclassification because all of our points all of our samples have labels. So this is a sample with\nthe plus label. And this here is another sample with the minus label. This is an abbreviation for\nwidth that I'll use. Alright, so we have this entire data set. And maybe around half the people\nown a car and maybe around half the people don't own a car. Okay, well, what if I had some new\npoint, let me use choose a different color, I'll use this nice green. Well, what if I have a new\npoint over here? So let's say that somebody makes 40,000 a year and has two kids. What do we think\nthat would be? Well, just logically looking at this plot, you might think, okay, it seems like\nthey wouldn't have a car, right? Because that kind of matches the pattern of everybody else around\nthem. So that's a whole concept of this nearest neighbors is you look at, okay, what's around you.\nAnd then you're basically like, okay, I'm going to take the label of the majority that's around me.\nSo the first thing that we have to do is we have to define a distance function. And a lot of times\nin, you know, 2d plots like this, our distance function is something known as Euclidean distance.\nAnd Euclidean distance is basically just this straight line distance like this. Okay. So this\nwould be the Euclidean distance, it seems like there's this point, there's this point, there's\nthat point, etc. So the length of this line, this green line that I just drew, that is what's known\nas Euclidean distance. If we want to get technical with that, this exact formula is the distance here,\nlet me zoom in. The distance is equal to the square root of one point x minus the other points x\nsquared plus extend that square root, the same thing for y. So y one of one minus y two of the\nother squared. Okay, so we're basically trying to find the length, the distances, the difference\nbetween x and y, and then square each of those sum it up and take the square root. Okay, so I'm\ngoing to erase this so it doesn't clutter my drawing. But anyways, now going back to this plot,\nso here in the nearest neighbor algorithm, we see that there is a K, right? And this K is basically\ntelling us, okay, how many neighbors do we use in order to judge what the label is? So usually,\nwe use a K of maybe, you know, three or five, depends on how big our data set is. But here,\nI would say, maybe a logical number would be three or five. So let's say that we take K to be equal\nto three. Okay, well, of this data point that I drew over here, let me use green to highlight this.\nOkay, so of this data point that I drew over here, it looks like the three closest points are definitely\nthis one, this one. And then this one has a length of four. And this one seems like it'd be a little\nbit further than four. So actually, this would be these would be our three points. Well, all those\npoints are blue. So chances are, my prediction for this point is going to be blue, it's going to be\nprobably don't have a car. All right, now what if my point is somewhere? What if my point is\nsomewhere over here, let's say that a couple has four kids, and they make 240,000 a year. All right,\nwell, now my closest points are this one, probably a little bit over that one. And then this one,\nright? Okay, still all pluses. Well, this one is more than likely to be plus. Right? Now,\nlet me get rid of some of these just so that it looks a little bit more clear. All right,\nlet's go through one more. What about a point that might be right here? Okay, let's see. Well,\ndefinitely this is the closest, right? This one's also closest. And then it's really close between\nthe two of these. But if we actually do the mathematics, it seems like if we zoom in,\nthis one is right here. And this one is in between these two. So this one here is actually shorter\nthan this one. And that means that that top one is the one that we're going to take. Now,\nwhat is the majority of the points that are close by? Well, we have one plus here, we have one plus\nhere, and we have one minus here, which means that the pluses are the majority. And that means\nthat this label is probably somebody with a car. Okay. So this is how K nearest neighbors would\nwork. It's that simple. And this can be extrapolated to further dimensions to higher dimensions. You\nknow, if you have here, we have two different features, we have the income, and then we have\nthe number of kids. But let's say we have 10 different features, we can expand our distance\nfunction so that it includes all 10 of those dimensions, we take the square root of everything,\nand then we figure out which one is the closest to the point that we desire to classify. Okay. So\nthat's K nearest neighbors. So now we've learned about K nearest neighbors. Let's see how we would\nbe able to do that within our code. So here, I'm going to label the section K nearest neighbors.\nAnd we're actually going to use a package from SK learn. So the reason why we, you know, use these\npackages and so that we don't have to manually code all these things ourselves, because it would\nbe really difficult. And chances are the way that we would code it, either would have bugs,\nor it'd be really slow, or I don't know a whole bunch of issues. So what we're going to do is\nhand it off to the pros. From here, I can say, okay, from SK learn, which is this package dot\nneighbors, I'm going to import K neighbors classifier, because we're classifying. Okay,\nso I run that. And our KNN model is going to be this K neighbors classifier. And we can pass in\na parameter of how many neighbors, you know, we want to use. So first, let's see what happens if\nwe just use one. So now if I do K, and then model dot fit, I can pass in my x training set and my\nweight y train data. Okay. So that effectively fits this model. And let's get all the predictions. So\nwhy can and I guess yeah, let's do y predictions. And my y predictions are going to be cannon model\ndot predict. So let's use the test set x test. Okay. Alright, so if I call y predict, you'll see\nthat we have those. But if I get my truth values for that test set, you'll see that this is what\nwe actually do. So just looking at this, we got five out of six of them. Okay, great. So let's\nactually take a look at something called the classification report that's offered by SK learn.\nSo if I go to from SK learn dot metrics, import classification report, what I can actually do is\nsay, hey, print out this classification report for me. And let's check, you know, I'm giving you the\ny test and the y prediction. We run this and we see we get this whole entire chart. So I'm going\nto tell you guys a few things on this chart. Alright, this accuracy is 82%, which is actually\npretty good. That's just saying, hey, if we just look at, you know, what each of these new points,\nwhat it's closest to, then we actually get an 82% accuracy, which means how many do we get right\nversus how many total are there. Now, precision is saying, okay, you might see that we have it\nfor class one, or class zero and class one. What precision is saying was, let's go to this Wikipedia\ndiagram over here, because I actually kind of like this diagram. So here, this is our entire data set.\nAnd on the left over here, we have everything that we know is positive. So everything that is\nactually truly positive, that we've labeled positive in our original data set. And over here,\nthis is everything that's truly negative. Now in the circle, we have things that are positive that\nwere labeled positive by our model. On the left here, we have things that are truly positive,\nbecause you know, this side is the positive side and the side is the negative side. So these are\ntruly positive. Whereas all these ones out here, well, they should have been positive, but they\nare labeled as negative. And in here, these are the ones that we've labeled positive, but they're\nactually negative. And out here, these are truly negative. So precision is saying, okay, out of all\nthe ones we've labeled as positive, how many of them are true positives? And recall is saying,\nokay, out of all the ones that we know are truly positive, how many do we actually get right? Okay,\nso going back to this over here, our precision score, so again, precision, out of all the ones\nthat we've labeled as the specific class, how many of them are actually that class, it's 7784%. Now,\nrecall how out of all the ones that are actually this class, how many of those that we get, this\nis 68% and 89%. Alright, so not too shabby, we can clearly see that this recall and precision for\nlike this, the class zero is worse than class one. Right? So that means for hadron, it's worked for\nhadrons and for our gammas. This f1 score over here is kind of a combination of the precision and\nrecall score. So we're actually going to mostly look at this one because we have an unbalanced\ntest data set. So here we have a measure of 72 and 87 or point seven two and point eight seven,\nwhich is not too shabby. All right. Well, what if we, you know, made this three. So we actually see\nthat, okay, so what was it originally with one? We see that our f1 score, you know, is now it was\npoint seven two and then point eight seven. And then our accuracy was 82%. So if I change that to\nthree. Alright, so we've kind of increased zero at the cost of one and then our overall accuracy\nis 81. So let's actually just make this five. Alright, so you know, again, very similar numbers,\nwe have 82% accuracy, which is pretty decent for a model that's relatively simple. Okay,\nthe next type of model that we're going to talk about is something known as naive Bayes. Now,\nin order to understand the concepts behind naive Bayes, we have to be able to understand\nconditional probability and Bayes rule. So let's say I have some sort of data set that's shown in\nthis table right here. People who have COVID are over here in this red row. And people who do not\nhave COVID are down here in this green row. Now, what about the COVID test? Well, people who have\ntested positive are over here in this column. And people who have tested negative are over here in\nthis column. Okay. Yeah, so basically, our categories are people who have COVID and test positive,\npeople who don't have COVID, but test positive, so a false false positive, people who have COVID\nand test negative, which is a false negative, and people who don't have COVID and test negative,\nwhich good means you don't have COVID. Okay, so let's make this slightly more legible. And here,\nin the margins, I've written down the sums of whatever it's referring to. So this here is the\nsum of this entire row. And this here might be the sum of this column over here. Okay. So the first\nquestion that I have is, what is the probability of having COVID given that you have a positive\ntest? And in probability, we write that out like this. So the probability of COVID given, so this\nline, that vertical line means given that, you know, some condition, so given a positive test,\nokay, so what is the probability of having COVID given a positive test? So what this is asking is\nsaying, okay, let's go into this condition. So the condition of having a positive test, that is this\nslice of the data, right? That means if you're in this slice of data, you have a positive test. So\ngiven that we have a positive test, given in this condition, in this circumstance, we have a positive\ntest. So what's the probability that we have COVID? Well, if we're just using this data, the number\nof people that have COVID is 531. So I'm gonna say that there's 531 people that have COVID. And then\nnow we divide that by the total number of people that have a positive test, which is 551. Okay,\nso that's the probability and doing a quick division, we get that this is equal to around\n96.4%. So according to this data set, which is data that I made up off the top of my head, so it's\nnot actually real COVID data. But according to this data, the probability of having COVID given\nthat you tested positive is 96.4%. Alright, now with that, let's talk about Bayes rule, which is\nthis section here. Let's ignore this bottom part for now. So Bayes rule is asking, okay, what is\nthe probability of some event A happening, given that B happened. So this, we already know has\nhappened. This is our condition, right? Well, what if we don't have data for that, right? Like, what\nif we don't know what the probability of A given B is? Well, Bayes rule is saying, okay, well, you\ncan actually go and calculate it, as long as you have a probability of B given A, the probability\nof A and the probability of B. Okay. And this is just a mathematical formula for that. Alright,\nso here we have Bayes rule. And let's actually see Bayes rule in action. Let's use it on an example.\nSo here, let's say that we have some disease statistics, okay. So not COVID different disease.\nAnd we know that the probability of obtaining a false positive is 0.05 probability of obtaining a\nfalse negative is 0.01. And the probability of the disease is 0.1. Okay, what is the probability of\nthe disease given that we got a positive test? Hmm, how do we even go about solving this? So\nwhat what do I mean by false positive? What's a different way to rewrite that? A false positive\nis when you test positive, but you don't actually have the disease. So this here is a probability\nthat you have a positive test given no disease, right? And similarly for the false negative,\nit's a probability that you test negative given that you actually have the disease. So if I put\nthat into a chart, for example, and this might be my positive and negative tests, and this might\nbe my diseases, disease and no disease. Well, the probability that I test positive, but actually\nhave no disease, okay, that's 0.05 over here. And then the false negatives up here for 0.01. So I'm\ntesting negative, but I don't actually have the disease. This so the probability that you test\npositive, and you don't have the disease, plus a probability that you test negative, given that you\ndon't have the disease, that should sum up to one. Okay, because if you don't have the disease,\nthen you should have some probability that you're testing positive and some probability that you're\ntesting negative. But that probability, in total should be one. So that means that the probability\nnegative and no disease, this should be the reciprocal, this should be the opposite. So it\nshould be 0.95 because it's one minus whatever this probability is. And then similarly, oops,\nup here, this should be 0.99 because the probability that we, you know,\ntest negative and have the disease plus the probability that we test positive and have the\ndisease should equal one. So this is our probability chart. And now, this probability of disease\nbeing point 0.1 just means I have 10% probability of actually of having the disease, right? Like,\nin the general population, the probability that I have the disease is 0.1. Okay, so what is the\nprobability that I have the disease given that I got a positive test? Well, remember that we\ncan write this out in terms of Bayes rule, right? So if I use this rule up here, this is the\nprobability of a positive test given that I have the disease times the probability of the disease\ndivided by the probability of the evidence, which is my positive test.\nAlright, now let's plug in some numbers for that. The probability of having a positive test given\nthat I have the disease is 0.99. And then the probability that I have the disease is this value\nover here 0.1. Okay. And then the probability that I have a positive test at all should be okay,\nwhat is the probability that I have a positive test given that I actually have the disease\nand then having having the disease. And then the other case, where the probability of me having a\nnegative test given or sorry, positive test giving no disease times the probability of not actually\nhaving a disease. Okay, so I can expand that probability of having a positive test out into\nthese two different cases, I have a disease, and then I don't. And then what's the probability of\nhaving positive tests in either one of those cases. So that expression would become 0.99 times 0.1\nplus 0.05. So that's the probability that I'm testing positive, but don't have the disease.\nAnd the times the probability that I don't actually have the disease. So that's one minus\n0.1 probability that the population doesn't have the disease is 90%. So 0.9. And let's do that\nmultiplication. And I get an answer of 0.6875 or 68.75%. Okay. All right, so we can actually expand\nthat we can expand Bayes rule and apply it to classification. And this is what we call naive\nbase. So first, a little terminology. So the posterior is this over here, because it's asking,\nHey, what is the probability of some class CK? So by CK, I just mean, you know, the different\ncategories, so C for category or class or whatever. So category one might be cats, category two,\ndogs, category three, lizards, all the way, we have k categories, k is just some number. Okay.\nSo what is the probability of having of this specific sample x, so this is our feature vector\nof this one sample. What is the probability of x fitting into category 123 for whatever, right,\nso that that's what this is asking, what is the probability that, you know, it's actually from\nthis class, given all this evidence that we see the x's. So the likelihood is this quantity over\nhere, it's saying, Okay, well, given that, you know, assume, assume we are, assume that this\nclass is class CK, okay, assume that this is a category. Well, what is the likelihood of\nactually seeing x, all these different features from that category. And then this here is the\nprior. So like in the entire population of things, what are the probabilities? What is the\nprobability of this class in general? Like if I have, you know, in my entire data set, what is the\npercentage? What is the chance that this image is a cat? How many cats do I have? Right. And then this\ndown here is called the evidence because what we're trying to do is we're changing our prior,\nwe're creating this new posterior probability built upon the prior by using some sort of evidence,\nright? And that evidence is a probability of x. So that's some vocab. And this here\nis a rule for naive Bayes. Whoa, okay, let's digest that a little bit. Okay. So what is\nlet me use a different color. What is this side of the equation asking? It's asking,\nwhat is the probability that we are in some class K, CK, given that, you know, this is my first\ninput, this is my second input, this is, you know, my third, fourth, this is my nth input. So let's\nsay that our classification is, do we play soccer today or not? Okay, and let's say our x's are,\nokay, is it how much wind is there? How much rain is there? And what day of the week is it? So let's\nSo let's say that it's raining, it's not windy, but it's Wednesday, do we play soccer? Do we not?\nSo let's use Bayes rule on this. So this here\nis equal to the probability of x one, x two, all these joint probabilities, given class K\ntimes the probability of that class, all over the probability of this evidence.\nOkay. So what is this fancy symbol over here, this means proportional to\nso how our equal sign means it's equal to this like little squiggly sign means that this is\nproportional to okay, and this denominator over here, you might notice that it has no impact on\nthe class like this, that number doesn't depend on the class, right? So this is going to be constant\nfor all of our different classes. So what I'm going to do is make things simpler. So I'm just\ngoing to say that this probability x one, x two, all the way to x n, this is going to be proportional\nto the numerator, I don't care about the denominator, because it's the same for every\nsingle class. So this is proportional to x one, x two, x n given class K times the probability of\nthat class. Okay. All right. So in naive Bayes, the point of it being naive, is that we're actually\nthis joint probability, we're just assuming that all of these different things\nare all independent. So in my soccer example, you know, the probability that we're playing soccer,\nor the probability that, you know, it's windy, and it's rainy, and, and it's Wednesday, all these\nthings are independent, we're assuming that they're independent. So that means that I can\nactually write this part of the equation here as this. So each term in here, I can just multiply\nall of them together. So the probability of the first feature, given that it's class K,\ntimes the probability of the second feature and given this problem, like class K all the way up\nall the way up until, you know, the nth feature of given that it's class K. So this expands to\nall of this. All right, which means that this here is now proportional to the thing that we just\nexpanded times this. So I'm going to write that out. So the probability of that class.\nAnd I'm actually going to use this symbol. So what this means is it's a huge multiplication,\nit means multiply everything to the right of this. So this probability x, given some class K,\nbut do it for all the i's. So I, what is I, okay, we're going to go from the first\nthe first x i all the way to the nth. So that means for every single i, we're just multiplying\nthese probabilities together. And that's where this up here comes from. So to wrap this up,\noops, this should be a line to wrap this up in plain English. Basically, what this is saying\nis a probability that you know, we're in some category, given that we have all these different\nfeatures is proportional to the probability of that class in general, times the probability of\neach of those features, given that we're in this one class that we're testing. So the probability\nof it, you know, of us playing soccer today, given that it's rainy, not windy, and and it's\nWednesday, is proportional to Okay, well, what is what is the probability that we play soccer\nanyways, and then times the probability that it's rainy, given that we're playing soccer,\ntimes the probability that it's not windy, given that we're playing soccer. So how many times are\nwe playing soccer when it's windy, how you know, and then how many times are what's the probability\nthat's Wednesday, given that we're playing soccer. Okay. So how do we use this in order to make a\nclassification. So that's where this comes in our y hat, our predicted y is going to be equal to\nsomething called the arg max. And then this expression over here, because we want to take\nthe arg max. Well, we want. So okay, if I write out this, again, this means the probability of\nbeing in some class CK given all of our evidence. Well, we're going to take the K that maximizes\nthis expression on the right. That's what arc max means. So if K is in zero, oops,\none through K, so this is how many categories are, we're going to go through each K. And we're going\nto solve this expression over here and find the K that makes that the largest. Okay. And remember\nthat instead of writing this, we have now a formula, thanks to Bayes rule for helping us\napproximate that right in something that maybe we can we maybe we have like the evidence for that,\nwe have the answers for that based on our training set. So this principle of going through each of\nthese and finding whatever class whatever category maximizes this expression on the right,\nthis is something known as MAP for short, or maximum a posteriori.\nPick the hypothesis. So pick the K that is the most probable so that we minimize the probability\nof misclassification. Right. So that is MAP. That is naive Bayes. Back to the notebook. So\njust like how I imported k nearest neighbor, k neighbors classifier up here for naive Bayes,\nI can go to SK learn naive Bayes. And I can import Gaussian naive Bayes.\nRight. And here I'm going to say my naive Bayes model is equal. This is very similar to what we\nhad above. And I'm just going to say with this model, we are going to fit x train and y train.\nAll right, just like above. So this, I might actually, so I'm going to set that. And\nexactly, just like above, I'm going to make my prediction. So here, I'm going to instead use my\nnaive Bayes model. And of course, I'm going to run the classification report again. So I'm actually\njust going to put these in the same cell. But here we have the y the new y prediction and then y test\nis still our original test data set. So if I run this, you'll see that. Okay, what's going on here,\nwe get worse scores, right? Our precision, for all of them, they look slightly worse. And our,\nyou know, for our precision, our recall, our f1 score, they look slightly worse for all the different\ncategories. And our total accuracy, I mean, it's still 72%, which is not too shabby. But it's still\n72%. Okay. Which, you know, is not not that great. Okay, so let's move on to logistic regression.\nHere, I've drawn a plot, I have y. So this is my label on one axis. And then this is maybe one of\nmy features. So let's just say I only have one feature in this case, text zero, right? Well,\nwe see that, you know, I have a few of one class type down here. And we know it's one class type\nbecause it's zero. And then we have our other class type one up here. And then we have our\ny. Okay. So many of you guys are familiar with regression. So let's start there. If I were to\ndraw a regression line through this, it might look something like like this. Right? Well, this\ndoesn't seem to be a very good model. Like, why would we use this specific line to predict why?\nRight? It's, it's iffy. Okay. For example, we might say, okay, well, it seems like, you know,\neverything from here downwards would be one class type in here, upwards would be another class type.\nBut when you look at this, you're just you, you visually can tell, okay, like, that line doesn't\nmake sense. Things are not those dots are not along that line. And the reason is because we\nare doing classification, not regression. Okay. Well, first of all, let's start here, we know that\nthis model, if we just use this line, it equals m x. So whatever this let's just say it's x plus b,\nwhich is the y intercept, right? And m is the slope. But when we use a linear regression,\nis it actually y hat? No, it's not right. So when we're working with linear regression,\nwhat we're actually estimating in our model is a probability, what's a probability between zero\nand one, that is class zero or class one. So here, let's rewrite this as p equals m x plus b.\nOkay, well, m x plus b, that can range, you know, from negative infinity to infinity,\nright? For any for any value of x, it goes from negative infinity to infinity.\nBut probability, we know probably one of the rules of probability is that probability has to stay\nbetween zero and one. So how do we fix this? Well, maybe instead of just setting the probability\nequal to that, we can set the odds equal to this. So by that, I mean, okay, let's do probability\ndivided by one minus the probability. Okay, so now becomes this ratio. Now this ratio is allowed to\ntake on infinite values. But there's still one issue here. Let me move this over a bit.\nThe one issue here is that m x plus b, that can still be negative, right? Like if you know,\nI have a negative slope, if I have a negative b, if I have some negative x's in there, I don't know,\nbut that can be that's allowed to be negative. So how do we fix that? We do that by actually taking\nthe log of the odds. Okay. So now I have the log of you know, some probability divided by one minus\nthe probability. And now that is on a range of negative infinity to infinity, which is good\nbecause the range of log should be negative infinity to infinity. Now how do I solve for P\nthe probability? Well, the first thing I can do is take, you know, I can remove the log by taking\nthe not the e to the whatever is on both sides. So that gives me the probability\nover the one minus the probability is now equal to e to the m x plus b. Okay. So let's multiply\nthat out. So the probability is equal to one minus probability e to the m x plus b. So P is equal to\ne to the m x plus b minus P times e to the m x plus b. And now we have we can move like terms to\none side. So if I do P, so basically, I'm moving this over, so I'm adding P. So now P one plus e\nto the m x plus b is equal to e to the m x plus b and let me change this parentheses make it a\nlittle bigger. So now my probability can be e to the m x plus b divided by one plus e to the m x plus b.\nOkay, well, let me just rewrite this really quickly, I want a numerator of one on top.\nOkay, so what I'm going to do is I'm going to multiply this by negative m x plus b,\nand then also the bottom by negative m x plus b, and I'm allowed to do that because\nthis over this is one. So now my probability is equal to one over\none plus e to the negative m x plus b. And now why did I rewrite it like that?\nIt's because this is actually a form of a special function, which is called the sigmoid\nfunction. And for the sigmoid function, it looks something like this. So s of x sigmoid, you know,\nthat some x is equal to one over one plus e to the negative x. So essentially, what I just did up here\nis rewrite this in some sigmoid function, where the x value is actually m x plus b.\nSo maybe I'll change this to y just to make that a bit more clear, it doesn't matter what\nthe variable name is. But this is our sigmoid function. And visually, what our sigmoid function\nlooks like is it goes from zero. So this here is zero to one. And it looks something like this\ncurved s, which I didn't draw too well. Let me try that again. It's hard to draw\nsomething if I can draw this right. Like that. Okay, so it goes in between zero and one.\nAnd you might notice that this form fits our shape up here.\nOops, let's draw it sharper. But if it's our shape up there a lot better, right?\nAlright, so that is what we call logistic regression, we're basically trying to fit our data\nto the sigmoid function. Okay. And when we only have, you know, one data point, so if we only have\none feature x, and that's what we call simple logistic regression. But then if we have, you know,\nso that's only x zero, but then if we have x zero, x one, all the way to x n, we call this\nmultiple logistic regression, because there are multiple features that we're considering\nwhen we're building our model, logistic regression. So I'm going to put that here.\nAnd again, from SK learn this linear model, we can import logistic regression. All right.\nAnd just like how we did above, we can repeat all of this. So here, instead of NB, I'm going to call\nthis log model, or LG logistic regression. I'm going to change this to logistic regression.\nSo I'm just going to use the default logistic regression. But actually, if you look here,\nyou see that you can use different penalties. So right now we're using\nan L2 penalty. But L2 is our quadratic formula. Okay, so that means that for,\nyou know, outliers, it would really penalize that. For all these other things, you know,\nyou can toggle these different parameters, and you might get slightly different results.\nIf I were building a production level logistic regression model, then I would want to go and I\nwould want to figure out how to do that. So I'm going to go ahead and I'm going to go ahead and\nI would want to figure out, you know, what are the best parameters to pass into here,\nbased on my validation data. But for now, we'll just we'll just use this out of the box.\nSo again, I'm going to fit the X train and the Y train. And I'm just going to predict again,\nso I can just call this again. And instead of LG, NB, I'm going to use LG. So here, this is decent\nprecision 65% recall 71, f 168, or 82 total accuracy of 77. Okay, so it performs slightly\nbetter than I base, but it's still not as good as K and N. Alright, so the last model for\nclassification that I wanted to talk about is something called support vector machines,\nor SVMs for short. So what exactly is an SVM model, I have two different features x zero and\nx one on the axes. And then I've told you if it's you know, class zero or class one based on the\nblue and red labels, my goal is to find some sort of line between these two labels that best divides\nthe data. Alright, so this line is our SVM model. So I call it a line here because in 2d, it's a\nline, but in 3d, it would be a plane and then you can also have more and more dimensions. So the\nproper term is actually I want to find the hyperplane that best differentiates these two\nclasses. Let's see a few examples. Okay, so first, between these three lines, let's say A, B, and C,\nand C, which one is the best divider of the data, which one has you know, all the data on one side\nor the other, or at least if it doesn't, which one divides it the most, right, like which one\nis has the most defined boundary between the two different groups. So this this question should be\npretty straightforward. It should be a right because a has a clear distinct line between where you\nknow, everything on this side of a is one label, it's negative and everything on this side of a\nis the other label, it's positive. So what if I have a but then what if I had drawn my B\nlike this, and my C, maybe like this, sorry, they're kind of the labels are kind of close together.\nBut now which one is the best? So I would argue that it's still a, right? And why is it still a?\nRight? And why is it still a? Because in these other two, look at how close this is to that,\nto these points. Right? So if I had some new point that I wanted to estimate, okay,\nsay I didn't have A or B. So let's say we're just working with C. Let's say I have some new point\nthat's right here. Or maybe a new point that's right there. Well, it seems like just logically\nlooking at this. I mean, without the boundary, that would probably go under the positives,\nright? I mean, it's pretty close to that other positive. So one thing that we care about in SVM\nis something known as the margin. Okay, so not only do we want to separate the two classes really\nwell, we also care about the boundary in between where the points in those classes in our data set\nare, and the line that we're drawing. So in a line like this, the closest values to this line\nmight be like here. And I'm trying to draw these perpendicular. Right? And so this effectively,\nif I switch over to these dotted lines, if I can draw this right. So these effectively\nare what's known as the margins. Okay, so these both here, these are our margins in our SVMs.\nAnd our goal is to maximize those margins. So not only do we want the line that best separates the\ntwo different classes, we want the line that has the largest margin. And the data points that lie\non the margin lines, the data. So basically, these are the data points that's helping us define our\ndivider. These are what we call support vectors. Hence the name support vector machines. Okay,\nso the issue with SVM sometimes is that they're not so robust to outliers. Right? So for example,\nif I had one outlier, like this up here, that would totally change where I want my support\nvector to be, even though that might be my only outlier. Okay. So that's just something to keep\nin mind. As you know, when you're working with SVM is, it might not be the best model if there\nare outliers in your data set. Okay, so another example of SVMs might be, let's say that we have\ndata like this, I'm just going to use a one dimensional data set for this example. Let's\nsay we have a data set that looks like this. Well, our, you know, separators should be\nperpendicular to this line. But it should be somewhere along this line. So it could be\nanywhere like this. You might argue, okay, well, there's one here. And then you could also just\ndraw another one over here, right? And then maybe you can have two SVMs. But that's not really how\nSVMs work. But one thing that we can do is we can create some sort of projection. So I realize here\nthat one thing I forgot to do was to label where zero was. So let's just say zero is here.\nNow, what I'm going to do is I'm going to say, okay, I'm going to have x, and then I'm going to\nhave x, sorry, x zero and x one. So x zero is just going to be my original x. But I'm going to make\nx one equal to let's say, x squared. So whatever is this squared, right? So now, my natives would be,\nyou know, maybe somewhere here, here, just pretend that it's somewhere up here.\nRight. And now my pluses might be something like\nthat. And I'm going to run out of space over here. So I'm just going to draw these together,\nuse your imagination. But once I draw it like this, well, it's a lot easier to apply a boundary,\nright? Now our SVM could be maybe something like this, this. And now you see that we've divided\nour data set. Now it's separable where one class is this way. And the other class is that way.\nOkay, so that's known as SVMs. I do highly suggest that, you know, any of these models that we just\nmentioned, if you're interested in them, do go more in depth mathematically into them. Like how\ndo we how do we find this hyperplane? Right? I'm not going to go over that in this specific course,\nbecause you're just learning what an SVM is. But it's a good idea to know, oh, okay, this is the\ntechnique behind finding, you know, what exactly are the are the how do you define the hyperplane\nthat we're going to use. So anyways, this transformation that we did down here, this is known\nas the kernel trick. So when we go from x to some coordinate x, and then x squared,\nwhat we're doing is we are applying a kernel. So that's why it's called the kernel trick.\nSo SVMs are actually really powerful. And you'll see that here. So from sk learn.svm, we are going\nto import SVC. And SVC is our support vector classifier. So with this, so with our SVM model,\nwe are going to, you know, create SVC model. And we are going to, again, fit this to X train, I\ncould have just copied and pasted this, I should be able to do that. So we're going to create SVC\nagain, fit this to X train, I could have just copied and pasted this, I should have probably\ndone that. Okay, taking a bit longer. All right. Let's predict using RSVM model. And here,\nlet's see if I can hover over this. Right. So again, you see a lot of these different\nparameters here that you can go back and change if you were creating a production level model. Okay,\nbut in this specific case, we'll just use it out of the box again. So if I make predictions,\nyou'll note that Wow, the accuracy actually jumps to 87% with the SVM. And even with class zero,\nthere's nothing less than, you know, point eight, which is great. And for class one,\nI mean, everything's at 0.9, which is higher than anything that we had seen to this point.\nSo so far, we've gone over four different classification models, we've done SVM,\nlogistic regression, naive Bayes and cannon. And these are just simple ways on how to implement\nthem. Each of these they have different, you know, they have different hyper parameters that you can\ngo and you can toggle. And you can try to see if that helps later on or not. But for the most part,\nthey perform, they give us around 70 to 80% accuracy. Okay, with SVM being the best. Now,\nlet's see if we can actually beat that using a neural net. Now the final type of model that\nI wanted to talk about is known as a neural net or neural network. And neural nets look something\nlike this. So you have an input layer, this is where all your features would go. And they have\nall these arrows pointing to some sort of hidden layer. And then all these arrows point to some\nsort of output layer. So what is what is all this mean? Each of these layers in here, this is\nsomething known as a neuron. Okay, so that's a neuron. In a neural net. These are all of our\nfeatures that we're inputting into the neural net. So that might be x zero x one all the way through\nx n. Right. And these are the features that we talked about there, they might be you know,\nthe pregnancy, the BMI, the age, etc. Now all of these get weighted by some value. So they\nare multiplied by some w number that applies to that one specific category that one specific\nfeature. So these two get multiplied. And the sum of all of these goes into that neuron. Okay,\nso basically, I'm taking w zero times x zero. And then I'm adding x one times w one and then\nI'm adding you know, x two times w two, etc, all the way to x n times w n. And that's getting\ninput into the neuron. Now I'm also adding this bias term, which just means okay, I might want\nto shift this by a little bit. So I might add five or I might add 0.1 or I might subtract 100,\nI don't know. But we're going to add this bias term. And the output of all these things. So\nthe sum of this, this, this and this, go into something known as an activation function,\nokay. And then after applying this activation function, we get an output. And this is what a\nneuron would look like. Now a whole network of them would look something like this.\nSo I kind of gloss over this activation function. What exactly is that? This is how a neural net\nlooks like if we have all our inputs here. And let's say all of these arrows represent some sort\nof addition, right? Then what's going on is we're just adding a bunch of times, right? We're adding\nthe some sort of weight times these input layer a bunch of times. And then if we were to go back\nand factor that all out, then this entire neural net is just a linear combination of these input\nlayers, which I don't know about you, but that just seems kind of useless, right? Because we could\nliterally just write that out in a formula, why would we need to set up this entire neural network,\nwe wouldn't. So the activation function is introduced, right? So without an activation\nfunction, this just becomes a linear model. An activation function might look something like\nthis. And as you can tell, these are not linear. And the reason why we introduce these is so that\nour entire model doesn't collapse on itself and become a linear model. So over here, this is\nsomething known as a sigmoid function, it runs between zero and one, tanh runs between negative\none all the way to one. And this is ReLU, which anything less than zero is zero, and then anything\ngreater than zero is linear. So with these activation functions, every single output of a neuron\nis no longer just the linear combination of these, it's some sort of altered linear state, which means\nthat the input into the next neuron is, you know, it doesn't it doesn't collapse on itself, it doesn't\nbecome linear, because we've introduced all these nonlinearities. So this is a training set, the\nmodel, the loss, right? And then we do this thing called training, where we have to feed the loss\nback into the model, and make certain adjustments to the model to improve this predicted output.\nLet's talk a little bit about the training, what exactly goes on during that step.\nLet's go back and take a look at our L2 loss function. This is what our L2 loss function\nlooks like it's a quadratic formula, right? Well, up here, the error is really, really, really, really\nlarge. And our goal is to get somewhere down here, where the loss is decreased, right? Because that\nmeans that our predicted value is closer to our true value. So that means that we want to go\nthis way. Okay. And thanks to a lot of properties of math, something that we can do is called\ngradient descent, in order to follow this slope down this way. This quadratic is, it has different\ndifferent slopes with respect to some value. Okay, so the loss with respect to some weight\nw zero, versus w one versus w n, they might all be different. Right? So some way that I kind of\nthink about it is, to what extent is this value contributing to our loss. And we can actually\nfigure that out through some calculus, which we're not going to touch up on in this specific course.\nBut if you want to learn more about neural nets, you should probably also learn some calculus\nand figure out what exactly back propagation is doing, in order to actually calculate, you know,\nhow much do we have to backstep by. So the thing is here, you might notice that this follows\nthis curve at all of these different points. And the closer we get to the bottom, the smaller\nthis step becomes. Now stick with me here. So my new value, this is what we call a weight update,\nI'm going to take w zero, and I'm going to set some new value for w zero. And what I'm going to\nset for that is the old value of w zero, plus some factor, which I'll just call alpha for now,\ntimes whatever this arrow is. So that's basically saying, okay, take our old w zero, our old weight,\nand just decrease it this way. So I guess increase it in this direction, right, like take a step in\nthis direction. But this alpha here is telling us, okay, don't don't take a huge step, right,\njust in case we're wrong, take a small step, take a small step in that direction, see if we get any\ncloser. And for those of you who, you know, do want to look more into the mathematics of things,\nthe reason why I use a plus here is because this here is the negative gradient, right, if this were\njust the if you were to use the actual gradient, this should be a minus.\nNow this alpha is something that we call the learning rate. Okay, and that adjusts how quickly\nwe're taking steps. And that might, you know, tell our that that will ultimately control\nhow long it takes for our neural net to converge. Or sometimes if you set it too high, it might even\ndiverge. But with all of these weights, so here I have w zero, w one, and then w n. We make the same\nupdate to all of them after we calculate the loss, the gradient of the loss with respect to that\nweight. So that's how back propagation works. And that is everything that's going on here. After we\ncalculate the loss, we're calculating gradients, making adjustments in the model. So we're setting\nall the all the weights to something adjusted slightly. And then we're going to calculate the\ngradient. And then we're saying, Okay, let's take the training set and run it through the model\nagain, and go through this loop all over again. So for machine learning, we already have seen some\nlibraries that we use, right, we've already seen SK learn. But when we start going into neural\nnetworks, this is kind of what we're trying to program. And it's not very fun to try to\ndo this from scratch, because not only will we probably have a lot of bugs, but also probably\nnot going to be fast enough, right? Wouldn't it be great if there are just some, you know,\nfull time professionals that are dedicated to solving this problem, and they could literally\njust give us their code that's already running really fast? Well, the answer is, yes, that exists.\nAnd that's why we use TensorFlow. So TensorFlow makes it really easy to define these models. But\nwe also have enough control over what exactly we're feeding into this model. So for example,\nthis line here is basically saying, Okay, let's create a sequential neural net. So sequential is\njust, you know, what we've seen here, it just goes one layer to the next. And a dense layer means that\na dense layer means that all of them are interconnected. So here, this is interconnected with all of these\nnodes, and this one's all these, and then this one gets connected to all of the next ones, and so on.\nSo we're going to create 16 dense nodes with relu activation functions. And then we're going\nto create another layer of 16 dense nodes with relu activation. And then our output layer is going\nto be just one node. Okay. And that's how easy it is to define something in TensorFlow. So TensorFlow\nis an open source library that helps you develop and train your ML models. Let's implement this\nfor a neural net. So we're using a neural net for classification. Now, so our neural net model,\nwe are going to use TensorFlow, and I don't think I imported that up here. So we are going to import\nthat down here. So I'm going to import TensorFlow as TF. And enter. Cool. So my neural net model\nis going to be, I'm going to use this. So essentially, this is saying layer all these\nthings that I'm about to pass in. So yeah, layer them linear stack of layers, layer them as a model.\nAnd what that means, nope, not that. So what that means is I can pass in\nsome sort of layer, and I'm just going to use a dense layer.\nOops, dot dense. And let's say we have 32 units. Okay, I will also\nset the activation as really. And at first we have to specify the input shape. So here we have 10,\nand comma. Alright. Alright, so that's our first layer. Now our next layer, I'm just going to have\nanother dense layer of 32 units all using relu. And that's it. So for the final layer, this is\njust going to be my output layer, it's going to just be one node. And the activation is going to\nbe sigmoid. So if you recall from our logistic regression, what happened there was when we had\na sigmoid, it looks something like this, right? So by creating a sigmoid activation on our last layer,\nwe're essentially projecting our predictions to be zero or one, just like in logistic regression.\nAnd that's going to help us, you know, we can just round to zero or one and classify that way.\nOkay. So this is my neural net model. And I'm going to compile this. So in TensorFlow,\nwe have to compile it. It's really cool, because I can just literally pass in what type of optimizer\nI want, and it'll do it. So here, if I go to optimizers, I'm actually going to use atom.\nAnd you'll see that, you know, the learning rate is 0.001. So I'm just going to use that default.\nSo 0.001. And my loss is going to be binary cross entropy. And the metrics that I'm also going to\ninclude on here, so it already will consider loss, but I'm, I'm also going to tack on accuracy.\nSo we can actually see that in a plot later on. Alright, so I'm going to run this.\nAnd one thing that I'm going to also do is I'm going to define these plot definitions. So I'm\nactually copying and pasting this, I got these from TensorFlow. So if you go on to some TensorFlow\ntutorial, they actually have these, this like, defined. And that's exactly what I'm doing here.\nSo I'm actually going to move this cell up, run that. So we're basically plotting the loss\nover all the different epochs. epochs means like training cycles. And we're going to run that. So\nmeans like training cycles. And we're going to plot the accuracy over all the epochs.\nAlright, so we have our model. And now all that's left is, let's train it. Okay.\nSo I'm going to say history. So TensorFlow is great, because it keeps track of the history\nof the training, which is why we can go and plot it later on. Now I'm going to set that equal to\nthis neural net model. And fit that with x train, y train, I'm going to make the number of epochs\nequal to let's say just let's just use 100 for now. And the batch size, I'm going to set equal to,\nlet's say 32. Alright. And the validation split. So what the validation split does, if it's down\nhere somewhere. Okay, so yeah, this validation split is just the fraction of the training data\nto be used as validation data. So essentially, every single epoch, what's going on is TensorFlow\nsaying, leave certain if this is point two, then leave 20% out. And we're going to test how the\nmodel performs on that 20% that we've left out. Okay, so it's basically like our validation data\nset. But TensorFlow does it on our training data set during the training. So we have now a measure\noutside of just our validation data set to see, you know, what's going on. So validation split,\nI'm going to make that 0.2. And we can run this. So if I run that, all right, and I'm actually going\nto set verbose equal to zero, which means, okay, don't print anything, because printing something\nfor 100 epochs might get kind of annoying. So I'm just going to let it run, let it train,\nand then we'll see what happens. Cool, so it finished training. And now what I can do is\nbecause you know, I've already defined these two functions, I can go ahead and I can plot the loss,\noops, loss of that history. And I can also plot the accuracy throughout the training.\nSo this is a little bit ish what we're looking for. We definitely are looking for a steadily\ndecreasing loss and an increasing accuracy. So here we do see that, you know, our validation\naccuracy improves from around point seven, seven or something all the way up to somewhere around\npoint, maybe eight one. And our loss is decreasing. So this is good. It is expected that the validation\nloss and accuracy is performing worse than the training loss or accuracy. And that's because\nour model is training on that data. So it's adapting to that data. Whereas the validation stuff is,\nyou know, stuff that it hasn't seen yet. So, so that's why. So in machine learning, as we saw above,\nwe could change a bunch of the parameters, right? Like I could change this to 64. So now it'd be\na row of 64 nodes, and then 32, and then one. So I can change some of these parameters.\nAnd a lot of machine learning is trying to find, hey, what do we set these hyper parameters to?\nSo what I'm actually going to do is I'm going to rewrite this so that we can do something what's\nknown as a grid search. So we can search through an entire space of hey, what happens if, you know,\nwe have 64 nodes and 64 nodes, or 16 nodes and 16 nodes, and so on. And then on top of all that,\nwe can, you know, we can change this learning rate, we can change how many epochs we can change,\nyou know, the batch size, all these things might affect our training. And just for kicks,\nI'm also going to add what's known as a dropout layer in here. And what dropout is doing is\nsaying, hey, randomly choose with at this rate, certain nodes, and don't train them in, you know,\nin a certain iteration. So this helps prevent overfitting. Okay, so I'm actually going to\ndefine this as a function called train model, we're going to pass in x train, y train,\nthe number of nodes, the dropout, you know, the probability that we just talked about\nlearning rate. So I'm actually going to say lr batch size. And we can also pass in number epochs,\nright? I mentioned that as a parameter. So indent this, so it goes under here. And with these two,\nI'm going to set this equal to number of nodes. And now with the two dropout layers, I'm going\nto set dropout prob. So now you know, the probability of turning off a node during the training\nis equal to dropout prob. And I'm going to keep the output layer the same. Now I'm compiling it,\nbut this here is now going to be my learning rate. And I still want binary cross entropy and\naccuracy. We are actually going to train our model inside of this function. But here we can do the\nepochs equal epochs, and this is equal to whatever, you know, we're passing in x train,\ny train belong right here. Okay, so those are getting passed in as well. And finally, at the\nend, I'm going to return this model and the history of that model. Okay. So now what I'll do\nis let's just go through all of these. So let's say let's keep epochs at 100. And now what I can\ndo is I can say, hey, for a number of nodes in, let's say, let's do 1632 and 64, to see what\nhappens for the different dropout probabilities. And I mean, zero would be nothing. Let's use 0.2.\nAlso, to see what happens. You know, for the learning rate in 0.005, 0.001. And you know,\nmaybe we want to throw on 0.1 in there as well. And then for the batch size, let's do 1632,\n64 as well. Actually, and let's also throw in 128. Actually, let's get rid of 16. Sorry,\nso 128 in there. That should be 01. I'm going to record the model and history using this\ntrain model here. So we're going to do x train y train, the number of nodes is going to be,\nyou know, the number of nodes that we've defined here, dropout, prob, LR, batch size, and epochs.\nOkay. And then now we have both the model and the history. And what I'm going to do is again,\nI want to plot the loss for the history. I'm also going to plot the accuracy.\nProbably should have done them side by side, that probably would have been easier.\nOkay, so what I'm going to do is split up, split this up. And that will be\nthe subplots. So now this is just saying, okay, I want one row and two columns in that row for my\nplots. Okay, so I'm going to plot on my axis one, the loss. I don't actually know this is going to\nwork. Okay, we don't care about the grid. Yeah, let's let's keep the grid. And then now my other.\nSo now on here, I'm going to plot all the accuracies on the second plot.\nI might have to debug this a bit.\nWe should be able to get rid of that. If we run this, we already have history saved as a variable\nin here. So if I just run it on this, okay, it has no attribute x label. Oh, I think it's because\nit's like set x label or something. Okay, yeah, so it's, it's set instead of just x label, y label.\nSo let's see if that works. All right, cool. Um, and let's actually make this a bit larger.\nOkay, so we can actually change the figure size that I'm gonna set. Let's see what happens if I\nset that to. Oh, that's not the way I wanted it. Okay, so that looks reasonable.\nAnd that's just going to be my plot history function. So now I can plot them side by side.\nHere, I'm going to plot the history. And what I'm actually going to do is I so here, first,\nI'm going to print out all these parameters. So I'm going to print out\nthe F string to print out all of this stuff. So here, I'm going to print out all these parameters.\nUh, all of this stuff. So here, I'm printing out how many nodes, um, the dropout probability,\nuh, the learning rate.\nAnd we already know how many you found, so I'm not even going to bother with that.\nSo once we plot this, uh, let's actually also figure out what the, um, what the validation\nlosses on our validation set that we have that we created all the way back up here.\nAlright, so remember, we created three data sets. Let's call our model and evaluate what the\nvalidation data with the validation data sets loss would be. And I actually want to record,\nlet's say I want to record whatever model has the least validation loss. So\nfirst, I'm going to initialize that to infinity so that you know, any model will beat that score.\nSo if I do float infinity, that will set that to infinity. And maybe I'll keep\ntrack of the parameters. Actually, it doesn't really matter. I'm just going to keep track of\nthe model. And I'm gonna set that to none. So now down here, if the validation loss is ever\nless than the least validation loss, then I am going to simply come down here and say,\nHey, this validation for this least validation loss is now equal to the validation loss.\nAnd the least loss model is whatever this model is that just earned that validation loss. Okay.\nSo we are actually just going to let this run for a while. And then we're going to get our least\nlast model after that. So let's just run. All right, and now we wait.\nAll right, so we've finally finished training. And you'll notice that okay, down here, the loss\nactually gets to like 0.29. The accuracy is around 88%, which is pretty good. So you might be wondering,\nokay, why is this accuracy in this? Like, these are both the validation. So this accuracy here\nis on the validation data set that we've defined at the beginning, right? And this one here,\nthis is actually taking 20% of our tests, our training set every time during the training,\nand saying, Okay, how much of it do I get right now? You know, after this one step where I didn't\ntrain with any of that. So they're slightly different. And actually, I realized later on\nthat I probably you know, probably what I should have done is over here, when we were defining\nthe model fit, instead of the validation split, you can define the validation data.\nAnd you can pass in the validation data, I don't know if this is the proper syntax. But\nthat's probably what I should have done. But instead, you know, we'll just stick with what\nwe have here. So you'll see at the end, you know, with the 64 nodes, it seems like this is our best\nperformance 64 nodes with a dropout of 0.2, a learning rate of 0.001, and a batch size of 64.\nAnd it does seem like yes, the validation, you know, the fake validation, but the validation\nloss is decreasing, and then the accuracy is increasing, which is a good sign. Okay,\nso finally, what I'm going to do is I'm actually just going to predict. So I'm going to take\nthis model, which we've called our least loss model, I'm going to take this model,\nand I'm going to predict x test on that. And you'll see that it gives me some values that\nare really close to zero and some that are really close to one. And that's because we have a sigmoid\noutput. So if I do this, and what I can do is I can cast them. So I'm going to say anything that's\ngreater than 0.5, set that to one. So if I actually, I think what happens if I do this?\nOh, okay, so I have to cast that as type. And so now you'll see that it's ones and zeros. And I'm\nactually going to transform this into a column as well. So here I'm going to Oh, oops, I didn't\nI didn't mean to do that. Okay, no, I wanted to just reshape it to that. So now it's one dimensional.\nOkay. And using that we can actually just rerun the classification report based on these this\nneural net output. And you'll see that okay, the the F ones are the accuracy gives us 87%. So it\nseems like what happened here is the precision on class zero. So the hadrons has increased a bit,\nbut the recall decreased. But the F one score is still at a good point eight one. And for the other\nclass, it looked like the precision decreased a bit the recall increased for an overall F one score.\nThat's also been increased. I think I interpreted that properly. I mean, we went through all this\nwork and we got a model that performs actually very, very similarly to the SVM model that we\nhad earlier. And the whole point of this exercise was to demonstrate, okay, these are how you can\ndefine your models. But it's also to say, hey, maybe, you know, neural nets are very, very\npowerful, as you can tell. But sometimes, you know, an SVM or some other model might actually be more\nappropriate. But in this case, I guess it didn't really matter which one we use at the end. An 87%\naccuracy score is still pretty good. So yeah, let's now move on to regression.\nWe just saw a bunch of different classification models. Now let's shift gears into regression,\nthe other type of supervised learning. If we look at this plot over here, we see a bunch of scattered\ndata points. And here we have our x value for those data points. And then we have the corresponding y\nvalue, which is now our label. And when we look at this plot, well, our goal in regression is to find\nthe line of best fit that best models this data. Essentially, we're trying to let's say we're given\nsome new value of x that we don't have in our sample, we're trying to say, okay, what would my\nprediction for y be for that given x value. So that, you know, might be somewhere around there.\nI don't know. But remember, in regression that, you know, given certain features,\nwe're trying to predict some continuous numerical value for y.\nIn linear regression, we want to take our data and fit a linear model to this data. So in this case,\nour linear model might look something along the lines of here. Right. So this here would be\nconsidered as maybe our line of best fit. And this line is modeled by the equation, I'm going to write\nit down here, y equals b zero, plus b one x. Now b zero just means it's this y intercept. So if we\nextend this y down here, this value here is b zero, and then b one defines the source of the\nline, defines the slope of this line. Okay. All right. So that's the that's the formula\nfor linear regression. And how exactly do we come up with that formula? What are we trying to do\nwith this linear regression? You know, we could just eyeball where the line be, but humans are\nnot very good at eyeballing certain things like that. I mean, we can get close, but a computer is\nbetter at giving us a precise value for b zero and b one. Well, let's introduce the concept of\nsomething known as a residual. Okay, so residual, you might also hear this being called the error.\nAnd what that means is, let's take some data point in our data set. And we're going to evaluate how\nfar off is our prediction from a data point that we already have. So this here is our y, let's say,\nthis is 12345678. So this is y eight, let's call it, you'll see that I use this y i in order to\nI in order to represent, hey, just one of these points. Okay. So this here is why and this here\nwould be the prediction. Oops, this here would be the prediction for y eight, which I've labeled\nwith this hat. Okay, if it has a hat on it, that means hey, this is what this is my guess this is\nmy prediction for you know, this specific value of x. Okay. Now the residual would be this distance\nhere between y eight and y hat eight. So y eight minus y hat eight. All right, because that would\ngive us this here. And I'm just going to take the absolute value of this. Because what if it's below\nthe line, right, then you would get a negative value, but distance can't be negative. So we're\njust going to put a little hat, or we're going to put a little absolute value around this quantity.\nAnd that gives us the residual or the error. So let me rewrite that. And you know, to generalize\nto all the points, I'm going to say the residual can be calculated as y i minus y hat of i. Okay.\nSo this just means the distance between some given point, and its prediction, its corresponding\nprediction on the line. So now, with this residual, this line of best fit is generally trying to\ndecrease these residuals as much as possible. So now that we have some value for the error,\nour line of best fit is trying to decrease the error as much as possible for all of the different\ndata points. And that might mean, you know, minimizing the sum of all the residuals. So this\nhere, this is the sum symbol. And if I just stick the residual calculation in there,\nit looks something like that, right. And I'm just going to say, okay, for all of the eyes in our\ndata set, so for all the different points, we're going to sum up all the residuals. And I'm going\nto try to decrease that with my line of best fit. So I'm going to find the B0 and B1, which gives\nme the lowest value of this. Okay. Now in other, you know, sometimes in different circumstances,\nwe might attach a squared to that. So we're trying to decrease the sum of the squared residuals.\nAnd what that does is it just, you know, it adds a higher penalty for how far off we are from,\nyou know, points that are further off. So that is linear regression, we're trying to find\nthis equation, some line of best fit that will help us decrease this measure of error\nwith respect to all the data points that we have in our data set, and try to come up with\nthe best prediction for all of them. This is known as simple linear regression.\nAnd basically, that means, you know, our equation looks something like this. Now, there's also\nmultiple linear regression, which just means that hey, if we have more than one value for x, so like\nthink of our feature vectors, we have multiple values in our x vector, then our predictor might\nlook something more like this. Actually, I'm just going to say etc, plus b n, x n. So now I'm coming\nup with some coefficient for all of the different x values that I have in my vector. Now you guys\nmight have noticed that I have some assumptions over here. And you might be asking, okay, Kylie,\nwhat in the world do these assumptions mean? So let's go over them.\nSo let's go over them. The first one is linearity.\nAnd what that means is, let's say I have a data set. Okay.\nLinearity just means, okay, my does my data follow a linear pattern? Does y increase as x\nincreases? Or does y decrease at as x increases? Does so if y increases or decreases at a constant\nrate as x increases, then you're probably looking at something linear. So what's the example of a\nnonlinear data set? Let's say I had data that might look something like that. Okay. So now just\nvisually judging this, you might say, okay, seems like the line of best fit might actually be some\ncurve like this. Right. And in this case, we don't satisfy that linearity assumption anymore.\nSo with linearity, we basically just want our data set to follow some sort of linear trajectory.\nAnd independence, our second assumption\njust means this point over here, it should have no influence on this point over here,\nor this point over here, or this point over here. So in other words, all the points,\nall the samples in our data set should be independent. Okay, they should not rely on\none another, they should not affect one another.\nOkay, now, normality and homoscedasticity, those are concepts which use this residual. Okay. So if\nI have a plot that looks something like this, and I have a plot that looks like this. Okay,\nsomething like this. And my line of best fit is somewhere here, maybe it's something like that.\nIn order to look at these normality and homoscedasticity assumptions, let's look at\nthe residual plot. Okay. And what that means is I'm going to keep my same x axis. But instead\nof plotting now where they are relative to this y, I'm going to plot these errors. So now I'm\ngoing to plot y minus y hat like this. Okay. And now you know, this one is slightly positive,\nso it might be here, this one down here is negative, it might be here. So our residual plot,\nit's literally just a plot of how you know, the values are distributed around our line of best\nfit. So it looks like it might, you know, look something like this. Okay. So this might be our\nresidual plot. And what normality means, so our assumptions are normality and homoscedasticity,\nI might have butchered that spelling, I don't really know. But what normality is saying is\nsaying, okay, these residuals should be normally distributed. Okay, around this line of best fit,\nit should follow a normal distribution. And now what homoscedasticity says, okay, our variants\nof these points should remain constant throughout. So this spread here should be approximately the\nsame as this spread over here. Now, what's an example of where you know, homoscedasticity is\nnot held? Well, let's say that our original plot actually looks something like this.\nOkay, so now if we looked at the residuals for that, it might look something\nlike that. And now if we look at this spread of the points, it decreases, right? So now the spread\nis not constant, which means that homoscedasticity, this assumption would not be fulfilled, and it\nmight not be appropriate to use linear regression. So that's just linear regression. Basically,\nwe have a bunch of data points, we want to predict some y value for those. And we're trying to come\nup with this line of best fit that best describes, hey, given some value x, what would be my best\nguess of what y is. So let's move on to how do we evaluate a linear regression model. So the first\nmeasure that I'm going to talk about is known as mean absolute error, or MAE\nfor short, okay. And mean absolute error is basically saying, all right, let's take\nall the errors. So all these residuals that we talked about, let's sum up the distance\nfor all of them, and then take the average. And then that can describe, you know, how far off are\nwe. So the mathematical formula for that would be, okay, let's take all the residuals.\nAlright, so this is the distance. Actually, let me redraw a plot down here. So\nsuppose I have a data set, look like this. And here are all my data points, right. And now let's\nsay my line looks something like that. So my mean absolute error would be summing up all of these\nvalues. This was a mistake. So summing up all of these, and then dividing by how many data points\nI have. So what would be all the residuals, it would be y i, right, so every single point,\nminus y hat i, so the prediction for that on here. And then we're going to sum over all of\nall of the different i's in our data set. Right, so i, and then we divide by the number of points\nwe have. So actually, I'm going to rewrite this to make it a little clearer. So i is equal to\nwhatever the first data point is all the way through the nth data point. And then we divide\nit by n, which is how many points there are. Okay, so this is our measure of mae. And this is basically\ntelling us, okay, in on average, this is the distance between our predicted value and the\nactual value in our training set. Okay. And mae is good because it allows us to, you know, when we\nget this value here, we can literally directly compare it to whatever units the y value is in.\nSo let's say y is we're talking, you know, the prediction of the price of a house, right, in\ndollars. Once we have once we calculate the mae, we can literally say, oh, the average, you know,\nprice, the average, how much we're off by is literally this many dollars. Okay. So that's the\nmean absolute error. An evaluation technique that's also closely related to that is called the mean\nsquared error. And this is MSE for short. Okay. Now, if I take this plot again, and I duplicated\nand move it down here, well, the gist of mean squared error is kind of the same, but instead\nof the absolute value, we're going to square. So now the MSE is something along the lines of,\nokay, let's sum up something, right, so we're going to sum up all of our errors.\nSo now I'm going to do y i minus y hat i. But instead of absolute valuing them,\nI'm going to square them all. And then I'm going to divide by n in order to find the mean. So\nbasically, now I'm taking all of these different values, and I'm squaring them first before I add\nthem to one another. And then I divide by n. And the reason why we like using mean squared error\nis that it helps us punish large errors in the prediction. And later on, MSE might be important\nbecause of differentiability, right? So a quadratic equation is differentiable, you know,\nif you're familiar with calculus, a quadratic equation is differentiable, whereas the absolute\nvalue function is not totally differentiable everywhere. But if you don't understand that,\ndon't worry about it, you won't really need it right now. And now one downside of mean squared\nerror is that once I calculate the mean squared error over here, and I go back over to y, and I\nwant to compare the values. Well, it gets a little bit trickier to do that because now my mean squared\nerror is in terms of y squared, right? It's this is now squared. So instead of just dollars, how,\nyou know, how many dollars off am I I'm talking how many dollars squared off am I. And that,\nyou know, to humans, it doesn't really make that much sense. Which is why we have created\nsomething known as the root mean squared error. And I'm just going to copy this diagram over here\nbecause it's very, very similar to mean squared error. Except now we take a big squared root.\nOkay, so this is our messy, and we take the square root of that mean squared error. And so now the\nterm in which you know, we're defining our error is now in terms of that dollar sign symbol again.\nSo that's a pro of root mean squared error is that now we can say, okay, our error according\nto this metric is this many dollar signs off from our predictor. Okay, so it's in the same unit,\nwhich is one of the pros of root mean squared error. And now finally, there is the coefficient\nof determination, or r squared. And this is a formula for r squared. So r squared is equal\nto one minus RSS over TSS. Okay, so what does that mean? Basically, RSS stands for the sum\nof the squared residuals. So maybe it should be SSR instead, but\nRSS sum of the squared residuals, and this is equal to if I take the sum of all the values,\nand I take y i minus y hat, i, and square that, that is my RSS, right, it's a sum of the squared\nresiduals. Now TSS, let me actually use a different color for that.\nSo TSS is the total sum of squares.\nAnd what that means is that instead of being with respect to this prediction,\nwe are instead going to\ntake each y value and just subtract the mean of all the y values, and square that.\nOkay, so if I drew this out,\nand if this were my\nactually, let's use a different color. Let's use green. If this were my predictor,\nso RSS is giving me this measure here, right? It's giving me some estimate of how far off we are from\nour regressor that we predicted. Actually, I'm gonna take this one, and I'm gonna take this one,\nand actually, I'm going to use red for that. Well, TSS, on the other hand, is saying, okay,\nhow far off are these values from the mean. So if we literally didn't do any calculations for the\nline of best fit, if we just took all the y values and average all of them, and said, hey,\nthis is the average value for every single x value, I'm just going to predict that average value\ninstead, then it's asking, okay, how far off are all these points from that line?\nOkay, and remember that this square means that we're punishing larger errors, right? So even if\nthey look somewhat close in terms of distance, the further a few data points are, then the further\nthe larger our total sum of squares is going to be. Sorry, that was my dog. So the total sum of\nsquares is taking all of these values and saying, okay, what is the sum of squares, if I didn't do\nany regressor, and I literally just calculated the average of all the y values in my data set,\nand for every single x value, I'm just going to predict that average, which means that okay,\nlike, that means that maybe y and x aren't associated with each other at all. Like the\nbest thing that I can do for any new x value, just predict, hey, this is the average of my data set.\nAnd this total sum of squares is saying, okay, well, with respect to that average,\nwhat is our error? Right? So up here, the sum of the squared residuals, this is telling us what is\nour what what is our error with respect to this line of best fit? Well, our total sum of squares\nsaying what is the error with respect to, you know, just the average y value. And if our line\nof best fit is a better fit, then this total sum of squares, that means that you know, this numerator,\nthat means that this numerator is going to be smaller than this denominator, right?\nAnd if our errors in our line of best fit are much smaller, then that means that this ratio\nof the RSS over TSS is going to be very small, which means that R squared is going to go towards\none. And now when R squared is towards one, that means that that's usually a sign that we have a\ngood predictor. It's one of the signs, not the only one. So over here, I also have, you know,\nthat there's this adjusted R squared. And what that does, it just adjusts for the number of terms.\nSo x1, x2, x3, etc. It adjusts for how many extra terms we add, because usually when we,\nyou know, add an extra term, the R squared value will increase because that'll help us predict\ny some more. But the value for the adjusted R squared increase if the new term actually\nimproves this model fit more than expected, you know, by chance. So that's what adjusted\nR squared is. I'm not, you know, it's out of the scope of this one specific course.\nAnd now that's linear regression. Basically, I've covered the concept of residuals or errors.\nAnd, you know, how do we use that in order to find the line of best fit? And you know,\nour computer can do all the calculations for us, which is nice. But behind the scenes,\nit's trying to minimize that error, right? And then we've gone through all the different\nways of actually evaluating a linear regression model and the pros and cons of each one.\nSo now let's look at an example. So we're still on supervised learning. But now we're just going to\ntalk about regression. So what happens when you don't just want to predict, you know, type 123?\nWhat happens if you actually want to predict a certain value? So again, I'm on the UCI machine\nlearning repository. And here I found this data set about bike sharing in Seoul, South Korea.\nSo this data set is predicting rental bike count. And here it's the kind of bikes rented at each\nhour. So what we're going to do, again, you're going to go into the data folder, and you're going\nto download this CSV file. And we're going to move over to collab again. And here I'm going to name\nthis FCC bikes and regression. I don't remember what I called the last one. But yeah, FCC bikes\nregression. Now I'm going to import a bunch of the same things that I did earlier. And, you know,\nI'm going to also continue to import the oversampler and the standard scaler. And then I'm actually\nalso just going to let you guys know that I have a few more things I wanted import. So this is a\nlibrary that lets us copy things. Seaborn is a wrapper over a matplotlib. So it also allows us\nto plot certain things. And then just letting you know that we're also going to be using\nTensorFlow. Okay, so one more thing that we're also going to be using, we're going to use the\nsklearn linear model library. Actually, let me make my screen a little bit bigger. So yeah,\nawesome. Run this and that'll import all the things that we need. So again, I'm just going to,\nyou know, give some credit to where we got this data set. So let me copy and paste this UCI thing.\nAnd I will also give credit to this here.\nOkay, cool. All right, cool. So this is our data set. And again, it tells us all the different\nattributes that we have right here. So I'm actually going to go ahead and paste this in here.\nFeel free to copy and paste this if you want me to read it out loud, so you can type it.\nIt's byte count, hour, temp, humidity, wind, visibility, dew point, temp, radiation, rain,\nsnow, and functional, whatever that means. Okay, so I'm going to come over here and import my data\nby dragging and dropping. All right. Now, one thing that you guys might actually need to do is\nyou might actually have to open up the CSV because there were, at first, a few like forbidding\ncharacters in mine, at least. So you might have to get rid of like, I think there was a degree here,\nbut my computer wasn't recognizing it. So I got rid of that. So you might have to go through\nand get rid of some of those labels that are incorrect. I'm going to do this. Okay. But\nafter we've done that, we've imported in here, I'm going to create a data a data frame from that. So,\nall right, so now what I can do is I can read that CSV file and I can get the data into here.\nSo so like data dot CSV. Okay, so now if I call data dot head, you'll see that I have all the\nvarious labels, right? And then I have the data in there. So I'm going to from here, I'm actually\ngoing to get rid of some of these columns that, you know, I don't really care about. So here,\nI'm going to, when I when I type this in, I'm going to drop maybe the date, whether or not it's a\nholiday, and the various seasons. So I'm just not going to care about these things. Access equals\none means drop it from the columns. So now you'll see that okay, we still have, I mean,\nI guess you don't really notice it. But if I set the data frames columns equal to data set calls,\nand I look at, you know, the first five things, then you'll see that this is now our data set.\nIt's a lot easier to read. So another thing is, I'm actually going to\ndf functional. And we're going to create this. So remember that our computers are not very good\nat language, we want it to be in zeros and ones. So here, I will convert that.\nWell, if this is equal to yes, then that that gets mapped as one. So then set type integer. All right.\nGreat. Cool. So the thing is, right now, these by counts are for whatever hour. So\nto make this example simpler, I'm just going to index on an hour, and I'm gonna say, okay,\nwe're only going to use that specific hour. So I'm just going to index on an hour, and I'm\ngoing to use an hour. So here, let's say. So this data frame is only going to be data frame where\nthe hour, let's say it equals 12. Okay, so it's noon. All right. So now you'll see that all the\nequal to 12. And I'm actually going to now drop that column. Our access equals one. Alright,\nso we run this cell. Okay, so now we got rid of the hour in here. And we just have the by count,\nthe temperature, humidity, wind, visibility, and yada, yada, yada. Alright, so what I want to do\nis I'm going to actually plot all of these. So for i in all the columns, so the range, length of\nwhatever its data frame is, and all the columns, because I don't have by count as\nactually, it's my first thing. So what I'm going to do is say for a label in data frame,\ncolumns, everything after the first thing, so that would give me the temperature and\nonwards. So these are all my features, right? I'm going to just scatter. So I want to see how that\nlabel how that specific data, how that affects the by count. So I'm going to plot the bike count on\nthe y axis. And I'm going to plot, you know, whatever the specific label is on the x axis.\nAnd I'm going to title this, whatever the label is. And, you know, make my y label, the bike count\nat noon. And the x label as just the label. Okay, now, I guess we don't even need the legend.\nWe don't even need the legend. So just show that plot. All right. So it seems like functional is\nnot really doesn't really give us any utility. So then snow rain seems like this radiation,\nyou know, is fairly linear dew point temperature, visibility, wind doesn't really seem like it does\nmuch humidity, kind of maybe like an inverse relationship. But the temperature definitely\nlooks like there's a relationship between that and the number of bikes, right. So what I'm actually\ngoing to do is I'm going to drop some of the ones that don't don't seem like they really matter. So\nmaybe wind, you know, visibility. Yeah, so I'm going to get rid of when visibility and functional.\nSo now data frame, and I'm going to drop wind, visibility, and functional. All right. And the\naxis again is the column. So that's one. So if I look at my data set, now, I have just the\ntemperature, the humidity, the dew point temperature, radiation, rain, and snow. So again,\nwhat I want to do is I want to split this into my training, my validation and my test data set,\njust as we talked before. Here, we can use the exact same thing that we just did. And we can say\nnumpy dot split, and sample, you know that the whole sample, and then create our splits\nof the data frame. And we're going to do that. But now set this to eight. Okay.\nSo I don't really care about, you know, the the full grid, the full array. So I'm just going to\nuse an underscore for that variable. But I will get my training x and y's. And actually, I don't\nhave a function for getting the x and y's. So here, I'm going to write a function defined,\nget x y. And I'm going to pass in the data frame. And I'm actually going to pass in what the name\nof the y label is, and what the x what specific x labels I want to look at. So here, if that's none,\nthen I'm just like, like, I'm only going to I'm going to get everything from the data set. That's\nnot the wildlife. So here, I'm actually going to make first a deep copy of my data frame.\nAnd that basically means I'm just copying everything over. If, if like x labels is none,\nso if not x labels, then all I'm going to do is say, all right, x is going to be whatever this\ndata frame is. And I'm just going to take all the columns. So C for C, and data frame, dot columns,\nif C does not equal the y label, right, and I'm going to get the values from that. But if there\nis the x labels, well, okay, so in order to index only one thing, so like, let's say I pass in only\none thing in here, then my data frame is, so let me make a case for that. So if the length of x\nlabels is equal to one, then what I'm going to do is just say that this is going to be x labels,\nand add that just that label values, and I actually need to reshape to make this 2d.\nSo I'm going to pass in negative one comma one there. Now, otherwise, if I have like a list of\nspecific x labels that I want to use, then I'm actually just going to say x is equal to data\nframe of those x labels, dot values. And that should suffice. Alright, so now that's just me\nextracting x. And in order to get my y, I'm going to do y equals data frame, and then passing the y\nlabel. And at the very end, I'm going to say data equals NP dot h stack. So I'm stacking them horizontally\none next to each other. And I'll take x and y, and return that. Oh, but this needs to be values.\nAnd I'm actually going to reshape this to make it 2d as well so that we can do this h stack.\nAnd I will return data x, y. So now I should be able to say, okay, get x, y, and take that data\nframe. And the y label, so my y label is byte count. And actually, so for the x label, I'm actually\ngoing to let's just do like one dimension right now. And earlier, I got rid of the plots, but we\nhad seen that maybe, you know, the temperature dimension does really well. And we might be able\nto use that to predict why. So I'm going to label this also that, you know, it's just using the\ntemperature. And I am also going to do this again for, oh, this should be train. And this should be\nvalidation. And this should be a test. Because oh, that's Val. Right. But here, it should be Val.\nAnd this should be test. Alright, so we run this and now we have our training validation and test\ndata sets for just the temperature. So if I look at x train temp, it's literally just the temperature.\nOkay, and I'm doing this first to show you simple linear regression. Alright, so right now I can\ncreate a regressor. So I can say the temp regressor here. And then I'm going to, you know, make a\nlinear regression model. And just like before, I can simply fix fit my x train temp, y train temp\nin order to train train this linear regression model. Alright, and then I can also, I can print\nthis regressor is coefficients and the intercept. So if I do that, okay, this is the coefficient\nfor whatever the temperature is, and then the the x intercept, okay, or the y intercept, sorry. All\nright. And I can, you know, score, so I can get the the r squared score. So I can score x test\nand y test. All right, so it's an r squared of around point three eight, which is better than\nzero, which would mean, hey, there's absolutely no association. But it's also not, you know, like,\ngood, it depends on the context. But, you know, the higher that number, it means the higher that\nthe two variables would be correlated, right? Which here, it's all right. It just means there's\nmaybe some association between the two. But the reason why I want to do this one D was to show\nyou, you know, if we plotted this, this is what it would look like. So if I create a scatterplot,\nand let's take the training. So this is our data. And then let's make it blue. And then if I\nalso plotted, so something that I can do is say, you know, the x range, I'm going to plot it,\nis when space, and this goes from negative 20 to 40, this piece of data. So I'm going to just say,\nlet's take 100 things from there. So I'm going to plot x, and I'm going to take this temper,\nthis, like, regressor, and predict x with that. Okay, and this label, I'm going to label that\nthe fit. And this color, let's make this red. And let's actually set the line with, so I can,\nI can change how thick that value is. Okay. Now at the very end, let's create a legend. And let's,\nall right, let's also create, you know, title, all these things that matter, in some sense. So\nhere, let's just say, this would be the bikes, versus the temperature, right? And the y label\nwould be number of bikes. And the x label would be the temperature. So I actually think that this\nmight cause an error. Yeah. So it's expecting a 2d array. So we actually have to reshape this.\nOkay, there we go. So I just had to make this an array and then reshape it. So it was 2d. Now,\nwe see that, all right, this increases. But again, remember those assumptions that we had about\nlinear regression, like this, I don't really know if this fits those assumptions, right? I just\nwanted to show you guys though, that like, all right, this is what a line of s fit through this\ndata would look like. Okay. Now, we can do multiple linear regression, right. So I'm going to go ahead\nand do that as well. Now, if I take my data set, and instead of the labels, it's actually what's\nmy current data set right now. Alright, so let's just use all of these except for the byte count,\nright. So I'm going to just say for the x labels, let's just take the data frames columns and just\nremove the byte count. So does that work? So if this part should be of x labels is none. And then\nthis should work now. Oops, sorry. Okay, so I have Oh, but this here, because it's not just the\ntemperature anymore, we should actually do this, let's say all, right. So I'm just going to quickly\nrerun this piece here so that we have our temperature only data set. And now we have our\nall data set. Okay. And this regressor, I can do the same thing. So I can do the all regressor.\nAnd I'm going to make this the linear regression. And I'm going to fit this to x train all and y\ntrain all. Okay. Alright, so let's go ahead and also score this regressor. And let's see how the\nR squared performs now. So if I test this on the test data set, what happens? Alright, so our R\nsquare seems to improve it went from point four to point five, two, which is a good sign. Okay.\nAnd I can't necessarily plot, you know, every single dimension. But this just this is just\nto say, okay, this is this is improved, right? Alright, so one cool thing that you can do with\ntensorflow is you can actually do regression, but with the neural net. So here, I'm going\nto we already have our our training data for just the temperature and just, you know, for all the\ndifferent columns. So I'm not going to bother with splitting up the data again, I'm just going to go\nahead and start building the model. So in this linear regression model, typically, you know,\nit does help if we normalize it. So that's very easy to do with tensorflow, I can just create some\nnormalizer layer. So I'm going to do tensorflow Keras layers, and get the normalization layer.\nAnd the input shape for that will just be one because let's just do it again on just the\ntemperature and the access I will make none. Now for this temp normalizer, and I should have had\nan equal sign there. I'm going to adapt this to X train temp, and reshape this to just a single vector.\nSo that should work great. Now with this model, so temp neural net model, what I can do is I can do,\nyou know, dot keras, sequential. And I'm going to pass in this normalizer layer. And then I'm\ngoing to say, hey, just give me one single dense layer with one single unit. And what that's doing\nis saying, all right, well, one single node just means that it's linear. And if you don't add any\nsort of activation function to it, the output is also linear. So here, I'm going to have tensorflow\nKeras layers dot dense. And I'm just going to have one unit. And that's going to be my model. Okay.\nSo with this model, let's compile. And for our optimizer, let's use,\nlet's use the atom again, dot atom, and we have to pass in the learning rate. So learning rate,\nand our learning rate, let's do 0.01. And now, the loss, we actually let's get this one 0.1. And the\nloss, I'm going to do mean squared error. Okay, so we run that we've compiled it, okay, great.\nAnd just like before, we can call history. And I'm going to fit this model. So here,\nif I call fit, I can just fit it, and I'm going to take the x train with the temperature,\nbut reshape it. Y train for the temperature. And I'm going to set verbose equal to zero so\nthat it doesn't, you know, display stuff. I'm actually going to set epochs equal to, let's do\n1000. And the validation data should be let's pass in the validation data set here\nas a tuple. And I know I spelled that wrong. So let's just run this.\nAnd up here, I've copied and pasted the plot loss from our previous but changed the y label\nto MSC. Because now we're talking we're dealing with mean squared error. And I'm going to plot\nthe loss of this history after it's done. So let's just wait for this to finish training and then to\nplot. Okay, so this actually looks pretty good. We see that the value is still the same. So\nthis actually looks pretty good. We see that the values are converging. So now what I can do is\nI'm going to go back up and take this plot. And we are going to just run that plot again. So\nhere, instead of this temperature regressor, I'm going to use the neural net regressor.\nThis neural net model.\nAnd if I run that, I can see that, you know, this also gives me a linear regressor,\nyou'll notice that this this fit is not entirely the same as the one\nup here. And that's due to the training process of, you know, of this neural net. So just two\ndifferent ways to try and try to find the best linear regressor. Okay, but here we're using back\npropagation to train a neural net node, whereas in the other one, they probably are not doing that.\nOkay, they're probably just trying to actually compute the line of s fit. So, okay, given this,\nwell, we can repeat the exact same exercise with our with our multiple linear regressions. Okay,\nbut I'm actually going to skip that part. I will leave that as an exercise to the viewer. Okay,\nso now what would happen if we use a neural net, a real neural net instead of just, you know,\none single node in order to predict this. So let's start on that code, we already have our\nnormalizer. So I'm actually going to take the same setup here. But instead of, you know, this\none dense layer, I'm going to set this equal to 32 units. And for my activation, I'm going to use\nRelu. And now let's duplicate that. And for the final output, I just want one answer. So I just\nwant one cell. And this activation is also going to be Relu, because I can't ever have less than\nzero bytes. So I'm just going to set that as Relu. I'm just going to name this the neural net model.\nOkay. And at the bottom, I'm going to have this neural net model. I'm going to have this neural\nnet model, I'm going to compile. And I will actually use the same compiler here. But instead of\ninstead of a learning rate of 0.01, I'll use 0.001. Okay. And I'm going to train this here.\nSo the history is this neural net model. And I'm going to fit that against x train temp, y train\ntemp, and valid validation data, I'm going to set this again equal to x val temp, and y val temp.\nNow, for the verbose, I'm going to say equal to zero epochs, let's do 100. And here for the batch\nsize, actually, let's just not do a batch size right now. Let's just try it. Let's see what happens\nhere. And again, we can plot the loss of this history after it's done training. So let's just\nrun this. And that's not what we're supposed to get. So what is going on? Here is sequential,\nwe have our temperature normalizer, which I'm wondering now if we have to redo that.\nDo that. Okay, so we do see this decline, it's an interesting curve, but we do we do see it eventually.\nSo this is our loss, which all right, if decreasing, that's a good sign.\nAnd actually, what's interesting is let's just let's plot this model again. So here instead of that.\nAnd you'll see that we actually have this like, curve that looks something like this. So actually,\nwhat if I got rid of this activation? Let's train this again. And see what happens.\nAlright, so even even when I got rid of that really at the end, it kind of knows, hey, you know, if\nit's not the best model, if we had maybe one more layer in here, these are just things that you have\nto play around with. When you're, you know, working with machine learning, it's like, you don't really\nknow what the best model is going to be. For example, this also is not brilliant. But I guess\nit's okay. So my point is, though, that with a neural net, I mean, this is not brilliant, but also\nthere's like no data down here, right? So it's kind of hard for our model to predict. In fact,\nwe probably should have started the prediction somewhere around here. My point, though, is that\nwith this neural net model, you can see that this is no longer a linear predictor, but yet we still\nget an estimate of the value, right? And we can repeat this exact same exercise, right? So let's\ndo that. Right. And we can repeat this exact same exercise with the multiple inputs. So here,\nif I now pass in all of the data, so this is my all normalizer,\nand I should just be able to pass in that. So let's move this to the next cell. Here,\nI'm going to pass in my all normalizer. And let's compile it. Yeah, those parameters look good.\nGreat. So here with the history, when we're trying to fit this model, instead of temp,\nwe're going to use our larger data set with all the features. And let's just train that.\nAnd of course, we want to plot the loss.\nOkay, so that's what our loss looks like. So an interesting curve, but it's decreasing.\nSo before we saw that our R squared score was around point five, two. Well, we don't really have\nthat with a neural net anymore. But one thing that we can measure is hey, what is the mean squared\nerror, right? So if I come down here, and I compare the two mean squared errors, so\nso I can predict x test all right. So these are my predictions using that linear regressor,\nwill linear multiple multiple linear regressor. So these are my live predictions, linear regression.\nOkay. I'm actually going to do that at the bottom. So let me just copy and paste that cell and bring\nit down here. So now I'm going to calculate the mean squared error for both the linear regressor\nand the neural net. Okay, so this is my linear and this is my neural net. So if I do my neural net\nmodel, and I predict x test all, I get my two, you know, different y predictions. And I can calculate\nthe mean squared error, right? So if I want to get the mean squared error, and I have y prediction\nand y real, I can do numpy dot square, and then I would need the y prediction minus, you know, the\nreal. So this this is basically squaring everything. And this should be a vector. So if I just take\nthis entire thing and take the mean of that, that should give me the MSC. So let's just try that out.\nAnd the y real is y test all, right? So that's my mean squared error for the linear regressor.\nAnd this is my mean squared error for the neural net. So that's interesting. I will debug this live,\nI guess. So my guess is that it's probably coming from this normalization layer. Because this input\nshape is probably just six. And okay, so that works now. And the reason why is because, like,\nmy inputs are only for every vector, it's only a one dimensional vector of length six. So I should\nhave I should have just had six, comma, which is a tuple of size six from the start, or it's a it's\na tuple containing one element, which is a six. Okay, so it's actually interesting that my neural\nnet results seem like they they have a larger mean squared error than my linear regressor.\nOne thing that we can look at is, we can actually plot the real versus, you know, the the actual\nresults versus what the predictions are. So if I say, some access, and I use plt dot axes, and make\naxes and make these equal, then I can scatter the the y, you know, the test. So what the actual\nvalues are on the x axis, and then what the prediction are on the x axis. Okay. And I can\nlabel this as the linear regression predictions. Okay, so then let me just label my axes. So the\nx axis, I'm going to say is the true values. The y axis is going to be my linear regression predictions.\nOr actually, let's plot. Let's just make this predictions.\nAnd then at the end, I'm going to plot. Oh, let's set some limits.\nBecause I think that's like approximately the max number of bikes.\nSo I'm going to set my x limit to this and my y limit to this.\nSo here, I'm going to pass that in here too. And all right, this is what we actually get for our\nlinear regressor. You see that actually, they align quite well, I mean, to some extent. So 2000 is\nprobably too much 2500. I mean, looks like maybe like 1800 would be enough here for our limits.\nAnd I'm actually going to label something else, the neural net predictions.\nLet's add a legend. So you can see that our neural net for the larger values, it seems like\nit's a little bit more spread out. And it seems like we tend to underestimate a little bit down\nhere in this area. Okay. And for some reason, these are way off as well.\nBut yeah, so we've basically used a linear regressor and a neural net. Honestly, there are\nsometimes where a neural net is more appropriate and a linear regressor is more appropriate.\nI think that it just comes with time and trying to figure out, you know, and just literally seeing\nlike, hey, what works better, like here, a linear, a multiple linear regressor might actually work\nbetter than a neural net. But for example, with the one dimensional case, a linear regressor would\nnever be able to see this curve. Okay. I mean, I'm not saying this is a great model either, but I'm\njust saying like, hey, you know, sometimes it might be more appropriate to use something that's not\nlinear. So yeah, I will leave regression at that. Okay, so we just talked about supervised learning.\nAnd in supervised learning, we have data, we have some a bunch of features and for a bunch of\ndifferent samples. But each of those samples has some sort of label on it, whether that's a number,\na category, a class, etc. Right, we were able to use that label in order to try to predict\nright, we were able to use that label in order to try to predict new labels of other points that\nwe haven't seen yet. Well, now let's move on to unsupervised learning. So with unsupervised\nlearning, we have a bunch of unlabeled data. And what can we do with that? You know, can we learn\nanything from this data? So the first algorithm that we're going to discuss is known as k means\nclustering. What k means clustering is trying to do is it's trying to compute k clusters from the data.\nSo in this example below, I have a bunch of scattered points. And you'll see that this\nis x zero and x one on the two axes, which means I'm actually plotting two different features,\nright of each point, but we don't know what the y label is for those points. And now, just looking\nat these scattered points, we can kind of see how there are different clusters in the data set,\nright. So depending on what we pick for k, we might have different clusters. Let's say k equals two,\nright, then we might pick, okay, this seems like it could be one cluster, but this here is also\nanother cluster. So those might be our two different clusters. If we have k equals three,\nfor example, then okay, this seems like it could be a cluster. This seems like it could be a\ncluster. And maybe this could be a cluster, right. So we could have three different clusters in the\ndata set. Now, this k here is predefined, if I can spell that correctly, by the person who's running\nthe model. So that would be you. All right. And let's discuss how you know, the computer actually\ngoes through and computes the k clusters. So I'm going to write those steps down here.\nNow, the first step that happens is we actually choose well, the computer chooses three random\npoints on this plot to be the centroids. And by centuries, I just mean the center of the clusters.\nOkay. So three random points, let's say we're doing k equals three, so we're choosing three\nrandom points to be the centroids of the three clusters. If it were two, we'd be choosing two\nrandom points. Okay. So maybe the three random points I'm choosing might be here.\nHere, here, and here. All right. So we have three different points. And the second thing that we do\nis we actually calculate\nthe distance for each point to those centroids. So between all the points and the centroid.\nSo basically, I'm saying, all right, this is this distance, this distance, this distance,\nall of these distances, I'm computing between oops, not those two, between the points, not the\ncentroids themselves. So I'm computing the distances for all of these plots to each of the centroids.\nOkay. And that comes with also assigning those points to the closest centroid.\nWhat do I mean by that? So let's take this point here, for example, so I'm computing\nthis distance, this distance, and this distance. And I'm saying, okay, it seems like the red one\nis the closest. So I'm actually going to put this into the red centroid. So if I do that for\nall of these points, it seems slightly closer to red, and this one seems slightly closer to red,\nright? Now for the blue, I actually wouldn't put any blue ones in here, but we would probably\nactually, that first one is closer to red. And now it seems like the rest of them are probably\ncloser to green. So let's just put all of these into green here, like that. And cool. So now we\nhave, you know, our two, three, technically centroid. So there's this group here, there's\nthis group here. And then blue is kind of just this group here, it hasn't really touched any\nof the points yet. So the next step, three that we do is we actually go and we recalculate the\ncentroid. So we compute new centroids based on the points that we have in all the centroids.\nAnd by that, I just mean, okay, well, let's take the average of all these points. And where is that\nnew centroid? That's probably going to be somewhere around here, right? The blue one, we don't have\nany points in there. So we won't touch and then the screen one, we can put that probably somewhere\nover here, oops, somewhere over here. Right. So now if I erase all of the previously computed centroids,\nI can go and I can actually redo step two over here, this calculation.\nAlright, so I'm going to go back and I'm going to iterate through everything again,\nand I'm going to recompute my three centroids. So let's see, we're going to take this red point,\nthese are definitely all red, right? This one still looks a bit red. Now,\nthis part, we actually start getting closer to the blues.\nSo this one still seems closer to a blue than a green, this one as well. And I think the rest\nwould belong to green. Okay, so now our three centroids are three, sorry, our three clusters\nwould be this, this, and then this, right? Those are our three centroids. And so now we go back\nand we compute the new sorry, those would be the three clusters. So now we go back and we compute\nthe three centroids. So I'm going to get rid of this, this and this. And now where would this\nred be centered, probably closer, you know, to this point here, this blue might be closer to\nup here. And then this green would probably be somewhere. It's pretty similar to what we had\nbefore. But it seems like it'd be pulled down a bit. So probably somewhere around there for green.\nAll right. And now, again, we go back and we compute the distance between all the points\nand the centroids. And then we assign them to the closest centroid. Okay. So the reds are all here,\nit's very clear. Actually, let me just circle that. And this it actually seems like this point is\nit actually seemed like this point is closer to this blue now. So the blues seem like they would\nbe maybe this point looks like it'd be blue. So all these look like they would be blue now.\nAnd the greens would probably be this cluster right here. So we go back, we compute the centroids,\nbam. This one probably like almost here, bam. And then the green looks like it would be probably\nhere ish. Okay. And now we go back and we compute the we compute the clusters again.\nSo red, still this blue, I would argue is now this cluster here. And green is this cluster here.\nOkay, so we go and we recompute the centroids, bam, bam. And, you know, bam. And now if I were\nto go and assign all the points to clusters again, I would get the exact same thing. Right. And so\nthat's when we know that we can stop iterating between steps two and three is when we've\nconverged on some solution when we've reached some stable point. And so now because none of\nthese points are really changing out of their clusters anymore, we can go back to the user\nand say, Hey, these are our three clusters. Okay. And this process, something known as\nexpectation maximization. This part where we're assigning the points to the closest centroid,\nthis is something this is our expectation step. And this part where we're computing the new\ncentroids, this is our maximization step. Okay, so that's expectation maximization.\nAnd we use this in order to compute the centroids, assign all the points to clusters,\naccording to those centroids. And then we're recomputing all that over again, until we reach\nsome stable point where nothing is changing anymore. Alright, so that's our first example\nof unsupervised learning. And basically, what this is doing is trying to find some structure,\nsome pattern in the data. So if I came up with another point, you know, might be somewhere here,\nI can say, Oh, it looks like that's closer to if this is a, b, c, it looks like that's closest to\ncluster B. And so I would probably put it in cluster B. Okay, so we can find some structure\nin the data based on just how, how the points are scattered relative to one another. Now,\nthe second unsupervised learning technique that I'm going to discuss with you guys, something noted,\nprincipal component analysis. And the point of principal component analysis is very often it's\nused as a dimensionality reduction technique. So let me write that down. It's used for dimensionality\nreduction. And what do I mean by dimensionality reduction is if I have a bunch of features like\nx1 x2 x3 x4, etc. Can I just reduce that down to one dimension that gives me the most information\nabout how all these points are spread relative to one another. And that's what PCA is for. So PCA\nprincipal component analysis. Let's say I have some points in the x zero and x one feature space.\nOkay, so these points might be spread, you know, something like this.\nOkay. So for example, if this were something to do with housing prices, right,\nthis here might be x zero might be hey, years since built, right, since the house was built,\nand x one might be square footage of the house. Alright, so like years since built, I mean, like\nright now it's been, you know, 22 years since a house in 2000 was built. Now principal component\nanalysis is just saying, alright, let's say we want to build a model, or let's say we want to,\nyou know, display something about our data, but we don't we don't have two axes to show it on.\nHow do we display, you know, how do we how do we demonstrate that this point is a further away from\nthis point than this point. And we can do that using principal component analysis. So\ntake what you know about linear regression and just forget about it for a second. Otherwise,\nyou might get confused. PCA is a way of trying to find direction in the space with the largest\nvariance. So this principal component, what that means is basically the component.\nSo some direction in this space with the largest variance, okay, it tells us the most about our\ndata set without the two different dimensions. Like, let's say we have these two different\nmentions, and somebody's telling us, hey, you only get one dimension in order to show your data set.\nWhat dimension do you want to show us? Okay, so let's say we want to show our data set,\nwhat dimension like what do we do, we want to project our data onto a single dimension.\nAlright, so that in this case might be a dimension that looks something like\nthis. And you might say, okay, we're not going to talk about linear regression, okay.\nWe don't have a y value. So linear regression, this would be why this is not why, okay, we don't\nhave a label for that. Instead, what we're doing is we're taking the right angle projection. So\nall of these take that's not very visible. But take this right angle projection onto this line.\nAnd what PCA is doing is saying, okay, map all of these points onto this one dimensional space.\nSo the transformed data set would be here.\nThis one's on the data sets are on the line. So we just put that there. But now this would be our\nnew one dimensional data set. Okay, it's not our prediction or anything. This is our new data set.\nIf somebody came to us said you only get one dimension, you only get one number to represent\neach of these 2d points. What number would you give us? What number would you give us?\nSo this would be our new one dimensional data set. Okay, it's not our prediction or anything.\nWhat number would you give me? This would be the number that we gave. Okay, this in this direction,\nthis is where our points are the most spread out. Right? If I took this plot,\nand let me actually duplicate this so I don't have to rewrite anything.\nOr so I don't have to erase and then redraw anything. Let me get rid of some of this stuff.\nAnd I just got rid of a point there too. So let me draw that back.\nAlright, so if this were my original data point, what if I had taken, you know, this to be\nthe PCA dimension? Okay, well, I then would have points that let me actually do that in different\ncolor. So if I were to draw a right angle to this for every point, my points would look something\nlike this. And so just intuitively looking at these two different plots, this top one and this one,\nwe can see that the points are squished a little bit closer together. Right? Which means that the\nvariance that's not the space with the largest variance. The thing about the largest variance\nis that this will give us the most discrimination between all of these points. The larger the\nvariance, the further spread out these points will likely be. Now, and so that's the that's the\ndimension that we should project it on a different way to actually look at that, like what is the\ndimension with the largest variance. It's actually it also happens to be the dimension that decreases\nto be the dimension that decreases that minimizes the residuals. So if we take all the points, and\nwe take the residual from that the XY residual, so in linear regression, in linear regression,\nwe were looking only at this residual, the differences between the predictions right between\ny and y hat, it's not that here in principal component analysis, we're taking the difference\nfrom our current point in two dimensional space, and then it's projected point. Okay, so we're\ntaking that dimension. And we're saying, alright, how much, you know, how much distance is there\nbetween that projection residual, and we're trying to minimize that for all of these points. So that\nactually equates to this largest variance dimension, this dimension here, the PCA dimension,\nyou can either look at it as minimizing, minimize, let me get rid of this,\nthe projection residuals. So that's the stuff in orange.\nOr to maximizing the variance between the points.\nOkay. And we're not really going to talk about, you know, the method that we need in order to\ncalculate out the principal components, or like what that projection would be, because you will\nneed to understand linear algebra for that, especially eigenvectors and eigenvalues, which\nI'm not going to cover in this class. But that's how you would find the principal components. Okay,\nnow, with this two dimensional data set here, sorry, this one dimensional data set, we started\nfrom a 2d data set, and we now boil it down to one dimension. Well, we can go and take that\ndimension, and we can do other things with it. Right, we can, like if there were a y label,\nthen we can now show x versus y, rather than x zero and x one in different plots with that y.\nNow we can just say, oh, this is a principal component. And we're going to plot that with\nthe y. Or for example, if there were 100 different dimensions, and you only wanted to take five of\nthem, well, you could go and you could find the top five PCA dimensions. And that might be a lot\nmore useful to you than 100 different feature vector values. Right. So that's principal component\nanalysis. Again, we're taking, you know, certain data that's unlabeled, and we're trying to make\nsome sort of estimation, like some guess about its structure from that original data set, if we\nwanted to take, you know, a 3d thing, so like a sphere, but we only have a 2d surface to draw it\non. Well, what's the best approximation that we can make? Oh, it's a circle. Right PCA is kind of\nthe same thing. It's saying if we have something with all these different dimensions, but we can't\nshow all of them, how do we boil it down to just one dimension? How do we extract the most\ninformation from that multiple dimensions? And that is exactly either you minimize the projection\nresiduals, or you maximize the variance. And that is PCA. So we'll go through an example of that.\nNow, finally, let's move on to implementing the unsupervised learning part of this class.\nHere, again, I'm on the UCI machine learning repository. And I have a seeds data set where,\nyou know, I have a bunch of kernels that belong to three different types of wheat. So there's\ncomma, Rosa and Canadian. And the different features that we have access to are, you know,\ngeometric parameters of those wheat kernels. So the area perimeter, compactness, length, width,\nwidth, asymmetry, and the length of the kernel groove. Okay, so all of these are real values,\nwhich is easy to work with. And what we're going to do is we're going to try to predict,\nor I guess we're going to try to cluster the different varieties of the wheat.\nSo let's get started. I have a colab notebook open again. Oh, you're gonna have to, you know,\ngo to the data folder, download this. And so I'm going to go to the data folder, download this,\nand let's get started. So the first thing to do is to import our seeds data set into our colab\nnotebook. So I've done that here. Okay, and then we're going to import all the classics again,\nso pandas. And then I'm also going to import seedborn because I'm going to want that for this\nspecific class. Okay. Great. So now our columns that we have in our seed data set are the area,\nthe perimeter, the compactness, the length, with asymmetry, groove, length, I mean, I'm just going\nto call it groove. And then the class, right, the wheat kernels class. So now we have to import this,\nI'm going to do that using pandas read CSV. And it's called seeds data.csv. So I'm going to turn\nthat into a data frame. And the names are equal to the columns over here. So what happens if I just\ndo that? Oops, what did I call this seeds data set text? Alright, so if we actually look at our\ndata frame right now, you'll notice something funky. Okay. And here, you know, we have all the\nstuff under area. And these are all our numbers with some dash t. So the reason is because we\nhaven't actually told pandas what the separator is, which we can do like this. And this t that's\njust a tab. So in order to ensure that like all whitespace gets recognized as a separator,\nwe can actually this is for like a space. So any spaces are going to get recognized as data\nseparators. So if I run that, now our this, you know, this is a lot better. Okay. Okay.\nSo now let's actually go and like visualize this data. So what I'm actually going to do is plot\neach of these against one another. So in this case, pretend that we don't have access to the\nclass, right? Pretend that so this class here, I'm just going to show you in this example,\nthat like, hey, we can predict our classes using unsupervised learning. But for this example,\nin unsupervised learning, we don't actually have access to the class. So I'm going to just try to\nplot these against one another and see what happens. So for some I in range, you know,\nthe columns minus one because the classes in the columns. And I'm just going to say for j in range,\nso take everything from I onwards, you know, so I like the next thing after I until the end of this.\nSo this will give us basically a grid of all the different like combinations. And our x label is\ngoing to be columns I our y label is going to be the columns j. So those are our labels up here.\nAnd I'm going to use seaborne this time. And I'm going to say scatter my data. So our x is going\nto be our x label. Or y is going to be our y label. And our data is going to be the data frame that\nwe're passing in. So what's interesting here is that we can say hue. And what this will do is say,\nlike if I give this class, it's going to separate the three different classes into three different\nhues. So now what we're doing is we're basically comparing the area and the perimeter or the area\nand the compactness. But we're going to visualize, you know, what classes they're in. So let's go\nahead and I might have to show. So great. So basically, we can see perimeter and area we give\nwe get these three groups. The area compactness, we get these three groups, and so on. So these all\nkind of look honestly like somewhat similar. Right, so Wow, look at this one. So this one,\nwe have the compactness and the asymmetry. And it looks like there's not really I mean,\nit just looks like they're blobs, right? Sure, maybe class three is over here more, but\none and two kind of look like they're on top of each other. Okay. I mean, there are some that\nmight look slightly better in terms of clustering. But let's go through some of the some of the\nclustering examples that we talked about, and try to implement those. The first thing that we're\ngoing to do is just straight up clustering. So what we learned about was k means clustering.\nSo from SK learn, I'm going to import k means. Okay. And just for the sake of being able to run,\nyou know, any x and any y, I'm just going to say, hey, let's use some x. What's a good one, maybe.\nI mean, perimeter asymmetry could be a good one. So x could be perimeter, y could be asymmetry.\nOkay. And for this, the x values, I'm going to just extract those specific values.\nAlright, well, let's make a k means algorithm, or let's, you know, define this. So k means,\nand in this specific case, we know that the number of clusters is three. So let's just use that. And\nI'm going to fit this against this x that I've just defined right here. Right. So, you know, if I\ncreate this clusters, so one thing, one cool thing is I can actually go to this clusters, and I can\nsay k mean dot labels. And it'll give give me if I can type correctly, it'll give me what its\npredictions for all the clusters are. And our actual, oops, not that. If we go to the data frame,\nand we get the class, and the values from those, we can actually compare these two and say, hey,\nlike, you know, everything in general, most of the zeros that it's predicted, are the ones, right.\nAnd in general, the twos are the twos here. And then this third class one, okay, that corresponds\nto three. Now remember, these are separate classes. So the labels, what we actually call them don't\nreally matter. We can say a map zero to one map two to two and map one to three. Okay, and our,\nyou know, our mapping would do fairly well. But we can actually visualize this. And in order to do\nthat, I'm going to create this cluster cluster data frame. So I'm going to create a data frame.\nAnd I'm going to pass in a horizontally stacked array with x, so my values for x and y. And then\nthe clusters that I have here, but I'm going to reshape them. So it's 2d.\nOkay. And the columns, the labels for that are going to be x, y, and plus. Okay. So I'm going\nto go ahead and do that same seaborne scatter plot. Again, where x is x, y is y. And now,\nthe hue is again the class. And the data is now this cluster data frame. Alright, so this here,\nthis here is my k means like, I guess classes.\nSo k means kind of looks like this. If I come down here and I plot, you know, my original data frame,\nthis is my original classes with respect to this specific x and y. And you'll see that, honestly,\nlike it doesn't do too poorly. Yeah, there's I mean, the colors are different, but that's fine.\nFor the most part, it gets information of the clusters, right. And now we can do that with\nhigher dimensions. So with the higher dimensions, if we make x equal to, you know, all the columns,\nexcept for the last one, which is our class, we can do the exact same thing.\nWe can do the exact same thing. So here, and we can\npredict this. But now, our columns are equal to our data frame columns all the way to the last one.\nAnd then with this class, actually, so we can literally just say data frame columns.\nAnd we can fit all of this. And now, if I want to plot the k means classes.\nAlright, so this was my that's my clustered and my original. So actually, let me see if I can\nget these on the same page. So yeah, I mean, pretty similar to what we just saw. But what's\nactually really cool is even something like, you know, if we change. So what's one of them\nwhere they were like on top of each other? Okay, so compactness and asymmetry, this one's messy.\nRight. So if I come down here, and I say compactness and asymmetry, and I'm trying to do this in 2d,\nthis is what my scatterplot. So this is what you know, my k means is telling me for these two\ndimensions for compactness and asymmetry, if we just look at those two, these are our three classes,\nright? And we know that the original looks something like this. And are these two remotely\nalike? No. Okay, so now if I come back down here, and I rerun this higher dimensions one,\nbut actually, this clusters, I need to get the labels of the k means again.\nOkay, so if I rerun this with higher dimensions,\nwell, if we zoom out, and we take a look at these two, sure, the colors are mixed up. But in general,\nthere are the three groups are there, right? This does a much better job at assessing, okay,\nwhat group is what. So, for example, we could relabel the one in the original class to two.\nAnd then we could make sorry, okay, this is kind of confusing. But for example, if this light pink\nwere projected onto this darker pink here, and then this dark one was actually the light pink,\nand this light one was this dark one, then you kind of see like these correspond to one another,\nright? Like even these two up here are the same class as all the other ones over here, which are\nthe same in the same color. So you don't want to compare the two colors between the plots,\nyou want to compare which points are in what colors in each of the plots. So that's one cool\napplication. So this is how k means functions, it's basically taking all the data sets and saying,\nAll right, where are my clusters given these pieces of data? And then the next thing that we\ntalked about is PCA. So PCA, we're reducing the dimension, but we're mapping all these like,\nyou know, seven dimensions. I don't know if there are seven, I made that number up, but we're\nmapping multiple dimensions into a lower dimension number. Right. And so let's see how that works.\nSo from SK learn decomposition, I can import PCA and that will be my PCA model.\nSo if I do PCA component, so this is how many dimensions you want to map it into.\nAnd you know, for this exercise, let's do two. Okay, so now I'm taking the top two dimensions.\nAnd my transformed x is going to be PCA dot fit transform, and the same x that I had up here.\nAnd the same x that I had up here. Okay, so all the other all the values basically, area,\nperimeter, compactness, length, width, asymmetry, groove. Okay. So let's run that. And we've\ntransformed it. So let's look at what the shape of x used to be. So they're okay. So seven was right,\nI had 210 samples, each seven, seven features long, basically. And now my transformed x\nis 210 samples, but only of length two, which means that I only have two dimensions now that\nI'm plotting. And we can actually even take a look at, you know, the first five things.\nOkay, so now we see each each one is a two dimensional point,\neach sample is now a two dimensional point in our new in our new dimensions.\nSo what's cool is I can actually scatter these\nzero and transformed x. So I actually have to\ntake the columns here. And if I show that,\nbasically, we've just taken this like seven dimensional thing, and we've made it into a\nsingle or I guess to a two dimensional representation. So that's a point of PCA.\nAnd actually, let's go ahead and do the same clustering exercise as we did up here. If I take\nthe k means this PCA data frame, I can let's construct data frame out of that. And the data\nframe is going to be H stack. I'm going to take this transformed x and the clusters that reshape.\nSo actually, instead of clusters, I'm going to use k means dot labels. And I need to reshape this.\nSo it's 2d. So we can do the H stack. And for the columns, I'm going to set this to PCA one PCA two,\nand the class. All right. So now if I take this, I can also do the same for the truth.\nBut instead of the k means labels, I want from the data frame the original classes.\nAnd I'm just going to take the values from that. And so now I have a data frame for the k means\nwith PCA and then a data frame for the truth with also the PCA. And I can now plot these similarly\nto how I plotted these up here. So let me actually take these two.\nInstead of the cluster data frame, I want the this is the k means PCA data frame. This is still going\nto be class, but now x and y are going to be the two PCA dimensions. Okay. So these are my two PCA\ndimensions. And you can see that the data frame is going to be the same as the cluster data frame.\nSo these are my two PCA dimensions. And you can see that, you know, they're, they're pretty spread\nout. And then here, I'm going to go to my truth classes. Again, it's PCA one PCA two, but instead\nof k means this should be truth PCA data frame. So you can see that like in the truth data frame\nalong these two dimensions, we actually are doing fairly well in terms of separation, right? It does\nseem like this is slightly more separable than the other like dimensions that we had been looking at\nup here. So that's a good sign. And up here, you can see that hey, some of these correspond to one\nanother. I mean, for the most part, our algorithm or unsupervised clustering algorithm is able to\nto give us is able to spit out, you know, what the proper labels are. I mean, if you map these\nspecific labels to the different types of kernels. But for example, this one might all be the comma\nkernel kernels and same here. And then these might all be the Canadian kernels. And these might all\nbe the Canadian kernels. So it does struggle a little bit with, you know, where they overlap.\nBut for the most part, our algorithm is able to find the three different categories, and do a\nfairly good job at predicting them without without any information from us, we haven't given our\nalgorithm any labels. So that's a gist of unsupervised learning. I hope you guys enjoyed\nthis course. I hope you know, a lot of these examples made sense. If there are certain things\nthat I have done, and you know, you're somebody with more experience than me, please let me know\nin the comments and we can all as a community learn from this together. So thank you all for watching."
  },
  {
    "id": 258990548,
    "timestamp": "2026-02-05T01:42:47.926Z",
    "title": "Complete Machine Learning In 6 Hours| Krish Naik",
    "url": "https://www.youtube.com/watch?v=JxgmHe2NyeY&t=190s",
    "text": "so today's session what all things we\nso today's session what all things we are basically going to discuss so first\nare basically going to discuss so first\nare basically going to discuss so first of all we going to discuss about\nof all we going to discuss about\nof all we going to discuss about different types of machine learning\ndifferent types of machine learning\ndifferent types of machine learning algorithm like how many different types\nalgorithm like how many different types\nalgorithm like how many different types of machine learning\nof machine learning\nof machine learning algor understand the purpose of taking\nalgor understand the purpose of taking\nalgor understand the purpose of taking this session is to clear the interviews\nthis session is to clear the interviews\nthis session is to clear the interviews okay clear the interviews once you go\nokay clear the interviews once you go\nokay clear the interviews once you go for a data science interviews and all\nfor a data science interviews and all\nfor a data science interviews and all the main purpose is to clear the\nthe main purpose is to clear the\nthe main purpose is to clear the interviews I've seen people who knew\ninterviews I've seen people who knew\ninterviews I've seen people who knew machine learning algorithms in a proper\nmachine learning algorithms in a proper\nmachine learning algorithms in a proper way okay they were definitely able to\nway okay they were definitely able to\nway okay they were definitely able to clear it because they just explain the\nclear it because they just explain the\nclear it because they just explain the algorithms in a better way to the\nalgorithms in a better way to the\nalgorithms in a better way to the recruiter so that they got hired first\nrecruiter so that they got hired first\nrecruiter so that they got hired first of all is the introduction to machine\nof all is the introduction to machine\nof all is the introduction to machine learning here I'm just specifically\nlearning here I'm just specifically\nlearning here I'm just specifically going to talk about AI versus ml versus\ngoing to talk about AI versus ml versus\ngoing to talk about AI versus ml versus DL versus data sign then the second\nDL versus data sign then the second\nDL versus data sign then the second thing that we are going to talk about\nthing that we are going to talk about\nthing that we are going to talk about over here is the difference between\nover here is the difference between\nover here is the difference between supervised MS\nsupervised MS\nsupervised MS and unsupervised ml the third thing that\nand unsupervised ml the third thing that\nand unsupervised ml the third thing that we are probably going to discuss about\nwe are probably going to discuss about\nwe are probably going to discuss about is something called as linear regression\nis something called as linear regression\nis something called as linear regression so we are going to clearly understand\nso we are going to clearly understand\nso we are going to clearly understand the maths and geometric intuition the\nthe maths and geometric intuition the\nthe maths and geometric intuition the next thing that we are probably going to\nnext thing that we are probably going to\nnext thing that we are probably going to discuss about is R square and adjusted R\ndiscuss about is R square and adjusted R\ndiscuss about is R square and adjusted R square the fifth topic that we are going\nsquare the fifth topic that we are going\nsquare the fifth topic that we are going to discuss about is Ridge and lasso\nto discuss about is Ridge and lasso\nto discuss about is Ridge and lasso regression the first topic that we are\nregression the first topic that we are\nregression the first topic that we are going to discuss about is AI versus ml\ngoing to discuss about is AI versus ml\ngoing to discuss about is AI versus ml versus DL versus data science so this is\nversus DL versus data science so this is\nversus DL versus data science so this is the first topic that we are probably\nthe first topic that we are probably\nthe first topic that we are probably going to discuss if you really want to\ngoing to discuss if you really want to\ngoing to discuss if you really want to understand the difference between AI\nunderstand the difference between AI\nunderstand the difference between AI versus ml versus DL versus data science\nversus ml versus DL versus data science\nversus ml versus DL versus data science we will go in this specific format so\nwe will go in this specific format so\nwe will go in this specific format so just imagine the entire universe so this\njust imagine the entire universe so this\njust imagine the entire universe so this entire universe I will probably call it\nentire universe I will probably call it\nentire universe I will probably call it as an AI now specifically when I say AI\nas an AI now specifically when I say AI\nas an AI now specifically when I say AI this basically means AI artificial\nthis basically means AI artificial\nthis basically means AI artificial intelligence whatever role you are in\nintelligence whatever role you are in\nintelligence whatever role you are in you are as a machine learning developer\nyou are as a machine learning developer\nyou are as a machine learning developer you working as a deep learning developer\nyou working as a deep learning developer\nyou working as a deep learning developer Vision developer or a data scientist or\nVision developer or a data scientist or\nVision developer or a data scientist or an AI engineer at the end of the day you\nan AI engineer at the end of the day you\nan AI engineer at the end of the day you are actually creating AI application so\nare actually creating AI application so\nare actually creating AI application so if I really want to Define what is this\nif I really want to Define what is this\nif I really want to Define what is this artificial intelligence you can just say\nartificial intelligence you can just say\nartificial intelligence you can just say that it is a process wherein we create\nthat it is a process wherein we create\nthat it is a process wherein we create some kind of applications in which it\nsome kind of applications in which it\nsome kind of applications in which it will be able to do its task without any\nwill be able to do its task without any\nwill be able to do its task without any human intervention so that basically\nhuman intervention so that basically\nhuman intervention so that basically means a person need not monitor this AI\nmeans a person need not monitor this AI\nmeans a person need not monitor this AI application automatically it'll be able\napplication automatically it'll be able\napplication automatically it'll be able to make decisions it will be able to\nto make decisions it will be able to\nto make decisions it will be able to perform its task and it will be able to\nperform its task and it will be able to\nperform its task and it will be able to do many things so this is what an AI\ndo many things so this is what an AI\ndo many things so this is what an AI application is some of the examples that\napplication is some of the examples that\napplication is some of the examples that I would definitely like to consider so\nI would definitely like to consider so\nI would definitely like to consider so the first example that I would like to\nthe first example that I would like to\nthe first example that I would like to consider AI application AI module\nconsider AI application AI module\nconsider AI application AI module Netflix has an AI module suppose if you\nNetflix has an AI module suppose if you\nNetflix has an AI module suppose if you see a kind of action movie for some time\nsee a kind of action movie for some time\nsee a kind of action movie for some time then the kind of AI work or AI work that\nthen the kind of AI work or AI work that\nthen the kind of AI work or AI work that is basically implemented over here is\nis basically implemented over here is\nis basically implemented over here is something called as recommendation\nsomething called as recommendation\nsomething called as recommendation so here through this application what\nso here through this application what\nso here through this application what happens is that when you're continuously\nhappens is that when you're continuously\nhappens is that when you're continuously seeing the action movies then\nseeing the action movies then\nseeing the action movies then automatically the AI module that is\nautomatically the AI module that is\nautomatically the AI module that is present inside Netflix will make sure\npresent inside Netflix will make sure\npresent inside Netflix will make sure that it gives us recommendation on\nthat it gives us recommendation on\nthat it gives us recommendation on action movies second if I take an\naction movies second if I take an\naction movies second if I take an example of comedy movie If I\nexample of comedy movie If I\nexample of comedy movie If I continuously see comedy movie then also\ncontinuously see comedy movie then also\ncontinuously see comedy movie then also it'll give us the recommendation of the\nit'll give us the recommendation of the\nit'll give us the recommendation of the comedy movie so this through this what\ncomedy movie so this through this what\ncomedy movie so this through this what happens is that it understands your\nhappens is that it understands your\nhappens is that it understands your behavior and it is being able to do its\nbehavior and it is being able to do its\nbehavior and it is being able to do its task without asking you anything the\ntask without asking you anything the\ntask without asking you anything the second example that I would like to take\nsecond example that I would like to take\nsecond example that I would like to take up in is\nup in is\nup in is amazon.in now amazon.in again if you buy\namazon.in now amazon.in again if you buy\namazon.in now amazon.in again if you buy an\nan\nan iPhone then it may recommend you a\niPhone then it may recommend you a\niPhone then it may recommend you a headphones so this kind of\nheadphones so this kind of\nheadphones so this kind of recommendation is also a part of AI\nrecommendation is also a part of AI\nrecommendation is also a part of AI module that is integrated with the\nmodule that is integrated with the\nmodule that is integrated with the amazon.in website the ads that you see\namazon.in website the ads that you see\namazon.in website the ads that you see probably when you opening my channel\nprobably when you opening my channel\nprobably when you opening my channel through which I get paid a little bit\nthrough which I get paid a little bit\nthrough which I get paid a little bit from my from a from the hard work that I\nfrom my from a from the hard work that I\nfrom my from a from the hard work that I do in YouTube right so through that ads\ndo in YouTube right so through that ads\ndo in YouTube right so through that ads how that is recommended to you uh that\nhow that is recommended to you uh that\nhow that is recommended to you uh that is also an AI engine that is included in\nis also an AI engine that is included in\nis also an AI engine that is included in the YouTube channel itself which really\nthe YouTube channel itself which really\nthe YouTube channel itself which really plays it is a business-driven goal\nplays it is a business-driven goal\nplays it is a business-driven goal understand it is a business driven\nunderstand it is a business driven\nunderstand it is a business driven things that we basically do with the\nthings that we basically do with the\nthings that we basically do with the help of AI one more example that I would\nhelp of AI one more example that I would\nhelp of AI one more example that I would like to give you is if I consider it\nlike to give you is if I consider it\nlike to give you is if I consider it self-driving cars so here you'll be able\nself-driving cars so here you'll be able\nself-driving cars so here you'll be able to see self-driving cars if you take an\nto see self-driving cars if you take an\nto see self-driving cars if you take an example of Tesla so self-driving cars\nexample of Tesla so self-driving cars\nexample of Tesla so self-driving cars what happens based on the road it is\nwhat happens based on the road it is\nwhat happens based on the road it is able ble to drive it automatically who\nable ble to drive it automatically who\nable ble to drive it automatically who is doing that there is an AI application\nis doing that there is an AI application\nis doing that there is an AI application integrated with the car itself right so\nintegrated with the car itself right so\nintegrated with the car itself right so if I consider all these things these all\nif I consider all these things these all\nif I consider all these things these all are AI application at the end of the day\nare AI application at the end of the day\nare AI application at the end of the day whatever role you do you are going to\nwhatever role you do you are going to\nwhatever role you do you are going to create an AI application this is the\ncreate an AI application this is the\ncreate an AI application this is the common mistake what people do you know\ncommon mistake what people do you know\ncommon mistake what people do you know like our CEO sudhansu Kumar he has\nlike our CEO sudhansu Kumar he has\nlike our CEO sudhansu Kumar he has written in his profile that he's an AI\nwritten in his profile that he's an AI\nwritten in his profile that he's an AI engineer that basically means his goal\nengineer that basically means his goal\nengineer that basically means his goal is to create an AI application so\nis to create an AI application so\nis to create an AI application so probably in a product based companies\nprobably in a product based companies\nprobably in a product based companies you'll be seeing this kind of roles\nyou'll be seeing this kind of roles\nyou'll be seeing this kind of roles called as AI engineer now let's go to\ncalled as AI engineer now let's go to\ncalled as AI engineer now let's go to the next role which is called as machine\nthe next role which is called as machine\nthe next role which is called as machine learning so where does machine learning\nlearning so where does machine learning\nlearning so where does machine learning comes into existence so if I try to\ncomes into existence so if I try to\ncomes into existence so if I try to create this machine learning is a subset\ncreate this machine learning is a subset\ncreate this machine learning is a subset of AI and what is the role of machine\nof AI and what is the role of machine\nof AI and what is the role of machine learning it provides stats\nlearning it provides stats\nlearning it provides stats tools\ntools\ntools to analyze the data visualize the data\nto analyze the data visualize the data\nto analyze the data visualize the data and apart from that to do\nand apart from that to do\nand apart from that to do predictions I'm\npredictions I'm\npredictions I'm forecasting so you will be seeing a lot\nforecasting so you will be seeing a lot\nforecasting so you will be seeing a lot of machine learning algorithms so\nof machine learning algorithms so\nof machine learning algorithms so internally those machine learning\ninternally those machine learning\ninternally those machine learning algorithm the equation that we are\nalgorithm the equation that we are\nalgorithm the equation that we are basically using it is basically using it\nbasically using it is basically using it\nbasically using it is basically using it is having a kind of stats tool stat\nis having a kind of stats tool stat\nis having a kind of stats tool stat techniques because whenever we work with\ntechniques because whenever we work with\ntechniques because whenever we work with data statistics is definitely very much\ndata statistics is definitely very much\ndata statistics is definitely very much important so this exactly is called as\nimportant so this exactly is called as\nimportant so this exactly is called as machine learning so it is a subset of AI\nmachine learning so it is a subset of AI\nmachine learning so it is a subset of AI this is very much important to\nthis is very much important to\nthis is very much important to understand ml is a subset of AI so here\nunderstand ml is a subset of AI so here\nunderstand ml is a subset of AI so here you can see that it is a part of this\nyou can see that it is a part of this\nyou can see that it is a part of this now let's go to the next one which is\nnow let's go to the next one which is\nnow let's go to the next one which is called called as deep learning deep\ncalled called as deep learning deep\ncalled called as deep learning deep learning is again a subset of ml now\nlearning is again a subset of ml now\nlearning is again a subset of ml now let's consider why deep learning came\nlet's consider why deep learning came\nlet's consider why deep learning came into existence because in 1950s 60s\ninto existence because in 1950s 60s\ninto existence because in 1950s 60s scientists thought that can we make\nscientists thought that can we make\nscientists thought that can we make machine learn like how we human being\nmachine learn like how we human being\nmachine learn like how we human being learn so for that particular purpose\nlearn so for that particular purpose\nlearn so for that particular purpose deep learning came into existence here\ndeep learning came into existence here\ndeep learning came into existence here the plan is to basically mimic human\nthe plan is to basically mimic human\nthe plan is to basically mimic human brain so when I say mimicking human\nbrain so when I say mimicking human\nbrain so when I say mimicking human brain that basically means we are trying\nbrain that basically means we are trying\nbrain that basically means we are trying to mimic the human brain to implement\nto mimic the human brain to implement\nto mimic the human brain to implement something to learn something so for this\nsomething to learn something so for this\nsomething to learn something so for this you use something called as\nyou use something called as\nyou use something called as multi-layered neural networks so this is\nmulti-layered neural networks so this is\nmulti-layered neural networks so this is what deep learning is it is a subset of\nwhat deep learning is it is a subset of\nwhat deep learning is it is a subset of machine learning its main aim is to\nmachine learning its main aim is to\nmachine learning its main aim is to mimic human brain so they actually\nmimic human brain so they actually\nmimic human brain so they actually create multi-layer neural network and\ncreate multi-layer neural network and\ncreate multi-layer neural network and this multi-layered neural network will\nthis multi-layered neural network will\nthis multi-layered neural network will basically help you to train the machines\nbasically help you to train the machines\nbasically help you to train the machines or applications whatever we are trying\nor applications whatever we are trying\nor applications whatever we are trying to create and deep learning has really\nto create and deep learning has really\nto create and deep learning has really really done an amazing work with the\nreally done an amazing work with the\nreally done an amazing work with the help of deep learning we are able to\nhelp of deep learning we are able to\nhelp of deep learning we are able to solve such a complex complex complex use\nsolve such a complex complex complex use\nsolve such a complex complex complex use cases that we will be probably\ncases that we will be probably\ncases that we will be probably discussing as we go ahead now if I come\ndiscussing as we go ahead now if I come\ndiscussing as we go ahead now if I come to data science see this is the thing\nto data science see this is the thing\nto data science see this is the thing guys if you want to say yourself as a\nguys if you want to say yourself as a\nguys if you want to say yourself as a data scientist tomorrow you given a\ndata scientist tomorrow you given a\ndata scientist tomorrow you given a business use case and situation comes\nbusiness use case and situation comes\nbusiness use case and situation comes that you probably have to solve that use\nthat you probably have to solve that use\nthat you probably have to solve that use case with the help of machine learning\ncase with the help of machine learning\ncase with the help of machine learning algorithms or deep learning algorithms\nalgorithms or deep learning algorithms\nalgorithms or deep learning algorithms again the final goal is to create an AI\nagain the final goal is to create an AI\nagain the final goal is to create an AI application right you cannot say that I\napplication right you cannot say that I\napplication right you cannot say that I am a data scientist and I'll just work\nam a data scientist and I'll just work\nam a data scientist and I'll just work in machine learning I or I'll work in\nin machine learning I or I'll work in\nin machine learning I or I'll work in deep learning or I may I don't know how\ndeep learning or I may I don't know how\ndeep learning or I may I don't know how to analyze the data no you cannot do\nto analyze the data no you cannot do\nto analyze the data no you cannot do that when I was working in Panasonic I\nthat when I was working in Panasonic I\nthat when I was working in Panasonic I got various different kind of task\ngot various different kind of task\ngot various different kind of task sometime I was told to use W powerbi to\nsometime I was told to use W powerbi to\nsometime I was told to use W powerbi to visualize analyze the data sometime I\nvisualize analyze the data sometime I\nvisualize analyze the data sometime I was given a machine learning project\nwas given a machine learning project\nwas given a machine learning project sometime I was given a deep learning\nsometime I was given a deep learning\nsometime I was given a deep learning project so as a data scientist if I\nproject so as a data scientist if I\nproject so as a data scientist if I consider where does data scientist fall\nconsider where does data scientist fall\nconsider where does data scientist fall into this it will be a part of\ninto this it will be a part of\ninto this it will be a part of everything so if I talk about machine\neverything so if I talk about machine\neverything so if I talk about machine learning and deep learning with respect\nlearning and deep learning with respect\nlearning and deep learning with respect to any kind of problem statement that we\nto any kind of problem statement that we\nto any kind of problem statement that we solve the majority of the business use\nsolve the majority of the business use\nsolve the majority of the business use cases will be falling in two sections\ncases will be falling in two sections\ncases will be falling in two sections one is supervised machine learning one\none is supervised machine learning one\none is supervised machine learning one is unsupervised machine learning so most\nis unsupervised machine learning so most\nis unsupervised machine learning so most of the problems that you are basically\nof the problems that you are basically\nof the problems that you are basically solving this is with respect to this two\nsolving this is with respect to this two\nsolving this is with respect to this two problem statement two different types of\nproblem statement two different types of\nproblem statement two different types of machine learning algorithms that is\nmachine learning algorithms that is\nmachine learning algorithms that is supervised machine learning and deep\nsupervised machine learning and deep\nsupervised machine learning and deep learning if I talk about supervised\nlearning if I talk about supervised\nlearning if I talk about supervised machine learning two major problem\nmachine learning two major problem\nmachine learning two major problem statements that you are basically\nstatements that you are basically\nstatements that you are basically solving here also one is regression\nsolving here also one is regression\nsolving here also one is regression problem\nproblem\nproblem and the other one is something called as\nand the other one is something called as\nand the other one is something called as classification problem and in the case\nclassification problem and in the case\nclassification problem and in the case of unsupervised machine learning problem\nof unsupervised machine learning problem\nof unsupervised machine learning problem statement you are basically solving two\nstatement you are basically solving two\nstatement you are basically solving two different types of problem one is\ndifferent types of problem one is\ndifferent types of problem one is clustering and one is dimensionality\nclustering and one is dimensionality\nclustering and one is dimensionality reduction and there is also one more\nreduction and there is also one more\nreduction and there is also one more type which is called as reinforcement\ntype which is called as reinforcement\ntype which is called as reinforcement learning reinforcement learning I can I\nlearning reinforcement learning I can I\nlearning reinforcement learning I can I I will definitely talk about this not\nI will definitely talk about this not\nI will definitely talk about this not right now right now we are just focusing\nright now right now we are just focusing\nright now right now we are just focusing on all these things now understand what\non all these things now understand what\non all these things now understand what happens in supervised machine learning\nhappens in supervised machine learning\nhappens in supervised machine learning let's consider consider a data set so\nlet's consider consider a data set so\nlet's consider consider a data set so here I have a data set which says this\nhere I have a data set which says this\nhere I have a data set which says this is my age and this is my weight suppose\nis my age and this is my weight suppose\nis my age and this is my weight suppose I have these two specific features let's\nI have these two specific features let's\nI have these two specific features let's say that I have values like 24 62 25 63\nsay that I have values like 24 62 25 63\nsay that I have values like 24 62 25 63 21 72\n21 72\n21 72 257 uh 62 and many more data over here\n257 uh 62 and many more data over here\n257 uh 62 and many more data over here let's say that my task is to basically\nlet's say that my task is to basically\nlet's say that my task is to basically take this particular data and create a\ntake this particular data and create a\ntake this particular data and create a model wherein so suppose my task is that\nmodel wherein so suppose my task is that\nmodel wherein so suppose my task is that I need to create a model whenever it\nI need to create a model whenever it\nI need to create a model whenever it takes the New Age first of all we train\ntakes the New Age first of all we train\ntakes the New Age first of all we train this model with this data and whenever\nthis model with this data and whenever\nthis model with this data and whenever we take age a new age it should be able\nwe take age a new age it should be able\nwe take age a new age it should be able to give us the output of weight this\nto give us the output of weight this\nto give us the output of weight this particular model is also called as\nparticular model is also called as\nparticular model is also called as hypothesis okay I'll discuss about this\nhypothesis okay I'll discuss about this\nhypothesis okay I'll discuss about this today when I we discussing about linear\ntoday when I we discussing about linear\ntoday when I we discussing about linear regression now what are the important\nregression now what are the important\nregression now what are the important components whenever we have this kind of\ncomponents whenever we have this kind of\ncomponents whenever we have this kind of problem statement first of all you need\nproblem statement first of all you need\nproblem statement first of all you need to understand there are two important\nto understand there are two important\nto understand there are two important things one is independent features and\nthings one is independent features and\nthings one is independent features and the other one is something called as\nthe other one is something called as\nthe other one is something called as dependent features now let's go ahead\ndependent features now let's go ahead\ndependent features now let's go ahead and discuss what is independent feature\nand discuss what is independent feature\nand discuss what is independent feature independent feature basically means in\nindependent feature basically means in\nindependent feature basically means in this particular case since the input\nthis particular case since the input\nthis particular case since the input that I'm basically training in all those\nthat I'm basically training in all those\nthat I'm basically training in all those features becomes an independent feature\nfeatures becomes an independent feature\nfeatures becomes an independent feature now in this particular case my age is\nnow in this particular case my age is\nnow in this particular case my age is independent feature and whatever I'm\nindependent feature and whatever I'm\nindependent feature and whatever I'm actually predicting so when I say\nactually predicting so when I say\nactually predicting so when I say predicting I know this is my output okay\npredicting I know this is my output okay\npredicting I know this is my output okay this is the what I have to basically\nthis is the what I have to basically\nthis is the what I have to basically make my model uh give this as a an\nmake my model uh give this as a an\nmake my model uh give this as a an output so in this particular casee my\noutput so in this particular casee my\noutput so in this particular casee my dependent feature becomes weight why we\ndependent feature becomes weight why we\ndependent feature becomes weight why we specifically say a dependent feature\nspecifically say a dependent feature\nspecifically say a dependent feature because this is completely dependent on\nbecause this is completely dependent on\nbecause this is completely dependent on this value whenever this is increasing\nthis value whenever this is increasing\nthis value whenever this is increasing or decreasing this value is basically\nor decreasing this value is basically\nor decreasing this value is basically getting changed so that is the reason\ngetting changed so that is the reason\ngetting changed so that is the reason why we basically say this has\nwhy we basically say this has\nwhy we basically say this has independent and dependent feature\nindependent and dependent feature\nindependent and dependent feature whenever we are solving a problem right\nwhenever we are solving a problem right\nwhenever we are solving a problem right in the case of supervised machine\nin the case of supervised machine\nin the case of supervised machine learning remember they will be one\nlearning remember they will be one\nlearning remember they will be one dependent feature and there can be any\ndependent feature and there can be any\ndependent feature and there can be any number of independent features now let's\nnumber of independent features now let's\nnumber of independent features now let's go ahead and let's discuss about\ngo ahead and let's discuss about\ngo ahead and let's discuss about regression and classification what is\nregression and classification what is\nregression and classification what is the difference between them now let\nthe difference between them now let\nthe difference between them now let let's go ahead and let's discuss about\nlet's go ahead and let's discuss about\nlet's go ahead and let's discuss about two things one\ntwo things one\ntwo things one is let's say I want a regression problem\nis let's say I want a regression problem\nis let's say I want a regression problem statement suppose I take the same\nstatement suppose I take the same\nstatement suppose I take the same example as age and weight so I have\nexample as age and weight so I have\nexample as age and weight so I have values like as discussed 24 72 23\nvalues like as discussed 24 72 23\nvalues like as discussed 24 72 23 71 uh 24 or 25\n71 uh 24 or 25\n71 uh 24 or 25 71.5 okay so this kind of data I have\n71.5 okay so this kind of data I have\n71.5 okay so this kind of data I have see this is my output variable which is\nsee this is my output variable which is\nsee this is my output variable which is my dependent feature now in this\nmy dependent feature now in this\nmy dependent feature now in this particular dependent feature now\nparticular dependent feature now\nparticular dependent feature now whenever I'm trying to find out the\nwhenever I'm trying to find out the\nwhenever I'm trying to find out the output and in this particular output you\noutput and in this particular output you\noutput and in this particular output you have a continuous variable when you have\nhave a continuous variable when you have\nhave a continuous variable when you have a continuous variable then this becomes\na continuous variable then this becomes\na continuous variable then this becomes a regression problem statement now one\na regression problem statement now one\na regression problem statement now one example I would like to give suppose\nexample I would like to give suppose\nexample I would like to give suppose this is my data set right this is my age\nthis is my data set right this is my age\nthis is my data set right this is my age this is my weight suppose I am\nthis is my weight suppose I am\nthis is my weight suppose I am populating this particular data set with\npopulating this particular data set with\npopulating this particular data set with the help of scatter plot then in order\nthe help of scatter plot then in order\nthe help of scatter plot then in order to basically solve this problem what\nto basically solve this problem what\nto basically solve this problem what we'll do suppose if I take an example of\nwe'll do suppose if I take an example of\nwe'll do suppose if I take an example of linear regression I will try to draw a\nlinear regression I will try to draw a\nlinear regression I will try to draw a straight line and this particular line\nstraight line and this particular line\nstraight line and this particular line is my equation which is called as yal mx\nis my equation which is called as yal mx\nis my equation which is called as yal mx + C and with the help of this particular\n+ C and with the help of this particular\n+ C and with the help of this particular equation I will try to find out the\nequation I will try to find out the\nequation I will try to find out the predicted points so this will be my\npredicted points so this will be my\npredicted points so this will be my predicted point this will be my\npredicted point this will be my\npredicted point this will be my predicted point this this any new points\npredicted point this this any new points\npredicted point this this any new points that I see over here will basically be\nthat I see over here will basically be\nthat I see over here will basically be my predicted point with respect to Y so\nmy predicted point with respect to Y so\nmy predicted point with respect to Y so in this way we basically solve a\nin this way we basically solve a\nin this way we basically solve a regression problem statement so this is\nregression problem statement so this is\nregression problem statement so this is very much important to understand let's\nvery much important to understand let's\nvery much important to understand let's go to the always understand in a\ngo to the always understand in a\ngo to the always understand in a regression problem statement your output\nregression problem statement your output\nregression problem statement your output will be a continuous variable the second\nwill be a continuous variable the second\nwill be a continuous variable the second one is basically a classification\none is basically a classification\none is basically a classification problem now in classification problem\nproblem now in classification problem\nproblem now in classification problem suppose I have a data set let's say that\nsuppose I have a data set let's say that\nsuppose I have a data set let's say that number of hours study number of study\nnumber of hours study number of study\nnumber of hours study number of study hours number of play\nhours number of play\nhours number of play hours so this is my independent feature\nhours so this is my independent feature\nhours so this is my independent feature let's say a number of sleeping hours and\nlet's say a number of sleeping hours and\nlet's say a number of sleeping hours and finally I have my output which will will\nfinally I have my output which will will\nfinally I have my output which will will be pass or fail so in this I have all\nbe pass or fail so in this I have all\nbe pass or fail so in this I have all this as my independent features and this\nthis as my independent features and this\nthis as my independent features and this is my dependent feature so I will be\nis my dependent feature so I will be\nis my dependent feature so I will be having some values like this and here\nhaving some values like this and here\nhaving some values like this and here either you'll be pass or fail or pass or\neither you'll be pass or fail or pass or\neither you'll be pass or fail or pass or fail now whenever you have in your\nfail now whenever you have in your\nfail now whenever you have in your output fixed number of categories then\noutput fixed number of categories then\noutput fixed number of categories then that becomes a classification problem\nthat becomes a classification problem\nthat becomes a classification problem suppose it just has two outputs then it\nsuppose it just has two outputs then it\nsuppose it just has two outputs then it becomes a binary classification if you\nbecomes a binary classification if you\nbecomes a binary classification if you have more than two different categories\nhave more than two different categories\nhave more than two different categories at that time it becomes a multiclass\nat that time it becomes a multiclass\nat that time it becomes a multiclass classification so this is the difference\nclassification so this is the difference\nclassification so this is the difference between regression problem statement and\nbetween regression problem statement and\nbetween regression problem statement and the classification problem statement now\nthe classification problem statement now\nthe classification problem statement now let's go ahead and let's discuss about\nlet's go ahead and let's discuss about\nlet's go ahead and let's discuss about something called as unsupervised machine\nsomething called as unsupervised machine\nsomething called as unsupervised machine learning now in unsupervised machine\nlearning now in unsupervised machine\nlearning now in unsupervised machine learning which is my second main topic\nlearning which is my second main topic\nlearning which is my second main topic over here I'm just going to write\nover here I'm just going to write\nover here I'm just going to write unsupervised machine learning now what\nunsupervised machine learning now what\nunsupervised machine learning now what exactly is unsupervised machine learning\nexactly is unsupervised machine learning\nexactly is unsupervised machine learning here whenever I talk about there are two\nhere whenever I talk about there are two\nhere whenever I talk about there are two main problem statement that we solve one\nmain problem statement that we solve one\nmain problem statement that we solve one is clustering\nis clustering\nis clustering one is dimensionality reduction let's\none is dimensionality reduction let's\none is dimensionality reduction let's take one example of a specific data set\ntake one example of a specific data set\ntake one example of a specific data set over here let's say that my data set is\nover here let's say that my data set is\nover here let's say that my data set is something called as salary and age now\nsomething called as salary and age now\nsomething called as salary and age now in this scenario we don't have any\nin this scenario we don't have any\nin this scenario we don't have any output variable no output variable no\noutput variable no output variable no\noutput variable no output variable no dependent variable then what kind of\ndependent variable then what kind of\ndependent variable then what kind of assumptions that we can take out from\nassumptions that we can take out from\nassumptions that we can take out from this particular data set suppose I have\nthis particular data set suppose I have\nthis particular data set suppose I have salary and age as my values so in this\nsalary and age as my values so in this\nsalary and age as my values so in this particular case I would like to do\nparticular case I would like to do\nparticular case I would like to do something called as clustering now why\nsomething called as clustering now why\nsomething called as clustering now why clustering is used just understand let's\nclustering is used just understand let's\nclustering is used just understand let's say I am going to do something called as\nsay I am going to do something called as\nsay I am going to do something called as customer segmentation now what does this\ncustomer segmentation now what does this\ncustomer segmentation now what does this customer segmentation do clustering\ncustomer segmentation do clustering\ncustomer segmentation do clustering basically means that based on this data\nbasically means that based on this data\nbasically means that based on this data I will try to find out similar groups\nI will try to find out similar groups\nI will try to find out similar groups groups of people suppose this is my one\ngroups of people suppose this is my one\ngroups of people suppose this is my one group this is my another group this is\ngroup this is my another group this is\ngroup this is my another group this is my third group let's say that I was able\nmy third group let's say that I was able\nmy third group let's say that I was able to create this many groups this many\nto create this many groups this many\nto create this many groups this many groups are clusters I'll say cluster 1 2\ngroups are clusters I'll say cluster 1 2\ngroups are clusters I'll say cluster 1 2 three each and every cluster will be\nthree each and every cluster will be\nthree each and every cluster will be specifying some information this cluster\nspecifying some information this cluster\nspecifying some information this cluster May specify that this person uh he was\nMay specify that this person uh he was\nMay specify that this person uh he was very young but he was able to get some\nvery young but he was able to get some\nvery young but he was able to get some amazing salary this person it may some\namazing salary this person it may some\namazing salary this person it may some specify that these people are basically\nspecify that these people are basically\nspecify that these people are basically having more age and they are getting\nhaving more age and they are getting\nhaving more age and they are getting good salary these people are like middle\ngood salary these people are like middle\ngood salary these people are like middle class background where with respect to\nclass background where with respect to\nclass background where with respect to the age the salary is not that much\nthe age the salary is not that much\nthe age the salary is not that much increasing so here what we are doing we\nincreasing so here what we are doing we\nincreasing so here what we are doing we are doing clustering we are grouping\nare doing clustering we are grouping\nare doing clustering we are grouping them together main thing is grouping\nthem together main thing is grouping\nthem together main thing is grouping this word is very much important now why\nthis word is very much important now why\nthis word is very much important now why do we use this suppose my company\ndo we use this suppose my company\ndo we use this suppose my company launches is a product and I want to just\nlaunches is a product and I want to just\nlaunches is a product and I want to just Target this particular product to rich\nTarget this particular product to rich\nTarget this particular product to rich people let's say product one is for rich\npeople let's say product one is for rich\npeople let's say product one is for rich people product two is for middle class\npeople product two is for middle class\npeople product two is for middle class people so if I make this kind of\npeople so if I make this kind of\npeople so if I make this kind of clusters I will be able to Target my ads\nclusters I will be able to Target my ads\nclusters I will be able to Target my ads only to this kind of people let's say\nonly to this kind of people let's say\nonly to this kind of people let's say that this is the rich people this is the\nthat this is the rich people this is the\nthat this is the rich people this is the middle class people I will be able to\nmiddle class people I will be able to\nmiddle class people I will be able to Target this particular ads or this\nTarget this particular ads or this\nTarget this particular ads or this particular product or send this\nparticular product or send this\nparticular product or send this particular things to those specific\nparticular things to those specific\nparticular things to those specific group of people by that that is\ngroup of people by that that is\ngroup of people by that that is basically called as ad marketing and\nbasically called as ad marketing and\nbasically called as ad marketing and this uses something called as customer\nthis uses something called as customer\nthis uses something called as customer segmentation a very important example\nsegmentation a very important example\nsegmentation a very important example and based on this customer segmentation\nand based on this customer segmentation\nand based on this customer segmentation we can later apply any regression or\nwe can later apply any regression or\nwe can later apply any regression or classification kind of problem statement\nclassification kind of problem statement\nclassification kind of problem statement now coming to the second one after\nnow coming to the second one after\nnow coming to the second one after clustering which is called as\nclustering which is called as\nclustering which is called as dimensionality reduction now in\ndimensionality reduction now in\ndimensionality reduction now in dimensionality reduction what we are\ndimensionality reduction what we are\ndimensionality reduction what we are focusing on suppose if we have th000\nfocusing on suppose if we have th000\nfocusing on suppose if we have th000 features can we reduce this features to\nfeatures can we reduce this features to\nfeatures can we reduce this features to lower Dimensions let's say that I want\nlower Dimensions let's say that I want\nlower Dimensions let's say that I want to convert this\nto convert this\nto convert this uh th000 feature to 100 features lower\nuh th000 feature to 100 features lower\nuh th000 feature to 100 features lower Dimension so can we do that yes it is\nDimension so can we do that yes it is\nDimension so can we do that yes it is possible with the help of dimensionality\npossible with the help of dimensionality\npossible with the help of dimensionality deduction algorithm there are some\ndeduction algorithm there are some\ndeduction algorithm there are some algorithms like PCA so I'll also try to\nalgorithms like PCA so I'll also try to\nalgorithms like PCA so I'll also try to cover this as we go ahead understand\ncover this as we go ahead understand\ncover this as we go ahead understand clustering is not a classification\nclustering is not a classification\nclustering is not a classification problem clustering is a grouping\nproblem clustering is a grouping\nproblem clustering is a grouping algorithm there is no output feature no\nalgorithm there is no output feature no\nalgorithm there is no output feature no dependent variable in clustering sorry\ndependent variable in clustering sorry\ndependent variable in clustering sorry in unsupervised ml so yes I will also\nin unsupervised ml so yes I will also\nin unsupervised ml so yes I will also try to cover up LDA we'll cover up PCA\ntry to cover up LDA we'll cover up PCA\ntry to cover up LDA we'll cover up PCA and all as we go ahead so with respect\nand all as we go ahead so with respect\nand all as we go ahead so with respect to supervised and unsupervised so first\nto supervised and unsupervised so first\nto supervised and unsupervised so first thing that we are going to cover is\nthing that we are going to cover is\nthing that we are going to cover is something called as linear regression\nsomething called as linear regression\nsomething called as linear regression the second algorithm that we will try to\nthe second algorithm that we will try to\nthe second algorithm that we will try to cover after linear regression is\ncover after linear regression is\ncover after linear regression is something called as Ridge and lasso\nsomething called as Ridge and lasso\nsomething called as Ridge and lasso third that we are going to cover is\nthird that we are going to cover is\nthird that we are going to cover is something called as logistic regression\nsomething called as logistic regression\nsomething called as logistic regression the fourth that we are basically going\nthe fourth that we are basically going\nthe fourth that we are basically going to cover is something called as decision\nto cover is something called as decision\nto cover is something called as decision tree decision tree includes both\ntree decision tree includes both\ntree decision tree includes both classification and regression four fifth\nclassification and regression four fifth\nclassification and regression four fifth that we are going to cover is something\nthat we are going to cover is something\nthat we are going to cover is something called as adab boost sixth that we are\ncalled as adab boost sixth that we are\ncalled as adab boost sixth that we are going to cover is something called as\ngoing to cover is something called as\ngoing to cover is something called as random Forest seventh that we are going\nrandom Forest seventh that we are going\nrandom Forest seventh that we are going to cover is something called as gradient\nto cover is something called as gradient\nto cover is something called as gradient boosting eighth that we are going to\nboosting eighth that we are going to\nboosting eighth that we are going to cover is something called as XG boost N9\ncover is something called as XG boost N9\ncover is something called as XG boost N9 that we are going to cover is something\nthat we are going to cover is something\nthat we are going to cover is something called as n bias then when we go to the\ncalled as n bias then when we go to the\ncalled as n bias then when we go to the unsupervised machine learning algorithm\nunsupervised machine learning algorithm\nunsupervised machine learning algorithm the first algorithm that we are going to\nthe first algorithm that we are going to\nthe first algorithm that we are going to do is something called as K means K\ndo is something called as K means K\ndo is something called as K means K means algorithm then we also have DV\nmeans algorithm then we also have DV\nmeans algorithm then we also have DV scan then we are also going to do higher\nscan then we are also going to do higher\nscan then we are also going to do higher C clustering there is also something\nC clustering there is also something\nC clustering there is also something called as K nearest neighbor clustering\ncalled as K nearest neighbor clustering\ncalled as K nearest neighbor clustering fifth we'll try to see about PCA then\nfifth we'll try to see about PCA then\nfifth we'll try to see about PCA then LDA so different different things we\nLDA so different different things we\nLDA so different different things we will try to cover up yes svm I have\nwill try to cover up yes svm I have\nwill try to cover up yes svm I have missed here I'm going to include svm KNN\nmissed here I'm going to include svm KNN\nmissed here I'm going to include svm KNN will also get covered so I have that in\nwill also get covered so I have that in\nwill also get covered so I have that in my list probably I may miss one or two\nmy list probably I may miss one or two\nmy list probably I may miss one or two but we are going to cover everything so\nbut we are going to cover everything so\nbut we are going to cover everything so let's start our first algorithm linear\nlet's start our first algorithm linear\nlet's start our first algorithm linear regression so let's go ahead and discuss\nregression so let's go ahead and discuss\nregression so let's go ahead and discuss about linear regression linear\nabout linear regression linear\nabout linear regression linear regression problem statement is very\nregression problem statement is very\nregression problem statement is very simple guys so suppose I have let's say\nsimple guys so suppose I have let's say\nsimple guys so suppose I have let's say I have two features one is my X feature\nI have two features one is my X feature\nI have two features one is my X feature and one is my y feature let's say that X\nand one is my y feature let's say that X\nand one is my y feature let's say that X is nothing but age and Y is nothing but\nis nothing but age and Y is nothing but\nis nothing but age and Y is nothing but weight so based on these two features I\nweight so based on these two features I\nweight so based on these two features I have some data points that has been\nhave some data points that has been\nhave some data points that has been present over here so in linear\npresent over here so in linear\npresent over here so in linear regression what we try to do is that we\nregression what we try to do is that we\nregression what we try to do is that we try to create a model with the help of\ntry to create a model with the help of\ntry to create a model with the help of this training data set so this will be\nthis training data set so this will be\nthis training data set so this will be my training data set what I'm actually\nmy training data set what I'm actually\nmy training data set what I'm actually going to do is that I'm going to\ngoing to do is that I'm going to\ngoing to do is that I'm going to basically train a model and this model\nbasically train a model and this model\nbasically train a model and this model is nothing but a kind of hypothesis\nis nothing but a kind of hypothesis\nis nothing but a kind of hypothesis testing or it is just kind of hypothesis\ntesting or it is just kind of hypothesis\ntesting or it is just kind of hypothesis which takes the new age and gives the\nwhich takes the new age and gives the\nwhich takes the new age and gives the output of the weights and then with the\noutput of the weights and then with the\noutput of the weights and then with the help of performance metrics we try to\nhelp of performance metrics we try to\nhelp of performance metrics we try to verify whether this model is performing\nverify whether this model is performing\nverify whether this model is performing well or not now in short what we are\nwell or not now in short what we are\nwell or not now in short what we are going to do in linear regression is that\ngoing to do in linear regression is that\ngoing to do in linear regression is that we'll try to find out a best fit line\nwe'll try to find out a best fit line\nwe'll try to find out a best fit line which will actually help us to do the\nwhich will actually help us to do the\nwhich will actually help us to do the prediction that basically means if I get\nprediction that basically means if I get\nprediction that basically means if I get my new age over here then what should be\nmy new age over here then what should be\nmy new age over here then what should be my output with respect to Y okay so with\nmy output with respect to Y okay so with\nmy output with respect to Y okay so with respect to this what should be my output\nrespect to this what should be my output\nrespect to this what should be my output over here in this particular case\nover here in this particular case\nover here in this particular case whenever we are drawing a diagram like\nwhenever we are drawing a diagram like\nwhenever we are drawing a diagram like this I can basically say that Y is a\nthis I can basically say that Y is a\nthis I can basically say that Y is a linear function of X so this is what we\nlinear function of X so this is what we\nlinear function of X so this is what we are going to do now understand how we\nare going to do now understand how we\nare going to do now understand how we are going to create this best fit line\nare going to create this best fit line\nare going to create this best fit line this is very much important whenever we\nthis is very much important whenever we\nthis is very much important whenever we say linear regression it basically means\nsay linear regression it basically means\nsay linear regression it basically means that we are going to create a linear\nthat we are going to create a linear\nthat we are going to create a linear line over there you may be thinking sir\nline over there you may be thinking sir\nline over there you may be thinking sir why to create linear line why not\nwhy to create linear line why not\nwhy to create linear line why not nonlinear line that I'll discuss about\nnonlinear line that I'll discuss about\nnonlinear line that I'll discuss about it as we go ahead see other other\nit as we go ahead see other other\nit as we go ahead see other other algorithms so to begin with let's\nalgorithms so to begin with let's\nalgorithms so to begin with let's consider this line that you see over\nconsider this line that you see over\nconsider this line that you see over here right this line equation can be\nhere right this line equation can be\nhere right this line equation can be given by multiple equations someone some\ngiven by multiple equations someone some\ngiven by multiple equations someone some people people write yal mx + C some\npeople people write yal mx + C some\npeople people write yal mx + C some people write uh H some people write yal\npeople write uh H some people write yal\npeople write uh H some people write yal beta 0 + beta 1 into X some people write\nbeta 0 + beta 1 into X some people write\nbeta 0 + beta 1 into X some people write H Theta of xal to Theta 0 + Theta 1 into\nH Theta of xal to Theta 0 + Theta 1 into\nH Theta of xal to Theta 0 + Theta 1 into X many many equations are there for this\nX many many equations are there for this\nX many many equations are there for this this straight line this straight line\nthis straight line this straight line\nthis straight line this straight line many many equations are there with\nmany many equations are there with\nmany many equations are there with respect to many many different kind of\nrespect to many many different kind of\nrespect to many many different kind of notations but the first algorithm that I\nnotations but the first algorithm that I\nnotations but the first algorithm that I have probably learned of linear\nhave probably learned of linear\nhave probably learned of linear regression is from Andrew Ng definitely\nregression is from Andrew Ng definitely\nregression is from Andrew Ng definitely I would like to give him the entire\nI would like to give him the entire\nI would like to give him the entire credits and based on his notation\ncredits and based on his notation\ncredits and based on his notation whatever he has explained I'll try to\nwhatever he has explained I'll try to\nwhatever he has explained I'll try to explain you over here so the credits for\nexplain you over here so the credits for\nexplain you over here so the credits for this algorithm specifically goes to\nthis algorithm specifically goes to\nthis algorithm specifically goes to Andrew NG so let's consider this one\nAndrew NG so let's consider this one\nAndrew NG so let's consider this one over here in order to create this\nover here in order to create this\nover here in order to create this straight line I will basically use a\nstraight line I will basically use a\nstraight line I will basically use a equation which is called as H Theta so\nequation which is called as H Theta so\nequation which is called as H Theta so this is the equation of a straight line\nthis is the equation of a straight line\nthis is the equation of a straight line if I know the equation of the straight\nif I know the equation of the straight\nif I know the equation of the straight line whatever I can write I can write\nline whatever I can write I can write\nline whatever I can write I can write many things yal mx + C yal beta 0 + beta\nmany things yal mx + C yal beta 0 + beta\nmany things yal mx + C yal beta 0 + beta 1 * X and then I can also write one more\n1 * X and then I can also write one more\n1 * X and then I can also write one more that is H Theta of xal theta 0 + Theta 1\nthat is H Theta of xal theta 0 + Theta 1\nthat is H Theta of xal theta 0 + Theta 1 into X of I here also you can basically\ninto X of I here also you can basically\ninto X of I here also you can basically say x of I here also you can say x of I\nsay x of I here also you can say x of I\nsay x of I here also you can say x of I now let's go ahead and let's take this\nnow let's go ahead and let's take this\nnow let's go ahead and let's take this equation for now let's take this\nequation for now let's take this\nequation for now let's take this equation of now so I'm I'm going to take\nequation of now so I'm I'm going to take\nequation of now so I'm I'm going to take out this equation and just write one\nout this equation and just write one\nout this equation and just write one equation through which I have also\nequation through which I have also\nequation through which I have also studied but I will definitely be adding\nstudied but I will definitely be adding\nstudied but I will definitely be adding some points which probably Andrew and\nsome points which probably Andrew and\nsome points which probably Andrew and could not mention mention in his video\ncould not mention mention in his video\ncould not mention mention in his video but I'll try my level best obviously he\nbut I'll try my level best obviously he\nbut I'll try my level best obviously he is the best I cannot even compare myself\nis the best I cannot even compare myself\nis the best I cannot even compare myself to him so Theta 0 + Theta 1 into X now\nto him so Theta 0 + Theta 1 into X now\nto him so Theta 0 + Theta 1 into X now let's understand what is Theta 0 Theta 1\nlet's understand what is Theta 0 Theta 1\nlet's understand what is Theta 0 Theta 1 as I said that let's say I have a\nas I said that let's say I have a\nas I said that let's say I have a problem statement over here let's say I\nproblem statement over here let's say I\nproblem statement over here let's say I this is my X and this is my y this is my\nthis is my X and this is my y this is my\nthis is my X and this is my y this is my data points now what I'm doing I'm\ndata points now what I'm doing I'm\ndata points now what I'm doing I'm trying to create a best fit line like\ntrying to create a best fit line like\ntrying to create a best fit line like this now what is this best fit line what\nthis now what is this best fit line what\nthis now what is this best fit line what is uh when I say this best fit line is\nis uh when I say this best fit line is\nis uh when I say this best fit line is basically given by this equation what\nbasically given by this equation what\nbasically given by this equation what does Theta 0 basically indicate Theta 0\ndoes Theta 0 basically indicate Theta 0\ndoes Theta 0 basically indicate Theta 0 over here is something called as\nover here is something called as\nover here is something called as intercept now what exactly is intercept\nintercept now what exactly is intercept\nintercept now what exactly is intercept intercept basically means that when your\nintercept basically means that when your\nintercept basically means that when your X is zero then H Theta of X is equal to\nX is zero then H Theta of X is equal to\nX is zero then H Theta of X is equal to Theta 0 so in this particular case\nTheta 0 so in this particular case\nTheta 0 so in this particular case intercept basically indicates that at\nintercept basically indicates that at\nintercept basically indicates that at what point you are meeting the Y AIS so\nwhat point you are meeting the Y AIS so\nwhat point you are meeting the Y AIS so this particular point is basically\nthis particular point is basically\nthis particular point is basically your intercept when your X is equal to 0\nyour intercept when your X is equal to 0\nyour intercept when your X is equal to 0 at that point of time you'll be seeing\nat that point of time you'll be seeing\nat that point of time you'll be seeing that this line is intersecting the y-\nthat this line is intersecting the y-\nthat this line is intersecting the y- AIS whatever value this will be that is\nAIS whatever value this will be that is\nAIS whatever value this will be that is your intercept now the second thing is\nyour intercept now the second thing is\nyour intercept now the second thing is about your Theta 1 what is Theta 1 this\nabout your Theta 1 what is Theta 1 this\nabout your Theta 1 what is Theta 1 this is nothing but slope or coefficient now\nis nothing but slope or coefficient now\nis nothing but slope or coefficient now what does this basically indicate this\nwhat does this basically indicate this\nwhat does this basically indicate this indicates let let's say that this is the\nindicates let let's say that this is the\nindicates let let's say that this is the unit one unit in the x-axis and probably\nunit one unit in the x-axis and probably\nunit one unit in the x-axis and probably with respect to this I can find one\nwith respect to this I can find one\nwith respect to this I can find one point over here one point over here and\npoint over here one point over here and\npoint over here one point over here and if I try to draw this over here to here\nif I try to draw this over here to here\nif I try to draw this over here to here this is the unit movement in y so what\nthis is the unit movement in y so what\nthis is the unit movement in y so what does it basically say slope with the\ndoes it basically say slope with the\ndoes it basically say slope with the unit movement in one one unit movement\nunit movement in one one unit movement\nunit movement in one one unit movement towards the x-axis what is the unit\ntowards the x-axis what is the unit\ntowards the x-axis what is the unit movement in y- axis that is basically\nmovement in y- axis that is basically\nmovement in y- axis that is basically slope or coefficient Theta 0 and Theta 1\nslope or coefficient Theta 0 and Theta 1\nslope or coefficient Theta 0 and Theta 1 two things and X of I is definitely your\ntwo things and X of I is definitely your\ntwo things and X of I is definitely your data points now our main aim is to\ndata points now our main aim is to\ndata points now our main aim is to create a best fit line in such a way\ncreate a best fit line in such a way\ncreate a best fit line in such a way that I I'll just try to show it to you\nthat I I'll just try to show it to you\nthat I I'll just try to show it to you what is our main aim let's let's\nwhat is our main aim let's let's\nwhat is our main aim let's let's understand what is the aim of a linear\nunderstand what is the aim of a linear\nunderstand what is the aim of a linear regression so if I take an example of\nregression so if I take an example of\nregression so if I take an example of linear regression I need to find out the\nlinear regression I need to find out the\nlinear regression I need to find out the best fit line in such a way that the\nbest fit line in such a way that the\nbest fit line in such a way that the distance\ndistance\ndistance between this data points that I have and\nbetween this data points that I have and\nbetween this data points that I have and the predicted points should be very very\nthe predicted points should be very very\nthe predicted points should be very very less suppose I'm creating a best fit\nless suppose I'm creating a best fit\nless suppose I'm creating a best fit line okay I'm creating a best fit line\nline okay I'm creating a best fit line\nline okay I'm creating a best fit line so with respect to this data points\nso with respect to this data points\nso with respect to this data points initially was this right but my\ninitially was this right but my\ninitially was this right but my predicted point is this point in this\npredicted point is this point in this\npredicted point is this point in this particular case my predicted point is\nparticular case my predicted point is\nparticular case my predicted point is this point so and if I do do the\nthis point so and if I do do the\nthis point so and if I do do the summation of all these points those\nsummation of all these points those\nsummation of all these points those distance should be minimal then only\ndistance should be minimal then only\ndistance should be minimal then only I'll be able to say that this is the\nI'll be able to say that this is the\nI'll be able to say that this is the best fit line so I I cannot definitely\nbest fit line so I I cannot definitely\nbest fit line so I I cannot definitely say that this is exactly the best fit\nsay that this is exactly the best fit\nsay that this is exactly the best fit line or not how will I say when I try to\nline or not how will I say when I try to\nline or not how will I say when I try to calculate the difference between this\ncalculate the difference between this\ncalculate the difference between this point and the predicted Point these are\npoint and the predicted Point these are\npoint and the predicted Point these are my predicted point right if I try to\nmy predicted point right if I try to\nmy predicted point right if I try to calculate the distance between them then\ncalculate the distance between them then\ncalculate the distance between them then I will basically have a aim to it should\nI will basically have a aim to it should\nI will basically have a aim to it should be minimal if I do the summation of all\nbe minimal if I do the summation of all\nbe minimal if I do the summation of all the distance it should be minimal\nthe distance it should be minimal\nthe distance it should be minimal so for that what I can do is that see\nso for that what I can do is that see\nso for that what I can do is that see you may be also thinking Krish why not\nyou may be also thinking Krish why not\nyou may be also thinking Krish why not just do one thing okay suppose if these\njust do one thing okay suppose if these\njust do one thing okay suppose if these are my data points why not just play and\nare my data points why not just play and\nare my data points why not just play and create multiple lines and try to compare\ncreate multiple lines and try to compare\ncreate multiple lines and try to compare what we can do is that we can compare\nwhat we can do is that we can compare\nwhat we can do is that we can compare multiple we can create multiple lines\nmultiple we can create multiple lines\nmultiple we can create multiple lines right like this and then whoever is\nright like this and then whoever is\nright like this and then whoever is giving the best minimal point I will go\ngiving the best minimal point I will go\ngiving the best minimal point I will go and select that but how many iteration\nand select that but how many iteration\nand select that but how many iteration you will do how you will come to know\nyou will do how you will come to know\nyou will do how you will come to know that okay this line is the best line so\nthat okay this line is the best line so\nthat okay this line is the best line so for that specific purpose we should\nfor that specific purpose we should\nfor that specific purpose we should start at one point and we should lead\nstart at one point and we should lead\nstart at one point and we should lead towards finding the best fit line start\ntowards finding the best fit line start\ntowards finding the best fit line start at one point and then we should go\nat one point and then we should go\nat one point and then we should go towards finding the best fit line so for\ntowards finding the best fit line so for\ntowards finding the best fit line so for this particular purpose what we do is\nthis particular purpose what we do is\nthis particular purpose what we do is that we create a something called as uh\nthat we create a something called as uh\nthat we create a something called as uh cost function I have already shown you\ncost function I have already shown you\ncost function I have already shown you what is my hypothesis function my best\nwhat is my hypothesis function my best\nwhat is my hypothesis function my best fit line equation is basically given as\nfit line equation is basically given as\nfit line equation is basically given as H Theta of x equal to Theta 0 + Theta 1\nH Theta of x equal to Theta 0 + Theta 1\nH Theta of x equal to Theta 0 + Theta 1 * X this is my hypothesis right now\n* X this is my hypothesis right now\n* X this is my hypothesis right now coming to the cost function which is\ncoming to the cost function which is\ncoming to the cost function which is super super important why this it is\nsuper super important why this it is\nsuper super important why this it is super important because cost function\nsuper important because cost function\nsuper important because cost function basically what what is cost function\nbasically what what is cost function\nbasically what what is cost function over here I told right right this\nover here I told right right this\nover here I told right right this distance when I do the\ndistance when I do the\ndistance when I do the summation this distance that I when I'm\nsummation this distance that I when I'm\nsummation this distance that I when I'm doing the summation it should be minimal\ndoing the summation it should be minimal\ndoing the summation it should be minimal so if I really want to find out this\nso if I really want to find out this\nso if I really want to find out this particular distance I will be using one\nparticular distance I will be using one\nparticular distance I will be using one more equation how can I use a distance\nmore equation how can I use a distance\nmore equation how can I use a distance formula between the predicted and the\nformula between the predicted and the\nformula between the predicted and the real point I will just say that H Theta\nreal point I will just say that H Theta\nreal point I will just say that H Theta of x - y so when I say h Theta of x - Y\nof x - y so when I say h Theta of x - Y\nof x - y so when I say h Theta of x - Y what does this basically mean this is my\nwhat does this basically mean this is my\nwhat does this basically mean this is my real point and this is my predicted\nreal point and this is my predicted\nreal point and this is my predicted Point predicted point is basically given\nPoint predicted point is basically given\nPoint predicted point is basically given by H Theta of X and what I'm going to do\nby H Theta of X and what I'm going to do\nby H Theta of X and what I'm going to do I'm going to basically do the squaring\nI'm going to basically do the squaring\nI'm going to basically do the squaring because I may get a negative value so\nbecause I may get a negative value so\nbecause I may get a negative value so because of that I really want to do the\nbecause of that I really want to do the\nbecause of that I really want to do the squaring part Now understand one thing I\nsquaring part Now understand one thing I\nsquaring part Now understand one thing I need to also do the\nneed to also do the\nneed to also do the summation I = 1 to compl complete M\nsummation I = 1 to compl complete M\nsummation I = 1 to compl complete M let's say that I'm taking the number of\nlet's say that I'm taking the number of\nlet's say that I'm taking the number of data points over here as M because I\ndata points over here as M because I\ndata points over here as M because I need to calculate the distance between\nneed to calculate the distance between\nneed to calculate the distance between all the points right with respect to the\nall the points right with respect to the\nall the points right with respect to the predicted and the predict with respect\npredicted and the predict with respect\npredicted and the predict with respect to the real\nto the real\nto the real points so after this I also need to\npoints so after this I also need to\npoints so after this I also need to divide by 1X 2m the reason why I'm\ndivide by 1X 2m the reason why I'm\ndivide by 1X 2m the reason why I'm dividing by first of all let me show you\ndividing by first of all let me show you\ndividing by first of all let me show you why we are dividing by 1 by m 1 by m\nwhy we are dividing by 1 by m 1 by m\nwhy we are dividing by 1 by m 1 by m will give us the average of all the\nwill give us the average of all the\nwill give us the average of all the values that we have the specific reason\nvalues that we have the specific reason\nvalues that we have the specific reason why we are dividing by 1 by 2 do is for\nwhy we are dividing by 1 by 2 do is for\nwhy we are dividing by 1 by 2 do is for the derivation purpose it helps us to\nthe derivation purpose it helps us to\nthe derivation purpose it helps us to make our equation very much simpler so\nmake our equation very much simpler so\nmake our equation very much simpler so that later on when I am updating the\nthat later on when I am updating the\nthat later on when I am updating the weights when I say weights I'm basically\nweights when I say weights I'm basically\nweights when I say weights I'm basically updating Theta 0 and Theta 1 Theta 0 and\nupdating Theta 0 and Theta 1 Theta 0 and\nupdating Theta 0 and Theta 1 Theta 0 and Theta 1 at that point of time you'll be\nTheta 1 at that point of time you'll be\nTheta 1 at that point of time you'll be able to see that this particular value\nable to see that this particular value\nable to see that this particular value when we probably do the derivative it\nwhen we probably do the derivative it\nwhen we probably do the derivative it will help us to do it again I'm going to\nwill help us to do it again I'm going to\nwill help us to do it again I'm going to repeat it I'm going to write it down for\nrepeat it I'm going to write it down for\nrepeat it I'm going to write it down for you first of\nyou first of\nyou first of all now in order to find find out the\nall now in order to find find out the\nall now in order to find find out the best fit line I need to keep on changing\nbest fit line I need to keep on changing\nbest fit line I need to keep on changing Theta 0 and Theta 1 unless and until I\nTheta 0 and Theta 1 unless and until I\nTheta 0 and Theta 1 unless and until I get the best fit line unless and until I\nget the best fit line unless and until I\nget the best fit line unless and until I don't get the best fit line I need to\ndon't get the best fit line I need to\ndon't get the best fit line I need to keep on updating Theta 0 and Theta 1 now\nkeep on updating Theta 0 and Theta 1 now\nkeep on updating Theta 0 and Theta 1 now if I need to keep on updating Theta 0\nif I need to keep on updating Theta 0\nif I need to keep on updating Theta 0 and Theta 1 I probably require a cost\nand Theta 1 I probably require a cost\nand Theta 1 I probably require a cost function okay what this cost function\nfunction okay what this cost function\nfunction okay what this cost function will do I'll just tell you so cost\nwill do I'll just tell you so cost\nwill do I'll just tell you so cost function over here I will specify as J\nfunction over here I will specify as J\nfunction over here I will specify as J of theta 0 comma Theta 1 is equal to now\nof theta 0 comma Theta 1 is equal to now\nof theta 0 comma Theta 1 is equal to now what is cost fun function over here what\nwhat is cost fun function over here what\nwhat is cost fun function over here what this distance I told right this distance\nthis distance I told right this distance\nthis distance I told right this distance between the H Theta of X and Y if I do\nbetween the H Theta of X and Y if I do\nbetween the H Theta of X and Y if I do the summation of all these things it\nthe summation of all these things it\nthe summation of all these things it needs to be minimal it needs to be less\nneeds to be minimal it needs to be less\nneeds to be minimal it needs to be less because with respect to an X point this\nbecause with respect to an X point this\nbecause with respect to an X point this is my y point\nis my y point\nis my y point right similarly with respect to this x\nright similarly with respect to this x\nright similarly with respect to this x point this is my y point so what I'm\npoint this is my y point so what I'm\npoint this is my y point so what I'm actually going to do I'm going to use a\nactually going to do I'm going to use a\nactually going to do I'm going to use a cost function now in this cost function\ncost function now in this cost function\ncost function now in this cost function my main aim is\nmy main aim is\nmy main aim is to basically write H Theta of x - y s\nto basically write H Theta of x - y s\nto basically write H Theta of x - y s this will be with respect to I I I why I\nthis will be with respect to I I I why I\nthis will be with respect to I I I why I am saying I because this will be moving\nam saying I because this will be moving\nam saying I because this will be moving from I equal to 1 to all the points that\nfrom I equal to 1 to all the points that\nfrom I equal to 1 to all the points that is m m is basically all the points over\nis m m is basically all the points over\nis m m is basically all the points over here now apart from this what I actually\nhere now apart from this what I actually\nhere now apart from this what I actually going to do I'm going to divide by 1X 2\ngoing to do I'm going to divide by 1X 2\ngoing to do I'm going to divide by 1X 2 m I'll tell you why I'm specifically\nm I'll tell you why I'm specifically\nm I'll tell you why I'm specifically dividing by 1X 2 m first of all by\ndividing by 1X 2 m first of all by\ndividing by 1X 2 m first of all by dividing by m I will be getting an\ndividing by m I will be getting an\ndividing by m I will be getting an average\naverage\naverage output average cost function because\noutput average cost function because\noutput average cost function because here I'm iterating M the reason why I'm\nhere I'm iterating M the reason why I'm\nhere I'm iterating M the reason why I'm dividing by two because it will help us\ndividing by two because it will help us\ndividing by two because it will help us in derivation why let's say that I have\nin derivation why let's say that I have\nin derivation why let's say that I have xÂ² if I try to find out derivative of xÂ²\nxÂ² if I try to find out derivative of xÂ²\nxÂ² if I try to find out derivative of xÂ² with respect to X then what will I get I\nwith respect to X then what will I get I\nwith respect to X then what will I get I will basically get 2x right that is what\nwill basically get 2x right that is what\nwill basically get 2x right that is what is the formula what is the derivation of\nis the formula what is the derivation of\nis the formula what is the derivation of X of n it is nothing but n x of n\nX of n it is nothing but n x of n\nX of n it is nothing but n x of n minus1 so that is the reason why I'm\nminus1 so that is the reason why I'm\nminus1 so that is the reason why I'm actually making it 1 by two so that when\nactually making it 1 by two so that when\nactually making it 1 by two so that when two comes over here this two and two\ntwo comes over here this two and two\ntwo comes over here this two and two will get cancelled so I hope everybody's\nwill get cancelled so I hope everybody's\nwill get cancelled so I hope everybody's able to understand so this is my cost\nable to understand so this is my cost\nable to understand so this is my cost function Now understand what is this\nfunction Now understand what is this\nfunction Now understand what is this called as this entire equation is\ncalled as this entire equation is\ncalled as this entire equation is basically called as squared error\nbasically called as squared error\nbasically called as squared error function yes mathematical Simplicity\nfunction yes mathematical Simplicity\nfunction yes mathematical Simplicity basically means because when we are\nbasically means because when we are\nbasically means because when we are updating Theta 0 and Theta 1 we\nupdating Theta 0 and Theta 1 we\nupdating Theta 0 and Theta 1 we basically find out derivation in the\nbasically find out derivation in the\nbasically find out derivation in the cost function so that is the reason why\ncost function so that is the reason why\ncost function so that is the reason why we are specifically doing it squaring\nwe are specifically doing it squaring\nwe are specifically doing it squaring off is basically done because so that we\noff is basically done because so that we\noff is basically done because so that we don't get any negative values here\ndon't get any negative values here\ndon't get any negative values here squared error function now let's go\nsquared error function now let's go\nsquared error function now let's go towards the what we need to solve this\ntowards the what we need to solve this\ntowards the what we need to solve this is my cost function okay so I need to\nis my cost function okay so I need to\nis my cost function okay so I need to minimize minimize this particular value\nminimize minimize this particular value\nminimize minimize this particular value that is 1x 2 m summation of I = 1 2 m\nthat is 1x 2 m summation of I = 1 2 m\nthat is 1x 2 m summation of I = 1 2 m and then this will basically be H Theta\nand then this will basically be H Theta\nand then this will basically be H Theta of X of I minus y of I whole Square we\nof X of I minus y of I whole Square we\nof X of I minus y of I whole Square we need to minimize this by adjusting\nneed to minimize this by adjusting\nneed to minimize this by adjusting parameter Theta 0 and Theta 1\nparameter Theta 0 and Theta 1\nparameter Theta 0 and Theta 1 this entirely is what this is nothing\nthis entirely is what this is nothing\nthis entirely is what this is nothing but J of theta 0 comma Theta 1 and we\nbut J of theta 0 comma Theta 1 and we\nbut J of theta 0 comma Theta 1 and we really need to minimize this so this is\nreally need to minimize this so this is\nreally need to minimize this so this is our task okay this is our task now let's\nour task okay this is our task now let's\nour task okay this is our task now let's go ahead and let's try to compare with\ngo ahead and let's try to compare with\ngo ahead and let's try to compare with two different thing one is the\ntwo different thing one is the\ntwo different thing one is the hypothesis testing and one is with\nhypothesis testing and one is with\nhypothesis testing and one is with respect to the cost\nrespect to the cost\nrespect to the cost function okay let's take an\nfunction okay let's take an\nfunction okay let's take an example so right now my equation of\nexample so right now my equation of\nexample so right now my equation of the\nthe\nthe hypothesis is nothing but H Theta of x\nhypothesis is nothing but H Theta of x\nhypothesis is nothing but H Theta of x equal to Theta 0 + Theta 1 *\nequal to Theta 0 + Theta 1 *\nequal to Theta 0 + Theta 1 * X if Theta 0 is 0 then what does this\nX if Theta 0 is 0 then what does this\nX if Theta 0 is 0 then what does this basically indicate can I say that it\nbasically indicate can I say that it\nbasically indicate can I say that it basically the line the line the best fit\nbasically the line the line the best fit\nbasically the line the line the best fit line passes through the origin and this\nline passes through the origin and this\nline passes through the origin and this is nothing but s Theta of xal to Theta\nis nothing but s Theta of xal to Theta\nis nothing but s Theta of xal to Theta 1 multiplied by X can I say like this\n1 multiplied by X can I say like this\n1 multiplied by X can I say like this obviously I can definitely say like this\nobviously I can definitely say like this\nobviously I can definitely say like this right so my equation will be like this\nright so my equation will be like this\nright so my equation will be like this so for right now let's consider that\nso for right now let's consider that\nso for right now let's consider that your Theta 0 is equal to 0 so this is\nyour Theta 0 is equal to 0 so this is\nyour Theta 0 is equal to 0 so this is what it is we have done till here we\nwhat it is we have done till here we\nwhat it is we have done till here we have minimized we have written the\nhave minimized we have written the\nhave minimized we have written the equation everything yes so it is passing\nequation everything yes so it is passing\nequation everything yes so it is passing through the origin and this is what is\nthrough the origin and this is what is\nthrough the origin and this is what is the equation I'm actually getting now\nthe equation I'm actually getting now\nthe equation I'm actually getting now let's take one example and let's try to\nlet's take one example and let's try to\nlet's take one example and let's try to solve this if I if I have H Theta of X\nsolve this if I if I have H Theta of X\nsolve this if I if I have H Theta of X so this is my new hypothesis considering\nso this is my new hypothesis considering\nso this is my new hypothesis considering that my intercept is passing through the\nthat my intercept is passing through the\nthat my intercept is passing through the region so with respect to this let's say\nregion so with respect to this let's say\nregion so with respect to this let's say that I will create one line over here\nthat I will create one line over here\nthat I will create one line over here let's say this is\nlet's say this is\nlet's say this is my this is my data points like X1 y1 I\nmy this is my data points like X1 y1 I\nmy this is my data points like X1 y1 I have 1 2 3 I have 1 2 3 now let's\nhave 1 2 3 I have 1 2 3 now let's\nhave 1 2 3 I have 1 2 3 now let's consider that if I have T I have data\nconsider that if I have T I have data\nconsider that if I have T I have data points like what I have data points like\npoints like what I have data points like\npoints like what I have data points like let's say I have three data points 1\nlet's say I have three data points 1\nlet's say I have three data points 1 comma 1 2A 2 3 comma 3 so 1A 1 is\ncomma 1 2A 2 3 comma 3 so 1A 1 is\ncomma 1 2A 2 3 comma 3 so 1A 1 is nothing but this is my data point 2A 2\nnothing but this is my data point 2A 2\nnothing but this is my data point 2A 2 is nothing but this is my data point and\nis nothing but this is my data point and\nis nothing but this is my data point and 3 comma 3 is this is my data point so\n3 comma 3 is this is my data point so\n3 comma 3 is this is my data point so these are my data points from the data\nthese are my data points from the data\nthese are my data points from the data set that I\nset that I\nset that I have so 2 comma 2 is this point and 3\nhave so 2 comma 2 is this point and 3\nhave so 2 comma 2 is this point and 3 comma 3 is basically this point let's\ncomma 3 is basically this point let's\ncomma 3 is basically this point let's consider that these are my points that I\nconsider that these are my points that I\nconsider that these are my points that I have these are my data points now if I\nhave these are my data points now if I\nhave these are my data points now if I consider Theta 1 as 1 where do you think\nconsider Theta 1 as 1 where do you think\nconsider Theta 1 as 1 where do you think the straight line will pass through\nthe straight line will pass through\nthe straight line will pass through where do you think the straight line\nwhere do you think the straight line\nwhere do you think the straight line will pass the straight line will\nwill pass the straight line will\nwill pass the straight line will definitely pass like this right my\ndefinitely pass like this right my\ndefinitely pass like this right my straight line will definitely pass\nstraight line will definitely pass\nstraight line will definitely pass through all the points this same point\nthrough all the points this same point\nthrough all the points this same point becomes a prediction point also right\nbecomes a prediction point also right\nbecomes a prediction point also right same point let's consider that this is\nsame point let's consider that this is\nsame point let's consider that this is also getting pass through this it passes\nalso getting pass through this it passes\nalso getting pass through this it passes through all the points when Theta 1 is\nthrough all the points when Theta 1 is\nthrough all the points when Theta 1 is equal to 1 Theta 1 is nothing but slope\nequal to 1 Theta 1 is nothing but slope\nequal to 1 Theta 1 is nothing but slope when slope is equal to 1 in this\nwhen slope is equal to 1 in this\nwhen slope is equal to 1 in this scenario it passes through all the\nscenario it passes through all the\nscenario it passes through all the points now go ahead and calculate your J\npoints now go ahead and calculate your J\npoints now go ahead and calculate your J of theta so what will the form of J of\nof theta so what will the form of J of\nof theta so what will the form of J of theta 1 become because Theta 0 is 0 okay\ntheta 1 become because Theta 0 is 0 okay\ntheta 1 become because Theta 0 is 0 okay we can basically write 1 by 2 m\nwe can basically write 1 by 2 m\nwe can basically write 1 by 2 m summation of I = 1 2 three how many\nsummation of I = 1 2 three how many\nsummation of I = 1 2 three how many points are there three right and here I\npoints are there three right and here I\npoints are there three right and here I have J of H of theta of X1\nhave J of H of theta of X1\nhave J of H of theta of X1 sorry X of theta of x i - y i\nsorry X of theta of x i - y i\nsorry X of theta of x i - y i s right now let's go ahead and compute\ns right now let's go ahead and compute\ns right now let's go ahead and compute now in this particular scenario what\nnow in this particular scenario what\nnow in this particular scenario what will happen 1X 2 m\nwill happen 1X 2 m\nwill happen 1X 2 m then what is what is this point minus y\nthen what is what is this point minus y\nthen what is what is this point minus y of I see h of X is also 1 y of I is also\nof I see h of X is also 1 y of I is also\nof I see h of X is also 1 y of I is also one both the point are 1 so this will\none both the point are 1 so this will\none both the point are 1 so this will become 1 - 1 whole S Plus because we are\nbecome 1 - 1 whole S Plus because we are\nbecome 1 - 1 whole S Plus because we are doing summation the next point is also\ndoing summation the next point is also\ndoing summation the next point is also falling in 2A 2 so this will become 2 -\nfalling in 2A 2 so this will become 2 -\nfalling in 2A 2 so this will become 2 - 2 s + 3 - 3 S so in total this will\n2 s + 3 - 3 S so in total this will\n2 s + 3 - 3 S so in total this will become zero so when your J of theta when\nbecome zero so when your J of theta when\nbecome zero so when your J of theta when Theta 1 is 1 Theta 1 is 1 so J of theta\nTheta 1 is 1 Theta 1 is 1 so J of theta\nTheta 1 is 1 Theta 1 is 1 so J of theta 1 is how much it is\n1 is how much it is\n1 is how much it is Z right so what is this J of theta 1 it\nZ right so what is this J of theta 1 it\nZ right so what is this J of theta 1 it is the cost function so let me draw the\nis the cost function so let me draw the\nis the cost function so let me draw the cost function graph over here let's say\ncost function graph over here let's say\ncost function graph over here let's say that this is my Theta and this is\nthat this is my Theta and this is\nthat this is my Theta and this is my so here I have 0.5 here I have 1 here\nmy so here I have 0.5 here I have 1 here\nmy so here I have 0.5 here I have 1 here I have 1.5 so this is my Theta here I\nI have 1.5 so this is my Theta here I\nI have 1.5 so this is my Theta here I have two then I have 2.5 okay then\nhave two then I have 2.5 okay then\nhave two then I have 2.5 okay then similarly I have 0. five then I have 1\n1.5 2 2.5 this is my J of theta 1 so\n1.5 2 2.5 this is my J of theta 1 so right now what is my Theta 1 my Theta 1\nright now what is my Theta 1 my Theta 1\nright now what is my Theta 1 my Theta 1 is 1 at this particular Point what did I\nis 1 at this particular Point what did I\nis 1 at this particular Point what did I get J of theta 1 is nothing but zero so\nget J of theta 1 is nothing but zero so\nget J of theta 1 is nothing but zero so this will be my first point this will be\nthis will be my first point this will be\nthis will be my first point this will be my first point guys I have discussed why\nmy first point guys I have discussed why\nmy first point guys I have discussed why why the value will be 1X 2m basically to\nwhy the value will be 1X 2m basically to\nwhy the value will be 1X 2m basically to make the calculation simpler we are\nmake the calculation simpler we are\nmake the calculation simpler we are dividing by 1X 2 m is basically used to\ndividing by 1X 2 m is basically used to\ndividing by 1X 2 m is basically used to average aage is the sumission that we\naverage aage is the sumission that we\naverage aage is the sumission that we are actually doing over here now let's\nare actually doing over here now let's\nare actually doing over here now let's go ahead and let's take the second\ngo ahead and let's take the second\ngo ahead and let's take the second scenario in the second scenario let's\nscenario in the second scenario let's\nscenario in the second scenario let's consider my Theta 1 let's say that my\nconsider my Theta 1 let's say that my\nconsider my Theta 1 let's say that my Theta 1 over here is now 0.5 if my Theta\nTheta 1 over here is now 0.5 if my Theta\nTheta 1 over here is now 0.5 if my Theta 1 is 0.5 then tell me what are the\n1 is 0.5 then tell me what are the\n1 is 0.5 then tell me what are the points that I will get for x equal to\npoints that I will get for x equal to\npoints that I will get for x equal to 1.5 * 1 so it will come as 0.5 over\n1.5 * 1 so it will come as 0.5 over\n1.5 * 1 so it will come as 0.5 over here right then similarly when X is\nhere right then similarly when X is\nhere right then similarly when X is equal to 2.5 * 2 is nothing but 1 over\nequal to 2.5 * 2 is nothing but 1 over\nequal to 2.5 * 2 is nothing but 1 over here and then similarly when uh for x\nhere and then similarly when uh for x\nhere and then similarly when uh for x equal to\nequal to\nequal to 35 multiplied by 3 see we are\n35 multiplied by 3 see we are\n35 multiplied by 3 see we are multiplying here right5 multi by 3 is\nmultiplying here right5 multi by 3 is\nmultiplying here right5 multi by 3 is 1.5 so the next point will come over\n1.5 so the next point will come over\n1.5 so the next point will come over here now when I create my best fit line\nhere now when I create my best fit line\nhere now when I create my best fit line what will happen so here is my next best\nwhat will happen so here is my next best\nwhat will happen so here is my next best fit line which I will probably create by\nfit line which I will probably create by\nfit line which I will probably create by green\ngreen\ngreen color okay so this is my second one\ncolor okay so this is my second one\ncolor okay so this is my second one which is green color here definitely\nwhich is green color here definitely\nwhich is green color here definitely slope is decreasing so if I go ahead and\nslope is decreasing so if I go ahead and\nslope is decreasing so if I go ahead and calculate my J of theta let's see what\ncalculate my J of theta let's see what\ncalculate my J of theta let's see what I'll get so J of theta\nI'll get so J of theta\nI'll get so J of theta 1 is nothing but 1X 2\n1 is nothing but 1X 2\n1 is nothing but 1X 2 m again same equation summation of I = 1\nm again same equation summation of I = 1\nm again same equation summation of I = 1 2 3 H Theta of X of\n2 3 H Theta of X of\n2 3 H Theta of X of i - y of\ni - y of\ni - y of iÂ² so what we have for over here we have\niÂ² so what we have for over here we have\niÂ² so what we have for over here we have nothing but 1X 2 m now let's do the\nnothing but 1X 2 m now let's do the\nnothing but 1X 2 m now let's do the summation what is this point this point\nsummation what is this point this point\nsummation what is this point this point is nothing but the predicted point and\nis nothing but the predicted point and\nis nothing but the predicted point and this point is the real point right so in\nthis point is the real point right so in\nthis point is the real point right so in this particular scenario the first point\nthis particular scenario the first point\nthis particular scenario the first point that I will get is nothing but. 5 - 1\nthat I will get is nothing but. 5 - 1\nthat I will get is nothing but. 5 - 1 whole s how I'm getting. 5 - 1 whole\nwhole s how I'm getting. 5 - 1 whole\nwhole s how I'm getting. 5 - 1 whole Square this is 1 this is the real Point\nSquare this is 1 this is the real Point\nSquare this is 1 this is the real Point 1 this is the predicted Point .5 so here\n1 this is the predicted Point .5 so here\n1 this is the predicted Point .5 so here I'm getting. 5 - 1 whole Square the\nI'm getting. 5 - 1 whole Square the\nI'm getting. 5 - 1 whole Square the second point will be 1 - 2 whole s right\nsecond point will be 1 - 2 whole s right\nsecond point will be 1 - 2 whole s right 2 so 1 - 2 whole\n2 so 1 - 2 whole\n2 so 1 - 2 whole s and then I will finally get 1.5 - 3\ns and then I will finally get 1.5 - 3\ns and then I will finally get 1.5 - 3 whole s so finally if I do this\nwhole s so finally if I do this\nwhole s so finally if I do this calculation how much I'm actually\ncalculation how much I'm actually\ncalculation how much I'm actually getting 1X 2 * 3 which is 6 here I'm\ngetting 1X 2 * 3 which is 6 here I'm\ngetting 1X 2 * 3 which is 6 here I'm getting\ngetting\ngetting .25 5 Square here I'm getting 1 here I'm\n.25 5 Square here I'm getting 1 here I'm\n.25 5 Square here I'm getting 1 here I'm getting 1.5 whole Square so my final\ngetting 1.5 whole Square so my final\ngetting 1.5 whole Square so my final output will be which I have already\noutput will be which I have already\noutput will be which I have already calculated it is nothing but point it\ncalculated it is nothing but point it\ncalculated it is nothing but point it will be approximately equal to. 58 so 58\nwill be approximately equal to. 58 so 58\nwill be approximately equal to. 58 so 58 now with Theta as this is nothing but\nnow with Theta as this is nothing but\nnow with Theta as this is nothing but Theta Theta 1 as\nTheta Theta 1 as\nTheta Theta 1 as .5 right that is what Theta 1 as .5 we\n.5 right that is what Theta 1 as .5 we\n.5 right that is what Theta 1 as .5 we are able to get. 58 so Theta 1 is .5\nare able to get. 58 so Theta 1 is .5\nare able to get. 58 so Theta 1 is .5 over here and. 58 will be coming\nover here and. 58 will be coming\nover here and. 58 will be coming somewhere here right so this is my next\nsomewhere here right so this is my next\nsomewhere here right so this is my next point which will be again in green color\npoint which will be again in green color\npoint which will be again in green color now let's go ahead and calculate the\nnow let's go ahead and calculate the\nnow let's go ahead and calculate the third condition now in third condition\nthird condition now in third condition\nthird condition now in third condition what I'm actually going to write I'm\nwhat I'm actually going to write I'm\nwhat I'm actually going to write I'm going to basically say Theta 1 as 0 at\ngoing to basically say Theta 1 as 0 at\ngoing to basically say Theta 1 as 0 at that point of time just go and assume\nthat point of time just go and assume\nthat point of time just go and assume what is 0 multiplied by X it will\nwhat is 0 multiplied by X it will\nwhat is 0 multiplied by X it will obviously be zero so I will be getting\nobviously be zero so I will be getting\nobviously be zero so I will be getting three points and my next line will be in\nthree points and my next line will be in\nthree points and my next line will be in this line that is the\nthis line that is the\nthis line that is the x-axis and this is basically all my\nx-axis and this is basically all my\nx-axis and this is basically all my points now if I go ahead and calculate\npoints now if I go ahead and calculate\npoints now if I go ahead and calculate this what is J of theta 1\nthis what is J of theta 1\nthis what is J of theta 1 now what is J of theta 1 now in this\nnow what is J of theta 1 now in this\nnow what is J of theta 1 now in this particular case when my Theta 1 is equal\nparticular case when my Theta 1 is equal\nparticular case when my Theta 1 is equal = to 0 1X 2 m now this part you'll be\n= to 0 1X 2 m now this part you'll be\n= to 0 1X 2 m now this part you'll be able to see this is 0 - 1 0 - 2 0 -\nable to see this is 0 - 1 0 - 2 0 -\nable to see this is 0 - 1 0 - 2 0 - 3 okay so it will become 0 - 1 s 0 - 2 s\n3 okay so it will become 0 - 1 s 0 - 2 s\n3 okay so it will become 0 - 1 s 0 - 2 s and 0 - 3\nand 0 - 3\nand 0 - 3 S okay so this will become 1X 6\nS okay so this will become 1X 6\nS okay so this will become 1X 6 * 1 + 4 + 9 which will not be it will be\n* 1 + 4 + 9 which will not be it will be\n* 1 + 4 + 9 which will not be it will be nothing but 2.3 which is approximately\nnothing but 2.3 which is approximately\nnothing but 2.3 which is approximately equal to\nequal to\nequal to 2.3 then what will happen with respect\n2.3 then what will happen with respect\n2.3 then what will happen with respect to Theta 1 as 0 we are getting 2.3 so if\nto Theta 1 as 0 we are getting 2.3 so if\nto Theta 1 as 0 we are getting 2.3 so if I draw this it is nothing but with\nI draw this it is nothing but with\nI draw this it is nothing but with respect to zero I'm getting 2.\nrespect to zero I'm getting 2.\nrespect to zero I'm getting 2. 2\n2\n2 2.3 this is my point so similarly when\n2.3 this is my point so similarly when\n2.3 this is my point so similarly when you start constructing with Theta 1 is\nyou start constructing with Theta 1 is\nyou start constructing with Theta 1 is equal 2 I may get some point over here\nequal 2 I may get some point over here\nequal 2 I may get some point over here so here when I join this points\nso here when I join this points\nso here when I join this points together you will be seeing that I will\ntogether you will be seeing that I will\ntogether you will be seeing that I will be getting this kind of\nbe getting this kind of\nbe getting this kind of curve okay and this curve is something\ncurve okay and this curve is something\ncurve okay and this curve is something called as gradient\ncalled as gradient\ncalled as gradient descent and this gradient descent will\ndescent and this gradient descent will\ndescent and this gradient descent will play a very very important role in\nplay a very very important role in\nplay a very very important role in making sure that in making sure that you\nmaking sure that in making sure that you\nmaking sure that in making sure that you get the right Theta 1 value or light\nget the right Theta 1 value or light\nget the right Theta 1 value or light slope value now which is the most\nslope value now which is the most\nslope value now which is the most suitable point the most suitable point\nsuitable point the most suitable point\nsuitable point the most suitable point is to come over here because this is\nis to come over here because this is\nis to come over here because this is this this point is basically called AS\nthis this point is basically called AS\nthis this point is basically called AS Global\nGlobal\nGlobal Minima because see out of all these\nMinima because see out of all these\nMinima because see out of all these three lines which is the best fit line\nthree lines which is the best fit line\nthree lines which is the best fit line this is the best fit line right this is\nthis is the best fit line right this is\nthis is the best fit line right this is the best fit line when I had this best\nthe best fit line when I had this best\nthe best fit line when I had this best fit line my point that came over here\nfit line my point that came over here\nfit line my point that came over here was here itself this was my point that\nwas here itself this was my point that\nwas here itself this was my point that came over here right and I want to\ncame over here right and I want to\ncame over here right and I want to basically come to this region because\nbasically come to this region because\nbasically come to this region because this is my Global\nthis is my Global\nthis is my Global Minima when I basically am over here the\nMinima when I basically am over here the\nMinima when I basically am over here the distance between the predicted and the\ndistance between the predicted and the\ndistance between the predicted and the real point is very very less right so\nreal point is very very less right so\nreal point is very very less right so this specific point is basically called\nthis specific point is basically called\nthis specific point is basically called AS Global minimum but still I did not\nAS Global minimum but still I did not\nAS Global minimum but still I did not discuss Krish you have assumed Theta 1\ndiscuss Krish you have assumed Theta 1\ndiscuss Krish you have assumed Theta 1 is 1 Theta 1 is .5 Theta 1 is 0 here\nis 1 Theta 1 is .5 Theta 1 is 0 here\nis 1 Theta 1 is .5 Theta 1 is 0 here also you're assuming many things right\nalso you're assuming many things right\nalso you're assuming many things right and then you probably calculating and\nand then you probably calculating and\nand then you probably calculating and you're creating this gradient descent\nyou're creating this gradient descent\nyou're creating this gradient descent but the thing should be that probably\nbut the thing should be that probably\nbut the thing should be that probably you come to one point over here and then\nyou come to one point over here and then\nyou come to one point over here and then you reach towards this so for that\nyou reach towards this so for that\nyou reach towards this so for that specific reason how do you do that how\nspecific reason how do you do that how\nspecific reason how do you do that how do I first of all come to a point and\ndo I first of all come to a point and\ndo I first of all come to a point and then move towards This Global Minima so\nthen move towards This Global Minima so\nthen move towards This Global Minima so for that specific case we will be using\nfor that specific case we will be using\nfor that specific case we will be using one convergence algorithm because if I\none convergence algorithm because if I\none convergence algorithm because if I come to one specific point after that I\ncome to one specific point after that I\ncome to one specific point after that I just need to keep on updating Theta 1\njust need to keep on updating Theta 1\njust need to keep on updating Theta 1 instead of using different different\ninstead of using different different\ninstead of using different different Theta 1 value so for this we use\nTheta 1 value so for this we use\nTheta 1 value so for this we use something called as convergence\nsomething called as convergence\nsomething called as convergence algorithm so here the convergence\nalgorithm so here the convergence\nalgorithm so here the convergence algorithm basically says\nconvergence that basically means I'm in\nconvergence that basically means I'm in a while loop let's say and here I'm\na while loop let's say and here I'm\na while loop let's say and here I'm basically going to update my Theta value\nbasically going to update my Theta value\nbasically going to update my Theta value which will be given by this notation\nwhich will be given by this notation\nwhich will be given by this notation which is continuous updation where I'll\nwhich is continuous updation where I'll\nwhich is continuous updation where I'll say Theta J minus I'll talk about this\nsay Theta J minus I'll talk about this\nsay Theta J minus I'll talk about this Alpha don't worry and then it will be\nAlpha don't worry and then it will be\nAlpha don't worry and then it will be derivative of theta\nderivative of theta\nderivative of theta J with respect to this J of theta\nJ with respect to this J of theta\nJ with respect to this J of theta 0 and Theta 1 so this should happen that\n0 and Theta 1 so this should happen that\n0 and Theta 1 so this should happen that basically means after we reach to a\nbasically means after we reach to a\nbasically means after we reach to a specific point of theta after performing\nspecific point of theta after performing\nspecific point of theta after performing this particular operation we should be\nthis particular operation we should be\nthis particular operation we should be able to come to the global Minima and\nable to come to the global Minima and\nable to come to the global Minima and this this specific thing that you are\nthis this specific thing that you are\nthis this specific thing that you are able to see is called as\nable to see is called as\nable to see is called as derivative this is called as derivative\nderivative this is called as derivative\nderivative this is called as derivative derivative basically means I'm trying to\nderivative basically means I'm trying to\nderivative basically means I'm trying to find out the slope\nfind out the slope\nfind out the slope derivative which I can also say it as\nderivative which I can also say it as\nderivative which I can also say it as slope this equation will definitely work\nslope this equation will definitely work\nslope this equation will definitely work guys trust me this will definitely work\nguys trust me this will definitely work\nguys trust me this will definitely work why it will work I'll just draw it show\nwhy it will work I'll just draw it show\nwhy it will work I'll just draw it show it to you let's say that this is my cost\nit to you let's say that this is my cost\nit to you let's say that this is my cost function let's say that I've got this\nfunction let's say that I've got this\nfunction let's say that I've got this gradient\ngradient\ngradient descent and let's say that my first\ndescent and let's say that my first\ndescent and let's say that my first point is somewhere here but I have to\npoint is somewhere here but I have to\npoint is somewhere here but I have to reach somewhere here right now when I\nreach somewhere here right now when I\nreach somewhere here right now when I reach this this is my Theta 1 and this\nreach this this is my Theta 1 and this\nreach this this is my Theta 1 and this is my J of theta 1 suppose I reach at\nis my J of theta 1 suppose I reach at\nis my J of theta 1 suppose I reach at this specific point and I will also have\nthis specific point and I will also have\nthis specific point and I will also have another gradient descent which looks\nanother gradient descent which looks\nanother gradient descent which looks like this let's say that in the initial\nlike this let's say that in the initial\nlike this let's say that in the initial time I reach the point over here how we\ntime I reach the point over here how we\ntime I reach the point over here how we will be coming to this minimal Global\nwill be coming to this minimal Global\nwill be coming to this minimal Global Minima by using this equation I'll talk\nMinima by using this equation I'll talk\nMinima by using this equation I'll talk about Alpha also don't worry now this is\nabout Alpha also don't worry now this is\nabout Alpha also don't worry now this is also my Theta 1 this is also my J of\nalso my Theta 1 this is also my J of\nalso my Theta 1 this is also my J of theta 1 now let's say suppose I came to\ntheta 1 now let's say suppose I came to\ntheta 1 now let's say suppose I came to this particular point right after coming\nthis particular point right after coming\nthis particular point right after coming to this particular point I will\nto this particular point I will\nto this particular point I will basically apply this derivative on this\nbasically apply this derivative on this\nbasically apply this derivative on this J of theta 1 okay now when I find out a\nJ of theta 1 okay now when I find out a\nJ of theta 1 okay now when I find out a derivative that basically means we are\nderivative that basically means we are\nderivative that basically means we are trying to find out the slope and in\ntrying to find out the slope and in\ntrying to find out the slope and in order to find the slope we just create a\norder to find the slope we just create a\norder to find the slope we just create a straight line like\nstraight line like\nstraight line like this which will look like this I'll just\nthis which will look like this I'll just\nthis which will look like this I'll just try to\ntry to\ntry to create so I'll try to create a slope\ncreate so I'll try to create a slope\ncreate so I'll try to create a slope like this this\nlike this this\nlike this this slope so if you try to find out with\nslope so if you try to find out with\nslope so if you try to find out with respect to this this is a positive slope\nrespect to this this is a positive slope\nrespect to this this is a positive slope how do we indicate it because understand\nhow do we indicate it because understand\nhow do we indicate it because understand the right hand side of the line of this\nthe right hand side of the line of this\nthe right hand side of the line of this is pointing on the top wordss Direction\nis pointing on the top wordss Direction\nis pointing on the top wordss Direction this is the best easy way to find out\nthis is the best easy way to find out\nthis is the best easy way to find out whether it is a positive slope or\nwhether it is a positive slope or\nwhether it is a positive slope or negative slope now in this particular\nnegative slope now in this particular\nnegative slope now in this particular case this is a positive slope now when I\ncase this is a positive slope now when I\ncase this is a positive slope now when I get a positive slope that basically\nget a positive slope that basically\nget a positive slope that basically means I will update my weights or Theta\nmeans I will update my weights or Theta\nmeans I will update my weights or Theta 1 as Theta 1 let's say I'm writing it\n1 as Theta 1 let's say I'm writing it\n1 as Theta 1 let's say I'm writing it over here so I will just apply this\nover here so I will just apply this\nover here so I will just apply this convergence algorithm see Theta\nconvergence algorithm see Theta\nconvergence algorithm see Theta 1 colon Theta 1 minus this learning rate\n1 colon Theta 1 minus this learning rate\n1 colon Theta 1 minus this learning rate which is called as Alpha this is my my\nwhich is called as Alpha this is my my\nwhich is called as Alpha this is my my learning rate I'll talk about learning\nlearning rate I'll talk about learning\nlearning rate I'll talk about learning rate don't worry then this derivative\nrate don't worry then this derivative\nrate don't worry then this derivative value in this particular case since I'm\nvalue in this particular case since I'm\nvalue in this particular case since I'm having a positive slope I will be\nhaving a positive slope I will be\nhaving a positive slope I will be getting a positive value let's say that\ngetting a positive value let's say that\ngetting a positive value let's say that for this Theta value I got this slope\nfor this Theta value I got this slope\nfor this Theta value I got this slope initially now I need to come to this\ninitially now I need to come to this\ninitially now I need to come to this location so for that I have to reduce\nlocation so for that I have to reduce\nlocation so for that I have to reduce Theta 1 so that I come to this main\nTheta 1 so that I come to this main\nTheta 1 so that I come to this main point now here you can see that I am I\npoint now here you can see that I am I\npoint now here you can see that I am I subtracting Theta 1 with something which\nsubtracting Theta 1 with something which\nsubtracting Theta 1 with something which is a positive number\nis a positive number\nis a positive number right this is a positive number so\nright this is a positive number so\nright this is a positive number so definitely I know that after some n\ndefinitely I know that after some n\ndefinitely I know that after some n number of iteration I will be able to\nnumber of iteration I will be able to\nnumber of iteration I will be able to come to the global Minima similarly if I\ncome to the global Minima similarly if I\ncome to the global Minima similarly if I take the right hand side and if I try to\ntake the right hand side and if I try to\ntake the right hand side and if I try to draw the slope in this particular case\ndraw the slope in this particular case\ndraw the slope in this particular case my slope will be\nmy slope will be\nmy slope will be negative so similarly I can write the\nnegative so similarly I can write the\nnegative so similarly I can write the equation as Theta\nequation as Theta\nequation as Theta 1 = to Theta 1 minus learning rate\n1 = to Theta 1 minus learning rate\n1 = to Theta 1 minus learning rate multiplied by a negative number so minus\nmultiplied by a negative number so minus\nmultiplied by a negative number so minus into minus will be positive right\ninto minus will be positive right\ninto minus will be positive right suppose initially my 1 was\nsuppose initially my 1 was\nsuppose initially my 1 was here my Theta 1 was here now I'll keep\nhere my Theta 1 was here now I'll keep\nhere my Theta 1 was here now I'll keep on updating the weight to come to this\non updating the weight to come to this\non updating the weight to come to this Global Minima so minus into minus is\nGlobal Minima so minus into minus is\nGlobal Minima so minus into minus is positive so I will basically get Theta 1\npositive so I will basically get Theta 1\npositive so I will basically get Theta 1 +\n+\n+ Alpha by a positive number because minus\nAlpha by a positive number because minus\nAlpha by a positive number because minus into minus is plus so this will\ninto minus is plus so this will\ninto minus is plus so this will definitely work so that we will be able\ndefinitely work so that we will be able\ndefinitely work so that we will be able to come over here to the global Minima\nto come over here to the global Minima\nto come over here to the global Minima whether it is a positive slope or a\nwhether it is a positive slope or a\nwhether it is a positive slope or a negative slope now what is this learning\nnegative slope now what is this learning\nnegative slope now what is this learning learning rate now learning rate based on\nlearning rate now learning rate based on\nlearning rate now learning rate based on this learning rate suppose I want to\nthis learning rate suppose I want to\nthis learning rate suppose I want to come from this point to the global\ncome from this point to the global\ncome from this point to the global Minima by what speed I should be coming\nMinima by what speed I should be coming\nMinima by what speed I should be coming what speed if my learning rate value is\nwhat speed if my learning rate value is\nwhat speed if my learning rate value is bigger what speed I may be coming\nbigger what speed I may be coming\nbigger what speed I may be coming suppose if I say usually we select\nsuppose if I say usually we select\nsuppose if I say usually we select learning rate as 01 if I select a small\nlearning rate as 01 if I select a small\nlearning rate as 01 if I select a small number then it'll start taking small\nnumber then it'll start taking small\nnumber then it'll start taking small small steps to move towards the optimal\nsmall steps to move towards the optimal\nsmall steps to move towards the optimal Minima but if I take a alpha value a\nMinima but if I take a alpha value a\nMinima but if I take a alpha value a huge value if it is a huge huge value\nhuge value if it is a huge huge value\nhuge value if it is a huge huge value then what will happen this uh this\nthen what will happen this uh this\nthen what will happen this uh this updation of the Theta 1 will keep on\nupdation of the Theta 1 will keep on\nupdation of the Theta 1 will keep on jumping here and there and the situation\njumping here and there and the situation\njumping here and there and the situation will be that it will never meet it will\nwill be that it will never meet it will\nwill be that it will never meet it will never reach the global Minima so it is a\nnever reach the global Minima so it is a\nnever reach the global Minima so it is a very very good decision to take a alpha\nvery very good decision to take a alpha\nvery very good decision to take a alpha small value it should also not be a very\nsmall value it should also not be a very\nsmall value it should also not be a very very small value if it becomes a very\nvery small value if it becomes a very\nvery small value if it becomes a very very small value then what will happen\nvery small value then what will happen\nvery small value then what will happen very tiny steps it will take forever to\nvery tiny steps it will take forever to\nvery tiny steps it will take forever to reach the global Minima that basically\nreach the global Minima that basically\nreach the global Minima that basically means my model will keep on training\nmeans my model will keep on training\nmeans my model will keep on training itself so definitely this Al is going to\nitself so definitely this Al is going to\nitself so definitely this Al is going to work now let me talk about one\nwork now let me talk about one\nwork now let me talk about one scenario one scenario will be that what\nscenario one scenario will be that what\nscenario one scenario will be that what if my my cost function has a local\nif my my cost function has a local\nif my my cost function has a local Minima what if I have a local Minima\nMinima what if I have a local Minima\nMinima what if I have a local Minima because here if I\nbecause here if I\nbecause here if I come here if I come this is a local\ncome here if I come this is a local\ncome here if I come this is a local Minima suppose one of my points come\nMinima suppose one of my points come\nMinima suppose one of my points come over here and finally I'm reaching over\nover here and finally I'm reaching over\nover here and finally I'm reaching over here what will happen in this particular\nhere what will happen in this particular\nhere what will happen in this particular case because in this case you'll be\ncase because in this case you'll be\ncase because in this case you'll be seeing that what will be my equation my\nseeing that what will be my equation my\nseeing that what will be my equation my equation will be simply Theta 1\nequation will be simply Theta 1\nequation will be simply Theta 1 Theta 1 minus Alpha in this point in\nTheta 1 minus Alpha in this point in\nTheta 1 minus Alpha in this point in this local Minima slope will be zero so\nthis local Minima slope will be zero so\nthis local Minima slope will be zero so in this particular case my Theta 1 will\nin this particular case my Theta 1 will\nin this particular case my Theta 1 will be equal to Theta 1 now you may be\nbe equal to Theta 1 now you may be\nbe equal to Theta 1 now you may be thinking what is if this is the scenario\nthinking what is if this is the scenario\nthinking what is if this is the scenario then we will be stuck in local Minima\nthen we will be stuck in local Minima\nthen we will be stuck in local Minima this is called as local\nthis is called as local\nthis is called as local Minima but usually with respect to the\nMinima but usually with respect to the\nMinima but usually with respect to the gradient descent and the equation that\ngradient descent and the equation that\ngradient descent and the equation that we are using here we do not get stuck in\nwe are using here we do not get stuck in\nwe are using here we do not get stuck in local Minima because our gradient\nlocal Minima because our gradient\nlocal Minima because our gradient descent in this particular scenar iio\ndescent in this particular scenar iio\ndescent in this particular scenar iio will always look like this but yes in\nwill always look like this but yes in\nwill always look like this but yes in deep learning when we are learning about\ndeep learning when we are learning about\ndeep learning when we are learning about grade in descent and a Ann at that point\ngrade in descent and a Ann at that point\ngrade in descent and a Ann at that point of time we have lot of local Minima and\nof time we have lot of local Minima and\nof time we have lot of local Minima and because of that we have different\nbecause of that we have different\nbecause of that we have different different G decent algorithm like RMS\ndifferent G decent algorithm like RMS\ndifferent G decent algorithm like RMS prop we have Adam optimizers which will\nprop we have Adam optimizers which will\nprop we have Adam optimizers which will solve that specific problem so this one\nsolve that specific problem so this one\nsolve that specific problem so this one point also I wanted to mention because\npoint also I wanted to mention because\npoint also I wanted to mention because tomorrow if someone asks you as an\ntomorrow if someone asks you as an\ntomorrow if someone asks you as an interview question that what if in your\ninterview question that what if in your\ninterview question that what if in your uh do you see any local Minima in linear\nuh do you see any local Minima in linear\nuh do you see any local Minima in linear regression you can just that the cost\nregression you can just that the cost\nregression you can just that the cost function that we use will definitely not\nfunction that we use will definitely not\nfunction that we use will definitely not give us local Minima but if in deep\ngive us local Minima but if in deep\ngive us local Minima but if in deep learning techniques with that we are\nlearning techniques with that we are\nlearning techniques with that we are trying to use like Ann we have different\ntrying to use like Ann we have different\ntrying to use like Ann we have different different kind of optimizers which will\ndifferent kind of optimizers which will\ndifferent kind of optimizers which will solve that particular problem so that is\nsolve that particular problem so that is\nsolve that particular problem so that is the answer you basically have to give\nthe answer you basically have to give\nthe answer you basically have to give now let me go ahead and write with\nnow let me go ahead and write with\nnow let me go ahead and write with respect to the gradient descent\nrespect to the gradient descent\nrespect to the gradient descent algorithm so here again I'm going to\nalgorithm so here again I'm going to\nalgorithm so here again I'm going to write the gradient descent algorithm so\nwrite the gradient descent algorithm so\nwrite the gradient descent algorithm so this will be my gradient descent\nthis will be my gradient descent\nthis will be my gradient descent algorithm and remember guys gradient\nalgorithm and remember guys gradient\nalgorithm and remember guys gradient descent is an amazing algorithm and you\ndescent is an amazing algorithm and you\ndescent is an amazing algorithm and you you will definitely be using it so\nyou will definitely be using it so\nyou will definitely be using it so please make sure that you know this\nplease make sure that you know this\nplease make sure that you know this perfectly now some questions are that\nperfectly now some questions are that\nperfectly now some questions are that when will convergence stop convergence\nwhen will convergence stop convergence\nwhen will convergence stop convergence will stop when we come to near this area\nwill stop when we come to near this area\nwill stop when we come to near this area where my uh J of theta will be very very\nwhere my uh J of theta will be very very\nwhere my uh J of theta will be very very less now in gradient descent algorithm I\nless now in gradient descent algorithm I\nless now in gradient descent algorithm I will again repeat it so what did I say I\nwill again repeat it so what did I say I\nwill again repeat it so what did I say I said\nsaid\nsaid repeat until convergence I told you\nrepeat until convergence I told you\nrepeat until convergence I told you right here we have written this\nright here we have written this\nright here we have written this algorithm\nalgorithm\nalgorithm and now let's take it for Theta 0 and\nand now let's take it for Theta 0 and\nand now let's take it for Theta 0 and Theta 1 so here I will write Theta 0\nTheta 1 so here I will write Theta 0\nTheta 1 so here I will write Theta 0 J equal to Theta\nJ equal to Theta\nJ equal to Theta J minus learning rate of derivative of\nJ minus learning rate of derivative of\nJ minus learning rate of derivative of theta\nJ J of theta 0 and Theta 1 so this is my\nJ J of theta 0 and Theta 1 so this is my repeat until convergence now we really\nrepeat until convergence now we really\nrepeat until convergence now we really need to find out what we'll try to\nneed to find out what we'll try to\nneed to find out what we'll try to equate we'll try to first of all find\nequate we'll try to first of all find\nequate we'll try to first of all find out what is this\nout what is this\nout what is this now if I really want to find out\nnow if I really want to find out\nnow if I really want to find out derivative\nderivative\nderivative of derivative of derivative of theta J\nof derivative of derivative of theta J\nof derivative of derivative of theta J with respect to J of theta 0 and Theta 1\nwith respect to J of theta 0 and Theta 1\nwith respect to J of theta 0 and Theta 1 so how do I write this I can definitely\nso how do I write this I can definitely\nso how do I write this I can definitely write this in a easy way okay so this\nwrite this in a easy way okay so this\nwrite this in a easy way okay so this will be derivative of theta J and\nwill be derivative of theta J and\nwill be derivative of theta J and remember J will be 0 and 1 right because\nremember J will be 0 and 1 right because\nremember J will be 0 and 1 right because we need to find out for 0 Theta 0 and\nwe need to find out for 0 Theta 0 and\nwe need to find out for 0 Theta 0 and Theta 1 so this will be 1 by 2 m what is\nTheta 1 so this will be 1 by 2 m what is\nTheta 1 so this will be 1 by 2 m what is what is J of theta 0a Theta 1 obviously\nwhat is J of theta 0a Theta 1 obviously\nwhat is J of theta 0a Theta 1 obviously my cost function so I will write\nmy cost function so I will write\nmy cost function so I will write summation of IAL 1 to M and here I will\nsummation of IAL 1 to M and here I will\nsummation of IAL 1 to M and here I will basically write J of theta of X of I\nbasically write J of theta of X of I\nbasically write J of theta of X of I minus y of I whole squar so if my J is\nminus y of I whole squar so if my J is\nminus y of I whole squar so if my J is equal to Z so what will happen for this\nequal to Z so what will happen for this\nequal to Z so what will happen for this so here I can specifically say that\nso here I can specifically say that\nso here I can specifically say that derivative of derivative of theta 0 J of\nderivative of derivative of theta 0 J of\nderivative of derivative of theta 0 J of theta 0a 1\ntheta 0a 1\ntheta 0a 1 now it's simple here what I will be\nnow it's simple here what I will be\nnow it's simple here what I will be doing is that I will be simply applying\ndoing is that I will be simply applying\ndoing is that I will be simply applying derivative function see guys what is\nderivative function see guys what is\nderivative function see guys what is this derivative let's consider this is\nthis derivative let's consider this is\nthis derivative let's consider this is something like this 1X 2 m xÂ² so if I\nsomething like this 1X 2 m xÂ² so if I\nsomething like this 1X 2 m xÂ² so if I try to find out the derivative this will\ntry to find out the derivative this will\ntry to find out the derivative this will be 2x 2 MX so 2 and 2 will get cancel so\nbe 2x 2 MX so 2 and 2 will get cancel so\nbe 2x 2 MX so 2 and 2 will get cancel so similarly I'll have 1 by m and here I\nsimilarly I'll have 1 by m and here I\nsimilarly I'll have 1 by m and here I will specifically be writing summation\nwill specifically be writing summation\nwill specifically be writing summation of I = 1 2 m h Theta of x X of I which\nof I = 1 2 m h Theta of x X of I which\nof I = 1 2 m h Theta of x X of I which will be my\nwill be my\nwill be my x - y of iÂ² so this will be my\nx - y of iÂ² so this will be my\nx - y of iÂ² so this will be my derivative with respect to Theta 0 this\nderivative with respect to Theta 0 this\nderivative with respect to Theta 0 this is what I got now the second thing will\nis what I got now the second thing will\nis what I got now the second thing will be that when J is equal to 1 derivative\nbe that when J is equal to 1 derivative\nbe that when J is equal to 1 derivative of derivative of theta 1 J of theta 0\nof derivative of theta 1 J of theta 0\nof derivative of theta 1 J of theta 0 comma Theta\ncomma Theta\ncomma Theta 1 in this particular case I will be\n1 in this particular case I will be\n1 in this particular case I will be having 1 by m summation of I = 1 to M\nhaving 1 by m summation of I = 1 to M\nhaving 1 by m summation of I = 1 to M then again see in this particular case\nthen again see in this particular case\nthen again see in this particular case Theta of 1 is there right Theta of 1\nTheta of 1 is there right Theta of 1\nTheta of 1 is there right Theta of 1 basically means what if I try to replace\nbasically means what if I try to replace\nbasically means what if I try to replace this let's say that I'm trying to\nthis let's say that I'm trying to\nthis let's say that I'm trying to replace this H Theta of X with something\nreplace this H Theta of X with something\nreplace this H Theta of X with something else what is s Theta of X I know that\nelse what is s Theta of X I know that\nelse what is s Theta of X I know that right it is Theta 0 + Theta 1 * X so\nright it is Theta 0 + Theta 1 * X so\nright it is Theta 0 + Theta 1 * X so Theta 0 + Theta 1 * X so after this if\nTheta 0 + Theta 1 * X so after this if\nTheta 0 + Theta 1 * X so after this if I'm trying to find out the derivative\nI'm trying to find out the derivative\nI'm trying to find out the derivative with respect to Theta 0 this will\nwith respect to Theta 0 this will\nwith respect to Theta 0 this will obviously become I will be able to get\nobviously become I will be able to get\nobviously become I will be able to get this much right now with respect to the\nthis much right now with respect to the\nthis much right now with respect to the second derivative what I will be writing\nsecond derivative what I will be writing\nsecond derivative what I will be writing I will again be writing H thet of X of i\nI will again be writing H thet of X of i\nI will again be writing H thet of X of i - y of i s\n- y of i s\n- y of i s multiplied X of I so this Square also\nmultiplied X of I so this Square also\nmultiplied X of I so this Square also went off understand this H Theta of X is\nwent off understand this H Theta of X is\nwent off understand this H Theta of X is what see they H Theta of X is nothing\nwhat see they H Theta of X is nothing\nwhat see they H Theta of X is nothing but Theta 0 + Theta 1 * X so if I'm\nbut Theta 0 + Theta 1 * X so if I'm\nbut Theta 0 + Theta 1 * X so if I'm trying to find out derivative with\ntrying to find out derivative with\ntrying to find out derivative with respect to Theta 0 nothing will be going\nrespect to Theta 0 nothing will be going\nrespect to Theta 0 nothing will be going to come okay Theta 1 of X will become a\nto come okay Theta 1 of X will become a\nto come okay Theta 1 of X will become a constant in this particular case in this\nconstant in this particular case in this\nconstant in this particular case in this case because Theta 1 of X is there so if\ncase because Theta 1 of X is there so if\ncase because Theta 1 of X is there so if I try to find out derivative of theta 1\nI try to find out derivative of theta 1\nI try to find out derivative of theta 1 into X only I'll be getting X Y Square\ninto X only I'll be getting X Y Square\ninto X only I'll be getting X Y Square will not be there it's easy right X squ\nwill not be there it's easy right X squ\nwill not be there it's easy right X squ means 2x this is the derivative of x\nmeans 2x this is the derivative of x\nmeans 2x this is the derivative of x square right so that square went and 1X\nsquare right so that square went and 1X\nsquare right so that square went and 1X 2 1 2 by two got cancelled so this will\n2 1 2 by two got cancelled so this will\n2 1 2 by two got cancelled so this will be now my convergence algorithm so here\nbe now my convergence algorithm so here\nbe now my convergence algorithm so here we have discussed about linear\nwe have discussed about linear\nwe have discussed about linear regression oh sorry I have to remove\nregression oh sorry I have to remove\nregression oh sorry I have to remove Square here also so let me write it\nSquare here also so let me write it\nSquare here also so let me write it again okay repeat until conver con let\nagain okay repeat until conver con let\nagain okay repeat until conver con let me write it down again repeat until\nme write it down again repeat until\nme write it down again repeat until convergence finally your two updates\nconvergence finally your two updates\nconvergence finally your two updates will be happening one is Theta 0 so here\nwill be happening one is Theta 0 so here\nwill be happening one is Theta 0 so here it will be Theta 0\nit will be Theta 0\nit will be Theta 0 minus Alpha that is my learning rate 1\nminus Alpha that is my learning rate 1\nminus Alpha that is my learning rate 1 by m summation of IAL 1 to M and this\nby m summation of IAL 1 to M and this\nby m summation of IAL 1 to M and this will basically be H Theta of X of I\nwill basically be H Theta of X of I\nwill basically be H Theta of X of I minus y of\nminus y of\nminus y of I and similarly if I want to update\nI and similarly if I want to update\nI and similarly if I want to update Theta 1 it will be - alpha 1 by m\nTheta 1 it will be - alpha 1 by m\nTheta 1 it will be - alpha 1 by m summation of I = 1 to m h Theta of X of\nsummation of I = 1 to m h Theta of X of\nsummation of I = 1 to m h Theta of X of I oh my God y of I uh multiplied by X of\nI oh my God y of I uh multiplied by X of\nI oh my God y of I uh multiplied by X of I Alpha is your learning rate guys Alpha\nI Alpha is your learning rate guys Alpha\nI Alpha is your learning rate guys Alpha is nothing but it is learning rate here\nis nothing but it is learning rate here\nis nothing but it is learning rate here we have to initialize some value like\nwe have to initialize some value like\nwe have to initialize some value like 0.1 see what is s Theta of X Theta 0 +\n0.1 see what is s Theta of X Theta 0 +\n0.1 see what is s Theta of X Theta 0 + Theta 1 into X right if I do derivative\nTheta 1 into X right if I do derivative\nTheta 1 into X right if I do derivative of theta 1 into x what is derivative of\nof theta 1 into x what is derivative of\nof theta 1 into x what is derivative of theta 1 with Theta 1 x it is nothing but\ntheta 1 with Theta 1 x it is nothing but\ntheta 1 with Theta 1 x it is nothing but X so this x will come over here now\nX so this x will come over here now\nX so this x will come over here now let's discuss about two important thing\nlet's discuss about two important thing\nlet's discuss about two important thing one is R square and adjusted R square\none is R square and adjusted R square\none is R square and adjusted R square now similarly what will happen you will\nnow similarly what will happen you will\nnow similarly what will happen you will have lot of convex functions now see if\nhave lot of convex functions now see if\nhave lot of convex functions now see if I talk about uh like if you have\nI talk about uh like if you have\nI talk about uh like if you have multiple features like X1 X2 X3 x4 at\nmultiple features like X1 X2 X3 x4 at\nmultiple features like X1 X2 X3 x4 at that point of time you will be having a\nthat point of time you will be having a\nthat point of time you will be having a 3D curve curve which looks like this\n3D curve curve which looks like this\n3D curve curve which looks like this gradient\ngradient\ngradient decent which will be something like this\ngradient it's just like coming down a\ngradient it's just like coming down a mountain now let's discuss about two\nmountain now let's discuss about two\nmountain now let's discuss about two performance metrics which is important\nperformance metrics which is important\nperformance metrics which is important in this particular case one is R\nin this particular case one is R\nin this particular case one is R square and adjusted R square\nsquare and adjusted R square\nsquare and adjusted R square we usually use this performance metrix\nwe usually use this performance metrix\nwe usually use this performance metrix to verify how our model is and how good\nto verify how our model is and how good\nto verify how our model is and how good our model is with respect to linear\nour model is with respect to linear\nour model is with respect to linear regression so R square is basically\nregression so R square is basically\nregression so R square is basically given R square is a performance Matrix\ngiven R square is a performance Matrix\ngiven R square is a performance Matrix to check how good the specific model is\nto check how good the specific model is\nto check how good the specific model is so here we basically have a formula\nso here we basically have a formula\nso here we basically have a formula which is like 1 minus sum of residual\nwhich is like 1 minus sum of residual\nwhich is like 1 minus sum of residual divided by sum of total now this is the\ndivided by sum of total now this is the\ndivided by sum of total now this is the formula of R squ now what is this sum of\nformula of R squ now what is this sum of\nformula of R squ now what is this sum of residual I can basically write like this\nresidual I can basically write like this\nresidual I can basically write like this summation of y i Min - y i hat whole\nsummation of y i Min - y i hat whole\nsummation of y i Min - y i hat whole Square this Yi hat is nothing but H\nSquare this Yi hat is nothing but H\nSquare this Yi hat is nothing but H Theta of X just consider in this way\nTheta of X just consider in this way\nTheta of X just consider in this way divided by summation of Y of i - y mean\ndivided by summation of Y of i - y mean\ndivided by summation of Y of i - y mean y mean y s to formula this is the\ny mean y s to formula this is the\ny mean y s to formula this is the formula I'll try to explain you what\nformula I'll try to explain you what\nformula I'll try to explain you what this formula definitely says okay so\nthis formula definitely says okay so\nthis formula definitely says okay so first thing first let's consider that\nfirst thing first let's consider that\nfirst thing first let's consider that this is my this is my problem statement\nthis is my this is my problem statement\nthis is my this is my problem statement that I'm trying to solve suppose these\nthat I'm trying to solve suppose these\nthat I'm trying to solve suppose these are my data points and if I try to\nare my data points and if I try to\nare my data points and if I try to create the best fit\ncreate the best fit\ncreate the best fit line This Yi hat Yi hat basically means\nline This Yi hat Yi hat basically means\nline This Yi hat Yi hat basically means this specific point we are trying to\nthis specific point we are trying to\nthis specific point we are trying to find out the difference between this\nfind out the difference between this\nfind out the difference between this things difference between these things\nthings difference between these things\nthings difference between these things let's say that these are my points I'm\nlet's say that these are my points I'm\nlet's say that these are my points I'm trying to find out a difference between\ntrying to find out a difference between\ntrying to find out a difference between this predicted this is my predicted the\nthis predicted this is my predicted the\nthis predicted this is my predicted the point in green color are my predicted\npoint in green color are my predicted\npoint in green color are my predicted points which I have denoted as y i hat\npoints which I have denoted as y i hat\npoints which I have denoted as y i hat and always understand this is what Su\nand always understand this is what Su\nand always understand this is what Su sum of residual is sum of residual is\nsum of residual is sum of residual is\nsum of residual is sum of residual is nothing but difference between this\nnothing but difference between this\nnothing but difference between this point to this point this point to this\npoint to this point this point to this\npoint to this point this point to this point this point to this point this\npoint this point to this point this\npoint this point to this point this point to this point and I doing the all\npoint to this point and I doing the all\npoint to this point and I doing the all the summation of those now the next\nthe summation of those now the next\nthe summation of those now the next point which is very much important here\npoint which is very much important here\npoint which is very much important here is my X and Y what is this y IUS y y bar\nis my X and Y what is this y IUS y y bar\nis my X and Y what is this y IUS y y bar Y Bar is nothing but mean mean of Y if I\nY Bar is nothing but mean mean of Y if I\nY Bar is nothing but mean mean of Y if I calculate the mean of Y then I will\ncalculate the mean of Y then I will\ncalculate the mean of Y then I will probably get a line which looks like\nprobably get a line which looks like\nprobably get a line which looks like this I'll get a line something like this\nthis I'll get a line something like this\nthis I'll get a line something like this and then I will probably try to\nand then I will probably try to\nand then I will probably try to calculate the distance between each and\ncalculate the distance between each and\ncalculate the distance between each and every point and this specific point with\nevery point and this specific point with\nevery point and this specific point with respect to the distance between this\nrespect to the distance between this\nrespect to the distance between this point and this point the denominator\npoint and this point the denominator\npoint and this point the denominator will definitely be high right this value\nwill definitely be high right this value\nwill definitely be high right this value obviously this value will be higher than\nobviously this value will be higher than\nobviously this value will be higher than this value right the reason why it will\nthis value right the reason why it will\nthis value right the reason why it will be higher because the mean of this\nbe higher because the mean of this\nbe higher because the mean of this particular value distance will obviously\nparticular value distance will obviously\nparticular value distance will obviously be higher so this 1 minus high this will\nbe higher so this 1 minus high this will\nbe higher so this 1 minus high this will be a low value and this will be a high\nbe a low value and this will be a high\nbe a low value and this will be a high value when I try to divide Low by\nvalue when I try to divide Low by\nvalue when I try to divide Low by High Low by high then obviously this\nHigh Low by high then obviously this\nHigh Low by high then obviously this entire number will become a small number\nentire number will become a small number\nentire number will become a small number when this is a small number 1 minus\nwhen this is a small number 1 minus\nwhen this is a small number 1 minus small number will be a big number so\nsmall number will be a big number so\nsmall number will be a big number so this basically shows that our R square\nthis basically shows that our R square\nthis basically shows that our R square has fitted properly right it has\nhas fitted properly right it has\nhas fitted properly right it has basically got a very good R square now\nbasically got a very good R square now\nbasically got a very good R square now tell me can I get this entire R square a\ntell me can I get this entire R square a\ntell me can I get this entire R square a negative number let's say that in this\nnegative number let's say that in this\nnegative number let's say that in this particular case I got 90% can I get this\nparticular case I got 90% can I get this\nparticular case I got 90% can I get this R square as negative number there will\nR square as negative number there will\nR square as negative number there will be situation guys what if I create a\nbe situation guys what if I create a\nbe situation guys what if I create a best fit line which looks like\nbest fit line which looks like\nbest fit line which looks like this if I create this best fit line\nthis if I create this best fit line\nthis if I create this best fit line which looks like this then this value\nwhich looks like this then this value\nwhich looks like this then this value will be quite High it is only possible\nwill be quite High it is only possible\nwill be quite High it is only possible when this value will be higher\nwhen this value will be higher\nwhen this value will be higher than higher than this\nthan higher than this\nthan higher than this value okay but in the usual scenario it\nvalue okay but in the usual scenario it\nvalue okay but in the usual scenario it will not happen because obviously we'll\nwill not happen because obviously we'll\nwill not happen because obviously we'll try to fit a line which will be at least\ntry to fit a line which will be at least\ntry to fit a line which will be at least good it's not just like pulling one line\ngood it's not just like pulling one line\ngood it's not just like pulling one line somewhere we don't want to create a best\nsomewhere we don't want to create a best\nsomewhere we don't want to create a best fit line which is worse than this right\nfit line which is worse than this right\nfit line which is worse than this right worse than this so in this particular\nworse than this so in this particular\nworse than this so in this particular scenario you'll be saying that in R\nscenario you'll be saying that in R\nscenario you'll be saying that in R square now here you'll be able to see\nsquare now here you'll be able to see\nsquare now here you'll be able to see one one amazing feature about R square\none one amazing feature about R square\none one amazing feature about R square is that let's say let's say one scenario\nis that let's say let's say one scenario\nis that let's say let's say one scenario suppose I have features like let's say\nsuppose I have features like let's say\nsuppose I have features like let's say that my feature is something like uh\nthat my feature is something like uh\nthat my feature is something like uh let's say I have a price of a house okay\nlet's say I have a price of a house okay\nlet's say I have a price of a house okay so suppose this is my bedrooms how many\nso suppose this is my bedrooms how many\nso suppose this is my bedrooms how many bedrooms I have and this is basically\nbedrooms I have and this is basically\nbedrooms I have and this is basically the price of the house now if I if I\nthe price of the house now if I if I\nthe price of the house now if I if I probably solve this Pro problem I'll\nprobably solve this Pro problem I'll\nprobably solve this Pro problem I'll definitely get an R square value let's\ndefinitely get an R square value let's\ndefinitely get an R square value let's say the R square value is 85% let's say\nsay the R square value is 85% let's say\nsay the R square value is 85% let's say that my R square is 85% now what if if I\nthat my R square is 85% now what if if I\nthat my R square is 85% now what if if I add one more feature the one more\nadd one more feature the one more\nadd one more feature the one more feature basically says that okay if I\nfeature basically says that okay if I\nfeature basically says that okay if I add\nadd\nadd location location of the house will be\nlocation location of the house will be\nlocation location of the house will be definitely correlated with price so\ndefinitely correlated with price so\ndefinitely correlated with price so there is a definite chance that the R\nthere is a definite chance that the R\nthere is a definite chance that the R square value will increase let's say\nsquare value will increase let's say\nsquare value will increase let's say that R square will become 90% if I\nthat R square will become 90% if I\nthat R square will become 90% if I probably have this two specific feature\nprobably have this two specific feature\nprobably have this two specific feature and obviously it is basically increasing\nand obviously it is basically increasing\nand obviously it is basically increasing the R square because this is also\nthe R square because this is also\nthe R square because this is also correlated to price\ncorrelated to price\ncorrelated to price and let me change the example see first\nand let me change the example see first\nand let me change the example see first case I got by R square as 85% let's say\ncase I got by R square as 85% let's say\ncase I got by R square as 85% let's say now as soon as I added location I got\nnow as soon as I added location I got\nnow as soon as I added location I got 90% now let's say that I added one more\n90% now let's say that I added one more\n90% now let's say that I added one more feature which gender is going to stay\nfeature which gender is going to stay\nfeature which gender is going to stay gender like male or female is going to\ngender like male or female is going to\ngender like male or female is going to stay you know that gender is no way\nstay you know that gender is no way\nstay you know that gender is no way correlated to price but even though I\ncorrelated to price but even though I\ncorrelated to price but even though I add one feature there is a scenario that\nadd one feature there is a scenario that\nadd one feature there is a scenario that my R square will still increase and it\nmy R square will still increase and it\nmy R square will still increase and it may become\nmay become\nmay become 91% even though my feature is not that\n91% even though my feature is not that\n91% even though my feature is not that important even gender is not that\nimportant even gender is not that\nimportant even gender is not that important the R square formula Works in\nimportant the R square formula Works in\nimportant the R square formula Works in such a way that if I keep on adding\nsuch a way that if I keep on adding\nsuch a way that if I keep on adding features and that are not nowhere\nfeatures and that are not nowhere\nfeatures and that are not nowhere correlated this is obviously nowhere\ncorrelated this is obviously nowhere\ncorrelated this is obviously nowhere correlated this is not correlated with\ncorrelated this is not correlated with\ncorrelated this is not correlated with price then also what it does is that it\nprice then also what it does is that it\nprice then also what it does is that it is basically increasing my rÂ² so this\nis basically increasing my rÂ² so this\nis basically increasing my rÂ² so this specific thing should not happen whether\nspecific thing should not happen whether\nspecific thing should not happen whether a male will stay or female will stay\na male will stay or female will stay\na male will stay or female will stay that does not matter at all still when\nthat does not matter at all still when\nthat does not matter at all still when you do the calculation the R square will\nyou do the calculation the R square will\nyou do the calculation the R square will still increase so in order to not impact\nstill increase so in order to not impact\nstill increase so in order to not impact the model because see now right now with\nthe model because see now right now with\nthe model because see now right now with this particular model where I have got\nthis particular model where I have got\nthis particular model where I have got 90% now as soon as I see R square as 91%\n90% now as soon as I see R square as 91%\n90% now as soon as I see R square as 91% because it is considering this\nbecause it is considering this\nbecause it is considering this particular gender so this model will be\nparticular gender so this model will be\nparticular gender so this model will be picked right because it is performing\npicked right because it is performing\npicked right because it is performing well and is giving you a better R square\nwell and is giving you a better R square\nwell and is giving you a better R square value but this should not happen because\nvalue but this should not happen because\nvalue but this should not happen because that is not at all corelated this model\nthat is not at all corelated this model\nthat is not at all corelated this model should have been picked so in order to\nshould have been picked so in order to\nshould have been picked so in order to prevent this situation what we do we\nprevent this situation what we do we\nprevent this situation what we do we basically Ally use something called as\nbasically Ally use something called as\nbasically Ally use something called as adjusted R square now what is this\nadjusted R square now what is this\nadjusted R square now what is this adjusted R square and how it will work\nadjusted R square and how it will work\nadjusted R square and how it will work I'll also show it to you very very nice\nI'll also show it to you very very nice\nI'll also show it to you very very nice concept of adjusted R square so adjusted\nconcept of adjusted R square so adjusted\nconcept of adjusted R square so adjusted R square R square\nR square R square\nR square R square adjusted is given by the\nadjusted is given by the\nadjusted is given by the formula is given by the Formula 1 - 1 -\nformula is given by the Formula 1 - 1 -\nformula is given by the Formula 1 - 1 - rÂ² * N - 1 where n is the total number\nrÂ² * N - 1 where n is the total number\nrÂ² * N - 1 where n is the total number of samples n minus P minus 1 this p p is\nof samples n minus P minus 1 this p p is\nof samples n minus P minus 1 this p p is nothing but number of features\nnothing but number of features\nnothing but number of features or predictors we'll also say or\nor predictors we'll also say or\nor predictors we'll also say or predictors suppose initially my number\npredictors suppose initially my number\npredictors suppose initially my number of predictors were in this particular\nof predictors were in this particular\nof predictors were in this particular scenario in this scenario where I saw\nscenario in this scenario where I saw\nscenario in this scenario where I saw this my number of predictors was two and\nthis my number of predictors was two and\nthis my number of predictors was two and in this particular case my number of\nin this particular case my number of\nin this particular case my number of predictor was three now if my predictor\npredictor was three now if my predictor\npredictor was three now if my predictor is 2 I got the r squ as 90% so in this\nis 2 I got the r squ as 90% so in this\nis 2 I got the r squ as 90% so in this particular scenario what all the\nparticular scenario what all the\nparticular scenario what all the calculation will happen okay all the\ncalculation will happen okay all the\ncalculation will happen okay all the calculation will happen and let's say\ncalculation will happen and let's say\ncalculation will happen and let's say that my R square adjusted it'll be\nthat my R square adjusted it'll be\nthat my R square adjusted it'll be little bit less it'll be little bit less\nlittle bit less it'll be little bit less\nlittle bit less it'll be little bit less let's say it8 is 6% let's say that my R\nlet's say it8 is 6% let's say that my R\nlet's say it8 is 6% let's say that my R square adjusted is 86% based on this\nsquare adjusted is 86% based on this\nsquare adjusted is 86% based on this predictor 2 now when I use my predictor\npredictor 2 now when I use my predictor\npredictor 2 now when I use my predictor 3 predictor basically means number of\n3 predictor basically means number of\n3 predictor basically means number of features that I'm going to use and now\nfeatures that I'm going to use and now\nfeatures that I'm going to use and now in this one one feature is nowhere\nin this one one feature is nowhere\nin this one one feature is nowhere related like gender but what we are\nrelated like gender but what we are\nrelated like gender but what we are getting we are basically getting R\ngetting we are basically getting R\ngetting we are basically getting R square increased to\nsquare increased to\nsquare increased to 91% now for the R square\n91% now for the R square\n91% now for the R square adjusted this will not increase this\nadjusted this will not increase this\nadjusted this will not increase this will in turn decrease right now it will\nwill in turn decrease right now it will\nwill in turn decrease right now it will become 82% how it will become I'll show\nbecome 82% how it will become I'll show\nbecome 82% how it will become I'll show you I've just considered some value 8682\nyou I've just considered some value 8682\nyou I've just considered some value 8682 here you can see that there is an\nhere you can see that there is an\nhere you can see that there is an increase here an increase is there here\nincrease here an increase is there here\nincrease here an increase is there here decrease is there now how this is\ndecrease is there now how this is\ndecrease is there now how this is basically happening see this P value\nbasically happening see this P value\nbasically happening see this P value that I will be putting okay if I put a p\nthat I will be putting okay if I put a p\nthat I will be putting okay if I put a p isal 3 obviously with n minus P minus 1\nisal 3 obviously with n minus P minus 1\nisal 3 obviously with n minus P minus 1 this will become a little bit smaller\nthis will become a little bit smaller\nthis will become a little bit smaller number or sorry little bit uh smaller\nnumber or sorry little bit uh smaller\nnumber or sorry little bit uh smaller number right so now in this particular\nnumber right so now in this particular\nnumber right so now in this particular case if it is not correlated obviously\ncase if it is not correlated obviously\ncase if it is not correlated obviously this will be high when I'm increasing\nthis will be high when I'm increasing\nthis will be high when I'm increasing this so this will also be high let me\nthis so this will also be high let me\nthis so this will also be high let me write the equation something like this\nwrite the equation something like this\nwrite the equation something like this just a second so this will basically\nbe okay now why probably this value may\nbe okay now why probably this value may have decreased let me talk about this\nhave decreased let me talk about this\nhave decreased let me talk about this one what is r squ I hope everybody\none what is r squ I hope everybody\none what is r squ I hope everybody understood n is the number of data\nunderstood n is the number of data\nunderstood n is the number of data points p is the number of\npoints p is the number of\npoints p is the number of predictors if p is increasing then what\npredictors if p is increasing then what\npredictors if p is increasing then what will happen as P keeps on increasing\nwill happen as P keeps on increasing\nwill happen as P keeps on increasing this value will keep on\nthis value will keep on\nthis value will keep on decreasing this value will keep on\ndecreasing this value will keep on\ndecreasing this value will keep on decreasing if this values keep on\ndecreasing if this values keep on\ndecreasing if this values keep on decreasing this will be a bigger number\ndecreasing this will be a bigger number\ndecreasing this will be a bigger number this will obviously be a big number a\nthis will obviously be a big number a\nthis will obviously be a big number a big number divided by a small number\nbig number divided by a small number\nbig number divided by a small number what it will be obviously this will be a\nwhat it will be obviously this will be a\nwhat it will be obviously this will be a little bit bigger number 1 minus bigger\nlittle bit bigger number 1 minus bigger\nlittle bit bigger number 1 minus bigger number we will basically get some values\nnumber we will basically get some values\nnumber we will basically get some values which will be decreasing if my P value\nwhich will be decreasing if my P value\nwhich will be decreasing if my P value is two in this particular case it will\nis two in this particular case it will\nis two in this particular case it will be less smaller than this right at least\nbe less smaller than this right at least\nbe less smaller than this right at least it will be greater than this this\nit will be greater than this this\nit will be greater than this this particular value right when p is equal\nparticular value right when p is equal\nparticular value right when p is equal to\nto\nto 3 so with the help of P obviously R\n3 so with the help of P obviously R\n3 so with the help of P obviously R square is there to support you okay\nsquare is there to support you okay\nsquare is there to support you okay whether it is correlated or not always\nwhether it is correlated or not always\nwhether it is correlated or not always remember when the features are highly\nremember when the features are highly\nremember when the features are highly correlated your R square value will\ncorrelated your R square value will\ncorrelated your R square value will increase tremendously if it is less\nincrease tremendously if it is less\nincrease tremendously if it is less correlated then it will be there will be\ncorrelated then it will be there will be\ncorrelated then it will be there will be a small increase but there will not be a\na small increase but there will not be a\na small increase but there will not be a very huge increase now if I consider p\nvery huge increase now if I consider p\nvery huge increase now if I consider p is equal to 2 obviously when I'm trying\nis equal to 2 obviously when I'm trying\nis equal to 2 obviously when I'm trying to find out this uh calculation n minus\nto find out this uh calculation n minus\nto find out this uh calculation n minus P minus 1 it will obviously be greater\nP minus 1 it will obviously be greater\nP minus 1 it will obviously be greater than p is equal to 3 when p is equal to\nthan p is equal to 3 when p is equal to\nthan p is equal to 3 when p is equal to 3 then this value will be still more\n3 then this value will be still more\n3 then this value will be still more smaller and when we are dividing a\nsmaller and when we are dividing a\nsmaller and when we are dividing a bigger number by a smaller number\nbigger number by a smaller number\nbigger number by a smaller number obviously we are subtracting with one so\nobviously we are subtracting with one so\nobviously we are subtracting with one so that basically means even though my R\nthat basically means even though my R\nthat basically means even though my R square is 86 over here there may be a\nsquare is 86 over here there may be a\nsquare is 86 over here there may be a scenario since this is nowhere\nscenario since this is nowhere\nscenario since this is nowhere correlated I'm basically getting an 82%\ncorrelated I'm basically getting an 82%\ncorrelated I'm basically getting an 82% because of this entire equation so I\nbecause of this entire equation so I\nbecause of this entire equation so I hope you are understanding this this is\nhope you are understanding this this is\nhope you are understanding this this is very much important to understand a very\nvery much important to understand a very\nvery much important to understand a very very important property simple way to\nvery important property simple way to\nvery important property simple way to define is that as my P value keeps on\ndefine is that as my P value keeps on\ndefine is that as my P value keeps on increasing the number of predictors\nincreasing the number of predictors\nincreasing the number of predictors keeps on increasing my R squ gets\nkeeps on increasing my R squ gets\nkeeps on increasing my R squ gets adjusted whatever R square I'm getting\nadjusted whatever R square I'm getting\nadjusted whatever R square I'm getting with respect to this it will always be\nwith respect to this it will always be\nwith respect to this it will always be less than this particular R square there\nless than this particular R square there\nless than this particular R square there was one interview question that was\nwas one interview question that was\nwas one interview question that was asked one of my student between R square\nasked one of my student between R square\nasked one of my student between R square and adjusted R square which will always\nand adjusted R square which will always\nand adjusted R square which will always be bigger definitely the student said R\nbe bigger definitely the student said R\nbe bigger definitely the student said R square then he told him to explain about\nsquare then he told him to explain about\nsquare then he told him to explain about adjusted R square why does that specific\nadjusted R square why does that specific\nadjusted R square why does that specific happen agenda one is about Ridge lasso\nhappen agenda one is about Ridge lasso\nhappen agenda one is about Ridge lasso regression second is assumptions of\nregression second is assumptions of\nregression second is assumptions of linear regression the third point that\nlinear regression the third point that\nlinear regression the third point that we are probably going to discuss about\nwe are probably going to discuss about\nwe are probably going to discuss about is logistic regression then the fourth\nis logistic regression then the fourth\nis logistic regression then the fourth thing that we are going to discuss about\nthing that we are going to discuss about\nthing that we are going to discuss about is something called as confusion\nis something called as confusion\nis something called as confusion Matrix the fifth thing that we are going\nMatrix the fifth thing that we are going\nMatrix the fifth thing that we are going to consider about\nto consider about\nto consider about is practicals\nis practicals\nis practicals for lead lineer Ridge lasso and logistic\nfor lead lineer Ridge lasso and logistic\nfor lead lineer Ridge lasso and logistic so first topic uh that we are probably\nso first topic uh that we are probably\nso first topic uh that we are probably going to discuss is something called as\ngoing to discuss is something called as\ngoing to discuss is something called as Ridge and lasso\nregression so let's understand about\nregression so let's understand about Ridge and lasso regression if you\nRidge and lasso regression if you\nRidge and lasso regression if you remember in our previous session what\nremember in our previous session what\nremember in our previous session what all things we discussed linear\nall things we discussed linear\nall things we discussed linear regression and then we had discussed\nregression and then we had discussed\nregression and then we had discussed about the cost function we have\nabout the cost function we have\nabout the cost function we have discussed about R square adjusted\ndiscussed about R square adjusted\ndiscussed about R square adjusted adjusted R square sorry R square and\nadjusted R square sorry R square and\nadjusted R square sorry R square and adjusted R square we have discussed\nadjusted R square we have discussed\nadjusted R square we have discussed about it gradient descent we have\nabout it gradient descent we have\nabout it gradient descent we have discussed about it it was nothing but 1\ndiscussed about it it was nothing but 1\ndiscussed about it it was nothing but 1 by 2 m summation of I = 1 2 m h Theta of\nby 2 m summation of I = 1 2 m h Theta of\nby 2 m summation of I = 1 2 m h Theta of x i -\nx i -\nx i - y - y i s so this is the cost function\ny - y i s so this is the cost function\ny - y i s so this is the cost function that we had discussed right yesterday\nthat we had discussed right yesterday\nthat we had discussed right yesterday and this cost function was able to give\nand this cost function was able to give\nand this cost function was able to give us a\nus a\nus a gradient descent with respect to the J\ngradient descent with respect to the J\ngradient descent with respect to the J of\nof\nof theta J of theta Zer or Theta not so I\ntheta J of theta Zer or Theta not so I\ntheta J of theta Zer or Theta not so I can also write this as J of theta comma\ncan also write this as J of theta comma\ncan also write this as J of theta comma Theta 0 comma Theta 1 now let me give\nTheta 0 comma Theta 1 now let me give\nTheta 0 comma Theta 1 now let me give you a scenario let's say that I have a\nyou a scenario let's say that I have a\nyou a scenario let's say that I have a scenario over here and I have this\nscenario over here and I have this\nscenario over here and I have this specific scenario let's say that I just\nspecific scenario let's say that I just\nspecific scenario let's say that I just have two points which looks like this\nhave two points which looks like this\nhave two points which looks like this okay now if I have these two specific\nokay now if I have these two specific\nokay now if I have these two specific points what will happen I will probably\npoints what will happen I will probably\npoints what will happen I will probably try to create a best fit line the best\ntry to create a best fit line the best\ntry to create a best fit line the best fit line will definitely pass through\nfit line will definitely pass through\nfit line will definitely pass through all the points like this if I try to\nall the points like this if I try to\nall the points like this if I try to calculate the cost function what will be\ncalculate the cost function what will be\ncalculate the cost function what will be the value of J of theta 0 comma Theta 1\nthe value of J of theta 0 comma Theta 1\nthe value of J of theta 0 comma Theta 1 let's say that in this particular case\nlet's say that in this particular case\nlet's say that in this particular case since it is passing through the origin\nsince it is passing through the origin\nsince it is passing through the origin my Theta 0 will be zero okay so what\nmy Theta 0 will be zero okay so what\nmy Theta 0 will be zero okay so what will be the value of theta 0 comma Theta\nwill be the value of theta 0 comma Theta\nwill be the value of theta 0 comma Theta 1 so here obviously you can see that\n1 so here obviously you can see that\n1 so here obviously you can see that there is no difference so it will\nthere is no difference so it will\nthere is no difference so it will obviously become zero Now understand\nobviously become zero Now understand\nobviously become zero Now understand this data that you see right right this\nthis data that you see right right this\nthis data that you see right right this data is basically called as training\ndata is basically called as training\ndata is basically called as training data so this data that I have actually\ndata so this data that I have actually\ndata so this data that I have actually plotted with two points these are\nplotted with two points these are\nplotted with two points these are specifically called as training\nspecifically called as training\nspecifically called as training data now what is the problem in this\ndata now what is the problem in this\ndata now what is the problem in this data right now see right now exactly\ndata right now see right now exactly\ndata right now see right now exactly whatever line is basically getting\nwhatever line is basically getting\nwhatever line is basically getting created over here which is through the\ncreated over here which is through the\ncreated over here which is through the uh hypothesis over here you can see that\nuh hypothesis over here you can see that\nuh hypothesis over here you can see that it is passing through every point so\nit is passing through every point so\nit is passing through every point so that is the reason your cost is zero and\nthat is the reason your cost is zero and\nthat is the reason your cost is zero and our main aim is to basically minimize\nour main aim is to basically minimize\nour main aim is to basically minimize the cost function that is absolutely\nthe cost function that is absolutely\nthe cost function that is absolutely fine now in this particular case in\nfine now in this particular case in\nfine now in this particular case in which my model this if this model is\nwhich my model this if this model is\nwhich my model this if this model is getting trained initially this data is\ngetting trained initially this data is\ngetting trained initially this data is basically called as training data now\nbasically called as training data now\nbasically called as training data now just imagine that tomorrow new data\njust imagine that tomorrow new data\njust imagine that tomorrow new data points comes so if my new data points\npoints comes so if my new data points\npoints comes so if my new data points are here let's consider that I I want to\nare here let's consider that I I want to\nare here let's consider that I I want to basically uh come up with this new data\nbasically uh come up with this new data\nbasically uh come up with this new data point now in this particular scenario if\npoint now in this particular scenario if\npoint now in this particular scenario if I want to predict with respect to this\nI want to predict with respect to this\nI want to predict with respect to this particular Point let's say my predicted\nparticular Point let's say my predicted\nparticular Point let's say my predicted point is here\npoint is here\npoint is here is this the difference between the\nis this the difference between the\nis this the difference between the predicted and the real Point quite\npredicted and the real Point quite\npredicted and the real Point quite huge yes or no so this is basically\nhuge yes or no so this is basically\nhuge yes or no so this is basically creating a condition which is called as\ncreating a condition which is called as\ncreating a condition which is called as overfitting that basically means even\noverfitting that basically means even\noverfitting that basically means even though my\nthough my\nthough my model has given or trained well with the\nmodel has given or trained well with the\nmodel has given or trained well with the training\ntraining\ntraining data or let me write it down properly\ndata or let me write it down properly\ndata or let me write it down properly over here so this condition since since\nover here so this condition since since\nover here so this condition since since you can see that over here my each and\nyou can see that over here my each and\nyou can see that over here my each and every point is basically passing through\nevery point is basically passing through\nevery point is basically passing through the best fit line so because of that\nthe best fit line so because of that\nthe best fit line so because of that what happens it causes something called\nwhat happens it causes something called\nwhat happens it causes something called as\nas\nas overfitting so you really need to\noverfitting so you really need to\noverfitting so you really need to understand what is overfitting now what\nunderstand what is overfitting now what\nunderstand what is overfitting now what does overfitting mean overfitting\ndoes overfitting mean overfitting\ndoes overfitting mean overfitting basically means my model performs well\nbasically means my model performs well\nbasically means my model performs well with training data but it fails to\nwith training data but it fails to\nwith training data but it fails to perform well with test data now what is\nperform well with test data now what is\nperform well with test data now what is the test data over here the test data is\nthe test data over here the test data is\nthe test data over here the test data is basically this points the real test data\nbasically this points the real test data\nbasically this points the real test data answer was this points but because the\nanswer was this points but because the\nanswer was this points but because the my line is like this I'm actually\nmy line is like this I'm actually\nmy line is like this I'm actually getting the predicted point over here so\ngetting the predicted point over here so\ngetting the predicted point over here so this distance if I try to calculate it\nthis distance if I try to calculate it\nthis distance if I try to calculate it is quite huge so in this scenario\nis quite huge so in this scenario\nis quite huge so in this scenario whenever I say my model performs well\nwhenever I say my model performs well\nwhenever I say my model performs well with training data and it fails to\nwith training data and it fails to\nwith training data and it fails to perform well with test data then this\nperform well with test data then this\nperform well with test data then this scenario we say it as overfitting so\nscenario we say it as overfitting so\nscenario we say it as overfitting so this scenario when the model performs\nthis scenario when the model performs\nthis scenario when the model performs well with training data I have a\nwell with training data I have a\nwell with training data I have a condition which is called as low bias\ncondition which is called as low bias\ncondition which is called as low bias and when it fails to perform with the\nand when it fails to perform with the\nand when it fails to perform with the test data then it is basically called as\ntest data then it is basically called as\ntest data then it is basically called as high High variance very important okay I\nhigh High variance very important okay I\nhigh High variance very important okay I will make each and everyone understand\nwill make each and everyone understand\nwill make each and everyone understand one by one if it is performing well with\none by one if it is performing well with\none by one if it is performing well with the training data that is basically low\nthe training data that is basically low\nthe training data that is basically low bias and whenever it performs well with\nbias and whenever it performs well with\nbias and whenever it performs well with the test sorry fails to perform well\nthe test sorry fails to perform well\nthe test sorry fails to perform well with the fails to perform well with the\nwith the fails to perform well with the\nwith the fails to perform well with the test data then it is basically High\ntest data then it is basically High\ntest data then it is basically High variance now similarly I may have\nvariance now similarly I may have\nvariance now similarly I may have another scenario which is called as\nanother scenario which is called as\nanother scenario which is called as underfitting so let's say that I have\nunderfitting so let's say that I have\nunderfitting so let's say that I have something called as\nsomething called as\nsomething called as underfitting now in this underfitting\nunderfitting now in this underfitting\nunderfitting now in this underfitting what is the scenario the\nwhat is the scenario the\nwhat is the scenario the model fails to perform it gives bad\nmodel fails to perform it gives bad\nmodel fails to perform it gives bad accuracy I say that model always\naccuracy I say that model always\naccuracy I say that model always remember whenever I talk about bias then\nremember whenever I talk about bias then\nremember whenever I talk about bias then you can understand that it is something\nyou can understand that it is something\nyou can understand that it is something related to the training data whenever I\nrelated to the training data whenever I\nrelated to the training data whenever I talk about test data at that point of\ntalk about test data at that point of\ntalk about test data at that point of time you talk about variance and that\ntime you talk about variance and that\ntime you talk about variance and that specifically whenever you talk about\nspecifically whenever you talk about\nspecifically whenever you talk about variance that basically means we are\nvariance that basically means we are\nvariance that basically means we are talking about the test data so for an\ntalking about the test data so for an\ntalking about the test data so for an overfitting you will basically have low\noverfitting you will basically have low\noverfitting you will basically have low bias and high variance low bias with\nbias and high variance low bias with\nbias and high variance low bias with respect to the training data and high\nrespect to the training data and high\nrespect to the training data and high variance with respect to the test data\nvariance with respect to the test data\nvariance with respect to the test data now if the model accuracy is bad with\nnow if the model accuracy is bad with\nnow if the model accuracy is bad with training data and the model accuracy is\ntraining data and the model accuracy is\ntraining data and the model accuracy is also bad with test data in this scenario\nalso bad with test data in this scenario\nalso bad with test data in this scenario we basically say it as underfitting so\nwe basically say it as underfitting so\nwe basically say it as underfitting so these are the two conditions that are\nthese are the two conditions that are\nthese are the two conditions that are with respect to underfitting that\nwith respect to underfitting that\nwith respect to underfitting that basically means that both for the\nbasically means that both for the\nbasically means that both for the training data also the model is giving\ntraining data also the model is giving\ntraining data also the model is giving bad accuracy and again for the test data\nbad accuracy and again for the test data\nbad accuracy and again for the test data also it is basically having a bad\nalso it is basically having a bad\nalso it is basically having a bad accuracy so in this particular scenario\naccuracy so in this particular scenario\naccuracy so in this particular scenario we can definitely say two things out of\nwe can definitely say two things out of\nwe can definitely say two things out of underfitting one is high bias and high\nunderfitting one is high bias and high\nunderfitting one is high bias and high variance so this is the condition with\nvariance so this is the condition with\nvariance so this is the condition with respect to underfitting very super\nrespect to underfitting very super\nrespect to underfitting very super important let me just explain you once\nimportant let me just explain you once\nimportant let me just explain you once again suppose let's consider I have one\nagain suppose let's consider I have one\nagain suppose let's consider I have one model I have model two this is model one\nmodel I have model two this is model one\nmodel I have model two this is model one this is model one this is model two and\nthis is model one this is model two and\nthis is model one this is model two and this is model 3 okay guys so suppose\nthis is model 3 okay guys so suppose\nthis is model 3 okay guys so suppose let's say that I have my model my\nlet's say that I have my model my\nlet's say that I have my model my training accuracy is let's say\ntraining accuracy is let's say\ntraining accuracy is let's say 90% And my let's say that my test\n90% And my let's say that my test\n90% And my let's say that my test accuracy is 80% now in this particular\naccuracy is 80% now in this particular\naccuracy is 80% now in this particular case let's say that my training accuracy\ncase let's say that my training accuracy\ncase let's say that my training accuracy is\nis\nis 92% and my test accuracy is 91% and\n92% and my test accuracy is 91% and\n92% and my test accuracy is 91% and let's say my model three is basically\nlet's say my model three is basically\nlet's say my model three is basically having training accuracy as\nhaving training accuracy as\nhaving training accuracy as 70% and my test accuracy is 65% so if I\n70% and my test accuracy is 65% so if I\n70% and my test accuracy is 65% so if I take this particular case it is\ntake this particular case it is\ntake this particular case it is basically overfitting if I take this\nbasically overfitting if I take this\nbasically overfitting if I take this particular thing this basically becomes\nparticular thing this basically becomes\nparticular thing this basically becomes my generalized model and when I talk\nmy generalized model and when I talk\nmy generalized model and when I talk about this this is my I'll just say that\nabout this this is my I'll just say that\nabout this this is my I'll just say that okay I'll also put nice color so that uh\nokay I'll also put nice color so that uh\nokay I'll also put nice color so that uh you'll be able to understand this this\nyou'll be able to understand this this\nyou'll be able to understand this this becomes our generalized model and this\nbecomes our generalized model and this\nbecomes our generalized model and this finally becomes our underfitting right\nfinally becomes our underfitting right\nfinally becomes our underfitting right under under fitting so here is my red\nunder under fitting so here is my red\nunder under fitting so here is my red color I will just say it as underfitting\ncolor I will just say it as underfitting\ncolor I will just say it as underfitting what are the main properties of this\nwhat are the main properties of this\nwhat are the main properties of this overfitting as I said in this scenario\noverfitting as I said in this scenario\noverfitting as I said in this scenario since it is performing well with the\nsince it is performing well with the\nsince it is performing well with the training data so it will be low bias\ntraining data so it will be low bias\ntraining data so it will be low bias High variance in this particular case it\nHigh variance in this particular case it\nHigh variance in this particular case it will be low bias low variance and this\nwill be low bias low variance and this\nwill be low bias low variance and this particular case it will be high bias and\nparticular case it will be high bias and\nparticular case it will be high bias and high variance understand in this\nhigh variance understand in this\nhigh variance understand in this terminology in this particular way\nterminology in this particular way\nterminology in this particular way you'll be able to understand so why do\nyou'll be able to understand so why do\nyou'll be able to understand so why do we require always a generalized model\nwe require always a generalized model\nwe require always a generalized model because whenever our new data will\nbecause whenever our new data will\nbecause whenever our new data will definitely come generalized model will\ndefinitely come generalized model will\ndefinitely come generalized model will be able to give us very good output\nbe able to give us very good output\nbe able to give us very good output let's go back to this particular example\nlet's go back to this particular example\nlet's go back to this particular example here you'll be able to see this straight\nhere you'll be able to see this straight\nhere you'll be able to see this straight line the red line that I have actually\nline the red line that I have actually\nline the red line that I have actually created is basically overfitting so that\ncreated is basically overfitting so that\ncreated is basically overfitting so that whenever I probably get the new points\nwhenever I probably get the new points\nwhenever I probably get the new points which is having this real value and the\nwhich is having this real value and the\nwhich is having this real value and the predicted points here you'll be able to\npredicted points here you'll be able to\npredicted points here you'll be able to see the difference is quite huge so\nsee the difference is quite huge so\nsee the difference is quite huge so because of this it will definitely be a\nbecause of this it will definitely be a\nbecause of this it will definitely be a scenario of overfitting where it has low\nscenario of overfitting where it has low\nscenario of overfitting where it has low bias and high weight\nbias and high weight\nbias and high weight so again let me go ahead and take this\nso again let me go ahead and take this\nso again let me go ahead and take this example so this was my line which I have\nexample so this was my line which I have\nexample so this was my line which I have actually drawn I had two points and when\nactually drawn I had two points and when\nactually drawn I had two points and when I draw this line which was a best fit\nI draw this line which was a best fit\nI draw this line which was a best fit line to which is passing through both\nline to which is passing through both\nline to which is passing through both the points this scenario is basically\nthe points this scenario is basically\nthe points this scenario is basically causing a overfitting problem and I've\ncausing a overfitting problem and I've\ncausing a overfitting problem and I've also shown you my J of theta 1 will be\nalso shown you my J of theta 1 will be\nalso shown you my J of theta 1 will be zero in this scenario since it is\nzero in this scenario since it is\nzero in this scenario since it is passing exactly and the predicted point\npassing exactly and the predicted point\npassing exactly and the predicted point is also over there now understand one\nis also over there now understand one\nis also over there now understand one thing is that what can can we take out\nthing is that what can can we take out\nthing is that what can can we take out from this what assumptions we can take\nfrom this what assumptions we can take\nfrom this what assumptions we can take out from this definitely if I talk about\nout from this definitely if I talk about\nout from this definitely if I talk about our cost function our cost function here\nour cost function our cost function here\nour cost function our cost function here is nothing but 1X 2 m summation of I = 1\nis nothing but 1X 2 m summation of I = 1\nis nothing but 1X 2 m summation of I = 1 2 m h Theta of X of i - y of I whole s\n2 m h Theta of X of i - y of I whole s\n2 m h Theta of X of i - y of I whole s now let's consider that I am going to\nnow let's consider that I am going to\nnow let's consider that I am going to use this H Theta X and I'm going to\nuse this H Theta X and I'm going to\nuse this H Theta X and I'm going to basically write it as y hat okay let's\nbasically write it as y hat okay let's\nbasically write it as y hat okay let's focus on this specific point so when I\nfocus on this specific point so when I\nfocus on this specific point so when I take this I'm I'm just going to focus on\ntake this I'm I'm just going to focus on\ntake this I'm I'm just going to focus on this particular point so here I will\nthis particular point so here I will\nthis particular point so here I will definitely write it as y hat minus y of\ndefinitely write it as y hat minus y of\ndefinitely write it as y hat minus y of I whole squ so this is my y y hat of I\nI whole squ so this is my y y hat of I\nI whole squ so this is my y y hat of I minus y hat y i whole Square so this is\nminus y hat y i whole Square so this is\nminus y hat y i whole Square so this is nothing but the difference between the\nnothing but the difference between the\nnothing but the difference between the predicted value and the real value okay\npredicted value and the real value okay\npredicted value and the real value okay this is what I'm actually trying to get\nthis is what I'm actually trying to get\nthis is what I'm actually trying to get now in this scenario if I am adding this\nnow in this scenario if I am adding this\nnow in this scenario if I am adding this values obviously I'm going to get the\nvalues obviously I'm going to get the\nvalues obviously I'm going to get the value as zero now I have to make sure\nvalue as zero now I have to make sure\nvalue as zero now I have to make sure that this value does not come to zero\nthat this value does not come to zero\nthat this value does not come to zero because this is still over fitting so\nbecause this is still over fitting so\nbecause this is still over fitting so that is where your Ridge regression will\nthat is where your Ridge regression will\nthat is where your Ridge regression will come into picture Ridge and lasso will\ncome into picture Ridge and lasso will\ncome into picture Ridge and lasso will come into picture now when I use Ridge\ncome into picture now when I use Ridge\ncome into picture now when I use Ridge and lasso suppose if I use Ridge now in\nand lasso suppose if I use Ridge now in\nand lasso suppose if I use Ridge now in Ridge what we say this this is also\nRidge what we say this this is also\nRidge what we say this this is also called as L2\ncalled as L2\ncalled as L2 regularization now L2 regularization\nregularization now L2 regularization\nregularization now L2 regularization what it does is that it basically adds a\nwhat it does is that it basically adds a\nwhat it does is that it basically adds a unique\nunique\nunique parameter add a One More Sample value\nparameter add a One More Sample value\nparameter add a One More Sample value which is like Lambda multiplied by slope\nwhich is like Lambda multiplied by slope\nwhich is like Lambda multiplied by slope Square now what is this slope whatever\nSquare now what is this slope whatever\nSquare now what is this slope whatever slope of this particular line it is we\nslope of this particular line it is we\nslope of this particular line it is we are just going to square it off now\nare just going to square it off now\nare just going to square it off now suppose if I take my equation which\nsuppose if I take my equation which\nsuppose if I take my equation which looks like this H Theta of X is equal to\nlooks like this H Theta of X is equal to\nlooks like this H Theta of X is equal to Theta 0 + Theta 1 x now in this\nTheta 0 + Theta 1 x now in this\nTheta 0 + Theta 1 x now in this particular case my Theta 0 was zero so\nparticular case my Theta 0 was zero so\nparticular case my Theta 0 was zero so my H Theta of X is nothing but Theta 1\nmy H Theta of X is nothing but Theta 1\nmy H Theta of X is nothing but Theta 1 what is Theta 1 this is specifically\nwhat is Theta 1 this is specifically\nwhat is Theta 1 this is specifically called as slope and I am basically\ncalled as slope and I am basically\ncalled as slope and I am basically taking this Theta 1 I'm actually making\ntaking this Theta 1 I'm actually making\ntaking this Theta 1 I'm actually making it as a square Square so always\nit as a square Square so always\nit as a square Square so always understand I don't want to make this as\nunderstand I don't want to make this as\nunderstand I don't want to make this as zero because if it becomes zero it may\nzero because if it becomes zero it may\nzero because if it becomes zero it may lead to overfitting condition now what\nlead to overfitting condition now what\nlead to overfitting condition now what will happen if I add this particular\nwill happen if I add this particular\nwill happen if I add this particular equation if I add this particular\nequation if I add this particular\nequation if I add this particular equation this will obviously come as\nequation this will obviously come as\nequation this will obviously come as zero let's consider my Lambda value over\nzero let's consider my Lambda value over\nzero let's consider my Lambda value over here my Lambda value is one I'll talk\nhere my Lambda value is one I'll talk\nhere my Lambda value is one I'll talk about how do you set up Lambda value\nabout how do you set up Lambda value\nabout how do you set up Lambda value okay let's consider that I'm\nokay let's consider that I'm\nokay let's consider that I'm initializing it to one let's say my\ninitializing it to one let's say my\ninitializing it to one let's say my Lambda value is 1 now what I will do is\nLambda value is 1 now what I will do is\nLambda value is 1 now what I will do is that this l Lambda value is 1 Let's\nthat this l Lambda value is 1 Let's\nthat this l Lambda value is 1 Let's consider our slope value initially is\nconsider our slope value initially is\nconsider our slope value initially is two and because of this two I got this\ntwo and because of this two I got this\ntwo and because of this two I got this best fit line I'm just going to consider\nbest fit line I'm just going to consider\nbest fit line I'm just going to consider it so if I do the total sum over here if\nit so if I do the total sum over here if\nit so if I do the total sum over here if I'm just considering this this value is\nI'm just considering this this value is\nI'm just considering this this value is three now the cost function will not\nthree now the cost function will not\nthree now the cost function will not stop over here because still it has to\nstop over here because still it has to\nstop over here because still it has to minimize it has to reduce this three\nminimize it has to reduce this three\nminimize it has to reduce this three value so what it will do it will again\nvalue so what it will do it will again\nvalue so what it will do it will again change the Theta 1 value and let's say\nchange the Theta 1 value and let's say\nchange the Theta 1 value and let's say that my Theta van value has changed now\nthat my Theta van value has changed now\nthat my Theta van value has changed now it got another best fit line which looks\nit got another best fit line which looks\nit got another best fit line which looks something like like this this is my next\nsomething like like this this is my next\nsomething like like this this is my next best fit line I'll talk about Lambda\nbest fit line I'll talk about Lambda\nbest fit line I'll talk about Lambda Lambda is a hyper parameter guys what\nLambda is a hyper parameter guys what\nLambda is a hyper parameter guys what exactly is Lambda I'll just talk about\nexactly is Lambda I'll just talk about\nexactly is Lambda I'll just talk about it now when I basically change this line\nit now when I basically change this line\nit now when I basically change this line now see why I'm getting this line let's\nnow see why I'm getting this line let's\nnow see why I'm getting this line let's consider I have changed my Theta 1 value\nconsider I have changed my Theta 1 value\nconsider I have changed my Theta 1 value since we need to minimize now when we\nsince we need to minimize now when we\nsince we need to minimize now when we need to minimize what it will do we'll\nneed to minimize what it will do we'll\nneed to minimize what it will do we'll again calculate the slope of this\nagain calculate the slope of this\nagain calculate the slope of this particular line and then we will try to\nparticular line and then we will try to\nparticular line and then we will try to create a new line when we sorry it is\ncreate a new line when we sorry it is\ncreate a new line when we sorry it is two two not three just a second guys 0 +\ntwo two not three just a second guys 0 +\ntwo two not three just a second guys 0 + 1 multiplied by 2 s which is nothing but\n1 multiplied by 2 s which is nothing but\n1 multiplied by 2 s which is nothing but 4 so now my cost function will not stop\n4 so now my cost function will not stop\n4 so now my cost function will not stop over here so we are going to still\nover here so we are going to still\nover here so we are going to still reduce this now in order to reduce this\nreduce this now in order to reduce this\nreduce this now in order to reduce this again Theta 1 value will get changed and\nagain Theta 1 value will get changed and\nagain Theta 1 value will get changed and then we will get a next best fit line\nthen we will get a next best fit line\nthen we will get a next best fit line for this point now what will happen in\nfor this point now what will happen in\nfor this point now what will happen in this scenario once we have this best fit\nthis scenario once we have this best fit\nthis scenario once we have this best fit line we will definitely get a kind of\nline we will definitely get a kind of\nline we will definitely get a kind of small difference so now if I go ahead\nsmall difference so now if I go ahead\nsmall difference so now if I go ahead and consider the new equation my y hat I\nand consider the new equation my y hat I\nand consider the new equation my y hat I minus y\nminus y\nminus y iÂ² + Lambda of slope squar this value\niÂ² + Lambda of slope squar this value\niÂ² + Lambda of slope squar this value will be a small value now because I have\nwill be a small value now because I have\nwill be a small value now because I have some difference and then plus again 1\nsome difference and then plus again 1\nsome difference and then plus again 1 multiplied by now understand whether the\nmultiplied by now understand whether the\nmultiplied by now understand whether the slope will increase in this particular\nslope will increase in this particular\nslope will increase in this particular case or whether it will decrease in this\ncase or whether it will decrease in this\ncase or whether it will decrease in this particular case there will be some slope\nparticular case there will be some slope\nparticular case there will be some slope value let's say that I have got some\nvalue let's say that I have got some\nvalue let's say that I have got some slope of this particular line in this\nslope of this particular line in this\nslope of this particular line in this particular scenario again your slope\nparticular scenario again your slope\nparticular scenario again your slope will definitely decrease so let's say in\nwill definitely decrease so let's say in\nwill definitely decrease so let's say in the case of two initially it was now it\nthe case of two initially it was now it\nthe case of two initially it was now it is basically\nis basically\nis basically 1.36 whole squ now this small Value\n1.36 whole squ now this small Value\n1.36 whole squ now this small Value Plus 1 + 1.3 squ or let me consider that\nPlus 1 + 1.3 squ or let me consider that\nPlus 1 + 1.3 squ or let me consider that my slope is now one simple value that is\nmy slope is now one simple value that is\nmy slope is now one simple value that is 5 so if I get this it is 2.25 2.25 plus\n5 so if I get this it is 2.25 2.25 plus\n5 so if I get this it is 2.25 2.25 plus small value it will be less than three\nsmall value it will be less than three\nsmall value it will be less than three only right it will obviously be less\nonly right it will obviously be less\nonly right it will obviously be less than three or equal to 3 but understand\nthan three or equal to 3 but understand\nthan three or equal to 3 but understand what is happening the value is getting\nwhat is happening the value is getting\nwhat is happening the value is getting reduced from 4 to 3 so this is is the\nreduced from 4 to 3 so this is is the\nreduced from 4 to 3 so this is is the importance of Ridge now what will happen\nimportance of Ridge now what will happen\nimportance of Ridge now what will happen is that you will try to get a\nis that you will try to get a\nis that you will try to get a generalized model which has low bias and\ngeneralized model which has low bias and\ngeneralized model which has low bias and low variance instead of this overfitting\nlow variance instead of this overfitting\nlow variance instead of this overfitting condition you know why specifically we\ncondition you know why specifically we\ncondition you know why specifically we are adding Ridge L2 regularization it is\nare adding Ridge L2 regularization it is\nare adding Ridge L2 regularization it is basically to prevent\nbasically to prevent\nbasically to prevent overfitting because here you are not\noverfitting because here you are not\noverfitting because here you are not stopping here you are trying to reduce\nstopping here you are trying to reduce\nstopping here you are trying to reduce it unless and until you get a line you\nit unless and until you get a line you\nit unless and until you get a line you get a line which will be able to handle\nget a line which will be able to handle\nget a line which will be able to handle the which will be able to handle as a uh\nthe which will be able to handle as a uh\nthe which will be able to handle as a uh generalized model now here you can see\ngeneralized model now here you can see\ngeneralized model now here you can see now if I have my new points like how I\nnow if I have my new points like how I\nnow if I have my new points like how I drew over here now the distance will be\ndrew over here now the distance will be\ndrew over here now the distance will be less so now you'll be able to see that\nless so now you'll be able to see that\nless so now you'll be able to see that it will be able to create a generalized\nit will be able to create a generalized\nit will be able to create a generalized model guys this will be a small value\nmodel guys this will be a small value\nmodel guys this will be a small value only see initially when we have this\nonly see initially when we have this\nonly see initially when we have this line obviously we have zero if we try to\nline obviously we have zero if we try to\nline obviously we have zero if we try to slightly move here and there so here\nslightly move here and there so here\nslightly move here and there so here you'll be able to see that it will just\nyou'll be able to see that it will just\nyou'll be able to see that it will just a slight movement but what this movement\na slight movement but what this movement\na slight movement but what this movement is basically specifying it is specifying\nis basically specifying it is specifying\nis basically specifying it is specifying that the slope should not be steep if we\nthat the slope should not be steep if we\nthat the slope should not be steep if we probably have a steep slope it obviously\nprobably have a steep slope it obviously\nprobably have a steep slope it obviously leads to most of the time overfitting\nleads to most of the time overfitting\nleads to most of the time overfitting condition it should not be steep it\ncondition it should not be steep it\ncondition it should not be steep it should be very very it should be less\nshould be very very it should be less\nshould be very very it should be less steeper but it should actually help you\nsteeper but it should actually help you\nsteeper but it should actually help you to create a generalized model so you\nto create a generalized model so you\nto create a generalized model so you will be seeing that after playing for\nwill be seeing that after playing for\nwill be seeing that after playing for some amount of time this value will not\nsome amount of time this value will not\nsome amount of time this value will not reduce after some point of time it'll\nreduce after some point of time it'll\nreduce after some point of time it'll get almost it'll be a minimal value\nget almost it'll be a minimal value\nget almost it'll be a minimal value it'll be a smaller value and for this\nit'll be a smaller value and for this\nit'll be a smaller value and for this also you have to specify iterations how\nalso you have to specify iterations how\nalso you have to specify iterations how many times you probably have to train\nmany times you probably have to train\nmany times you probably have to train them now this iterations is also a\nthem now this iterations is also a\nthem now this iterations is also a hyperparameter based on number of\nhyperparameter based on number of\nhyperparameter based on number of iterations you will probably see your R\niterations you will probably see your R\niterations you will probably see your R square or adjusted R square over here so\nsquare or adjusted R square over here so\nsquare or adjusted R square over here so this iterations based on the number of\nthis iterations based on the number of\nthis iterations based on the number of iterations it will never become zero\niterations it will never become zero\niterations it will never become zero guys understand because zero it is not\nguys understand because zero it is not\nguys understand because zero it is not possible if it becomes zero trust me it\npossible if it becomes zero trust me it\npossible if it becomes zero trust me it is an overfitting model you cannot get\nis an overfitting model you cannot get\nis an overfitting model you cannot get that is something zero now what is\nthat is something zero now what is\nthat is something zero now what is Lambda coming to this Lambda this Lambda\nLambda coming to this Lambda this Lambda\nLambda coming to this Lambda this Lambda is a\nis a\nis a hyperparameter this is basically to\nhyperparameter this is basically to\nhyperparameter this is basically to check how fast you want to lessen the\ncheck how fast you want to lessen the\ncheck how fast you want to lessen the steepness or how fast you want to make a\nsteepness or how fast you want to make a\nsteepness or how fast you want to make a steepness grow higher right and this\nsteepness grow higher right and this\nsteepness grow higher right and this Lambda will also be selected by using\nLambda will also be selected by using\nLambda will also be selected by using hyper parameter and this also I'll show\nhyper parameter and this also I'll show\nhyper parameter and this also I'll show you today in Practical what do you mean\nyou today in Practical what do you mean\nyou today in Practical what do you mean by iterations iteration basically means\nby iterations iteration basically means\nby iterations iteration basically means how many time I want to change the Theta\nhow many time I want to change the Theta\nhow many time I want to change the Theta 1 value how many times you want to\n1 value how many times you want to\n1 value how many times you want to change the Theta value that is the\nchange the Theta value that is the\nchange the Theta value that is the convergence algorithm right\nconvergence algorithm right\nconvergence algorithm right convergence algorithm over here L2\nconvergence algorithm over here L2\nconvergence algorithm over here L2 regularization or Ridge is basically\nregularization or Ridge is basically\nregularization or Ridge is basically used in such a way that you should never\nused in such a way that you should never\nused in such a way that you should never overfit why we assume Theta 0 is equal\noverfit why we assume Theta 0 is equal\noverfit why we assume Theta 0 is equal to 0 because I'm considering that it\nto 0 because I'm considering that it\nto 0 because I'm considering that it passes through a origin right origin\npasses through a origin right origin\npasses through a origin right origin over here Lambda is a hyper\nover here Lambda is a hyper\nover here Lambda is a hyper parameter steep basically means how\nparameter steep basically means how\nparameter steep basically means how steep the line is if I have this line\nsteep the line is if I have this line\nsteep the line is if I have this line this line is quite steep if I have this\nthis line is quite steep if I have this\nthis line is quite steep if I have this line This is less steep now if I go to\nline This is less steep now if I go to\nline This is less steep now if I go to the next regularization which is called\nthe next regularization which is called\nthe next regularization which is called as lasso raso R lasso regression this is\nas lasso raso R lasso regression this is\nas lasso raso R lasso regression this is also called as L1\nalso called as L1\nalso called as L1 regularization now here the formula will\nregularization now here the formula will\nregularization now here the formula will be changing little bit here you will be\nbe changing little bit here you will be\nbe changing little bit here you will be having y hat of minus of Y whole Square\nhaving y hat of minus of Y whole Square\nhaving y hat of minus of Y whole Square here you'll be adding a parameter Lambda\nhere you'll be adding a parameter Lambda\nhere you'll be adding a parameter Lambda but understand here you'll not be adding\nbut understand here you'll not be adding\nbut understand here you'll not be adding slope Square no here you'll be adding\nslope Square no here you'll be adding\nslope Square no here you'll be adding mode of slope here you'll be adding mode\nmode of slope here you'll be adding mode\nmode of slope here you'll be adding mode of slope and this mode of slope will\nof slope and this mode of slope will\nof slope and this mode of slope will work is that it will actually help you\nwork is that it will actually help you\nwork is that it will actually help you to do feature selection now you may be\nto do feature selection now you may be\nto do feature selection now you may be thinking how feature selection crash\nthinking how feature selection crash\nthinking how feature selection crash let's consider a equation over here\nlet's consider a equation over here\nlet's consider a equation over here let's say that I have many many features\nlet's say that I have many many features\nlet's say that I have many many features I have many many many features okay so\nI have many many many features okay so\nI have many many many features okay so my H Theta of X which I'm indicating\nmy H Theta of X which I'm indicating\nmy H Theta of X which I'm indicating here as y hat let's say that I'm I'm\nhere as y hat let's say that I'm I'm\nhere as y hat let's say that I'm I'm writing this equation apart from\nwriting this equation apart from\nwriting this equation apart from preventing for overfitting it will also\npreventing for overfitting it will also\npreventing for overfitting it will also help you to do feature selection here\nhelp you to do feature selection here\nhelp you to do feature selection here let me just show you over here with an\nlet me just show you over here with an\nlet me just show you over here with an example this H Theta of X which I'm\nexample this H Theta of X which I'm\nexample this H Theta of X which I'm probably writing as y hat will basically\nprobably writing as y hat will basically\nprobably writing as y hat will basically be indicated by something over here\nbe indicated by something over here\nbe indicated by something over here you'll be able to see that it is nothing\nyou'll be able to see that it is nothing\nyou'll be able to see that it is nothing but let's say that I have multiple\nbut let's say that I have multiple\nbut let's say that I have multiple features like this now in this\nfeatures like this now in this\nfeatures like this now in this particular features obviously there are\nparticular features obviously there are\nparticular features obviously there are so many coefficients over here so many\nso many coefficients over here so many\nso many coefficients over here so many slopes over here now mod of slope will\nslopes over here now mod of slope will\nslopes over here now mod of slope will be what it will be nothing but mod of X1\nbe what it will be nothing but mod of X1\nbe what it will be nothing but mod of X1 plus X2 plus X3 plus X4 plus X5 like\nplus X2 plus X3 plus X4 plus X5 like\nplus X2 plus X3 plus X4 plus X5 like this up to xn now in this particular\nthis up to xn now in this particular\nthis up to xn now in this particular case how it is basically helping you to\ncase how it is basically helping you to\ncase how it is basically helping you to sorry not X1 sorry just a second this\nsorry not X1 sorry just a second this\nsorry not X1 sorry just a second this mod of I have taken the data point this\nmod of I have taken the data point this\nmod of I have taken the data point this is not data points this should be your\nis not data points this should be your\nis not data points this should be your mod of theta 0 + Theta 1 + Theta 2 +\nmod of theta 0 + Theta 1 + Theta 2 +\nmod of theta 0 + Theta 1 + Theta 2 + theta 3 + Theta 4 + Theta 5 like this up\ntheta 3 + Theta 4 + Theta 5 like this up\ntheta 3 + Theta 4 + Theta 5 like this up to Theta n so here you'll be able to see\nto Theta n so here you'll be able to see\nto Theta n so here you'll be able to see that this is how I will basically uh\nthat this is how I will basically uh\nthat this is how I will basically uh I'll basically be calculating the slope\nI'll basically be calculating the slope\nI'll basically be calculating the slope now as we go ahead guys whichever\nnow as we go ahead guys whichever\nnow as we go ahead guys whichever features are probably not playing an\nfeatures are probably not playing an\nfeatures are probably not playing an amazing role the Theta value the\namazing role the Theta value the\namazing role the Theta value the coefficient value the slope value will\ncoefficient value the slope value will\ncoefficient value the slope value will be very very small it is just like that\nbe very very small it is just like that\nbe very very small it is just like that entire feature is neglected that entire\nentire feature is neglected that entire\nentire feature is neglected that entire feature is neglected now in this\nfeature is neglected now in this\nfeature is neglected now in this particular case we were doing squaring\nparticular case we were doing squaring\nparticular case we were doing squaring because of the squaring that value was\nbecause of the squaring that value was\nbecause of the squaring that value was also increasing but here because of the\nalso increasing but here because of the\nalso increasing but here because of the mode that value will not increase\nmode that value will not increase\nmode that value will not increase instead it will be a condition wherein\ninstead it will be a condition wherein\ninstead it will be a condition wherein we are basically neglecting those\nwe are basically neglecting those\nwe are basically neglecting those features that are not at all important\nfeatures that are not at all important\nfeatures that are not at all important in this specific problem statement so\nin this specific problem statement so\nin this specific problem statement so with the help of L1 regularization that\nwith the help of L1 regularization that\nwith the help of L1 regularization that is lasso you are able to do two\nis lasso you are able to do two\nis lasso you are able to do two important things one is preventing\nimportant things one is preventing\nimportant things one is preventing overfitting and the second case is that\noverfitting and the second case is that\noverfitting and the second case is that if you have many features and many of\nif you have many features and many of\nif you have many features and many of the features are not that important okay\nthe features are not that important okay\nthe features are not that important okay in basically finding out your slope or\nin basically finding out your slope or\nin basically finding out your slope or your line or the best fit line in that\nyour line or the best fit line in that\nyour line or the best fit line in that particular case it will also help you to\nparticular case it will also help you to\nparticular case it will also help you to perform feature selection so this is the\nperform feature selection so this is the\nperform feature selection so this is the importance of the entire what is the\nimportance of the entire what is the\nimportance of the entire what is the importance of this this is the\nimportance of this this is the\nimportance of this this is the importance of the uh Ridge and the lasso\nimportance of the uh Ridge and the lasso\nimportance of the uh Ridge and the lasso regression that we are doing here I'm\nregression that we are doing here I'm\nregression that we are doing here I'm just going to write L1\njust going to write L1\njust going to write L1 regularization and obviously we have\nregularization and obviously we have\nregularization and obviously we have discussed about L2 regularization also\ndiscussed about L2 regularization also\ndiscussed about L2 regularization also now you have probably understood Lambda\nnow you have probably understood Lambda\nnow you have probably understood Lambda is one hyperparameter okay which we will\nis one hyperparameter okay which we will\nis one hyperparameter okay which we will specifically using okay and based on\nspecifically using okay and based on\nspecifically using okay and based on this Lambda this will be found out\nthis Lambda this will be found out\nthis Lambda this will be found out through cross\nthrough cross\nthrough cross validation cross validation is a\nvalidation cross validation is a\nvalidation cross validation is a technique wherein we will try to\ntechnique wherein we will try to\ntechnique wherein we will try to probably train our model and try to find\nprobably train our model and try to find\nprobably train our model and try to find out the specific things okay what should\nout the specific things okay what should\nout the specific things okay what should be the exact value and there also we\nbe the exact value and there also we\nbe the exact value and there also we play with multiple values in short what\nplay with multiple values in short what\nplay with multiple values in short what we are doing we just trying to reduce\nwe are doing we just trying to reduce\nwe are doing we just trying to reduce the cost function in such a way that uh\nthe cost function in such a way that uh\nthe cost function in such a way that uh it will definitely never become zero but\nit will definitely never become zero but\nit will definitely never become zero but it will basically reduce based on the\nit will basically reduce based on the\nit will basically reduce based on the Lambda and the slope value in most of\nLambda and the slope value in most of\nLambda and the slope value in most of the scenario if you ask me we should\nthe scenario if you ask me we should\nthe scenario if you ask me we should definitely try both the regularization\ndefinitely try both the regularization\ndefinitely try both the regularization and see that wherever the performance\nand see that wherever the performance\nand see that wherever the performance Matrix is good we should use that what\nMatrix is good we should use that what\nMatrix is good we should use that what is cross validation basically means I\nis cross validation basically means I\nis cross validation basically means I will try to use different different\nwill try to use different different\nwill try to use different different Lambda value and basically Ally use it\nLambda value and basically Ally use it\nLambda value and basically Ally use it so in a short let me write it down again\nso in a short let me write it down again\nso in a short let me write it down again for Ridge regression which is an L2 Norm\nfor Ridge regression which is an L2 Norm\nfor Ridge regression which is an L2 Norm here I'm simply writing my cost function\nhere I'm simply writing my cost function\nhere I'm simply writing my cost function in this particular case will be little\nin this particular case will be little\nin this particular case will be little bit different here I can definitely\nbit different here I can definitely\nbit different here I can definitely write my cost function as H Theta X of i\nwrite my cost function as H Theta X of i\nwrite my cost function as H Theta X of i - y of I S Plus Lambda multiplied slope\n- y of I S Plus Lambda multiplied slope\n- y of I S Plus Lambda multiplied slope Square what is the purpose of this the\nSquare what is the purpose of this the\nSquare what is the purpose of this the purpose is very simple here we are\npurpose is very simple here we are\npurpose is very simple here we are preventing overfitting this was with\npreventing overfitting this was with\npreventing overfitting this was with respect to the Ridge Recreation that is\nrespect to the Ridge Recreation that is\nrespect to the Ridge Recreation that is L2 nor now if I go ahead and discuss\nL2 nor now if I go ahead and discuss\nL2 nor now if I go ahead and discuss about the next one which is called as\nabout the next one which is called as\nabout the next one which is called as lasso regression which is also called as\nlasso regression which is also called as\nlasso regression which is also called as L1 regularization in the case of lasso\nL1 regularization in the case of lasso\nL1 regularization in the case of lasso regression your cost function will be H\nregression your cost function will be H\nregression your cost function will be H Theta of X of\nTheta of X of\nTheta of X of IUS y of\nIUS y of\nIUS y of iÂ² plus Lambda ultied mode of flow so\niÂ² plus Lambda ultied mode of flow so\niÂ² plus Lambda ultied mode of flow so here you have this specific thing and\nhere you have this specific thing and\nhere you have this specific thing and what is the purpose the purpose are two\nwhat is the purpose the purpose are two\nwhat is the purpose the purpose are two one is prevent overfitting and the\none is prevent overfitting and the\none is prevent overfitting and the second one is something called as\nsecond one is something called as\nsecond one is something called as feature selection so these two are the\nfeature selection so these two are the\nfeature selection so these two are the outcomes of the entire thing see with\noutcomes of the entire thing see with\noutcomes of the entire thing see with respect to this lasso right you have\nrespect to this lasso right you have\nrespect to this lasso right you have slopes slopes here you'll be having\nslopes slopes here you'll be having\nslopes slopes here you'll be having Theta 0 plus Theta 1 plus Theta 2 plus\nTheta 0 plus Theta 1 plus Theta 2 plus\nTheta 0 plus Theta 1 plus Theta 2 plus theta 3 like this up to Theta n now when\ntheta 3 like this up to Theta n now when\ntheta 3 like this up to Theta n now when you'll have this many number of thetas\nyou'll have this many number of thetas\nyou'll have this many number of thetas when you have many number of features\nwhen you have many number of features\nwhen you have many number of features and when you have many number of\nand when you have many number of\nand when you have many number of features that basically means you'll\nfeatures that basically means you'll\nfeatures that basically means you'll have multiple slopes right those\nhave multiple slopes right those\nhave multiple slopes right those features that are not performing well or\nfeatures that are not performing well or\nfeatures that are not performing well or that has no contribution in finding out\nthat has no contribution in finding out\nthat has no contribution in finding out your output that coefficient value will\nyour output that coefficient value will\nyour output that coefficient value will be almost nil right it will be very much\nbe almost nil right it will be very much\nbe almost nil right it will be very much near to zero in short you neglecting\nnear to zero in short you neglecting\nnear to zero in short you neglecting that value by using modulus you're not\nthat value by using modulus you're not\nthat value by using modulus you're not squaring them up you're not increasing\nsquaring them up you're not increasing\nsquaring them up you're not increasing those values now I will continue and uh\nthose values now I will continue and uh\nthose values now I will continue and uh probably I will also discuss about the\nprobably I will also discuss about the\nprobably I will also discuss about the assumptions of linear regressions so\nassumptions of linear regressions so\nassumptions of linear regressions so what are the assumptions of linear\nwhat are the assumptions of linear\nwhat are the assumptions of linear regression in this particular scenario\nregression in this particular scenario\nregression in this particular scenario so assumption is that number one point\nso assumption is that number one point\nso assumption is that number one point linear regression if our features are in\nlinear regression if our features are in\nlinear regression if our features are in normal or gion\nnormal or gion\nnormal or gion distribution if our features follows\ndistribution if our features follows\ndistribution if our features follows this particular distribution it is\nthis particular distribution it is\nthis particular distribution it is obviously good our model will get\nobviously good our model will get\nobviously good our model will get trained well so there is one concept\ntrained well so there is one concept\ntrained well so there is one concept which is called as feature\nwhich is called as feature\nwhich is called as feature transformation now in future\ntransformation now in future\ntransformation now in future transformation always understand what\ntransformation always understand what\ntransformation always understand what will happen if a model does not fall\nwill happen if a model does not fall\nwill happen if a model does not fall follow a gan distribution then we apply\nfollow a gan distribution then we apply\nfollow a gan distribution then we apply some kind of mathematical equation onto\nsome kind of mathematical equation onto\nsome kind of mathematical equation onto the data and try to convert them into\nthe data and try to convert them into\nthe data and try to convert them into normal orian distribution the second\nnormal orian distribution the second\nnormal orian distribution the second assumption that I would definitely like\nassumption that I would definitely like\nassumption that I would definitely like to make is that standard scalar or\nto make is that standard scalar or\nto make is that standard scalar or standard digestion standard dig is\nstandard digestion standard dig is\nstandard digestion standard dig is nothing but it is a kind of scaling your\nnothing but it is a kind of scaling your\nnothing but it is a kind of scaling your data by using Z score I hope everybody\ndata by using Z score I hope everybody\ndata by using Z score I hope everybody remembers Z score this is what we\nremembers Z score this is what we\nremembers Z score this is what we basically apply there your mean is equal\nbasically apply there your mean is equal\nbasically apply there your mean is equal to zero and standard deviation equal to\nto zero and standard deviation equal to\nto zero and standard deviation equal to 1 see guys wherever you have gradient\n1 see guys wherever you have gradient\n1 see guys wherever you have gradient descent involved it is good to basically\ndescent involved it is good to basically\ndescent involved it is good to basically do\ndo\ndo standardization because if our initial\nstandardization because if our initial\nstandardization because if our initial point is a small Point somewhere here\npoint is a small Point somewhere here\npoint is a small Point somewhere here then to reach the global Minima or\nthen to reach the global Minima or\nthen to reach the global Minima or training will happen quickly otherwise\ntraining will happen quickly otherwise\ntraining will happen quickly otherwise what will happen if your values are\nwhat will happen if your values are\nwhat will happen if your values are quite huge then your graph may be very\nquite huge then your graph may be very\nquite huge then your graph may be very big and the point can come over any over\nbig and the point can come over any over\nbig and the point can come over any over there and the third point is that this\nthere and the third point is that this\nthere and the third point is that this linear regression works with respect to\nlinear regression works with respect to\nlinear regression works with respect to linearity it works if your data is\nlinearity it works if your data is\nlinearity it works if your data is linearly separable\nlinearly separable\nlinearly separable I'll not say linearly separable but this\nI'll not say linearly separable but this\nI'll not say linearly separable but this linearity will come into picture if your\nlinearity will come into picture if your\nlinearity will come into picture if your data is too much linear it will\ndata is too much linear it will\ndata is too much linear it will obviously be able to give a very good\nobviously be able to give a very good\nobviously be able to give a very good answer like logistic regression also\nanswer like logistic regression also\nanswer like logistic regression also which we are going to discuss today this\nwhich we are going to discuss today this\nwhich we are going to discuss today this also has the same property now you may\nalso has the same property now you may\nalso has the same property now you may be asking is it compulsory to do\nbe asking is it compulsory to do\nbe asking is it compulsory to do standardization guys if you want to\nstandardization guys if you want to\nstandardization guys if you want to increase the training time of your model\nincrease the training time of your model\nincrease the training time of your model or if you want to optimize your model I\nor if you want to optimize your model I\nor if you want to optimize your model I would suggest go ahead and do\nwould suggest go ahead and do\nwould suggest go ahead and do standardization now coming to the fourth\nstandardization now coming to the fourth\nstandardization now coming to the fourth Point here you really need to check\nPoint here you really need to check\nPoint here you really need to check about multicolinearity\nabout multicolinearity\nabout multicolinearity this is also one kind of check we\nthis is also one kind of check we\nthis is also one kind of check we basically do what is multicol\nbasically do what is multicol\nbasically do what is multicol linearities let's say I have X1 I have\nlinearities let's say I have X1 I have\nlinearities let's say I have X1 I have X2 and this is my output feature I have\nX2 and this is my output feature I have\nX2 and this is my output feature I have let's say X3 also now let's say that if\nlet's say X3 also now let's say that if\nlet's say X3 also now let's say that if I try to see the colinearity of this two\nI try to see the colinearity of this two\nI try to see the colinearity of this two feature how how correlated these two\nfeature how how correlated these two\nfeature how how correlated these two feature are let's say that these two\nfeature are let's say that these two\nfeature are let's say that these two feature are 95% correlated is it is it a\nfeature are 95% correlated is it is it a\nfeature are 95% correlated is it is it a wise decision to use both the features\nwise decision to use both the features\nwise decision to use both the features and let's say that let's let's say that\nand let's say that let's let's say that\nand let's say that let's let's say that these two features are 95% correlated\nthese two features are 95% correlated\nthese two features are 95% correlated but it is highly correlated with Y is it\nbut it is highly correlated with Y is it\nbut it is highly correlated with Y is it necessary that we should use both the\nnecessary that we should use both the\nnecessary that we should use both the feature in this particular scenario the\nfeature in this particular scenario the\nfeature in this particular scenario the answer should be no we can drop this\nanswer should be no we can drop this\nanswer should be no we can drop this particular feature okay we can drop this\nparticular feature okay we can drop this\nparticular feature okay we can drop this particular feature any one of the\nparticular feature any one of the\nparticular feature any one of the feature we can definitely drop it and\nfeature we can definitely drop it and\nfeature we can definitely drop it and based on that I can just use one single\nbased on that I can just use one single\nbased on that I can just use one single feature and basically we do the\nfeature and basically we do the\nfeature and basically we do the prediction there is also a concept which\nprediction there is also a concept which\nprediction there is also a concept which is called as variation inflation factor\nis called as variation inflation factor\nis called as variation inflation factor I will try to make a dedicated video\nI will try to make a dedicated video\nI will try to make a dedicated video about this multical is also solved with\nabout this multical is also solved with\nabout this multical is also solved with the help of variation inflation Factor\nthe help of variation inflation Factor\nthe help of variation inflation Factor one more term is there homos orc so that\none more term is there homos orc so that\none more term is there homos orc so that kind of terminologies also we use one\nkind of terminologies also we use one\nkind of terminologies also we use one more condition in this but if you almost\nmore condition in this but if you almost\nmore condition in this but if you almost satisfied with this assumptions you will\nsatisfied with this assumptions you will\nsatisfied with this assumptions you will definitely be able to outperform in\ndefinitely be able to outperform in\ndefinitely be able to outperform in linear regression so you have got an\nlinear regression so you have got an\nlinear regression so you have got an idea of the assumptions you have also\nidea of the assumptions you have also\nidea of the assumptions you have also got an idea of multiple things okay now\ngot an idea of multiple things okay now\ngot an idea of multiple things okay now let's go towards something called as\nlet's go towards something called as\nlet's go towards something called as logistic regression now logistic\nlogistic regression now logistic\nlogistic regression now logistic regression what logistic regression is\nregression what logistic regression is\nregression what logistic regression is the first type of algorithm that we are\nthe first type of algorithm that we are\nthe first type of algorithm that we are going to learn in classification let's\ngoing to learn in classification let's\ngoing to learn in classification let's say that in classification I have one\nsay that in classification I have one\nsay that in classification I have one example you know so suppose I have say\nexample you know so suppose I have say\nexample you know so suppose I have say number of hours study hours and number\nnumber of hours study hours and number\nnumber of hours study hours and number of play hours based on this I want to\nof play hours based on this I want to\nof play hours based on this I want to predict whether a child is passing or\npredict whether a child is passing or\npredict whether a child is passing or failing suppose these two are my\nfailing suppose these two are my\nfailing suppose these two are my features I want to predict whether it is\nfeatures I want to predict whether it is\nfeatures I want to predict whether it is pass or fail so here you'll be able to\npass or fail so here you'll be able to\npass or fail so here you'll be able to see that I have some fixed number of\nsee that I have some fixed number of\nsee that I have some fixed number of categories specifically in this\ncategories specifically in this\ncategories specifically in this particular scenario I have two\nparticular scenario I have two\nparticular scenario I have two categories binary logistic regression\ncategories binary logistic regression\ncategories binary logistic regression works very well with binary\nworks very well with binary\nworks very well with binary classification now the uh question comes\nclassification now the uh question comes\nclassification now the uh question comes that can we solve multiclass\nthat can we solve multiclass\nthat can we solve multiclass classification using logistic the answer\nclassification using logistic the answer\nclassification using logistic the answer is simply yes you can definitely do it\nis simply yes you can definitely do it\nis simply yes you can definitely do it so let's go ahead and let's try to\nso let's go ahead and let's try to\nso let's go ahead and let's try to discuss about uh logistic regression now\ndiscuss about uh logistic regression now\ndiscuss about uh logistic regression now what is the main purpose of the logistic\nwhat is the main purpose of the logistic\nwhat is the main purpose of the logistic regression first of all let's let's uh\nregression first of all let's let's uh\nregression first of all let's let's uh understand one scenario okay suppose I\nunderstand one scenario okay suppose I\nunderstand one scenario okay suppose I have a feature which basically says um\nhave a feature which basically says um\nhave a feature which basically says um number of study hours and this is like 1\nnumber of study hours and this is like 1\nnumber of study hours and this is like 1 2 3 4 5 6 7 and let's say that I have\n2 3 4 5 6 7 and let's say that I have\n2 3 4 5 6 7 and let's say that I have pass this point is basically pass and\npass this point is basically pass and\npass this point is basically pass and this point is basically\nthis point is basically\nthis point is basically fail so I have this two conditions these\nfail so I have this two conditions these\nfail so I have this two conditions these are my outcomes now what I'll do I will\nare my outcomes now what I'll do I will\nare my outcomes now what I'll do I will just try to make some data points let's\njust try to make some data points let's\njust try to make some data points let's say that if I study Less Than 3 hours I\nsay that if I study Less Than 3 hours I\nsay that if I study Less Than 3 hours I will probably be fail if I study more\nwill probably be fail if I study more\nwill probably be fail if I study more than 3 hours then probably I will pass\nthan 3 hours then probably I will pass\nthan 3 hours then probably I will pass this I'll make it as fail and this I\nthis I'll make it as fail and this I\nthis I'll make it as fail and this I will make it as pass so I will be having\nwill make it as pass so I will be having\nwill make it as pass so I will be having points over here this 1 2 3 let's say\npoints over here this 1 2 3 let's say\npoints over here this 1 2 3 let's say that this is my training data set now\nthat this is my training data set now\nthat this is my training data set now the first question says that okay Chris\nthe first question says that okay Chris\nthe first question says that okay Chris fine you have some data over here\nfine you have some data over here\nfine you have some data over here whenever it is less than three you are\nwhenever it is less than three you are\nwhenever it is less than three you are basically the person is failing if it is\nbasically the person is failing if it is\nbasically the person is failing if it is greater than five greater than three it\ngreater than five greater than three it\ngreater than five greater than three it is basically showing data points points\nis basically showing data points points\nis basically showing data points points with respect to pass now can't we solve\nwith respect to pass now can't we solve\nwith respect to pass now can't we solve this problem first with linear\nthis problem first with linear\nthis problem first with linear regression now with the help of linear\nregression now with the help of linear\nregression now with the help of linear regression here the first point will be\nregression here the first point will be\nregression here the first point will be that yes I can definitely draw a best\nthat yes I can definitely draw a best\nthat yes I can definitely draw a best fit line my best fit line in this\nfit line my best fit line in this\nfit line my best fit line in this particular scenario may be something\nparticular scenario may be something\nparticular scenario may be something like this it may it may look something\nlike this it may it may look something\nlike this it may it may look something like this so here fail is nothing but\nlike this so here fail is nothing but\nlike this so here fail is nothing but zero pass is one the middle point is\nzero pass is one the middle point is\nzero pass is one the middle point is basically 0.5 so obviously with the help\nbasically 0.5 so obviously with the help\nbasically 0.5 so obviously with the help of linear\nof linear\nof linear regression I'm able to create this best\nregression I'm able to create this best\nregression I'm able to create this best fit line and I'll put a scenario that\nfit line and I'll put a scenario that\nfit line and I'll put a scenario that whenever the value is less\nwhenever the value is less\nwhenever the value is less than5 whenever the value is less than\nthan5 whenever the value is less than\nthan5 whenever the value is less than 0.5 whenever the output is less than5\n0.5 whenever the output is less than5\n0.5 whenever the output is less than5 let's say that new data point is this\nlet's say that new data point is this\nlet's say that new data point is this and based on this I'll try to do the\nand based on this I'll try to do the\nand based on this I'll try to do the prediction I'm actually able to get the\nprediction I'm actually able to get the\nprediction I'm actually able to get the output over here now when I'm getting\noutput over here now when I'm getting\noutput over here now when I'm getting the output over here this basically is\nthe output over here this basically is\nthe output over here this basically is 0.25 now in this particular scenario\n0.25 now in this particular scenario\n0.25 now in this particular scenario obviously I'm able to say that yes the\nobviously I'm able to say that yes the\nobviously I'm able to say that yes the person I'll write a condition over here\nperson I'll write a condition over here\nperson I'll write a condition over here saying that if my H Theta of x value is\nsaying that if my H Theta of x value is\nsaying that if my H Theta of x value is less than 0.5 then my output should be\nless than 0.5 then my output should be\nless than 0.5 then my output should be zero let's say less than 0.5 I'll say\nzero let's say less than 0.5 I'll say\nzero let's say less than 0.5 I'll say not less than or equal to less than5\nnot less than or equal to less than5\nnot less than or equal to less than5 then my output will be zero right so in\nthen my output will be zero right so in\nthen my output will be zero right so in this particular case Zero basically\nthis particular case Zero basically\nthis particular case Zero basically means fail similarly I'll have a\nmeans fail similarly I'll have a\nmeans fail similarly I'll have a scenario where I'll say that when if my\nscenario where I'll say that when if my\nscenario where I'll say that when if my S of theta of X is greater than or equal\nS of theta of X is greater than or equal\nS of theta of X is greater than or equal to 5 then this will basically be one\nto 5 then this will basically be one\nto 5 then this will basically be one which is nothing but pass so this two\nwhich is nothing but pass so this two\nwhich is nothing but pass so this two condition I can definitely write over\ncondition I can definitely write over\ncondition I can definitely write over here this is my center point so that any\nhere this is my center point so that any\nhere this is my center point so that any point that will probably come over here\npoint that will probably come over here\npoint that will probably come over here let's say that this point is coming over\nlet's say that this point is coming over\nlet's say that this point is coming over here right let's say new data point is\nhere right let's say new data point is\nhere right let's say new data point is somewhere coming over here with this red\nsomewhere coming over here with this red\nsomewhere coming over here with this red point\npoint\npoint now what I'll do I'll basically draw a\nnow what I'll do I'll basically draw a\nnow what I'll do I'll basically draw a straight line it will come over here I\nstraight line it will come over here I\nstraight line it will come over here I will just extend this line\nwill just extend this line\nwill just extend this line long I will extend this line over here\nlong I will extend this line over here\nlong I will extend this line over here and I will extend this line over here\nand I will extend this line over here\nand I will extend this line over here and here you can see that based on this\nand here you can see that based on this\nand here you can see that based on this I'm actually getting this particular\nI'm actually getting this particular\nI'm actually getting this particular prediction which is greater than 0.5 so\nprediction which is greater than 0.5 so\nprediction which is greater than 0.5 so I will say that okay the person has\nI will say that okay the person has\nI will say that okay the person has passed obviously this is fine this is\npassed obviously this is fine this is\npassed obviously this is fine this is obviously working better this is\nobviously working better this is\nobviously working better this is obviously working better so what what is\nobviously working better so what what is\nobviously working better so what what is the problem why we are not using linear\nthe problem why we are not using linear\nthe problem why we are not using linear regression okay in order to solve this\nregression okay in order to solve this\nregression okay in order to solve this particular problem why you are\nparticular problem why you are\nparticular problem why you are specifically having logistic regression\nspecifically having logistic regression\nspecifically having logistic regression the answer is very much simple guys the\nthe answer is very much simple guys the\nthe answer is very much simple guys the answer is that whenever let's say that\nanswer is that whenever let's say that\nanswer is that whenever let's say that if I have an outlier which looks\nif I have an outlier which looks\nif I have an outlier which looks something like this suppose I have an\nsomething like this suppose I have an\nsomething like this suppose I have an outlier which comes like this over here\noutlier which comes like this over here\noutlier which comes like this over here what is this value let's say that this\nwhat is this value let's say that this\nwhat is this value let's say that this value is nothing but 7 8 9 10 let's say\nvalue is nothing but 7 8 9 10 let's say\nvalue is nothing but 7 8 9 10 let's say that the number of study hours and I'm\nthat the number of study hours and I'm\nthat the number of study hours and I'm studying for nine it is obviously pass\nstudying for nine it is obviously pass\nstudying for nine it is obviously pass now in this particular scenario when I\nnow in this particular scenario when I\nnow in this particular scenario when I have an outlier this entire line will\nhave an outlier this entire line will\nhave an outlier this entire line will change now I will probably get my line\nchange now I will probably get my line\nchange now I will probably get my line which looks something like this okay my\nwhich looks something like this okay my\nwhich looks something like this okay my line will basically move something like\nline will basically move something like\nline will basically move something like this it will now get moved something\nthis it will now get moved something\nthis it will now get moved something like this now when it gets moves\nlike this now when it gets moves\nlike this now when it gets moves completely like this now for even five\ncompletely like this now for even five\ncompletely like this now for even five or even at any point that I am actually\nor even at any point that I am actually\nor even at any point that I am actually predicting let's say that at this\npredicting let's say that at this\npredicting let's say that at this particular point if I try to find out\nparticular point if I try to find out\nparticular point if I try to find out it'll be showing less than 0. five so\nit'll be showing less than 0. five so\nit'll be showing less than 0. five so here this particular value or answer\nhere this particular value or answer\nhere this particular value or answer will be wrong right because if we are\nwill be wrong right because if we are\nwill be wrong right because if we are studying more than 5 hours OB viously B\nstudying more than 5 hours OB viously B\nstudying more than 5 hours OB viously B based on the previous line the person\nbased on the previous line the person\nbased on the previous line the person had to pass but in this scenario it is\nhad to pass but in this scenario it is\nhad to pass but in this scenario it is failing it is coming less than 0.5 but\nfailing it is coming less than 0.5 but\nfailing it is coming less than 0.5 but the real value for this is basically\nthe real value for this is basically\nthe real value for this is basically passed so I hope you are understanding\npassed so I hope you are understanding\npassed so I hope you are understanding because of the outlier the entire line\nbecause of the outlier the entire line\nbecause of the outlier the entire line is getting changed so how do we fix this\nis getting changed so how do we fix this\nis getting changed so how do we fix this particular problem now in this two\nparticular problem now in this two\nparticular problem now in this two scenarios are there first of all\nscenarios are there first of all\nscenarios are there first of all obviously because of just an outlier\nobviously because of just an outlier\nobviously because of just an outlier your entire line is getting shifted here\nyour entire line is getting shifted here\nyour entire line is getting shifted here and there the second point is that over\nand there the second point is that over\nand there the second point is that over here sometimes you're also getting\nhere sometimes you're also getting\nhere sometimes you're also getting greater than one you you're also getting\ngreater than one you you're also getting\ngreater than one you you're also getting less than one suppose if I try to\nless than one suppose if I try to\nless than one suppose if I try to calculate for this particular point if I\ncalculate for this particular point if I\ncalculate for this particular point if I project it in behind I'll be getting\nproject it in behind I'll be getting\nproject it in behind I'll be getting some negative value so we have to squash\nsome negative value so we have to squash\nsome negative value so we have to squash this function if I squash this function\nthis function if I squash this function\nthis function if I squash this function then it'll become a plain line right how\nthen it'll become a plain line right how\nthen it'll become a plain line right how do we squash it and for this we use\ndo we squash it and for this we use\ndo we squash it and for this we use something called as sigmoid activation\nsomething called as sigmoid activation\nsomething called as sigmoid activation function or sigmoid function if somebody\nfunction or sigmoid function if somebody\nfunction or sigmoid function if somebody ask you why don't you use linear\nask you why don't you use linear\nask you why don't you use linear regession in order to solve this\nregession in order to solve this\nregession in order to solve this classification problem then your answer\nclassification problem then your answer\nclassification problem then your answer should be very much simple you should\nshould be very much simple you should\nshould be very much simple you should say this to specific points so we will\nsay this to specific points so we will\nsay this to specific points so we will try to go ahead and solve some linear\ntry to go ahead and solve some linear\ntry to go ahead and solve some linear regression now with the help of cost\nregression now with the help of cost\nregression now with the help of cost function everything as such and we'll\nfunction everything as such and we'll\nfunction everything as such and we'll try to understand how the cost function\ntry to understand how the cost function\ntry to understand how the cost function will look for logistic regression second\nwill look for logistic regression second\nwill look for logistic regression second reason I told you right it is greater\nreason I told you right it is greater\nreason I told you right it is greater than zero over here the line is going\nthan zero over here the line is going\nthan zero over here the line is going greater than zero right greater than\ngreater than zero right greater than\ngreater than zero right greater than zero I have only Z and one and it is\nzero I have only Z and one and it is\nzero I have only Z and one and it is becoming greater than zero but I have\nbecoming greater than zero but I have\nbecoming greater than zero but I have already told that our maximum and\nalready told that our maximum and\nalready told that our maximum and minimum value are 1 and zero so I hope\nminimum value are 1 and zero so I hope\nminimum value are 1 and zero so I hope you have understood why linear Reg\nyou have understood why linear Reg\nyou have understood why linear Reg cannot be used okay I showed you all the\ncannot be used okay I showed you all the\ncannot be used okay I showed you all the scenarios why linear regression should\nscenarios why linear regression should\nscenarios why linear regression should not be used now we'll continue and\nnot be used now we'll continue and\nnot be used now we'll continue and probably discuss about the other things\nprobably discuss about the other things\nprobably discuss about the other things over here and uh we will now try to\nover here and uh we will now try to\nover here and uh we will now try to understand fine what exactly logistic\nunderstand fine what exactly logistic\nunderstand fine what exactly logistic regression is all about and how the\nregression is all about and how the\nregression is all about and how the decision boundaries basically created\ndecision boundaries basically created\ndecision boundaries basically created now we'll go ahead and discuss about\nnow we'll go ahead and discuss about\nnow we'll go ahead and discuss about that specific thing so let's go ahead\nthat specific thing so let's go ahead\nthat specific thing so let's go ahead our values should be always between 0 to\nour values should be always between 0 to\nour values should be always between 0 to one over here in this particular case\none over here in this particular case\none over here in this particular case because it is a binary classification\nbecause it is a binary classification\nbecause it is a binary classification problem only this should be the answer\nproblem only this should be the answer\nproblem only this should be the answer so let's go ahead and let's define our\nso let's go ahead and let's define our\nso let's go ahead and let's define our decision boundary so my decision\ndecision boundary so my decision\ndecision boundary so my decision boundary decision boundary in the case\nboundary decision boundary in the case\nboundary decision boundary in the case of logistic regression first of all as\nof logistic regression first of all as\nof logistic regression first of all as usual in logistic regression we defined\nusual in logistic regression we defined\nusual in logistic regression we defined our hypothesis okay guys first of all\nour hypothesis okay guys first of all\nour hypothesis okay guys first of all let's see if I'm writing my my h of\nlet's see if I'm writing my my h of\nlet's see if I'm writing my my h of theta my H Theta of X as Theta 0 + Theta\ntheta my H Theta of X as Theta 0 + Theta\ntheta my H Theta of X as Theta 0 + Theta 1 into x + Theta 2 into X like this X1\n1 into x + Theta 2 into X like this X1\n1 into x + Theta 2 into X like this X1 X2 + Theta n into xn\nX2 + Theta n into xn\nX2 + Theta n into xn now in this scenario can I write this\nnow in this scenario can I write this\nnow in this scenario can I write this entire equation as Theta transpose X\nentire equation as Theta transpose X\nentire equation as Theta transpose X obviously I can definitely write this\nobviously I can definitely write this\nobviously I can definitely write this way right and this is what is the\nway right and this is what is the\nway right and this is what is the notation that you will probably seeing\nnotation that you will probably seeing\nnotation that you will probably seeing in many places so with respect to the\nin many places so with respect to the\nin many places so with respect to the decision boundary of logistic regression\ndecision boundary of logistic regression\ndecision boundary of logistic regression our Theta see like this we can write I'm\nour Theta see like this we can write I'm\nour Theta see like this we can write I'm saying okay but since we have to\nsaying okay but since we have to\nsaying okay but since we have to consider two things one is squashing the\nconsider two things one is squashing the\nconsider two things one is squashing the line okay how that squashing will\nline okay how that squashing will\nline okay how that squashing will basically happen see if I have this if I\nbasically happen see if I have this if I\nbasically happen see if I have this if I have this line\nhave this line\nhave this line we saw in the above right if I have this\nwe saw in the above right if I have this\nwe saw in the above right if I have this line suppose I have some data points\nline suppose I have some data points\nline suppose I have some data points over here and I have some data points\nover here and I have some data points\nover here and I have some data points over here if I want to create the best\nover here if I want to create the best\nover here if I want to create the best fit line how will I create I will\nfit line how will I create I will\nfit line how will I create I will basically create like this but I have to\nbasically create like this but I have to\nbasically create like this but I have to also do two things one is squash over\nalso do two things one is squash over\nalso do two things one is squash over here and squash over here right squash\nhere and squash over here right squash\nhere and squash over here right squash over here and squash over here now in\nover here and squash over here now in\nover here and squash over here now in order to squash I'm saying squash squash\norder to squash I'm saying squash squash\norder to squash I'm saying squash squash means\nmeans\nmeans okay now in order to do this I use a\nokay now in order to do this I use a\nokay now in order to do this I use a function which is called as sigmoid\nfunction which is called as sigmoid\nfunction which is called as sigmoid activation function\nactivation function\nactivation function that basically means what happens\nthat basically means what happens\nthat basically means what happens obviously you know this line is\nobviously you know this line is\nobviously you know this line is basically denoted by H Theta of x equal\nbasically denoted by H Theta of x equal\nbasically denoted by H Theta of x equal to how do you denote this straight line\nto how do you denote this straight line\nto how do you denote this straight line let me write it down nicely for you so\nlet me write it down nicely for you so\nlet me write it down nicely for you so how do you denote this straight line the\nhow do you denote this straight line the\nhow do you denote this straight line the straight line is obviously denoted by\nstraight line is obviously denoted by\nstraight line is obviously denoted by Theta 0 + Theta 1 * X1 let's say now on\nTheta 0 + Theta 1 * X1 let's say now on\nTheta 0 + Theta 1 * X1 let's say now on top of this on top of this I have to\ntop of this on top of this I have to\ntop of this on top of this I have to apply something on top of this value I\napply something on top of this value I\napply something on top of this value I have to apply something so that I can\nhave to apply something so that I can\nhave to apply something so that I can make this line straight instead of just\nmake this line straight instead of just\nmake this line straight instead of just expanding in this way so my hypothesis\nexpanding in this way so my hypothesis\nexpanding in this way so my hypothesis will basically be now G of G is\nwill basically be now G of G is\nwill basically be now G of G is basically a function on Theta 0 and\nbasically a function on Theta 0 and\nbasically a function on Theta 0 and Theta 1 * X1 so here I'm trying to\nTheta 1 * X1 so here I'm trying to\nTheta 1 * X1 so here I'm trying to basically what I'm trying to do I will\nbasically what I'm trying to do I will\nbasically what I'm trying to do I will apply a mathematical formula on top of\napply a mathematical formula on top of\napply a mathematical formula on top of this linear regression to squash this\nthis linear regression to squash this\nthis linear regression to squash this line now let's go ahead and let's try to\nline now let's go ahead and let's try to\nline now let's go ahead and let's try to find out what is this G okay what is\nfind out what is this G okay what is\nfind out what is this G okay what is this G I will say let Z equal to Theta 0\nthis G I will say let Z equal to Theta 0\nthis G I will say let Z equal to Theta 0 + Theta 1 * X I'm just initializing this\n+ Theta 1 * X I'm just initializing this\n+ Theta 1 * X I'm just initializing this now my H Theta of X is nothing but G of\nnow my H Theta of X is nothing but G of\nnow my H Theta of X is nothing but G of Z now we need to understand what is this\nZ now we need to understand what is this\nZ now we need to understand what is this z g of Z and how do we basically specify\nz g of Z and how do we basically specify\nz g of Z and how do we basically specify what is the G function so my G function\nwhat is the G function so my G function\nwhat is the G function so my G function is nothing but H Theta of x equal to 1\nis nothing but H Theta of x equal to 1\nis nothing but H Theta of x equal to 1 by 1 + e ^ of minus Z which in short if\nby 1 + e ^ of minus Z which in short if\nby 1 + e ^ of minus Z which in short if I try to initialize Zed now it is 1 ^ of\nI try to initialize Zed now it is 1 ^ of\nI try to initialize Zed now it is 1 ^ of e ^ of minus Theta 0 + Theta 1 * X so\ne ^ of minus Theta 0 + Theta 1 * X so\ne ^ of minus Theta 0 + Theta 1 * X so this is what is my H Theta of X which is\nthis is what is my H Theta of X which is\nthis is what is my H Theta of X which is my hypothesis and this obviously works\nmy hypothesis and this obviously works\nmy hypothesis and this obviously works well because it is being able to squash\nwell because it is being able to squash\nwell because it is being able to squash the function so this is basically my\nthe function so this is basically my\nthe function so this is basically my hypothesis which I am definitely trying\nhypothesis which I am definitely trying\nhypothesis which I am definitely trying to use it and this function that you are\nto use it and this function that you are\nto use it and this function that you are actually able to see is called as\nactually able to see is called as\nactually able to see is called as sigmoid or logistic function now you\nsigmoid or logistic function now you\nsigmoid or logistic function now you need to understand what does this\nneed to understand what does this\nneed to understand what does this sigmoid function look like in graph in\nsigmoid function look like in graph in\nsigmoid function look like in graph in graph it looks something like this so\ngraph it looks something like this so\ngraph it looks something like this so this this is my Zed value and this is my\nthis this is my Zed value and this is my\nthis this is my Zed value and this is my G of Z this is my 05 your sigmoid\nG of Z this is my 05 your sigmoid\nG of Z this is my 05 your sigmoid function will have this curve so this is\nfunction will have this curve so this is\nfunction will have this curve so this is your one this is zero your value when\nyour one this is zero your value when\nyour one this is zero your value when now from this we can make a lot of\nnow from this we can make a lot of\nnow from this we can make a lot of assumptions what are the assumptions\nassumptions what are the assumptions\nassumptions what are the assumptions that we can basically make your G of Zed\nthat we can basically make your G of Zed\nthat we can basically make your G of Zed your G of Zed is greater than or equal\nyour G of Zed is greater than or equal\nyour G of Zed is greater than or equal to\nto\nto 5.5 is obviously greater than or equal\n5.5 is obviously greater than or equal\n5.5 is obviously greater than or equal to 0.5 when your Zed value is greater\nto 0.5 when your Zed value is greater\nto 0.5 when your Zed value is greater than or equal to zero this is the major\nthan or equal to zero this is the major\nthan or equal to zero this is the major assumptions that we can basically make\nassumptions that we can basically make\nassumptions that we can basically make that is whenever your G of Z is greater\nthat is whenever your G of Z is greater\nthat is whenever your G of Z is greater than your G of Z is greater than or\nthan your G of Z is greater than or\nthan your G of Z is greater than or equal to 0.5 whenever your Zed is\nequal to 0.5 whenever your Zed is\nequal to 0.5 whenever your Zed is greater than or equal to Z so obviously\ngreater than or equal to Z so obviously\ngreater than or equal to Z so obviously whenever your Zed value is greater than\nwhenever your Zed value is greater than\nwhenever your Zed value is greater than Z it is greater than 0.5 if your Zed\nZ it is greater than 0.5 if your Zed\nZ it is greater than 0.5 if your Zed value is less than zero what it will\nvalue is less than zero what it will\nvalue is less than zero what it will become it will basically be less than\nbecome it will basically be less than\nbecome it will basically be less than 0.5 so you can write that specific\n0.5 so you can write that specific\n0.5 so you can write that specific condition also you want so this is the\ncondition also you want so this is the\ncondition also you want so this is the most important condition\nmost important condition\nmost important condition over here why it is called as logistic\nover here why it is called as logistic\nover here why it is called as logistic regression see guys with the help of\nregression see guys with the help of\nregression see guys with the help of regression you creating this straight\nregression you creating this straight\nregression you creating this straight line and with the help of the concept of\nline and with the help of the concept of\nline and with the help of the concept of sigmo you are able to squash it so they\nsigmo you are able to squash it so they\nsigmo you are able to squash it so they have probably combined that name and uh\nhave probably combined that name and uh\nhave probably combined that name and uh basically have written in this way will\nbasically have written in this way will\nbasically have written in this way will squashing of the best fit L line help to\nsquashing of the best fit L line help to\nsquashing of the best fit L line help to overcome the outlier issues yes\novercome the outlier issues yes\novercome the outlier issues yes obviously it'll be able to help you so\nobviously it'll be able to help you so\nobviously it'll be able to help you so let's go ahead and let's try to solve\nlet's go ahead and let's try to solve\nlet's go ahead and let's try to solve the problem statement now usually let's\nthe problem statement now usually let's\nthe problem statement now usually let's consider my training set let's consider\nconsider my training set let's consider\nconsider my training set let's consider my training set suppose I have some\nmy training set suppose I have some\nmy training set suppose I have some training points like this x of 1 comma y\ntraining points like this x of 1 comma y\ntraining points like this x of 1 comma y of 1\nof 1\nof 1 let's say x of 2A y of 2 okay X of 3A y\nlet's say x of 2A y of 2 okay X of 3A y\nlet's say x of 2A y of 2 okay X of 3A y of 3 like this I have lot of training\nof 3 like this I have lot of training\nof 3 like this I have lot of training points and finally X of n comma y of n\npoints and finally X of n comma y of n\npoints and finally X of n comma y of n let's say that this is my training data\nlet's say that this is my training data\nlet's say that this is my training data so here uh my y y will belong to what\nso here uh my y y will belong to what\nso here uh my y y will belong to what zero or 1 because I will only have two\nzero or 1 because I will only have two\nzero or 1 because I will only have two outputs since we are solving a binary\noutputs since we are solving a binary\noutputs since we are solving a binary classification problem here is my\nclassification problem here is my\nclassification problem here is my training set with two outputs and I hope\ntraining set with two outputs and I hope\ntraining set with two outputs and I hope everybody knows about J Theta of Z\neverybody knows about J Theta of Z\neverybody knows about J Theta of Z it is nothing but 1 + e ^ of minus Z\nit is nothing but 1 + e ^ of minus Z\nit is nothing but 1 + e ^ of minus Z here your Z is nothing but Theta 0 +\nhere your Z is nothing but Theta 0 +\nhere your Z is nothing but Theta 0 + Theta 1 * X1 so this is your Theta 0 now\nTheta 1 * X1 so this is your Theta 0 now\nTheta 1 * X1 so this is your Theta 0 now what we have to do we have to select\nwhat we have to do we have to select\nwhat we have to do we have to select this Theta now in this particular case\nthis Theta now in this particular case\nthis Theta now in this particular case let's consider that my Theta 0 is 0\nlet's consider that my Theta 0 is 0\nlet's consider that my Theta 0 is 0 because it is passing through the origin\nbecause it is passing through the origin\nbecause it is passing through the origin just for time pass sake suppose my Z is\njust for time pass sake suppose my Z is\njust for time pass sake suppose my Z is Theta 1 into X so now I need to change\nTheta 1 into X so now I need to change\nTheta 1 into X so now I need to change what is my parameter my parameter is\nwhat is my parameter my parameter is\nwhat is my parameter my parameter is Theta 1\nTheta 1\nTheta 1 I have to change parameter Theta 1 in\nI have to change parameter Theta 1 in\nI have to change parameter Theta 1 in such a way that I get the best fit line\nsuch a way that I get the best fit line\nsuch a way that I get the best fit line and along that I apply this sigmoid\nand along that I apply this sigmoid\nand along that I apply this sigmoid activation function now let's go ahead\nactivation function now let's go ahead\nactivation function now let's go ahead and let's first of all Define our cost\nand let's first of all Define our cost\nand let's first of all Define our cost function because for this we definitely\nfunction because for this we definitely\nfunction because for this we definitely require our cost\nrequire our cost\nrequire our cost function now everything will be same\nfunction now everything will be same\nfunction now everything will be same obviously you know the cost function of\nobviously you know the cost function of\nobviously you know the cost function of linear regression because the first best\nlinear regression because the first best\nlinear regression because the first best fit line that you are probably creating\nfit line that you are probably creating\nfit line that you are probably creating is with the help of linear\nis with the help of linear\nis with the help of linear regression now in this particular case\nregression now in this particular case\nregression now in this particular case in the case of linear regression so here\nin the case of linear regression so here\nin the case of linear regression so here you can basically write J J of theta 1\nyou can basically write J J of theta 1\nyou can basically write J J of theta 1 is nothing but 1 by m summation of I = 1\nis nothing but 1 by m summation of I = 1\nis nothing but 1 by m summation of I = 1 2 m 1X 2 and here you have H Theta of x\n2 m 1X 2 and here you have H Theta of x\n2 m 1X 2 and here you have H Theta of x minus y of I I whole Square so this is\nminus y of I I whole Square so this is\nminus y of I I whole Square so this is your entire thing of if you remember\nyour entire thing of if you remember\nyour entire thing of if you remember linear regression whatever things we\nlinear regression whatever things we\nlinear regression whatever things we have discussed yesterday okay so this is\nhave discussed yesterday okay so this is\nhave discussed yesterday okay so this is the cost function let's consider that\nthe cost function let's consider that\nthe cost function let's consider that for linear regression for this is for\nfor linear regression for this is for\nfor linear regression for this is for the linear regression now for the\nthe linear regression now for the\nthe linear regression now for the logistic regression what will happen for\nlogistic regression what will happen for\nlogistic regression what will happen for your logistic regression I will take the\nyour logistic regression I will take the\nyour logistic regression I will take the same cost function H Theta of X now you\nsame cost function H Theta of X now you\nsame cost function H Theta of X now you know what is s Theta of X it is nothing\nknow what is s Theta of X it is nothing\nknow what is s Theta of X it is nothing but 1 + 1 + e ^ of minus Theta 0 + Theta\nbut 1 + 1 + e ^ of minus Theta 0 + Theta\nbut 1 + 1 + e ^ of minus Theta 0 + Theta sorry Theta 1 multiplied by X right this\nsorry Theta 1 multiplied by X right this\nsorry Theta 1 multiplied by X right this is my with respect to logistic\nis my with respect to logistic\nis my with respect to logistic regression this is my entire equation\nregression this is my entire equation\nregression this is my entire equation now similarly I will try to only put\nnow similarly I will try to only put\nnow similarly I will try to only put this H Theta of X let's consider that\nthis H Theta of X let's consider that\nthis H Theta of X let's consider that this is my cost function only only my H\nthis is my cost function only only my H\nthis is my cost function only only my H Theta of X is changing in this\nTheta of X is changing in this\nTheta of X is changing in this particular case so if I go ahead and\nparticular case so if I go ahead and\nparticular case so if I go ahead and write my cost function I can basically\nwrite my cost function I can basically\nwrite my cost function I can basically say 1x2 h Theta of X of i - y of\nsay 1x2 h Theta of X of i - y of\nsay 1x2 h Theta of X of i - y of iÂ² and in this particular scenario what\niÂ² and in this particular scenario what\niÂ² and in this particular scenario what is h Theta of X it is nothing but 1 + 1\nis h Theta of X it is nothing but 1 + 1\nis h Theta of X it is nothing but 1 + 1 + e ^ minus Theta 1 x so this is what\n+ e ^ minus Theta 1 x so this is what\n+ e ^ minus Theta 1 x so this is what this is getting replaced and this is my\nthis is getting replaced and this is my\nthis is getting replaced and this is my logistic regression cost function I'm\nlogistic regression cost function I'm\nlogistic regression cost function I'm just considering this cost function part\njust considering this cost function part\njust considering this cost function part this part later on if you replace this\nthis part later on if you replace this\nthis part later on if you replace this to this see if I replace this to this\nto this see if I replace this to this\nto this see if I replace this to this and if I replace this to this it becomes\nand if I replace this to this it becomes\nand if I replace this to this it becomes a logistic regression cost function\na logistic regression cost function\na logistic regression cost function intercept I'm considering it as zero\nintercept I'm considering it as zero\nintercept I'm considering it as zero guys now when I'm replacing this to this\nguys now when I'm replacing this to this\nguys now when I'm replacing this to this this to this then it becomes a logistic\nthis to this then it becomes a logistic\nthis to this then it becomes a logistic uh regression cost function but there is\nuh regression cost function but there is\nuh regression cost function but there is one problem we cannot we cannot use we\none problem we cannot we cannot use we\none problem we cannot we cannot use we cannot use this cost function there is a\ncannot use this cost function there is a\ncannot use this cost function there is a reason for this because this equation\nreason for this because this equation\nreason for this because this equation that you're seeing 1/ 1 + e^ of minus\nthat you're seeing 1/ 1 + e^ of minus\nthat you're seeing 1/ 1 + e^ of minus Theta 1 * X this is a non-convex\nTheta 1 * X this is a non-convex\nTheta 1 * X this is a non-convex function now you may be considering what\nfunction now you may be considering what\nfunction now you may be considering what is a non-convex function so let me write\nis a non-convex function so let me write\nis a non-convex function so let me write it down so here this this term this\nit down so here this this term this\nit down so here this this term this terminology right it is a non-convex\nterminology right it is a non-convex\nterminology right it is a non-convex function now what is this non-convex\nfunction now what is this non-convex\nfunction now what is this non-convex function let me show you and let me\nfunction let me show you and let me\nfunction let me show you and let me differentiate it with convex function\ndifferentiate it with convex function\ndifferentiate it with convex function okay we'll try to understand what is the\nokay we'll try to understand what is the\nokay we'll try to understand what is the difference between non-convex function\ndifference between non-convex function\ndifference between non-convex function and convex function this is related to\nand convex function this is related to\nand convex function this is related to gradient descent very important this is\ngradient descent very important this is\ngradient descent very important this is related to gradient desent if you\nrelated to gradient desent if you\nrelated to gradient desent if you remember with the help of linear\nremember with the help of linear\nremember with the help of linear regression whatever gradient Dent we are\nregression whatever gradient Dent we are\nregression whatever gradient Dent we are actually getting it is a convex function\nactually getting it is a convex function\nactually getting it is a convex function like this this is the convex function\nlike this this is the convex function\nlike this this is the convex function which looks like a parabola curve\nwhich looks like a parabola curve\nwhich looks like a parabola curve Parabola curve because of this Parabola\nParabola curve because of this Parabola\nParabola curve because of this Parabola curve whenever we use this linear\ncurve whenever we use this linear\ncurve whenever we use this linear regression cost function specifically\nregression cost function specifically\nregression cost function specifically because here my H Theta of X is what it\nbecause here my H Theta of X is what it\nbecause here my H Theta of X is what it is nothing but Theta 0 + Theta 1 into X\nis nothing but Theta 0 + Theta 1 into X\nis nothing but Theta 0 + Theta 1 into X because of this this equ\nbecause of this this equ\nbecause of this this equ will always give you a parabola curve\nwill always give you a parabola curve\nwill always give you a parabola curve this kind of cost function or convex\nthis kind of cost function or convex\nthis kind of cost function or convex function you can say but here your s\nfunction you can say but here your s\nfunction you can say but here your s Theta of X is changing so in the case of\nTheta of X is changing so in the case of\nTheta of X is changing so in the case of if I use that cost function you will be\nif I use that cost function you will be\nif I use that cost function you will be getting some curves which looks like\ngetting some curves which looks like\ngetting some curves which looks like this now what is the problem with this\nthis now what is the problem with this\nthis now what is the problem with this curve here you have lot of local Minima\ncurve here you have lot of local Minima\ncurve here you have lot of local Minima if local Minima is there you will never\nif local Minima is there you will never\nif local Minima is there you will never reach This Global Minima so that is the\nreach This Global Minima so that is the\nreach This Global Minima so that is the reason we cannot use that c function now\nreason we cannot use that c function now\nreason we cannot use that c function now mathematically you can also go and\nmathematically you can also go and\nmathematically you can also go and probably search in the Google what is\nprobably search in the Google what is\nprobably search in the Google what is the\nthe\nthe what is the graph or what is a convex or\nwhat is the graph or what is a convex or\nwhat is the graph or what is a convex or non-convex function but always remember\nnon-convex function but always remember\nnon-convex function but always remember whenever we updates Theta 1 with this\nwhenever we updates Theta 1 with this\nwhenever we updates Theta 1 with this within this particular equation by\nwithin this particular equation by\nwithin this particular equation by finding the slope then this way it will\nfinding the slope then this way it will\nfinding the slope then this way it will not be differentiable and here you have\nnot be differentiable and here you have\nnot be differentiable and here you have lot of local Minima and because of this\nlot of local Minima and because of this\nlot of local Minima and because of this local Minima you will never be able to\nlocal Minima you will never be able to\nlocal Minima you will never be able to reach the global Minima this is your\nreach the global Minima this is your\nreach the global Minima this is your Global Minima right in case\nGlobal Minima right in case\nGlobal Minima right in case of in case of linear regression you'll\nof in case of linear regression you'll\nof in case of linear regression you'll reach This Global Minima but in this\nreach This Global Minima but in this\nreach This Global Minima but in this case you will never reach never never\ncase you will never reach never never\ncase you will never reach never never you'll be stuck over here or you may get\nyou'll be stuck over here or you may get\nyou'll be stuck over here or you may get stuck over here you may get stuck over\nstuck over here you may get stuck over\nstuck over here you may get stuck over here okay so this has a local Minima\nhere okay so this has a local Minima\nhere okay so this has a local Minima problem so how do we solve this\nproblem so how do we solve this\nproblem so how do we solve this understand in local Minima these are my\nunderstand in local Minima these are my\nunderstand in local Minima these are my points right I have to come over here\npoints right I have to come over here\npoints right I have to come over here this is my deepest point in this\nthis is my deepest point in this\nthis is my deepest point in this particular case I don't have any local\nparticular case I don't have any local\nparticular case I don't have any local Minima now in local Minima also you'll\nMinima now in local Minima also you'll\nMinima now in local Minima also you'll get slope is equal to Z so that is the\nget slope is equal to Z so that is the\nget slope is equal to Z so that is the reason your Theta 1 will never get\nreason your Theta 1 will never get\nreason your Theta 1 will never get updated so in order to solve this\nupdated so in order to solve this\nupdated so in order to solve this problem you can see this diagram we have\nproblem you can see this diagram we have\nproblem you can see this diagram we have something called as logistic regression\nsomething called as logistic regression\nsomething called as logistic regression cost function so I can now write my\ncost function so I can now write my\ncost function so I can now write my logistic regression cost function in a\nlogistic regression cost function in a\nlogistic regression cost function in a different way so this researcher\ndifferent way so this researcher\ndifferent way so this researcher researcher thought of it and basically\nresearcher thought of it and basically\nresearcher thought of it and basically came up with this proposal that the\ncame up with this proposal that the\ncame up with this proposal that the logistic cost function should look\nlogistic cost function should look\nlogistic cost function should look something like this so the entire cost\nsomething like this so the entire cost\nsomething like this so the entire cost function of logistic regression that is\nfunction of logistic regression that is\nfunction of logistic regression that is specifically H Theta of X of I comma y\nspecifically H Theta of X of I comma y\nspecifically H Theta of X of I comma y this should be written something like\nthis should be written something like\nthis should be written something like this and it should be written like this\nthis and it should be written like this\nthis and it should be written like this see here I'm just going to write cost\nsee here I'm just going to write cost\nsee here I'm just going to write cost function of J of theta 1 let's say that\nfunction of J of theta 1 let's say that\nfunction of J of theta 1 let's say that I'm writing J of theta 1 okay so J of\nI'm writing J of theta 1 okay so J of\nI'm writing J of theta 1 okay so J of theta 1 what are the different different\ntheta 1 what are the different different\ntheta 1 what are the different different output that I'll be getting I'll be get\noutput that I'll be getting I'll be get\noutput that I'll be getting I'll be get I'll be getting yal 1 or y equal to 0 So\nI'll be getting yal 1 or y equal to 0 So\nI'll be getting yal 1 or y equal to 0 So based on this two scenarios our cost\nbased on this two scenarios our cost\nbased on this two scenarios our cost function will look something like this\nfunction will look something like this\nfunction will look something like this minus log of H of theta of X and I know\nminus log of H of theta of X and I know\nminus log of H of theta of X and I know I hope you all know what is h Theta of x\nI hope you all know what is h Theta of x\nI hope you all know what is h Theta of x h Theta of X is nothing but 1 + 1 ^ of -\nh Theta of X is nothing but 1 + 1 ^ of -\nh Theta of X is nothing but 1 + 1 ^ of - Theta 1 x so this is what is my H Theta\nTheta 1 x so this is what is my H Theta\nTheta 1 x so this is what is my H Theta of X and whenever Y is Zer then you\nof X and whenever Y is Zer then you\nof X and whenever Y is Zer then you basically have minus log * 1 - H Theta\nbasically have minus log * 1 - H Theta\nbasically have minus log * 1 - H Theta of X of I of I okay so this is how you\nof X of I of I okay so this is how you\nof X of I of I okay so this is how you basically write your cost function in\nbasically write your cost function in\nbasically write your cost function in this particular scenario now with the\nthis particular scenario now with the\nthis particular scenario now with the help of this cost function it is always\nhelp of this cost function it is always\nhelp of this cost function it is always possible since it is getting log log is\npossible since it is getting log log is\npossible since it is getting log log is basically getting used in this scenario\nbasically getting used in this scenario\nbasically getting used in this scenario you'll always get a global Minima that\nyou'll always get a global Minima that\nyou'll always get a global Minima that is the reason why they have completely\nis the reason why they have completely\nis the reason why they have completely neglected this cost function and utiliz\nneglected this cost function and utiliz\nneglected this cost function and utiliz this cost function now what does this\nthis cost function now what does this\nthis cost function now what does this cost function basically mean two\ncost function basically mean two\ncost function basically mean two scenarios if Y is equal to 1 Let's\nscenarios if Y is equal to 1 Let's\nscenarios if Y is equal to 1 Let's consider this is my cost function\nconsider this is my cost function\nconsider this is my cost function graph I have H Theta of X and you know\ngraph I have H Theta of X and you know\ngraph I have H Theta of X and you know that H Theta of x value will be ranging\nthat H Theta of x value will be ranging\nthat H Theta of x value will be ranging between 0 to 1 since it is a\nbetween 0 to 1 since it is a\nbetween 0 to 1 since it is a classification problem so it will be\nclassification problem so it will be\nclassification problem so it will be ranging between 0 to 1 and this is\nranging between 0 to 1 and this is\nranging between 0 to 1 and this is basically of J of theta 1 which is my\nbasically of J of theta 1 which is my\nbasically of J of theta 1 which is my cost function so if Y is equal to 1 this\ncost function so if Y is equal to 1 this\ncost function so if Y is equal to 1 this specific equation will be used and\nspecific equation will be used and\nspecific equation will be used and whenever this equation is is basically\nwhenever this equation is is basically\nwhenever this equation is is basically used you get a you get a curve see minus\nused you get a you get a curve see minus\nused you get a you get a curve see minus log s of X of I you get a curve which\nlog s of X of I you get a curve which\nlog s of X of I you get a curve which looks something like this okay which\nlooks something like this okay which\nlooks something like this okay which you'll get a curve which looks like this\nyou'll get a curve which looks like this\nyou'll get a curve which looks like this now what does this curve basically\nnow what does this curve basically\nnow what does this curve basically specify the curve come up with two\nspecify the curve come up with two\nspecify the curve come up with two assumptions the cost will be zero if Y\nassumptions the cost will be zero if Y\nassumptions the cost will be zero if Y is = 1 and H Theta of x equal to 1 that\nis = 1 and H Theta of x equal to 1 that\nis = 1 and H Theta of x equal to 1 that basically when your s Theta of X is 1\nbasically when your s Theta of X is 1\nbasically when your s Theta of X is 1 and the Y is output is one that\nand the Y is output is one that\nand the Y is output is one that basically means you're going to assign\nbasically means you're going to assign\nbasically means you're going to assign over here one right so in this\nover here one right so in this\nover here one right so in this particular case you will be seeing that\nparticular case you will be seeing that\nparticular case you will be seeing that your cost function will be zero cost is\nyour cost function will be zero cost is\nyour cost function will be zero cost is zero so here is my zero it is meeting\nzero so here is my zero it is meeting\nzero so here is my zero it is meeting over here if you of x equal to 1 and Y\nover here if you of x equal to 1 and Y\nover here if you of x equal to 1 and Y is equal to 1 so this is this is again a\nis equal to 1 so this is this is again a\nis equal to 1 so this is this is again a convex function only then the next point\nconvex function only then the next point\nconvex function only then the next point that you can probably discuss over here\nthat you can probably discuss over here\nthat you can probably discuss over here is with respect to Y is equal to 0 if\nis with respect to Y is equal to 0 if\nis with respect to Y is equal to 0 if your Y is Z then what kind of curve you\nyour Y is Z then what kind of curve you\nyour Y is Z then what kind of curve you will be getting you'll get a different\nwill be getting you'll get a different\nwill be getting you'll get a different kind of curve which will look like this\nkind of curve which will look like this\nkind of curve which will look like this H Theta of x here your value will be 0\nH Theta of x here your value will be 0\nH Theta of x here your value will be 0 to one and here you'll be having a curve\nto one and here you'll be having a curve\nto one and here you'll be having a curve which looks like this so when you\nwhich looks like this so when you\nwhich looks like this so when you combine this two you'll be able to see\ncombine this two you'll be able to see\ncombine this two you'll be able to see that you are able to get a kind of\nthat you are able to get a kind of\nthat you are able to get a kind of gradient descent so this will definitely\ngradient descent so this will definitely\ngradient descent so this will definitely help us to create a cost function so I\nhelp us to create a cost function so I\nhelp us to create a cost function so I hope everybody is able to understand\nhope everybody is able to understand\nhope everybody is able to understand till here with respect to this and this\ntill here with respect to this and this\ntill here with respect to this and this will definitely work so finally I can\nwill definitely work so finally I can\nwill definitely work so finally I can also write my cost function in a\nalso write my cost function in a\nalso write my cost function in a different way the cost function that I\ndifferent way the cost function that I\ndifferent way the cost function that I will probably write over here so this\nwill probably write over here so this\nwill probably write over here so this will be my J of theta 1\nwill be my J of theta 1\nwill be my J of theta 1 so I can come up with a cost function\nso I can come up with a cost function\nso I can come up with a cost function which looks like this\nwhich looks like this\nwhich looks like this cost of H of theta of X of I comma Yus\ncost of H of theta of X of I comma Yus\ncost of H of theta of X of I comma Yus log of H Theta of x if Y is equal\nlog of H Theta of x if Y is equal\nlog of H Theta of x if Y is equal 1 and then minus\n1 and then minus\n1 and then minus log 1 - H Theta of x if Y is equal\nlog 1 - H Theta of x if Y is equal\nlog 1 - H Theta of x if Y is equal 0 now I can combine this both and\n0 now I can combine this both and\n0 now I can combine this both and probably write something like like this\nprobably write something like like this\nprobably write something like like this I can combine this both and I can\nI can combine this both and I can\nI can combine this both and I can basically write cost of H Theta of X of\nbasically write cost of H Theta of X of\nbasically write cost of H Theta of X of IA Y is equal to - y log H Theta of X of\nIA Y is equal to - y log H Theta of X of\nIA Y is equal to - y log H Theta of X of I minus log 1 -\nI minus log 1 -\nI minus log 1 - y okay 1 - y log of 1 - H Theta of X so\ny okay 1 - y log of 1 - H Theta of X so\ny okay 1 - y log of 1 - H Theta of X so this will be my final cost\nthis will be my final cost\nthis will be my final cost function and here also you can see that\nfunction and here also you can see that\nfunction and here also you can see that if I\nif I\nif I replace if I replace y with one then\nreplace if I replace y with one then\nreplace if I replace y with one then what will remain only this particular\nwhat will remain only this particular\nwhat will remain only this particular value will remain right this value when\nvalue will remain right this value when\nvalue will remain right this value when Y is equal to 1 this thing only will\nY is equal to 1 this thing only will\nY is equal to 1 this thing only will come you see over here replace y with\ncome you see over here replace y with\ncome you see over here replace y with one probably replace y with one and then\none probably replace y with one and then\none probably replace y with one and then you'll be able to see so here I can now\nyou'll be able to see so here I can now\nyou'll be able to see so here I can now write if Y is equal to 1 my cost\nwrite if Y is equal to 1 my cost\nwrite if Y is equal to 1 my cost function will Rook something like this\nfunction will Rook something like this\nfunction will Rook something like this which is nothing\nwhich is nothing\nwhich is nothing but see Y is 1 then what will happen my\nbut see Y is 1 then what will happen my\nbut see Y is 1 then what will happen my log of H Theta of X of I will come and\nlog of H Theta of X of I will come and\nlog of H Theta of X of I will come and this 1 - 1 is 0 so 0 multili by anything\nthis 1 - 1 is 0 so 0 multili by anything\nthis 1 - 1 is 0 so 0 multili by anything will be 0 if Y is equal to 0 then what\nwill be 0 if Y is equal to 0 then what\nwill be 0 if Y is equal to 0 then what will happen my cost function will be so\nwill happen my cost function will be so\nwill happen my cost function will be so when it is zero this will - y will\nwhen it is zero this will - y will\nwhen it is zero this will - y will become 0 0 multili by anything is z so\nbecome 0 0 multili by anything is z so\nbecome 0 0 multili by anything is z so here you'll be able to see that I am\nhere you'll be able to see that I am\nhere you'll be able to see that I am I'll be having minus log 1 - H Theta of\nI'll be having minus log 1 - H Theta of\nI'll be having minus log 1 - H Theta of x i so this both the condition has been\nx i so this both the condition has been\nx i so this both the condition has been proved by this cost function\nproved by this cost function\nproved by this cost function so this is my cost function yes cost\nso this is my cost function yes cost\nso this is my cost function yes cost function and loss function with respect\nfunction and loss function with respect\nfunction and loss function with respect to the number of parameters will be\nto the number of parameters will be\nto the number of parameters will be almost same so finally if I try to write\nalmost same so finally if I try to write\nalmost same so finally if I try to write J of theta because I have that 1X 2 m\nJ of theta because I have that 1X 2 m\nJ of theta because I have that 1X 2 m also right so 1X 2 m also I have so what\nalso right so 1X 2 m also I have so what\nalso right so 1X 2 m also I have so what I'm actually going to do here you will\nI'm actually going to do here you will\nI'm actually going to do here you will be able to see that I can write J of\nbe able to see that I can write J of\nbe able to see that I can write J of theta 1 is equal to 1 by 2 m summation\ntheta 1 is equal to 1 by 2 m summation\ntheta 1 is equal to 1 by 2 m summation of IAL 1 to M and then write down the\nof IAL 1 to M and then write down the\nof IAL 1 to M and then write down the entire equation that you have probably\nentire equation that you have probably\nentire equation that you have probably over here so here you have minus y or I\nover here so here you have minus y or I\nover here so here you have minus y or I I'll just remove this minus and put it\nI'll just remove this minus and put it\nI'll just remove this minus and put it over here and this will become plus\nover here and this will become plus\nover here and this will become plus sorry y of I\nsorry y of I\nsorry y of I * log H Theta of X of I 1 - y of i y\n* log H Theta of X of I 1 - y of i y\n* log H Theta of X of I 1 - y of i y log 1 - H Theta of X of I so this\nlog 1 - H Theta of X of I so this\nlog 1 - H Theta of X of I so this becomes my entire first function and\nbecomes my entire first function and\nbecomes my entire first function and obviously you know what is h thet of x H\nobviously you know what is h thet of x H\nobviously you know what is h thet of x H Theta of X of I is nothing but 1 + 1 e^\nTheta of X of I is nothing but 1 + 1 e^\nTheta of X of I is nothing but 1 + 1 e^ minus Theta 1 * X and finally my\nminus Theta 1 * X and finally my\nminus Theta 1 * X and finally my convergence algorithm I have to repeat\nconvergence algorithm I have to repeat\nconvergence algorithm I have to repeat this to update Theta 1 repeat until this\nthis to update Theta 1 repeat until this\nthis to update Theta 1 repeat until this updation that is Theta Theta\nupdation that is Theta Theta\nupdation that is Theta Theta J is equal to Theta J minus learning\nJ is equal to Theta J minus learning\nJ is equal to Theta J minus learning rate derivative with respect to Theta J\nrate derivative with respect to Theta J\nrate derivative with respect to Theta J and this will be my J of theta 1 this is\nand this will be my J of theta 1 this is\nand this will be my J of theta 1 this is my repeat until conversion so this is my\nmy repeat until conversion so this is my\nmy repeat until conversion so this is my cost function this is my repeat\ncost function this is my repeat\ncost function this is my repeat algorithm and here I will be updating my\nalgorithm and here I will be updating my\nalgorithm and here I will be updating my entire Theta\nentire Theta\nentire Theta 1 and this solves your problem with\n1 and this solves your problem with\n1 and this solves your problem with respect to logistic regression simple\nrespect to logistic regression simple\nrespect to logistic regression simple simple questions may come like how it is\nsimple questions may come like how it is\nsimple questions may come like how it is different from linear regression how it\ndifferent from linear regression how it\ndifferent from linear regression how it is not different from linear regression\nis not different from linear regression\nis not different from linear regression can we say log likelihood a topic from\ncan we say log likelihood a topic from\ncan we say log likelihood a topic from probabilistic yes this is uh this is log\nprobabilistic yes this is uh this is log\nprobabilistic yes this is uh this is log likelihood if now I will discuss about\nlikelihood if now I will discuss about\nlikelihood if now I will discuss about performance metrics and this is specific\nperformance metrics and this is specific\nperformance metrics and this is specific to classification problem and binary\nto classification problem and binary\nto classification problem and binary classification I'm talking let's\nclassification I'm talking let's\nclassification I'm talking let's consider let's consider I have a data\nconsider let's consider I have a data\nconsider let's consider I have a data set which has X1 X2 and this is y and\nset which has X1 X2 and this is y and\nset which has X1 X2 and this is y and obviously in logistic uh classification\nobviously in logistic uh classification\nobviously in logistic uh classification you have outputs like 0 1 0 1 1 0 1 and\nyou have outputs like 0 1 0 1 1 0 1 and\nyou have outputs like 0 1 0 1 1 0 1 and your y hat y hat is basically the output\nyour y hat y hat is basically the output\nyour y hat y hat is basically the output of the predicted model now in this\nof the predicted model now in this\nof the predicted model now in this particular scenario my y hat will\nparticular scenario my y hat will\nparticular scenario my y hat will probably be 1 1 0 uh 1 1 1 Z so in this\nprobably be 1 1 0 uh 1 1 1 Z so in this\nprobably be 1 1 0 uh 1 1 1 Z so in this particular scenario this is my predicted\nparticular scenario this is my predicted\nparticular scenario this is my predicted output and this is my actual output so\noutput and this is my actual output so\noutput and this is my actual output so can we come to some kind of conclusions\ncan we come to some kind of conclusions\ncan we come to some kind of conclusions wherein probably we will be able to\nwherein probably we will be able to\nwherein probably we will be able to identify what may be the accuracy of\nidentify what may be the accuracy of\nidentify what may be the accuracy of this specific model with respect to this\nthis specific model with respect to this\nthis specific model with respect to this many data points because confusion\nmany data points because confusion\nmany data points because confusion Matrix is all dealt with this is called\nMatrix is all dealt with this is called\nMatrix is all dealt with this is called as we will first of all have to create a\nas we will first of all have to create a\nas we will first of all have to create a confusion Matrix now for a binary\nconfusion Matrix now for a binary\nconfusion Matrix now for a binary classification problem the confusion\nclassification problem the confusion\nclassification problem the confusion Matrix will look like this so here you\nMatrix will look like this so here you\nMatrix will look like this so here you have 1 0 1 0 Let's say that this is\nhave 1 0 1 0 Let's say that this is\nhave 1 0 1 0 Let's say that this is prediction let's say that these are my\nprediction let's say that these are my\nprediction let's say that these are my actual value and these are my prediction\nactual value and these are my prediction\nactual value and these are my prediction value okay these both are prediction\nvalue okay these both are prediction\nvalue okay these both are prediction value these are my output value when my\nvalue these are my output value when my\nvalue these are my output value when my actual value is zero my predicted value\nactual value is zero my predicted value\nactual value is zero my predicted value is one does this what does this mean\nis one does this what does this mean\nis one does this what does this mean wrong prediction right so when my actual\nwrong prediction right so when my actual\nwrong prediction right so when my actual value is zero my predicted value is 1 so\nvalue is zero my predicted value is 1 so\nvalue is zero my predicted value is 1 so here my count will increase to one let's\nhere my count will increase to one let's\nhere my count will increase to one let's go to the second scenario when the\ngo to the second scenario when the\ngo to the second scenario when the actual value is one and my predicted\nactual value is one and my predicted\nactual value is one and my predicted value is one that basically means one\nvalue is one that basically means one\nvalue is one that basically means one and one so here I'm going to increase my\nand one so here I'm going to increase my\nand one so here I'm going to increase my count similarly when my actual value is\ncount similarly when my actual value is\ncount similarly when my actual value is zero my predicted value is zero so that\nzero my predicted value is zero so that\nzero my predicted value is zero so that basically mean when my actual value is z\nbasically mean when my actual value is z\nbasically mean when my actual value is z my predicted value is zero I'm going to\nmy predicted value is zero I'm going to\nmy predicted value is zero I'm going to increase the count by one if I go over\nincrease the count by one if I go over\nincrease the count by one if I go over here 1 one again it is so instead of\nhere 1 one again it is so instead of\nhere 1 one again it is so instead of writing one now this will become two I'm\nwriting one now this will become two I'm\nwriting one now this will become two I'm going to increase the count similarly\ngoing to increase the count similarly\ngoing to increase the count similarly I'll go over here one more one is there\nI'll go over here one more one is there\nI'll go over here one more one is there so I'm going to increase the count three\nso I'm going to increase the count three\nso I'm going to increase the count three then I have 01 01 basically means when\nthen I have 01 01 basically means when\nthen I have 01 01 basically means when my actual value is zero I'm actually\nmy actual value is zero I'm actually\nmy actual value is zero I'm actually getting it as one so I'm also going to\ngetting it as one so I'm also going to\ngetting it as one so I'm also going to increase this particular value as two\nincrease this particular value as two\nincrease this particular value as two and then finally I have 1 and zero where\nand then finally I have 1 and zero where\nand then finally I have 1 and zero where I'm going to increase like this now what\nI'm going to increase like this now what\nI'm going to increase like this now what does this basically mean now what does\ndoes this basically mean now what does\ndoes this basically mean now what does this basically mean see with respect to\nthis basically mean see with respect to\nthis basically mean see with respect to this kind of predictions whenever we are\nthis kind of predictions whenever we are\nthis kind of predictions whenever we are discussing this basically basically says\ndiscussing this basically basically says\ndiscussing this basically basically says so this is my actual values and I have Z\nso this is my actual values and I have Z\nso this is my actual values and I have Z 1 and zero and this is my predicted\n1 and zero and this is my predicted\n1 and zero and this is my predicted values I also have 1 and zero this value\nvalues I also have 1 and zero this value\nvalues I also have 1 and zero this value when one and one are there this is\nwhen one and one are there this is\nwhen one and one are there this is called as true positive this value when\ncalled as true positive this value when\ncalled as true positive this value when 0 and Zer are there this is called as\n0 and Zer are there this is called as\n0 and Zer are there this is called as false negative whenever your actual\nfalse negative whenever your actual\nfalse negative whenever your actual value is zero and you have predicted one\nvalue is zero and you have predicted one\nvalue is zero and you have predicted one this becomes false positive and whenever\nthis becomes false positive and whenever\nthis becomes false positive and whenever your actual value is one you have\nyour actual value is one you have\nyour actual value is one you have predicted zero this becomes false\npredicted zero this becomes false\npredicted zero this becomes false negative now coming to this I really\nnegative now coming to this I really\nnegative now coming to this I really need to find out the accuracy of this\nneed to find out the accuracy of this\nneed to find out the accuracy of this model now if I really want to find out\nmodel now if I really want to find out\nmodel now if I really want to find out and this is what is called as confusion\nand this is what is called as confusion\nand this is what is called as confusion Matrix now in this confusion Matrix if I\nMatrix now in this confusion Matrix if I\nMatrix now in this confusion Matrix if I really want to find out the accuracy the\nreally want to find out the accuracy the\nreally want to find out the accuracy the accuracy of this model it is very much\naccuracy of this model it is very much\naccuracy of this model it is very much simple this middle elements that you are\nsimple this middle elements that you are\nsimple this middle elements that you are able to see will basically give us the\nable to see will basically give us the\nable to see will basically give us the right output so this and this if I add\nright output so this and this if I add\nright output so this and this if I add it up it will give us the right output\nit up it will give us the right output\nit up it will give us the right output so here I'm going to get TP + TN divided\nso here I'm going to get TP + TN divided\nso here I'm going to get TP + TN divided by TP + FP + FN + TN so once I calculate\nby TP + FP + FN + TN so once I calculate\nby TP + FP + FN + TN so once I calculate this so I have 3 + 1\nthis so I have 3 + 1\nthis so I have 3 + 1 / 3 + 2 + 1 + 1 so this is nothing but 4\n/ 3 + 2 + 1 + 1 so this is nothing but 4\n/ 3 + 2 + 1 + 1 so this is nothing but 4 by 7 what is 4 by\nby 7 what is 4 by\nby 7 what is 4 by 757 so am I getting 57 percentage\n757 so am I getting 57 percentage\n757 so am I getting 57 percentage accuracy so I'm actually getting 57%\naccuracy so I'm actually getting 57%\naccuracy so I'm actually getting 57% accuracy over here with respect to the\naccuracy over here with respect to the\naccuracy over here with respect to the accuracy so this is how we basically\naccuracy so this is how we basically\naccuracy so this is how we basically calculate with respect to basic accuracy\ncalculate with respect to basic accuracy\ncalculate with respect to basic accuracy with the help of uh the confusion Matrix\nwith the help of uh the confusion Matrix\nwith the help of uh the confusion Matrix okay so this is specifically called as\nokay so this is specifically called as\nokay so this is specifically called as confusion Matrix now there are some more\nconfusion Matrix now there are some more\nconfusion Matrix now there are some more things that you really need to specify\nthings that you really need to specify\nthings that you really need to specify always remember our model aim should be\nalways remember our model aim should be\nalways remember our model aim should be that we should try to reduce false\nthat we should try to reduce false\nthat we should try to reduce false positive and false negative now let's\npositive and false negative now let's\npositive and false negative now let's say that I want to discuss about two\nsay that I want to discuss about two\nsay that I want to discuss about two topics what one is suppose in our data\ntopics what one is suppose in our data\ntopics what one is suppose in our data set I have zeros and one category let's\nset I have zeros and one category let's\nset I have zeros and one category let's say in my output if I say Zer are 900\nsay in my output if I say Zer are 900\nsay in my output if I say Zer are 900 and ones are 100 this becomes an\nand ones are 100 this becomes an\nand ones are 100 this becomes an imbalanced data very clear right so this\nimbalanced data very clear right so this\nimbalanced data very clear right so this become an imbalanced data set it is a\nbecome an imbalanced data set it is a\nbecome an imbalanced data set it is a biased data suppose if I say zeros are\nbiased data suppose if I say zeros are\nbiased data suppose if I say zeros are probab\nprobab\nprobab 600 and ones are probably 400 in this\n600 and ones are probably 400 in this\n600 and ones are probably 400 in this particular scenario I will say that this\nparticular scenario I will say that this\nparticular scenario I will say that this is the balance data because yes you have\nis the balance data because yes you have\nis the balance data because yes you have 100 less but it's okay the it may not\n100 less but it's okay the it may not\n100 less but it's okay the it may not impact many of the algorithm now see\nimpact many of the algorithm now see\nimpact many of the algorithm now see guys most of the algorithm that we will\nguys most of the algorithm that we will\nguys most of the algorithm that we will be probably discussing imbalanced if we\nbe probably discussing imbalanced if we\nbe probably discussing imbalanced if we have an imbalanced data set it will\nhave an imbalanced data set it will\nhave an imbalanced data set it will obviously affect the algorithms let me\nobviously affect the algorithms let me\nobviously affect the algorithms let me talk about this let's say that I have\ntalk about this let's say that I have\ntalk about this let's say that I have number of zeros as 900 and number of\nnumber of zeros as 900 and number of\nnumber of zeros as 900 and number of ones is 100 now let's say that my model\nones is 100 now let's say that my model\nones is 100 now let's say that my model I have created which will directly\nI have created which will directly\nI have created which will directly predict\npredict\npredict zero it'll I'll just say that all my\nzero it'll I'll just say that all my\nzero it'll I'll just say that all my inputs that it is probably getting with\ninputs that it is probably getting with\ninputs that it is probably getting with respect to this training data it'll just\nrespect to this training data it'll just\nrespect to this training data it'll just output zero now in this particular\noutput zero now in this particular\noutput zero now in this particular scenario what will be my accuracy my\nscenario what will be my accuracy my\nscenario what will be my accuracy my accuracy will be 900 divid by 1,000\naccuracy will be 900 divid by 1,000\naccuracy will be 900 divid by 1,000 right so this is nothing but 90% so is\nright so this is nothing but 90% so is\nright so this is nothing but 90% so is this a good\nthis a good\nthis a good accuracy obviously it is a good accuracy\naccuracy obviously it is a good accuracy\naccuracy obviously it is a good accuracy but this is a biased data if my model is\nbut this is a biased data if my model is\nbut this is a biased data if my model is basically just outputting 00000000 0 if\nbasically just outputting 00000000 0 if\nbasically just outputting 00000000 0 if it is outputting 00 00 0 obviously most\nit is outputting 00 00 0 obviously most\nit is outputting 00 00 0 obviously most of the answer will be zeros but this\nof the answer will be zeros but this\nof the answer will be zeros but this will be a scenario like you know where\nwill be a scenario like you know where\nwill be a scenario like you know where it is just outputting one thing then\nit is just outputting one thing then\nit is just outputting one thing then also it is able to get 90% accuracy so\nalso it is able to get 90% accuracy so\nalso it is able to get 90% accuracy so you should only not be dependent on\nyou should only not be dependent on\nyou should only not be dependent on accuracy so there are lot of\naccuracy so there are lot of\naccuracy so there are lot of terminologies that we will basically use\nterminologies that we will basically use\nterminologies that we will basically use one terminology that we specifically use\none terminology that we specifically use\none terminology that we specifically use is something called as Precision then\nis something called as Precision then\nis something called as Precision then we'll also use recall what is precision\nwe'll also use recall what is precision\nwe'll also use recall what is precision what is recall I'll write the formula\nwhat is recall I'll write the formula\nwhat is recall I'll write the formula over here in Precision what do we need\nover here in Precision what do we need\nover here in Precision what do we need to focus and then finally we will\nto focus and then finally we will\nto focus and then finally we will discuss about f score so we have to use\ndiscuss about f score so we have to use\ndiscuss about f score so we have to use different kind of parametrics of sorry\ndifferent kind of parametrics of sorry\ndifferent kind of parametrics of sorry different kind of formulas whenever you\ndifferent kind of formulas whenever you\ndifferent kind of formulas whenever you have an imbalanced data set you can also\nhave an imbalanced data set you can also\nhave an imbalanced data set you can also do oversampling but again understand in\ndo oversampling but again understand in\ndo oversampling but again understand in most of the scenarios in some of the\nmost of the scenarios in some of the\nmost of the scenarios in some of the scenarios oversampling may work but we\nscenarios oversampling may work but we\nscenarios oversampling may work but we have to focus on the type of performance\nhave to focus on the type of performance\nhave to focus on the type of performance metrics that we are focusing on right\nmetrics that we are focusing on right\nmetrics that we are focusing on right now I'll not say F1 score I'll say F\nnow I'll not say F1 score I'll say F\nnow I'll not say F1 score I'll say F score the reason why I'm saying I'll\nscore the reason why I'm saying I'll\nscore the reason why I'm saying I'll just let you know so let's talk about\njust let you know so let's talk about\njust let you know so let's talk about recall recall formula is basically given\nrecall recall formula is basically given\nrecall recall formula is basically given by true positive divided by true\nby true positive divided by true\nby true positive divided by true positive plus false negative\npositive plus false negative\npositive plus false negative Precision is given by true positive\nPrecision is given by true positive\nPrecision is given by true positive divided by true positive plus false\ndivided by true positive plus false\ndivided by true positive plus false positive and then I will probably\npositive and then I will probably\npositive and then I will probably discuss about F sore also or we\ndiscuss about F sore also or we\ndiscuss about F sore also or we basically say fbaa also now I'll just\nbasically say fbaa also now I'll just\nbasically say fbaa also now I'll just draw this confusion Matrix again okay\ndraw this confusion Matrix again okay\ndraw this confusion Matrix again okay which is having true positive true\nwhich is having true positive true\nwhich is having true positive true negative so let me draw it over here so\nnegative so let me draw it over here so\nnegative so let me draw it over here so this is my ones and zeros these are my\nthis is my ones and zeros these are my\nthis is my ones and zeros these are my actual values and these are my predicted\nactual values and these are my predicted\nactual values and these are my predicted values I have true positive I have true\nvalues I have true positive I have true\nvalues I have true positive I have true negative false positive and false\nnegative false positive and false\nnegative false positive and false negative now in this particular scenario\nnegative now in this particular scenario\nnegative now in this particular scenario when I'm actually discussing understand\nwhen I'm actually discussing understand\nwhen I'm actually discussing understand what is recall and what focus it is\nwhat is recall and what focus it is\nwhat is recall and what focus it is basically given on so here whenever I\nbasically given on so here whenever I\nbasically given on so here whenever I talk about recall recall basically says\ntalk about recall recall basically says\ntalk about recall recall basically says that TP TP divided by TP plus FN so I'm\nthat TP TP divided by TP plus FN so I'm\nthat TP TP divided by TP plus FN so I'm actually focusing on this so what does\nactually focusing on this so what does\nactually focusing on this so what does this basically say true uh recall out of\nthis basically say true uh recall out of\nthis basically say true uh recall out of all the actual true positives how many\nall the actual true positives how many\nall the actual true positives how many have been predicted correctly that is\nhave been predicted correctly that is\nhave been predicted correctly that is basically mentioned by TP out of all the\nbasically mentioned by TP out of all the\nbasically mentioned by TP out of all the positive values how many of them have\npositive values how many of them have\npositive values how many of them have predicted as positive so this is what it\npredicted as positive so this is what it\npredicted as positive so this is what it is basically saying and this scenario is\nis basically saying and this scenario is\nis basically saying and this scenario is called as recall in this the false\ncalled as recall in this the false\ncalled as recall in this the false negative is basically given more\nnegative is basically given more\nnegative is basically given more priority and our focus should be that we\npriority and our focus should be that we\npriority and our focus should be that we should try to reduce false positive\nshould try to reduce false positive\nshould try to reduce false positive false negative sorry we should try to\nfalse negative sorry we should try to\nfalse negative sorry we should try to reduce this now let's go ahead and let's\nreduce this now let's go ahead and let's\nreduce this now let's go ahead and let's discuss about Precision in Precision\ndiscuss about Precision in Precision\ndiscuss about Precision in Precision what we are doing we are basically\nwhat we are doing we are basically\nwhat we are doing we are basically taking out of all the predicted values\ntaking out of all the predicted values\ntaking out of all the predicted values out of all the predicted positive values\nout of all the predicted positive values\nout of all the predicted positive values how many of them are actual true or\nhow many of them are actual true or\nhow many of them are actual true or positive okay this is what Precision\npositive okay this is what Precision\npositive okay this is what Precision basically means now suppose if I\nbasically means now suppose if I\nbasically means now suppose if I consider spam classification suppose\nconsider spam classification suppose\nconsider spam classification suppose this is my task tell me in this\nthis is my task tell me in this\nthis is my task tell me in this particular case should we use Precision\nparticular case should we use Precision\nparticular case should we use Precision or recall and one more use case I'm\nor recall and one more use case I'm\nor recall and one more use case I'm saying that whether the person has\nsaying that whether the person has\nsaying that whether the person has cancer or not in which case we have to\ncancer or not in which case we have to\ncancer or not in which case we have to support recall and in which case we have\nsupport recall and in which case we have\nsupport recall and in which case we have to go ahead with Precision has cancer or\nto go ahead with Precision has cancer or\nto go ahead with Precision has cancer or not in spam what is important okay guys\nnot in spam what is important okay guys\nnot in spam what is important okay guys the recall is also called as true\nthe recall is also called as true\nthe recall is also called as true positive rate I can also say recall as\npositive rate I can also say recall as\npositive rate I can also say recall as sensitivity so if I go with Spam\nsensitivity so if I go with Spam\nsensitivity so if I go with Spam classification it should definitely go\nclassification it should definitely go\nclassification it should definitely go with Precision why it should go with\nwith Precision why it should go with\nwith Precision why it should go with Precision if I probably get a Spam ma\nPrecision if I probably get a Spam ma\nPrecision if I probably get a Spam ma the main aim should be that whenever I\nthe main aim should be that whenever I\nthe main aim should be that whenever I get a Spam Mill it should be identified\nget a Spam Mill it should be identified\nget a Spam Mill it should be identified as spam okay in that specific scenario\nas spam okay in that specific scenario\nas spam okay in that specific scenario my positive false positive we should try\nmy positive false positive we should try\nmy positive false positive we should try to reduce and in this scenario my false\nto reduce and in this scenario my false\nto reduce and in this scenario my false pository talks about the spam\npository talks about the spam\npository talks about the spam classification a lot in a better way in\nclassification a lot in a better way in\nclassification a lot in a better way in the case of cancer I should definitely\nthe case of cancer I should definitely\nthe case of cancer I should definitely use recall let's let's focus on the\nuse recall let's let's focus on the\nuse recall let's let's focus on the recall formula tp/ by TP plus FN if a\nrecall formula tp/ by TP plus FN if a\nrecall formula tp/ by TP plus FN if a person has a cancer see one actually he\nperson has a cancer see one actually he\nperson has a cancer see one actually he has a cancer it should be predicted as\nhas a cancer it should be predicted as\nhas a cancer it should be predicted as one otherwise if we have FN it is\none otherwise if we have FN it is\none otherwise if we have FN it is basically predicting it does not have a\nbasically predicting it does not have a\nbasically predicting it does not have a cancer that is really a big situation in\ncancer that is really a big situation in\ncancer that is really a big situation in this case if a person does not have a\nthis case if a person does not have a\nthis case if a person does not have a Cancer and if he's predict if the model\nCancer and if he's predict if the model\nCancer and if he's predict if the model predicts okay fine he has a cancer he\npredicts okay fine he has a cancer he\npredicts okay fine he has a cancer he may go and further do the test and then\nmay go and further do the test and then\nmay go and further do the test and then he'll come to know whether he has a\nhe'll come to know whether he has a\nhe'll come to know whether he has a cancer or not but this scenario is very\ncancer or not but this scenario is very\ncancer or not but this scenario is very dangerous if a person has a cancer but\ndangerous if a person has a cancer but\ndangerous if a person has a cancer but he is being indicated that he does not\nhe is being indicated that he does not\nhe is being indicated that he does not have that cancer\nhave that cancer\nhave that cancer so here false negative is given more\nso here false negative is given more\nso here false negative is given more priority over here in the case of spam\npriority over here in the case of spam\npriority over here in the case of spam classification false positive is given\nclassification false positive is given\nclassification false positive is given more priority so this is something\nmore priority so this is something\nmore priority so this is something important over here and you really need\nimportant over here and you really need\nimportant over here and you really need to understand with respect to different\nto understand with respect to different\nto understand with respect to different different problem statement let me give\ndifferent problem statement let me give\ndifferent problem statement let me give you one more example tomorrow the stock\nyou one more example tomorrow the stock\nyou one more example tomorrow the stock market is going to crash in this what we\nmarket is going to crash in this what we\nmarket is going to crash in this what we need to focus on should we focus on\nneed to focus on should we focus on\nneed to focus on should we focus on Precision or should we focus on recall\nPrecision or should we focus on recall\nPrecision or should we focus on recall now here two things are there who is\nnow here two things are there who is\nnow here two things are there who is solving what kind of problem see many\nsolving what kind of problem see many\nsolving what kind of problem see many people will say recall or Precision but\npeople will say recall or Precision but\npeople will say recall or Precision but here two things are there on whose point\nhere two things are there on whose point\nhere two things are there on whose point of view you are creating this model are\nof view you are creating this model are\nof view you are creating this model are you creating this model for the industry\nyou creating this model for the industry\nyou creating this model for the industry or are you creating this model for the\nor are you creating this model for the\nor are you creating this model for the people for the people he should\npeople for the people he should\npeople for the people he should definitely get identified that okay in\ndefinitely get identified that okay in\ndefinitely get identified that okay in this particular scenario you need to\nthis particular scenario you need to\nthis particular scenario you need to sell your stock because tomorrow stock\nsell your stock because tomorrow stock\nsell your stock because tomorrow stock market is going to crash but for\nmarket is going to crash but for\nmarket is going to crash but for companies this is very bad okay I hope\ncompanies this is very bad okay I hope\ncompanies this is very bad okay I hope everybody is able to understand for\neverybody is able to understand for\neverybody is able to understand for companies it is very very bad so in this\ncompanies it is very very bad so in this\ncompanies it is very very bad so in this particular case sometime we need to\nparticular case sometime we need to\nparticular case sometime we need to focus both on false positive and false\nfocus both on false positive and false\nfocus both on false positive and false negative and again I'm telling you for\nnegative and again I'm telling you for\nnegative and again I'm telling you for which problem statement you are solving\nwhich problem statement you are solving\nwhich problem statement you are solving that indicates if you are solving for\nthat indicates if you are solving for\nthat indicates if you are solving for people then they should be able to get\npeople then they should be able to get\npeople then they should be able to get the notification saying that it is going\nthe notification saying that it is going\nthe notification saying that it is going to crash if you're probably uh doing it\nto crash if you're probably uh doing it\nto crash if you're probably uh doing it for companies at that time your\nfor companies at that time your\nfor companies at that time your Precision recall may change but if I\nPrecision recall may change but if I\nPrecision recall may change but if I consider for both the scenarios at that\nconsider for both the scenarios at that\nconsider for both the scenarios at that point of time I will definitely use\npoint of time I will definitely use\npoint of time I will definitely use something called as F score F score or\nsomething called as F score F score or\nsomething called as F score F score or I'll also say it as F beta now how is\nI'll also say it as F beta now how is\nI'll also say it as F beta now how is fbaa Formula given as I will talk about\nfbaa Formula given as I will talk about\nfbaa Formula given as I will talk about it and here in the F score you have\nit and here in the F score you have\nit and here in the F score you have three different formulas the first\nthree different formulas the first\nthree different formulas the first Formula I will say basically as when\nFormula I will say basically as when\nFormula I will say basically as when your beta value is 1 okay first of all\nyour beta value is 1 okay first of all\nyour beta value is 1 okay first of all I'll just give a generic definition of f\nI'll just give a generic definition of f\nI'll just give a generic definition of f s or F beta here you are basically going\ns or F beta here you are basically going\ns or F beta here you are basically going to consider 1 + beta squ Precision\nto consider 1 + beta squ Precision\nto consider 1 + beta squ Precision multiplied by recall divided beta Square\nmultiplied by recall divided beta Square\nmultiplied by recall divided beta Square * Precision plus recall whenever your\n* Precision plus recall whenever your\n* Precision plus recall whenever your both false positive and false negative\nboth false positive and false negative\nboth false positive and false negative are important we select beta as one so\nare important we select beta as one so\nare important we select beta as one so if I select beta as 1 it becomes 1 + 4\nif I select beta as 1 it becomes 1 + 4\nif I select beta as 1 it becomes 1 + 4 Precision multiplied by recall then you\nPrecision multiplied by recall then you\nPrecision multiplied by recall then you have Precision plus recall so here sorry\nhave Precision plus recall so here sorry\nhave Precision plus recall so here sorry 1 + 1 so this becomes 2 multiplied by\n1 + 1 so this becomes 2 multiplied by\n1 + 1 so this becomes 2 multiplied by Precision into recall divided by\nPrecision into recall divided by\nPrecision into recall divided by Precision plus recall so here you have\nPrecision plus recall so here you have\nPrecision plus recall so here you have this is basically called as harmonic\nthis is basically called as harmonic\nthis is basically called as harmonic mean harmonic mean probably you have\nmean harmonic mean probably you have\nmean harmonic mean probably you have seen this kind of equation where you\nseen this kind of equation where you\nseen this kind of equation where you have written 2x y / x + y same type you\nhave written 2x y / x + y same type you\nhave written 2x y / x + y same type you are able to see this this is called as\nare able to see this this is called as\nare able to see this this is called as harmonic mean here the focus is on both\nharmonic mean here the focus is on both\nharmonic mean here the focus is on both false positive and false negative let's\nfalse positive and false negative let's\nfalse positive and false negative let's say that your false positive is more\nsay that your false positive is more\nsay that your false positive is more important than false negative at that\nimportant than false negative at that\nimportant than false negative at that point of time you will try to decrease\npoint of time you will try to decrease\npoint of time you will try to decrease or you will try to decrease your beta\nor you will try to decrease your beta\nor you will try to decrease your beta value let's say that I'm decreasing my\nvalue let's say that I'm decreasing my\nvalue let's say that I'm decreasing my Beta value to 0.5 then what will happen\nBeta value to 0.5 then what will happen\nBeta value to 0.5 then what will happen 1 +5 whole\n1 +5 whole\n1 +5 whole s and then you have P * R Precision\ns and then you have P * R Precision\ns and then you have P * R Precision recall and here also you have 25 p + r\nrecall and here also you have 25 p + r\nrecall and here also you have 25 p + r now in this particular scenario I'm\nnow in this particular scenario I'm\nnow in this particular scenario I'm decreasing my Beta decreasing the beta\ndecreasing my Beta decreasing the beta\ndecreasing my Beta decreasing the beta basically means that you are providing\nbasically means that you are providing\nbasically means that you are providing more importance to false positive than\nmore importance to false positive than\nmore importance to false positive than false negative and finally you'll be\nfalse negative and finally you'll be\nfalse negative and finally you'll be able to see that if I consider beta\nable to see that if I consider beta\nable to see that if I consider beta value as let me just say my notes if I\nvalue as let me just say my notes if I\nvalue as let me just say my notes if I consider beta value as two that\nconsider beta value as two that\nconsider beta value as two that basically means you are giving more\nbasically means you are giving more\nbasically means you are giving more importance to false negative than false\nimportance to false negative than false\nimportance to false negative than false positive so with this specific case you\npositive so with this specific case you\npositive so with this specific case you can come up to a conclusion what value\ncan come up to a conclusion what value\ncan come up to a conclusion what value you basically want to use now whenever I\nyou basically want to use now whenever I\nyou basically want to use now whenever I use beta is equal to 1 it becomes fub1\nuse beta is equal to 1 it becomes fub1\nuse beta is equal to 1 it becomes fub1 score if I use beta as .5 then this\nscore if I use beta as .5 then this\nscore if I use beta as .5 then this basically becomes f.5 score and this\nbasically becomes f.5 score and this\nbasically becomes f.5 score and this becomes your F2 score So based on which\nbecomes your F2 score So based on which\nbecomes your F2 score So based on which is important okay which is important\nis important okay which is important\nis important okay which is important whether your Precision or false positive\nwhether your Precision or false positive\nwhether your Precision or false positive or false negative is important you can\nor false negative is important you can\nor false negative is important you can consider those things F score will have\nconsider those things F score will have\nconsider those things F score will have different values if you're using beta is\ndifferent values if you're using beta is\ndifferent values if you're using beta is equal to 1 that basically means you are\nequal to 1 that basically means you are\nequal to 1 that basically means you are giving importance to both precision and\ngiving importance to both precision and\ngiving importance to both precision and recall if your false positive is more\nrecall if your false positive is more\nrecall if your false positive is more important then at that point of time you\nimportant then at that point of time you\nimportant then at that point of time you reduce beta value if false negative is\nreduce beta value if false negative is\nreduce beta value if false negative is greater than false bet uh false positive\ngreater than false bet uh false positive\ngreater than false bet uh false positive then your beta value is\nthen your beta value is\nthen your beta value is increasing beta is a deciding parameter\nincreasing beta is a deciding parameter\nincreasing beta is a deciding parameter to decide your F1 score or F2 score or F\nto decide your F1 score or F2 score or F\nto decide your F1 score or F2 score or F Point score now first thing first what\nPoint score now first thing first what\nPoint score now first thing first what is the agenda of today's session first\nis the agenda of today's session first\nis the agenda of today's session first of all we will complete practicals for\nof all we will complete practicals for\nof all we will complete practicals for all the algorithms that we have\nall the algorithms that we have\nall the algorithms that we have discussed these all algorithms that we\ndiscussed these all algorithms that we\ndiscussed these all algorithms that we have discussed we will cover the\nhave discussed we will cover the\nhave discussed we will cover the practicals probably we will be doing\npracticals probably we will be doing\npracticals probably we will be doing hyper parameter tuning everything the\nhyper parameter tuning everything the\nhyper parameter tuning everything the second thing and again here we are going\nsecond thing and again here we are going\nsecond thing and again here we are going to take just simple examples so yes uh\nto take just simple examples so yes uh\nto take just simple examples so yes uh so today's session I said practicals\nso today's session I said practicals\nso today's session I said practicals with simple examples where I'll probably\nwith simple examples where I'll probably\nwith simple examples where I'll probably discuss about all the hyper parameter\ndiscuss about all the hyper parameter\ndiscuss about all the hyper parameter tuning then the second one the second\ntuning then the second one the second\ntuning then the second one the second algorithm that I'm going to discuss\nalgorithm that I'm going to discuss\nalgorithm that I'm going to discuss about is something called as n bias this\nabout is something called as n bias this\nabout is something called as n bias this is a classification algorithm so we are\nis a classification algorithm so we are\nis a classification algorithm so we are going to understand the intuition and\ngoing to understand the intuition and\ngoing to understand the intuition and the third one that we are going to\nthe third one that we are going to\nthe third one that we are going to probably discusses KNN algorithm so KNN\nprobably discusses KNN algorithm so KNN\nprobably discusses KNN algorithm so KNN algorithms is definitely there\nalgorithms is definitely there\nalgorithms is definitely there so this our today's plan I know I've\nso this our today's plan I know I've\nso this our today's plan I know I've written very less but this much maths\nwritten very less but this much maths\nwritten very less but this much maths and involved in na bias right we'll\nand involved in na bias right we'll\nand involved in na bias right we'll understand the probability theorem again\nunderstand the probability theorem again\nunderstand the probability theorem again over there there is something called as\nover there there is something called as\nover there there is something called as bias theorem we'll try to understand and\nbias theorem we'll try to understand and\nbias theorem we'll try to understand and then we'll try to solve a problem on\nthen we'll try to solve a problem on\nthen we'll try to solve a problem on that so let's proceed and let's enjoy\nthat so let's proceed and let's enjoy\nthat so let's proceed and let's enjoy today's session how do we enjoy first of\ntoday's session how do we enjoy first of\ntoday's session how do we enjoy first of all we enjoy by creating a practical\nall we enjoy by creating a practical\nall we enjoy by creating a practical problem so I am actually opening a\nproblem so I am actually opening a\nproblem so I am actually opening a notebook file in front of you so here uh\nnotebook file in front of you so here uh\nnotebook file in front of you so here uh we will try to Sol solve it with the\nwe will try to Sol solve it with the\nwe will try to Sol solve it with the help of linear regression Ridge lasso\nhelp of linear regression Ridge lasso\nhelp of linear regression Ridge lasso and try to solve some problems let's see\nand try to solve some problems let's see\nand try to solve some problems let's see how much we will be able to solve it but\nhow much we will be able to solve it but\nhow much we will be able to solve it but again the aim is that we learn in a\nagain the aim is that we learn in a\nagain the aim is that we learn in a better way okay uh so that everybody\nbetter way okay uh so that everybody\nbetter way okay uh so that everybody understands some basic basic things okay\nunderstands some basic basic things okay\nunderstands some basic basic things okay so first of all as usual uh everybody\nso first of all as usual uh everybody\nso first of all as usual uh everybody open your jupyter notebook file the\nopen your jupyter notebook file the\nopen your jupyter notebook file the first algorithm that I'm going to\nfirst algorithm that I'm going to\nfirst algorithm that I'm going to discuss about is something called as SK\ndiscuss about is something called as SK\ndiscuss about is something called as SK learn linear regression so everybody I\nlearn linear regression so everybody I\nlearn linear regression so everybody I hope everybody knows about this SK learn\nhope everybody knows about this SK learn\nhope everybody knows about this SK learn let's see what all things are basically\nlet's see what all things are basically\nlet's see what all things are basically there in this we will be using fit\nthere in this we will be using fit\nthere in this we will be using fit intercept everything as such but here\nintercept everything as such but here\nintercept everything as such but here the main aim is to find out the\nthe main aim is to find out the\nthe main aim is to find out the coefficients which is basically\ncoefficients which is basically\ncoefficients which is basically indicated by Theta 0 Theta 1 and all the\nindicated by Theta 0 Theta 1 and all the\nindicated by Theta 0 Theta 1 and all the first thing we'll start with linear\nfirst thing we'll start with linear\nfirst thing we'll start with linear regression and then we will go ahead and\nregression and then we will go ahead and\nregression and then we will go ahead and discuss with r and lassor I'm just going\ndiscuss with r and lassor I'm just going\ndiscuss with r and lassor I'm just going to make this as\nto make this as\nto make this as markdown how many different libraries of\nmarkdown how many different libraries of\nmarkdown how many different libraries of for linear regression you can do with\nfor linear regression you can do with\nfor linear regression you can do with stats you can do with skyi you can do\nstats you can do with skyi you can do\nstats you can do with skyi you can do with many things okay so first thing\nwith many things okay so first thing\nwith many things okay so first thing first let's first of all we require a\nfirst let's first of all we require a\nfirst let's first of all we require a data set so for the data set what we are\ndata set so for the data set what we are\ndata set so for the data set what we are going to do is that we are going to\ngoing to do is that we are going to\ngoing to do is that we are going to basically take up some smaller smaller\nbasically take up some smaller smaller\nbasically take up some smaller smaller data just let me do this so for this uh\ndata just let me do this so for this uh\ndata just let me do this so for this uh we are going to take the house pricing\nwe are going to take the house pricing\nwe are going to take the house pricing data set so we are going to solve house\ndata set so we are going to solve house\ndata set so we are going to solve house pricing data set problem a simple data\npricing data set problem a simple data\npricing data set problem a simple data set which is already present in SK learn\nset which is already present in SK learn\nset which is already present in SK learn only now in order to import the data set\nonly now in order to import the data set\nonly now in order to import the data set I will write a line of code which is\nI will write a line of code which is\nI will write a line of code which is like from SK learn dot data sets data\nlike from SK learn dot data sets data\nlike from SK learn dot data sets data sets\nsets\nsets import load uncore Boston so we have\nimport load uncore Boston so we have\nimport load uncore Boston so we have some Boston house pricing data set so\nsome Boston house pricing data set so\nsome Boston house pricing data set so I'm just going to execute this I'm also\nI'm just going to execute this I'm also\nI'm just going to execute this I'm also going to make a lot of Sals so that I\ngoing to make a lot of Sals so that I\ngoing to make a lot of Sals so that I don't have to again go ahead and create\ndon't have to again go ahead and create\ndon't have to again go ahead and create all the sales again some basic libraries\nall the sales again some basic libraries\nall the sales again some basic libraries that I probably want is pro import numai\nthat I probably want is pro import numai\nthat I probably want is pro import numai as\nas\nas NP\nNP\nNP import pandas\nimport pandas\nimport pandas SPD okay import cbon as\nSPD okay import cbon as\nSPD okay import cbon as SNS and then I will also import Matt\nSNS and then I will also import Matt\nSNS and then I will also import Matt Matt plot lib do p plot as PLT and then\nMatt plot lib do p plot as PLT and then\nMatt plot lib do p plot as PLT and then percentile matplot lib matlot lib do\npercentile matplot lib matlot lib do\npercentile matplot lib matlot lib do inline and I will try to execute this\ninline and I will try to execute this\ninline and I will try to execute this see this my typing speed has become a\nsee this my typing speed has become a\nsee this my typing speed has become a little bit faster by writing by\nlittle bit faster by writing by\nlittle bit faster by writing by executing this queries again and again\nexecuting this queries again and again\nexecuting this queries again and again and uh let's go ahead uh so I have\nand uh let's go ahead uh so I have\nand uh let's go ahead uh so I have imported all the necessary libraries\nimported all the necessary libraries\nimported all the necessary libraries that is required which which will be\nthat is required which which will be\nthat is required which which will be more than sufficient for you all to\nmore than sufficient for you all to\nmore than sufficient for you all to start with now in order to load this\nstart with now in order to load this\nstart with now in order to load this particular data set I will just use this\nparticular data set I will just use this\nparticular data set I will just use this Library called as load uncore Boston and\nLibrary called as load uncore Boston and\nLibrary called as load uncore Boston and I'm going to just initialize this so if\nI'm going to just initialize this so if\nI'm going to just initialize this so if you press shift tab you will be able to\nyou press shift tab you will be able to\nyou press shift tab you will be able to see that return load and return the\nsee that return load and return the\nsee that return load and return the Boston house prices data set it is a\nBoston house prices data set it is a\nBoston house prices data set it is a regression problem it is saying and then\nregression problem it is saying and then\nregression problem it is saying and then probably I'm just going to execute it\nprobably I'm just going to execute it\nprobably I'm just going to execute it now once I execute it I will go and\nnow once I execute it I will go and\nnow once I execute it I will go and probably see the type of DF so it is\nprobably see the type of DF so it is\nprobably see the type of DF so it is basically saying skarn dos. bunch now if\nbasically saying skarn dos. bunch now if\nbasically saying skarn dos. bunch now if I go and probably execute DF you'll be\nI go and probably execute DF you'll be\nI go and probably execute DF you'll be able to see that this will be in the\nable to see that this will be in the\nable to see that this will be in the form of key value pairs okay like Target\nform of key value pairs okay like Target\nform of key value pairs okay like Target is here data is here okay so data is\nis here data is here okay so data is\nis here data is here okay so data is here Target is here and probably you'll\nhere Target is here and probably you'll\nhere Target is here and probably you'll be able to find out feature names is\nbe able to find out feature names is\nbe able to find out feature names is here so we definitely require feature\nhere so we definitely require feature\nhere so we definitely require feature names we require our Target value and\nnames we require our Target value and\nnames we require our Target value and our data value so we really need to\nour data value so we really need to\nour data value so we really need to combine this specific thing in a proper\ncombine this specific thing in a proper\ncombine this specific thing in a proper way in the form of a data frame so that\nway in the form of a data frame so that\nway in the form of a data frame so that you will be able to see so what I'm\nyou will be able to see so what I'm\nyou will be able to see so what I'm actually going to do over here I'm just\nactually going to do over here I'm just\nactually going to do over here I'm just going to say PD do data frame I'll\ngoing to say PD do data frame I'll\ngoing to say PD do data frame I'll convert this entirely into a data frame\nconvert this entirely into a data frame\nconvert this entirely into a data frame and I will say DF do data see this is a\nand I will say DF do data see this is a\nand I will say DF do data see this is a key value pair right so DF do data is\nkey value pair right so DF do data is\nkey value pair right so DF do data is basically giving me all the features\nbasically giving me all the features\nbasically giving me all the features value so if I write DF do data and just\nvalue so if I write DF do data and just\nvalue so if I write DF do data and just execute it you'll be able to see that I\nexecute it you'll be able to see that I\nexecute it you'll be able to see that I you will be able to get my entire data\nyou will be able to get my entire data\nyou will be able to get my entire data set in this way my entire data set in\nset in this way my entire data set in\nset in this way my entire data set in this way this is my feature one feature\nthis way this is my feature one feature\nthis way this is my feature one feature two feature three feature 4 this feature\ntwo feature three feature 4 this feature\ntwo feature three feature 4 this feature 12 I have 12 features over here and\n12 I have 12 features over here and\n12 I have 12 features over here and based on that I have that specific value\nbased on that I have that specific value\nbased on that I have that specific value now the next thing thing that I'm going\nnow the next thing thing that I'm going\nnow the next thing thing that I'm going to do probably I should also be able to\nto do probably I should also be able to\nto do probably I should also be able to add the target feature name over here so\nadd the target feature name over here so\nadd the target feature name over here so what I will do I will just convert this\nwhat I will do I will just convert this\nwhat I will do I will just convert this into DF and then I will also say DF do\ninto DF and then I will also say DF do\ninto DF and then I will also say DF do columns and I'll set it to DF do Target\ncolumns and I'll set it to DF do Target\ncolumns and I'll set it to DF do Target okay and let me change this to data set\nokay and let me change this to data set\nokay and let me change this to data set so I'm going to change this to data set\nso I'm going to change this to data set\nso I'm going to change this to data set and I'm going to say data set. columns\nand I'm going to say data set. columns\nand I'm going to say data set. columns is equal to DF do Target so if I execute\nis equal to DF do Target so if I execute\nis equal to DF do Target so if I execute this and now if I probably\nthis and now if I probably\nthis and now if I probably print my data set do head you will be\nprint my data set do head you will be\nprint my data set do head you will be able to see this specific thing okay it\nable to see this specific thing okay it\nable to see this specific thing okay it is an error let's see expected axis has\nis an error let's see expected axis has\nis an error let's see expected axis has 13 element new values has\n13 element new values has\n13 element new values has 506 so Target okay I should not use\n506 so Target okay I should not use\n506 so Target okay I should not use Target over here instead I had a column\nTarget over here instead I had a column\nTarget over here instead I had a column which is called as features feature\nwhich is called as features feature\nwhich is called as features feature names like if I go and probably see\nnames like if I go and probably see\nnames like if I go and probably see DF DF over here you'll be able to see\nDF DF over here you'll be able to see\nDF DF over here you'll be able to see there is one thing which is called as\nthere is one thing which is called as\nthere is one thing which is called as feature names so I'm going to use DF do\nfeature names so I'm going to use DF do\nfeature names so I'm going to use DF do feature names over here so here it is DF\nfeature names over here so here it is DF\nfeature names over here so here it is DF do feature names I'm just going to paste\ndo feature names I'm just going to paste\ndo feature names I'm just going to paste it over here and now if I go and write\nit over here and now if I go and write\nit over here and now if I go and write here you can see print DF data set. head\nhere you can see print DF data set. head\nhere you can see print DF data set. head if I go and execute without print you'll\nif I go and execute without print you'll\nif I go and execute without print you'll be able to see my entire data set so\nbe able to see my entire data set so\nbe able to see my entire data set so these are my features with respect to\nthese are my features with respect to\nthese are my features with respect to different different things and this is\ndifferent different things and this is\ndifferent different things and this is basically a house pricing data set so\nbasically a house pricing data set so\nbasically a house pricing data set so initially I have this features CRM ZN\ninitially I have this features CRM ZN\ninitially I have this features CRM ZN indust CH nox RM age distance radius tax\nindust CH nox RM age distance radius tax\nindust CH nox RM age distance radius tax PT ratio b l stack that so I have my\nPT ratio b l stack that so I have my\nPT ratio b l stack that so I have my entire data set over here the same data\nentire data set over here the same data\nentire data set over here the same data set I have basically put it over here\nset I have basically put it over here\nset I have basically put it over here now here also you'll be able to see what\nnow here also you'll be able to see what\nnow here also you'll be able to see what all this feature basically means this is\nall this feature basically means this is\nall this feature basically means this is showing wasted weighted distance to five\nshowing wasted weighted distance to five\nshowing wasted weighted distance to five do uh Five Boston employment center rad\ndo uh Five Boston employment center rad\ndo uh Five Boston employment center rad basically means index of accessibility\nbasically means index of accessibility\nbasically means index of accessibility to radial Highway tax basically means\nto radial Highway tax basically means\nto radial Highway tax basically means full value property tax rate this much\nfull value property tax rate this much\nfull value property tax rate this much PT rate basically means pupil teacher\nPT rate basically means pupil teacher\nPT rate basically means pupil teacher ratio I don't know what the hell it\nratio I don't know what the hell it\nratio I don't know what the hell it means but it's fine we have some kind of\nmeans but it's fine we have some kind of\nmeans but it's fine we have some kind of data over here properly in front of you\ndata over here properly in front of you\ndata over here properly in front of you so these are my independent features\nso these are my independent features\nso these are my independent features what are these these all are my\nwhat are these these all are my\nwhat are these these all are my independent features if you want the\nindependent features if you want the\nindependent features if you want the features detail here you can see it\nfeatures detail here you can see it\nfeatures detail here you can see it right everything what is CRM this\nright everything what is CRM this\nright everything what is CRM this basically means per capita crime rate by\nbasically means per capita crime rate by\nbasically means per capita crime rate by town which is important ZN it is\ntown which is important ZN it is\ntown which is important ZN it is proportional of residential land zone\nproportional of residential land zone\nproportional of residential land zone for Lots over 25,000 Square ft so this\nfor Lots over 25,000 Square ft so this\nfor Lots over 25,000 Square ft so this is my DF I did not do much I'm just\nis my DF I did not do much I'm just\nis my DF I did not do much I'm just using data frame DF do data column\nusing data frame DF do data column\nusing data frame DF do data column features name I'm getting this value\nfeatures name I'm getting this value\nfeatures name I'm getting this value very much simple now let's go a little\nvery much simple now let's go a little\nvery much simple now let's go a little bit slowly so that many people will be\nbit slowly so that many people will be\nbit slowly so that many people will be able to also understand now this is my\nable to also understand now this is my\nable to also understand now this is my data set. head now the thing is that I\ndata set. head now the thing is that I\ndata set. head now the thing is that I obviously have taken all these\nobviously have taken all these\nobviously have taken all these particular values but this is my\nparticular values but this is my\nparticular values but this is my independent feature I still have my\nindependent feature I still have my\nindependent feature I still have my dependent feature so what I'm actually\ndependent feature so what I'm actually\ndependent feature so what I'm actually going to do I will create a new feature\ngoing to do I will create a new feature\ngoing to do I will create a new feature which is like data set of price I'll\nwhich is like data set of price I'll\nwhich is like data set of price I'll create my feature name price price of\ncreate my feature name price price of\ncreate my feature name price price of the house and what I will assign this\nthe house and what I will assign this\nthe house and what I will assign this particular value this value will be\nparticular value this value will be\nparticular value this value will be assigned with this target this target\nassigned with this target this target\nassigned with this target this target value this target value is basically the\nvalue this target value is basically the\nvalue this target value is basically the sale the price of the houses right it is\nsale the price of the houses right it is\nsale the price of the houses right it is again in the form of array so I'm going\nagain in the form of array so I'm going\nagain in the form of array so I'm going to take this and put it as a dependent\nto take this and put it as a dependent\nto take this and put it as a dependent feature so here you'll be able to see\nfeature so here you'll be able to see\nfeature so here you'll be able to see that my price will be my dependent\nthat my price will be my dependent\nthat my price will be my dependent feature so here I'll basically write DF\nfeature so here I'll basically write DF\nfeature so here I'll basically write DF do Target so once I execute it and now\ndo Target so once I execute it and now\ndo Target so once I execute it and now if I probably go and see my data set do\nif I probably go and see my data set do\nif I probably go and see my data set do head you'll be able to see features over\nhead you'll be able to see features over\nhead you'll be able to see features over here and one more feature is getting\nhere and one more feature is getting\nhere and one more feature is getting added that is price now this price may\nadded that is price now this price may\nadded that is price now this price may be the units may be in\nbe the units may be in\nbe the units may be in millions somewhere Target should be here\nmillions somewhere Target should be here\nmillions somewhere Target should be here or there it should be probably in\nor there it should be probably in\nor there it should be probably in millions\nmillions\nmillions or I cannot see it but it should be\nor I cannot see it but it should be\nor I cannot see it but it should be somewhere here it should have definitely\nsomewhere here it should have definitely\nsomewhere here it should have definitely said that it is probably in millions or\nsaid that it is probably in millions or\nsaid that it is probably in millions or okay but that is not a problem I think\nokay but that is not a problem I think\nokay but that is not a problem I think but mostly it'll be in millions\nbut mostly it'll be in millions\nbut mostly it'll be in millions somewhere I think it should be\nsomewhere I think it should be\nsomewhere I think it should be here okay I cannot see it but probably\nhere okay I cannot see it but probably\nhere okay I cannot see it but probably if I put more time I'll be able to\nif I put more time I'll be able to\nif I put more time I'll be able to understand it okay so over here what is\nunderstand it okay so over here what is\nunderstand it okay so over here what is the thing main thing this all are my\nthe thing main thing this all are my\nthe thing main thing this all are my independent features and this is my\nindependent features and this is my\nindependent features and this is my dependent feature right so if I'm trying\ndependent feature right so if I'm trying\ndependent feature right so if I'm trying to solve linear regression I have to\nto solve linear regression I have to\nto solve linear regression I have to divide my independent and dependent\ndivide my independent and dependent\ndivide my independent and dependent features properly now let's go to the\nfeatures properly now let's go to the\nfeatures properly now let's go to the next step that\nnext step that\nnext step that is\nis\nis dividing the data\ndividing the data\ndividing the data set dividing the oh my God dividing the\nset dividing the oh my God dividing the\nset dividing the oh my God dividing the data\nset\nset into\ninto\ninto train into first of all I'll try try to\ntrain into first of all I'll try try to\ntrain into first of all I'll try try to divide into\ndivide into\ndivide into independent and dependent\nindependent and dependent\nindependent and dependent features so I want my entire features\nfeatures so I want my entire features\nfeatures so I want my entire features data set divided into independent and\ndata set divided into independent and\ndata set divided into independent and dependent features X I will be using as\ndependent features X I will be using as\ndependent features X I will be using as my independent featur so I will write\nmy independent featur so I will write\nmy independent featur so I will write data set dot I will use an iock which is\ndata set dot I will use an iock which is\ndata set dot I will use an iock which is present in data frames and understand\npresent in data frames and understand\npresent in data frames and understand from which feature to which feature I\nfrom which feature to which feature I\nfrom which feature to which feature I will be taking as my independent feature\nwill be taking as my independent feature\nwill be taking as my independent feature to this feature till lat so the best way\nto this feature till lat so the best way\nto this feature till lat so the best way that basically means that I just need to\nthat basically means that I just need to\nthat basically means that I just need to skip the last feature in order to skip\nskip the last feature in order to skip\nskip the last feature in order to skip the last feature what I'm actually going\nthe last feature what I'm actually going\nthe last feature what I'm actually going to do from all the columns I will just\nto do from all the columns I will just\nto do from all the columns I will just skip the last column so this is how you\nskip the last column so this is how you\nskip the last column so this is how you basically do an indexing with respect to\nbasically do an indexing with respect to\nbasically do an indexing with respect to just skipping the last feature and this\njust skipping the last feature and this\njust skipping the last feature and this will basically be my independent\nwill basically be my independent\nwill basically be my independent features and here I will basically say Y\nfeatures and here I will basically say Y\nfeatures and here I will basically say Y is equal to data set do iock and here I\nis equal to data set do iock and here I\nis equal to data set do iock and here I just want the last feature so I will\njust want the last feature so I will\njust want the last feature so I will write colon all the records I want and\nwrite colon all the records I want and\nwrite colon all the records I want and see the first term that we are probably\nsee the first term that we are probably\nsee the first term that we are probably WR writing over here this basically\nWR writing over here this basically\nWR writing over here this basically specifies with respect to records here\nspecifies with respect to records here\nspecifies with respect to records here this specifies with respect to columns\nthis specifies with respect to columns\nthis specifies with respect to columns from all the columns I'm taking the last\nfrom all the columns I'm taking the last\nfrom all the columns I'm taking the last column here I will just take the last\ncolumn here I will just take the last\ncolumn here I will just take the last column and this will basically be my\ncolumn and this will basically be my\ncolumn and this will basically be my dependent features dependent features so\ndependent features dependent features so\ndependent features dependent features so here I have basically executed now if\nhere I have basically executed now if\nhere I have basically executed now if you can go and probably see x. head here\nyou can go and probably see x. head here\nyou can go and probably see x. head here you'll be able to find all my\nyou'll be able to find all my\nyou'll be able to find all my independent features in y do head you'll\nindependent features in y do head you'll\nindependent features in y do head you'll be able to find the dependent feature\nbe able to find the dependent feature\nbe able to find the dependent feature now let's go to the first algorithm that\nnow let's go to the first algorithm that\nnow let's go to the first algorithm that is called as linear regression\nis called as linear regression\nis called as linear regression always remember whenever I definitely\nalways remember whenever I definitely\nalways remember whenever I definitely start with linear regression I'll\nstart with linear regression I'll\nstart with linear regression I'll definitely not go directly with linear\ndefinitely not go directly with linear\ndefinitely not go directly with linear regression instead what I will do is\nregression instead what I will do is\nregression instead what I will do is that I'll try to go with Ridge\nthat I'll try to go with Ridge\nthat I'll try to go with Ridge regression and uh lasso regression\nregression and uh lasso regression\nregression and uh lasso regression because there you are lot of options\nbecause there you are lot of options\nbecause there you are lot of options with respect to hyper pment T but I'll\nwith respect to hyper pment T but I'll\nwith respect to hyper pment T but I'll just show you how linear regression is\njust show you how linear regression is\njust show you how linear regression is done so basically you really really need\ndone so basically you really really need\ndone so basically you really really need to use a lot of libraries okay over here\nto use a lot of libraries okay over here\nto use a lot of libraries okay over here and based on this libraries this\nand based on this libraries this\nand based on this libraries this libraries will try to install okay and\nlibraries will try to install okay and\nlibraries will try to install okay and what are these libraries these are\nwhat are these libraries these are\nwhat are these libraries these are basically the linear regression Library\nbasically the linear regression Library\nbasically the linear regression Library so here I'm basically going to use two\nso here I'm basically going to use two\nso here I'm basically going to use two specific thing one is linear regression\nspecific thing one is linear regression\nspecific thing one is linear regression Library so I will just use from SK learn\nLibrary so I will just use from SK learn\nLibrary so I will just use from SK learn do linear uncore model import linear\ndo linear uncore model import linear\ndo linear uncore model import linear regression do you need to remember this\nregression do you need to remember this\nregression do you need to remember this the answer is no because I also do the\nthe answer is no because I also do the\nthe answer is no because I also do the Google and I try to find out where in\nGoogle and I try to find out where in\nGoogle and I try to find out where in escal and it is present okay so here is\nescal and it is present okay so here is\nescal and it is present okay so here is my linear regression so I will try to\nmy linear regression so I will try to\nmy linear regression so I will try to initialize linear reg is equal to\ninitialize linear reg is equal to\ninitialize linear reg is equal to initialize with linear regression and\ninitialize with linear regression and\ninitialize with linear regression and then here what I'm actually going to do\nthen here what I'm actually going to do\nthen here what I'm actually going to do I'm going to basically apply something\nI'm going to basically apply something\nI'm going to basically apply something called as cross validation cross\ncalled as cross validation cross\ncalled as cross validation cross validation is very much important\nvalidation is very much important\nvalidation is very much important because in Cross validation we divide\nbecause in Cross validation we divide\nbecause in Cross validation we divide out train and test data in such a way\nout train and test data in such a way\nout train and test data in such a way that every combination of the train and\nthat every combination of the train and\nthat every combination of the train and test data is basically taken by care is\ntest data is basically taken by care is\ntest data is basically taken by care is taken by the model and whoever accuracy\ntaken by the model and whoever accuracy\ntaken by the model and whoever accuracy is better that all entire thing is\nis better that all entire thing is\nis better that all entire thing is basically combined so here what I'm\nbasically combined so here what I'm\nbasically combined so here what I'm going to do I'm going to say mean square\ngoing to do I'm going to say mean square\ngoing to do I'm going to say mean square error is equal to here I will import one\nerror is equal to here I will import one\nerror is equal to here I will import one more Library let's say from SK learn\nmore Library let's say from SK learn\nmore Library let's say from SK learn dot model selection I'm going to import\ndot model selection I'm going to import\ndot model selection I'm going to import cross Val\ncross Val\ncross Val score so cross Val score cross\nscore so cross Val score cross\nscore so cross Val score cross validation score basically means it is\nvalidation score basically means it is\nvalidation score basically means it is going to do a lot of train and test\ngoing to do a lot of train and test\ngoing to do a lot of train and test split it's something like this one\nsplit it's something like this one\nsplit it's something like this one example I will show it to you here only\nexample I will show it to you here only\nexample I will show it to you here only so what does cross validation basically\nso what does cross validation basically\nso what does cross validation basically do okay so in Cross validation what\ndo okay so in Cross validation what\ndo okay so in Cross validation what happens what you do suppose this is your\nhappens what you do suppose this is your\nhappens what you do suppose this is your entire data\nentire data\nentire data set suppose this is 100 records if you\nset suppose this is 100 records if you\nset suppose this is 100 records if you do five cross validation then in the\ndo five cross validation then in the\ndo five cross validation then in the first this will be your test data and\nfirst this will be your test data and\nfirst this will be your test data and remaining all will be your training data\nremaining all will be your training data\nremaining all will be your training data if in the second cross validation this\nif in the second cross validation this\nif in the second cross validation this will be your test data and remaining all\nwill be your test data and remaining all\nwill be your test data and remaining all will be your test uh training data like\nwill be your test uh training data like\nwill be your test uh training data like this five times you'll be doing cross\nthis five times you'll be doing cross\nthis five times you'll be doing cross validation by taking the different\nvalidation by taking the different\nvalidation by taking the different combination of train and test but I'm\ncombination of train and test but I'm\ncombination of train and test but I'm not going to discuss much about it in\nnot going to discuss much about it in\nnot going to discuss much about it in the future if you want a separate\nthe future if you want a separate\nthe future if you want a separate session I will include that in one of\nsession I will include that in one of\nsession I will include that in one of the session itself so this was uh\nthe session itself so this was uh\nthe session itself so this was uh basically the plan with respect to cross\nbasically the plan with respect to cross\nbasically the plan with respect to cross validation or cross Val score so here\nvalidation or cross Val score so here\nvalidation or cross Val score so here I'm going to basically take cross\nI'm going to basically take cross\nI'm going to basically take cross Val\nVal\nVal score and here the first parameter that\nscore and here the first parameter that\nscore and here the first parameter that I give is my model so linear regression\nI give is my model so linear regression\nI give is my model so linear regression is my model and here I will take X and Y\nis my model and here I will take X and Y\nis my model and here I will take X and Y I'm not doing a train test split\nI'm not doing a train test split\nI'm not doing a train test split specifically over here I'm giving the\nspecifically over here I'm giving the\nspecifically over here I'm giving the entire X and Y and probably based on\nentire X and Y and probably based on\nentire X and Y and probably based on that I'm going to do a cross validation\nthat I'm going to do a cross validation\nthat I'm going to do a cross validation over here you can also do train test\nover here you can also do train test\nover here you can also do train test plate initially and then just give the X\nplate initially and then just give the X\nplate initially and then just give the X train and Y train over here to do the\ntrain and Y train over here to do the\ntrain and Y train over here to do the cross validation it is up to you but the\ncross validation it is up to you but the\ncross validation it is up to you but the best practices will be that first you do\nbest practices will be that first you do\nbest practices will be that first you do the train test split and then only give\nthe train test split and then only give\nthe train test split and then only give the train data over here to do the cross\nthe train data over here to do the cross\nthe train data over here to do the cross validation I'm just going to use scoring\nvalidation I'm just going to use scoring\nvalidation I'm just going to use scoring is equal to you can use mean squared\nis equal to you can use mean squared\nis equal to you can use mean squared error negative mean squared error let's\nerror negative mean squared error let's\nerror negative mean squared error let's say that I'm going to use negative mean\nsay that I'm going to use negative mean\nsay that I'm going to use negative mean squ error again where do you find all\nsqu error again where do you find all\nsqu error again where do you find all these things you will be able to see in\nthese things you will be able to see in\nthese things you will be able to see in the SK learn page of L uh cross Val\nthe SK learn page of L uh cross Val\nthe SK learn page of L uh cross Val score and then finally in the cross Val\nscore and then finally in the cross Val\nscore and then finally in the cross Val score you give cross validation value as\nscore you give cross validation value as\nscore you give cross validation value as 5 10 whatever you want so after this\n5 10 whatever you want so after this\n5 10 whatever you want so after this what I'm actually going to do I'm just\nwhat I'm actually going to do I'm just\nwhat I'm actually going to do I'm just going to basically from this how many\ngoing to basically from this how many\ngoing to basically from this how many scores I will get the mean squar error\nscores I will get the mean squar error\nscores I will get the mean squar error will be five since I'm doing five cross\nwill be five since I'm doing five cross\nwill be five since I'm doing five cross validation if you don't believe me just\nvalidation if you don't believe me just\nvalidation if you don't believe me just see over here print msse so here you'll\nsee over here print msse so here you'll\nsee over here print msse so here you'll be able to see five different values 1 2\nbe able to see five different values 1 2\nbe able to see five different values 1 2 3 4 5 right five different mean values\n3 4 5 right five different mean values\n3 4 5 right five different mean values because we are doing cross five five\nbecause we are doing cross five five\nbecause we are doing cross five five cross validation so here what I'm going\ncross validation so here what I'm going\ncross validation so here what I'm going to write I'm just going to say np. mean\nto write I'm just going to say np. mean\nto write I'm just going to say np. mean I want to take the average of all the\nI want to take the average of all the\nI want to take the average of all the five so here will basically be my\nfive so here will basically be my\nfive so here will basically be my meanor\nmeanor\nmeanor msse okay and then probably I'll print I\nmsse okay and then probably I'll print I\nmsse okay and then probably I'll print I will print my Ms meanor MSC so this will\nwill print my Ms meanor MSC so this will\nwill print my Ms meanor MSC so this will be my average score with respect to this\nbe my average score with respect to this\nbe my average score with respect to this the negative value is there because we\nthe negative value is there because we\nthe negative value is there because we have used negative mean squ error but if\nhave used negative mean squ error but if\nhave used negative mean squ error but if you just consider mean square error then\nyou just consider mean square error then\nyou just consider mean square error then it is only 37.1 3 okay so this I have\nit is only 37.1 3 okay so this I have\nit is only 37.1 3 okay so this I have actually shown you how to do cross\nactually shown you how to do cross\nactually shown you how to do cross validation see with respect to linear\nvalidation see with respect to linear\nvalidation see with respect to linear regression you can't modify much with\nregression you can't modify much with\nregression you can't modify much with the parameter so that is the reason why\nthe parameter so that is the reason why\nthe parameter so that is the reason why specifically in order to overcome\nspecifically in order to overcome\nspecifically in order to overcome overfitting and do the feature selection\noverfitting and do the feature selection\noverfitting and do the feature selection we use uh R and lasso regression so here\nwe use uh R and lasso regression so here\nwe use uh R and lasso regression so here I will show show you how to do ridge\nI will show show you how to do ridge\nI will show show you how to do ridge ridge regression\nridge regression\nridge regression now now in order to do the prediction\nnow now in order to do the prediction\nnow now in order to do the prediction all you have to do is that just go over\nall you have to do is that just go over\nall you have to do is that just go over here take the model okay what is the\nhere take the model okay what is the\nhere take the model okay what is the model linear R and just say do\npredict so here you can see uh you'll be\npredict so here you can see uh you'll be getting a function called as do predict\ngetting a function called as do predict\ngetting a function called as do predict and give the test value whatever you\nand give the test value whatever you\nand give the test value whatever you want to predict automatically the\nwant to predict automatically the\nwant to predict automatically the prediction will be done so I'm just\nprediction will be done so I'm just\nprediction will be done so I'm just going to remove this and focus on Ridge\ngoing to remove this and focus on Ridge\ngoing to remove this and focus on Ridge regression right now because I I want to\nregression right now because I I want to\nregression right now because I I want to show how hyperparameter tuning is done\nshow how hyperparameter tuning is done\nshow how hyperparameter tuning is done in R regression so for R regression the\nin R regression so for R regression the\nin R regression so for R regression the simple thing is that I'll be using two\nsimple thing is that I'll be using two\nsimple thing is that I'll be using two different libraries from skarn do\ndifferent libraries from skarn do\ndifferent libraries from skarn do linear linear uncore model I'm going to\nlinear linear uncore model I'm going to\nlinear linear uncore model I'm going to import Ridge so for the ridge it is also\nimport Ridge so for the ridge it is also\nimport Ridge so for the ridge it is also present in linear underscore model for\npresent in linear underscore model for\npresent in linear underscore model for doing the hyperparameter tuning I will\ndoing the hyperparameter tuning I will\ndoing the hyperparameter tuning I will be using from SK learn do modore\nbe using from SK learn do modore\nbe using from SK learn do modore selection and then I'm going to import\nselection and then I'm going to import\nselection and then I'm going to import grid SE CV so these are the two\ngrid SE CV so these are the two\ngrid SE CV so these are the two libraries that I'm actually going to use\nlibraries that I'm actually going to use\nlibraries that I'm actually going to use grid SE CV will be able to help you out\ngrid SE CV will be able to help you out\ngrid SE CV will be able to help you out with the um okay will be able to help\nwith the um okay will be able to help\nwith the um okay will be able to help you out with Hyper parameter tuning and\nyou out with Hyper parameter tuning and\nyou out with Hyper parameter tuning and then probably you'll be able to do\nthen probably you'll be able to do\nthen probably you'll be able to do that uh difference between MSE and\nthat uh difference between MSE and\nthat uh difference between MSE and negative MSE not big thing guys if you\nnegative MSE not big thing guys if you\nnegative MSE not big thing guys if you use MSE here mean squ error you'll be\nuse MSE here mean squ error you'll be\nuse MSE here mean squ error you'll be getting 37 I've just used negation of\ngetting 37 I've just used negation of\ngetting 37 I've just used negation of MSE it's okay anything is fine you can\nMSE it's okay anything is fine you can\nMSE it's okay anything is fine you can go with MSE also means square error\ngo with MSE also means square error\ngo with MSE also means square error there is also another uh another scoring\nthere is also another uh another scoring\nthere is also another uh another scoring area which is like which focuses on\narea which is like which focuses on\narea which is like which focuses on square root square mean Square uh sorry\nsquare root square mean Square uh sorry\nsquare root square mean Square uh sorry root means Square eror okay so there are\nroot means Square eror okay so there are\nroot means Square eror okay so there are different different things which you can\ndifferent different things which you can\ndifferent different things which you can basically focus on okay now in order to\nbasically focus on okay now in order to\nbasically focus on okay now in order to give you this specific good value I'm\ngive you this specific good value I'm\ngive you this specific good value I'm actually going to do hyper Peter tuning\nactually going to do hyper Peter tuning\nactually going to do hyper Peter tuning now let's go ahead with uh grid s CV so\nnow let's go ahead with uh grid s CV so\nnow let's go ahead with uh grid s CV so here what I'm going to do again I'm\nhere what I'm going to do again I'm\nhere what I'm going to do again I'm going to basically Define my model which\ngoing to basically Define my model which\ngoing to basically Define my model which will be\nwill be\nwill be Ridge okay so this this is what I have\nRidge okay so this this is what I have\nRidge okay so this this is what I have actually imported now uh let me open the\nactually imported now uh let me open the\nactually imported now uh let me open the ridge skarn so SK learn\nRidge we need need to understand what\nRidge we need need to understand what all parameters are basically\nall parameters are basically\nall parameters are basically used do you remember this Alpha value\nused do you remember this Alpha value\nused do you remember this Alpha value guys do you remember this Alpha value\nguys do you remember this Alpha value\nguys do you remember this Alpha value why do we use Alpha I I told you now\nwhy do we use Alpha I I told you now\nwhy do we use Alpha I I told you now Alpha multiplied by slope square if you\nAlpha multiplied by slope square if you\nAlpha multiplied by slope square if you remember in Ridge we specifically use\nremember in Ridge we specifically use\nremember in Ridge we specifically use this right Ridge and lasso regression\nthis right Ridge and lasso regression\nthis right Ridge and lasso regression Alpha so this is the alpha the this is\nAlpha so this is the alpha the this is\nAlpha so this is the alpha the this is probably the best parameter we can\nprobably the best parameter we can\nprobably the best parameter we can perform hyper parameter tuning the next\nperform hyper parameter tuning the next\nperform hyper parameter tuning the next parameter that we can probably perform\nparameter that we can probably perform\nparameter that we can probably perform is basically uh this Max iteration okay\nis basically uh this Max iteration okay\nis basically uh this Max iteration okay Max iteration basically means how many\nMax iteration basically means how many\nMax iteration basically means how many number of iteration how many number of\nnumber of iteration how many number of\nnumber of iteration how many number of times we may probably change the Theta 1\ntimes we may probably change the Theta 1\ntimes we may probably change the Theta 1 value to get the right value so we can\nvalue to get the right value so we can\nvalue to get the right value so we can do this so what I'm actually going to do\ndo this so what I'm actually going to do\ndo this so what I'm actually going to do I'm going to select some Alpha values\nI'm going to select some Alpha values\nI'm going to select some Alpha values I'm going to play with this apart from\nI'm going to play with this apart from\nI'm going to play with this apart from that if I want I can also play with the\nthat if I want I can also play with the\nthat if I want I can also play with the other parameters which are uh like kind\nother parameters which are uh like kind\nother parameters which are uh like kind of uh you know probably you can you can\nof uh you know probably you can you can\nof uh you know probably you can you can also play with the iteration parameter\nalso play with the iteration parameter\nalso play with the iteration parameter it is up to you try whichever parameter\nit is up to you try whichever parameter\nit is up to you try whichever parameter you want to change you can go ahead and\nyou want to change you can go ahead and\nyou want to change you can go ahead and change it now let me show you how do we\nchange it now let me show you how do we\nchange it now let me show you how do we write this and how do we make sure that\nwrite this and how do we make sure that\nwrite this and how do we make sure that this specific thing is done now uh\nthis specific thing is done now uh\nthis specific thing is done now uh before doing grid s CV uh let me do one\nbefore doing grid s CV uh let me do one\nbefore doing grid s CV uh let me do one thing I will Define my parameters okay\nthing I will Define my parameters okay\nthing I will Define my parameters okay so here is my Ridge now what I'm going\nso here is my Ridge now what I'm going\nso here is my Ridge now what I'm going to do I'm going to say parameters and in\nto do I'm going to say parameters and in\nto do I'm going to say parameters and in this parameter two important value that\nthis parameter two important value that\nthis parameter two important value that I'm probably going to take is this one\nI'm probably going to take is this one\nI'm probably going to take is this one that is my C value and I will try to\nthat is my C value and I will try to\nthat is my C value and I will try to Define this in the form of dictionaries\nDefine this in the form of dictionaries\nDefine this in the form of dictionaries so here the C value that I sorry not C\nso here the C value that I sorry not C\nso here the C value that I sorry not C just a second\njust a second\njust a second guys my mistake it is not C it is\nguys my mistake it is not C it is\nguys my mistake it is not C it is Alpha let's see so how do I Define my\nAlpha let's see so how do I Define my\nAlpha let's see so how do I Define my Alpha value we'll try to see so here the\nAlpha value we'll try to see so here the\nAlpha value we'll try to see so here the parameters will be Alpha C is basically\nparameters will be Alpha C is basically\nparameters will be Alpha C is basically for uh logistic regression I'll show you\nfor uh logistic regression I'll show you\nfor uh logistic regression I'll show you so the alpha value I will just mention\nso the alpha value I will just mention\nso the alpha value I will just mention some values like\nsome values like\nsome values like 1 e to the power of -5 that basically Me\n1 e to the power of -5 that basically Me\n1 e to the power of -5 that basically Me 00000000 0 0 0 1 similarly I I can write\n00000000 0 0 0 1 similarly I I can write\n00000000 0 0 0 1 similarly I I can write 1 E to the^ of - 10 that again means 0 0\n1 E to the^ of - 10 that again means 0 0\n1 E to the^ of - 10 that again means 0 0 0 0 0 0 0 0 10 * 0 1 I'm just making fun\n0 0 0 0 0 0 10 * 0 1 I'm just making fun\n0 0 0 0 0 0 10 * 0 1 I'm just making fun okay so that you will also get\nokay so that you will also get\nokay so that you will also get entertained 1 E to the^ of minus 8 okay\nentertained 1 E to the^ of minus 8 okay\nentertained 1 E to the^ of minus 8 okay similarly I can write 1 E to the^ of\nsimilarly I can write 1 E to the^ of\nsimilarly I can write 1 E to the^ of minus 3 from this particular value now\nminus 3 from this particular value now\nminus 3 from this particular value now I'm increasing this value see 1 E to\nI'm increasing this value see 1 E to\nI'm increasing this value see 1 E to the^ of minus 2 and then probably I can\nthe^ of minus 2 and then probably I can\nthe^ of minus 2 and then probably I can have 1 5 10 um 20 something like this so\nhave 1 5 10 um 20 something like this so\nhave 1 5 10 um 20 something like this so I'm going to play with all this\nI'm going to play with all this\nI'm going to play with all this particular parameters for right now\nparticular parameters for right now\nparticular parameters for right now because in grit or CV what they do is\nbecause in grit or CV what they do is\nbecause in grit or CV what they do is that they take all the combination of\nthat they take all the combination of\nthat they take all the combination of this Alpha value and wherever your uh\nthis Alpha value and wherever your uh\nthis Alpha value and wherever your uh your your model performs well it is\nyour your model performs well it is\nyour your model performs well it is going to take that specific parameter\ngoing to take that specific parameter\ngoing to take that specific parameter and it is going to give you that okay\nand it is going to give you that okay\nand it is going to give you that okay this is the best fit parameter that is\nthis is the best fit parameter that is\nthis is the best fit parameter that is got selected so here I have got all\ngot selected so here I have got all\ngot selected so here I have got all these things now what I'm going to do\nthese things now what I'm going to do\nthese things now what I'm going to do I'm going to basically apply the grid C\nI'm going to basically apply the grid C\nI'm going to basically apply the grid C TV so here I have uh gridge uh sorry\nTV so here I have uh gridge uh sorry\nTV so here I have uh gridge uh sorry Ridge GD I'm\nRidge GD I'm\nRidge GD I'm saying ridore regressor so I'm going to\nsaying ridore regressor so I'm going to\nsaying ridore regressor so I'm going to use git s\nuse git s\nuse git s CV git s CV and here I'm basically going\nCV git s CV and here I'm basically going\nCV git s CV and here I'm basically going to take the parameters regge okay Ridge\nto take the parameters regge okay Ridge\nto take the parameters regge okay Ridge is my first model and then I will take\nis my first model and then I will take\nis my first model and then I will take up all this params that I have actually\nup all this params that I have actually\nup all this params that I have actually defined see in git CV if I press shift\ndefined see in git CV if I press shift\ndefined see in git CV if I press shift tab I have to first of all execute this\ntab I have to first of all execute this\ntab I have to first of all execute this then only it will be able to press shift\nthen only it will be able to press shift\nthen only it will be able to press shift tab so here if I press shift tab here\ntab so here if I press shift tab here\ntab so here if I press shift tab here you'll be able to see estimator and\nyou'll be able to see estimator and\nyou'll be able to see estimator and parameter grid is my second parameter\nparameter grid is my second parameter\nparameter grid is my second parameter then scoring and then all the other\nthen scoring and then all the other\nthen scoring and then all the other parameters so here the first thing that\nparameters so here the first thing that\nparameters so here the first thing that goes is your model then your parameters\ngoes is your model then your parameters\ngoes is your model then your parameters which what you are actually playing then\nwhich what you are actually playing then\nwhich what you are actually playing then the third parameter is basically your\nthe third parameter is basically your\nthe third parameter is basically your scoring\nscoring\nscoring scoring and again here I'm going to use\nscoring and again here I'm going to use\nscoring and again here I'm going to use negative mean squ error some people are\nnegative mean squ error some people are\nnegative mean squ error some people are saying that mean squared error is not\nsaying that mean squared error is not\nsaying that mean squared error is not present so that is the reason why\npresent so that is the reason why\npresent so that is the reason why negative mean squ error is done why it\nnegative mean squ error is done why it\nnegative mean squ error is done why it may not be present because\nmay not be present because\nmay not be present because uh they try to always create a generic\nuh they try to always create a generic\nuh they try to always create a generic Library probably this kind of uh scoring\nLibrary probably this kind of uh scoring\nLibrary probably this kind of uh scoring parameter may also get used in other\nparameter may also get used in other\nparameter may also get used in other algorithms so that is the reason they\nalgorithms so that is the reason they\nalgorithms so that is the reason they may not have created but if you want to\nmay not have created but if you want to\nmay not have created but if you want to Deep dive into it Google\nDeep dive into it Google\nDeep dive into it Google Google then what is r regress dot fit on\nGoogle then what is r regress dot fit on\nGoogle then what is r regress dot fit on X comma y again I'm telling you you can\nX comma y again I'm telling you you can\nX comma y again I'm telling you you can first of all do train test split on X\nfirst of all do train test split on X\nfirst of all do train test split on X and Y and then probably only do this on\nand Y and then probably only do this on\nand Y and then probably only do this on X train and Y train parameter is not oh\nX train and Y train parameter is not oh\nX train and Y train parameter is not oh sorry\nsorry\nsorry okay I get this okay parameter is not\nokay I get this okay parameter is not\nokay I get this okay parameter is not and why it is not and oh yeah it has\nand why it is not and oh yeah it has\nand why it is not and oh yeah it has become a\nbecome a\nbecome a list I'm going to make this as\nlist I'm going to make this as\nlist I'm going to make this as dictionary right now I'm fully focused\ndictionary right now I'm fully focused\ndictionary right now I'm fully focused on implementing things if I get an error\non implementing things if I get an error\non implementing things if I get an error I'll definitely make sure that it'll get\nI'll definitely make sure that it'll get\nI'll definitely make sure that it'll get fixed anyhow if I get that error I will\nfixed anyhow if I get that error I will\nfixed anyhow if I get that error I will not say oh Kish why why this error came\nnot say oh Kish why why this error came\nnot say oh Kish why why this error came you\nyou\nyou know why this error came I I'll not get\nknow why this error came I I'll not get\nknow why this error came I I'll not get worried I'll get the error down only you\nworried I'll get the error down only you\nworried I'll get the error down only you cannot give this as the one okay so try\ncannot give this as the one okay so try\ncannot give this as the one okay so try to understand okay so this is your gitar\nto understand okay so this is your gitar\nto understand okay so this is your gitar CV I've also done the fit and let's go\nCV I've also done the fit and let's go\nCV I've also done the fit and let's go and select the best parameter so what I\nand select the best parameter so what I\nand select the best parameter so what I can do I will write print\ncan do I will write print\ncan do I will write print ridore\nridore\nridore regressor dot\nregressor dot\nregressor dot params sorry there will be a parameter\nparams sorry there will be a parameter\nparams sorry there will be a parameter called as best params I'm going to print\ncalled as best params I'm going to print\ncalled as best params I'm going to print this and I'm going to print ridore\nthis and I'm going to print ridore\nthis and I'm going to print ridore regressor Dot\nregressor Dot\nregressor Dot best\nbest\nbest score so these are all the values that\nscore so these are all the values that\nscore so these are all the values that are got selected one is Alpha is equal\nare got selected one is Alpha is equal\nare got selected one is Alpha is equal to 20 and the best score is - 32 so\nto 20 and the best score is - 32 so\nto 20 and the best score is - 32 so initially I gotus 37 but because of\ninitially I gotus 37 but because of\ninitially I gotus 37 but because of Ridge regression you can see that our\nRidge regression you can see that our\nRidge regression you can see that our negative mean square error has\nnegative mean square error has\nnegative mean square error has definitely become better there is a\ndefinitely become better there is a\ndefinitely become better there is a minus sign don't worry but from 37 it\nminus sign don't worry but from 37 it\nminus sign don't worry but from 37 it has come to 32 cross validation guys\nhas come to 32 cross validation guys\nhas come to 32 cross validation guys over here inside grids s CV also when it\nover here inside grids s CV also when it\nover here inside grids s CV also when it is probably taking the entire\nis probably taking the entire\nis probably taking the entire combination over there the CV Value\ncombination over there the CV Value\ncombination over there the CV Value Cross validation also we can use\nCross validation also we can use\nCross validation also we can use so probably if I am probably considering\nso probably if I am probably considering\nso probably if I am probably considering all these\nall these\nall these things many people has a question Chris\nthings many people has a question Chris\nthings many people has a question Chris is this minus value increased that\nis this minus value increased that\nis this minus value increased that basically means you cannot use Ridge\nbasically means you cannot use Ridge\nbasically means you cannot use Ridge regression you are right in this\nregression you are right in this\nregression you are right in this particular case Ridge regression is not\nparticular case Ridge regression is not\nparticular case Ridge regression is not helping you out so guys let me again\nhelping you out so guys let me again\nhelping you out so guys let me again write it down everybody don't worry yeah\nwrite it down everybody don't worry yeah\nwrite it down everybody don't worry yeah previous I got minus 32 right now I'm\nprevious I got minus 32 right now I'm\nprevious I got minus 32 right now I'm getting - 37\ngetting - 37\ngetting - 37 right sorry previously I got what - 37\n- 37 now I got - 32 so here you can see\n- 37 now I got - 32 so here you can see this I got it from linear regression\nthis I got it from linear regression\nthis I got it from linear regression this I got it from what Ridge which one\nthis I got it from what Ridge which one\nthis I got it from what Ridge which one should I select I should select this\nshould I select I should select this\nshould I select I should select this model only because it is performing well\nmodel only because it is performing well\nmodel only because it is performing well than this but again understand Ridge\nthan this but again understand Ridge\nthan this but again understand Ridge also tries to reduce the overfitting so\nalso tries to reduce the overfitting so\nalso tries to reduce the overfitting so probably in this particular scenario we\nprobably in this particular scenario we\nprobably in this particular scenario we cannot use Ridge because the performance\ncannot use Ridge because the performance\ncannot use Ridge because the performance is becoming more bad so what I will do I\nis becoming more bad so what I will do I\nis becoming more bad so what I will do I will go and try with lasso regression\nwill go and try with lasso regression\nwill go and try with lasso regression now I'll copy and paste the same thing\nnow I'll copy and paste the same thing\nnow I'll copy and paste the same thing so linear model import lasso then this\nso linear model import lasso then this\nso linear model import lasso then this will basically be my\nwill basically be my\nwill basically be my lasso let's see with lasso whether it\nlasso let's see with lasso whether it\nlasso let's see with lasso whether it will increase or not let's\nwill increase or not let's\nwill increase or not let's see this is my parameter that got\nsee this is my parameter that got\nsee this is my parameter that got selected now let me write lasto\nselected now let me write lasto\nselected now let me write lasto regressor\nregressor\nregressor dot best params so this is Alpha is\ndot best params so this is Alpha is\ndot best params so this is Alpha is equal to one is got selected over here\nequal to one is got selected over here\nequal to one is got selected over here I'm just going to print it okay and then\nI'm just going to print it okay and then\nI'm just going to print it okay and then I'm going to print with last one\nI'm going to print with last one\nI'm going to print with last one regression DOT score will be the best so\nregression DOT score will be the best so\nregression DOT score will be the best so here I'm actually getting - 35 - 35 here\nhere I'm actually getting - 35 - 35 here\nhere I'm actually getting - 35 - 35 here I'm actually getting - 32 so minus 35\nI'm actually getting - 32 so minus 35\nI'm actually getting - 32 so minus 35 still I will focus on linear regression\nstill I will focus on linear regression\nstill I will focus on linear regression now see what will happen if I add more\nnow see what will happen if I add more\nnow see what will happen if I add more parameters if I add more parameters see\nparameters if I add more parameters see\nparameters if I add more parameters see what will happen so now I'm going to\nwhat will happen so now I'm going to\nwhat will happen so now I'm going to take Alpha different different values\ntake Alpha different different values\ntake Alpha different different values see this I'm just going to remove this\nsee this I'm just going to remove this\nsee this I'm just going to remove this and probably add Alpha value in this\nand probably add Alpha value in this\nand probably add Alpha value in this way see here I have added more values 5\nway see here I have added more values 5\nway see here I have added more values 5 10 20 30 35 40 45 100 okay let's see\n10 20 30 35 40 45 100 okay let's see\n10 20 30 35 40 45 100 okay let's see whether we our performance will increase\nwhether we our performance will increase\nwhether we our performance will increase or not so here\nor not so here\nor not so here uh first of all let me remove from here\nuh first of all let me remove from here\nuh first of all let me remove from here in Ridge just take it down guys I'm I'm\nin Ridge just take it down guys I'm I'm\nin Ridge just take it down guys I'm I'm adding more parameters like this just\nadding more parameters like this just\nadding more parameters like this just take it down yeah CV is equal to 5\ntake it down yeah CV is equal to 5\ntake it down yeah CV is equal to 5 nobody okay you're not able to see it um\nnobody okay you're not able to see it um\nnobody okay you're not able to see it um CV is equal to 5 now here it is uh what\nCV is equal to 5 now here it is uh what\nCV is equal to 5 now here it is uh what you can basically focus on so here you\nyou can basically focus on so here you\nyou can basically focus on so here you can see I have added some values like\ncan see I have added some values like\ncan see I have added some values like this you can also\nthis you can also\nthis you can also add and just try to execute and now if I\nadd and just try to execute and now if I\nadd and just try to execute and now if I go and probably see this is my see first\ngo and probably see this is my see first\ngo and probably see this is my see first I have tried for Ridge I'm getting minus\nI have tried for Ridge I'm getting minus\nI have tried for Ridge I'm getting minus 29 do you see after adding more\n29 do you see after adding more\n29 do you see after adding more parameters what happened in Ridge after\nparameters what happened in Ridge after\nparameters what happened in Ridge after adding more parameters what happened in\nadding more parameters what happened in\nadding more parameters what happened in Ridge you can see om minus 29 and the\nRidge you can see om minus 29 and the\nRidge you can see om minus 29 and the alpha value that is got selected is 100\nalpha value that is got selected is 100\nalpha value that is got selected is 100 if you want try with cross validation\nif you want try with cross validation\nif you want try with cross validation 10 and just try to execute now\n10 and just try to execute now\n10 and just try to execute now now so these are are some hyper\nnow so these are are some hyper\nnow so these are are some hyper parameters that we will definitely play\nparameters that we will definitely play\nparameters that we will definitely play with here you can see - 29 so here you\nwith here you can see - 29 so here you\nwith here you can see - 29 so here you can see minus 29 you can also increase\ncan see minus 29 you can also increase\ncan see minus 29 you can also increase the cross validation\nthe cross validation\nthe cross validation value over here also and probably\nvalue over here also and probably\nvalue over here also and probably execute it but with lasso I don't know\nexecute it but with lasso I don't know\nexecute it but with lasso I don't know whether it is improving or not it is\nwhether it is improving or not it is\nwhether it is improving or not it is coming to minus 34 you just have to play\ncoming to minus 34 you just have to play\ncoming to minus 34 you just have to play with this parameters as now for a bigger\nwith this parameters as now for a bigger\nwith this parameters as now for a bigger problem statement the thing is not\nproblem statement the thing is not\nproblem statement the thing is not limited to here right we try to take\nlimited to here right we try to take\nlimited to here right we try to take multiples and many parameters multiples\nmultiples and many parameters multiples\nmultiples and many parameters multiples and many parameters and try to do these\nand many parameters and try to do these\nand many parameters and try to do these things it is up to you we play with\nthings it is up to you we play with\nthings it is up to you we play with multiple parameters whichever gives us\nmultiple parameters whichever gives us\nmultiple parameters whichever gives us the best result we are basically taking\nthe best result we are basically taking\nthe best result we are basically taking it it's okay error is increased I know\nit it's okay error is increased I know\nit it's okay error is increased I know that no error is increasing definitely\nthat no error is increasing definitely\nthat no error is increasing definitely error is increasing even though by\nerror is increasing even though by\nerror is increasing even though by trying with different different\ntrying with different different\ntrying with different different parameters but about most of the\nparameters but about most of the\nparameters but about most of the scenario see here I gotus 37 probably\nscenario see here I gotus 37 probably\nscenario see here I gotus 37 probably what I can actually do is that uh try to\nwhat I can actually do is that uh try to\nwhat I can actually do is that uh try to get better one with respect to this\nget better one with respect to this\nget better one with respect to this now the best way what I can also do is\nnow the best way what I can also do is\nnow the best way what I can also do is that I can basically take up train and\nthat I can basically take up train and\nthat I can basically take up train and test split also and probably do these\ntest split also and probably do these\ntest split also and probably do these things let's see let's see one example\nthings let's see let's see one example\nthings let's see let's see one example so how do we do train and test from SK\nso how do we do train and test from SK\nso how do we do train and test from SK scalar dot I think model selection\nscalar dot I think model selection\nscalar dot I think model selection import train test split okay it's okay\nimport train test split okay it's okay\nimport train test split okay it's okay guys you may get a different value okay\nguys you may get a different value okay\nguys you may get a different value okay let's do one thing okay let's make your\nlet's do one thing okay let's make your\nlet's do one thing okay let's make your problem statement little bit simpler now\nproblem statement little bit simpler now\nproblem statement little bit simpler now what I'm going to do just tell me in\nwhat I'm going to do just tell me in\nwhat I'm going to do just tell me in train test plate what we need to do so\ntrain test plate what we need to do so\ntrain test plate what we need to do so I'm going to take the same code I'm\nI'm going to take the same code I'm\nI'm going to take the same code I'm going to paste it over here or let me do\ngoing to paste it over here or let me do\ngoing to paste it over here or let me do one thing let me insert a cell below and\none thing let me insert a cell below and\none thing let me insert a cell below and let me do it for train test split so in\nlet me do it for train test split so in\nlet me do it for train test split so in train test plate what we can do so I'm\ntrain test plate what we can do so I'm\ntrain test plate what we can do so I'm just going to take the syntax paste it\njust going to take the syntax paste it\njust going to take the syntax paste it over here let's say that I'm taking XT\nover here let's say that I'm taking XT\nover here let's say that I'm taking XT train y train and then I'm using train\ntrain y train and then I'm using train\ntrain y train and then I'm using train test split with 33% now if I execute\ntest split with 33% now if I execute\ntest split with 33% now if I execute with respect to X train and Y train so\nwith respect to X train and Y train so\nwith respect to X train and Y train so here is my you can see this I have\nhere is my you can see this I have\nhere is my you can see this I have written this code from SK learn. model\nwritten this code from SK learn. model\nwritten this code from SK learn. model selection uh train test plate random\nselection uh train test plate random\nselection uh train test plate random State can be anything whatever you write\nState can be anything whatever you write\nState can be anything whatever you write it is fine then you basically give X and\nit is fine then you basically give X and\nit is fine then you basically give X and Y with test sizes 33 uh this is\nY with test sizes 33 uh this is\nY with test sizes 33 uh this is basically saying that the test will have\nbasically saying that the test will have\nbasically saying that the test will have 33% and the train data will be 77% so\n33% and the train data will be 77% so\n33% and the train data will be 77% so this is what I'm actually getting with\nthis is what I'm actually getting with\nthis is what I'm actually getting with respect to X train and Y train here what\nrespect to X train and Y train here what\nrespect to X train and Y train here what I'm going to do I'm going to basically\nI'm going to do I'm going to basically\nI'm going to do I'm going to basically take X train comma y train and now if I\ntake X train comma y train and now if I\ntake X train comma y train and now if I go and probably see this here you can\ngo and probably see this here you can\ngo and probably see this here you can see minus 25 understand this value\nsee minus 25 understand this value\nsee minus 25 understand this value should go towards zero if it is going\nshould go towards zero if it is going\nshould go towards zero if it is going towards zero that basically means the\ntowards zero that basically means the\ntowards zero that basically means the performance is better now similarly I do\nperformance is better now similarly I do\nperformance is better now similarly I do it for Ridge in Ridge what I'm actually\nit for Ridge in Ridge what I'm actually\nit for Ridge in Ridge what I'm actually going to do here I'm going to write X\ngoing to do here I'm going to write X\ngoing to do here I'm going to write X train and Y train and if I go and\ntrain and Y train and if I go and\ntrain and Y train and if I go and probably select the best score than this\nprobably select the best score than this\nprobably select the best score than this here you'll be able to see I'm getting\nhere you'll be able to see I'm getting\nhere you'll be able to see I'm getting how much I'm getting minus\nhow much I'm getting minus\nhow much I'm getting minus 2.47 okay here I'm getting\n2.47 okay here I'm getting\n2.47 okay here I'm getting 25.8 here 25. 47 that basically means\n25.8 here 25. 47 that basically means\n25.8 here 25. 47 that basically means now still the Improvement is little bit\nnow still the Improvement is little bit\nnow still the Improvement is little bit bad because here we are not going\nbad because here we are not going\nbad because here we are not going towards zero so the next part again here\ntowards zero so the next part again here\ntowards zero so the next part again here also you can basically do it for X train\nalso you can basically do it for X train\nalso you can basically do it for X train and Y train X train and Y train so here\nand Y train X train and Y train so here\nand Y train X train and Y train so here you have this one and let's go and\nyou have this one and let's go and\nyou have this one and let's go and execute this so here you can see minus\nexecute this so here you can see minus\nexecute this so here you can see minus 2.47 now what you can also do is that\n2.47 now what you can also do is that\n2.47 now what you can also do is that you can use this\nyou can use this\nyou can use this lasso regressor do predict and you can\nlasso regressor do predict and you can\nlasso regressor do predict and you can basically predict with respect to X test\nbasically predict with respect to X test\nbasically predict with respect to X test so this is your white test value suppose\nso this is your white test value suppose\nso this is your white test value suppose let's say that this is my y PR Yore PR\nlet's say that this is my y PR Yore PR\nlet's say that this is my y PR Yore PR then what I can do from SK\nthen what I can do from SK\nthen what I can do from SK learn I will be using R square and\nlearn I will be using R square and\nlearn I will be using R square and adjusted R square if you remember SK\nadjusted R square if you remember SK\nadjusted R square if you remember SK learn R square rÂ² so this is my R2 score\nlearn R square rÂ² so this is my R2 score\nlearn R square rÂ² so this is my R2 score so where it is present in SK learn.\nso where it is present in SK learn.\nso where it is present in SK learn. Matrix so I'm going to write from SK\nMatrix so I'm going to write from SK\nMatrix so I'm going to write from SK learn import let's say I'm saying from\nlearn import let's say I'm saying from\nlearn import let's say I'm saying from skarn do Matrix import rÂ² R2 score now\nskarn do Matrix import rÂ² R2 score now\nskarn do Matrix import rÂ² R2 score now what I'm going to do over here I'm\nwhat I'm going to do over here I'm\nwhat I'm going to do over here I'm basically going to say my R2 score which\nbasically going to say my R2 score which\nbasically going to say my R2 score which is my variable I'll say this is nothing\nis my variable I'll say this is nothing\nis my variable I'll say this is nothing but R2 score here I'm just going to give\nbut R2 score here I'm just going to give\nbut R2 score here I'm just going to give my y PR comma Yore test so if I go and\nmy y PR comma Yore test so if I go and\nmy y PR comma Yore test so if I go and probably see the output here I will be\nprobably see the output here I will be\nprobably see the output here I will be able to see print R2 score this is all I\nable to see print R2 score this is all I\nable to see print R2 score this is all I have discussed guys there is also\nhave discussed guys there is also\nhave discussed guys there is also adjusted rant score is there where is R2\nadjusted rant score is there where is R2\nadjusted rant score is there where is R2 R2 score one adjusted rÂ² okay R2 score\nR2 score one adjusted rÂ² okay R2 score\nR2 score one adjusted rÂ² okay R2 score is there but adjusted R square should be\nis there but adjusted R square should be\nis there but adjusted R square should be here somewhere in some manner so this is\nhere somewhere in some manner so this is\nhere somewhere in some manner so this is how your output looks like with respect\nhow your output looks like with respect\nhow your output looks like with respect to by using this lasso regressor okay\nto by using this lasso regressor okay\nto by using this lasso regressor okay which is very good okay it should be I\nwhich is very good okay it should be I\nwhich is very good okay it should be I told it should be near 100% right now\ntold it should be near 100% right now\ntold it should be near 100% right now I'm getting 67% if I want to tie with\nI'm getting 67% if I want to tie with\nI'm getting 67% if I want to tie with the ridge you can also try that so you\nthe ridge you can also try that so you\nthe ridge you can also try that so you can say Ridge regressor do predict and\ncan say Ridge regressor do predict and\ncan say Ridge regressor do predict and here you can see 7 68% then you can also\nhere you can see 7 68% then you can also\nhere you can see 7 68% then you can also try linear regressor and\ntry linear regressor and\ntry linear regressor and predict what is the error saying the\npredict what is the error saying the\npredict what is the error saying the regression is not fitted yet why why it\nregression is not fitted yet why why it\nregression is not fitted yet why why it is not fitted why it is not\nis not fitted why it is not\nis not fitted why it is not fitted let's say that I have fitted here\nfitted let's say that I have fitted here\nfitted let's say that I have fitted here linear\nlinear\nlinear regression dot fit on X train and Y\nregression dot fit on X train and Y\nregression dot fit on X train and Y train X train and comma y train so I'm\ntrain X train and comma y train so I'm\ntrain X train and comma y train so I'm just going to fit it now if I go and\njust going to fit it now if I go and\njust going to fit it now if I go and probably try to do the\nprobably try to do the\nprobably try to do the calculation so if I go and see my R2\ncalculation so if I go and see my R2\ncalculation so if I go and see my R2 score it is also coming somewhere around\nscore it is also coming somewhere around\nscore it is also coming somewhere around 68% 67% now since this is just a linear\n68% 67% now since this is just a linear\n68% 67% now since this is just a linear regression you won't be able to get 100%\nregression you won't be able to get 100%\nregression you won't be able to get 100% because you're drawing a straight line\nbecause you're drawing a straight line\nbecause you're drawing a straight line right so for that you basically have to\nright so for that you basically have to\nright so for that you basically have to other use other algorithms like XG boost\nother use other algorithms like XG boost\nother use other algorithms like XG boost and all n bias so many algorithms are\nand all n bias so many algorithms are\nand all n bias so many algorithms are there it's okay see you give y test over\nthere it's okay see you give y test over\nthere it's okay see you give y test over here y PR over here both are same right\nhere y PR over here both are same right\nhere y PR over here both are same right they're\nthey're\nthey're comparing by see at one limit you can\ncomparing by see at one limit you can\ncomparing by see at one limit you can you can increase the performance after\nyou can increase the performance after\nyou can increase the performance after that you cannot see again I'm telling\nthat you cannot see again I'm telling\nthat you cannot see again I'm telling you in linear regression what we do\nyou in linear regression what we do\nyou in linear regression what we do these are my points right I will be only\nthese are my points right I will be only\nthese are my points right I will be only able to create one best line I cannot\nable to create one best line I cannot\nable to create one best line I cannot create a curve line right over here so\ncreate a curve line right over here so\ncreate a curve line right over here so obviously my accuracy will be only\nobviously my accuracy will be only\nobviously my accuracy will be only limited let's go and do it logistic\nlimited let's go and do it logistic\nlimited let's go and do it logistic practical\npractical\npractical quickly and here uh in logistic also we\nquickly and here uh in logistic also we\nquickly and here uh in logistic also we can do git SE CV now what I'm actually\ncan do git SE CV now what I'm actually\ncan do git SE CV now what I'm actually going to do first of all let's go ahead\ngoing to do first of all let's go ahead\ngoing to do first of all let's go ahead with the data set so I will quickly\nwith the data set so I will quickly\nwith the data set so I will quickly Implement logistic so from LC learn.\nImplement logistic so from LC learn.\nImplement logistic so from LC learn. linear\nlinear\nlinear model I'm going to import logistic\nmodel I'm going to import logistic\nmodel I'm going to import logistic regression so I'm going to use logistic\nregression so I'm going to use logistic\nregression so I'm going to use logistic regression and apart from that we know\nregression and apart from that we know\nregression and apart from that we know that let's take a new data set because\nthat let's take a new data set because\nthat let's take a new data set because for logistic we need to solve using\nfor logistic we need to solve using\nfor logistic we need to solve using classification problem so this is\nclassification problem so this is\nclassification problem so this is basically my logistic regression I'll\nbasically my logistic regression I'll\nbasically my logistic regression I'll take one data set so from SK learn. data\ntake one data set so from SK learn. data\ntake one data set so from SK learn. data sets import we'll take a data set which\nsets import we'll take a data set which\nsets import we'll take a data set which is like uh breast cancer data set so\nis like uh breast cancer data set so\nis like uh breast cancer data set so that is also present in SK learn with\nthat is also present in SK learn with\nthat is also present in SK learn with respect to the breast cancer data set\nrespect to the breast cancer data set\nrespect to the breast cancer data set I'm just going to use this see load best\nI'm just going to use this see load best\nI'm just going to use this see load best cancer data set I'm loading it and all\ncancer data set I'm loading it and all\ncancer data set I'm loading it and all the independent features are in data and\nthe independent features are in data and\nthe independent features are in data and my columns are feature names the same\nmy columns are feature names the same\nmy columns are feature names the same thing like how we did previously okay so\nthing like how we did previously okay so\nthing like how we did previously okay so this will basically be my\nthis will basically be my\nthis will basically be my complete uh complete independent feature\ncomplete uh complete independent feature\ncomplete uh complete independent feature so if I go and probably see this x. head\nso if I go and probably see this x. head\nso if I go and probably see this x. head here you'll be able to see that based on\nhere you'll be able to see that based on\nhere you'll be able to see that based on this input features the independent\nthis input features the independent\nthis input features the independent feature we need to determine whether the\nfeature we need to determine whether the\nfeature we need to determine whether the person is having cancer or not these are\nperson is having cancer or not these are\nperson is having cancer or not these are some of the features over here and this\nsome of the features over here and this\nsome of the features over here and this is like many many features are actually\nis like many many features are actually\nis like many many features are actually present so next thing I this that was my\npresent so next thing I this that was my\npresent so next thing I this that was my independent feature now I'll take my\nindependent feature now I'll take my\nindependent feature now I'll take my dependent feature dependent feature will\ndependent feature dependent feature will\ndependent feature dependent feature will already present in DF Target okay this\nalready present in DF Target okay this\nalready present in DF Target okay this particular data set that we have taken\nparticular data set that we have taken\nparticular data set that we have taken in DF in DF do Target we will basically\nin DF in DF do Target we will basically\nin DF in DF do Target we will basically have all our dependent feature these are\nhave all our dependent feature these are\nhave all our dependent feature these are my independent features so what I'm\nmy independent features so what I'm\nmy independent features so what I'm actually going to do I'm going to create\nactually going to do I'm going to create\nactually going to do I'm going to create Y and I'm going to say PD do data frame\nY and I'm going to say PD do data frame\nY and I'm going to say PD do data frame and here I'm going to say DF do Target\nand here I'm going to say DF do Target\nand here I'm going to say DF do Target Target and this column name should be\nTarget and this column name should be\nTarget and this column name should be Target right so this will be my column\nTarget right so this will be my column\nTarget right so this will be my column name and now if I go and see my y y is\nname and now if I go and see my y y is\nname and now if I go and see my y y is basically having zeros and one in the\nbasically having zeros and one in the\nbasically having zeros and one in the target feature now the next thing that\ntarget feature now the next thing that\ntarget feature now the next thing that we are going to do is that uh apply\nwe are going to do is that uh apply\nwe are going to do is that uh apply basically apply the first of all we need\nbasically apply the first of all we need\nbasically apply the first of all we need to check whether this data set is uh\nto check whether this data set is uh\nto check whether this data set is uh this particular y column is balanced or\nthis particular y column is balanced or\nthis particular y column is balanced or imbalanced okay in order to do that I\nimbalanced okay in order to do that I\nimbalanced okay in order to do that I will just write F\nwill just write F\nwill just write F Target if the data set is imbalanced\nTarget if the data set is imbalanced\nTarget if the data set is imbalanced definitely we need to work on that and\ndefinitely we need to work on that and\ndefinitely we need to work on that and try to perform upsampling so if I write\ntry to perform upsampling so if I write\ntry to perform upsampling so if I write y target. Valore counts if I execute\ny target. Valore counts if I execute\ny target. Valore counts if I execute this so here you'll be able to see that\nthis so here you'll be able to see that\nthis so here you'll be able to see that value SC counts will basically give that\nvalue SC counts will basically give that\nvalue SC counts will basically give that how many number of ones are and how many\nhow many number of ones are and how many\nhow many number of ones are and how many number of zeros are so now total number\nnumber of zeros are so now total number\nnumber of zeros are so now total number of ones are 357 and total number of\nof ones are 357 and total number of\nof ones are 357 and total number of zeros are 22 so is this a imbalanced\nzeros are 22 so is this a imbalanced\nzeros are 22 so is this a imbalanced data set probably this is a balanced\ndata set probably this is a balanced\ndata set probably this is a balanced data set so here I'm actually going to\ndata set so here I'm actually going to\ndata set so here I'm actually going to now do train test spit train test spit I\nnow do train test spit train test spit I\nnow do train test spit train test spit I will try to do again train test spit how\nwill try to do again train test spit how\nwill try to do again train test spit how do we do we can quickly do copy the same\ndo we do we can quickly do copy the same\ndo we do we can quickly do copy the same thing entirely I'll copy this entirely\nthing entirely I'll copy this entirely\nthing entirely I'll copy this entirely over here and then I will get my X and Y\nover here and then I will get my X and Y\nover here and then I will get my X and Y so here is my X train X test y train y\nso here is my X train X test y train y\nso here is my X train X test y train y test so train test plate obviously I'll\ntest so train test plate obviously I'll\ntest so train test plate obviously I'll be doing it now in logistic regression\nbe doing it now in logistic regression\nbe doing it now in logistic regression if I go and search for\nif I go and search for\nif I go and search for logistic regression escalar I will be\nlogistic regression escalar I will be\nlogistic regression escalar I will be able to see this what all parameters are\nable to see this what all parameters are\nable to see this what all parameters are there this is basically the L1 Norm or\nthere this is basically the L1 Norm or\nthere this is basically the L1 Norm or L2 Norm or L1 regularization or L2\nL2 Norm or L1 regularization or L2\nL2 Norm or L1 regularization or L2 regularization with respect to whatever\nregularization with respect to whatever\nregularization with respect to whatever things we have discussed in logistic and\nthings we have discussed in logistic and\nthings we have discussed in logistic and then the C value these two parameter\nthen the C value these two parameter\nthen the C value these two parameter values are very much important if I\nvalues are very much important if I\nvalues are very much important if I probably show you over here the penalty\nprobably show you over here the penalty\nprobably show you over here the penalty what kind of penalty whether you want to\nwhat kind of penalty whether you want to\nwhat kind of penalty whether you want to add L2 penalty L1 penalty you can use L2\nadd L2 penalty L1 penalty you can use L2\nadd L2 penalty L1 penalty you can use L2 or L1 the next thing is C this is\nor L1 the next thing is C this is\nor L1 the next thing is C this is nothing but inverse of regularization\nnothing but inverse of regularization\nnothing but inverse of regularization strength this basically says 1 by Lambda\nstrength this basically says 1 by Lambda\nstrength this basically says 1 by Lambda something like that this parameter is\nsomething like that this parameter is\nsomething like that this parameter is also very much important guys class\nalso very much important guys class\nalso very much important guys class weight suppose if your data set is not\nweight suppose if your data set is not\nweight suppose if your data set is not balanced at that point of time you can\nbalanced at that point of time you can\nbalanced at that point of time you can apply weights to your classes if\napply weights to your classes if\napply weights to your classes if probably your data set is balanced you\nprobably your data set is balanced you\nprobably your data set is balanced you can directly use class weight is equal\ncan directly use class weight is equal\ncan directly use class weight is equal to balanced other than that you can use\nto balanced other than that you can use\nto balanced other than that you can use other other weight which you basically\nother other weight which you basically\nother other weight which you basically want so this is specifically some of\nwant so this is specifically some of\nwant so this is specifically some of this right no this is not Ridge or lasso\nthis right no this is not Ridge or lasso\nthis right no this is not Ridge or lasso okay this is logistic in logistic also\nokay this is logistic in logistic also\nokay this is logistic in logistic also you have L1 norm and L2\nyou have L1 norm and L2\nyou have L1 norm and L2 Norms understand probably I missed that\nNorms understand probably I missed that\nNorms understand probably I missed that particular part in the theory but here\nparticular part in the theory but here\nparticular part in the theory but here also you have an L2 penalty norm and L1\nalso you have an L2 penalty norm and L1\nalso you have an L2 penalty norm and L1 penalty Norm I probably did not teach\npenalty Norm I probably did not teach\npenalty Norm I probably did not teach you in theory because if you look see\nyou in theory because if you look see\nyou in theory because if you look see logistic regression can be learned by\nlogistic regression can be learned by\nlogistic regression can be learned by two different ways one is through\ntwo different ways one is through\ntwo different ways one is through probabilistic method and one is through\nprobabilistic method and one is through\nprobabilistic method and one is through geometric method if you go and probably\ngeometric method if you go and probably\ngeometric method if you go and probably see my video that is present with\nsee my video that is present with\nsee my video that is present with respect to logistic regression right now\nrespect to logistic regression right now\nrespect to logistic regression right now in my YouTube channel there I have\nin my YouTube channel there I have\nin my YouTube channel there I have explained you about this L1 and L2 Norms\nexplained you about this L1 and L2 Norms\nexplained you about this L1 and L2 Norms also over there so in this also it is\nalso over there so in this also it is\nalso over there so in this also it is basically present it is a kind of\nbasically present it is a kind of\nbasically present it is a kind of penalty again just for uh using for this\npenalty again just for uh using for this\npenalty again just for uh using for this kind of classification problem so what\nkind of classification problem so what\nkind of classification problem so what I'm actually going to do let's go and\nI'm actually going to do let's go and\nI'm actually going to do let's go and play with the parameters that I am\nplay with the parameters that I am\nplay with the parameters that I am looking at so I will play with two\nlooking at so I will play with two\nlooking at so I will play with two parameters one is params C value here\nparameters one is params C value here\nparameters one is params C value here I'm defining 1 10 20 anything that you\nI'm defining 1 10 20 anything that you\nI'm defining 1 10 20 anything that you can Define one set of values you can\ncan Define one set of values you can\ncan Define one set of values you can Define and there was one more parameter\nDefine and there was one more parameter\nDefine and there was one more parameter which is called as Max iteration this is\nwhich is called as Max iteration this is\nwhich is called as Max iteration this is specifically for grits or CV okay that\nspecifically for grits or CV okay that\nspecifically for grits or CV okay that I'm specifically going to apply so I\nI'm specifically going to apply so I\nI'm specifically going to apply so I will just try to execute this this will\nwill just try to execute this this will\nwill just try to execute this this will be my params now I'm going to quickly\nbe my params now I'm going to quickly\nbe my params now I'm going to quickly Define my model one which will be my\nDefine my model one which will be my\nDefine my model one which will be my logistic regression model so my logistic\nlogistic regression model so my logistic\nlogistic regression model so my logistic regression here by default one value\nregression here by default one value\nregression here by default one value I'll give for C and Max itra let's say\nI'll give for C and Max itra let's say\nI'll give for C and Max itra let's say I'm giving this value later on what I\nI'm giving this value later on what I\nI'm giving this value later on what I will do for this model I'll apply it to\nwill do for this model I'll apply it to\nwill do for this model I'll apply it to grid sear CV so I'm just going to say\ngrid sear CV so I'm just going to say\ngrid sear CV so I'm just going to say grid s CV and I'm going to apply it for\ngrid s CV and I'm going to apply it for\ngrid s CV and I'm going to apply it for model one param grid is equal to params\nmodel one param grid is equal to params\nmodel one param grid is equal to params this parameter that I'm specifically\nthis parameter that I'm specifically\nthis parameter that I'm specifically trying to apply since this is a\ntrying to apply since this is a\ntrying to apply since this is a classification problem and I am not\nclassification problem and I am not\nclassification problem and I am not pretty sure that whether true positive\npretty sure that whether true positive\npretty sure that whether true positive is important or true negative is\nis important or true negative is\nis important or true negative is important I'm going to use F1 scoring\nimportant I'm going to use F1 scoring\nimportant I'm going to use F1 scoring okay F1 scoring is basically again the\nokay F1 scoring is basically again the\nokay F1 scoring is basically again the parametric term which we discussed\nparametric term which we discussed\nparametric term which we discussed yesterday which is nothing but\nyesterday which is nothing but\nyesterday which is nothing but performance metrics and then I'm going\nperformance metrics and then I'm going\nperformance metrics and then I'm going to use CV is equal to 5 so this will be\nto use CV is equal to 5 so this will be\nto use CV is equal to 5 so this will be entirely my model with respect to grid s\nentirely my model with respect to grid s\nentirely my model with respect to grid s CV and I'll be executing this then I\nCV and I'll be executing this then I\nCV and I'll be executing this then I will do model. fit on my X train and Y\nwill do model. fit on my X train and Y\nwill do model. fit on my X train and Y train data so once I execute it here you\ntrain data so once I execute it here you\ntrain data so once I execute it here you can see all the output along with\ncan see all the output along with\ncan see all the output along with warnings a lot of warnings will be\nwarnings a lot of warnings will be\nwarnings a lot of warnings will be coming I don't know because this many\ncoming I don't know because this many\ncoming I don't know because this many parameters are there and finally you can\nparameters are there and finally you can\nparameters are there and finally you can see that this has got selected now if\nsee that this has got selected now if\nsee that this has got selected now if you really want to find out what is your\nyou really want to find out what is your\nyou really want to find out what is your best param score model\nbest param score model\nbest param score model dot best params so here you can see Max\ndot best params so here you can see Max\ndot best params so here you can see Max iteration as\niteration as\niteration as 150 and what you can actually do with\n150 and what you can actually do with\n150 and what you can actually do with respect to your best score model do best\nrespect to your best score model do best\nrespect to your best score model do best score is 95 percentage but still we want\nscore is 95 percentage but still we want\nscore is 95 percentage but still we want to test it with test data so can we do\nto test it with test data so can we do\nto test it with test data so can we do it yes we can definitely do it I'll say\nit yes we can definitely do it I'll say\nit yes we can definitely do it I'll say model do core or I'll say model dot\nmodel do core or I'll say model dot\nmodel do core or I'll say model dot predict on my X test data and this will\npredict on my X test data and this will\npredict on my X test data and this will basically be my y red so this will be my\nbasically be my y red so this will be my\nbasically be my y red so this will be my y red all the Y prediction that I'm\ny red all the Y prediction that I'm\ny red all the Y prediction that I'm actually getting so if you go and see y\nactually getting so if you go and see y\nactually getting so if you go and see y red so these are my ones and zeros with\nred so these are my ones and zeros with\nred so these are my ones and zeros with respect to the Y\nrespect to the Y\nrespect to the Y prediction at finally after getting the\nprediction at finally after getting the\nprediction at finally after getting the prediction values I can apply confusion\nprediction values I can apply confusion\nprediction values I can apply confusion Matrix I hope I have taught you about\nMatrix I hope I have taught you about\nMatrix I hope I have taught you about confusion Matrix so from sklearn do\nconfusion Matrix so from sklearn do\nconfusion Matrix so from sklearn do confusion Matrix sorry sklearn do metrix\nconfusion Matrix sorry sklearn do metrix\nconfusion Matrix sorry sklearn do metrix I'm going to import confusion metrix\nI'm going to import confusion metrix\nI'm going to import confusion metrix classification report and the next thing\nclassification report and the next thing\nclassification report and the next thing that I would like to do is this two I\nthat I would like to do is this two I\nthat I would like to do is this two I will try to import confusion Matrix and\nwill try to import confusion Matrix and\nwill try to import confusion Matrix and classification report now if you want to\nclassification report now if you want to\nclassification report now if you want to see the confusion Matrix with respect to\nsee the confusion Matrix with respect to\nsee the confusion Matrix with respect to your I can just write\nyour I can just write\nyour I can just write Yore frad or Yore test whatever you want\nYore frad or Yore test whatever you want\nYore frad or Yore test whatever you want go ahead with it and this is basically\ngo ahead with it and this is basically\ngo ahead with it and this is basically my confusion Matrix if I put this\nmy confusion Matrix if I put this\nmy confusion Matrix if I put this forward no difference will be there only\nforward no difference will be there only\nforward no difference will be there only this thing will be moving that also I\nthis thing will be moving that also I\nthis thing will be moving that also I showed you 63 118 3 and 4 now finally if\nshowed you 63 118 3 and 4 now finally if\nshowed you 63 118 3 and 4 now finally if I want to accuracy score I can also\nI want to accuracy score I can also\nI want to accuracy score I can also import accuracy score over here so here\nimport accuracy score over here so here\nimport accuracy score over here so here you can see accuracy score is imported I\nyou can see accuracy score is imported I\nyou can see accuracy score is imported I can also find out my accuracy score\ncan also find out my accuracy score\ncan also find out my accuracy score which is my the total accuracy with\nwhich is my the total accuracy with\nwhich is my the total accuracy with respect to this I we can give y test and\nrespect to this I we can give y test and\nrespect to this I we can give y test and Yore PR which we have discussed\nYore PR which we have discussed\nYore PR which we have discussed yesterday this is giving\nyesterday this is giving\nyesterday this is giving 96% if you want detailed Precision\n96% if you want detailed Precision\n96% if you want detailed Precision recall all the score then at that point\nrecall all the score then at that point\nrecall all the score then at that point of time I can use this classification\nof time I can use this classification\nof time I can use this classification report and here I can give white test\nreport and here I can give white test\nreport and here I can give white test and wied here is what I'm actually\nand wied here is what I'm actually\nand wied here is what I'm actually getting so here you can see with respect\ngetting so here you can see with respect\ngetting so here you can see with respect to F1 F1 score Precision recall since\nto F1 F1 score Precision recall since\nto F1 F1 score Precision recall since this is a balanced data set obviously\nthis is a balanced data set obviously\nthis is a balanced data set obviously the performance will be best yes you can\nthe performance will be best yes you can\nthe performance will be best yes you can also use Roc see I'll also show you how\nalso use Roc see I'll also show you how\nalso use Roc see I'll also show you how to use Roc and probably you'll be able\nto use Roc and probably you'll be able\nto use Roc and probably you'll be able to see this you have to probably\nto see this you have to probably\nto see this you have to probably calculate false positive rate two\ncalculate false positive rate two\ncalculate false positive rate two positive rate but don't worry about Roc\npositive rate but don't worry about Roc\npositive rate but don't worry about Roc I will first of all explain you the\nI will first of all explain you the\nI will first of all explain you the theoretical part now let's go ahead and\ntheoretical part now let's go ahead and\ntheoretical part now let's go ahead and discuss about n bias n bias is an\ndiscuss about n bias n bias is an\ndiscuss about n bias n bias is an important algorithm so here I'm just\nimportant algorithm so here I'm just\nimportant algorithm so here I'm just going to go ahead so now let's go ahead\ngoing to go ahead so now let's go ahead\ngoing to go ahead so now let's go ahead and discuss about na bias and here we\nand discuss about na bias and here we\nand discuss about na bias and here we are going to discuss about the intuition\nare going to discuss about the intuition\nare going to discuss about the intuition so na bias is an another amazing\nso na bias is an another amazing\nso na bias is an another amazing algorithm which is specifically used for\nalgorithm which is specifically used for\nalgorithm which is specifically used for classification and this specifically\nclassification and this specifically\nclassification and this specifically works on something called as base\nworks on something called as base\nworks on something called as base theorem now what exactly is base theorem\ntheorem now what exactly is base theorem\ntheorem now what exactly is base theorem first of all we need to understand about\nfirst of all we need to understand about\nfirst of all we need to understand about base theorem let's say that guys I have\nbase theorem let's say that guys I have\nbase theorem let's say that guys I have base theorem let's say that I have an\nbase theorem let's say that I have an\nbase theorem let's say that I have an experiment which is called as rolling a\nexperiment which is called as rolling a\nexperiment which is called as rolling a dis now in rolling a dis how many number\ndis now in rolling a dis how many number\ndis now in rolling a dis how many number of elements I have have so if I say what\nof elements I have have so if I say what\nof elements I have have so if I say what is the probability of 1 then obviously\nis the probability of 1 then obviously\nis the probability of 1 then obviously you'll be saying 1X 6 if I say\nyou'll be saying 1X 6 if I say\nyou'll be saying 1X 6 if I say probability of two then also here you'll\nprobability of two then also here you'll\nprobability of two then also here you'll say 1X 6 if I say probability of three\nsay 1X 6 if I say probability of three\nsay 1X 6 if I say probability of three then I will definitely say it is 1x 6 so\nthen I will definitely say it is 1x 6 so\nthen I will definitely say it is 1x 6 so here you know that this kind of events\nhere you know that this kind of events\nhere you know that this kind of events are basically called as independent\nare basically called as independent\nare basically called as independent events now rolling a dice why it is\nevents now rolling a dice why it is\nevents now rolling a dice why it is called as an independent event because\ncalled as an independent event because\ncalled as an independent event because getting one or two in every experiment\ngetting one or two in every experiment\ngetting one or two in every experiment one is not dependent on two two is not\none is not dependent on two two is not\none is not dependent on two two is not dependent on three so they are all\ndependent on three so they are all\ndependent on three so they are all independent that is the reason why we\nindependent that is the reason why we\nindependent that is the reason why we specifically say is an independent event\nspecifically say is an independent event\nspecifically say is an independent event but if I take an example of dependent\nbut if I take an example of dependent\nbut if I take an example of dependent events let's consider that I have a bag\nevents let's consider that I have a bag\nevents let's consider that I have a bag of marbles okay in this marble I\nof marbles okay in this marble I\nof marbles okay in this marble I basically have three red marbles and I\nbasically have three red marbles and I\nbasically have three red marbles and I have two green marbles now tell me what\nhave two green marbles now tell me what\nhave two green marbles now tell me what is the probability of suppose I have a\nis the probability of suppose I have a\nis the probability of suppose I have a event in the first event I take out a\nevent in the first event I take out a\nevent in the first event I take out a red marble so what is the probability of\nred marble so what is the probability of\nred marble so what is the probability of taking out a red marble so here you can\ntaking out a red marble so here you can\ntaking out a red marble so here you can definitely say that it is\ndefinitely say that it is\ndefinitely say that it is 3x5 okay so this is my first event now\n3x5 okay so this is my first event now\n3x5 okay so this is my first event now in the second event let's say that in\nin the second event let's say that in\nin the second event let's say that in this you have taken out the red marble\nthis you have taken out the red marble\nthis you have taken out the red marble now what is the second second time again\nnow what is the second second time again\nnow what is the second second time again you are taking out the second red marble\nyou are taking out the second red marble\nyou are taking out the second red marble or forget about second Rand marble now\nor forget about second Rand marble now\nor forget about second Rand marble now you want to take out the green marble\nyou want to take out the green marble\nyou want to take out the green marble now what is the probability with respect\nnow what is the probability with respect\nnow what is the probability with respect to taking out a green marble so here\nto taking out a green marble so here\nto taking out a green marble so here you'll be definitely saying that okay\nyou'll be definitely saying that okay\nyou'll be definitely saying that okay one red marble has been removed then the\none red marble has been removed then the\none red marble has been removed then the total number of marbles that are left\ntotal number of marbles that are left\ntotal number of marbles that are left are four so here you can definitely\nare four so here you can definitely\nare four so here you can definitely write that probability of getting a\nwrite that probability of getting a\nwrite that probability of getting a green marble is nothing but 2x4 which is\ngreen marble is nothing but 2x4 which is\ngreen marble is nothing but 2x4 which is nothing but 1x2 so here what is\nnothing but 1x2 so here what is\nnothing but 1x2 so here what is happening first first element you took\nhappening first first element you took\nhappening first first element you took out first marble that you took out first\nout first marble that you took out first\nout first marble that you took out first event from from the first event you took\nevent from from the first event you took\nevent from from the first event you took out red marble from the second event you\nout red marble from the second event you\nout red marble from the second event you took out green marble this two are in\ntook out green marble this two are in\ntook out green marble this two are in these two are dependent events because\nthese two are dependent events because\nthese two are dependent events because the number of marbles are getting\nthe number of marbles are getting\nthe number of marbles are getting reduced as you take out from them so if\nreduced as you take out from them so if\nreduced as you take out from them so if I tell you what is the probability of\nI tell you what is the probability of\nI tell you what is the probability of taking out a red marble and then a green\ntaking out a red marble and then a green\ntaking out a red marble and then a green marble so it's the simple the formula\nmarble so it's the simple the formula\nmarble so it's the simple the formula will be very much simple right which we\nwill be very much simple right which we\nwill be very much simple right which we have already discussed in stats it is\nhave already discussed in stats it is\nhave already discussed in stats it is nothing but probability of probability\nnothing but probability of probability\nnothing but probability of probability of red multiplied by probability of\nof red multiplied by probability of\nof red multiplied by probability of green given Red so this specific thing\ngreen given Red so this specific thing\ngreen given Red so this specific thing is called as conditional probability\nis called as conditional probability\nis called as conditional probability here understand what is happening\nhere understand what is happening\nhere understand what is happening probability of green marble given the\nprobability of green marble given the\nprobability of green marble given the red marble event has occurred here both\nred marble event has occurred here both\nred marble event has occurred here both the events are independent now let me\nthe events are independent now let me\nthe events are independent now let me write it down very nicely so I can write\nwrite it down very nicely so I can write\nwrite it down very nicely so I can write probability of A and B is equal to\nprobability of A and B is equal to\nprobability of A and B is equal to probability of a multiplied probability\nprobability of a multiplied probability\nprobability of a multiplied probability of B divided by probability of a let's\nof B divided by probability of a let's\nof B divided by probability of a let's go and derive something can can I write\ngo and derive something can can I write\ngo and derive something can can I write probability of A and B is equal to\nprobability of A and B is equal to\nprobability of A and B is equal to probability of b and a so answer is yes\nprobability of b and a so answer is yes\nprobability of b and a so answer is yes we can definitely say we can definitely\nwe can definitely say we can definitely\nwe can definitely say we can definitely say if you go and do the calculation\nsay if you go and do the calculation\nsay if you go and do the calculation you'll be able to get the answer you\nyou'll be able to get the answer you\nyou'll be able to get the answer you should not say no now what is the\nshould not say no now what is the\nshould not say no now what is the formula for probability of A and B so\nformula for probability of A and B so\nformula for probability of A and B so here you can basically write probability\nhere you can basically write probability\nhere you can basically write probability of a multiplied by probability of B\nof a multiplied by probability of B\nof a multiplied by probability of B given a if I take out probability of\ngiven a if I take out probability of\ngiven a if I take out probability of green what is probability of green in\ngreen what is probability of green in\ngreen what is probability of green in this particular case 2x 5 what is\nthis particular case 2x 5 what is\nthis particular case 2x 5 what is probability of red 3x 4 for right now\nprobability of red 3x 4 for right now\nprobability of red 3x 4 for right now let's consider this now this part I can\nlet's consider this now this part I can\nlet's consider this now this part I can definitely write as this part I can\ndefinitely write as this part I can\ndefinitely write as this part I can definitely write as probability of B\ndefinitely write as probability of B\ndefinitely write as probability of B multiplied by probability of B\nmultiplied by probability of B\nmultiplied by probability of B probability of B this one probability of\nprobability of B this one probability of\nprobability of B this one probability of B and this will be probability of a\nB and this will be probability of a\nB and this will be probability of a given B so I can definitely write this\ngiven B so I can definitely write this\ngiven B so I can definitely write this much with respect to all this\nmuch with respect to all this\nmuch with respect to all this information now can I derive probability\ninformation now can I derive probability\ninformation now can I derive probability of a is equal to probability of B\nof a is equal to probability of B\nof a is equal to probability of B multiplied by probability of a / B me\nmultiplied by probability of a / B me\nmultiplied by probability of a / B me probability of a given B divided by\nprobability of a given B divided by\nprobability of a given B divided by probability of sorry I'll write this as\nprobability of sorry I'll write this as\nprobability of sorry I'll write this as probability of B given a divided by\nprobability of B given a divided by\nprobability of B given a divided by probability of a and this is\nprobability of a and this is\nprobability of a and this is specifically called as base theorem and\nspecifically called as base theorem and\nspecifically called as base theorem and this is the Crux behind na bias\nthis is the Crux behind na bias\nthis is the Crux behind na bias understand this is the Crux behind the\nunderstand this is the Crux behind the\nunderstand this is the Crux behind the base theorem now let's go ahead and\nbase theorem now let's go ahead and\nbase theorem now let's go ahead and let's discuss about how we are using\nlet's discuss about how we are using\nlet's discuss about how we are using this to solve let's take some examples\nthis to solve let's take some examples\nthis to solve let's take some examples and probably make you understand let's\nand probably make you understand let's\nand probably make you understand let's say that I have some features like X1 X2\nsay that I have some features like X1 X2\nsay that I have some features like X1 X2 X3 X4 X5 like this till xn and I have my\nX3 X4 X5 like this till xn and I have my\nX3 X4 X5 like this till xn and I have my output y so these are my independent\noutput y so these are my independent\noutput y so these are my independent features these all are my independent\nfeatures these all are my independent\nfeatures these all are my independent features these all are my independent\nfeatures these all are my independent\nfeatures these all are my independent features so here I'm going to write\nfeatures so here I'm going to write\nfeatures so here I'm going to write independent features and this is my\nindependent features and this is my\nindependent features and this is my output feature which is also my\noutput feature which is also my\noutput feature which is also my dependent feature now what is happening\ndependent feature now what is happening\ndependent feature now what is happening if I say probability of b or a what does\nif I say probability of b or a what does\nif I say probability of b or a what does this basically mean I need to really\nthis basically mean I need to really\nthis basically mean I need to really find what is the probability of Y and\nfind what is the probability of Y and\nfind what is the probability of Y and you know that guys I will have some\nyou know that guys I will have some\nyou know that guys I will have some values over here and basically I'll have\nvalues over here and basically I'll have\nvalues over here and basically I'll have some output value over here so based on\nsome output value over here so based on\nsome output value over here so based on this input values I need to predict what\nthis input values I need to predict what\nthis input values I need to predict what is the output initially on a training\nis the output initially on a training\nis the output initially on a training data set I will have your input and then\ndata set I will have your input and then\ndata set I will have your input and then your output initially my model will get\nyour output initially my model will get\nyour output initially my model will get trained on this now let's consider what\ntrained on this now let's consider what\ntrained on this now let's consider what this entire terminology is I will try to\nthis entire terminology is I will try to\nthis entire terminology is I will try to write in terms of this equation so I\nwrite in terms of this equation so I\nwrite in terms of this equation so I will say probability of Y given x1a x2a\nwill say probability of Y given x1a x2a\nwill say probability of Y given x1a x2a X3 up till xn then this equation will\nX3 up till xn then this equation will\nX3 up till xn then this equation will become probability of Y see probability\nbecome probability of Y see probability\nbecome probability of Y see probability of Y given X X1 X2 X3 xn this a is\nof Y given X X1 X2 X3 xn this a is\nof Y given X X1 X2 X3 xn this a is nothing but X1 X2 X3 xn and I'm trying\nnothing but X1 X2 X3 xn and I'm trying\nnothing but X1 X2 X3 xn and I'm trying to find out what is the probability of Y\nto find out what is the probability of Y\nto find out what is the probability of Y and then I will write probability of b b\nand then I will write probability of b b\nand then I will write probability of b b is nothing but y but before that what\nis nothing but y but before that what\nis nothing but y but before that what I'll write probability of a / B right a\nI'll write probability of a / B right a\nI'll write probability of a / B right a given b or probability of B probability\ngiven b or probability of B probability\ngiven b or probability of B probability of B is nothing but y multiplied by\nof B is nothing but y multiplied by\nof B is nothing but y multiplied by probability of a given B probability of\nprobability of a given B probability of\nprobability of a given B probability of a given B basically means probability of\na given B basically means probability of\na given B basically means probability of x1a X2 comma xn and given b b is given\nx1a X2 comma xn and given b b is given\nx1a X2 comma xn and given b b is given right so I'm able to find this entire\nright so I'm able to find this entire\nright so I'm able to find this entire value now just a second I made some\nvalue now just a second I made some\nvalue now just a second I made some mistakes I guess now it is correct sorry\nmistakes I guess now it is correct sorry\nmistakes I guess now it is correct sorry I I just missed one term that is this\nI I just missed one term that is this\nI I just missed one term that is this given y this is how it will become and\ngiven y this is how it will become and\ngiven y this is how it will become and this will be equal to probability of a\nthis will be equal to probability of a\nthis will be equal to probability of a that is X1 comma X2 like this up to XL\nthat is X1 comma X2 like this up to XL\nthat is X1 comma X2 like this up to XL so probability of Y multiplied by\nso probability of Y multiplied by\nso probability of Y multiplied by probability of a given y now if I try to\nprobability of a given y now if I try to\nprobability of a given y now if I try to expand this then this will basically\nexpand this then this will basically\nexpand this then this will basically become something like this see\nbecome something like this see\nbecome something like this see probability of Y multiplied by\nprobability of Y multiplied by\nprobability of Y multiplied by probability of X1 given yes a given y\nprobability of X1 given yes a given y\nprobability of X1 given yes a given y sorry given y multiplied by probability\nsorry given y multiplied by probability\nsorry given y multiplied by probability of X2 given y probability of x3 given Y\nof X2 given y probability of x3 given Y\nof X2 given y probability of x3 given Y and like this it will be probability of\nand like this it will be probability of\nand like this it will be probability of xn given y so this will also be y1 Y2 Y3\nxn given y so this will also be y1 Y2 Y3\nxn given y so this will also be y1 Y2 Y3 YN this I can expand it like this and\nYN this I can expand it like this and\nYN this I can expand it like this and then this will basically become\nthen this will basically become\nthen this will basically become probability of X Y 1 multiplied by\nprobability of X Y 1 multiplied by\nprobability of X Y 1 multiplied by probability of X2 multiplied by\nprobability of X2 multiplied by\nprobability of X2 multiplied by probability of x3 like this up to\nprobability of x3 like this up to\nprobability of x3 like this up to probability of xn so this is with\nprobability of xn so this is with\nprobability of xn so this is with respect to all the probability y will be\nrespect to all the probability y will be\nrespect to all the probability y will be different see here for this particular\ndifferent see here for this particular\ndifferent see here for this particular record y will be different for this y\nrecord y will be different for this y\nrecord y will be different for this y will be different for this y will be\nwill be different for this y will be\nwill be different for this y will be different but why output it may be yes\ndifferent but why output it may be yes\ndifferent but why output it may be yes or no right it may be yes or no okay I\nor no right it may be yes or no okay I\nor no right it may be yes or no okay I I'll solve a problem it will make\nI'll solve a problem it will make\nI'll solve a problem it will make everything understand and this will\neverything understand and this will\neverything understand and this will probably be probability of Y it can be\nprobably be probability of Y it can be\nprobably be probability of Y it can be binary multiclass whatever things you\nbinary multiclass whatever things you\nbinary multiclass whatever things you want I'll solve a problem in front of\nwant I'll solve a problem in front of\nwant I'll solve a problem in front of you now let's say that I have my y as\nyou now let's say that I have my y as\nyou now let's say that I have my y as let's say that I have a lot of features\nlet's say that I have a lot of features\nlet's say that I have a lot of features X1 X2 X3 X X4 with respect to this let's\nX1 X2 X3 X X4 with respect to this let's\nX1 X2 X3 X X4 with respect to this let's say in my one of my data set I have this\nsay in my one of my data set I have this\nsay in my one of my data set I have this many x1s this many features and this is\nmany x1s this many features and this is\nmany x1s this many features and this is my y so these are my feature number and\nmy y so these are my feature number and\nmy y so these are my feature number and this is my y let's say that in y I have\nthis is my y let's say that in y I have\nthis is my y let's say that in y I have yes or no so how I will probably write\nyes or no so how I will probably write\nyes or no so how I will probably write we really need to understand this okay I\nwe really need to understand this okay I\nwe really need to understand this okay I will basically\nwill basically\nwill basically say what is the probability of Y is\nsay what is the probability of Y is\nsay what is the probability of Y is equal to yes given this x of I this is\nequal to yes given this x of I this is\nequal to yes given this x of I this is my first record first record of X of I\nmy first record first record of X of I\nmy first record first record of X of I this is my second record of X of I so I\nthis is my second record of X of I so I\nthis is my second record of X of I so I may write like this what is the\nmay write like this what is the\nmay write like this what is the probability of Y being yes if x of I is\nprobability of Y being yes if x of I is\nprobability of Y being yes if x of I is given to you X of I basically means X1\ngiven to you X of I basically means X1\ngiven to you X of I basically means X1 X2 X3 X4 so here you'll obviously write\nX2 X3 X4 so here you'll obviously write\nX2 X3 X4 so here you'll obviously write what kind of equation you'll basically\nwhat kind of equation you'll basically\nwhat kind of equation you'll basically say probability of yes multiplied by\nsay probability of yes multiplied by\nsay probability of yes multiplied by probability of yes multiplied by\nprobability of yes multiplied by\nprobability of yes multiplied by probability of X of 1 given\nprobability of X of 1 given\nprobability of X of 1 given yes multiplied by probability of X2\nyes multiplied by probability of X2\nyes multiplied by probability of X2 given yes probability of x3 given yes\ngiven yes probability of x3 given yes\ngiven yes probability of x3 given yes and probability of X4 given yes divided\nand probability of X4 given yes divided\nand probability of X4 given yes divided by probability of X1 multiplied by\nby probability of X1 multiplied by\nby probability of X1 multiplied by probability of X2 multiplied by\nprobability of X2 multiplied by\nprobability of X2 multiplied by probability of x3 multiplied by\nprobability of x3 multiplied by\nprobability of x3 multiplied by probability of X4 Y is fixed it may be\nprobability of X4 Y is fixed it may be\nprobability of X4 Y is fixed it may be yes or it may be no but with respect to\nyes or it may be no but with respect to\nyes or it may be no but with respect to different different records this value\ndifferent different records this value\ndifferent different records this value may change similarly if I write\nmay change similarly if I write\nmay change similarly if I write probability of Y is equal to no given X\nprobability of Y is equal to no given X\nprobability of Y is equal to no given X of I what it will be then it will be\nof I what it will be then it will be\nof I what it will be then it will be probability of no multiplied by\nprobability of no multiplied by\nprobability of no multiplied by probability of X1 given no then\nprobability of X1 given no then\nprobability of X1 given no then probability of\nprobability of\nprobability of X2 given\nX2 given\nX2 given no probability of\nno probability of\nno probability of x3 given\nx3 given\nx3 given no and probability of X4 given no so\nno and probability of X4 given no so\nno and probability of X4 given no so here because every any input that I give\nhere because every any input that I give\nhere because every any input that I give any input X of I that I give I may\nany input X of I that I give I may\nany input X of I that I give I may either get yes or no so I need to find\neither get yes or no so I need to find\neither get yes or no so I need to find both the probability so probability of\nboth the probability so probability of\nboth the probability so probability of X1 multiplied by probability of X2\nX1 multiplied by probability of X2\nX1 multiplied by probability of X2 multiplied by probability of x3\nmultiplied by probability of x3\nmultiplied by probability of x3 multiplied by probability of X4 see with\nmultiplied by probability of X4 see with\nmultiplied by probability of X4 see with respect to Any X of I the output can be\nrespect to Any X of I the output can be\nrespect to Any X of I the output can be yes or no and I really need to find out\nyes or no and I really need to find out\nyes or no and I really need to find out the probabilities so both the formula is\nthe probabilities so both the formula is\nthe probabilities so both the formula is written over here what is the\nwritten over here what is the\nwritten over here what is the probability of with respect to yes and\nprobability of with respect to yes and\nprobability of with respect to yes and what is the probability with respect to\nwhat is the probability with respect to\nwhat is the probability with respect to no now in this case one common thing you\nno now in this case one common thing you\nno now in this case one common thing you see that this this denominator is fixed\nsee that this this denominator is fixed\nsee that this this denominator is fixed this is definitely fixed it is fixed it\nthis is definitely fixed it is fixed it\nthis is definitely fixed it is fixed it is it is not going to change for both of\nis it is not going to change for both of\nis it is not going to change for both of them and I can consider that this is a\nthem and I can consider that this is a\nthem and I can consider that this is a constant so what I can do I can\nconstant so what I can do I can\nconstant so what I can do I can definitely ignore so here I can\ndefinitely ignore so here I can\ndefinitely ignore so here I can definitely ignore these things ignore\ndefinitely ignore these things ignore\ndefinitely ignore these things ignore this also ignore this Al because see\nthis also ignore this Al because see\nthis also ignore this Al because see this is constant so I don't want to\nthis is constant so I don't want to\nthis is constant so I don't want to consider this in the next time I'll just\nconsider this in the next time I'll just\nconsider this in the next time I'll just use this specific formula to calculate\nuse this specific formula to calculate\nuse this specific formula to calculate the probability now let's say that if my\nthe probability now let's say that if my\nthe probability now let's say that if my first probability for a specific data\nfirst probability for a specific data\nfirst probability for a specific data set yes of X of I is let's say that I'm\nset yes of X of I is let's say that I'm\nset yes of X of I is let's say that I'm getting\ngetting\ngetting as13 and similarly probability of no\nas13 and similarly probability of no\nas13 and similarly probability of no with respect to X of I if I get\nwith respect to X of I if I get\nwith respect to X of I if I get 05 you know that in a binary\n05 you know that in a binary\n05 you know that in a binary classification any values if it get\nclassification any values if it get\nclassification any values if it get greater than or equal to 5 we are going\ngreater than or equal to 5 we are going\ngreater than or equal to 5 we are going to consider it as 1 and if it is less\nto consider it as 1 and if it is less\nto consider it as 1 and if it is less than 0.5 I'm going to consider it as\nthan 0.5 I'm going to consider it as\nthan 0.5 I'm going to consider it as zero now I'm getting values like this 13\nzero now I'm getting values like this 13\nzero now I'm getting values like this 13 and .1 05 obviously I'm getting .13 05\nand .1 05 obviously I'm getting .13 05\nand .1 05 obviously I'm getting .13 05 so we do something called as\nso we do something called as\nso we do something called as normalization it says that if I really\nnormalization it says that if I really\nnormalization it says that if I really want to find out the probability of X\nwant to find out the probability of X\nwant to find out the probability of X with X of I if I do normalization it is\nwith X of I if I do normalization it is\nwith X of I if I do normalization it is nothing but .13 divided by .13 +\nnothing but .13 divided by .13 +\nnothing but .13 divided by .13 + 05 72 this is nothing but\n05 72 this is nothing but\n05 72 this is nothing but 72% and similarly if I do for\n72% and similarly if I do for\n72% and similarly if I do for probability of no given X of I here\nprobability of no given X of I here\nprobability of no given X of I here obviously it will say 1 - 72 which will\nobviously it will say 1 - 72 which will\nobviously it will say 1 - 72 which will be your remaining answer that is 28\nbe your remaining answer that is 28\nbe your remaining answer that is 28 which is nothing but 28% so your final\nwhich is nothing but 28% so your final\nwhich is nothing but 28% so your final answer will be this one this formulas\nanswer will be this one this formulas\nanswer will be this one this formulas you have to remember now we'll solve a\nyou have to remember now we'll solve a\nyou have to remember now we'll solve a problem let's solve a problem this will\nproblem let's solve a problem this will\nproblem let's solve a problem this will be a very very interesting problem let's\nbe a very very interesting problem let's\nbe a very very interesting problem let's say I have a data set which has like\nsay I have a data set which has like\nsay I have a data set which has like this feature day let me just copy this\nthis feature day let me just copy this\nthis feature day let me just copy this data set okay for you all now in this\ndata set okay for you all now in this\ndata set okay for you all now in this data set I want to take out some\ndata set I want to take out some\ndata set I want to take out some information let's take out Outlook\ninformation let's take out Outlook\ninformation let's take out Outlook table now based on this output Outlook\ntable now based on this output Outlook\ntable now based on this output Outlook feature see over here Outlook my day\nfeature see over here Outlook my day\nfeature see over here Outlook my day outlook temperature humidity wind are\noutlook temperature humidity wind are\noutlook temperature humidity wind are the input features independent feature\nthe input features independent feature\nthe input features independent feature this is my output feature this one that\nthis is my output feature this one that\nthis is my output feature this one that you are probably seeing play tennis is\nyou are probably seeing play tennis is\nyou are probably seeing play tennis is my output feature which is specifically\nmy output feature which is specifically\nmy output feature which is specifically a binary\na binary\na binary classification so what I'm actually\nclassification so what I'm actually\nclassification so what I'm actually going to do I'm basically going to take\ngoing to do I'm basically going to take\ngoing to do I'm basically going to take my Outlook feature and based on this\nmy Outlook feature and based on this\nmy Outlook feature and based on this Outlook feature I will just try to\nOutlook feature I will just try to\nOutlook feature I will just try to create a smaller table which will give\ncreate a smaller table which will give\ncreate a smaller table which will give some information now based on Outlook\nsome information now based on Outlook\nsome information now based on Outlook first of all try to find out how many\nfirst of all try to find out how many\nfirst of all try to find out how many categories are there in Outlook one is\ncategories are there in Outlook one is\ncategories are there in Outlook one is sunny one is\nsunny one is\nsunny one is overcast and one is rain right three\novercast and one is rain right three\novercast and one is rain right three categories are there so I'm going to\ncategories are there so I'm going to\ncategories are there so I'm going to write it down over here Sunny overcast\nwrite it down over here Sunny overcast\nwrite it down over here Sunny overcast and rain so these three are my features\nand rain so these three are my features\nand rain so these three are my features with respect to Sunny uh with Outlook I\nwith respect to Sunny uh with Outlook I\nwith respect to Sunny uh with Outlook I have three categories one is sunny one\nhave three categories one is sunny one\nhave three categories one is sunny one is overcast and one is RA here I'm going\nis overcast and one is RA here I'm going\nis overcast and one is RA here I'm going to basically say with respect to Sunny\nto basically say with respect to Sunny\nto basically say with respect to Sunny how many yes are there and how many no\nhow many yes are there and how many no\nhow many yes are there and how many no are there and what is the probability of\nare there and what is the probability of\nare there and what is the probability of yes and probability of no so I'm going\nyes and probability of no so I'm going\nyes and probability of no so I'm going to again write it over here so this is\nto again write it over here so this is\nto again write it over here so this is my Outlook feature\nmy Outlook feature\nmy Outlook feature and then I have categories first yes no\nand then I have categories first yes no\nand then I have categories first yes no Sunny overcast rain yes no then\nSunny overcast rain yes no then\nSunny overcast rain yes no then probability of yes and probability of no\nprobability of yes and probability of no\nprobability of yes and probability of no now the next thing that we need to find\nnow the next thing that we need to find\nnow the next thing that we need to find out is that with respect to Sunny how\nout is that with respect to Sunny how\nout is that with respect to Sunny how many of them are yes see yes we have so\nmany of them are yes see yes we have so\nmany of them are yes see yes we have so when we have sunny over here the answer\nwhen we have sunny over here the answer\nwhen we have sunny over here the answer is no so I will increase the count over\nis no so I will increase the count over\nis no so I will increase the count over here one then again I have sunny again\nhere one then again I have sunny again\nhere one then again I have sunny again answer is no so I'm going to increase\nanswer is no so I'm going to increase\nanswer is no so I'm going to increase the count to two with this sunny this is\nthe count to two with this sunny this is\nthe count to two with this sunny this is basically no okay so again I'm going to\nbasically no okay so again I'm going to\nbasically no okay so again I'm going to increase the count to three now with\nincrease the count to three now with\nincrease the count to three now with sunny how many of them are yes one and\nsunny how many of them are yes one and\nsunny how many of them are yes one and two so I have this one and this one so I\ntwo so I have this one and this one so I\ntwo so I have this one and this one so I have two so I'm going to say with\nhave two so I'm going to say with\nhave two so I'm going to say with respect to Sunny I have two\nrespect to Sunny I have two\nrespect to Sunny I have two yes understand Outlook is my X1 X1\nyes understand Outlook is my X1 X1\nyes understand Outlook is my X1 X1 feature let's consider now the next\nfeature let's consider now the next\nfeature let's consider now the next thing is that let's see with respect to\nthing is that let's see with respect to\nthing is that let's see with respect to overcost with overcast how many of them\novercost with overcast how many of them\novercost with overcast how many of them are yes so this overcast is there yes 1\nare yes so this overcast is there yes 1\nare yes so this overcast is there yes 1 2 3 and four so total four yes are there\n2 3 and four so total four yes are there\n2 3 and four so total four yes are there with respect to overcast then with\nwith respect to overcast then with\nwith respect to overcast then with respect to overcast how many are on no\nrespect to overcast how many are on no\nrespect to overcast how many are on no you can go ah and find out it is\nyou can go ah and find out it is\nyou can go ah and find out it is basically zero NOS then with respect to\nbasically zero NOS then with respect to\nbasically zero NOS then with respect to rain how many of them are yes so here\nrain how many of them are yes so here\nrain how many of them are yes so here you can see with respect to one rain yes\nyou can see with respect to one rain yes\nyou can see with respect to one rain yes yes no no so this is nothing but 3 2\nyes no no so this is nothing but 3 2\nyes no no so this is nothing but 3 2 let's try to find out there are three is\nlet's try to find out there are three is\nlet's try to find out there are three is two or\ntwo or\ntwo or not one here also one yes is there right\nnot one here also one yes is there right\nnot one here also one yes is there right so 3 yes two NOS so the total number of\nso 3 yes two NOS so the total number of\nso 3 yes two NOS so the total number of yes and NOS if you count it there are\nyes and NOS if you count it there are\nyes and NOS if you count it there are nine yes and five NOS this is my total\nnine yes and five NOS this is my total\nnine yes and five NOS this is my total count so if you totally count this 9 + 5\ncount so if you totally count this 9 + 5\ncount so if you totally count this 9 + 5 is 14 you'll be able to compare that\nis 14 you'll be able to compare that\nis 14 you'll be able to compare that there will be 9 yes and five NOS what is\nthere will be 9 yes and five NOS what is\nthere will be 9 yes and five NOS what is the probability of yes when Sunny is\nthe probability of yes when Sunny is\nthe probability of yes when Sunny is given so here you have 2X 9 here you\ngiven so here you have 2X 9 here you\ngiven so here you have 2X 9 here you have 4X 9 here you have 3x 9 now if if I\nhave 4X 9 here you have 3x 9 now if if I\nhave 4X 9 here you have 3x 9 now if if I say what is the probability of no given\nsay what is the probability of no given\nsay what is the probability of no given Sunny now see probability of yes given\nSunny now see probability of yes given\nSunny now see probability of yes given Sunny probability of yes given forecast\nSunny probability of yes given forecast\nSunny probability of yes given forecast probability of yes given rain so it is\nprobability of yes given rain so it is\nprobability of yes given rain so it is basically that I will just try to write\nbasically that I will just try to write\nbasically that I will just try to write it in a simpler manner so that you'll\nit in a simpler manner so that you'll\nit in a simpler manner so that you'll not get confused okay so this is my\nnot get confused okay so this is my\nnot get confused okay so this is my probability of yes and this is my\nprobability of yes and this is my\nprobability of yes and this is my probability of no but understand what\nprobability of no but understand what\nprobability of no but understand what does this basically mean this\ndoes this basically mean this\ndoes this basically mean this terminology basically means probability\nterminology basically means probability\nterminology basically means probability of yes given Sunny probability of yes\nof yes given Sunny probability of yes\nof yes given Sunny probability of yes given overcast probability of yes given\ngiven overcast probability of yes given\ngiven overcast probability of yes given rain similarly what is probability of no\nrain similarly what is probability of no\nrain similarly what is probability of no probability of no obviously you know\nprobability of no obviously you know\nprobability of no obviously you know that 3x 5 is my first probability then\nthat 3x 5 is my first probability then\nthat 3x 5 is my first probability then you have 0x 5 and then you have 2X 5 now\nyou have 0x 5 and then you have 2X 5 now\nyou have 0x 5 and then you have 2X 5 now with respect to the next feature let's\nwith respect to the next feature let's\nwith respect to the next feature let's consider that I'm going to consider one\nconsider that I'm going to consider one\nconsider that I'm going to consider one more feature and in this feature I will\nmore feature and in this feature I will\nmore feature and in this feature I will say let's consider\nsay let's consider\nsay let's consider temperature okay let's consider\ntemperature okay let's consider\ntemperature okay let's consider temperature now in temperature how many\ntemperature now in temperature how many\ntemperature now in temperature how many features I have or how many categories I\nfeatures I have or how many categories I\nfeatures I have or how many categories I have I have hot you can see hot mild and\nhave I have hot you can see hot mild and\nhave I have hot you can see hot mild and and cold now with respect to hot mild\nand cold now with respect to hot mild\nand cold now with respect to hot mild cold here also I will be having yes no\ncold here also I will be having yes no\ncold here also I will be having yes no probability of yes and probability of no\nprobability of yes and probability of no\nprobability of yes and probability of no now try to find out with respect to hot\nnow try to find out with respect to hot\nnow try to find out with respect to hot how many are yes so here no is there\nhow many are yes so here no is there\nhow many are yes so here no is there here also no is there two NOS uh 1 yes\nhere also no is there two NOS uh 1 yes\nhere also no is there two NOS uh 1 yes uh 2 yes so two yes and two NOS probably\nuh 2 yes so two yes and two NOS probably\nuh 2 yes so two yes and two NOS probably then similarly with respect to mild mild\nthen similarly with respect to mild mild\nthen similarly with respect to mild mild how many are there 1 yes 1 No 2 yes 3s\nhow many are there 1 yes 1 No 2 yes 3s\nhow many are there 1 yes 1 No 2 yes 3s 4s 4S and two knows okay so here you\n4s 4S and two knows okay so here you\n4s 4S and two knows okay so here you basically go and calculate 4 yes and two\nbasically go and calculate 4 yes and two\nbasically go and calculate 4 yes and two knows with respect to cold how many are\nknows with respect to cold how many are\nknows with respect to cold how many are there cool cool or cold 1 yes 1 No 2 yes\nthere cool cool or cold 1 yes 1 No 2 yes\nthere cool cool or cold 1 yes 1 No 2 yes 3 S 3 S and 1 no so here I have\n3 S 3 S and 1 no so here I have\n3 S 3 S and 1 no so here I have specifically have 3s and 1 no again the\nspecifically have 3s and 1 no again the\nspecifically have 3s and 1 no again the total number is 9 and five which will be\ntotal number is 9 and five which will be\ntotal number is 9 and five which will be equal to the same thing that what we\nequal to the same thing that what we\nequal to the same thing that what we have got now really go ahead with\nhave got now really go ahead with\nhave got now really go ahead with finding probability of yes given hot so\nfinding probability of yes given hot so\nfinding probability of yes given hot so it will be 2x 9 over here then here it\nit will be 2x 9 over here then here it\nit will be 2x 9 over here then here it will be how much 4X 9 here it will be 3x\nwill be how much 4X 9 here it will be 3x\nwill be how much 4X 9 here it will be 3x 9 again here what will be the\n9 again here what will be the\n9 again here what will be the probability of no given given hot so\nprobability of no given given hot so\nprobability of no given given hot so it'll be 2x 5 2x 5 1X 5 so this two\nit'll be 2x 5 2x 5 1X 5 so this two\nit'll be 2x 5 2x 5 1X 5 so this two tables has already been created and\ntables has already been created and\ntables has already been created and finally with respect to play the total\nfinally with respect to play the total\nfinally with respect to play the total number of plays are yes is 9 no is five\nnumber of plays are yes is 9 no is five\nnumber of plays are yes is 9 no is five and the answer is total 14 if if I say\nand the answer is total 14 if if I say\nand the answer is total 14 if if I say what is the probability of yes only yes\nwhat is the probability of yes only yes\nwhat is the probability of yes only yes then it is nothing but 9 by4 what is the\nthen it is nothing but 9 by4 what is the\nthen it is nothing but 9 by4 what is the probability of no it is nothing but\nprobability of no it is nothing but\nprobability of no it is nothing but 5x4 okay so this two values also you\n5x4 okay so this two values also you\n5x4 okay so this two values also you require now let's say that you get a new\nrequire now let's say that you get a new\nrequire now let's say that you get a new data set you need get a new data set\ndata set you need get a new data set\ndata set you need get a new data set let's say you get a new test data where\nlet's say you get a new test data where\nlet's say you get a new test data where it says that suppose if you are having\nit says that suppose if you are having\nit says that suppose if you are having sunny and hot tell me what is the output\nsunny and hot tell me what is the output\nsunny and hot tell me what is the output so this is my problem statement so let\nso this is my problem statement so let\nso this is my problem statement so let me write it down so here I will write\nme write it down so here I will write\nme write it down so here I will write probability of yes given Sunny comma hot\nprobability of yes given Sunny comma hot\nprobability of yes given Sunny comma hot then here I will write probability of\nthen here I will write probability of\nthen here I will write probability of yes multiplied by probability of so here\nyes multiplied by probability of so here\nyes multiplied by probability of so here I will write probability of Sunny given\nI will write probability of Sunny given\nI will write probability of Sunny given yes multiplied by probability of hot\nyes multiplied by probability of hot\nyes multiplied by probability of hot given yes divided by what is it\ngiven yes divided by what is it\ngiven yes divided by what is it probability of Sunny multiplied by\nprobability of Sunny multiplied by\nprobability of Sunny multiplied by probability of hot\nequation because it is a\nequation because it is a constant because probability of no also\nconstant because probability of no also\nconstant because probability of no also I'll be getting the same value 9 by4 so\nI'll be getting the same value 9 by4 so\nI'll be getting the same value 9 by4 so probability of yes I'm going to replace\nprobability of yes I'm going to replace\nprobability of yes I'm going to replace it with 9\nit with 9\nit with 9 by4 multiplied by 2x 9 then probability\nby4 multiplied by 2x 9 then probability\nby4 multiplied by 2x 9 then probability of hot given yes so I am going to get 2\nof hot given yes so I am going to get 2\nof hot given yes so I am going to get 2 by 9 so\nby 9 so\nby 9 so here 99 cancel or 2 1 7 then this is\nhere 99 cancel or 2 1 7 then this is\nhere 99 cancel or 2 1 7 then this is nothing but 2 by\n6331 I read this statement little bit\n6331 I read this statement little bit wrong it should be probability of Sunny\nwrong it should be probability of Sunny\nwrong it should be probability of Sunny given yes now go ahead and calculate go\ngiven yes now go ahead and calculate go\ngiven yes now go ahead and calculate go ahead and calculate what is probability\nahead and calculate what is probability\nahead and calculate what is probability of no given sunny and hot so here you\nof no given sunny and hot so here you\nof no given sunny and hot so here you have probability of no multiplied by\nhave probability of no multiplied by\nhave probability of no multiplied by probability of Sunny given\nprobability of Sunny given\nprobability of Sunny given no multiplied by probability of hot\nno multiplied by probability of hot\nno multiplied by probability of hot given\ngiven\ngiven no divided by probability of Sunny\nno divided by probability of Sunny\nno divided by probability of Sunny multiplied by probability of heart this\nmultiplied by probability of heart this\nmultiplied by probability of heart this will get cancelled denominator is a\nwill get cancelled denominator is a\nwill get cancelled denominator is a constant guys this is a constant so what\nconstant guys this is a constant so what\nconstant guys this is a constant so what is probability of no so probability of\nis probability of no so probability of\nis probability of no so probability of no is nothing but 5 by4 so I will write\nno is nothing but 5 by4 so I will write\nno is nothing but 5 by4 so I will write over here 5 by4 multiplied by\nover here 5 by4 multiplied by\nover here 5 by4 multiplied by probability of Sunny given no what is\nprobability of Sunny given no what is\nprobability of Sunny given no what is probability of Sunny given no what is\nprobability of Sunny given no what is\nprobability of Sunny given no what is probability of Sunny given no is nothing\nprobability of Sunny given no is nothing\nprobability of Sunny given no is nothing but probability of Sunny given no is\nbut probability of Sunny given no is\nbut probability of Sunny given no is nothing but 3x 5 so here I'm going to\nnothing but 3x 5 so here I'm going to\nnothing but 3x 5 so here I'm going to get 3x 5 multiplied probability of H\nget 3x 5 multiplied probability of H\nget 3x 5 multiplied probability of H given no that is nothing but 2x 5 so 2x\ngiven no that is nothing but 2x 5 so 2x\ngiven no that is nothing but 2x 5 so 2x 5 is here 3x 5 is there five and five\n5 is here 3x 5 is there five and five\n5 is here 3x 5 is there five and five will get cancelled 2 1 2 7 and then I'm\nwill get cancelled 2 1 2 7 and then I'm\nwill get cancelled 2 1 2 7 and then I'm getting 3x 35 which is nothing but\ngetting 3x 35 which is nothing but\ngetting 3x 35 which is nothing but calculator uh if I'm actually getting\ncalculator uh if I'm actually getting\ncalculator uh if I'm actually getting three ID by 35 it's nothing but\nthree ID by 35 it's nothing but\nthree ID by 35 it's nothing but 857 I will write it down again\n857 I will write it down again\n857 I will write it down again probability of yes given Sunny comma hot\nprobability of yes given Sunny comma hot\nprobability of yes given Sunny comma hot which is my independent feature is\nwhich is my independent feature is\nwhich is my independent feature is nothing but\nnothing but\nnothing but 031\n031\n031 031 and this is probability of no given\n031 and this is probability of no given\n031 and this is probability of no given Sunny comma hot 85 now we'll try to\nSunny comma hot 85 now we'll try to\nSunny comma hot 85 now we'll try to normalize this 85 + Point divided by 031\nnormalize this 85 + Point divided by 031\nnormalize this 85 + Point divided by 031 + 085 73 this is nothing but 73% and\n+ 085 73 this is nothing but 73% and\n+ 085 73 this is nothing but 73% and here I can basically say 1 -73 which is\nhere I can basically say 1 -73 which is\nhere I can basically say 1 -73 which is my27 which is nothing but 27% if the\nmy27 which is nothing but 27% if the\nmy27 which is nothing but 27% if the input comes as sunny and hot if the\ninput comes as sunny and hot if the\ninput comes as sunny and hot if the weather is sunny and hot what will the\nweather is sunny and hot what will the\nweather is sunny and hot what will the person do whether he will play or not\nperson do whether he will play or not\nperson do whether he will play or not the answer is no okay now my next\nthe answer is no okay now my next\nthe answer is no okay now my next question will be that if your new data\nquestion will be that if your new data\nquestion will be that if your new data is overcast and Mild now tell me what\nis overcast and Mild now tell me what\nis overcast and Mild now tell me what will be the probability using name bias\nwill be the probability using name bias\nwill be the probability using name bias now you can add any number of features\nnow you can add any number of features\nnow you can add any number of features let's say that I will say that okay\nlet's say that I will say that okay\nlet's say that I will say that okay let's let's say that I will I will\nlet's let's say that I will I will\nlet's let's say that I will I will probably say we can consider humidity\nprobably say we can consider humidity\nprobably say we can consider humidity mind wind also you basically create this\nmind wind also you basically create this\nmind wind also you basically create this kind of table to find it out but this\nkind of table to find it out but this\nkind of table to find it out but this will be an assignment just do\nwill be an assignment just do\nwill be an assignment just do it overcast and Mild if it is with\nit overcast and Mild if it is with\nit overcast and Mild if it is with respect to NB try to solve it so the\nrespect to NB try to solve it so the\nrespect to NB try to solve it so the second algorithm that we are going to\nsecond algorithm that we are going to\nsecond algorithm that we are going to discuss about is something called as KNN\ndiscuss about is something called as KNN\ndiscuss about is something called as KNN algorithm KNN algorithm is a very simple\nalgorithm KNN algorithm is a very simple\nalgorithm KNN algorithm is a very simple problem statement okay which can be used\nproblem statement okay which can be used\nproblem statement okay which can be used to solve both classification and\nto solve both classification and\nto solve both classification and regression so KNN basically means K\nregression so KNN basically means K\nregression so KNN basically means K nearest neighbor let's first of all\nnearest neighbor let's first of all\nnearest neighbor let's first of all discuss about classification problem\ndiscuss about classification problem\ndiscuss about classification problem number one classification problem let's\nnumber one classification problem let's\nnumber one classification problem let's say that I have a binary classification\nsay that I have a binary classification\nsay that I have a binary classification problem which looks like this I have two\nproblem which looks like this I have two\nproblem which looks like this I have two data points like this one and this is\ndata points like this one and this is\ndata points like this one and this is another one suppose a new data point\nanother one suppose a new data point\nanother one suppose a new data point suppose a new data point which comes\nsuppose a new data point which comes\nsuppose a new data point which comes over\nover\nover here then how do I say that whether this\nhere then how do I say that whether this\nhere then how do I say that whether this belongs to this category or whether it\nbelongs to this category or whether it\nbelongs to this category or whether it belongs to this category if I probably\nbelongs to this category if I probably\nbelongs to this category if I probably create a logistic regression I may\ncreate a logistic regression I may\ncreate a logistic regression I may divide a line but in this particular\ndivide a line but in this particular\ndivide a line but in this particular scenario how do we Define or how do we\nscenario how do we Define or how do we\nscenario how do we Define or how do we come to a conclusion that\ncome to a conclusion that\ncome to a conclusion that whether this will belong to this\nwhether this will belong to this\nwhether this will belong to this category or this category so for here we\ncategory or this category so for here we\ncategory or this category so for here we basically use something called as K\nbasically use something called as K\nbasically use something called as K nearest neighbor let's say that I say\nnearest neighbor let's say that I say\nnearest neighbor let's say that I say that my K value is five so what it is\nthat my K value is five so what it is\nthat my K value is five so what it is going to do it is going to basically\ngoing to do it is going to basically\ngoing to do it is going to basically take the five nearest closest point\ntake the five nearest closest point\ntake the five nearest closest point let's say from this you have two nearest\nlet's say from this you have two nearest\nlet's say from this you have two nearest closest point and from here you have\nclosest point and from here you have\nclosest point and from here you have three nearest closest point so here we\nthree nearest closest point so here we\nthree nearest closest point so here we basically see from the distance the\nbasically see from the distance the\nbasically see from the distance the distance that which is my nearest point\ndistance that which is my nearest point\ndistance that which is my nearest point now in this particular case you see that\nnow in this particular case you see that\nnow in this particular case you see that maximum number of points are from Red\nmaximum number of points are from Red\nmaximum number of points are from Red categories from Red from Red categories\ncategories from Red from Red categories\ncategories from Red from Red categories I'm getting three points and from White\nI'm getting three points and from White\nI'm getting three points and from White categories I'm getting two points now in\ncategories I'm getting two points now in\ncategories I'm getting two points now in this particular scenario maximum number\nthis particular scenario maximum number\nthis particular scenario maximum number of categories from where it is coming we\nof categories from where it is coming we\nof categories from where it is coming we basically categorize that into that\nbasically categorize that into that\nbasically categorize that into that particular class just with the help of\nparticular class just with the help of\nparticular class just with the help of distance which all distance we\ndistance which all distance we\ndistance which all distance we specifically use we use two distance one\nspecifically use we use two distance one\nspecifically use we use two distance one is ukan distance and the other one is\nis ukan distance and the other one is\nis ukan distance and the other one is something called as Manhattan distance\nsomething called as Manhattan distance\nsomething called as Manhattan distance so ukan and Manhattan distance now what\nso ukan and Manhattan distance now what\nso ukan and Manhattan distance now what does ukan distance basically say suppose\ndoes ukan distance basically say suppose\ndoes ukan distance basically say suppose if this is your two points which is\nif this is your two points which is\nif this is your two points which is denoted by X1 y1\ndenoted by X1 y1\ndenoted by X1 y1 X2 Y2 ukine distance in order to\nX2 Y2 ukine distance in order to\nX2 Y2 ukine distance in order to calculate we apply a formula which looks\ncalculate we apply a formula which looks\ncalculate we apply a formula which looks like this X2 - X1 s + Y2 - y1 s whereas\nlike this X2 - X1 s + Y2 - y1 s whereas\nlike this X2 - X1 s + Y2 - y1 s whereas in the case of magetan distance suppose\nin the case of magetan distance suppose\nin the case of magetan distance suppose this are my two points then we calculate\nthis are my two points then we calculate\nthis are my two points then we calculate the distance in this way we calculate\nthe distance in this way we calculate\nthe distance in this way we calculate the distance from here then here right\nthe distance from here then here right\nthe distance from here then here right this is the distance we calculate we\nthis is the distance we calculate we\nthis is the distance we calculate we don't calculate the hypothenuse distance\ndon't calculate the hypothenuse distance\ndon't calculate the hypothenuse distance so this is the basic difference between\nso this is the basic difference between\nso this is the basic difference between ukan and magetan distance now you may be\nukan and magetan distance now you may be\nukan and magetan distance now you may be thinking Chris then fine that is for\nthinking Chris then fine that is for\nthinking Chris then fine that is for classification problem for regression\nclassification problem for regression\nclassification problem for regression what do we do for regression also it is\nwhat do we do for regression also it is\nwhat do we do for regression also it is very much simple suppose I have all the\nvery much simple suppose I have all the\nvery much simple suppose I have all the data points which looks like this now\ndata points which looks like this now\ndata points which looks like this now for a new data point like this if I want\nfor a new data point like this if I want\nfor a new data point like this if I want to calculate then we basically take up\nto calculate then we basically take up\nto calculate then we basically take up the nearest Five Points let's say my K\nthe nearest Five Points let's say my K\nthe nearest Five Points let's say my K is five k is a hyper parameter which we\nis five k is a hyper parameter which we\nis five k is a hyper parameter which we play now suppose let's say that K it\nplay now suppose let's say that K it\nplay now suppose let's say that K it finds the nearest point over here here\nfinds the nearest point over here here\nfinds the nearest point over here here here here and here so if we need to find\nhere here and here so if we need to find\nhere here and here so if we need to find out the point for this particular output\nout the point for this particular output\nout the point for this particular output with respect to the K is equal to 5 it\nwith respect to the K is equal to 5 it\nwith respect to the K is equal to 5 it will try to calculate the average of all\nwill try to calculate the average of all\nwill try to calculate the average of all the points once it calculates the\nthe points once it calculates the\nthe points once it calculates the average of all the points that becomes\naverage of all the points that becomes\naverage of all the points that becomes your output so regression and\nyour output so regression and\nyour output so regression and classification that is the only\nclassification that is the only\nclassification that is the only difference because this K is actually an\ndifference because this K is actually an\ndifference because this K is actually an hyper parameter we try with K is equal\nhyper parameter we try with K is equal\nhyper parameter we try with K is equal to 1 to 50 and then we probably try to\nto 1 to 50 and then we probably try to\nto 1 to 50 and then we probably try to check the error rate and if the error\ncheck the error rate and if the error\ncheck the error rate and if the error rate is less then only we select the\nrate is less then only we select the\nrate is less then only we select the model now two more things with respect\nmodel now two more things with respect\nmodel now two more things with respect to K nearish neighbor K nearest neighbor\nto K nearish neighbor K nearest neighbor\nto K nearish neighbor K nearest neighbor works very bad with respect to two\nworks very bad with respect to two\nworks very bad with respect to two things one is outliers and and one is\nthings one is outliers and and one is\nthings one is outliers and and one is imbalanced data set now if I have an\nimbalanced data set now if I have an\nimbalanced data set now if I have an outlier let's say I have an outlier over\noutlier let's say I have an outlier over\noutlier let's say I have an outlier over here this is one of my categories like\nhere this is one of my categories like\nhere this is one of my categories like this and this is my another category\nthis and this is my another category\nthis and this is my another category let's consider that I have some outliers\nlet's consider that I have some outliers\nlet's consider that I have some outliers which looks like this now if I'm trying\nwhich looks like this now if I'm trying\nwhich looks like this now if I'm trying to find out the point for this you can\nto find out the point for this you can\nto find out the point for this you can see that the nearest point is basically\nsee that the nearest point is basically\nsee that the nearest point is basically blue only and it belongs to the blue\nblue only and it belongs to the blue\nblue only and it belongs to the blue category but because this outlier you\ncategory but because this outlier you\ncategory but because this outlier you know it'll consider that the nearest\nknow it'll consider that the nearest\nknow it'll consider that the nearest neighbor is this so then this will be\nneighbor is this so then this will be\nneighbor is this so then this will be basically treated in this group only\nbasically treated in this group only\nbasically treated in this group only formula for Manhattan distance it uses\nformula for Manhattan distance it uses\nformula for Manhattan distance it uses modulus X2 - X1 + Y2 - y1 mode X2 - X1\nmodulus X2 - X1 + Y2 - y1 mode X2 - X1\nmodulus X2 - X1 + Y2 - y1 mode X2 - X1 Y2 - y1 uh this was it from my side guys\nY2 - y1 uh this was it from my side guys\nY2 - y1 uh this was it from my side guys and yes I've also made detailed videos\nand yes I've also made detailed videos\nand yes I've also made detailed videos about whatever topics we have discussed\nabout whatever topics we have discussed\nabout whatever topics we have discussed today you can directly go and search for\ntoday you can directly go and search for\ntoday you can directly go and search for that particular\nthat particular\nthat particular topic so this is the agenda of this\ntopic so this is the agenda of this\ntopic so this is the agenda of this session we will try to complete this all\nsession we will try to complete this all\nsession we will try to complete this all things again here we are going to\nthings again here we are going to\nthings again here we are going to understand the mathematical equations\nunderstand the mathematical equations\nunderstand the mathematical equations and all uh so today's session we are\nand all uh so today's session we are\nand all uh so today's session we are basically going to discuss about uh\nbasically going to discuss about uh\nbasically going to discuss about uh decision tree okay and uh in this\ndecision tree okay and uh in this\ndecision tree okay and uh in this session we are going to basically\nsession we are going to basically\nsession we are going to basically understand what is the exact purpose of\nunderstand what is the exact purpose of\nunderstand what is the exact purpose of decision tree with the help of decision\ndecision tree with the help of decision\ndecision tree with the help of decision tree you are actually solving two\ntree you are actually solving two\ntree you are actually solving two different problems one is regression and\ndifferent problems one is regression and\ndifferent problems one is regression and the other one is\nthe other one is\nthe other one is classification so we'll try to\nclassification so we'll try to\nclassification so we'll try to understand both this particular part\nunderstand both this particular part\nunderstand both this particular part well we will take a specific data set\nwell we will take a specific data set\nwell we will take a specific data set and try to solve those problems now\nand try to solve those problems now\nand try to solve those problems now coming to the decision tree one thing\ncoming to the decision tree one thing\ncoming to the decision tree one thing you need to understand I'll say that if\nyou need to understand I'll say that if\nyou need to understand I'll say that if age is less than 8 let's say I'm writing\nage is less than 8 let's say I'm writing\nage is less than 8 let's say I'm writing this condition if age is less than or\nthis condition if age is less than or\nthis condition if age is less than or equal to 18 I'm going to say print go to\nequal to 18 I'm going to say print go to\nequal to 18 I'm going to say print go to college here I'm printing print college\ncollege here I'm printing print college\ncollege here I'm printing print college and then I'll write else if age is\nand then I'll write else if age is\nand then I'll write else if age is greater than 18 and pag is less than or\ngreater than 18 and pag is less than or\ngreater than 18 and pag is less than or equal to 35 I'll say print work then\nequal to 35 I'll say print work then\nequal to 35 I'll say print work then again I'll write else if age is let me\nagain I'll write else if age is let me\nagain I'll write else if age is let me let me put this condition little bit\nlet me put this condition little bit\nlet me put this condition little bit better then I'll write here L if if age\nbetter then I'll write here L if if age\nbetter then I'll write here L if if age is greater than 18 and age is less than\nis greater than 18 and age is less than\nis greater than 18 and age is less than or equal to 35 I'm going to say print\nor equal to 35 I'm going to say print\nor equal to 35 I'm going to say print work basically people needs to work in\nwork basically people needs to work in\nwork basically people needs to work in this age else I'm just going to consider\nthis age else I'm just going to consider\nthis age else I'm just going to consider print retire so here is my ifls\nprint retire so here is my ifls\nprint retire so here is my ifls condition over here now whenever we have\ncondition over here now whenever we have\ncondition over here now whenever we have this kind of nested if Els condition\nthis kind of nested if Els condition\nthis kind of nested if Els condition what we can do is that we can also\nwhat we can do is that we can also\nwhat we can do is that we can also represent this in the form of decision\nrepresent this in the form of decision\nrepresent this in the form of decision trees we'll also we can actually form\ntrees we'll also we can actually form\ntrees we'll also we can actually form this in the form of decision and the\nthis in the form of decision and the\nthis in the form of decision and the decision tree here first of all we will\ndecision tree here first of all we will\ndecision tree here first of all we will have a specific root node let's say this\nhave a specific root node let's say this\nhave a specific root node let's say this is my root node now in this root node\nis my root node now in this root node\nis my root node now in this root node the first condition is less than or\nthe first condition is less than or\nthe first condition is less than or equal to 18 so here obviously I will be\nequal to 18 so here obviously I will be\nequal to 18 so here obviously I will be having two conditions saying that if it\nhaving two conditions saying that if it\nhaving two conditions saying that if it is less than or equal to 18 and one\nis less than or equal to 18 and one\nis less than or equal to 18 and one condition will be yes one condition will\ncondition will be yes one condition will\ncondition will be yes one condition will be no so if this is yes and if this is\nbe no so if this is yes and if this is\nbe no so if this is yes and if this is no right if this condition is true that\nno right if this condition is true that\nno right if this condition is true that basically means we'll go in this side if\nbasically means we'll go in this side if\nbasically means we'll go in this side if it is true then here we will basically\nit is true then here we will basically\nit is true then here we will basically have something like college so this is\nhave something like college so this is\nhave something like college so this is your Leaf node similarly when I have no\nyour Leaf node similarly when I have no\nyour Leaf node similarly when I have no okay no no in this particular case we\nokay no no in this particular case we\nokay no no in this particular case we will go to the next condition in this\nwill go to the next condition in this\nwill go to the next condition in this next condition I will again create a\nnext condition I will again create a\nnext condition I will again create a node and I'll say that okay this is less\nnode and I'll say that okay this is less\nnode and I'll say that okay this is less than 18 and greater than sorry less than\nthan 18 and greater than sorry less than\nthan 18 and greater than sorry less than or equal to 35 so if this is also there\nor equal to 35 so if this is also there\nor equal to 35 so if this is also there then again I'll have two conditions\nthen again I'll have two conditions\nthen again I'll have two conditions which is basically yes or no now when I\nwhich is basically yes or no now when I\nwhich is basically yes or no now when I create this yes or no over here you'll\ncreate this yes or no over here you'll\ncreate this yes or no over here you'll be able to see that basically means here\nbe able to see that basically means here\nbe able to see that basically means here again two condition will be there if it\nagain two condition will be there if it\nagain two condition will be there if it is yes I will say print work so this\nis yes I will say print work so this\nis yes I will say print work so this will again be my leaf\nwill again be my leaf\nwill again be my leaf node and again for no again I will do\nnode and again for no again I will do\nnode and again for no again I will do the further splitting which is retire so\nthe further splitting which is retire so\nthe further splitting which is retire so here you can see that this entire\nhere you can see that this entire\nhere you can see that this entire algorithm this entire code that I have\nalgorithm this entire code that I have\nalgorithm this entire code that I have actually written you can see that it has\nactually written you can see that it has\nactually written you can see that it has got converted to this kind of\ngot converted to this kind of\ngot converted to this kind of trees where you specifically able to\ntrees where you specifically able to\ntrees where you specifically able to take decisions yes or no so can we solve\ntake decisions yes or no so can we solve\ntake decisions yes or no so can we solve a classification\na classification\na classification problem sorry this is greater than 18\nproblem sorry this is greater than 18\nproblem sorry this is greater than 18 again if it is greater than 18 and less\nagain if it is greater than 18 and less\nagain if it is greater than 18 and less than or 35 so can we solve a\nthan or 35 so can we solve a\nthan or 35 so can we solve a regression and a classification problem\nregression and a classification problem\nregression and a classification problem regression and classification problem\nregression and classification problem\nregression and classification problem using this decision trees by creating\nusing this decision trees by creating\nusing this decision trees by creating this kind of\nthis kind of\nthis kind of nodes so in short whenever we talk about\nnodes so in short whenever we talk about\nnodes so in short whenever we talk about decision\ndecision\ndecision trees whenever we talk about decision\ntrees whenever we talk about decision\ntrees whenever we talk about decision trees\ntrees\ntrees you will be seeing that decision trees\nyou will be seeing that decision trees\nyou will be seeing that decision trees are nothing but decision trees are\nare nothing but decision trees are\nare nothing but decision trees are nothing but by using this nested if El\nnothing but by using this nested if El\nnothing but by using this nested if El condition we can definitely solve some\ncondition we can definitely solve some\ncondition we can definitely solve some specific problem statement but here in\nspecific problem statement but here in\nspecific problem statement but here in the visualized way we will specifically\nthe visualized way we will specifically\nthe visualized way we will specifically create this decision tree in the form of\ncreate this decision tree in the form of\ncreate this decision tree in the form of nodes now you need to understand that\nnodes now you need to understand that\nnodes now you need to understand that what type of maths we will probably use\nwhat type of maths we will probably use\nwhat type of maths we will probably use okay so let's do one thing let's take a\nokay so let's do one thing let's take a\nokay so let's do one thing let's take a specific data set which I will\nspecific data set which I will\nspecific data set which I will definitely do it over here in front of\ndefinitely do it over here in front of\ndefinitely do it over here in front of you\nyou\nyou okay and we will try to solve this\nokay and we will try to solve this\nokay and we will try to solve this particular data set and this will\nparticular data set and this will\nparticular data set and this will basically give you an idea like how we\nbasically give you an idea like how we\nbasically give you an idea like how we can probably solve these problems so uh\ncan probably solve these problems so uh\ncan probably solve these problems so uh let me just open my snippet tool so this\nlet me just open my snippet tool so this\nlet me just open my snippet tool so this is my data set that I have let's\nis my data set that I have let's\nis my data set that I have let's consider that I have this specific data\nconsider that I have this specific data\nconsider that I have this specific data set now this data set are pretty much\nset now this data set are pretty much\nset now this data set are pretty much important because this probably in\nimportant because this probably in\nimportant because this probably in research papers also probably people who\nresearch papers also probably people who\nresearch papers also probably people who have come up with this algorithm they\nhave come up with this algorithm they\nhave come up with this algorithm they usually take this they take this thing\nusually take this they take this thing\nusually take this they take this thing but but right now this particular\nbut but right now this particular\nbut but right now this particular problem statement if I talk about this\nproblem statement if I talk about this\nproblem statement if I talk about this is a classification problem statement\nis a classification problem statement\nis a classification problem statement okay but don't worry I will also help\nokay but don't worry I will also help\nokay but don't worry I will also help you to explain I'll also explain you\nyou to explain I'll also explain you\nyou to explain I'll also explain you about regression also how decision tree\nabout regression also how decision tree\nabout regression also how decision tree regression will definitely work so let's\nregression will definitely work so let's\nregression will definitely work so let's go ahead and let's try to understand\ngo ahead and let's try to understand\ngo ahead and let's try to understand suppose if I have this specific problem\nsuppose if I have this specific problem\nsuppose if I have this specific problem statement how do we solve this this is\nstatement how do we solve this this is\nstatement how do we solve this this is my output feature play tennis yes or no\nmy output feature play tennis yes or no\nmy output feature play tennis yes or no okay whether the person is going to pay\nokay whether the person is going to pay\nokay whether the person is going to pay tennis or not yesterday or there after\ntennis or not yesterday or there after\ntennis or not yesterday or there after yesterday or whenever you want so if I\nyesterday or whenever you want so if I\nyesterday or whenever you want so if I have this input features like Outlook\nhave this input features like Outlook\nhave this input features like Outlook temperature humidity and wind is the\ntemperature humidity and wind is the\ntemperature humidity and wind is the person going to play tennis or not this\nperson going to play tennis or not this\nperson going to play tennis or not this is what my model should predict with the\nis what my model should predict with the\nis what my model should predict with the help of decision tree so how decision\nhelp of decision tree so how decision\nhelp of decision tree so how decision tree will work in this particular case\ntree will work in this particular case\ntree will work in this particular case first of all let's consider any any any\nfirst of all let's consider any any any\nfirst of all let's consider any any any specific uh feature let's say that\nspecific uh feature let's say that\nspecific uh feature let's say that Outlook is my feature so this will be my\nOutlook is my feature so this will be my\nOutlook is my feature so this will be my first\nfirst\nfirst feature which is specifically Outlook\nfeature which is specifically Outlook\nfeature which is specifically Outlook now just tell me how many are basically\nnow just tell me how many are basically\nnow just tell me how many are basically having no and how many are basically\nhaving no and how many are basically\nhaving no and how many are basically having yes in the case of Outlook there\nhaving yes in the case of Outlook there\nhaving yes in the case of Outlook there you'll be able to find out there are\nyou'll be able to find out there are\nyou'll be able to find out there are nine yes see 1 2 3 4 5 6 7 8 9 and how\nnine yes see 1 2 3 4 5 6 7 8 9 and how\nnine yes see 1 2 3 4 5 6 7 8 9 and how many NOS are there 1 2 3 4 5 I think 1 2\nmany NOS are there 1 2 3 4 5 I think 1 2\nmany NOS are there 1 2 3 4 5 I think 1 2 3 4 5 so nine yes and five NOS what we\n3 4 5 so nine yes and five NOS what we\n3 4 5 so nine yes and five NOS what we are going to do in this specific thing\nare going to do in this specific thing\nare going to do in this specific thing now we have N9 yes and five Nos and the\nnow we have N9 yes and five Nos and the\nnow we have N9 yes and five Nos and the first node that I have actually taken\nfirst node that I have actually taken\nfirst node that I have actually taken is basically Outlook so Outlook feature\nis basically Outlook so Outlook feature\nis basically Outlook so Outlook feature now just try to find out we are focusing\nnow just try to find out we are focusing\nnow just try to find out we are focusing on this specific feature now in this\non this specific feature now in this\non this specific feature now in this feature how many categories I have I\nfeature how many categories I have I\nfeature how many categories I have I have one Sunny category you can see over\nhave one Sunny category you can see over\nhave one Sunny category you can see over here I have Sunny one category then I\nhere I have Sunny one category then I\nhere I have Sunny one category then I have another category called as\nhave another category called as\nhave another category called as overcast then I have another category as\novercast then I have another category as\novercast then I have another category as rain so I have three unique categories\nrain so I have three unique categories\nrain so I have three unique categories So based on these three categories I\nSo based on these three categories I\nSo based on these three categories I will try to create three nodes so here\nwill try to create three nodes so here\nwill try to create three nodes so here is my one node here is my second node\nis my one node here is my second node\nis my one node here is my second node here is my third node so these are my\nhere is my third node so these are my\nhere is my third node so these are my three categories so this category is\nthree categories so this category is\nthree categories so this category is basically called as Sunny this category\nbasically called as Sunny this category\nbasically called as Sunny this category is basically called as overcast and this\nis basically called as overcast and this\nis basically called as overcast and this category is basically called as rain\ncategory is basically called as rain\ncategory is basically called as rain based on these three categories so I'm\nbased on these three categories so I'm\nbased on these three categories so I'm splitting it now just go ahead and see\nsplitting it now just go ahead and see\nsplitting it now just go ahead and see in Sunny how many yes and how many no\nin Sunny how many yes and how many no\nin Sunny how many yes and how many no are there how many yes with respect to\nare there how many yes with respect to\nare there how many yes with respect to Sunny are there see in sunny I have two\nSunny are there see in sunny I have two\nSunny are there see in sunny I have two NOS see one and two no uh one more no is\nNOS see one and two no uh one more no is\nNOS see one and two no uh one more no is there three NOS so here you can see this\nthere three NOS so here you can see this\nthere three NOS so here you can see this is my one no then this is my two no this\nis my one no then this is my two no this\nis my one no then this is my two no this is my three no and yes are two so this\nis my three no and yes are two so this\nis my three no and yes are two so this one and this one so how many total\none and this one so how many total\none and this one so how many total number of yes so here you can see that\nnumber of yes so here you can see that\nnumber of yes so here you can see that there are 1 2 2 yes and three no let's\nthere are 1 2 2 yes and three no let's\nthere are 1 2 2 yes and three no let's say that I have randomly selected one\nsay that I have randomly selected one\nsay that I have randomly selected one feature which is Outlook why can't I\nfeature which is Outlook why can't I\nfeature which is Outlook why can't I when like see it is up to it it is up to\nwhen like see it is up to it it is up to\nwhen like see it is up to it it is up to the decision tree to select any of the\nthe decision tree to select any of the\nthe decision tree to select any of the feature here I have specifically taken\nfeature here I have specifically taken\nfeature here I have specifically taken Outlook later on I'll explain why it it\nOutlook later on I'll explain why it it\nOutlook later on I'll explain why it it can basically select how it selects the\ncan basically select how it selects the\ncan basically select how it selects the feature okay I'll I'll talk about it\nfeature okay I'll I'll talk about it\nfeature okay I'll I'll talk about it don't worry so in the Outlook we have\ndon't worry so in the Outlook we have\ndon't worry so in the Outlook we have two yes sorry in the case of Sunny we\ntwo yes sorry in the case of Sunny we\ntwo yes sorry in the case of Sunny we have two yes and three NOS now the next\nhave two yes and three NOS now the next\nhave two yes and three NOS now the next thing is that let's go and see for\nthing is that let's go and see for\nthing is that let's go and see for overcast in overcast I have 1 yes uh 2s\novercast in overcast I have 1 yes uh 2s\novercast in overcast I have 1 yes uh 2s um 3s and 4 yes I don't have any no in\num 3s and 4 yes I don't have any no in\num 3s and 4 yes I don't have any no in overcast so over here my thing will be\novercast so over here my thing will be\novercast so over here my thing will be that four yes and Zer Nos and then\nthat four yes and Zer Nos and then\nthat four yes and Zer Nos and then finally when we go to the Rain part see\nfinally when we go to the Rain part see\nfinally when we go to the Rain part see in Rain how many features are there in\nin Rain how many features are there in\nin Rain how many features are there in rain if you go and probably see it how\nrain if you go and probably see it how\nrain if you go and probably see it how many number of yes and NOS are there go\nmany number of yes and NOS are there go\nmany number of yes and NOS are there go and see in one one yes in row rain two\nand see in one one yes in row rain two\nand see in one one yes in row rain two yes then one no then again you have one\nyes then one no then again you have one\nyes then one no then again you have one yes and one no right so here you can\nyes and one no right so here you can\nyes and one no right so here you can basically say that in rain in the case\nbasically say that in rain in the case\nbasically say that in rain in the case of rain if I take a as an example how\nof rain if I take a as an example how\nof rain if I take a as an example how many number of yes and NOS are there it\nmany number of yes and NOS are there it\nmany number of yes and NOS are there it will be 3 yes and two\nwill be 3 yes and two\nwill be 3 yes and two NOS understand understanding\nalgorithm then everything will you'll be\nalgorithm then everything will you'll be able to understand now let's go ahead\nable to understand now let's go ahead\nable to understand now let's go ahead and try to cease for sunny sunny\nand try to cease for sunny sunny\nand try to cease for sunny sunny definitely has 2 yes and three NOS this\ndefinitely has 2 yes and three NOS this\ndefinitely has 2 yes and three NOS this has four yes and zero NOS here you have\nhas four yes and zero NOS here you have\nhas four yes and zero NOS here you have three Y and two NOS now if I probably\nthree Y and two NOS now if I probably\nthree Y and two NOS now if I probably take overcast here you need to\ntake overcast here you need to\ntake overcast here you need to understand understand about two things\nunderstand understand about two things\nunderstand understand about two things one is pure\none is pure\none is pure split and one is impure split now what\nsplit and one is impure split now what\nsplit and one is impure split now what does pure split basically mean pure spit\ndoes pure split basically mean pure spit\ndoes pure split basically mean pure spit basically means that now see in this\nbasically means that now see in this\nbasically means that now see in this particular scenario in overcast in\nparticular scenario in overcast in\nparticular scenario in overcast in overcast I have either yes or no so here\novercast I have either yes or no so here\novercast I have either yes or no so here you can see that I have four yes and Zer\nyou can see that I have four yes and Zer\nyou can see that I have four yes and Zer NOS so that basically means this is a\nNOS so that basically means this is a\nNOS so that basically means this is a pure split anybody tomorrow in my data\npure split anybody tomorrow in my data\npure split anybody tomorrow in my data set if I just take this Outlook feature\nset if I just take this Outlook feature\nset if I just take this Outlook feature suppose in one day in day 15 the Outlook\nsuppose in one day in day 15 the Outlook\nsuppose in one day in day 15 the Outlook is Outlook is basically overcast then I\nis Outlook is basically overcast then I\nis Outlook is basically overcast then I know directly it is the person is going\nknow directly it is the person is going\nknow directly it is the person is going to play so this part is already created\nto play so this part is already created\nto play so this part is already created and this node is called as pure\nand this node is called as pure\nand this node is called as pure node understand this why it is called as\nnode understand this why it is called as\nnode understand this why it is called as pure node because either you have all\npure node because either you have all\npure node because either you have all Yes or zeros NOS or zero yes or all NOS\nYes or zeros NOS or zero yes or all NOS\nYes or zeros NOS or zero yes or all NOS like that in this particular case I have\nlike that in this particular case I have\nlike that in this particular case I have all yes so if I take this specific path\nall yes so if I take this specific path\nall yes so if I take this specific path I know that with respect to overcast my\nI know that with respect to overcast my\nI know that with respect to overcast my final decision which is yes it is always\nfinal decision which is yes it is always\nfinal decision which is yes it is always going to become yes so this is what it\ngoing to become yes so this is what it\ngoing to become yes so this is what it basically says so I don't have to split\nbasically says so I don't have to split\nbasically says so I don't have to split further so from here I will probably not\nfurther so from here I will probably not\nfurther so from here I will probably not split I will definitely not split more\nsplit I will definitely not split more\nsplit I will definitely not split more because I don't require it because I\nbecause I don't require it because I\nbecause I don't require it because I have it is a pure leaf node okay you can\nhave it is a pure leaf node okay you can\nhave it is a pure leaf node okay you can also say that this is a pure leaf node\nalso say that this is a pure leaf node\nalso say that this is a pure leaf node so I'm just going to mention it again\nso I'm just going to mention it again\nso I'm just going to mention it again this one I'm specifically talking about\nthis one I'm specifically talking about\nthis one I'm specifically talking about now let's talk about sunny in the case\nnow let's talk about sunny in the case\nnow let's talk about sunny in the case of Sunny you have two yes and three NOS\nof Sunny you have two yes and three NOS\nof Sunny you have two yes and three NOS so this is obviously impure so what we\nso this is obviously impure so what we\nso this is obviously impure so what we do we take next feature and again how do\ndo we take next feature and again how do\ndo we take next feature and again how do we calculate that which feature we\nwe calculate that which feature we\nwe calculate that which feature we should take next I'll discuss about it\nshould take next I'll discuss about it\nshould take next I'll discuss about it let's say that after this I take up\nlet's say that after this I take up\nlet's say that after this I take up temperature I take up temperature and I\ntemperature I take up temperature and I\ntemperature I take up temperature and I start splitting again since this is\nstart splitting again since this is\nstart splitting again since this is impure okay and this split will happen\nimpure okay and this split will happen\nimpure okay and this split will happen until we get finally a pure split\nuntil we get finally a pure split\nuntil we get finally a pure split similarly with respect to rain we will\nsimilarly with respect to rain we will\nsimilarly with respect to rain we will go ahead and take another feature and\ngo ahead and take another feature and\ngo ahead and take another feature and we'll keep on splitting unless and until\nwe'll keep on splitting unless and until\nwe'll keep on splitting unless and until we get a leaf node which is completely\nwe get a leaf node which is completely\nwe get a leaf node which is completely pure I hope you understood how this\npure I hope you understood how this\npure I hope you understood how this exactly work now two questions two\nexactly work now two questions two\nexactly work now two questions two questions is that Kish the first thing\nquestions is that Kish the first thing\nquestions is that Kish the first thing is that how do we calculate this\nis that how do we calculate this\nis that how do we calculate this Purity and how do we come to know that\nPurity and how do we come to know that\nPurity and how do we come to know that this is a pure split just by seeing\nthis is a pure split just by seeing\nthis is a pure split just by seeing definitely I can say I can definitely\ndefinitely I can say I can definitely\ndefinitely I can say I can definitely say by just seeing that how many number\nsay by just seeing that how many number\nsay by just seeing that how many number of yes or NOS are there based on that I\nof yes or NOS are there based on that I\nof yes or NOS are there based on that I can def itely say it is a pure split or\ncan def itely say it is a pure split or\ncan def itely say it is a pure split or not so for this we use two different\nnot so for this we use two different\nnot so for this we use two different things one is\nthings one is\nthings one is entropy and the other one is something\nentropy and the other one is something\nentropy and the other one is something called as guine coefficient so we will\ncalled as guine coefficient so we will\ncalled as guine coefficient so we will try to understand how does entropy work\ntry to understand how does entropy work\ntry to understand how does entropy work and how does Guinea coefficient work in\nand how does Guinea coefficient work in\nand how does Guinea coefficient work in decision tree which will help us to\ndecision tree which will help us to\ndecision tree which will help us to determine whether the split is pure\ndetermine whether the split is pure\ndetermine whether the split is pure split or not or whether this node is\nsplit or not or whether this node is\nsplit or not or whether this node is leaf node or not then coming to the\nleaf node or not then coming to the\nleaf node or not then coming to the second thing okay coming to the second\nsecond thing okay coming to the second\nsecond thing okay coming to the second thing one is with respect to Purity\nthing one is with respect to Purity\nthing one is with respect to Purity second thing your first most important\nsecond thing your first most important\nsecond thing your first most important question which you had asked why did I\nquestion which you had asked why did I\nquestion which you had asked why did I probably select Outlook how the features\nprobably select Outlook how the features\nprobably select Outlook how the features are selected and here you have a topic\nare selected and here you have a topic\nare selected and here you have a topic which is called as Information Gain and\nwhich is called as Information Gain and\nwhich is called as Information Gain and if you know this both your problem is\nif you know this both your problem is\nif you know this both your problem is solved so now let's go ahead and let's\nsolved so now let's go ahead and let's\nsolved so now let's go ahead and let's understand about entropy or guinea\nunderstand about entropy or guinea\nunderstand about entropy or guinea coefficient or Information Gain entropy\ncoefficient or Information Gain entropy\ncoefficient or Information Gain entropy or guine coefficient oh sorry Guinea\nor guine coefficient oh sorry Guinea\nor guine coefficient oh sorry Guinea coefficient I'm saying guine impurity\ncoefficient I'm saying guine impurity\ncoefficient I'm saying guine impurity also you can say over here\nalso you can say over here\nalso you can say over here I'll write it as guine impurity not\nI'll write it as guine impurity not\nI'll write it as guine impurity not coefficient also I'll just say it as\ncoefficient also I'll just say it as\ncoefficient also I'll just say it as Guinea impurity but I hope everybody is\nGuinea impurity but I hope everybody is\nGuinea impurity but I hope everybody is understood till here let's go ahead and\nunderstood till here let's go ahead and\nunderstood till here let's go ahead and let's discuss about the first thing that\nlet's discuss about the first thing that\nlet's discuss about the first thing that is\nis\nis entropy how does entropy work and how we\nentropy how does entropy work and how we\nentropy how does entropy work and how we are going to use the formula so entropy\nare going to use the formula so entropy\nare going to use the formula so entropy here I will just write guine so we are\nhere I will just write guine so we are\nhere I will just write guine so we are going to discuss about this both the\ngoing to discuss about this both the\ngoing to discuss about this both the things let's say that the entropy\nthings let's say that the entropy\nthings let's say that the entropy formula which is given by I will write h\nformula which is given by I will write h\nformula which is given by I will write h of s is equal to so h of s is equal to\nof s is equal to so h of s is equal to\nof s is equal to so h of s is equal to minus P plus I'll talk about what is\nminus P plus I'll talk about what is\nminus P plus I'll talk about what is minus what is p plus log base 2 p\nminus what is p plus log base 2 p\nminus what is p plus log base 2 p +- p\n+- p\n+- p minus log base 2 p minus so this is the\nminus log base 2 p minus so this is the\nminus log base 2 p minus so this is the formula and in guine impurity the\nformula and in guine impurity the\nformula and in guine impurity the formula is 1 minus summation of I equal\nformula is 1 minus summation of I equal\nformula is 1 minus summation of I equal 1 2 N pÂ² I even talk about when you\n1 2 N pÂ² I even talk about when you\n1 2 N pÂ² I even talk about when you should use guine impurity when you\nshould use guine impurity when you\nshould use guine impurity when you should not use guine impurity\nshould not use guine impurity\nshould not use guine impurity when you should use entropy you know by\nwhen you should use entropy you know by\nwhen you should use entropy you know by default the decision tree regression or\ndefault the decision tree regression or\ndefault the decision tree regression or classific sorry decision tree\nclassific sorry decision tree\nclassific sorry decision tree classification uses Guinea impurity now\nclassification uses Guinea impurity now\nclassification uses Guinea impurity now let's take one specific example so my\nlet's take one specific example so my\nlet's take one specific example so my example is that I have a feature one my\nexample is that I have a feature one my\nexample is that I have a feature one my root node I have a feature one which is\nroot node I have a feature one which is\nroot node I have a feature one which is my root node and let's say that in this\nmy root node and let's say that in this\nmy root node and let's say that in this root node I have six yes and three NOS\nroot node I have six yes and three NOS\nroot node I have six yes and three NOS very simple let's say that this has two\nvery simple let's say that this has two\nvery simple let's say that this has two categories and based on this two\ncategories and based on this two\ncategories and based on this two categories of split has happened that is\ncategories of split has happened that is\ncategories of split has happened that is a C1 let's say in this I have 3 S3 Nos\na C1 let's say in this I have 3 S3 Nos\na C1 let's say in this I have 3 S3 Nos and here I have 3 s0 Nos and this is my\nand here I have 3 s0 Nos and this is my\nand here I have 3 s0 Nos and this is my second category always understand if I\nsecond category always understand if I\nsecond category always understand if I do the sumission 3s and 3s is 6s see\ndo the sumission 3s and 3s is 6s see\ndo the sumission 3s and 3s is 6s see this this sumission if I do 3 + 3 is\nthis this sumission if I do 3 + 3 is\nthis this sumission if I do 3 + 3 is obviously 6 3 + 0 is obviously so this\nobviously 6 3 + 0 is obviously so this\nobviously 6 3 + 0 is obviously so this you need to understand based on the\nyou need to understand based on the\nyou need to understand based on the number of root nodes only almost it'll\nnumber of root nodes only almost it'll\nnumber of root nodes only almost it'll be same now let's go ahead and let's\nbe same now let's go ahead and let's\nbe same now let's go ahead and let's understand how do we Cal calculate let's\nunderstand how do we Cal calculate let's\nunderstand how do we Cal calculate let's take this example how do we calculate\ntake this example how do we calculate\ntake this example how do we calculate the entropy of this so I have already\nthe entropy of this so I have already\nthe entropy of this so I have already shown you the entropy formula over here\nshown you the entropy formula over here\nshown you the entropy formula over here now let's understand the components I\nnow let's understand the components I\nnow let's understand the components I will write h of s is equal to minus sign\nwill write h of s is equal to minus sign\nwill write h of s is equal to minus sign is there what is p+ p+ basically means\nis there what is p+ p+ basically means\nis there what is p+ p+ basically means that what is the probability of yes what\nthat what is the probability of yes what\nthat what is the probability of yes what is the probability of yes this is a\nis the probability of yes this is a\nis the probability of yes this is a simple thing for you all out of this\nsimple thing for you all out of this\nsimple thing for you all out of this what is the probability of yes yes out\nwhat is the probability of yes yes out\nwhat is the probability of yes yes out of this so obviously how you'll write if\nof this so obviously how you'll write if\nof this so obviously how you'll write if you want to find out the probability of\nyou want to find out the probability of\nyou want to find out the probability of yes out of this see when I say plus that\nyes out of this see when I say plus that\nyes out of this see when I say plus that basically means yes when I say minus\nbasically means yes when I say minus\nbasically means yes when I say minus that basically means no so what is the\nthat basically means no so what is the\nthat basically means no so what is the probability of yes so it is be nothing\nprobability of yes so it is be nothing\nprobability of yes so it is be nothing but yes plus and minus are specifically\nbut yes plus and minus are specifically\nbut yes plus and minus are specifically for binary\nfor binary\nfor binary class this can be positive negative so\nclass this can be positive negative so\nclass this can be positive negative so the probability with respect to yes can\nthe probability with respect to yes can\nthe probability with respect to yes can I write 3x 3 only for this what is the\nI write 3x 3 only for this what is the\nI write 3x 3 only for this what is the probability out of this total number of\nprobability out of this total number of\nprobability out of this total number of this is there 3x3 similarly if I go and\nthis is there 3x3 similarly if I go and\nthis is there 3x3 similarly if I go and see the next term log to the base 2 p+\nsee the next term log to the base 2 p+\nsee the next term log to the base 2 p+ so again if I go ahead and write over\nso again if I go ahead and write over\nso again if I go ahead and write over here log to the base 2 p+ p+ is again\nhere log to the base 2 p+ p+ is again\nhere log to the base 2 p+ p+ is again 3x3 so then again we have minus and this\n3x3 so then again we have minus and this\n3x3 so then again we have minus and this is now P minus what is p minus 0 by 3\nis now P minus what is p minus 0 by 3\nis now P minus what is p minus 0 by 3 log base 2 0 by 3 this obviously will\nlog base 2 0 by 3 this obviously will\nlog base 2 0 by 3 this obviously will become zero this will obviously become 0\nbecome zero this will obviously become 0\nbecome zero this will obviously become 0 because 0 divid by anything is zero what\nbecause 0 divid by anything is zero what\nbecause 0 divid by anything is zero what will this be 1 log to the base 1 what is\nwill this be 1 log to the base 1 what is\nwill this be 1 log to the base 1 what is this this is nothing but zero log to the\nthis this is nothing but zero log to the\nthis this is nothing but zero log to the base 1 is nothing but zero tell me\nbase 1 is nothing but zero tell me\nbase 1 is nothing but zero tell me whether this is a pure split or impure\nwhether this is a pure split or impure\nwhether this is a pure split or impure split so this is a pure split whenever\nsplit so this is a pure split whenever\nsplit so this is a pure split whenever we have a pure split the answer of the\nwe have a pure split the answer of the\nwe have a pure split the answer of the entropy is going to come to zero so here\nentropy is going to come to zero so here\nentropy is going to come to zero so here I'm going to Define one graph\nI'm going to Define one graph\nI'm going to Define one graph this is H of s and let's say this is p+\nthis is H of s and let's say this is p+\nthis is H of s and let's say this is p+ or P minus if my probability of plus see\nor P minus if my probability of plus see\nor P minus if my probability of plus see when I say probability of plus is 0.5\nwhen I say probability of plus is 0.5\nwhen I say probability of plus is 0.5 what will be probability of minus it\nwhat will be probability of minus it\nwhat will be probability of minus it will also be 0. five right because it's\nwill also be 0. five right because it's\nwill also be 0. five right because it's just like P is equal to 1 - Q right if p\njust like P is equal to 1 - Q right if p\njust like P is equal to 1 - Q right if p is .5 then Q will be 1 - P same thing\nis .5 then Q will be 1 - P same thing\nis .5 then Q will be 1 - P same thing right so when it\nright so when it\nright so when it is5 obviously my h of s will be 1 let's\nis5 obviously my h of s will be 1 let's\nis5 obviously my h of s will be 1 let's say so this is this is the graph that\nsay so this is this is the graph that\nsay so this is this is the graph that will basically get formed let's go ahead\nwill basically get formed let's go ahead\nwill basically get formed let's go ahead and try to calculate the entropy of this\nand try to calculate the entropy of this\nand try to calculate the entropy of this guys what will be the entropy of this\nguys what will be the entropy of this\nguys what will be the entropy of this node so here I'm going to just make a\nnode so here I'm going to just make a\nnode so here I'm going to just make a graph h of s minus what is p+ p+ is\ngraph h of s minus what is p+ p+ is\ngraph h of s minus what is p+ p+ is nothing but 3x 6 log base 2 3x 6\nnothing but 3x 6 log base 2 3x 6\nnothing but 3x 6 log base 2 3x 6 minus three no are there 3x 6 log base 2\nminus three no are there 3x 6 log base 2\nminus three no are there 3x 6 log base 2 3x 6 so if you compute this\n3x 6 so if you compute this\n3x 6 so if you compute this log base 2 to the^ of 1 if you do the\nlog base 2 to the^ of 1 if you do the\nlog base 2 to the^ of 1 if you do the calculation here I'm actually going to\ncalculation here I'm actually going to\ncalculation here I'm actually going to get one so when I'm getting one when I'm\nget one so when I'm getting one when I'm\nget one so when I'm getting one when I'm actually getting one when you have three\nactually getting one when you have three\nactually getting one when you have three yes and three NOS what is the\nyes and three NOS what is the\nyes and three NOS what is the probability it is 50/50% right so when\nprobability it is 50/50% right so when\nprobability it is 50/50% right so when your p+ is5 that basically means your h\nyour p+ is5 that basically means your h\nyour p+ is5 that basically means your h of s is coming as one so from this graph\nof s is coming as one so from this graph\nof s is coming as one so from this graph you can see that I'm getting one if this\nyou can see that I'm getting one if this\nyou can see that I'm getting one if this is zero this is one this is zero and\nis zero this is one this is zero and\nis zero this is one this is zero and this is one I hope everybody is able to\nthis is one I hope everybody is able to\nthis is one I hope everybody is able to to understand guys 0o and one if your p+\nto understand guys 0o and one if your p+\nto understand guys 0o and one if your p+ is\nis\nis zero or if your p+ is one that basically\nzero or if your p+ is one that basically\nzero or if your p+ is one that basically means it becomes a pure split so in h of\nmeans it becomes a pure split so in h of\nmeans it becomes a pure split so in h of s you are going to get\ns you are going to get\ns you are going to get zero so always understand your entropy\nzero so always understand your entropy\nzero so always understand your entropy will be between 0 to\nwill be between 0 to\nwill be between 0 to 1 if I have a impure this is a\n1 if I have a impure this is a\n1 if I have a impure this is a completely impure split because here you\ncompletely impure split because here you\ncompletely impure split because here you have 50% probability of getting yes 50%\nhave 50% probability of getting yes 50%\nhave 50% probability of getting yes 50% probability of getting no h ofs is\nprobability of getting no h ofs is\nprobability of getting no h ofs is entropy this is entropy for the sample H\nentropy this is entropy for the sample H\nentropy this is entropy for the sample H ofs notation that I'm using is H ofs so\nofs notation that I'm using is H ofs so\nofs notation that I'm using is H ofs so if whenever the split is happening the\nif whenever the split is happening the\nif whenever the split is happening the first thing is done the purity test the\nfirst thing is done the purity test the\nfirst thing is done the purity test the purity test is done with the help of\npurity test is done with the help of\npurity test is done with the help of entropy right now I'll also show guinea\nentropy right now I'll also show guinea\nentropy right now I'll also show guinea guinea impurity don't worry so with the\nguinea impurity don't worry so with the\nguinea impurity don't worry so with the entropy you'll be able to find if I am\nentropy you'll be able to find if I am\nentropy you'll be able to find if I am getting one that basically means it is a\ngetting one that basically means it is a\ngetting one that basically means it is a impure split and if I'm getting zero it\nimpure split and if I'm getting zero it\nimpure split and if I'm getting zero it is pure split so this is the graph okay\nis pure split so this is the graph okay\nis pure split so this is the graph okay this is the graph and this graph is\nthis is the graph and this graph is\nthis is the graph and this graph is basically the entropy graph again\nbasically the entropy graph again\nbasically the entropy graph again understand if your probability of\nunderstand if your probability of\nunderstand if your probability of getting yes or no is 0.5 that basically\ngetting yes or no is 0.5 that basically\ngetting yes or no is 0.5 that basically means 50/50 is there 3s and three NOS\nmeans 50/50 is there 3s and three NOS\nmeans 50/50 is there 3s and three NOS then your entropy is going to be 1 h of\nthen your entropy is going to be 1 h of\nthen your entropy is going to be 1 h of s if your probability is completely one\ns if your probability is completely one\ns if your probability is completely one that basically means either you're\nthat basically means either you're\nthat basically means either you're getting completely yes or completely no\ngetting completely yes or completely no\ngetting completely yes or completely no so your your entropy will be zero that\nso your your entropy will be zero that\nso your your entropy will be zero that basically means it is pure split so in\nbasically means it is pure split so in\nbasically means it is pure split so in the case of probability .5 you're\nthe case of probability .5 you're\nthe case of probability .5 you're getting plus one then it'll keep on\ngetting plus one then it'll keep on\ngetting plus one then it'll keep on reducing now let's go ahead and let's\nreducing now let's go ahead and let's\nreducing now let's go ahead and let's try to understand so here you have\ntry to understand so here you have\ntry to understand so here you have understood about purity test definitely\nunderstood about purity test definitely\nunderstood about purity test definitely you'll use entropy try to find out\nyou'll use entropy try to find out\nyou'll use entropy try to find out whether it is pure or impure if it is\nwhether it is pure or impure if it is\nwhether it is pure or impure if it is impure you go ahead with the further\nimpure you go ahead with the further\nimpure you go ahead with the further shift further division of the categories\nshift further division of the categories\nshift further division of the categories again you take another feature divide it\nagain you take another feature divide it\nagain you take another feature divide it because here from this two which split\nbecause here from this two which split\nbecause here from this two which split you will do further you will do this\nyou will do further you will do this\nyou will do further you will do this split as further if you are getting 6 6\nsplit as further if you are getting 6 6\nsplit as further if you are getting 6 6 is this specific value then you probably\nis this specific value then you probably\nis this specific value then you probably go and draw over here this is your\ngo and draw over here this is your\ngo and draw over here this is your entropy if your probability is here\nentropy if your probability is here\nentropy if your probability is here which\nwhich\nwhich is.3 then you will go here and create\nis.3 then you will go here and create\nis.3 then you will go here and create this this may be0 4 or3 something like\nthis this may be0 4 or3 something like\nthis this may be0 4 or3 something like this it will be between 0 to 1 let's go\nthis it will be between 0 to 1 let's go\nthis it will be between 0 to 1 let's go ahead and discuss about the second issue\nahead and discuss about the second issue\nahead and discuss about the second issue I hope everybody is discussed about we\nI hope everybody is discussed about we\nI hope everybody is discussed about we have discussed about checking the pure\nhave discussed about checking the pure\nhave discussed about checking the pure split or not and we have understood this\nsplit or not and we have understood this\nsplit or not and we have understood this much but the next thing is that okay\nmuch but the next thing is that okay\nmuch but the next thing is that okay fine chish this is very good we have\nfine chish this is very good we have\nfine chish this is very good we have explained well I know many people will\nexplained well I know many people will\nexplained well I know many people will say that but there are some people I\nsay that but there are some people I\nsay that but there are some people I can't help let's say that I have some\ncan't help let's say that I have some\ncan't help let's say that I have some features okay now coming to the second\nfeatures okay now coming to the second\nfeatures okay now coming to the second problem how do we consider which node to\nproblem how do we consider which node to\nproblem how do we consider which node to cap which which feature to take and\ncap which which feature to take and\ncap which which feature to take and split because here I may have one one\nsplit because here I may have one one\nsplit because here I may have one one split so again let's see that what is\nsplit so again let's see that what is\nsplit so again let's see that what is the second problem which feature to take\nthe second problem which feature to take\nthe second problem which feature to take to split right this is the second\nto split right this is the second\nto split right this is the second problem that we are trying to solve\nproblem that we are trying to solve\nproblem that we are trying to solve let's say that I have one feature one\nlet's say that I have one feature one\nlet's say that I have one feature one over here and I have two categories\nover here and I have two categories\nover here and I have two categories let's say this is there C1 and C2 here\nlet's say this is there C1 and C2 here\nlet's say this is there C1 and C2 here let's say that I have 9 years 5 Nos and\nlet's say that I have 9 years 5 Nos and\nlet's say that I have 9 years 5 Nos and then I have 6 years 2 NOS here I have\nthen I have 6 years 2 NOS here I have\nthen I have 6 years 2 NOS here I have basically three yes and three NOS let's\nbasically three yes and three NOS let's\nbasically three yes and three NOS let's say and in my data set I have features\nsay and in my data set I have features\nsay and in my data set I have features like F1 FS2 F3 now let's say that\nlike F1 FS2 F3 now let's say that\nlike F1 FS2 F3 now let's say that another split I can actually start with\nanother split I can actually start with\nanother split I can actually start with feature two also and in feature two I\nfeature two also and in feature two I\nfeature two also and in feature two I may have probably three categories like\nmay have probably three categories like\nmay have probably three categories like C1 C2 C3 so with respect to the root\nC1 C2 C3 so with respect to the root\nC1 C2 C3 so with respect to the root node and all the other features because\nnode and all the other features because\nnode and all the other features because after this also I may have to split\nafter this also I may have to split\nafter this also I may have to split right I may have to take another feature\nright I may have to take another feature\nright I may have to take another feature and keep on splitting right based on the\nand keep on splitting right based on the\nand keep on splitting right based on the Pure or impure split how do I decide\nPure or impure split how do I decide\nPure or impure split how do I decide should I take fub1 first or F2 first or\nshould I take fub1 first or F2 first or\nshould I take fub1 first or F2 first or F3 first or any other feature first how\nF3 first or any other feature first how\nF3 first or any other feature first how should I decide that which feature\nshould I decide that which feature\nshould I decide that which feature should I take and probably do the split\nshould I take and probably do the split\nshould I take and probably do the split that is the major question so for this\nthat is the major question so for this\nthat is the major question so for this we specifically use something called as\nwe specifically use something called as\nwe specifically use something called as Information Gain so here I'm just going\nInformation Gain so here I'm just going\nInformation Gain so here I'm just going to say here we basically use Information\nto say here we basically use Information\nto say here we basically use Information Gain now what is this Information Gain\nGain now what is this Information Gain\nGain now what is this Information Gain I'll talk about it so Information Gain\nI'll talk about it so Information Gain\nI'll talk about it so Information Gain first of all I will write the formula we\nfirst of all I will write the formula we\nfirst of all I will write the formula we basically write gain with sample first\nbasically write gain with sample first\nbasically write gain with sample first with feature one I will compute so first\nwith feature one I will compute so first\nwith feature one I will compute so first with feature one I will compute suppose\nwith feature one I will compute suppose\nwith feature one I will compute suppose this is my first split of my data and\nthis is my first split of my data and\nthis is my first split of my data and probably I'm Computing over here this\nprobably I'm Computing over here this\nprobably I'm Computing over here this can be written as h of s I'll discuss\ncan be written as h of s I'll discuss\ncan be written as h of s I'll discuss about each and every parameter don't\nabout each and every parameter don't\nabout each and every parameter don't worry summation of V belong to values s\nworry summation of V belong to values s\nworry summation of V belong to values s of V don't worry guys if you have not\nof V don't worry guys if you have not\nof V don't worry guys if you have not understood the formula I will explain it\nunderstood the formula I will explain it\nunderstood the formula I will explain it then the sample size H of SV I'll\nthen the sample size H of SV I'll\nthen the sample size H of SV I'll discuss about each and every parameter\ndiscuss about each and every parameter\ndiscuss about each and every parameter let's say that I'm taking this feature\nlet's say that I'm taking this feature\nlet's say that I'm taking this feature one split I have you have already seen\none split I have you have already seen\none split I have you have already seen what is feature one so this is my\nwhat is feature one so this is my\nwhat is feature one so this is my feature one I have two categories C1 C2\nfeature one I have two categories C1 C2\nfeature one I have two categories C1 C2 this has 9 yes 5 NOS this has 6s and two\nthis has 9 yes 5 NOS this has 6s and two\nthis has 9 yes 5 NOS this has 6s and two Nos and this has 3 yes and three NOS now\nNos and this has 3 yes and three NOS now\nNos and this has 3 yes and three NOS now I will try to calculate the information\nI will try to calculate the information\nI will try to calculate the information gain of this specific split now I will\ngain of this specific split now I will\ngain of this specific split now I will go ahead and probably take this up now\ngo ahead and probably take this up now\ngo ahead and probably take this up now see over here we'll try to understand\nsee over here we'll try to understand\nsee over here we'll try to understand what is this now if I want to compute\nwhat is this now if I want to compute\nwhat is this now if I want to compute the gain of s of F1 first is first first\nthe gain of s of F1 first is first first\nthe gain of s of F1 first is first first thing that I need to find out is H of s\nthing that I need to find out is H of s\nthing that I need to find out is H of s now this h of s is specifically of the\nnow this h of s is specifically of the\nnow this h of s is specifically of the root node so I need to first of all\nroot node so I need to first of all\nroot node so I need to first of all calculate what is h of s h ofs is\ncalculate what is h of s h ofs is\ncalculate what is h of s h ofs is nothing but entropy entropy of the root\nnothing but entropy entropy of the root\nnothing but entropy entropy of the root node so if I want to compute the entropy\nnode so if I want to compute the entropy\nnode so if I want to compute the entropy of the node node tell me how should I\nof the node node tell me how should I\nof the node node tell me how should I compute h of s is equal to minus p + log\ncompute h of s is equal to minus p + log\ncompute h of s is equal to minus p + log base 2 p+ calculate guys along with me -\nbase 2 p+ calculate guys along with me -\nbase 2 p+ calculate guys along with me - P minus log base to P minus so I hope\nP minus log base to P minus so I hope\nP minus log base to P minus so I hope everybody knows this so here I'm going\neverybody knows this so here I'm going\neverybody knows this so here I'm going to compute by what is ability of plus\nto compute by what is ability of plus\nto compute by what is ability of plus over here in this specific root node it\nover here in this specific root node it\nover here in this specific root node it is nothing but 9 by4 then I have log\nis nothing but 9 by4 then I have log\nis nothing but 9 by4 then I have log base 2 again 9\nbase 2 again 9\nbase 2 again 9 by4 then I have P minus what is p minus\nby4 then I have P minus what is p minus\nby4 then I have P minus what is p minus 5x4 log base 2 5 by4 so this calculation\n5x4 log base 2 5 by4 so this calculation\n5x4 log base 2 5 by4 so this calculation I will probably get it as\nI will probably get it as\nI will probably get it as 94 approximately equal to 94 just check\n94 approximately equal to 94 just check\n94 approximately equal to 94 just check it whether you're getting this or not\nit whether you're getting this or not\nit whether you're getting this or not again you can use calculator if you want\nagain you can use calculator if you want\nagain you can use calculator if you want now now I have definitely found out this\nnow now I have definitely found out this\nnow now I have definitely found out this this is specifically for the root node\nthis is specifically for the root node\nthis is specifically for the root node now let's see the next thing the next\nnow let's see the next thing the next\nnow let's see the next thing the next important thing which is this part what\nimportant thing which is this part what\nimportant thing which is this part what is s of v and what is s and what is h of\nis s of v and what is s and what is h of\nis s of v and what is s and what is h of SV now very important just have a look\nSV now very important just have a look\nSV now very important just have a look everybody see this graph okay see this\neverybody see this graph okay see this\neverybody see this graph okay see this graph I will talk about h of SV first of\ngraph I will talk about h of SV first of\ngraph I will talk about h of SV first of all I'll talk about h of SV okay this\nall I'll talk about h of SV okay this\nall I'll talk about h of SV okay this one this is the entropy of category one\none this is the entropy of category one\none this is the entropy of category one you need to find and entropy of category\nyou need to find and entropy of category\nyou need to find and entropy of category 2 you need to find so if I write h of SV\n2 you need to find so if I write h of SV\n2 you need to find so if I write h of SV of category 1 so what is category 1 for\nof category 1 so what is category 1 for\nof category 1 so what is category 1 for this I'll write SC1 let's say I'm going\nthis I'll write SC1 let's say I'm going\nthis I'll write SC1 let's say I'm going to write like this quickly calculate the\nto write like this quickly calculate the\nto write like this quickly calculate the H of SV of this and this separately you\nH of SV of this and this separately you\nH of SV of this and this separately you need to calculate so h of SV of C1 okay\nneed to calculate so h of SV of C1 okay\nneed to calculate so h of SV of C1 okay so here again you'll write - 6X 8 log\nso here again you'll write - 6X 8 log\nso here again you'll write - 6X 8 log base 2 6X\nbase 2 6X\nbase 2 6X 8us 2x 8 log base to 2x 8 I hope\n8us 2x 8 log base to 2x 8 I hope\n8us 2x 8 log base to 2x 8 I hope everybody knows this how we got it so h\neverybody knows this how we got it so h\neverybody knows this how we got it so h of SV basically means I'm going to\nof SV basically means I'm going to\nof SV basically means I'm going to compute the entropy of this category and\ncompute the entropy of this category and\ncompute the entropy of this category and this category so for that I will\nthis category so for that I will\nthis category so for that I will basically write h of so here I will\nbasically write h of so here I will\nbasically write h of so here I will write - 6 by8 log base 2 6X 8 - 2x 8 log\nwrite - 6 by8 log base 2 6X 8 - 2x 8 log\nwrite - 6 by8 log base 2 6X 8 - 2x 8 log base 2 2x 8 so if I get it I'm actually\nbase 2 2x 8 so if I get it I'm actually\nbase 2 2x 8 so if I get it I'm actually going to get 81 and similarly if I if I\ngoing to get 81 and similarly if I if I\ngoing to get 81 and similarly if I if I calculate h of C2 quickly calculate how\ncalculate h of C2 quickly calculate how\ncalculate h of C2 quickly calculate how much you are going to get guys 6X 8 6X 8\nmuch you are going to get guys 6X 8 6X 8\nmuch you are going to get guys 6X 8 6X 8 with respect to this we need to find out\nwith respect to this we need to find out\nwith respect to this we need to find out so now we have all these values we'll\nso now we have all these values we'll\nso now we have all these values we'll start equating them to this equation so\nstart equating them to this equation so\nstart equating them to this equation so here we have finally gain of s comma\nhere we have finally gain of s comma\nhere we have finally gain of s comma fub1 so let's say that here I'm going to\nfub1 so let's say that here I'm going to\nfub1 so let's say that here I'm going to basically add\nbasically add\nbasically add 94 minus see minus summation of okay\n94 minus see minus summation of okay\n94 minus see minus summation of okay summation of what is s s of V understand\nsummation of what is s s of V understand\nsummation of what is s s of V understand s of V basically means that how many\ns of V basically means that how many\ns of V basically means that how many samples I have over here let's say for\nsamples I have over here let's say for\nsamples I have over here let's say for category one how many samples I have for\ncategory one how many samples I have for\ncategory one how many samples I have for category one over here simple if you\ncategory one over here simple if you\ncategory one over here simple if you really want to just calculate it is\nreally want to just calculate it is\nreally want to just calculate it is nothing but eight and total number of\nnothing but eight and total number of\nnothing but eight and total number of sample is how much if I go and see over\nsample is how much if I go and see over\nsample is how much if I go and see over here there are 9 years five NOS okay 9\nhere there are 9 years five NOS okay 9\nhere there are 9 years five NOS okay 9 years and five NOS that basically means\nyears and five NOS that basically means\nyears and five NOS that basically means 14 total sample here you have eight\n14 total sample here you have eight\n14 total sample here you have eight sample Okay so this will become\nsample Okay so this will become\nsample Okay so this will become 8x4 then you multiply by what see see\n8x4 then you multiply by what see see\n8x4 then you multiply by what see see from this equation you multiply by h of\nfrom this equation you multiply by h of\nfrom this equation you multiply by h of SV so h of SV is nothing but the entropy\nSV so h of SV is nothing but the entropy\nSV so h of SV is nothing but the entropy of category 1 so entropy of category 1\nof category 1 so entropy of category 1\nof category 1 so entropy of category 1 is nothing but 81 plus then you go again\nis nothing but 81 plus then you go again\nis nothing but 81 plus then you go again back to the graph and try to see that\nback to the graph and try to see that\nback to the graph and try to see that for C2 how much how many total number of\nfor C2 how much how many total number of\nfor C2 how much how many total number of samples are there 3 + 3 is 6 so 6 by 14\nsamples are there 3 + 3 is 6 so 6 by 14\nsamples are there 3 + 3 is 6 so 6 by 14 it will\nit will\nit will become multiplied by 1 right so this is\nbecome multiplied by 1 right so this is\nbecome multiplied by 1 right so this is your entire thing so here after all the\nyour entire thing so here after all the\nyour entire thing so here after all the calculation you are going to get\ncalculation you are going to get\ncalculation you are going to get 0.041 so this is my gain with s comma F1\n0.041 so this is my gain with s comma F1\n0.041 so this is my gain with s comma F1 so here I have got this value amazing I\nso here I have got this value amazing I\nso here I have got this value amazing I did this with feature one only what\ndid this with feature one only what\ndid this with feature one only what about feature two let's say that this\nabout feature two let's say that this\nabout feature two let's say that this was my split for feature two and suppose\nwas my split for feature two and suppose\nwas my split for feature two and suppose I get the gain for S comma feature 2 as\n.51 if I get this now tell\n.51 if I get this now tell me in using which feature should I start\nme in using which feature should I start\nme in using which feature should I start splitting first whether it should be\nsplitting first whether it should be\nsplitting first whether it should be fub1 or whether it should be FS2 based\nfub1 or whether it should be FS2 based\nfub1 or whether it should be FS2 based on this value you know that over here\non this value you know that over here\non this value you know that over here the gain the information gain of s comma\nthe gain the information gain of s comma\nthe gain the information gain of s comma F2 is greater than gain of s comma fub1\nF2 is greater than gain of s comma fub1\nF2 is greater than gain of s comma fub1 so your answer is very much simple we\nso your answer is very much simple we\nso your answer is very much simple we will definitely use feature 2 to start\nwill definitely use feature 2 to start\nwill definitely use feature 2 to start the split the thing over here you are\nthe split the thing over here you are\nthe split the thing over here you are trying to understand that if I really\ntrying to understand that if I really\ntrying to understand that if I really want to select which feature to select\nwant to select which feature to select\nwant to select which feature to select to start my splitting then I have to\nto start my splitting then I have to\nto start my splitting then I have to basically calculate the information gain\nbasically calculate the information gain\nbasically calculate the information gain and go throughout the all the paths and\nand go throughout the all the paths and\nand go throughout the all the paths and whichever path has the highest\nwhichever path has the highest\nwhichever path has the highest Information\nInformation\nInformation Gain then we will select that specific\nGain then we will select that specific\nGain then we will select that specific thing now the question Rises Kish\nthing now the question Rises Kish\nthing now the question Rises Kish obviously this is good but you had\nobviously this is good but you had\nobviously this is good but you had written about guinea impurity what is\nwritten about guinea impurity what is\nwritten about guinea impurity what is the purpose of that please explain us\nthe purpose of that please explain us\nthe purpose of that please explain us and why Guinea impurity is basically\nand why Guinea impurity is basically\nand why Guinea impurity is basically used so let me go ahead with guine\nused so let me go ahead with guine\nused so let me go ahead with guine impurity I told that yes you can\nimpurity I told that yes you can\nimpurity I told that yes you can obviously\nobviously\nobviously use you can obviously use entropy but\nuse you can obviously use entropy but\nuse you can obviously use entropy but why Guinea impurity so guine impurity\nwhy Guinea impurity so guine impurity\nwhy Guinea impurity so guine impurity formula which I have specifically\nformula which I have specifically\nformula which I have specifically written as 1 minus summation of IAL 1\nwritten as 1 minus summation of IAL 1\nwritten as 1 minus summation of IAL 1 2 N\n2 N\n2 N pÂ² now what is this pÂ² suppose let's say\npÂ² now what is this pÂ² suppose let's say\npÂ² now what is this pÂ² suppose let's say that in my n n is the number of outputs\nthat in my n n is the number of outputs\nthat in my n n is the number of outputs right now how many outputs I have I have\nright now how many outputs I have I have\nright now how many outputs I have I have two outputs yes or no so I will expand\ntwo outputs yes or no so I will expand\ntwo outputs yes or no so I will expand this 1 minus since this is summation I\nthis 1 minus since this is summation I\nthis 1 minus since this is summation I equal to 1 to n I'm basically going to\nequal to 1 to n I'm basically going to\nequal to 1 to n I'm basically going to basically say that okay fine I will\nbasically say that okay fine I will\nbasically say that okay fine I will write probability of plus whole\nwrite probability of plus whole\nwrite probability of plus whole Square uh plus probability of minus\nSquare uh plus probability of minus\nSquare uh plus probability of minus whole Square so this is the formula for\nwhole Square so this is the formula for\nwhole Square so this is the formula for guinea impurity now you may be thinking\nguinea impurity now you may be thinking\nguinea impurity now you may be thinking okay fine the calculation will be\nokay fine the calculation will be\nokay fine the calculation will be obviously very much equal easy right\nobviously very much equal easy right\nobviously very much equal easy right suppose if I have a node sorry if I have\nsuppose if I have a node sorry if I have\nsuppose if I have a node sorry if I have a node which which has 2 yes two NOS now\na node which which has 2 yes two NOS now\na node which which has 2 yes two NOS now in this particular case how do I\nin this particular case how do I\nin this particular case how do I calculate my this probability if I have\ncalculate my this probability if I have\ncalculate my this probability if I have two yes or two NOS suppose let's say\ntwo yes or two NOS suppose let's say\ntwo yes or two NOS suppose let's say that I have a node over here which is my\nthat I have a node over here which is my\nthat I have a node over here which is my split and this is having two yes and two\nsplit and this is having two yes and two\nsplit and this is having two yes and two no so how do I calculate I will write 1\nno so how do I calculate I will write 1\nno so how do I calculate I will write 1 minus what is probability of square 1X 2\nminus what is probability of square 1X 2\nminus what is probability of square 1X 2 square sorry not 1 by two\nsquare sorry not 1 by two\nsquare sorry not 1 by two yeah 1X 2 squ + 1 by 2\nyeah 1X 2 squ + 1 by 2\nyeah 1X 2 squ + 1 by 2 squ right then I will say 1 by 1X 4 + 1X\nsqu right then I will say 1 by 1X 4 + 1X\nsqu right then I will say 1 by 1X 4 + 1X 4 is nothing but 2x 4 which is nothing\n4 is nothing but 2x 4 which is nothing\n4 is nothing but 2x 4 which is nothing but 1X 2 so I will be getting 0.5 now\nbut 1X 2 so I will be getting 0.5 now\nbut 1X 2 so I will be getting 0.5 now here here you understand this is a\nhere here you understand this is a\nhere here you understand this is a complete impure split right if you have\ncomplete impure split right if you have\ncomplete impure split right if you have an impure split in entropy the output\nan impure split in entropy the output\nan impure split in entropy the output you getting it as one whereas in the\nyou getting it as one whereas in the\nyou getting it as one whereas in the case of Guinea impurity\ncase of Guinea impurity\ncase of Guinea impurity as Z sorry\nas Z sorry\nas Z sorry 0.5 so if I go ahead with the graph that\n0.5 so if I go ahead with the graph that\n0.5 so if I go ahead with the graph that I probably had created here so my Guinea\nI probably had created here so my Guinea\nI probably had created here so my Guinea impurity line will look something like\nimpurity line will look something like\nimpurity line will look something like this so it will be looking something\nthis so it will be looking something\nthis so it will be looking something like this for zero obviously I'll be\nlike this for zero obviously I'll be\nlike this for zero obviously I'll be getting zero but whenever my probability\ngetting zero but whenever my probability\ngetting zero but whenever my probability of plus is 0.5 I'm going to get 0.5 over\nof plus is 0.5 I'm going to get 0.5 over\nof plus is 0.5 I'm going to get 0.5 over here and that is the difference between\nhere and that is the difference between\nhere and that is the difference between Guinea\nGuinea\nGuinea impurity and entropy but the re but you\nimpurity and entropy but the re but you\nimpurity and entropy but the re but you may be seeing Kish when to use what now\nmay be seeing Kish when to use what now\nmay be seeing Kish when to use what now let's understand that when to use Guinea\nlet's understand that when to use Guinea\nlet's understand that when to use Guinea and when to use entropy tell me guys if\nand when to use entropy tell me guys if\nand when to use entropy tell me guys if I consider this formula of guine\nI consider this formula of guine\nI consider this formula of guine impurity and if I probably\nimpurity and if I probably\nimpurity and if I probably consider if I consider entropy this\nconsider if I consider entropy this\nconsider if I consider entropy this formula where do you think more time\nformula where do you think more time\nformula where do you think more time will take for execution for this\nwill take for execution for this\nwill take for execution for this particular formula whether for entropy\nparticular formula whether for entropy\nparticular formula whether for entropy it will take or for guinea impurity it\nit will take or for guinea impurity it\nit will take or for guinea impurity it will take more time where it will\nwill take more time where it will\nwill take more time where it will probably take for the execution purpose\nprobably take for the execution purpose\nprobably take for the execution purpose see understand decision tree is having a\nsee understand decision tree is having a\nsee understand decision tree is having a worst time complexity because if you\nworst time complexity because if you\nworst time complexity because if you have 100 features probably you'll keep\nhave 100 features probably you'll keep\nhave 100 features probably you'll keep on comparing by dividing many many\non comparing by dividing many many\non comparing by dividing many many features then probably compute a\nfeatures then probably compute a\nfeatures then probably compute a Information Gain like this if you have\nInformation Gain like this if you have\nInformation Gain like this if you have just 100 features so which is faster\njust 100 features so which is faster\njust 100 features so which is faster entrop\nentrop\nentrop or guine impurity understand in entropy\nor guine impurity understand in entropy\nor guine impurity understand in entropy you have log function here you have log\nyou have log function here you have log\nyou have log function here you have log function here you have simple maths the\nfunction here you have simple maths the\nfunction here you have simple maths the more amount of time out of entropy and\nmore amount of time out of entropy and\nmore amount of time out of entropy and guine impurity the more amount of time\nguine impurity the more amount of time\nguine impurity the more amount of time basically is taken\nbasically is taken\nbasically is taken by\nentropy so if you have huge number of\nentropy so if you have huge number of features like 100 200 features and you\nfeatures like 100 200 features and you\nfeatures like 100 200 features and you are planning to apply decision Tre I\nare planning to apply decision Tre I\nare planning to apply decision Tre I would suggest try to use Guinea impurity\nwould suggest try to use Guinea impurity\nwould suggest try to use Guinea impurity then entropy if you have small set of\nthen entropy if you have small set of\nthen entropy if you have small set of features then you can go ahead with\nfeatures then you can go ahead with\nfeatures then you can go ahead with entropy so over here definitely with\nentropy so over here definitely with\nentropy so over here definitely with respect to fast Guinea is greater than\nrespect to fast Guinea is greater than\nrespect to fast Guinea is greater than entropy now let's go ahead and\nentropy now let's go ahead and\nentropy now let's go ahead and understand with respect to you may be\nunderstand with respect to you may be\nunderstand with respect to you may be thinking Kish okay fine you have\nthinking Kish okay fine you have\nthinking Kish okay fine you have basically explained us about categorical\nbasically explained us about categorical\nbasically explained us about categorical variables over here see over here you\nvariables over here see over here you\nvariables over here see over here you have you have explained about\nhave you have explained about\nhave you have explained about categorical variables what if I have\ncategorical variables what if I have\ncategorical variables what if I have numerical feature let's say I have F1\nnumerical feature let's say I have F1\nnumerical feature let's say I have F1 over here which is a numerical\nover here which is a numerical\nover here which is a numerical feature I have an F1 feature which is\nfeature I have an F1 feature which is\nfeature I have an F1 feature which is numerical feature and I may have values\nnumerical feature and I may have values\nnumerical feature and I may have values let's say that I have sorted all the\nlet's say that I have sorted all the\nlet's say that I have sorted all the values over here okay let's say that I\nvalues over here okay let's say that I\nvalues over here okay let's say that I have F1 and output okay so this F1 let's\nhave F1 and output okay so this F1 let's\nhave F1 and output okay so this F1 let's say that I have values\nsay that I have values\nsay that I have values like ass sorted order values I'm sorting\nlike ass sorted order values I'm sorting\nlike ass sorted order values I'm sorting this features I'm basically doing this\nthis features I'm basically doing this\nthis features I'm basically doing this let's say that initially I have this\nlet's say that initially I have this\nlet's say that initially I have this features like this and let's say I have\nfeatures like this and let's say I have\nfeatures like this and let's say I have values like 2.3 1.3 4 5 7 3 let's say I\nvalues like 2.3 1.3 4 5 7 3 let's say I\nvalues like 2.3 1.3 4 5 7 3 let's say I have this features now this is a\nhave this features now this is a\nhave this features now this is a continuous\ncontinuous\ncontinuous feature this is a continuous feature so\nfeature this is a continuous feature so\nfeature this is a continuous feature so for a continuous feature how probably\nfor a continuous feature how probably\nfor a continuous feature how probably the decision tree entropy will be\nthe decision tree entropy will be\nthe decision tree entropy will be calculated and the Information Gain will\ncalculated and the Information Gain will\ncalculated and the Information Gain will get calculated so here you'll be able to\nget calculated so here you'll be able to\nget calculated so here you'll be able to see that I will first of all sort these\nsee that I will first of all sort these\nsee that I will first of all sort these values so in F1 the decision tree will B\nvalues so in F1 the decision tree will B\nvalues so in F1 the decision tree will B basically first of all sort this values\nbasically first of all sort this values\nbasically first of all sort this values so I have 1.3 then you have 2.3 then you\nso I have 1.3 then you have 2.3 then you\nso I have 1.3 then you have 2.3 then you have four then you have three three then\nhave four then you have three three then\nhave four then you have three three then you have four then you have five and\nyou have four then you have five and\nyou have four then you have five and then you have six now whenever you have\nthen you have six now whenever you have\nthen you have six now whenever you have a continuous feature so how the\na continuous feature so how the\na continuous feature so how the continuous feature will basically work\ncontinuous feature will basically work\ncontinuous feature will basically work in this case first of all your decision\nin this case first of all your decision\nin this case first of all your decision tree node will say\ntree node will say\ntree node will say that we'll take this one only one first\nthat we'll take this one only one first\nthat we'll take this one only one first record and say that if it is less than\nrecord and say that if it is less than\nrecord and say that if it is less than or equal to 1.3\nor equal to 1.3\nor equal to 1.3 okay if it is less than or equal to 1.3\nokay if it is less than or equal to 1.3\nokay if it is less than or equal to 1.3 so you here you'll be getting two\nso you here you'll be getting two\nso you here you'll be getting two branches yes or no so yes and no\nbranches yes or no so yes and no\nbranches yes or no so yes and no definitely your output over here will be\ndefinitely your output over here will be\ndefinitely your output over here will be put over here right and then for the no\nput over here right and then for the no\nput over here right and then for the no here you'll be having another node over\nhere you'll be having another node over\nhere you'll be having another node over here how many number of Records you'll\nhere how many number of Records you'll\nhere how many number of Records you'll be having in this particular case you'll\nbe having in this particular case you'll\nbe having in this particular case you'll be having one record in this particular\nbe having one record in this particular\nbe having one record in this particular case you will be having around five to\ncase you will be having around five to\ncase you will be having around five to six records and here also you'll be able\nsix records and here also you'll be able\nsix records and here also you'll be able to see right how many yes and NOS are\nto see right how many yes and NOS are\nto see right how many yes and NOS are there definitely this will be a leaf\nthere definitely this will be a leaf\nthere definitely this will be a leaf node so in the first instance they will\nnode so in the first instance they will\nnode so in the first instance they will go ahead and calculate the information\ngo ahead and calculate the information\ngo ahead and calculate the information gain of this then probably once the\ngain of this then probably once the\ngain of this then probably once the Information Gain Is got then what\nInformation Gain Is got then what\nInformation Gain Is got then what they'll do they will take the first two\nthey'll do they will take the first two\nthey'll do they will take the first two records and again create a new decision\nrecords and again create a new decision\nrecords and again create a new decision tree let's say that this will be my\ntree let's say that this will be my\ntree let's say that this will be my suggestion where they'll say it is less\nsuggestion where they'll say it is less\nsuggestion where they'll say it is less than or equal to 2.3 so I will get one\nthan or equal to 2.3 so I will get one\nthan or equal to 2.3 so I will get one and one over here so in this now you'll\nand one over here so in this now you'll\nand one over here so in this now you'll be having two records which will\nbe having two records which will\nbe having two records which will basically say how many yes and no are\nbasically say how many yes and no are\nbasically say how many yes and no are there and remaining all records will\nthere and remaining all records will\nthere and remaining all records will come over here then again Information\ncome over here then again Information\ncome over here then again Information Gain will be computed here then again\nGain will be computed here then again\nGain will be computed here then again what will happen they'll go to the next\nwhat will happen they'll go to the next\nwhat will happen they'll go to the next record then then again they'll create\nrecord then then again they'll create\nrecord then then again they'll create another feature where they'll say less\nanother feature where they'll say less\nanother feature where they'll say less than or equal to three and they will\nthan or equal to three and they will\nthan or equal to three and they will create this many nodes again they'll try\ncreate this many nodes again they'll try\ncreate this many nodes again they'll try to understand that how many yes or no\nto understand that how many yes or no\nto understand that how many yes or no are there and then they'll again compute\nare there and then they'll again compute\nare there and then they'll again compute The Information Gain like this they'll\nThe Information Gain like this they'll\nThe Information Gain like this they'll do it for each and every record and\ndo it for each and every record and\ndo it for each and every record and finally whichever Information Gain is\nfinally whichever Information Gain is\nfinally whichever Information Gain is higher they will select that specific\nhigher they will select that specific\nhigher they will select that specific value in that feature and they'll split\nvalue in that feature and they'll split\nvalue in that feature and they'll split the node so in a continuous feature\nthe node so in a continuous feature\nthe node so in a continuous feature whenever you have a continuous feature\nwhenever you have a continuous feature\nwhenever you have a continuous feature this is how it will basically have and\nthis is how it will basically have and\nthis is how it will basically have and then it will try to compute who is\nthen it will try to compute who is\nthen it will try to compute who is having the highest Information Gain the\nhaving the highest Information Gain the\nhaving the highest Information Gain the best Information Gain will get selected\nbest Information Gain will get selected\nbest Information Gain will get selected and from there the splitting will\nand from there the splitting will\nand from there the splitting will happen now let's go ahead and understand\nhappen now let's go ahead and understand\nhappen now let's go ahead and understand about the next topic is that how this\nabout the next topic is that how this\nabout the next topic is that how this entirely things work in decision tree\nentirely things work in decision tree\nentirely things work in decision tree regressor because in decision tree\nregressor because in decision tree\nregressor because in decision tree regressor my output is an continuous\nregressor my output is an continuous\nregressor my output is an continuous variable so suppose if I have one\nvariable so suppose if I have one\nvariable so suppose if I have one feature one feature two and this output\nfeature one feature two and this output\nfeature one feature two and this output is a continuous feature it will be\nis a continuous feature it will be\nis a continuous feature it will be continuous any value can be there so in\ncontinuous any value can be there so in\ncontinuous any value can be there so in this particular case how do I split it\nthis particular case how do I split it\nthis particular case how do I split it so let's say that f1c feature is getting\nso let's say that f1c feature is getting\nso let's say that f1c feature is getting selected now in this f1c feature what\nselected now in this f1c feature what\nselected now in this f1c feature what value will come when it is getting\nvalue will come when it is getting\nvalue will come when it is getting selected first of all the entire mean\nselected first of all the entire mean\nselected first of all the entire mean will get calculated of the output mean\nwill get calculated of the output mean\nwill get calculated of the output mean will get calculated so here I will have\nwill get calculated so here I will have\nwill get calculated so here I will have the mean and here here the cost function\nthe mean and here here the cost function\nthe mean and here here the cost function that is used is not Guinea coefficient\nthat is used is not Guinea coefficient\nthat is used is not Guinea coefficient or guinea impurity or entropy here we\nor guinea impurity or entropy here we\nor guinea impurity or entropy here we use mean squared\nuse mean squared\nuse mean squared error or you can also use mean absolute\nerror or you can also use mean absolute\nerror or you can also use mean absolute error now what is mean squared error if\nerror now what is mean squared error if\nerror now what is mean squared error if you remember from our logistic linear\nyou remember from our logistic linear\nyou remember from our logistic linear regression how do we calculate 1 by 2 m\nregression how do we calculate 1 by 2 m\nregression how do we calculate 1 by 2 m summation of I = 1 to n y hat minus y\nsummation of I = 1 to n y hat minus y\nsummation of I = 1 to n y hat minus y whole Square y hat of i y - y whole\nwhole Square y hat of i y - y whole\nwhole Square y hat of i y - y whole Square this is what is mean square error\nSquare this is what is mean square error\nSquare this is what is mean square error so what it will do first based on F1\nso what it will do first based on F1\nso what it will do first based on F1 feature it will try to assign a mean\nfeature it will try to assign a mean\nfeature it will try to assign a mean value and then it will compute the MSE\nvalue and then it will compute the MSE\nvalue and then it will compute the MSE value and then it'll go ahead and do the\nvalue and then it'll go ahead and do the\nvalue and then it'll go ahead and do the splitting now when it is doing splitting\nsplitting now when it is doing splitting\nsplitting now when it is doing splitting based on categories of continuous\nbased on categories of continuous\nbased on categories of continuous variable I will be having different\nvariable I will be having different\nvariable I will be having different different categories now in this\ndifferent categories now in this\ndifferent categories now in this categories what will happen after split\ncategories what will happen after split\ncategories what will happen after split some records will go over\nsome records will go over\nsome records will go over here then I will be having a mean value\nhere then I will be having a mean value\nhere then I will be having a mean value of this over here\nof this over here\nof this over here that will be my output and then again\nthat will be my output and then again\nthat will be my output and then again the MSC will get calculated over here as\nthe MSC will get calculated over here as\nthe MSC will get calculated over here as the msse gets reduced that basically\nthe msse gets reduced that basically\nthe msse gets reduced that basically means we are reaching near the leaf\nmeans we are reaching near the leaf\nmeans we are reaching near the leaf note and the same thing will happen over\nnote and the same thing will happen over\nnote and the same thing will happen over here so finally when you follow this\nhere so finally when you follow this\nhere so finally when you follow this path whatever mean value is present over\npath whatever mean value is present over\npath whatever mean value is present over here that will be your output this is\nhere that will be your output this is\nhere that will be your output this is the difference between the decision tree\nthe difference between the decision tree\nthe difference between the decision tree regressor and the classifier here\nregressor and the classifier here\nregressor and the classifier here instead of using entropy and all you use\ninstead of using entropy and all you use\ninstead of using entropy and all you use mean squar error or mean absolute error\nmean squar error or mean absolute error\nmean squar error or mean absolute error and this is the formula of mean square\nand this is the formula of mean square\nand this is the formula of mean square error now let's go to the one more topic\nerror now let's go to the one more topic\nerror now let's go to the one more topic which is called as the hyperparameters\nwhich is called as the hyperparameters\nwhich is called as the hyperparameters tell me decision tree if I keep on\ntell me decision tree if I keep on\ntell me decision tree if I keep on growing this to any depth what kind of\ngrowing this to any depth what kind of\ngrowing this to any depth what kind of problem it will face regressor part you\nproblem it will face regressor part you\nproblem it will face regressor part you want me to explain okay let's\nwant me to explain okay let's\nwant me to explain okay let's see okay let's let's do the\nsee okay let's let's do the\nsee okay let's let's do the regression decision\nregression decision\nregression decision tree\ntree\ntree regressor let's say I have feature F1\nregressor let's say I have feature F1\nregressor let's say I have feature F1 and this is my output let's say I have\nand this is my output let's say I have\nand this is my output let's say I have values like 20 24 26 28 30 and this is\nvalues like 20 24 26 28 30 and this is\nvalues like 20 24 26 28 30 and this is my feature one with category one\nmy feature one with category one\nmy feature one with category one category one let's\ncategory one let's\ncategory one let's say some categories are there let's say\nsay some categories are there let's say\nsay some categories are there let's say I have done\nI have done\nI have done the division by\nthe division by\nthe division by F1 that is this feature initially tell\nF1 that is this feature initially tell\nF1 that is this feature initially tell me what is the mean of this that mean\nme what is the mean of this that mean\nme what is the mean of this that mean value will get assigned over here then\nvalue will get assigned over here then\nvalue will get assigned over here then using msse that is mean squar error here\nusing msse that is mean squar error here\nusing msse that is mean squar error here you will try to calculate suppose I get\nyou will try to calculate suppose I get\nyou will try to calculate suppose I get an msse of some 37 47 something like\nan msse of some 37 47 something like\nan msse of some 37 47 something like this and then I will try to split this\nthis and then I will try to split this\nthis and then I will try to split this then I will be getting two more nodes or\nthen I will be getting two more nodes or\nthen I will be getting two more nodes or three more nodes it depends then that\nthree more nodes it depends then that\nthree more nodes it depends then that specific nodes will be the part of this\nspecific nodes will be the part of this\nspecific nodes will be the part of this again the mean will change again the\nagain the mean will change again the\nagain the mean will change again the mean will change over here suppose this\nmean will change over here suppose this\nmean will change over here suppose this two is there this two records goes here\ntwo is there this two records goes here\ntwo is there this two records goes here right then again MC will get calculated\nright then again MC will get calculated\nright then again MC will get calculated I'm just taking as an example over here\nI'm just taking as an example over here\nI'm just taking as an example over here just try to assume this thing now if I\njust try to assume this thing now if I\njust try to assume this thing now if I talk about hyper parameters see this is\ntalk about hyper parameters see this is\ntalk about hyper parameters see this is what is the formula that gets applied\nwhat is the formula that gets applied\nwhat is the formula that gets applied over MSC now let's see in this hyper\nover MSC now let's see in this hyper\nover MSC now let's see in this hyper parameter always understand decision\nparameter always understand decision\nparameter always understand decision tree leads to overfitting because we are\ntree leads to overfitting because we are\ntree leads to overfitting because we are just going to divide the nodes to\njust going to divide the nodes to\njust going to divide the nodes to whatever level we want so this obviously\nwhatever level we want so this obviously\nwhatever level we want so this obviously will lead to\nwill lead to\nwill lead to overfitting now in order to prevent\noverfitting now in order to prevent\noverfitting now in order to prevent overfitting we perform two important\noverfitting we perform two important\noverfitting we perform two important steps one is post pruning and one is\nsteps one is post pruning and one is\nsteps one is post pruning and one is pre- pruning so this two post pruning\npre- pruning so this two post pruning\npre- pruning so this two post pruning and pre pruning is a condition let's say\nand pre pruning is a condition let's say\nand pre pruning is a condition let's say that I have done some\nthat I have done some\nthat I have done some splits I have done some splits let's say\nsplits I have done some splits let's say\nsplits I have done some splits let's say over here I have seven yes and two\nover here I have seven yes and two\nover here I have seven yes and two no and again probably I do the further\nno and again probably I do the further\nno and again probably I do the further split like this now in this particular\nsplit like this now in this particular\nsplit like this now in this particular scenario you know that if 7 yes and two\nscenario you know that if 7 yes and two\nscenario you know that if 7 yes and two NOS are there there is a maximum there\nNOS are there there is a maximum there\nNOS are there there is a maximum there is more than 80% chances that this node\nis more than 80% chances that this node\nis more than 80% chances that this node is saying that the output is yes so\nis saying that the output is yes so\nis saying that the output is yes so should we further do more\nshould we further do more\nshould we further do more pruning the answer is no we can close it\npruning the answer is no we can close it\npruning the answer is no we can close it and we can cut the branch from here this\nand we can cut the branch from here this\nand we can cut the branch from here this technique is basically called as post\ntechnique is basically called as post\ntechnique is basically called as post pruning that basically means first of\npruning that basically means first of\npruning that basically means first of all you create your decision tree then\nall you create your decision tree then\nall you create your decision tree then probably see the decision tree and see\nprobably see the decision tree and see\nprobably see the decision tree and see that whether there is an extra Branch or\nthat whether there is an extra Branch or\nthat whether there is an extra Branch or not and just try to cut it there is one\nnot and just try to cut it there is one\nnot and just try to cut it there is one more thing which is called as\nmore thing which is called as\nmore thing which is called as pre-pruning now pre-pruning is decided\npre-pruning now pre-pruning is decided\npre-pruning now pre-pruning is decided by hyperparameters what kind of hyper\nby hyperparameters what kind of hyper\nby hyperparameters what kind of hyper parameters you can basically say that\nparameters you can basically say that\nparameters you can basically say that how many number of decision tree needs\nhow many number of decision tree needs\nhow many number of decision tree needs to be used not number of decision tree\nto be used not number of decision tree\nto be used not number of decision tree sorry over here you may say that what is\nsorry over here you may say that what is\nsorry over here you may say that what is the max\nthe max\nthe max depth what is the max depth how many Max\ndepth what is the max depth how many Max\ndepth what is the max depth how many Max Leaf you can\nLeaf you can\nLeaf you can have so this all parameters you can set\nhave so this all parameters you can set\nhave so this all parameters you can set it with grid SE\nit with grid SE\nit with grid SE CV and you can try it and you can\nCV and you can try it and you can\nCV and you can try it and you can basically come up with a pre- pruning\nbasically come up with a pre- pruning\nbasically come up with a pre- pruning technique so this is the idea about\ntechnique so this is the idea about\ntechnique so this is the idea about decision tree uh regressor yes yes it is\ndecision tree uh regressor yes yes it is\ndecision tree uh regressor yes yes it is possible your guinea value will be one\npossible your guinea value will be one\npossible your guinea value will be one no this graph is there\nno this graph is there\nno this graph is there no Guinea value are you talking about\nno Guinea value are you talking about\nno Guinea value are you talking about this Guinea entropy it will not be one\nthis Guinea entropy it will not be one\nthis Guinea entropy it will not be one it will always be between 0\nit will always be between 0\nit will always be between 0 to.5 so the first thing first as usual\nto.5 so the first thing first as usual\nto.5 so the first thing first as usual what we should do we should import the\nwhat we should do we should import the\nwhat we should do we should import the libraries so here I will go ahead and\nlibraries so here I will go ahead and\nlibraries so here I will go ahead and import the librar so I'll say\nimport the librar so I'll say\nimport the librar so I'll say import pandas as NP PD import matplot\nimport pandas as NP PD import matplot\nimport pandas as NP PD import matplot li. pyplot as PLT\nli. pyplot as PLT\nli. pyplot as PLT uh\nuh\nuh import so this basic things I have with\nimport so this basic things I have with\nimport so this basic things I have with me so I will go and take any data set\nme so I will go and take any data set\nme so I will go and take any data set that I want from SK\nthat I want from SK\nthat I want from SK learn. data sets import let's say that\nlearn. data sets import let's say that\nlearn. data sets import let's say that I'm going to take load Iris data set and\nI'm going to take load Iris data set and\nI'm going to take load Iris data set and then I'm going to upload the iris data\nthen I'm going to upload the iris data\nthen I'm going to upload the iris data set so I'm going to write load Iris\nset so I'm going to write load Iris\nset so I'm going to write load Iris there is my Iris data set then the next\nthere is my Iris data set then the next\nthere is my Iris data set then the next step uh once you get your iris data set\nstep uh once you get your iris data set\nstep uh once you get your iris data set so this is my iris. dat\nso this is my iris. dat\nso this is my iris. dat okay these are all my features the four\nokay these are all my features the four\nokay these are all my features the four features will be there these four\nfeatures will be there these four\nfeatures will be there these four features are petal length petal width\nfeatures are petal length petal width\nfeatures are petal length petal width SLE length and SLE width this is my\nSLE length and SLE width this is my\nSLE length and SLE width this is my independent features then if I really\nindependent features then if I really\nindependent features then if I really want to apply\nwant to apply\nwant to apply for classifier so decision tree\nfor classifier so decision tree\nfor classifier so decision tree classifier so I can first of all import\nclassifier so I can first of all import\nclassifier so I can first of all import from\nfrom\nfrom skarn do tree import decision let's see\nskarn do tree import decision let's see\nskarn do tree import decision let's see where decision tree present in a scalon\nwhere decision tree present in a scalon\nwhere decision tree present in a scalon decision tree\ndecision tree\ndecision tree classifier the name is absolutely fine\nclassifier the name is absolutely fine\nclassifier the name is absolutely fine but I was not getting over here\nbut I was not getting over here\nbut I was not getting over here so so this is got no module SK okay SK\nso so this is got no module SK okay SK\nso so this is got no module SK okay SK skar\nskar\nskar skn learn so here you have\nskn learn so here you have\nskn learn so here you have classifier right now I'm just going to\nclassifier right now I'm just going to\nclassifier right now I'm just going to overfit the data then I'll probably show\noverfit the data then I'll probably show\noverfit the data then I'll probably show you how you can go ahead with uh\nyou how you can go ahead with uh\nyou how you can go ahead with uh pruning so by default what are the\npruning so by default what are the\npruning so by default what are the parameters over here if you probably go\nparameters over here if you probably go\nparameters over here if you probably go and see in in the classifier over here\nand see in in the classifier over here\nand see in in the classifier over here you have Criterion see this the first P\nyou have Criterion see this the first P\nyou have Criterion see this the first P parameter is Criterion by default it is\nparameter is Criterion by default it is\nparameter is Criterion by default it is Guinea then you have Splitter Splitter\nGuinea then you have Splitter Splitter\nGuinea then you have Splitter Splitter basically means how you're going to\nbasically means how you're going to\nbasically means how you're going to split and there also you have two types\nsplit and there also you have two types\nsplit and there also you have two types best and random you can randomly select\nbest and random you can randomly select\nbest and random you can randomly select the features and do it okay you should\nthe features and do it okay you should\nthe features and do it okay you should always go with\nalways go with\nalways go with best max depth is a hyper parameter\nbest max depth is a hyper parameter\nbest max depth is a hyper parameter minimum sample lift is a hyper parameter\nminimum sample lift is a hyper parameter\nminimum sample lift is a hyper parameter Max Fe features how many number of\nMax Fe features how many number of\nMax Fe features how many number of features we are going to take in order\nfeatures we are going to take in order\nfeatures we are going to take in order to fix that that is also an hyper\nto fix that that is also an hyper\nto fix that that is also an hyper parameter so all these things are hyper\nparameter so all these things are hyper\nparameter so all these things are hyper parameter okay so I will just by default\nparameter okay so I will just by default\nparameter okay so I will just by default executed whatever is giving me in\nexecuted whatever is giving me in\nexecuted whatever is giving me in decision tree and the next thing that\ndecision tree and the next thing that\ndecision tree and the next thing that I'm actually going to do is create a\nI'm actually going to do is create a\nI'm actually going to do is create a decision tree so for this I will be\ndecision tree so for this I will be\ndecision tree so for this I will be using plot. fig size plot. figure inside\nusing plot. fig size plot. figure inside\nusing plot. fig size plot. figure inside figure I have this fix\nfigure I have this fix\nfigure I have this fix size okay and I will probably show in\nsize okay and I will probably show in\nsize okay and I will probably show in some better figure size so that\nsome better figure size so that\nsome better figure size so that everybody body will be able to see it so\neverybody body will be able to see it so\neverybody body will be able to see it so here let me say that I'm going to take\nhere let me say that I'm going to take\nhere let me say that I'm going to take an area of\nan area of\nan area of 1510 and then probably I'm going to say\n1510 and then probably I'm going to say\n1510 and then probably I'm going to say tree Dot\ntree Dot\ntree Dot Plot and here I'm going to say a\nPlot and here I'm going to say a\nPlot and here I'm going to say a classifier and it should be filled the\nclassifier and it should be filled the\nclassifier and it should be filled the coloring should be filled with this so\ncoloring should be filled with this so\ncoloring should be filled with this so tree sorry Tre Tre Tre Tre\ntree sorry Tre Tre Tre Tre\ntree sorry Tre Tre Tre Tre Tre it should be classifi tree. plot\nTre it should be classifi tree. plot\nTre it should be classifi tree. plot okay I have to also import uh tree so I\nokay I have to also import uh tree so I\nokay I have to also import uh tree so I have to basically import tree so from SK\nhave to basically import tree so from SK\nhave to basically import tree so from SK learn\nlearn\nlearn import three again I'm getting\nimport three again I'm getting\nimport three again I'm getting error has no attribute plot\nerror has no attribute plot\nerror has no attribute plot why let me just see the documentation\nwhy let me just see the documentation\nwhy let me just see the documentation guys so this plot function is like plot\nguys so this plot function is like plot\nguys so this plot function is like plot uncore tree dot tab plot _ tree now what\nuncore tree dot tab plot _ tree now what\nuncore tree dot tab plot _ tree now what is the error we are getting okay not\nis the error we are getting okay not\nis the error we are getting okay not fitted yet\nfitted yet\nfitted yet sorry so I'm going to say\nsorry so I'm going to say\nsorry so I'm going to say classifier do fit on data what data\nclassifier do fit on data what data\nclassifier do fit on data what data iris.\niris.\niris. data and then I'm going to fit with Iris\ndata and then I'm going to fit with Iris\ndata and then I'm going to fit with Iris dot\ndot\ndot Target so once this is done I think now\nTarget so once this is done I think now\nTarget so once this is done I think now it will get\nit will get\nit will get executed so this is how your graph will\nexecuted so this is how your graph will\nexecuted so this is how your graph will look like guys so here you can see this\nlook like guys so here you can see this\nlook like guys so here you can see this is how your graph looks like now if I\nis how your graph looks like now if I\nis how your graph looks like now if I show you the graph over here see you can\nshow you the graph over here see you can\nshow you the graph over here see you can see some amazing things over here three\nsee some amazing things over here three\nsee some amazing things over here three outputs are actually there in this when\noutputs are actually there in this when\noutputs are actually there in this when you see in this left hand side this\nyou see in this left hand side this\nyou see in this left hand side this become a leaf node so this first one is\nbecome a leaf node so this first one is\nbecome a leaf node so this first one is probably vers color uh versol flower\nprobably vers color uh versol flower\nprobably vers color uh versol flower okay if you go on the right hand side\nokay if you go on the right hand side\nokay if you go on the right hand side here you can see 50/50 is there so based\nhere you can see 50/50 is there so based\nhere you can see 50/50 is there so based on one feature based on one feature here\non one feature based on one feature here\non one feature based on one feature here you'll be able to see that you are\nyou'll be able to see that you are\nyou'll be able to see that you are getting a leaf node based on another\ngetting a leaf node based on another\ngetting a leaf node based on another Branch here you are getting\nBranch here you are getting\nBranch here you are getting 05050 so again you have two more\n05050 so again you have two more\n05050 so again you have two more features getting splitted over here so\nfeatures getting splitted over here so\nfeatures getting splitted over here so here you have 495 here you have\nhere you have 495 here you have\nhere you have 495 here you have 471 do we require this split anybody\n471 do we require this split anybody\n471 do we require this split anybody tell me from here do we require any any\ntell me from here do we require any any\ntell me from here do we require any any more split just try to think this is\nmore split just try to think this is\nmore split just try to think this is after post pruning I want to find out\nafter post pruning I want to find out\nafter post pruning I want to find out whether more splits are required or not\nwhether more splits are required or not\nwhether more splits are required or not now in this particular case you see this\nnow in this particular case you see this\nnow in this particular case you see this after this do you require any\nafter this do you require any\nafter this do you require any split you do not require right here you\nsplit you do not require right here you\nsplit you do not require right here you are basically getting 47 and one I guess\nare basically getting 47 and one I guess\nare basically getting 47 and one I guess after this also you require no split\nafter this also you require no split\nafter this also you require no split understand this so this is basically\nunderstand this so this is basically\nunderstand this so this is basically post pruning so you can then decide your\npost pruning so you can then decide your\npost pruning so you can then decide your level and probably do it gu value is\nlevel and probably do it gu value is\nlevel and probably do it gu value is more than\nmore than\nmore than 0.5 okay this side H this is coming as\n0.5 okay this side H this is coming as\n0.5 okay this side H this is coming as 0.5 greater than 0.5 it should not had\n0.5 greater than 0.5 it should not had\n0.5 greater than 0.5 it should not had here it is\nhere it is\nhere it is 0.5 no maximum .5 can come 0 to.5 only\n0.5 no maximum .5 can come 0 to.5 only\n0.5 no maximum .5 can come 0 to.5 only should come I don't know why this is\nshould come I don't know why this is\nshould come I don't know why this is coming as 667\nI'll have a look onto this guys but\nI'll have a look onto this guys but anywhere you see other than that you're\nanywhere you see other than that you're\nanywhere you see other than that you're everywhere you're getting less\neverywhere you're getting less\neverywhere you're getting less than5 the plotting graph is very much\nthan5 the plotting graph is very much\nthan5 the plotting graph is very much easy you use SK learn import tree then\neasy you use SK learn import tree then\neasy you use SK learn import tree then you basically do this get classify and\nyou basically do this get classify and\nyou basically do this get classify and field is equal to true and you can just\nfield is equal to true and you can just\nfield is equal to true and you can just do this so the agenda let me Define the\ndo this so the agenda let me Define the\ndo this so the agenda let me Define the agenda what all things are there first\nagenda what all things are there first\nagenda what all things are there first we'll understand about\nwe'll understand about\nwe'll understand about emble techniques in this assemble\nemble techniques in this assemble\nemble techniques in this assemble techniques we are basically going to\ntechniques we are basically going to\ntechniques we are basically going to discuss about what is the difference\ndiscuss about what is the difference\ndiscuss about what is the difference between\nbetween\nbetween bagging and boosting\nbagging and boosting\nbagging and boosting second what we are basically going to\nsecond what we are basically going to\nsecond what we are basically going to discuss about is so uh the agenda of\ndiscuss about is so uh the agenda of\ndiscuss about is so uh the agenda of this session is emble techniques bagging\nthis session is emble techniques bagging\nthis session is emble techniques bagging and boosting then we are probably going\nand boosting then we are probably going\nand boosting then we are probably going to cover random forest and then probably\nto cover random forest and then probably\nto cover random forest and then probably we will try to cover adab boost and if I\nwe will try to cover adab boost and if I\nwe will try to cover adab boost and if I have more energy I will also try to\nhave more energy I will also try to\nhave more energy I will also try to cover XG boost so all this Al lthms\ncover XG boost so all this Al lthms\ncover XG boost so all this Al lthms we'll discuss about it so let's go ahead\nwe'll discuss about it so let's go ahead\nwe'll discuss about it so let's go ahead and let's start the\nand let's start the\nand let's start the topics the first topic that we are going\ntopics the first topic that we are going\ntopics the first topic that we are going to discuss is about emble\nto discuss is about emble\nto discuss is about emble techniques now what exactly is emble\ntechniques now what exactly is emble\ntechniques now what exactly is emble techniques and we are going to discuss\ntechniques and we are going to discuss\ntechniques and we are going to discuss about it okay so emble techniques what\nabout it okay so emble techniques what\nabout it okay so emble techniques what exactly is emble techniques till now we\nexactly is emble techniques till now we\nexactly is emble techniques till now we have solved two different kind of\nhave solved two different kind of\nhave solved two different kind of problem statement one is\nproblem statement one is\nproblem statement one is classification and regression and you\nclassification and regression and you\nclassification and regression and you have learned about different different\nhave learned about different different\nhave learned about different different algorithms like uh linear regression\nalgorithms like uh linear regression\nalgorithms like uh linear regression logistic regression we have discussed\nlogistic regression we have discussed\nlogistic regression we have discussed about KNN we have discussed about\nabout KNN we have discussed about\nabout KNN we have discussed about yesterday what disc what did we discuss\nyesterday what disc what did we discuss\nyesterday what disc what did we discuss about n bias different different\nabout n bias different different\nabout n bias different different algorithms we have already finished now\nalgorithms we have already finished now\nalgorithms we have already finished now with respect to classification\nwith respect to classification\nwith respect to classification regression Problem whatever algorithm we\nregression Problem whatever algorithm we\nregression Problem whatever algorithm we are discussing there was only one\nare discussing there was only one\nare discussing there was only one algorithm at a time we were discussing\nalgorithm at a time we were discussing\nalgorithm at a time we were discussing one algorithm at a time we are\none algorithm at a time we are\none algorithm at a time we are discussing and we are trying to either\ndiscussing and we are trying to either\ndiscussing and we are trying to either solve a classification or a regression\nsolve a classification or a regression\nsolve a classification or a regression problem now the next thing is over here\nproblem now the next thing is over here\nproblem now the next thing is over here is that can we use multiple algorithms\nis that can we use multiple algorithms\nis that can we use multiple algorithms mul multiple algorithm to solve a\nmul multiple algorithm to solve a\nmul multiple algorithm to solve a problem multiple algorithms basically\nproblem multiple algorithms basically\nproblem multiple algorithms basically means can we I'll just talk about it\nmeans can we I'll just talk about it\nmeans can we I'll just talk about it okay now the if I ask this specific\nokay now the if I ask this specific\nokay now the if I ask this specific question can we use multiple algorithms\nquestion can we use multiple algorithms\nquestion can we use multiple algorithms to solve a problem at that point of time\nto solve a problem at that point of time\nto solve a problem at that point of time I will definitely say yes we can because\nI will definitely say yes we can because\nI will definitely say yes we can because we are going to use something called as\nwe are going to use something called as\nwe are going to use something called as emble techniques there now what this\nemble techniques there now what this\nemble techniques there now what this emble techniques is okay so emble\nemble techniques is okay so emble\nemble techniques is okay so emble techniques in emble techniques we\ntechniques in emble techniques we\ntechniques in emble techniques we specifically use two different ways one\nspecifically use two different ways one\nspecifically use two different ways one is one one way is that we specifically\nis one one way is that we specifically\nis one one way is that we specifically use and the other one I'll just go to\nuse and the other one I'll just go to\nuse and the other one I'll just go to write it over here so one that we\nwrite it over here so one that we\nwrite it over here so one that we basically use is something called as\nbasically use is something called as\nbasically use is something called as bagging technique and the other one we\nbagging technique and the other one we\nbagging technique and the other one we specifically use is something called as\nspecifically use is something called as\nspecifically use is something called as boosting technique so in bagging\nboosting technique so in bagging\nboosting technique so in bagging Technique we what exactly we can do and\nTechnique we what exactly we can do and\nTechnique we what exactly we can do and in boosting technique what we can\nin boosting technique what we can\nin boosting technique what we can actually do and how we are combining\nactually do and how we are combining\nactually do and how we are combining multiple models to solve a problem so\nmultiple models to solve a problem so\nmultiple models to solve a problem so let's first of all discuss about bagging\nlet's first of all discuss about bagging\nlet's first of all discuss about bagging now how does bagging work let's say that\nnow how does bagging work let's say that\nnow how does bagging work let's say that I have a specific data set so this is my\nI have a specific data set so this is my\nI have a specific data set so this is my data set with uh with features rows\ndata set with uh with features rows\ndata set with uh with features rows columns everything like this I have this\ncolumns everything like this I have this\ncolumns everything like this I have this specific data set just imagine I have\nspecific data set just imagine I have\nspecific data set just imagine I have many many features over here like this\nmany many features over here like this\nmany many features over here like this fub1 F2 F3 and probably I have my output\nfub1 F2 F3 and probably I have my output\nfub1 F2 F3 and probably I have my output so this is my data set D let's consider\nso this is my data set D let's consider\nso this is my data set D let's consider it now what we do in bagging is that we\nit now what we do in bagging is that we\nit now what we do in bagging is that we create models and this model can be\ncreate models and this model can be\ncreate models and this model can be anything it can be logistic it can be\nanything it can be logistic it can be\nanything it can be logistic it can be linear for a classification problem\nlinear for a classification problem\nlinear for a classification problem let's say that this is logistic model so\nlet's say that this is logistic model so\nlet's say that this is logistic model so this is my model M1 let's say I have\nthis is my model M1 let's say I have\nthis is my model M1 let's say I have another model M2 then I may have another\nanother model M2 then I may have another\nanother model M2 then I may have another model M3 let's say that this is\nmodel M3 let's say that this is\nmodel M3 let's say that this is logistic and this is probably the other\nlogistic and this is probably the other\nlogistic and this is probably the other model which is like decision tree and\nmodel which is like decision tree and\nmodel which is like decision tree and then probably we use this model as KNN\nthen probably we use this model as KNN\nthen probably we use this model as KNN classification and this model can again\nclassification and this model can again\nclassification and this model can again be decision tree it's fine let's use\nbe decision tree it's fine let's use\nbe decision tree it's fine let's use another decision tree so now here you\nanother decision tree so now here you\nanother decision tree so now here you can see that we have used so many models\ncan see that we have used so many models\ncan see that we have used so many models okay so many models are there now with\nokay so many models are there now with\nokay so many models are there now with respect to this particular model what I\nrespect to this particular model what I\nrespect to this particular model what I will do is that the first step that I\nwill do is that the first step that I\nwill do is that the first step that I will do from this particular data set I\nwill do from this particular data set I\nwill do from this particular data set I will just take up some rows so I'll\nwill just take up some rows so I'll\nwill just take up some rows so I'll basically do row\nbasically do row\nbasically do row sampling and I'll take a row sampling of\nsampling and I'll take a row sampling of\nsampling and I'll take a row sampling of D Dash D Das basically means this D Das\nD Dash D Das basically means this D Das\nD Dash D Das basically means this D Das is always less than D some of the rows\nis always less than D some of the rows\nis always less than D some of the rows I'll push it to M1 okay I can also use n\nI'll push it to M1 okay I can also use n\nI'll push it to M1 okay I can also use n fine so what I'll do is that some of the\nfine so what I'll do is that some of the\nfine so what I'll do is that some of the rows I'll push it to model one this\nrows I'll push it to model one this\nrows I'll push it to model one this model one will be training let's say\nmodel one will be training let's say\nmodel one will be training let's say that for out of this 10,000 record th000\nthat for out of this 10,000 record th000\nthat for out of this 10,000 record th000 rows I'm actually doing a row sampling\nrows I'm actually doing a row sampling\nrows I'm actually doing a row sampling of th rows and giving it to M1 to train\nof th rows and giving it to M1 to train\nof th rows and giving it to M1 to train it then what I'm actually going to do\nit then what I'm actually going to do\nit then what I'm actually going to do over here I'm basically going to give\nover here I'm basically going to give\nover here I'm basically going to give this specific model M2 and again I'm\nthis specific model M2 and again I'm\nthis specific model M2 and again I'm going to do row row sampling and I'm\ngoing to do row row sampling and I'm\ngoing to do row row sampling and I'm again going to sample some of the rows\nagain going to sample some of the rows\nagain going to sample some of the rows and give it to model two and again\nand give it to model two and again\nand give it to model two and again remember some of the rows may get\nremember some of the rows may get\nremember some of the rows may get repeated from this D Dash to next dble\nrepeated from this D Dash to next dble\nrepeated from this D Dash to next dble Dash similarly I will do row sampling\nDash similarly I will do row sampling\nDash similarly I will do row sampling and give it to this and again I may have\nand give it to this and again I may have\nand give it to this and again I may have d triple Dash and D4 Dash so different\nd triple Dash and D4 Dash so different\nd triple Dash and D4 Dash so different different different different rows data\ndifferent different different rows data\ndifferent different different rows data points when I say row sampling basically\npoints when I say row sampling basically\npoints when I say row sampling basically I'm talking about data points different\nI'm talking about data points different\nI'm talking about data points different different data points I will give it to\ndifferent data points I will give it to\ndifferent data points I will give it to separate separate model and this model\nseparate separate model and this model\nseparate separate model and this model will specifically train when I say D\nwill specifically train when I say D\nwill specifically train when I say D Dash that basically means uh suppose I\nDash that basically means uh suppose I\nDash that basically means uh suppose I say th 10,000 are my total number of\nsay th 10,000 are my total number of\nsay th 10,000 are my total number of data points when I say D Dash This D\ndata points when I say D Dash This D\ndata points when I say D Dash This D Dash may be th000 points then D Double\nDash may be th000 points then D Double\nDash may be th000 points then D Double Dash may be another th000 points and\nDash may be another th000 points and\nDash may be another th000 points and some of the rows may get repeated over\nsome of the rows may get repeated over\nsome of the rows may get repeated over here dle Dash here also I can basically\nhere dle Dash here also I can basically\nhere dle Dash here also I can basically use so here specifically row sampling\nuse so here specifically row sampling\nuse so here specifically row sampling will be used now when I have this many\nwill be used now when I have this many\nwill be used now when I have this many specific each and every model will be\nspecific each and every model will be\nspecific each and every model will be trained with different kind of data now\ntrained with different kind of data now\ntrained with different kind of data now how the inferencing will happen for the\nhow the inferencing will happen for the\nhow the inferencing will happen for the test data so first thing first let's say\ntest data so first thing first let's say\ntest data so first thing first let's say that I'm going to get a new test data\nthat I'm going to get a new test data\nthat I'm going to get a new test data over here now new test data will be\nover here now new test data will be\nover here now new test data will be passed to M1 and this M1 suppose it\npassed to M1 and this M1 suppose it\npassed to M1 and this M1 suppose it gives zero as my output suppose let's\ngives zero as my output suppose let's\ngives zero as my output suppose let's say that I'm doing a binary\nsay that I'm doing a binary\nsay that I'm doing a binary classification it gives a Zer as an\nclassification it gives a Zer as an\nclassification it gives a Zer as an output so this is my output of zero next\noutput so this is my output of zero next\noutput so this is my output of zero next M2 for the new test data gives one M3\nM2 for the new test data gives one M3\nM2 for the new test data gives one M3 gives one and M4 also gives one as the\ngives one and M4 also gives one as the\ngives one and M4 also gives one as the the output now in this particular case\nthe output now in this particular case\nthe output now in this particular case in this particular case what will happen\nin this particular case what will happen\nin this particular case what will happen now you can see over here it's simple\nnow you can see over here it's simple\nnow you can see over here it's simple what what do you think the output may be\nwhat what do you think the output may be\nwhat what do you think the output may be in this particular case now M1 has\nin this particular case now M1 has\nin this particular case now M1 has predicted for this particular test data\npredicted for this particular test data\npredicted for this particular test data as zero the model M2 has predicted 1 M3\nas zero the model M2 has predicted 1 M3\nas zero the model M2 has predicted 1 M3 has predicted 1 and M4 has predicted one\nhas predicted 1 and M4 has predicted one\nhas predicted 1 and M4 has predicted one so finally all these outputs are going\nso finally all these outputs are going\nso finally all these outputs are going to get\nto get\nto get aggregated are going to get aggregated\naggregated are going to get aggregated\naggregated are going to get aggregated and a simple thing that gets applied is\nand a simple thing that gets applied is\nand a simple thing that gets applied is majority voting majority voting so tell\nmajority voting majority voting so tell\nmajority voting majority voting so tell me what will be the output for with\nme what will be the output for with\nme what will be the output for with respect to this the output will\nrespect to this the output will\nrespect to this the output will obviously be one because the majority\nobviously be one because the majority\nobviously be one because the majority voting that you can see three people are\nvoting that you can see three people are\nvoting that you can see three people are basically saying it as one so my output\nbasically saying it as one so my output\nbasically saying it as one so my output over here will be one okay this is the\nover here will be one okay this is the\nover here will be one okay this is the concept of bagging wherein you are\nconcept of bagging wherein you are\nconcept of bagging wherein you are providing different different rows with\nproviding different different rows with\nproviding different different rows with probably all the features in this case\nprobably all the features in this case\nprobably all the features in this case and giving it to different different\nand giving it to different different\nand giving it to different different model again which is a classification\nmodel again which is a classification\nmodel again which is a classification model and then finally you are combining\nmodel and then finally you are combining\nmodel and then finally you are combining them based on majority voting and you're\nthem based on majority voting and you're\nthem based on majority voting and you're getting the answer as one so this step\ngetting the answer as one so this step\ngetting the answer as one so this step is called as bootstrap aggregator that\nis called as bootstrap aggregator that\nis called as bootstrap aggregator that basically means you're aggregating all\nbasically means you're aggregating all\nbasically means you're aggregating all the output that is basically coming from\nthe output that is basically coming from\nthe output that is basically coming from all the specific models all the specific\nall the specific models all the specific\nall the specific models all the specific models now many people will say Krish\nmodels now many people will say Krish\nmodels now many people will say Krish what about Tai guys like this kind of\nwhat about Tai guys like this kind of\nwhat about Tai guys like this kind of situation you know we will be having\nsituation you know we will be having\nsituation you know we will be having more than 100 to 200 models so it is\nmore than 100 to 200 models so it is\nmore than 100 to 200 models so it is very very difficult that it will be a\nvery very difficult that it will be a\nvery very difficult that it will be a tie who are repeating questions they\ntie who are repeating questions they\ntie who are repeating questions they will be put up in time out so what if\nwill be put up in time out so what if\nwill be put up in time out so what if you're saying that if the 50% of model\nyou're saying that if the 50% of model\nyou're saying that if the 50% of model says yes 50% of our models says no\nsays yes 50% of our models says no\nsays yes 50% of our models says no always understand guys we will be having\nalways understand guys we will be having\nalways understand guys we will be having more than 100 to 200 plus models so in\nmore than 100 to 200 plus models so in\nmore than 100 to 200 plus models so in this particular case there will be high\nthis particular case there will be high\nthis particular case there will be high probability that always there will be a\nprobability that always there will be a\nprobability that always there will be a majority voting available it will always\nmajority voting available it will always\nmajority voting available it will always not be in that specific scenario so this\nnot be in that specific scenario so this\nnot be in that specific scenario so this was the concept about bagging now some\nwas the concept about bagging now some\nwas the concept about bagging now some people will be saying that Krish why are\npeople will be saying that Krish why are\npeople will be saying that Krish why are you using different different models\nyou using different different models\nyou using different different models guys I'm not discussing about random\nguys I'm not discussing about random\nguys I'm not discussing about random Forest over here random Forest uses only\nForest over here random Forest uses only\nForest over here random Forest uses only one type of model that is decision tree\none type of model that is decision tree\none type of model that is decision tree but if we think as an concept of bagging\nbut if we think as an concept of bagging\nbut if we think as an concept of bagging you can have different different models\nyou can have different different models\nyou can have different different models over here and you can basically combine\nover here and you can basically combine\nover here and you can basically combine them so this is a technique of emble\nthem so this is a technique of emble\nthem so this is a technique of emble techniques and this is basically called\ntechniques and this is basically called\ntechniques and this is basically called as bagging okay now tell me one point I\nas bagging okay now tell me one point I\nas bagging okay now tell me one point I missed out fine this is with respect to\nmissed out fine this is with respect to\nmissed out fine this is with respect to the classification problem with respect\nthe classification problem with respect\nthe classification problem with respect to the regression problem what will\nto the regression problem what will\nto the regression problem what will happen in case of a regression problem\nhappen in case of a regression problem\nhappen in case of a regression problem let's say that I got here 120 here 140\nlet's say that I got here 120 here 140\nlet's say that I got here 120 here 140 here 122 here 148 as my output so in\nhere 122 here 148 as my output so in\nhere 122 here 148 as my output so in regression what will happen is that the\nregression what will happen is that the\nregression what will happen is that the entire mean will be taken mean will be\nentire mean will be taken mean will be\nentire mean will be taken mean will be taken the output mean will be basically\ntaken the output mean will be basically\ntaken the output mean will be basically taken and that will be your output of\ntaken and that will be your output of\ntaken and that will be your output of the model average or mean very simple\nthe model average or mean very simple\nthe model average or mean very simple right so average or mean will be\nright so average or mean will be\nright so average or mean will be basically taken up and here based on the\nbasically taken up and here based on the\nbasically taken up and here based on the average you'll be able to solve the\naverage you'll be able to solve the\naverage you'll be able to solve the regression problem great now let's go\nregression problem great now let's go\nregression problem great now let's go ahead and try to understand with respect\nahead and try to understand with respect\nahead and try to understand with respect to bagging and boosting how many\nto bagging and boosting how many\nto bagging and boosting how many different types of algorithm are but\ndifferent types of algorithm are but\ndifferent types of algorithm are but before that I need to make you\nbefore that I need to make you\nbefore that I need to make you understand what exactly is boosting now\nunderstand what exactly is boosting now\nunderstand what exactly is boosting now here in bagging you have seen that you\nhere in bagging you have seen that you\nhere in bagging you have seen that you have parallel models right one one one\nhave parallel models right one one one\nhave parallel models right one one one independent you have parallel models\nindependent you have parallel models\nindependent you have parallel models you're giving some row samples in\nyou're giving some row samples in\nyou're giving some row samples in different different models and basically\ndifferent different models and basically\ndifferent different models and basically are able to find out the output now in\nare able to find out the output now in\nare able to find out the output now in case of boosting boosting is a\ncase of boosting boosting is a\ncase of boosting boosting is a sequential combination of models like\nsequential combination of models like\nsequential combination of models like this you have lot of sequential models\nthis you have lot of sequential models\nthis you have lot of sequential models like this and one after the model like\nlike this and one after the model like\nlike this and one after the model like first I'll give my training data to this\nfirst I'll give my training data to this\nfirst I'll give my training data to this particular model then it will go to this\nparticular model then it will go to this\nparticular model then it will go to this data then this model then this model so\ndata then this model then this model so\ndata then this model then this model so this will be my M1 M2 M3 M4 and finally\nthis will be my M1 M2 M3 M4 and finally\nthis will be my M1 M2 M3 M4 and finally I will be getting my output so here you\nI will be getting my output so here you\nI will be getting my output so here you can basically say that boosting is all\ncan basically say that boosting is all\ncan basically say that boosting is all about and this M1 M2 M3 we basically\nabout and this M1 M2 M3 we basically\nabout and this M1 M2 M3 we basically mention it as weak Learners so this will\nmention it as weak Learners so this will\nmention it as weak Learners so this will be weak learner weak learner weak\nbe weak learner weak learner weak\nbe weak learner weak learner weak learner weak learner and finally when we\nlearner weak learner and finally when we\nlearner weak learner and finally when we go till here it it'll if I combine all\ngo till here it it'll if I combine all\ngo till here it it'll if I combine all these weak ners weak\nthese weak ners weak\nthese weak ners weak learner weak learner okay once I combine\nlearner weak learner okay once I combine\nlearner weak learner okay once I combine all this weak learner it becomes a it\nall this weak learner it becomes a it\nall this weak learner it becomes a it becomes a strong learner finally if I\nbecomes a strong learner finally if I\nbecomes a strong learner finally if I try to combine this this will basically\ntry to combine this this will basically\ntry to combine this this will basically become a strong learner so here you have\nbecome a strong learner so here you have\nbecome a strong learner so here you have all the models sequentially one after\nall the models sequentially one after\nall the models sequentially one after the other and then you will probably try\nthe other and then you will probably try\nthe other and then you will probably try to provide your uh input from one model\nto provide your uh input from one model\nto provide your uh input from one model to the next model to the next model and\nto the next model to the next model and\nto the next model to the next model and these all models will be a very simpler\nthese all models will be a very simpler\nthese all models will be a very simpler weak learner model which will not be\nweak learner model which will not be\nweak learner model which will not be able to predict properly but when you\nable to predict properly but when you\nable to predict properly but when you combine all this particular models\ncombine all this particular models\ncombine all this particular models together sequentially it becomes a\ntogether sequentially it becomes a\ntogether sequentially it becomes a strong learner how this specifically\nstrong learner how this specifically\nstrong learner how this specifically works I'll take an example example of AD\nworks I'll take an example example of AD\nworks I'll take an example example of AD boost XG boost I will show you that okay\nboost XG boost I will show you that okay\nboost XG boost I will show you that okay week learner basically means the\nweek learner basically means the\nweek learner basically means the prediction is very bad but as you go\nprediction is very bad but as you go\nprediction is very bad but as you go sequentially you combine them they\nsequentially you combine them they\nsequentially you combine them they become a strong learner okay one example\nbecome a strong learner okay one example\nbecome a strong learner okay one example I want to give you let's say that you\nI want to give you let's say that you\nI want to give you let's say that you are a data scientist right let's say\nare a data scientist right let's say\nare a data scientist right let's say that this model one may be a teacher\nthat this model one may be a teacher\nthat this model one may be a teacher with respect to physics then this model\nwith respect to physics then this model\nwith respect to physics then this model two may be a teacher with respect to\ntwo may be a teacher with respect to\ntwo may be a teacher with respect to chemistry let's say model 3 is basically\nchemistry let's say model 3 is basically\nchemistry let's say model 3 is basically a teacher of maths and model four is a\na teacher of maths and model four is a\na teacher of maths and model four is a teacher of geography now suppose if you\nteacher of geography now suppose if you\nteacher of geography now suppose if you are trying to solve one problem\nare trying to solve one problem\nare trying to solve one problem obviously if the physics teacher is not\nobviously if the physics teacher is not\nobviously if the physics teacher is not able to solve that particular problem\nable to solve that particular problem\nable to solve that particular problem then probably chemistry can help or\nthen probably chemistry can help or\nthen probably chemistry can help or maths can help or geography can help or\nmaths can help or geography can help or\nmaths can help or geography can help or someone can help so when we combine this\nsomeone can help so when we combine this\nsomeone can help so when we combine this many expertise together they will be\nmany expertise together they will be\nmany expertise together they will be able to give you the output in an\nable to give you the output in an\nable to give you the output in an efficient way Sumit I'll talk about it\nefficient way Sumit I'll talk about it\nefficient way Sumit I'll talk about it where whether all the features are\nwhere whether all the features are\nwhere whether all the features are basically passed to all the models or\nbasically passed to all the models or\nbasically passed to all the models or not I'll just talk about it just give me\nnot I'll just talk about it just give me\nnot I'll just talk about it just give me some time okay but I just want to give\nsome time okay but I just want to give\nsome time okay but I just want to give you an idea about in short if someone\nyou an idea about in short if someone\nyou an idea about in short if someone asks you in an interview what exactly is\nasks you in an interview what exactly is\nasks you in an interview what exactly is boosting okay boosting is you can just\nboosting okay boosting is you can just\nboosting okay boosting is you can just say that it is a sequential set of all\nsay that it is a sequential set of all\nsay that it is a sequential set of all the models combined together and these\nthe models combined together and these\nthe models combined together and these all models that I initialized are\nall models that I initialized are\nall models that I initialized are usually weak Learners and when they are\nusually weak Learners and when they are\nusually weak Learners and when they are combined together they become a strong\ncombined together they become a strong\ncombined together they become a strong learner and based on the strong learner\nlearner and based on the strong learner\nlearner and based on the strong learner they gives an amazing output and right\nthey gives an amazing output and right\nthey gives an amazing output and right now if I say in most of the kaggle\nnow if I say in most of the kaggle\nnow if I say in most of the kaggle competition they use different types of\ncompetition they use different types of\ncompetition they use different types of boosting or bagging technique so we have\nboosting or bagging technique so we have\nboosting or bagging technique so we have basically as I said\nbasically as I said\nbasically as I said bagging and boosting in bagging what\nbagging and boosting in bagging what\nbagging and boosting in bagging what kind of algorithm we specifically use we\nkind of algorithm we specifically use we\nkind of algorithm we specifically use we use something called as random forest\nuse something called as random forest\nuse something called as random forest classifier and the second model that we\nclassifier and the second model that we\nclassifier and the second model that we specifically use is something called as\nspecifically use is something called as\nspecifically use is something called as random\nrandom\nrandom Forest regress so we specifically use\nForest regress so we specifically use\nForest regress so we specifically use these two kind of models which I'm\nthese two kind of models which I'm\nthese two kind of models which I'm actually going to discuss right now\nactually going to discuss right now\nactually going to discuss right now after this and then in boosting we\nafter this and then in boosting we\nafter this and then in boosting we basically use techniques like ad boost\nbasically use techniques like ad boost\nbasically use techniques like ad boost gradi Boost number three is Extreme\ngradi Boost number three is Extreme\ngradi Boost number three is Extreme gradient boost which we also say it as\ngradient boost which we also say it as\ngradient boost which we also say it as XG boost extreme gradient boost so let's\nXG boost extreme gradient boost so let's\nXG boost extreme gradient boost so let's go ahead and let's discuss about the\ngo ahead and let's discuss about the\ngo ahead and let's discuss about the first algorithm which is called as\nfirst algorithm which is called as\nfirst algorithm which is called as random forest classifier and regressor\nrandom forest classifier and regressor\nrandom forest classifier and regressor now first thing first let's understand\nnow first thing first let's understand\nnow first thing first let's understand some things from the yesterday's class I\nsome things from the yesterday's class I\nsome things from the yesterday's class I hope uh what is the main problem with\nhope uh what is the main problem with\nhope uh what is the main problem with respect to decision tree whenever we\nrespect to decision tree whenever we\nrespect to decision tree whenever we create a decision tree without any\ncreate a decision tree without any\ncreate a decision tree without any hyperparameter it does it not lead to\nhyperparameter it does it not lead to\nhyperparameter it does it not lead to overit\noverit\noverit does it not lead to overfitting uh\ndoes it not lead to overfitting uh\ndoes it not lead to overfitting uh whenever you probably have a decision\nwhenever you probably have a decision\nwhenever you probably have a decision tree right it leads to something like\ntree right it leads to something like\ntree right it leads to something like overfitting why overfitting because it\noverfitting why overfitting because it\noverfitting why overfitting because it completely splits all the feature till\ncompletely splits all the feature till\ncompletely splits all the feature till it's complete depth overfitting\nit's complete depth overfitting\nit's complete depth overfitting basically means for training data the\nbasically means for training data the\nbasically means for training data the accuracy is high for test data the\naccuracy is high for test data the\naccuracy is high for test data the accuracy is low so training data when\naccuracy is low so training data when\naccuracy is low so training data when the accuracy is high I may basically say\nthe accuracy is high I may basically say\nthe accuracy is high I may basically say it as high bias and then I may basically\nit as high bias and then I may basically\nit as high bias and then I may basically say it as sorry not high bias low bias\nsay it as sorry not high bias low bias\nsay it as sorry not high bias low bias and high V variance so low bias and high\nand high V variance so low bias and high\nand high V variance so low bias and high variance yes obviously we can do pruning\nvariance yes obviously we can do pruning\nvariance yes obviously we can do pruning and all guys but again understand\nand all guys but again understand\nand all guys but again understand pruning is an extensive task probably if\npruning is an extensive task probably if\npruning is an extensive task probably if your if you have 100 features if you\nyour if you have 100 features if you\nyour if you have 100 features if you have data points which is like 1 million\nhave data points which is like 1 million\nhave data points which is like 1 million to do pruning also it is very much\nto do pruning also it is very much\nto do pruning also it is very much difficult yes pre pruning can be done\ndifficult yes pre pruning can be done\ndifficult yes pre pruning can be done but again we cannot confirm that it may\nbut again we cannot confirm that it may\nbut again we cannot confirm that it may work well or not so right now with\nwork well or not so right now with\nwork well or not so right now with respect to decision tree you have this\nrespect to decision tree you have this\nrespect to decision tree you have this specific problem that is low bias and\nspecific problem that is low bias and\nspecific problem that is low bias and high variance now in low Biance and high\nhigh variance now in low Biance and high\nhigh variance now in low Biance and high variance you know that my model is\nvariance you know that my model is\nvariance you know that my model is basically the generalized model that I\nbasically the generalized model that I\nbasically the generalized model that I should get it should have low bias and\nshould get it should have low bias and\nshould get it should have low bias and low variance so if somebody asks you why\nlow variance so if somebody asks you why\nlow variance so if somebody asks you why do you use random Forest you can\ndo you use random Forest you can\ndo you use random Forest you can basically explain about decision trees\nbasically explain about decision trees\nbasically explain about decision trees like this now my main aim is to convert\nlike this now my main aim is to convert\nlike this now my main aim is to convert this High variance to low variance now I\nthis High variance to low variance now I\nthis High variance to low variance now I will be able to convert this High\nwill be able to convert this High\nwill be able to convert this High variance to low variance using random\nvariance to low variance using random\nvariance to low variance using random forest classifier or random Forest\nforest classifier or random Forest\nforest classifier or random Forest regressor now what does random Forest do\nregressor now what does random Forest do\nregressor now what does random Forest do random Forest is a bagging technique\nrandom Forest is a bagging technique\nrandom Forest is a bagging technique similarly I have a data set over here\nsimilarly I have a data set over here\nsimilarly I have a data set over here let's say that I have this data set\nlet's say that I have this data set\nlet's say that I have this data set and then here I will be having multiple\nand then here I will be having multiple\nand then here I will be having multiple models like\nmodels like\nmodels like M1\nM1\nM1 M2\nM2\nM2 M3 M4 let's say I have this four models\nM3 M4 let's say I have this four models\nM3 M4 let's say I have this four models like this we have many many models now\nlike this we have many many models now\nlike this we have many many models now with respect to this models this models\nwith respect to this models this models\nwith respect to this models this models all the models are actually decision\nall the models are actually decision\nall the models are actually decision Tree in random forest all are decision\nTree in random forest all are decision\nTree in random forest all are decision trees you don't have a different model\ntrees you don't have a different model\ntrees you don't have a different model over there so over here you can see that\nover there so over here you can see that\nover there so over here you can see that all the models are decision trees that\nall the models are decision trees that\nall the models are decision trees that is going to get used used in random\nis going to get used used in random\nis going to get used used in random Forest so decision trees always gets\nForest so decision trees always gets\nForest so decision trees always gets used in random Forest the first thing\nused in random Forest the first thing\nused in random Forest the first thing that you should know now whenever we are\nthat you should know now whenever we are\nthat you should know now whenever we are using decision trees you know that\nusing decision trees you know that\nusing decision trees you know that decision tree if I by default if we try\ndecision tree if I by default if we try\ndecision tree if I by default if we try to create it it may lead to overfitting\nto create it it may lead to overfitting\nto create it it may lead to overfitting and because of that every decision tree\nand because of that every decision tree\nand because of that every decision tree will basically create low V low bias and\nwill basically create low V low bias and\nwill basically create low V low bias and high variance but if we combine in the\nhigh variance but if we combine in the\nhigh variance but if we combine in the form of bootstrap aggregator this High\nform of bootstrap aggregator this High\nform of bootstrap aggregator this High variance will be getting converted to\nvariance will be getting converted to\nvariance will be getting converted to low variance because why because\nlow variance because why because\nlow variance because why because majority of voting we will be taking\nmajority of voting we will be taking\nmajority of voting we will be taking from this particular decision trees like\nfrom this particular decision trees like\nfrom this particular decision trees like there will be many many decision tree so\nthere will be many many decision tree so\nthere will be many many decision tree so they lot of outputs will be coming and\nthey lot of outputs will be coming and\nthey lot of outputs will be coming and with the help of majority voting\nwith the help of majority voting\nwith the help of majority voting classifier this High variance will get\nclassifier this High variance will get\nclassifier this High variance will get converted to low variance now in random\nconverted to low variance now in random\nconverted to low variance now in random Forest how it works in the first case if\nForest how it works in the first case if\nForest how it works in the first case if I talk about random Forest over here two\nI talk about random Forest over here two\nI talk about random Forest over here two things basically happen with respect to\nthings basically happen with respect to\nthings basically happen with respect to the D- data set let's say in first model\nthe D- data set let's say in first model\nthe D- data set let's say in first model we do some kind of row\nwe do some kind of row\nwe do some kind of row sampling plus\nsampling plus\nsampling plus Feature Feature\nFeature Feature\nFeature Feature sampling that basically means we have to\nsampling that basically means we have to\nsampling that basically means we have to select some set of rows and some set of\nselect some set of rows and some set of\nselect some set of rows and some set of features and give it to M1 similarly you\nfeatures and give it to M1 similarly you\nfeatures and give it to M1 similarly you do row sampling and feature sampling and\ndo row sampling and feature sampling and\ndo row sampling and feature sampling and give it to M2 then you do row sampling\ngive it to M2 then you do row sampling\ngive it to M2 then you do row sampling and feature sampling you give it to M3\nand feature sampling you give it to M3\nand feature sampling you give it to M3 and then you do row sampling and feature\nand then you do row sampling and feature\nand then you do row sampling and feature sampling you give it to M4 now when you\nsampling you give it to M4 now when you\nsampling you give it to M4 now when you do this so what will happen\ndo this so what will happen\ndo this so what will happen independently you're giving some\nindependently you're giving some\nindependently you're giving some features along with some rows now there\nfeatures along with some rows now there\nfeatures along with some rows now there may be a situation that your features\nmay be a situation that your features\nmay be a situation that your features may also get repeated it may also get\nmay also get repeated it may also get\nmay also get repeated it may also get repeated your records or data points may\nrepeated your records or data points may\nrepeated your records or data points may also get repeated so when you are\nalso get repeated so when you are\nalso get repeated so when you are probably training your model with this\nprobably training your model with this\nprobably training your model with this specific data sets and specific features\nspecific data sets and specific features\nspecific data sets and specific features this model become expert in predicting\nthis model become expert in predicting\nthis model become expert in predicting something right as I said one example\nsomething right as I said one example\nsomething right as I said one example over here I'm giving a physics model\nover here I'm giving a physics model\nover here I'm giving a physics model some data I'm giving chemistry data\nsome data I'm giving chemistry data\nsome data I'm giving chemistry data chemistry model with some data similarly\nchemistry model with some data similarly\nchemistry model with some data similarly here I'm giving some information to some\nhere I'm giving some information to some\nhere I'm giving some information to some model so the model will be an expert\nmodel so the model will be an expert\nmodel so the model will be an expert with respect to that specific data So\nwith respect to that specific data So\nwith respect to that specific data So based on all this particular data\nbased on all this particular data\nbased on all this particular data whenever I get a new test data so what\nwhenever I get a new test data so what\nwhenever I get a new test data so what will happen suppose let's say that this\nwill happen suppose let's say that this\nwill happen suppose let's say that this this is a classification problem the M1\nthis is a classification problem the M1\nthis is a classification problem the M1 model will be predicting zero this will\nmodel will be predicting zero this will\nmodel will be predicting zero this will be predicting one this will be\nbe predicting one this will be\nbe predicting one this will be predicting zero and this will be\npredicting zero and this will be\npredicting zero and this will be predicting zero now in this particular\npredicting zero now in this particular\npredicting zero now in this particular case again the majority voting\ncase again the majority voting\ncase again the majority voting classifier or majority voting will\nclassifier or majority voting will\nclassifier or majority voting will happen in the case of classification\nhappen in the case of classification\nhappen in the case of classification problem and then here you will be\nproblem and then here you will be\nproblem and then here you will be specifically able to get the output as\nspecifically able to get the output as\nspecifically able to get the output as zero so I hope everybody is able to\nzero so I hope everybody is able to\nzero so I hope everybody is able to understand all the models over here are\nunderstand all the models over here are\nunderstand all the models over here are decision trees and based on that you\ndecision trees and based on that you\ndecision trees and based on that you will be doing see when in I interview\nwill be doing see when in I interview\nwill be doing see when in I interview should be very very uh things the things\nshould be very very uh things the things\nshould be very very uh things the things that I'm telling you over here is all\nthat I'm telling you over here is all\nthat I'm telling you over here is all all the points are very much important\nall the points are very much important\nall the points are very much important and similarly if you tell the\nand similarly if you tell the\nand similarly if you tell the interviewer definitely your interview is\ninterviewer definitely your interview is\ninterviewer definitely your interview is cracked in this kind of algorithm I've\ncracked in this kind of algorithm I've\ncracked in this kind of algorithm I've seen some of my students saying that\nseen some of my students saying that\nseen some of my students saying that okay uh Kish um when the interviewer\nokay uh Kish um when the interviewer\nokay uh Kish um when the interviewer asked me that which is my favorite\nasked me that which is my favorite\nasked me that which is my favorite algorithm I said random Forest I told\nalgorithm I said random Forest I told\nalgorithm I said random Forest I told why did you say like that because he\nwhy did you say like that because he\nwhy did you say like that because he said that because that person let me let\nsaid that because that person let me let\nsaid that because that person let me let him ask any questions in random Forest\nhim ask any questions in random Forest\nhim ask any questions in random Forest I'm very much confident about it and I'm\nI'm very much confident about it and I'm\nI'm very much confident about it and I'm also going to prove him you know\nalso going to prove him you know\nalso going to prove him you know why they are very very good so with this\nwhy they are very very good so with this\nwhy they are very very good so with this specific case here you can basically see\nspecific case here you can basically see\nspecific case here you can basically see that because of the overfitting\nthat because of the overfitting\nthat because of the overfitting condition of the decision tree you're\ncondition of the decision tree you're\ncondition of the decision tree you're combining multiple decision tree so that\ncombining multiple decision tree so that\ncombining multiple decision tree so that you get a generalized model which has\nyou get a generalized model which has\nyou get a generalized model which has low bias and low variance so I hope\nlow bias and low variance so I hope\nlow bias and low variance so I hope everybody is able to understand boost\neverybody is able to understand boost\neverybody is able to understand boost feature sampling basically means suppose\nfeature sampling basically means suppose\nfeature sampling basically means suppose if I have 1 2 3 four feature for the\nif I have 1 2 3 four feature for the\nif I have 1 2 3 four feature for the first model I may give two features for\nfirst model I may give two features for\nfirst model I may give two features for the second model I may get three\nthe second model I may get three\nthe second model I may get three features for the fourth model I may give\nfeatures for the fourth model I may give\nfeatures for the fourth model I may give four features or uh any one feature ALS\nfour features or uh any one feature ALS\nfour features or uh any one feature ALS I can give to a specific model so\nI can give to a specific model so\nI can give to a specific model so internally that random Forest it take\ninternally that random Forest it take\ninternally that random Forest it take carees of over here these things are\ncarees of over here these things are\ncarees of over here these things are there and this is how random Forest\nthere and this is how random Forest\nthere and this is how random Forest Works only the difference between random\nWorks only the difference between random\nWorks only the difference between random Forest classify and regression is that\nForest classify and regression is that\nForest classify and regression is that in regression again whatever output you\nin regression again whatever output you\nin regression again whatever output you are basically getting you basically do\nare basically getting you basically do\nare basically getting you basically do the mean that's it average you just do\nthe mean that's it average you just do\nthe mean that's it average you just do the average you'll be able to get the\nthe average you'll be able to get the\nthe average you'll be able to get the output based on all the models output\noutput based on all the models output\noutput based on all the models output that you are actually getting now let's\nthat you are actually getting now let's\nthat you are actually getting now let's talk about some of the important points\ntalk about some of the important points\ntalk about some of the important points in random Forest the first thing first\nin random Forest the first thing first\nin random Forest the first thing first question is that is normalization\nquestion is that is normalization\nquestion is that is normalization required in random Forest then the next\nrequired in random Forest then the next\nrequired in random Forest then the next question is that in KNN is normalization\nquestion is that in KNN is normalization\nquestion is that in KNN is normalization when I say normalization or\nwhen I say normalization or\nwhen I say normalization or standardization I I'll just talk about\nstandardization I I'll just talk about\nstandardization I I'll just talk about standardization is standardization is\nstandardization is standardization is\nstandardization is standardization is required so this will be my another\nrequired so this will be my another\nrequired so this will be my another question so is normalization required in\nquestion so is normalization required in\nquestion so is normalization required in random forest or decision tree you here\nrandom forest or decision tree you here\nrandom forest or decision tree you here you can also say it as decision tree is\nyou can also say it as decision tree is\nyou can also say it as decision tree is it required so for this the answer will\nit required so for this the answer will\nit required so for this the answer will be no because understand decision tree\nbe no because understand decision tree\nbe no because understand decision tree will basically do the splits if you Mini\nwill basically do the splits if you Mini\nwill basically do the splits if you Mini minimize the data also that split won't\nminimize the data also that split won't\nminimize the data also that split won't be that much important but if I talk\nbe that much important but if I talk\nbe that much important but if I talk about KNN whether standardization\nabout KNN whether standardization\nabout KNN whether standardization normalization required over here the\nnormalization required over here the\nnormalization required over here the answer is yes because here we use two\nanswer is yes because here we use two\nanswer is yes because here we use two things one is ukan distance and\nthings one is ukan distance and\nthings one is ukan distance and Manhattan distance because of this you\nManhattan distance because of this you\nManhattan distance because of this you definitely have to apply standardization\ndefinitely have to apply standardization\ndefinitely have to apply standardization so that the computation or distance\nso that the computation or distance\nso that the computation or distance becomes easy so this is one of the most\nbecomes easy so this is one of the most\nbecomes easy so this is one of the most common interview questions that is\ncommon interview questions that is\ncommon interview questions that is basically asked in random Forest coming\nbasically asked in random Forest coming\nbasically asked in random Forest coming to the third question is random Forest\nto the third question is random Forest\nto the third question is random Forest impacted by outlier\nimpacted by outlier\nimpacted by outlier over here the answer will be no just\nover here the answer will be no just\nover here the answer will be no just check it out outside basically means\ncheck it out outside basically means\ncheck it out outside basically means Google and check it out check it out in\nGoogle and check it out check it out in\nGoogle and check it out check it out in Google okay perfect so I hope I've\nGoogle okay perfect so I hope I've\nGoogle okay perfect so I hope I've covered most of the things in random\ncovered most of the things in random\ncovered most of the things in random Forest is random Forest impacted by\nForest is random Forest impacted by\nForest is random Forest impacted by outliers this is the third question is\noutliers this is the third question is\noutliers this is the third question is KNN impacted by\nKNN impacted by\nKNN impacted by outliers is this KNN algorithm impacted\noutliers is this KNN algorithm impacted\noutliers is this KNN algorithm impacted by outliers is KNN impacted Byers the\nby outliers is KNN impacted Byers the\nby outliers is KNN impacted Byers the answer is yes big yes perfect so so\nanswer is yes big yes perfect so so\nanswer is yes big yes perfect so so these all are the interview questions\nthese all are the interview questions\nthese all are the interview questions that needs to be covered now let's go\nthat needs to be covered now let's go\nthat needs to be covered now let's go ahead and discuss about adab boost now\nahead and discuss about adab boost now\nahead and discuss about adab boost now in bagging most of the time we\nin bagging most of the time we\nin bagging most of the time we specifically use random forest or you\nspecifically use random forest or you\nspecifically use random forest or you can also create custom bagging\ncan also create custom bagging\ncan also create custom bagging techniques custom bagging techniques\ntechniques custom bagging techniques\ntechniques custom bagging techniques means whatever algorithm you want use\nmeans whatever algorithm you want use\nmeans whatever algorithm you want use the combination of them and try to give\nthe combination of them and try to give\nthe combination of them and try to give the output this also you can do it\nthe output this also you can do it\nthe output this also you can do it manually with the help of hands okay\nmanually with the help of hands okay\nmanually with the help of hands okay guys so second thing uh we are going to\nguys so second thing uh we are going to\nguys so second thing uh we are going to discuss about is boosting technique in\ndiscuss about is boosting technique in\ndiscuss about is boosting technique in this\nthis\nthis the first thing that uh first algorithm\nthe first thing that uh first algorithm\nthe first thing that uh first algorithm that we are going to discuss about is\nthat we are going to discuss about is\nthat we are going to discuss about is adab Boost so adab boost we going to\nadab Boost so adab boost we going to\nadab Boost so adab boost we going to discuss about how does adab Boost uh\ndiscuss about how does adab Boost uh\ndiscuss about how does adab Boost uh work now let's solve uh the first\nwork now let's solve uh the first\nwork now let's solve uh the first boosting technique which is called as\nboosting technique which is called as\nboosting technique which is called as adab boost okay and uh this is a\nadab boost okay and uh this is a\nadab boost okay and uh this is a boosting technique um in the boosting\nboosting technique um in the boosting\nboosting technique um in the boosting technique you have heard that we have to\ntechnique you have heard that we have to\ntechnique you have heard that we have to basically solve in a sequential way this\nbasically solve in a sequential way this\nbasically solve in a sequential way this at least you know I know there is a lot\nat least you know I know there is a lot\nat least you know I know there is a lot of confusion within you all but we'll\nof confusion within you all but we'll\nof confusion within you all but we'll try to solve a problem let's say so\ntry to solve a problem let's say so\ntry to solve a problem let's say so suppose I have a data set which looks\nsuppose I have a data set which looks\nsuppose I have a data set which looks like this fub1 F2 F3 F4 so these are my\nlike this fub1 F2 F3 F4 so these are my\nlike this fub1 F2 F3 F4 so these are my features and probably these are my\nfeatures and probably these are my\nfeatures and probably these are my output okay so let's say that I'm having\noutput okay so let's say that I'm having\noutput okay so let's say that I'm having this features like this and this is my\nthis features like this and this is my\nthis features like this and this is my output like yes or no like this so let's\noutput like yes or no like this so let's\noutput like yes or no like this so let's say that how many records I have over\nsay that how many records I have over\nsay that how many records I have over here three\nhere three\nhere three 4 5 6 and one more is there 7 so this\n4 5 6 and one more is there 7 so this\n4 5 6 and one more is there 7 so this seven records are there now in adab\nseven records are there now in adab\nseven records are there now in adab boost the first thing is that\nboost the first thing is that\nboost the first thing is that specifically with adab Boost uh you\nspecifically with adab Boost uh you\nspecifically with adab Boost uh you really need to understand that what all\nreally need to understand that what all\nreally need to understand that what all things we can basically do how do we\nthings we can basically do how do we\nthings we can basically do how do we solve this classification problem that\nsolve this classification problem that\nsolve this classification problem that we are going to understand the first\nwe are going to understand the first\nwe are going to understand the first thing first is that we Define a weight\nthing first is that we Define a weight\nthing first is that we Define a weight and the weight is very much simple\nand the weight is very much simple\nand the weight is very much simple initially to all the records to all this\ninitially to all the records to all this\ninitially to all the records to all this input records we provide an equal weight\ninput records we provide an equal weight\ninput records we provide an equal weight now how do we provide an equal weight we\nnow how do we provide an equal weight we\nnow how do we provide an equal weight we just go and count how many number of\njust go and count how many number of\njust go and count how many number of records are there now in this particular\nrecords are there now in this particular\nrecords are there now in this particular case the total number of records are one\ncase the total number of records are one\ncase the total number of records are one 2 3 4 5 6 7 now every record I have to\n2 3 4 5 6 7 now every record I have to\n2 3 4 5 6 7 now every record I have to provide an equal weight that is between\nprovide an equal weight that is between\nprovide an equal weight that is between 0 to 1 so the overall sum should be one\n0 to 1 so the overall sum should be one\n0 to 1 so the overall sum should be one so in this particular case what I can do\nso in this particular case what I can do\nso in this particular case what I can do if I make 1X 7 1X 7 1X 7 to everyone\nif I make 1X 7 1X 7 1X 7 to everyone\nif I make 1X 7 1X 7 1X 7 to everyone this will definitely become\nthis will definitely become\nthis will definitely become a equal weights to all right and if I do\na equal weights to all right and if I do\na equal weights to all right and if I do the total sum it will obviously be one\nthe total sum it will obviously be one\nthe total sum it will obviously be one let's go to the next one now after this\nlet's go to the next one now after this\nlet's go to the next one now after this what do we do okay after this in adab\nwhat do we do okay after this in adab\nwhat do we do okay after this in adab the first thing that we do is that we\nthe first thing that we do is that we\nthe first thing that we do is that we take any of this feature how do you\ntake any of this feature how do you\ntake any of this feature how do you decide which feature to take whether we\ndecide which feature to take whether we\ndecide which feature to take whether we should go with F1 or whether we should\nshould go with F1 or whether we should\nshould go with F1 or whether we should go with FS2 or whether we should go with\ngo with FS2 or whether we should go with\ngo with FS2 or whether we should go with F3 this we can do it with the help of\nF3 this we can do it with the help of\nF3 this we can do it with the help of Information Gain and Information Gain\nInformation Gain and Information Gain\nInformation Gain and Information Gain and entropy or guinea right based on\nand entropy or guinea right based on\nand entropy or guinea right based on this we can definitely understand\nthis we can definitely understand\nthis we can definitely understand whether we should start making decision\nwhether we should start making decision\nwhether we should start making decision here also you specifically make decision\nhere also you specifically make decision\nhere also you specifically make decision trees so here what you do is that you\ntrees so here what you do is that you\ntrees so here what you do is that you probably have to determine by using\nprobably have to determine by using\nprobably have to determine by using which feature I have to start my\nwhich feature I have to start my\nwhich feature I have to start my decision tree so suppose out of all this\ndecision tree so suppose out of all this\ndecision tree so suppose out of all this feature one feature two feature three\nfeature one feature two feature three\nfeature one feature two feature three you have selected that okay the\nyou have selected that okay the\nyou have selected that okay the information gain and entropy of feature\ninformation gain and entropy of feature\ninformation gain and entropy of feature one is higher so I'm going to use\none is higher so I'm going to use\none is higher so I'm going to use feature one and probably divide this\nfeature one and probably divide this\nfeature one and probably divide this into decision trees now when I divide\ninto decision trees now when I divide\ninto decision trees now when I divide this into decision tree let's say that\nthis into decision tree let's say that\nthis into decision tree let's say that I'm dividing like this into decision\nI'm dividing like this into decision\nI'm dividing like this into decision tree this decision tree depth will be\ntree this decision tree depth will be\ntree this decision tree depth will be only one one depth and this depth since\nonly one one depth and this depth since\nonly one one depth and this depth since it has only one depth we basically call\nit has only one depth we basically call\nit has only one depth we basically call it as stumps so what we do over here\nit as stumps so what we do over here\nit as stumps so what we do over here specifically we will create a decision\nspecifically we will create a decision\nspecifically we will create a decision Tre by taking only one feature and we\nTre by taking only one feature and we\nTre by taking only one feature and we will only divide it to one level okay\nwill only divide it to one level okay\nwill only divide it to one level okay one level or one depth that's\none level or one depth that's\none level or one depth that's and this is specifically called as stump\nand this is specifically called as stump\nand this is specifically called as stump what we are going to do next is that\nwhat we are going to do next is that\nwhat we are going to do next is that from this particular stump okay the\nfrom this particular stump okay the\nfrom this particular stump okay the stump is basically getting created only\nstump is basically getting created only\nstump is basically getting created only one so that is adab Boost right we say\none so that is adab Boost right we say\none so that is adab Boost right we say it as weak Learners because this is weak\nit as weak Learners because this is weak\nit as weak Learners because this is weak learner weak learner why there is a\nlearner weak learner why there is a\nlearner weak learner why there is a reason we say this as weak learner so\nreason we say this as weak learner so\nreason we say this as weak learner so only weak learner so that is the first\nonly weak learner so that is the first\nonly weak learner so that is the first thing with respect to uh this particular\nthing with respect to uh this particular\nthing with respect to uh this particular adab boost so the first step is that\nadab boost so the first step is that\nadab boost so the first step is that this is a weak learner so for the weak\nthis is a weak learner so for the weak\nthis is a weak learner so for the weak learner we basically create a stump\nlearner we basically create a stump\nlearner we basically create a stump stump basically means one level decision\nstump basically means one level decision\nstump basically means one level decision tree that's it based on the information\ntree that's it based on the information\ntree that's it based on the information gain and entropy I have selected the\ngain and entropy I have selected the\ngain and entropy I have selected the feature and then I just made a decision\nfeature and then I just made a decision\nfeature and then I just made a decision tree with only one level why it is\ntree with only one level why it is\ntree with only one level why it is called as it is called as weak learner\ncalled as it is called as weak learner\ncalled as it is called as weak learner okay so that is the reason we use only\nokay so that is the reason we use only\nokay so that is the reason we use only stum that is just a one level decision\nstum that is just a one level decision\nstum that is just a one level decision tree now the next step happens is that\ntree now the next step happens is that\ntree now the next step happens is that we provide all the specific records to\nwe provide all the specific records to\nwe provide all the specific records to this F1 and we train this specific model\nthis F1 and we train this specific model\nthis F1 and we train this specific model only with one level decision tree we\nonly with one level decision tree we\nonly with one level decision tree we train them\ntrain them\ntrain them now after we train them let's say that\nnow after we train them let's say that\nnow after we train them let's say that we are going to pass all these\nwe are going to pass all these\nwe are going to pass all these particular records to find out how many\nparticular records to find out how many\nparticular records to find out how many are correct and how many are wrong this\nare correct and how many are wrong this\nare correct and how many are wrong this decision this decision tree is basically\ndecision this decision tree is basically\ndecision this decision tree is basically giving so let's say that out of this\ngiving so let's say that out of this\ngiving so let's say that out of this entire records one\nentire records one\nentire records one record one record was just given as\nrecord one record was just given as\nrecord one record was just given as wrong let's say that this is the this is\nwrong let's say that this is the this is\nwrong let's say that this is the this is the record which was given as wrong okay\nthe record which was given as wrong okay\nthe record which was given as wrong okay so let's say that this record output was\nso let's say that this record output was\nso let's say that this record output was predicted wrong from this particular\npredicted wrong from this particular\npredicted wrong from this particular model only one wrong was there after\nmodel only one wrong was there after\nmodel only one wrong was there after training the model now what we need to\ntraining the model now what we need to\ntraining the model now what we need to do in this specific case understand a\ndo in this specific case understand a\ndo in this specific case understand a very important thing so let's say that\nvery important thing so let's say that\nvery important thing so let's say that we have done this and probably after\nwe have done this and probably after\nwe have done this and probably after this what we are actually going to do we\nthis what we are actually going to do we\nthis what we are actually going to do we are going to calculate the total error\nare going to calculate the total error\nare going to calculate the total error so how many error this particular model\nso how many error this particular model\nso how many error this particular model made let's say that in this particular\nmade let's say that in this particular\nmade let's say that in this particular case only one was wrong so this was only\ncase only one was wrong so this was only\ncase only one was wrong so this was only wrong right one was wrong so if I want\nwrong right one was wrong so if I want\nwrong right one was wrong so if I want to calculate the total error how will I\nto calculate the total error how will I\nto calculate the total error how will I calculate how many how many of them are\ncalculate how many how many of them are\ncalculate how many how many of them are wrong how many of them are wrong only\nwrong how many of them are wrong only\nwrong how many of them are wrong only one is wrong what is the weight of this\none is wrong what is the weight of this\none is wrong what is the weight of this so I will go and write 1X 7 so this is\nso I will go and write 1X 7 so this is\nso I will go and write 1X 7 so this is specifically my total error out of this\nspecifically my total error out of this\nspecifically my total error out of this specific model which is my stump over\nspecific model which is my stump over\nspecific model which is my stump over here okay which is my F1 stop now this\nhere okay which is my F1 stop now this\nhere okay which is my F1 stop now this is my first\nis my first\nis my first step the second step is that I need to\nstep the second step is that I need to\nstep the second step is that I need to see the performance of stump which stump\nsee the performance of stump which stump\nsee the performance of stump which stump this specific stump and the performance\nthis specific stump and the performance\nthis specific stump and the performance is basically checked by a formula which\nis basically checked by a formula which\nis basically checked by a formula which is 1 by log e 1us total error divided\nis 1 by log e 1us total error divided\nis 1 by log e 1us total error divided total error why we are doing this\ntotal error why we are doing this\ntotal error why we are doing this everything will make sense okay in just\neverything will make sense okay in just\neverything will make sense okay in just time every every in just a small time\ntime every every in just a small time\ntime every every in just a small time everything will make sense the first\neverything will make sense the first\neverything will make sense the first step that we do in adaab boost is that\nstep that we do in adaab boost is that\nstep that we do in adaab boost is that we try to find out the total error the\nwe try to find out the total error the\nwe try to find out the total error the second step we try to find out the\nsecond step we try to find out the\nsecond step we try to find out the performance of stump now in this\nperformance of stump now in this\nperformance of stump now in this particular case it will be 1 by log e 1\nparticular case it will be 1 by log e 1\nparticular case it will be 1 by log e 1 - 1 by 7 / 1X 7 so once I calculate it\n- 1 by 7 / 1X 7 so once I calculate it\n- 1 by 7 / 1X 7 so once I calculate it it will be coming as\nit will be coming as\nit will be coming as 895 F2 and F3 see again understand out\n895 F2 and F3 see again understand out\n895 F2 and F3 see again understand out of all these features I found out from\nof all these features I found out from\nof all these features I found out from Information Gain and entropy that this\nInformation Gain and entropy that this\nInformation Gain and entropy that this is the best feature let's say that I\nis the best feature let's say that I\nis the best feature let's say that I have calculated this\nhave calculated this\nhave calculated this as895 so this is my second step the\nas895 so this is my second step the\nas895 so this is my second step the first step is find out the total error\nfirst step is find out the total error\nfirst step is find out the total error the second step is performance of stum\nthe second step is performance of stum\nthe second step is performance of stum what is te te basically means total\nwhat is te te basically means total\nwhat is te te basically means total error te basically means total error now\nerror te basically means total error now\nerror te basically means total error now see see the steps okay see the steps\nsee see the steps okay see the steps\nsee see the steps okay see the steps whenever I'm discussing about boosting\nwhenever I'm discussing about boosting\nwhenever I'm discussing about boosting I'm going to combine weak Learners\nI'm going to combine weak Learners\nI'm going to combine weak Learners together to get a strong learner now\ntogether to get a strong learner now\ntogether to get a strong learner now what is the next step out of this now\nwhat is the next step out of this now\nwhat is the next step out of this now what what will be my third step\nwhat what will be my third step\nwhat what will be my third step understand over here my third step will\nunderstand over here my third step will\nunderstand over here my third step will be to update all these weights and that\nbe to update all these weights and that\nbe to update all these weights and that is the reason why I'm calculating this\nis the reason why I'm calculating this\nis the reason why I'm calculating this total error and performance of Step so\ntotal error and performance of Step so\ntotal error and performance of Step so my third step will basically be new\nmy third step will basically be new\nmy third step will basically be new sample weight from the decision tree one\nsample weight from the decision tree one\nsample weight from the decision tree one which is my stump so I'll say new sample\nwhich is my stump so I'll say new sample\nwhich is my stump so I'll say new sample weight is equal to I need to update all\nweight is equal to I need to update all\nweight is equal to I need to update all these weights why I need to update all\nthese weights why I need to update all\nthese weights why I need to update all these weights again understand I'll I'll\nthese weights again understand I'll I'll\nthese weights again understand I'll I'll talk about it just a second so if I want\ntalk about it just a second so if I want\ntalk about it just a second so if I want to up update the sample weights first\nto up update the sample weights first\nto up update the sample weights first update I will do it for correct records\nupdate I will do it for correct records\nupdate I will do it for correct records see for correct records whichever are\nsee for correct records whichever are\nsee for correct records whichever are correct like these all records are\ncorrect like these all records are\ncorrect like these all records are correct these all records are correct\ncorrect these all records are correct\ncorrect these all records are correct now when I update the weights of this\nnow when I update the weights of this\nnow when I update the weights of this update the weights of this particular\nupdate the weights of this particular\nupdate the weights of this particular record it should reduce and when the the\nrecord it should reduce and when the the\nrecord it should reduce and when the the the wrong records that I have this\nthe wrong records that I have this\nthe wrong records that I have this update should increase why because\nupdate should increase why because\nupdate should increase why because because if I increase this weights then\nbecause if I increase this weights then\nbecause if I increase this weights then the wrong records that are there that\nthe wrong records that are there that\nthe wrong records that are there that record should go to the next week\nrecord should go to the next week\nrecord should go to the next week learner that is the reason why I'm doing\nlearner that is the reason why I'm doing\nlearner that is the reason why I'm doing it now how to update this particular\nit now how to update this particular\nit now how to update this particular weights for correct records for correct\nweights for correct records for correct\nweights for correct records for correct records the formula looks something like\nrecords the formula looks something like\nrecords the formula looks something like this weight multiplied by weight\nthis weight multiplied by weight\nthis weight multiplied by weight multiplied by E to the^ of\nmultiplied by E to the^ of\nmultiplied by E to the^ of minus this specific performance okay\nminus this specific performance okay\nminus this specific performance okay this specific performance so e to the\nthis specific performance so e to the\nthis specific performance so e to the power of PS I'll write performance of\npower of PS I'll write performance of\npower of PS I'll write performance of stump and then I will basically be able\nstump and then I will basically be able\nstump and then I will basically be able to write 1X 7 * e to the^ of minus\nto write 1X 7 * e to the^ of minus\nto write 1X 7 * e to the^ of minus 895 if I do the calculation everybody\n895 if I do the calculation everybody\n895 if I do the calculation everybody try to do it the answer will be\ntry to do it the answer will be\ntry to do it the answer will be 05 now this is for correct records what\n05 now this is for correct records what\n05 now this is for correct records what about incorrect records for the\nabout incorrect records for the\nabout incorrect records for the incorrect\nincorrect\nincorrect records the the weights that is going to\nrecords the the weights that is going to\nrecords the the weights that is going to the formula that we going to apply is\nthe formula that we going to apply is\nthe formula that we going to apply is multiplied by E to the^ of plus PS not\nmultiplied by E to the^ of plus PS not\nmultiplied by E to the^ of plus PS not minus PS plus PS so here I'll write 1 by\nminus PS plus PS so here I'll write 1 by\nminus PS plus PS so here I'll write 1 by 7 multiplied e to the^ of\n7 multiplied e to the^ of\n7 multiplied e to the^ of 895 so if I go and probably calcul this\n895 so if I go and probably calcul this\n895 so if I go and probably calcul this I'm going to get it\nI'm going to get it\nI'm going to get it as 349 so this two are the weights that\nas 349 so this two are the weights that\nas 349 so this two are the weights that I have got that basically means all\nI have got that basically means all\nI have got that basically means all these records now which are correct 1X 7\nthese records now which are correct 1X 7\nthese records now which are correct 1X 7 the new updated weights will be 05 05\nthe new updated weights will be 05 05\nthe new updated weights will be 05 05 05\n05\n05 05 sorry not for the wrong\n05 sorry not for the wrong\n05 sorry not for the wrong records then this will be 05 then 05 and\nrecords then this will be 05 then 05 and\nrecords then this will be 05 then 05 and 05 so let me just see what is 1x 7 so\n05 so let me just see what is 1x 7 so\n05 so let me just see what is 1x 7 so here you can see initially it was. 142\nhere you can see initially it was. 142\nhere you can see initially it was. 142 now it has got reduced to 05 because all\nnow it has got reduced to 05 because all\nnow it has got reduced to 05 because all these records are correct but the wrong\nthese records are correct but the wrong\nthese records are correct but the wrong record value is 349 so my weights will\nrecord value is 349 so my weights will\nrecord value is 349 so my weights will now become over here as 349 now I will\nnow become over here as 349 now I will\nnow become over here as 349 now I will just go and go ahead and write over here\njust go and go ahead and write over here\njust go and go ahead and write over here my new weight my new weight is nothing\nmy new weight my new weight is nothing\nmy new weight my new weight is nothing but 05\n055\n055 05 05 05 1 2 how many 1 2 3 okay fourth\n05 05 05 1 2 how many 1 2 3 okay fourth\n05 05 05 1 2 how many 1 2 3 okay fourth record is here fourth record is there 1\nrecord is here fourth record is there 1\nrecord is here fourth record is there 1 2 3 4 05 05 okay how many records are\n2 3 4 05 05 okay how many records are\n2 3 4 05 05 okay how many records are there 1 2 3 4 5 6 7 so my fourth record\nthere 1 2 3 4 5 6 7 so my fourth record\nthere 1 2 3 4 5 6 7 so my fourth record will basically become the new value that\nwill basically become the new value that\nwill basically become the new value that I'm having is something called as\nI'm having is something called as\nI'm having is something called as 349 now tell me guys if I do the\n349 now tell me guys if I do the\n349 now tell me guys if I do the summation of all these weights is this\nsummation of all these weights is this\nsummation of all these weights is this is it one so prob\nis it one so prob\nis it one so prob no I don't think so it is one because if\nno I don't think so it is one because if\nno I don't think so it is one because if I try to add it up it is not one but if\nI try to add it up it is not one but if\nI try to add it up it is not one but if I go and see over here these all are one\nI go and see over here these all are one\nI go and see over here these all are one if I combine all the things 1 2 3 4 5 6\nif I combine all the things 1 2 3 4 5 6\nif I combine all the things 1 2 3 4 5 6 7 these all are one so here I have need\n7 these all are one so here I have need\n7 these all are one so here I have need to find out my normalized weight now in\nto find out my normalized weight now in\nto find out my normalized weight now in order to find out the normalized\norder to find out the normalized\norder to find out the normalized weight all I have to do is that what I\nweight all I have to do is that what I\nweight all I have to do is that what I have to do because the entire sumission\nhave to do because the entire sumission\nhave to do because the entire sumission should be one so we have to\nshould be one so we have to\nshould be one so we have to normalize now in order to normalize all\nnormalize now in order to normalize all\nnormalize now in order to normalize all you have to do is that go and find out\nyou have to do is that go and find out\nyou have to do is that go and find out what is the sum of all this things the\nwhat is the sum of all this things the\nwhat is the sum of all this things the summation of all these things will be\nsummation of all these things will be\nsummation of all these things will be 0 649 all you have to do is that divide\n0 649 all you have to do is that divide\n0 649 all you have to do is that divide all the numbers\nall the numbers\nall the numbers by 649 divided by\nby 649 divided by\nby 649 divided by 649\n649\n649 649 like this divide all the numbers by\n649 like this divide all the numbers by\n649 like this divide all the numbers by 649 and tell me what will be the answer\n649 and tell me what will be the answer\n649 and tell me what will be the answer that you'll be getting so here your\nthat you'll be getting so here your\nthat you'll be getting so here your normalized weight will now look like\nnormalized weight will now look like\nnormalized weight will now look like 077 07 and this value will be somewhere\n077 07 and this value will be somewhere\n077 07 and this value will be somewhere around uh\naround uh\naround uh 537 I guess in this case then this will\n537 I guess in this case then this will\n537 I guess in this case then this will be 07\nbe 07\nbe 07 077 here we are going to divide by all\n077 here we are going to divide by all\n077 here we are going to divide by all this 64 649 now this is my normalized\nthis 64 649 now this is my normalized\nthis 64 649 now this is my normalized weight now after you get a normalized\nweight now after you get a normalized\nweight now after you get a normalized weight we will try to create something\nweight we will try to create something\nweight we will try to create something called as buckets because see one\ncalled as buckets because see one\ncalled as buckets because see one decision tree we have already created\ndecision tree we have already created\ndecision tree we have already created which is a stump and you know from this\nwhich is a stump and you know from this\nwhich is a stump and you know from this particular stum what you're going to get\nparticular stum what you're going to get\nparticular stum what you're going to get okay as an output then in the sequential\nokay as an output then in the sequential\nokay as an output then in the sequential model we will go and combine another\nmodel we will go and combine another\nmodel we will go and combine another model over here now it's the time that I\nmodel over here now it's the time that I\nmodel over here now it's the time that I have to create this specific model now\nhave to create this specific model now\nhave to create this specific model now in order to create this specific model I\nin order to create this specific model I\nin order to create this specific model I need to provide some specific rows only\nneed to provide some specific rows only\nneed to provide some specific rows only to this model to train because this\nto this model to train because this\nto this model to train because this model is giving one wrong now what I\nmodel is giving one wrong now what I\nmodel is giving one wrong now what I have to do is that whatever is wrong\nhave to do is that whatever is wrong\nhave to do is that whatever is wrong along with other data points I need to\nalong with other data points I need to\nalong with other data points I need to provide this specific model with those\nprovide this specific model with those\nprovide this specific model with those records so that this model will be able\nrecords so that this model will be able\nrecords so that this model will be able to train on this and probably be able to\nto train on this and probably be able to\nto train on this and probably be able to get the output now let's create buckets\nget the output now let's create buckets\nget the output now let's create buckets now based on buckets how the buckets\nnow based on buckets how the buckets\nnow based on buckets how the buckets will be created over here I will take 07\nwill be created over here I will take 07\nwill be created over here I will take 07 until\nuntil\nuntil sorry whatever is the value over here\nsorry whatever is the value over here\nsorry whatever is the value over here normal we value okay so I will start\nnormal we value okay so I will start\nnormal we value okay so I will start creating my buckets buckets basically\ncreating my buckets buckets basically\ncreating my buckets buckets basically from 0 to\nfrom 0 to\nfrom 0 to 07 what did I say now for this decision\n07 what did I say now for this decision\n07 what did I say now for this decision tree or stump I need to provide some\ntree or stump I need to provide some\ntree or stump I need to provide some records so the maximum number of record\nrecords so the maximum number of record\nrecords so the maximum number of record that should be going should be the wrong\nthat should be going should be the wrong\nthat should be going should be the wrong records that should go over here now how\nrecords that should go over here now how\nrecords that should go over here now how do we decide that okay there should be a\ndo we decide that okay there should be a\ndo we decide that okay there should be a way that we should be able to say that\nway that we should be able to say that\nway that we should be able to say that that specific wrong number of Records\nthat specific wrong number of Records\nthat specific wrong number of Records should go to that decision tree so for\nshould go to that decision tree so for\nshould go to that decision tree so for that purpose what we do is that this\nthat purpose what we do is that this\nthat purpose what we do is that this decision tree will randomly create some\ndecision tree will randomly create some\ndecision tree will randomly create some numbers between 0 to 1 randomly create\nnumbers between 0 to 1 randomly create\nnumbers between 0 to 1 randomly create those numbers between 0 to 1 and\nthose numbers between 0 to 1 and\nthose numbers between 0 to 1 and whichever bucket it will come in like 07\nwhichever bucket it will come in like 07\nwhichever bucket it will come in like 07 to 014 014 to 07 basically means 0 2 1\nto 014 014 to 07 basically means 0 2 1\nto 014 014 to 07 basically means 0 2 1 then 0 2 1 2 see how the bucket is\nthen 0 2 1 2 see how the bucket is\nthen 0 2 1 2 see how the bucket is getting cre this value is getting added\ngetting cre this value is getting added\ngetting cre this value is getting added to this so that becomes this bucket 021\nto this so that becomes this bucket 021\nto this so that becomes this bucket 021 +3 537 how much it is it is nothing but\n+3 537 how much it is it is nothing but\n+3 537 how much it is it is nothing but 470 747 then 747\n470 747 then 747\n470 747 then 747 to\nto\nto 751 like this you create all the buckets\n751 like this you create all the buckets\n751 like this you create all the buckets okay you can create all the buckets now\nokay you can create all the buckets now\nokay you can create all the buckets now tell me which record is basically having\ntell me which record is basically having\ntell me which record is basically having the biggest bucket size obviously this\nthe biggest bucket size obviously this\nthe biggest bucket size obviously this record so if I randomly create a number\nrecord so if I randomly create a number\nrecord so if I randomly create a number between 0 to one what is the highest\nbetween 0 to one what is the highest\nbetween 0 to one what is the highest probability that the values will be\nprobability that the values will be\nprobability that the values will be going in so in this particular case most\ngoing in so in this particular case most\ngoing in so in this particular case most of the wrong records will be passed\nof the wrong records will be passed\nof the wrong records will be passed along with the other records obviously\nalong with the other records obviously\nalong with the other records obviously other records there are chances that\nother records there are chances that\nother records there are chances that other records will go to the next\nother records will go to the next\nother records will go to the next decision tree but understand maximum\ndecision tree but understand maximum\ndecision tree but understand maximum number will go with the wrong records\nnumber will go with the wrong records\nnumber will go with the wrong records because the bucket is high over here so\nbecause the bucket is high over here so\nbecause the bucket is high over here so the bucket is high over here so most of\nthe bucket is high over here so most of\nthe bucket is high over here so most of the time this specific record will get\nthe time this specific record will get\nthe time this specific record will get create selected and then it will be gone\ncreate selected and then it will be gone\ncreate selected and then it will be gone to the second tree now suppose I have\nto the second tree now suppose I have\nto the second tree now suppose I have this all records\nthis all records\nthis all records so this is my first stump this is my\nso this is my first stump this is my\nso this is my first stump this is my second stump this is my third stump\nsecond stump this is my third stump\nsecond stump this is my third stump similarly the third stump from the\nsimilarly the third stump from the\nsimilarly the third stump from the second stump whichever wrong records\nsecond stump whichever wrong records\nsecond stump whichever wrong records will be going maximum number of Records\nwill be going maximum number of Records\nwill be going maximum number of Records will go over here then again it will be\nwill go over here then again it will be\nwill go over here then again it will be trained like this we'll be having lot of\ntrained like this we'll be having lot of\ntrained like this we'll be having lot of stumps minimum 100 decision trees can be\nstumps minimum 100 decision trees can be\nstumps minimum 100 decision trees can be added you know that every decision tree\nadded you know that every decision tree\nadded you know that every decision tree will give one output for a new test data\nwill give one output for a new test data\nwill give one output for a new test data new test data this week learner will\nnew test data this week learner will\nnew test data this week learner will give one output this week learner will\ngive one output this week learner will\ngive one output this week learner will give one output this week learner and\ngive one output this week learner and\ngive one output this week learner and this will week learner will be giving\nthis will week learner will be giving\nthis will week learner will be giving one output obviously the time complexity\none output obviously the time complexity\none output obviously the time complexity will be more now from this particular\nwill be more now from this particular\nwill be more now from this particular output suppose it is a binary\noutput suppose it is a binary\noutput suppose it is a binary classification I will be getting 0 1 1 1\nclassification I will be getting 0 1 1 1\nclassification I will be getting 0 1 1 1 so again over here majority voting will\nso again over here majority voting will\nso again over here majority voting will happen and the output will be one in\nhappen and the output will be one in\nhappen and the output will be one in case of regression problem I will be\ncase of regression problem I will be\ncase of regression problem I will be having a continuous value over here and\nhaving a continuous value over here and\nhaving a continuous value over here and for this the average average will be\nfor this the average average will be\nfor this the average average will be computed and that will give me an output\ncomputed and that will give me an output\ncomputed and that will give me an output over here so for regression the average\nover here so for regression the average\nover here so for regression the average will be done for classification what\nwill be done for classification what\nwill be done for classification what will happen majority will be be\nwill happen majority will be be\nwill happen majority will be be happening so everywhere that same part\nhappening so everywhere that same part\nhappening so everywhere that same part will be going on buckets is very much\nwill be going on buckets is very much\nwill be going on buckets is very much simple guys buckets basically means\nsimple guys buckets basically means\nsimple guys buckets basically means based on this weights normalized weight\nbased on this weights normalized weight\nbased on this weights normalized weight we are going to create bucket so that\nwe are going to create bucket so that\nwe are going to create bucket so that whichever records has the highest bucket\nwhichever records has the highest bucket\nwhichever records has the highest bucket based on this randomly creating code you\nbased on this randomly creating code you\nbased on this randomly creating code you know it will select those specific\nknow it will select those specific\nknow it will select those specific buckets and put it into random Forest\nbuckets and put it into random Forest\nbuckets and put it into random Forest understand why this bucket size is Big\nunderstand why this bucket size is Big\nunderstand why this bucket size is Big the other wrong records which are\nthe other wrong records which are\nthe other wrong records which are present right suppose they are have more\npresent right suppose they are have more\npresent right suppose they are have more than four to five wrong records their\nthan four to five wrong records their\nthan four to five wrong records their bucket size will also be bigger and\nbucket size will also be bigger and\nbucket size will also be bigger and because based on this randomly creating\nbecause based on this randomly creating\nbecause based on this randomly creating num between 0 to 1 most of the wrong\nnum between 0 to 1 most of the wrong\nnum between 0 to 1 most of the wrong records will be selected and given to\nrecords will be selected and given to\nrecords will be selected and given to the second stum similarly this\nthe second stum similarly this\nthe second stum similarly this particular decision tree will be doing\nparticular decision tree will be doing\nparticular decision tree will be doing some mistakes then that wrong records\nsome mistakes then that wrong records\nsome mistakes then that wrong records will get updated all the weights will\nwill get updated all the weights will\nwill get updated all the weights will get updated and it will be passed to the\nget updated and it will be passed to the\nget updated and it will be passed to the next decision tree guys when I say wrong\nnext decision tree guys when I say wrong\nnext decision tree guys when I say wrong record the output will be same only no\nrecord the output will be same only no\nrecord the output will be same only no zero and one so interesting everyone I\nzero and one so interesting everyone I\nzero and one so interesting everyone I hope you understood so much of maths in\nhope you understood so much of maths in\nhope you understood so much of maths in adab boost and how adab boost actually\nadab boost and how adab boost actually\nadab boost and how adab boost actually work three main things one is total\nwork three main things one is total\nwork three main things one is total error one is performance of stump and\nerror one is performance of stump and\nerror one is performance of stump and one is the new sample weight these\none is the new sample weight these\none is the new sample weight these things are getting calculated extensive\nthings are getting calculated extensive\nthings are getting calculated extensive max normalized weight was basically used\nmax normalized weight was basically used\nmax normalized weight was basically used because the sum of all these weights are\nbecause the sum of all these weights are\nbecause the sum of all these weights are approximately equal to one when boosting\napproximately equal to one when boosting\napproximately equal to one when boosting why not take the last output no no no we\nwhy not take the last output no no no we\nwhy not take the last output no no no we have to give the importance of every\nhave to give the importance of every\nhave to give the importance of every decision tree output every decision tree\ndecision tree output every decision tree\ndecision tree output every decision tree output are important okay let me talk\noutput are important okay let me talk\noutput are important okay let me talk about one model which is called as\nabout one model which is called as\nabout one model which is called as blackbox model versus white box what is\nblackbox model versus white box what is\nblackbox model versus white box what is the difference between blackbox model\nthe difference between blackbox model\nthe difference between blackbox model and white box if I take an example of\nand white box if I take an example of\nand white box if I take an example of linear regression tell me what kind of\nlinear regression tell me what kind of\nlinear regression tell me what kind of model it is is is it a white box model\nmodel it is is is it a white box model\nmodel it is is is it a white box model or black box if I take an example of\nor black box if I take an example of\nor black box if I take an example of random\nrandom\nrandom Forest is this a white box or black box\nForest is this a white box or black box\nForest is this a white box or black box if I take an example of decision tree it\nif I take an example of decision tree it\nif I take an example of decision tree it is a white box of blackbox model if I\nis a white box of blackbox model if I\nis a white box of blackbox model if I take an example of a Ann is it a white\ntake an example of a Ann is it a white\ntake an example of a Ann is it a white box of blackbox model linear regression\nbox of blackbox model linear regression\nbox of blackbox model linear regression is basically called as an wide Box model\nis basically called as an wide Box model\nis basically called as an wide Box model because here you can basically visualize\nbecause here you can basically visualize\nbecause here you can basically visualize how the Theta value is basically\nhow the Theta value is basically\nhow the Theta value is basically changing and how it is coming to a\nchanging and how it is coming to a\nchanging and how it is coming to a global Minima and all those things in\nglobal Minima and all those things in\nglobal Minima and all those things in random Forest I will say this as\nrandom Forest I will say this as\nrandom Forest I will say this as blackbox model because it is impossible\nblackbox model because it is impossible\nblackbox model because it is impossible to see all the decision tree how it is\nto see all the decision tree how it is\nto see all the decision tree how it is working so that is the reason the maths\nworking so that is the reason the maths\nworking so that is the reason the maths is so complex inside this if I talk\nis so complex inside this if I talk\nis so complex inside this if I talk about decision tree this is basically a\nabout decision tree this is basically a\nabout decision tree this is basically a white box model because in decision tree\nwhite box model because in decision tree\nwhite box model because in decision tree we know how the split are basically\nwe know how the split are basically\nwe know how the split are basically happening with the help of paper and pen\nhappening with the help of paper and pen\nhappening with the help of paper and pen you'll be able to do it in the case of\nyou'll be able to do it in the case of\nyou'll be able to do it in the case of an Ann this is a blackbox model because\nan Ann this is a blackbox model because\nan Ann this is a blackbox model because here you don't know like how many\nhere you don't know like how many\nhere you don't know like how many neurons are there how they are\nneurons are there how they are\nneurons are there how they are performing and how the weights are\nperforming and how the weights are\nperforming and how the weights are getting updated so this is the basic\ngetting updated so this is the basic\ngetting updated so this is the basic difference between the blackbox and uh\ndifference between the blackbox and uh\ndifference between the blackbox and uh uh white box model this entire thing is\nuh white box model this entire thing is\nuh white box model this entire thing is the agenda of today's session so let's\nthe agenda of today's session so let's\nthe agenda of today's session so let's start uh the first algorithm that we are\nstart uh the first algorithm that we are\nstart uh the first algorithm that we are probably going to discuss today is\nprobably going to discuss today is\nprobably going to discuss today is something called as K\nsomething called as K\nsomething called as K means\nmeans\nmeans clustering K means clustering and this\nclustering K means clustering and this\nclustering K means clustering and this is a kind of unsupervised machine\nis a kind of unsupervised machine\nis a kind of unsupervised machine learning now always remember\nlearning now always remember\nlearning now always remember unsupervised machine learning basically\nunsupervised machine learning basically\nunsupervised machine learning basically means that uh the one and the most\nmeans that uh the one and the most\nmeans that uh the one and the most important thing is that in unsupervised\nimportant thing is that in unsupervised\nimportant thing is that in unsupervised machine learning\nmachine learning\nmachine learning in unsupervised ml you don't have any\nin unsupervised ml you don't have any\nin unsupervised ml you don't have any specific output so you don't have any\nspecific output so you don't have any\nspecific output so you don't have any specific output so suppose you have\nspecific output so suppose you have\nspecific output so suppose you have feature one and feature two and suppose\nfeature one and feature two and suppose\nfeature one and feature two and suppose you have datas different different data\nyou have datas different different data\nyou have datas different different data you know and based on this data what we\nyou know and based on this data what we\nyou know and based on this data what we do we basically try to create clusters\ndo we basically try to create clusters\ndo we basically try to create clusters this clusters basically says what are\nthis clusters basically says what are\nthis clusters basically says what are the similar kind of data so this is what\nthe similar kind of data so this is what\nthe similar kind of data so this is what we basically do from uh clustering and\nwe basically do from uh clustering and\nwe basically do from uh clustering and there are various techniques like K\nthere are various techniques like K\nthere are various techniques like K Mains uh it is hierle clustering and all\nMains uh it is hierle clustering and all\nMains uh it is hierle clustering and all so first of all we'll try to understand\nso first of all we'll try to understand\nso first of all we'll try to understand about K means and how does it\nabout K means and how does it\nabout K means and how does it specifically work it's simple uh suppose\nspecifically work it's simple uh suppose\nspecifically work it's simple uh suppose you have a data points like this okay\nyou have a data points like this okay\nyou have a data points like this okay let's say that this is your F1 feature\nlet's say that this is your F1 feature\nlet's say that this is your F1 feature F2 feature and based on this in two\nF2 feature and based on this in two\nF2 feature and based on this in two dimensional probably I will be plotting\ndimensional probably I will be plotting\ndimensional probably I will be plotting this points and suppose this is my\nthis points and suppose this is my\nthis points and suppose this is my another points so our main purpose is\nanother points so our main purpose is\nanother points so our main purpose is basically to Cluster together in\nbasically to Cluster together in\nbasically to Cluster together in different different groups okay so this\ndifferent different groups okay so this\ndifferent different groups okay so this will be my one group and probably the\nwill be my one group and probably the\nwill be my one group and probably the other group will be this group right so\nother group will be this group right so\nother group will be this group right so two groups because obviously you can see\ntwo groups because obviously you can see\ntwo groups because obviously you can see from this clusters here you have two\nfrom this clusters here you have two\nfrom this clusters here you have two similar kind of data which is basically\nsimilar kind of data which is basically\nsimilar kind of data which is basically grouped together right this is my\ngrouped together right this is my\ngrouped together right this is my cluster one and this is my cluster 2 let\ncluster one and this is my cluster 2 let\ncluster one and this is my cluster 2 let me talk about this and why specifically\nme talk about this and why specifically\nme talk about this and why specifically it'll be very much useful then we'll try\nit'll be very much useful then we'll try\nit'll be very much useful then we'll try to understand about math intuition also\nto understand about math intuition also\nto understand about math intuition also now always understand guys uh where does\nnow always understand guys uh where does\nnow always understand guys uh where does clustering gets used okay in most of the\nclustering gets used okay in most of the\nclustering gets used okay in most of the Ensemble techniques I told you about\nEnsemble techniques I told you about\nEnsemble techniques I told you about custom emble technique right so custom\ncustom emble technique right so custom\ncustom emble technique right so custom emble techniques in custom assemble\nemble techniques in custom assemble\nemble techniques in custom assemble techniques you know whenever we are\ntechniques you know whenever we are\ntechniques you know whenever we are probably creating a model first of all\nprobably creating a model first of all\nprobably creating a model first of all on our data set what we do is that we\non our data set what we do is that we\non our data set what we do is that we create clusters so suppose this is my\ncreate clusters so suppose this is my\ncreate clusters so suppose this is my data set during my model creation the\ndata set during my model creation the\ndata set during my model creation the first algorithm we will probably apply\nfirst algorithm we will probably apply\nfirst algorithm we will probably apply will be clustering algorithm and after\nwill be clustering algorithm and after\nwill be clustering algorithm and after that it is obviously good that we can\nthat it is obviously good that we can\nthat it is obviously good that we can apply regression or classification\napply regression or classification\napply regression or classification problem suppose in this clustering I\nproblem suppose in this clustering I\nproblem suppose in this clustering I have two or three groups let's say that\nhave two or three groups let's say that\nhave two or three groups let's say that I have two or three groups over here for\nI have two or three groups over here for\nI have two or three groups over here for each group we can apply a separate\neach group we can apply a separate\neach group we can apply a separate supervis machine learning algorithm if\nsupervis machine learning algorithm if\nsupervis machine learning algorithm if we know the specific output that we\nwe know the specific output that we\nwe know the specific output that we really want to take ahead I'll talk\nreally want to take ahead I'll talk\nreally want to take ahead I'll talk about this and uh give you some of the\nabout this and uh give you some of the\nabout this and uh give you some of the examples as I go ahead now let's go on\nexamples as I go ahead now let's go on\nexamples as I go ahead now let's go on go ahead and focus more on understanding\ngo ahead and focus more on understanding\ngo ahead and focus more on understanding how does kin's clustering algorithm work\nhow does kin's clustering algorithm work\nhow does kin's clustering algorithm work so let's go over here the word K means\nso let's go over here the word K means\nso let's go over here the word K means has this K value this K are nothing but\nhas this K value this K are nothing but\nhas this K value this K are nothing but this K basically means centroids K\nthis K basically means centroids K\nthis K basically means centroids K basically means centroids so suppose if\nbasically means centroids so suppose if\nbasically means centroids so suppose if I have a data set which looks like this\nI have a data set which looks like this\nI have a data set which looks like this let's say that this is my data set now\nlet's say that this is my data set now\nlet's say that this is my data set now over here just by seeing the data set\nover here just by seeing the data set\nover here just by seeing the data set what are the possible groups you think\nwhat are the possible groups you think\nwhat are the possible groups you think definitely you'll be saying K is equal\ndefinitely you'll be saying K is equal\ndefinitely you'll be saying K is equal to 2 So when you say k is equal to two\nto 2 So when you say k is equal to two\nto 2 So when you say k is equal to two that basically means you will be able to\nthat basically means you will be able to\nthat basically means you will be able to get two groups like this and each and\nget two groups like this and each and\nget two groups like this and each and every group will be having a centroid a\nevery group will be having a centroid a\nevery group will be having a centroid a centroid Point here also there will be a\ncentroid Point here also there will be a\ncentroid Point here also there will be a centroid point so this centroid will\ncentroid point so this centroid will\ncentroid point so this centroid will determine basically this is a separate\ndetermine basically this is a separate\ndetermine basically this is a separate group over here this is a separate group\ngroup over here this is a separate group\ngroup over here this is a separate group over here so over here here you can\nover here so over here here you can\nover here so over here here you can definitely say that fine this is two\ndefinitely say that fine this is two\ndefinitely say that fine this is two groups but but how do we come to a\ngroups but but how do we come to a\ngroups but but how do we come to a conclusion that there is only two groups\nconclusion that there is only two groups\nconclusion that there is only two groups okay we cannot just directly say that\nokay we cannot just directly say that\nokay we cannot just directly say that okay we'll try to just by seeing the\nokay we'll try to just by seeing the\nokay we'll try to just by seeing the data because your data will be having a\ndata because your data will be having a\ndata because your data will be having a high dimension data right right now I'm\nhigh dimension data right right now I'm\nhigh dimension data right right now I'm just showing your two Dimension data but\njust showing your two Dimension data but\njust showing your two Dimension data but for a high dimension data definitely\nfor a high dimension data definitely\nfor a high dimension data definitely you'll not be able to see the data\nyou'll not be able to see the data\nyou'll not be able to see the data points how it is plotted so how do you\npoints how it is plotted so how do you\npoints how it is plotted so how do you come to a conclusion that only two\ncome to a conclusion that only two\ncome to a conclusion that only two groups are there so for this there is\ngroups are there so for this there is\ngroups are there so for this there is some steps that we basically perform in\nsome steps that we basically perform in\nsome steps that we basically perform in K mins the first step is that we try\nK mins the first step is that we try\nK mins the first step is that we try with different K values we try with\nwith different K values we try with\nwith different K values we try with different K values and which is the\ndifferent K values and which is the\ndifferent K values and which is the suitable K value K is nothing but\nsuitable K value K is nothing but\nsuitable K value K is nothing but centroids okay it is nothing but\ncentroids okay it is nothing but\ncentroids okay it is nothing but centroids we try with different\ncentroids we try with different\ncentroids we try with different different centroids in this particular\ndifferent centroids in this particular\ndifferent centroids in this particular case let's say that I have this\ncase let's say that I have this\ncase let's say that I have this particular data point and I actually\nparticular data point and I actually\nparticular data point and I actually start with k is equal 1 or 2 or 3 any\nstart with k is equal 1 or 2 or 3 any\nstart with k is equal 1 or 2 or 3 any one you want let's say that I'm going to\none you want let's say that I'm going to\none you want let's say that I'm going to start with k is equal 2 how to come up\nstart with k is equal 2 how to come up\nstart with k is equal 2 how to come up with this K is equal to 2 as a perfect\nwith this K is equal to 2 as a perfect\nwith this K is equal to 2 as a perfect value that I'll talk about it we need to\nvalue that I'll talk about it we need to\nvalue that I'll talk about it we need to know there is a concept which is called\nknow there is a concept which is called\nknow there is a concept which is called as within cluster sum of square so when\nas within cluster sum of square so when\nas within cluster sum of square so when we try different K values let's say that\nwe try different K values let's say that\nwe try different K values let's say that for K is equal to 2 what will happen the\nfor K is equal to 2 what will happen the\nfor K is equal to 2 what will happen the first step we select a we try K values\nfirst step we select a we try K values\nfirst step we select a we try K values so let's say that we are considering K\nso let's say that we are considering K\nso let's say that we are considering K is equal to 2 the second step is that we\nis equal to 2 the second step is that we\nis equal to 2 the second step is that we initialize K number of centroids now in\ninitialize K number of centroids now in\ninitialize K number of centroids now in this particular case I know my K value\nthis particular case I know my K value\nthis particular case I know my K value is 2 so we will be initializing randomly\nis 2 so we will be initializing randomly\nis 2 so we will be initializing randomly let's say that K is equal to 2 so what\nlet's say that K is equal to 2 so what\nlet's say that K is equal to 2 so what we can actually do let's say that this\nwe can actually do let's say that this\nwe can actually do let's say that this is this is my one centroid I will I'll\nis this is my one centroid I will I'll\nis this is my one centroid I will I'll put it in another color so this will be\nput it in another color so this will be\nput it in another color so this will be my one centroid and let's say that this\nmy one centroid and let's say that this\nmy one centroid and let's say that this is my another centroid so I have\nis my another centroid so I have\nis my another centroid so I have initialized two centroids randomly in\ninitialized two centroids randomly in\ninitialized two centroids randomly in this space now after this particular\nthis space now after this particular\nthis space now after this particular centroid what we have to do is that\ncentroid what we have to do is that\ncentroid what we have to do is that after initializing this centroid what we\nafter initializing this centroid what we\nafter initializing this centroid what we have to do is that we have to basically\nhave to do is that we have to basically\nhave to do is that we have to basically find out which points are near to the\nfind out which points are near to the\nfind out which points are near to the centroid and which points are near to\ncentroid and which points are near to\ncentroid and which points are near to this centroid now in order to find out\nthis centroid now in order to find out\nthis centroid now in order to find out it is a very easy step we can basically\nit is a very easy step we can basically\nit is a very easy step we can basically use ukan distance to find out the\nuse ukan distance to find out the\nuse ukan distance to find out the distance between the points in an easy\ndistance between the points in an easy\ndistance between the points in an easy way if I really want to show you that\nway if I really want to show you that\nway if I really want to show you that you know like how many points I want to\nyou know like how many points I want to\nyou know like how many points I want to in an easy way what I can do I can\nin an easy way what I can do I can\nin an easy way what I can do I can basically draw a straight line over here\nbasically draw a straight line over here\nbasically draw a straight line over here let's say that I'm drawing a straight\nlet's say that I'm drawing a straight\nlet's say that I'm drawing a straight line over here in another color I can\nline over here in another color I can\nline over here in another color I can draw a straight line and I can also draw\ndraw a straight line and I can also draw\ndraw a straight line and I can also draw one parallel line like this so This\none parallel line like this so This\none parallel line like this so This basically indicates that whichever\nbasically indicates that whichever\nbasically indicates that whichever points you see over here suppose if I\npoints you see over here suppose if I\npoints you see over here suppose if I draw a straight line in between all\ndraw a straight line in between all\ndraw a straight line in between all these points you will be able to see\nthese points you will be able to see\nthese points you will be able to see that let's say that I'm drawing one more\nthat let's say that I'm drawing one more\nthat let's say that I'm drawing one more parallel line\nparallel line\nparallel line which is intersecting together so from\nwhich is intersecting together so from\nwhich is intersecting together so from this you can definitely find out let's\nthis you can definitely find out let's\nthis you can definitely find out let's say that these are all my points that\nsay that these are all my points that\nsay that these are all my points that are nearer to this green line Green\nare nearer to this green line Green\nare nearer to this green line Green Point so what I'm actually going to do\nPoint so what I'm actually going to do\nPoint so what I'm actually going to do in this particular case all these points\nin this particular case all these points\nin this particular case all these points that you are seeing near the green it\nthat you are seeing near the green it\nthat you are seeing near the green it will become green color so that\nwill become green color so that\nwill become green color so that basically means this is basically nearer\nbasically means this is basically nearer\nbasically means this is basically nearer to this centroid and whichever points\nto this centroid and whichever points\nto this centroid and whichever points are nearer to this particular point that\nare nearer to this particular point that\nare nearer to this particular point that will become red point so that basically\nwill become red point so that basically\nwill become red point so that basically means this belongs to this group okay\nmeans this belongs to this group okay\nmeans this belongs to this group okay this belongs to this group so I hope\nthis belongs to this group so I hope\nthis belongs to this group so I hope everybody's clear till here then what\neverybody's clear till here then what\neverybody's clear till here then what will happen is that this summation of\nwill happen is that this summation of\nwill happen is that this summation of all the values then we initialize the K\nall the values then we initialize the K\nall the values then we initialize the K number of centroids that is done then we\nnumber of centroids that is done then we\nnumber of centroids that is done then we try to calculate the distance we try to\ntry to calculate the distance we try to\ntry to calculate the distance we try to find out which all points is nearer to\nfind out which all points is nearer to\nfind out which all points is nearer to the centroid let's say that this is my\nthe centroid let's say that this is my\nthe centroid let's say that this is my one centroid this is my another centroid\none centroid this is my another centroid\none centroid this is my another centroid and we have seen that okay these all\nand we have seen that okay these all\nand we have seen that okay these all points belong to this centroid it near\npoints belong to this centroid it near\npoints belong to this centroid it near to this particular centroid so this is\nto this particular centroid so this is\nto this particular centroid so this is becoming red so that is based on the\nbecoming red so that is based on the\nbecoming red so that is based on the shortage distance and here it is\nshortage distance and here it is\nshortage distance and here it is becoming green now the next step let's\nbecoming green now the next step let's\nbecoming green now the next step let's see what is the next step after this so\nsee what is the next step after this so\nsee what is the next step after this so I am going to remove this thing now the\nI am going to remove this thing now the\nI am going to remove this thing now the next step will be that the entire points\nnext step will be that the entire points\nnext step will be that the entire points that is in red color all the average\nthat is in red color all the average\nthat is in red color all the average will be taken so here again the average\nwill be taken so here again the average\nwill be taken so here again the average will be taken now third step here I'm\nwill be taken now third step here I'm\nwill be taken now third step here I'm going to write here we are going to\ngoing to write here we are going to\ngoing to write here we are going to compute the average the reason we\ncompute the average the reason we\ncompute the average the reason we compute the average is that because we\ncompute the average is that because we\ncompute the average is that because we need to update the centroid so compute\nneed to update the centroid so compute\nneed to update the centroid so compute the average to update centroid to update\nthe average to update centroid to update\nthe average to update centroid to update centroids so here you'll be able to see\ncentroids so here you'll be able to see\ncentroids so here you'll be able to see that what I'm actually doing as soon as\nthat what I'm actually doing as soon as\nthat what I'm actually doing as soon as we compute the average this centroid is\nwe compute the average this centroid is\nwe compute the average this centroid is going to move to some other location so\ngoing to move to some other location so\ngoing to move to some other location so what location it will move it will\nwhat location it will move it will\nwhat location it will move it will obviously become somewhere in Center so\nobviously become somewhere in Center so\nobviously become somewhere in Center so here now I'm going to rub this and now\nhere now I'm going to rub this and now\nhere now I'm going to rub this and now my new centroid will be this point where\nmy new centroid will be this point where\nmy new centroid will be this point where I am actually going to draw like this\nI am actually going to draw like this\nI am actually going to draw like this let's say this is my new centroid now\nlet's say this is my new centroid now\nlet's say this is my new centroid now similarly this thing will happen with\nsimilarly this thing will happen with\nsimilarly this thing will happen with respect to the green color so with\nrespect to the green color so with\nrespect to the green color so with respect to the green color also it will\nrespect to the green color also it will\nrespect to the green color also it will happen and this green will also Al get\nhappen and this green will also Al get\nhappen and this green will also Al get updated so I'm going to rub this and\nupdated so I'm going to rub this and\nupdated so I'm going to rub this and this will be my new Green Point which\nthis will be my new Green Point which\nthis will be my new Green Point which will get updated over here then again\nwill get updated over here then again\nwill get updated over here then again what will happen again the distance will\nwhat will happen again the distance will\nwhat will happen again the distance will be calculated and again a perpendicular\nbe calculated and again a perpendicular\nbe calculated and again a perpendicular line will be calculated here you can see\nline will be calculated here you can see\nline will be calculated here you can see that now all the points are towards\nthat now all the points are towards\nthat now all the points are towards there okay again the centroid based on\nthere okay again the centroid based on\nthere okay again the centroid based on this particular distance again it will\nthis particular distance again it will\nthis particular distance again it will be calculated and here you can see that\nbe calculated and here you can see that\nbe calculated and here you can see that all the points are in its own location\nall the points are in its own location\nall the points are in its own location so here now no update will actually\nso here now no update will actually\nso here now no update will actually happen let's say that there was one\nhappen let's say that there was one\nhappen let's say that there was one point which was red color over here\npoint which was red color over here\npoint which was red color over here then this would have become green color\nthen this would have become green color\nthen this would have become green color but since the updation has happened\nbut since the updation has happened\nbut since the updation has happened perfectly we are not going to update it\nperfectly we are not going to update it\nperfectly we are not going to update it and we are not going to update the\nand we are not going to update the\nand we are not going to update the centroid right so now you can understand\ncentroid right so now you can understand\ncentroid right so now you can understand that yes now we have actually got the\nthat yes now we have actually got the\nthat yes now we have actually got the perfect centroid and now this will be\nperfect centroid and now this will be\nperfect centroid and now this will be considered as one group and this will be\nconsidered as one group and this will be\nconsidered as one group and this will be basically considered as the another\nbasically considered as the another\nbasically considered as the another group it will not intersect but right by\ngroup it will not intersect but right by\ngroup it will not intersect but right by default here intersection is happening\ndefault here intersection is happening\ndefault here intersection is happening so I hope everybody's understood the\nso I hope everybody's understood the\nso I hope everybody's understood the steps that you have actually followed in\nsteps that you have actually followed in\nsteps that you have actually followed in initializing the centroids in updating\ninitializing the centroids in updating\ninitializing the centroids in updating the centroids and in updating the points\nthe centroids and in updating the points\nthe centroids and in updating the points is it clear everybody with respect to K\nis it clear everybody with respect to K\nis it clear everybody with respect to K means now let's discuss about one\nmeans now let's discuss about one\nmeans now let's discuss about one point how do we decide this K value okay\npoint how do we decide this K value okay\npoint how do we decide this K value okay how do we decide this K value so for\nhow do we decide this K value so for\nhow do we decide this K value so for deciding the K value there is a concept\ndeciding the K value there is a concept\ndeciding the K value there is a concept which is called as elbow method so here\nwhich is called as elbow method so here\nwhich is called as elbow method so here I'm going to basically Define my elbow\nI'm going to basically Define my elbow\nI'm going to basically Define my elbow method now elbow method says something\nmethod now elbow method says something\nmethod now elbow method says something very much important because this will\nvery much important because this will\nvery much important because this will actually help us to find out what is the\nactually help us to find out what is the\nactually help us to find out what is the optimized K value whether the K value\noptimized K value whether the K value\noptimized K value whether the K value should be two whether uh the K value is\nshould be two whether uh the K value is\nshould be two whether uh the K value is going to be three whether the K value is\ngoing to be three whether the K value is\ngoing to be three whether the K value is going to become four and always\ngoing to become four and always\ngoing to become four and always understand suppose this is my data set\nunderstand suppose this is my data set\nunderstand suppose this is my data set suppose this is my data set initially\nsuppose this is my data set initially\nsuppose this is my data set initially let's say that I have my data points\nlet's say that I have my data points\nlet's say that I have my data points like this we cannot go ahead and\nlike this we cannot go ahead and\nlike this we cannot go ahead and directly say say that okay K is equal to\ndirectly say say that okay K is equal to\ndirectly say say that okay K is equal to 2 is going to work so obviously we are\n2 is going to work so obviously we are\n2 is going to work so obviously we are going to go with iteration for I is\ngoing to go with iteration for I is\ngoing to go with iteration for I is equal to probably 1 to 10 I'm going to\nequal to probably 1 to 10 I'm going to\nequal to probably 1 to 10 I'm going to move towards iteration from 1 to 10\nmove towards iteration from 1 to 10\nmove towards iteration from 1 to 10 let's say so for every iteration we will\nlet's say so for every iteration we will\nlet's say so for every iteration we will construct a graph with respect to K\nconstruct a graph with respect to K\nconstruct a graph with respect to K value and with respect to something\nvalue and with respect to something\nvalue and with respect to something called as W CSS now what is this W CSS W\ncalled as W CSS now what is this W CSS W\ncalled as W CSS now what is this W CSS W CSS basically means within cluster sum\nCSS basically means within cluster sum\nCSS basically means within cluster sum of\nof\nof square okay this is the meaning of wcss\nsquare okay this is the meaning of wcss\nsquare okay this is the meaning of wcss within cluster sum of square now let's\nwithin cluster sum of square now let's\nwithin cluster sum of square now let's say that initially we start with one\nsay that initially we start with one\nsay that initially we start with one centroid so one centroid let's say it is\ncentroid so one centroid let's say it is\ncentroid so one centroid let's say it is initialized here one centroid is\ninitialized here one centroid is\ninitialized here one centroid is basically initialized here if we go and\nbasically initialized here if we go and\nbasically initialized here if we go and compute the distance\ncompute the distance\ncompute the distance between each and every points to the\nbetween each and every points to the\nbetween each and every points to the centroid and if we try to find out the\ncentroid and if we try to find out the\ncentroid and if we try to find out the distance will the distance value be\ndistance will the distance value be\ndistance will the distance value be greater or it will be smaller will it be\ngreater or it will be smaller will it be\ngreater or it will be smaller will it be smaller or greater tell me if you try to\nsmaller or greater tell me if you try to\nsmaller or greater tell me if you try to calculate this distance from this\ncalculate this distance from this\ncalculate this distance from this centroid to every point this is what is\ncentroid to every point this is what is\ncentroid to every point this is what is within cluster sum of square it will\nwithin cluster sum of square it will\nwithin cluster sum of square it will always be very very much greater so\nalways be very very much greater so\nalways be very very much greater so let's say that my first point has come\nlet's say that my first point has come\nlet's say that my first point has come somewhere here it is going to be\nsomewhere here it is going to be\nsomewhere here it is going to be obviously greater let's say that my\nobviously greater let's say that my\nobviously greater let's say that my first point is coming over here find\nfirst point is coming over here find\nfirst point is coming over here find So within K is equal to 1 initially we\nSo within K is equal to 1 initially we\nSo within K is equal to 1 initially we took and we found out the distance of w\ntook and we found out the distance of w\ntook and we found out the distance of w CSS and it is a very huge value okay\nCSS and it is a very huge value okay\nCSS and it is a very huge value okay because we're going to compute the\nbecause we're going to compute the\nbecause we're going to compute the distance between each and every point to\ndistance between each and every point to\ndistance between each and every point to the centroid now the next thing that I'm\nthe centroid now the next thing that I'm\nthe centroid now the next thing that I'm actually going to do is that now we'll\nactually going to do is that now we'll\nactually going to do is that now we'll go with next value that is K is equal to\ngo with next value that is K is equal to\ngo with next value that is K is equal to 2 now in K is equal to 2 I will\n2 now in K is equal to 2 I will\n2 now in K is equal to 2 I will initialize two points okay I will\ninitialize two points okay I will\ninitialize two points okay I will initialize two points and then probably\ninitialize two points and then probably\ninitialize two points and then probably I will do the entire process which I\nI will do the entire process which I\nI will do the entire process which I have written on the top now tell me me\nhave written on the top now tell me me\nhave written on the top now tell me me whichever points is nearer to this green\nwhichever points is nearer to this green\nwhichever points is nearer to this green point if we compute the distance and\npoint if we compute the distance and\npoint if we compute the distance and whichever points is nearer to the red\nwhichever points is nearer to the red\nwhichever points is nearer to the red point if you compute the distance like\npoint if you compute the distance like\npoint if you compute the distance like this now this summation of the distance\nthis now this summation of the distance\nthis now this summation of the distance will be lesser than the previous W CSS\nwill be lesser than the previous W CSS\nwill be lesser than the previous W CSS or not obviously it is going to be\nor not obviously it is going to be\nor not obviously it is going to be lesser than the previous W CSS so what\nlesser than the previous W CSS so what\nlesser than the previous W CSS so what I'm actually going to do probably with K\nI'm actually going to do probably with K\nI'm actually going to do probably with K is equal to 2 your value may come\nis equal to 2 your value may come\nis equal to 2 your value may come somewhere here then with K is equal to 3\nsomewhere here then with K is equal to 3\nsomewhere here then with K is equal to 3 your value May come somewhere here then\nyour value May come somewhere here then\nyour value May come somewhere here then K is equal to 4 will come here to 5 6\nK is equal to 4 will come here to 5 6\nK is equal to 4 will come here to 5 6 like this it will go so here if I\nlike this it will go so here if I\nlike this it will go so here if I probably join this line you'll be able\nprobably join this line you'll be able\nprobably join this line you'll be able to see that there will be an Abrupt\nto see that there will be an Abrupt\nto see that there will be an Abrupt changes in the W CSS value in the wcss\nchanges in the W CSS value in the wcss\nchanges in the W CSS value in the wcss value there will be an Abrupt changes\nvalue there will be an Abrupt changes\nvalue there will be an Abrupt changes and this this is basically called as\nand this this is basically called as\nand this this is basically called as elbow curve now why we say it as elbow\nelbow curve now why we say it as elbow\nelbow curve now why we say it as elbow curve because it is in the shape of\ncurve because it is in the shape of\ncurve because it is in the shape of elbow and here at one specific point\nelbow and here at one specific point\nelbow and here at one specific point there will be an Abrupt change and then\nthere will be an Abrupt change and then\nthere will be an Abrupt change and then it will be straight so that is the\nit will be straight so that is the\nit will be straight so that is the reason why we basically say this as\nreason why we basically say this as\nreason why we basically say this as elbow okay so this is a very important\nelbow okay so this is a very important\nelbow okay so this is a very important thing see in finding the K value we use\nthing see in finding the K value we use\nthing see in finding the K value we use elbow method but for validating purpose\nelbow method but for validating purpose\nelbow method but for validating purpose how do we validate that this model is\nhow do we validate that this model is\nhow do we validate that this model is performing well we use silard score that\nperforming well we use silard score that\nperforming well we use silard score that I'll show you just in some time but\nI'll show you just in some time but\nI'll show you just in some time but understand that in K means clustering we\nunderstand that in K means clustering we\nunderstand that in K means clustering we need to update the centroids and based\nneed to update the centroids and based\nneed to update the centroids and based on that we calculate the distance and as\non that we calculate the distance and as\non that we calculate the distance and as the K value keep on increasing you'll be\nthe K value keep on increasing you'll be\nthe K value keep on increasing you'll be able to see that the distance will\nable to see that the distance will\nable to see that the distance will become normal or the wcss value will\nbecome normal or the wcss value will\nbecome normal or the wcss value will become normal and then we really need to\nbecome normal and then we really need to\nbecome normal and then we really need to find out which is the phys K value where\nfind out which is the phys K value where\nfind out which is the phys K value where the abrupt change see over here suppose\nthe abrupt change see over here suppose\nthe abrupt change see over here suppose abrupt change is there and then it is\nabrupt change is there and then it is\nabrupt change is there and then it is normal then I will probably take this as\nnormal then I will probably take this as\nnormal then I will probably take this as my K value so obviously the model\nmy K value so obviously the model\nmy K value so obviously the model complexity will be high because we are\ncomplexity will be high because we are\ncomplexity will be high because we are going to check with respect to different\ngoing to check with respect to different\ngoing to check with respect to different different K values and wcss values and\ndifferent K values and wcss values and\ndifferent K values and wcss values and this basically means that the value that\nthis basically means that the value that\nthis basically means that the value that we'll probably get first of all we need\nwe'll probably get first of all we need\nwe'll probably get first of all we need to construct this elbow curve then see\nto construct this elbow curve then see\nto construct this elbow curve then see the changes where it is basically\nthe changes where it is basically\nthe changes where it is basically happening we'll need to find out the\nhappening we'll need to find out the\nhappening we'll need to find out the abrupt change and once we get the abrupt\nabrupt change and once we get the abrupt\nabrupt change and once we get the abrupt change we basically say that this may be\nchange we basically say that this may be\nchange we basically say that this may be the K value so K is equal to 4 as an\nthe K value so K is equal to 4 as an\nthe K value so K is equal to 4 as an example I'm telling you so unless and\nexample I'm telling you so unless and\nexample I'm telling you so unless and until if you really want to find the\nuntil if you really want to find the\nuntil if you really want to find the cluster it is very much simple we take a\ncluster it is very much simple we take a\ncluster it is very much simple we take a k value we initialize K number of\nk value we initialize K number of\nk value we initialize K number of centroids we compute the average to\ncentroids we compute the average to\ncentroids we compute the average to update the centroids then again we try\nupdate the centroids then again we try\nupdate the centroids then again we try to find out the distance try to see that\nto find out the distance try to see that\nto find out the distance try to see that whether any points has changed and\nwhether any points has changed and\nwhether any points has changed and continue that process unless and until\ncontinue that process unless and until\ncontinue that process unless and until we get separate groups okay so this is\nwe get separate groups okay so this is\nwe get separate groups okay so this is the entire funa of claim in clustering\nthe entire funa of claim in clustering\nthe entire funa of claim in clustering so finally you'll be able to see that\nso finally you'll be able to see that\nso finally you'll be able to see that with respect to the K value we will be\nwith respect to the K value we will be\nwith respect to the K value we will be able to get that many number of groups\nable to get that many number of groups\nable to get that many number of groups if my K value is four that basically\nif my K value is four that basically\nif my K value is four that basically means I will be probably getting four\nmeans I will be probably getting four\nmeans I will be probably getting four different groups like this 1 two right\ndifferent groups like this 1 two right\ndifferent groups like this 1 two right three like this and four I will be\nthree like this and four I will be\nthree like this and four I will be getting four groups like this with K is\ngetting four groups like this with K is\ngetting four groups like this with K is equal to 4 that basically means K is\nequal to 4 that basically means K is\nequal to 4 that basically means K is equal to four clusters and every group\nequal to four clusters and every group\nequal to four clusters and every group will be having its own centroids okay\nwill be having its own centroids okay\nwill be having its own centroids okay every group will be having okay\nevery group will be having okay\nevery group will be having okay centroids are very much important yes\ncentroids are very much important yes\ncentroids are very much important yes I'll try to show you in the coding also\nI'll try to show you in the coding also\nI'll try to show you in the coding also guys let's go towards the second\nguys let's go towards the second\nguys let's go towards the second algorithm the second algorithm that we\nalgorithm the second algorithm that we\nalgorithm the second algorithm that we will be probably discussing is called as\nwill be probably discussing is called as\nwill be probably discussing is called as hierarchical clustering now hierarchal\nhierarchical clustering now hierarchal\nhierarchical clustering now hierarchal clustering is very much simple guys all\nclustering is very much simple guys all\nclustering is very much simple guys all you have to do is that let's say this is\nyou have to do is that let's say this is\nyou have to do is that let's say this is your data points this is your data\nyour data points this is your data\nyour data points this is your data points and this is my P1 let's say P2\npoints and this is my P1 let's say P2\npoints and this is my P1 let's say P2 now hierle clustering says that we will\nnow hierle clustering says that we will\nnow hierle clustering says that we will go step by step the first thing is that\ngo step by step the first thing is that\ngo step by step the first thing is that we will try to find out the most nearest\nwe will try to find out the most nearest\nwe will try to find out the most nearest Value let's say this is my X and Y let's\nValue let's say this is my X and Y let's\nValue let's say this is my X and Y let's say these are my points like this is my\nsay these are my points like this is my\nsay these are my points like this is my P1 point this is my P2 point this is my\nP1 point this is my P2 point this is my\nP1 point this is my P2 point this is my P3 point this is my P4 Point P5 Point P6\nP3 point this is my P4 Point P5 Point P6\nP3 point this is my P4 Point P5 Point P6 point p7 point okay so these are my\npoint p7 point okay so these are my\npoint p7 point okay so these are my points that I have actually named over\npoints that I have actually named over\npoints that I have actually named over here let's say that this may be the\nhere let's say that this may be the\nhere let's say that this may be the nearest point to each other so what it\nnearest point to each other so what it\nnearest point to each other so what it will do it will combine this together\nwill do it will combine this together\nwill do it will combine this together into one cluster this we have computed\ninto one cluster this we have computed\ninto one cluster this we have computed the distance so it will C create one\nthe distance so it will C create one\nthe distance so it will C create one cluster now what will happen on the\ncluster now what will happen on the\ncluster now what will happen on the right hand side there will be another\nright hand side there will be another\nright hand side there will be another notation which you may be using in\nnotation which you may be using in\nnotation which you may be using in connecting all the points one so suppose\nconnecting all the points one so suppose\nconnecting all the points one so suppose this is my P1 this is my P2 this is my\nthis is my P1 this is my P2 this is my\nthis is my P1 this is my P2 this is my P3 P4 let's say that I have this many\nP3 P4 let's say that I have this many\nP3 P4 let's say that I have this many points and probably I will also try to\npoints and probably I will also try to\npoints and probably I will also try to make\nmake\nmake p7 so these are my points p7 now you\np7 so these are my points p7 now you\np7 so these are my points p7 now you know that the nearest point that we are\nknow that the nearest point that we are\nknow that the nearest point that we are having okay this will probably be\nhaving okay this will probably be\nhaving okay this will probably be distance 1 2 3 this is distance okay 4 5\ndistance 1 2 3 this is distance okay 4 5\ndistance 1 2 3 this is distance okay 4 5 6 like this we have lot of distance so\n6 like this we have lot of distance so\n6 like this we have lot of distance so hierle clustering will first of all find\nhierle clustering will first of all find\nhierle clustering will first of all find out the nearest point and try to compute\nout the nearest point and try to compute\nout the nearest point and try to compute the distance between them and just try\nthe distance between them and just try\nthe distance between them and just try to combine them together into one what\nto combine them together into one what\nto combine them together into one what do we do we basically combine them into\ndo we do we basically combine them into\ndo we do we basically combine them into one group okay so P1 and P2 has been\none group okay so P1 and P2 has been\none group okay so P1 and P2 has been combined let's say then it'll go and\ncombined let's say then it'll go and\ncombined let's say then it'll go and find out the other nearest point so\nfind out the other nearest point so\nfind out the other nearest point so let's say P6 and p7 are near so they are\nlet's say P6 and p7 are near so they are\nlet's say P6 and p7 are near so they are also going to combine into one group so\nalso going to combine into one group so\nalso going to combine into one group so once they combine into one group then we\nonce they combine into one group then we\nonce they combine into one group then we have P6 and p7 which will be obviously L\nhave P6 and p7 which will be obviously L\nhave P6 and p7 which will be obviously L greater than the previous distance and\ngreater than the previous distance and\ngreater than the previous distance and we may get this kind of computation and\nwe may get this kind of computation and\nwe may get this kind of computation and another combination or cluster will form\nanother combination or cluster will form\nanother combination or cluster will form get formed over here then you have seen\nget formed over here then you have seen\nget formed over here then you have seen that okay P3 and P5 are nearer to each\nthat okay P3 and P5 are nearer to each\nthat okay P3 and P5 are nearer to each other so we are going to combine this so\nother so we are going to combine this so\nother so we are going to combine this so I'm going to basically combine P3 and\nI'm going to basically combine P3 and\nI'm going to basically combine P3 and P5 okay and let's say that this distance\nP5 okay and let's say that this distance\nP5 okay and let's say that this distance is greater than the previous one because\nis greater than the previous one because\nis greater than the previous one because we are basically going to sh start with\nwe are basically going to sh start with\nwe are basically going to sh start with the shortest distance and then we are\nthe shortest distance and then we are\nthe shortest distance and then we are going to capture the longest distance\ngoing to capture the longest distance\ngoing to capture the longest distance now this is done now you can see that\nnow this is done now you can see that\nnow this is done now you can see that the next point that is near right to\nthe next point that is near right to\nthe next point that is near right to this particular group is P4 so we are\nthis particular group is P4 so we are\nthis particular group is P4 so we are going to combine this together into one\ngoing to combine this together into one\ngoing to combine this together into one group so once we combine this into one\ngroup so once we combine this into one\ngroup so once we combine this into one group this P4 will get connected like\ngroup this P4 will get connected like\ngroup this P4 will get connected like this let's say it is getting connected\nthis let's say it is getting connected\nthis let's say it is getting connected like this P4 has got connected then what\nlike this P4 has got connected then what\nlike this P4 has got connected then what is the nearest Point whether it is P6 p7\nis the nearest Point whether it is P6 p7\nis the nearest Point whether it is P6 p7 group or P1 P2 obviously here you can\ngroup or P1 P2 obviously here you can\ngroup or P1 P2 obviously here you can see that P1 P2 is there so I am probably\nsee that P1 P2 is there so I am probably\nsee that P1 P2 is there so I am probably going to combine this group together\ngoing to combine this group together\ngoing to combine this group together that basically means P1 P2 let's say I'm\nthat basically means P1 P2 let's say I'm\nthat basically means P1 P2 let's say I'm just going to combine this group group\njust going to combine this group group\njust going to combine this group group together again circle is coming so I\ntogether again circle is coming so I\ntogether again circle is coming so I will make a dot let's say I'm going to\nwill make a dot let's say I'm going to\nwill make a dot let's say I'm going to combine this group together because\ncombine this group together because\ncombine this group together because these are my nearest groups so what will\nthese are my nearest groups so what will\nthese are my nearest groups so what will happen P1 and P2 will get combined to P5\nhappen P1 and P2 will get combined to P5\nhappen P1 and P2 will get combined to P5 sorry P4 P5 this one so I will be\nsorry P4 P5 this one so I will be\nsorry P4 P5 this one so I will be getting another line like this and then\ngetting another line like this and then\ngetting another line like this and then finally you'll be seeing that P6 p7 is\nfinally you'll be seeing that P6 p7 is\nfinally you'll be seeing that P6 p7 is the nearest group to this so this will\nthe nearest group to this so this will\nthe nearest group to this so this will totally get combined and it may look\ntotally get combined and it may look\ntotally get combined and it may look something like this so this will become\nsomething like this so this will become\nsomething like this so this will become a total group like\na total group like\na total group like this so all the groups are combined so\nthis so all the groups are combined so\nthis so all the groups are combined so finally you'll be able to see that there\nfinally you'll be able to see that there\nfinally you'll be able to see that there will be one more line which will get\nwill be one more line which will get\nwill be one more line which will get combined like\ncombined like\ncombined like this this is basically called as\nthis this is basically called as\nthis this is basically called as dendogram dendogram okay which is like\ndendogram dendogram okay which is like\ndendogram dendogram okay which is like bottom root to top now the question\nbottom root to top now the question\nbottom root to top now the question arises is that how do you find that how\narises is that how do you find that how\narises is that how do you find that how many groups should be here how do you\nmany groups should be here how do you\nmany groups should be here how do you find out that how many groups should be\nfind out that how many groups should be\nfind out that how many groups should be here the funa is very much Clear guys in\nhere the funa is very much Clear guys in\nhere the funa is very much Clear guys in this is that you need\nthis is that you need\nthis is that you need to find the longest\nvertical line you need to find out the\nvertical line you need to find out the longest vertical line that has no\nlongest vertical line that has no\nlongest vertical line that has no horizontal line pass through it no\nhorizontal line pass through it no\nhorizontal line pass through it no horizontal\nhorizontal\nhorizontal line passed through it this is very much\nline passed through it this is very much\nline passed through it this is very much important that has no horizontal line\nimportant that has no horizontal line\nimportant that has no horizontal line pass through it now what this is\npass through it now what this is\npass through it now what this is basically meaning is that I will try to\nbasically meaning is that I will try to\nbasically meaning is that I will try to find out the longest line longest\nfind out the longest line longest\nfind out the longest line longest vertical line in such a way that none of\nvertical line in such a way that none of\nvertical line in such a way that none of the horizontal line passes through it\nthe horizontal line passes through it\nthe horizontal line passes through it what is horizontal line suppose if I\nwhat is horizontal line suppose if I\nwhat is horizontal line suppose if I consider this vertical line This\nconsider this vertical line This\nconsider this vertical line This vertical line over here if you see that\nvertical line over here if you see that\nvertical line over here if you see that if I extend this green line it is\nif I extend this green line it is\nif I extend this green line it is passing through this if I extend this\npassing through this if I extend this\npassing through this if I extend this line it is passing through this right if\nline it is passing through this right if\nline it is passing through this right if I'm extending this line it is passing\nI'm extending this line it is passing\nI'm extending this line it is passing through this right so out of this the\nthrough this right so out of this the\nthrough this right so out of this the longest line that may be passing in such\nlongest line that may be passing in such\nlongest line that may be passing in such a way that no horizontal line probably\na way that no horizontal line probably\na way that no horizontal line probably is this line that I can actually see so\nis this line that I can actually see so\nis this line that I can actually see so what you do over here is that you\nwhat you do over here is that you\nwhat you do over here is that you basically just create a straight line\nbasically just create a straight line\nbasically just create a straight line over this and then you try to find out\nover this and then you try to find out\nover this and then you try to find out that how many clusters it will be there\nthat how many clusters it will be there\nthat how many clusters it will be there by understanding that how many lines it\nby understanding that how many lines it\nby understanding that how many lines it is passing through if it is passing\nis passing through if it is passing\nis passing through if it is passing through this one line two line three\nthrough this one line two line three\nthrough this one line two line three line four line that basically means your\nline four line that basically means your\nline four line that basically means your clusters will be four\nclusters will be four\nclusters will be four clusters this is how we basically do the\nclusters this is how we basically do the\nclusters this is how we basically do the calculation in heral clustering again\ncalculation in heral clustering again\ncalculation in heral clustering again here it may not be the perfect line I've\nhere it may not be the perfect line I've\nhere it may not be the perfect line I've just drawn with some assumptions but if\njust drawn with some assumptions but if\njust drawn with some assumptions but if you are trying to do this probably you\nyou are trying to do this probably you\nyou are trying to do this probably you have to do in this specific way okay\nhave to do in this specific way okay\nhave to do in this specific way okay I've already uploaded a lot of practical\nI've already uploaded a lot of practical\nI've already uploaded a lot of practical videos with respect to highill\nvideos with respect to highill\nvideos with respect to highill clustering and all now now tell me\nclustering and all now now tell me\nclustering and all now now tell me maximum effort or maximum time is taken\nmaximum effort or maximum time is taken\nmaximum effort or maximum time is taken by is taken\nby is taken\nby is taken by K\nby K\nby K means or hierle clustering this is a\nmeans or hierle clustering this is a\nmeans or hierle clustering this is a question for you yes guys number of\nquestion for you yes guys number of\nquestion for you yes guys number of clusters may be three but here I'm just\nclusters may be three but here I'm just\nclusters may be three but here I'm just showing you that how many lines it may\nshowing you that how many lines it may\nshowing you that how many lines it may be passed by how do you basically\nbe passed by how do you basically\nbe passed by how do you basically determine whether maximum time will be\ndetermine whether maximum time will be\ndetermine whether maximum time will be taken by kin or Hier clustering this is\ntaken by kin or Hier clustering this is\ntaken by kin or Hier clustering this is an interview question the maximum time\nan interview question the maximum time\nan interview question the maximum time that will be taken is by hierarchical\nthat will be taken is by hierarchical\nthat will be taken is by hierarchical clustering why because let's say that I\nclustering why because let's say that I\nclustering why because let's say that I have many many many data points at that\nhave many many many data points at that\nhave many many many data points at that point of time hierle clustering will\npoint of time hierle clustering will\npoint of time hierle clustering will keep on constructing this kind of\nkeep on constructing this kind of\nkeep on constructing this kind of dendograms and it will be taking many\ndendograms and it will be taking many\ndendograms and it will be taking many many many time lot time right so hierle\nmany many time lot time right so hierle\nmany many time lot time right so hierle clustering will take more time maximum\nclustering will take more time maximum\nclustering will take more time maximum time that it is going to basically take\ntime that it is going to basically take\ntime that it is going to basically take so it is very much important that that\nso it is very much important that that\nso it is very much important that that you understand which is making basically\nyou understand which is making basically\nyou understand which is making basically taking more time so if your data set is\ntaking more time so if your data set is\ntaking more time so if your data set is small you may go ahead with hierle\nsmall you may go ahead with hierle\nsmall you may go ahead with hierle clustering if your data set is large go\nclustering if your data set is large go\nclustering if your data set is large go with K means clustering go with K means\nwith K means clustering go with K means\nwith K means clustering go with K means clustering in short both will take more\nclustering in short both will take more\nclustering in short both will take more time but K Min will perform better than\ntime but K Min will perform better than\ntime but K Min will perform better than hle clustering see guys you will be\nhle clustering see guys you will be\nhle clustering see guys you will be forming this kind of dendograms right\nforming this kind of dendograms right\nforming this kind of dendograms right and just imagine if you have 10 features\nand just imagine if you have 10 features\nand just imagine if you have 10 features and many data points how you're going to\nand many data points how you're going to\nand many data points how you're going to do it it will be a cubers some process\ndo it it will be a cubers some process\ndo it it will be a cubers some process you'll not be even able to see this\nyou'll not be even able to see this\nyou'll not be even able to see this dendogram properly and manually\ndendogram properly and manually\ndendogram properly and manually obviously you cannot do it so this was\nobviously you cannot do it so this was\nobviously you cannot do it so this was with respect to K means clust swing and\nwith respect to K means clust swing and\nwith respect to K means clust swing and H mean clust swing I hope everybody's\nH mean clust swing I hope everybody's\nH mean clust swing I hope everybody's understood now the next topic that we'll\nunderstood now the next topic that we'll\nunderstood now the next topic that we'll focus on is that how do we\nfocus on is that how do we\nfocus on is that how do we validate see how do we validate a\nvalidate see how do we validate a\nvalidate see how do we validate a classification problem we use\nclassification problem we use\nclassification problem we use performance metric like confusion Matrix\nperformance metric like confusion Matrix\nperformance metric like confusion Matrix accuracy um different different true\naccuracy um different different true\naccuracy um different different true positive rate Precision recall but how\npositive rate Precision recall but how\npositive rate Precision recall but how do we validate clustering model Model S\ndo we validate clustering model Model S\ndo we validate clustering model Model S we are going to use something called as\nwe are going to use something called as\nwe are going to use something called as so we are going to basically use\nso we are going to basically use\nso we are going to basically use something called as\nsomething called as\nsomething called as Sil score I'll show you what Sid score\nSil score I'll show you what Sid score\nSil score I'll show you what Sid score is I'm going to just open the Wikipedia\nis I'm going to just open the Wikipedia\nis I'm going to just open the Wikipedia so this is how a CID score looks like a\nso this is how a CID score looks like a\nso this is how a CID score looks like a very very amazing topic okay how do we\nvery very amazing topic okay how do we\nvery very amazing topic okay how do we validate whether my model basically has\nvalidate whether my model basically has\nvalidate whether my model basically has perfect three or four model perfect\nperfect three or four model perfect\nperfect three or four model perfect three suppose if I find out my K value\nthree suppose if I find out my K value\nthree suppose if I find out my K value is three how do we find out now see one\nis three how do we find out now see one\nis three how do we find out now see one more one more issue with K means one\nmore one more issue with K means one\nmore one more issue with K means one issue with K means which I forgot to\nissue with K means which I forgot to\nissue with K means which I forgot to tell you let's say that I have a data\ntell you let's say that I have a data\ntell you let's say that I have a data point which looks like this and suppose\npoint which looks like this and suppose\npoint which looks like this and suppose I have some data points like this I have\nI have some data points like this I have\nI have some data points like this I have some data points which looks like this\nsome data points which looks like this\nsome data points which looks like this let's say I have like this now in this\nlet's say I have like this now in this\nlet's say I have like this now in this one issue will be that suppose I try to\none issue will be that suppose I try to\none issue will be that suppose I try to make a cluster over here obviously\nmake a cluster over here obviously\nmake a cluster over here obviously you'll be saying my K value will be two\nyou'll be saying my K value will be two\nyou'll be saying my K value will be two okay in this particular case suppose\nokay in this particular case suppose\nokay in this particular case suppose this is one cluster this is my another\nthis is one cluster this is my another\nthis is one cluster this is my another cluster\ncluster\ncluster right because of my wrong initialization\nright because of my wrong initialization\nright because of my wrong initialization of the points okay understand because\nof the points okay understand because\nof the points okay understand because suppose if I initialize just randomly\nsuppose if I initialize just randomly\nsuppose if I initialize just randomly some centroids like this then what may\nsome centroids like this then what may\nsome centroids like this then what may happen is that there is a possibility\nhappen is that there is a possibility\nhappen is that there is a possibility that we may also have three clusters\nthat we may also have three clusters\nthat we may also have three clusters like like like this kind of clusters one\nlike like like this kind of clusters one\nlike like like this kind of clusters one cluster will be here one cluster will be\ncluster will be here one cluster will be\ncluster will be here one cluster will be here one cluster will be here so this\nhere one cluster will be here so this\nhere one cluster will be here so this initialization of the centroids one\ninitialization of the centroids one\ninitialization of the centroids one condition is that it should be very very\ncondition is that it should be very very\ncondition is that it should be very very far if we initialize our centroids very\nfar if we initialize our centroids very\nfar if we initialize our centroids very very far at that point of time we will\nvery far at that point of time we will\nvery far at that point of time we will be able to find the centroid exactly in\nbe able to find the centroid exactly in\nbe able to find the centroid exactly in the center because it will keep on\nthe center because it will keep on\nthe center because it will keep on updating it'll keep on going ahead right\nupdating it'll keep on going ahead right\nupdating it'll keep on going ahead right but if we don't initialize that very far\nbut if we don't initialize that very far\nbut if we don't initialize that very far then there will be a situation that\nthen there will be a situation that\nthen there will be a situation that probably if I wanted to get only the\nprobably if I wanted to get only the\nprobably if I wanted to get only the real thing was to get only two centroids\nreal thing was to get only two centroids\nreal thing was to get only two centroids I was probably getting three centroids\nI was probably getting three centroids\nI was probably getting three centroids right so this is a problem so for this\nright so this is a problem so for this\nright so this is a problem so for this there is an algorithm which is called as\nthere is an algorithm which is called as\nthere is an algorithm which is called as K means Plus+ and what this K means\nK means Plus+ and what this K means\nK means Plus+ and what this K means Plus+ will do which I will probably show\nPlus+ will do which I will probably show\nPlus+ will do which I will probably show you in Practical this will make sure\nyou in Practical this will make sure\nyou in Practical this will make sure that all the centroids that are\nthat all the centroids that are\nthat all the centroids that are initialized it is very very\ninitialized it is very very\ninitialized it is very very far okay all the in centroids that is\nfar okay all the in centroids that is\nfar okay all the in centroids that is basically there it is initialized very\nbasically there it is initialized very\nbasically there it is initialized very very far we'll see that in practical\nvery far we'll see that in practical\nvery far we'll see that in practical application where specifically those\napplication where specifically those\napplication where specifically those centroids are basically used now let me\ncentroids are basically used now let me\ncentroids are basically used now let me go ahead and let me show\ngo ahead and let me show\ngo ahead and let me show you with respect to Sid clust string now\nyou with respect to Sid clust string now\nyou with respect to Sid clust string now what is the solo color string I'm going\nwhat is the solo color string I'm going\nwhat is the solo color string I'm going to explain you in an amazing way this is\nto explain you in an amazing way this is\nto explain you in an amazing way this is important\nimportant\nimportant if someone says you how do we validate\nif someone says you how do we validate\nif someone says you how do we validate how do we validate cluster\nhow do we validate cluster\nhow do we validate cluster model then at that point of time we\nmodel then at that point of time we\nmodel then at that point of time we basically use this site it will be used\nbasically use this site it will be used\nbasically use this site it will be used in it will be used with respect\nin it will be used with respect\nin it will be used with respect to it will be used with respect to K\nto it will be used with respect to K\nto it will be used with respect to K means it can be used in hierle mean\nmeans it can be used in hierle mean\nmeans it can be used in hierle mean right if you want to validate how do we\nright if you want to validate how do we\nright if you want to validate how do we validate okay that is what we are\nvalidate okay that is what we are\nvalidate okay that is what we are basically going to see over here now in\nbasically going to see over here now in\nbasically going to see over here now in C's clustering\nC's clustering\nC's clustering what are the most important things the\nwhat are the most important things the\nwhat are the most important things the first and the most important thing is\nfirst and the most important thing is\nfirst and the most important thing is that we will try to find out we will try\nthat we will try to find out we will try\nthat we will try to find out we will try to find out a ofi we will try to find\nto find out a ofi we will try to find\nto find out a ofi we will try to find out a of I now what is this a ofi see\nout a of I now what is this a ofi see\nout a of I now what is this a ofi see this a ofi that you basically see a ofi\nthis a ofi that you basically see a ofi\nthis a ofi that you basically see a ofi is nothing but see three major steps\nis nothing but see three major steps\nis nothing but see three major steps happens in order to validate cluster\nhappens in order to validate cluster\nhappens in order to validate cluster model with the help of solo first thing\nmodel with the help of solo first thing\nmodel with the help of solo first thing is that I will probably take one cluster\nis that I will probably take one cluster\nis that I will probably take one cluster okay there will be one point\nokay there will be one point\nokay there will be one point which will be my centroid let's say and\nwhich will be my centroid let's say and\nwhich will be my centroid let's say and then what I'm going to do I'm just going\nthen what I'm going to do I'm just going\nthen what I'm going to do I'm just going to whatever points are there inside this\nto whatever points are there inside this\nto whatever points are there inside this cluster I'm going to compute the\ncluster I'm going to compute the\ncluster I'm going to compute the distance between them so I'm going to do\ndistance between them so I'm going to do\ndistance between them so I'm going to do the summation and I'm also going to do\nthe summation and I'm also going to do\nthe summation and I'm also going to do the average of all this distance so here\nthe average of all this distance so here\nthe average of all this distance so here you can see that when I said distance of\nyou can see that when I said distance of\nyou can see that when I said distance of I comma J I basically means this point J\nI comma J I basically means this point J\nI comma J I basically means this point J basically means all these points I is\nbasically means all these points I is\nbasically means all these points I is nothing but it is the centroid so here\nnothing but it is the centroid so here\nnothing but it is the centroid so here is nothing but this this is the centroid\nis nothing but this this is the centroid\nis nothing but this this is the centroid let's say that I'm having the centroid\nlet's say that I'm having the centroid\nlet's say that I'm having the centroid so I'm going to compute all the distance\nso I'm going to compute all the distance\nso I'm going to compute all the distance over here which is mentioned by this and\nover here which is mentioned by this and\nover here which is mentioned by this and this value that you see that I'm\nthis value that you see that I'm\nthis value that you see that I'm actually dividing by C of I minus one in\nactually dividing by C of I minus one in\nactually dividing by C of I minus one in Short I am actually trying to calculate\nShort I am actually trying to calculate\nShort I am actually trying to calculate the average\nthe average\nthe average distance so this is the first point\ndistance so this is the first point\ndistance so this is the first point where I'm actually Computing the a ofi\nwhere I'm actually Computing the a ofi\nwhere I'm actually Computing the a ofi now similarly what I will do is\nnow similarly what I will do is\nnow similarly what I will do is that what I will do is that the next\nthat what I will do is that the next\nthat what I will do is that the next point will be that suppose I have\npoint will be that suppose I have\npoint will be that suppose I have computed a ofi the next the next that we\ncomputed a ofi the next the next that we\ncomputed a ofi the next the next that we need to compute is B ofi now what is b\nneed to compute is B ofi now what is b\nneed to compute is B ofi now what is b ofi b ofi is nothing but there will be\nofi b ofi is nothing but there will be\nofi b ofi is nothing but there will be multiple clusters in a k means problem\nmultiple clusters in a k means problem\nmultiple clusters in a k means problem statement we will try to find out the\nstatement we will try to find out the\nstatement we will try to find out the nearest cluster okay suppose let's say\nnearest cluster okay suppose let's say\nnearest cluster okay suppose let's say that this is the nearest cluster and in\nthat this is the nearest cluster and in\nthat this is the nearest cluster and in this I have all the variety of points\nthis I have all the variety of points\nthis I have all the variety of points then B ofi basically says that I will\nthen B ofi basically says that I will\nthen B ofi basically says that I will try to compute the distance between each\ntry to compute the distance between each\ntry to compute the distance between each point and the other point in this\npoint and the other point in this\npoint and the other point in this centroid sorry in this cluster so this\ncentroid sorry in this cluster so this\ncentroid sorry in this cluster so this is my cluster one this is my cluster two\nis my cluster one this is my cluster two\nis my cluster one this is my cluster two so what I'm actually going to do is that\nso what I'm actually going to do is that\nso what I'm actually going to do is that here I'm going to compute the distance\nhere I'm going to compute the distance\nhere I'm going to compute the distance between this point to this point then\nbetween this point to this point then\nbetween this point to this point then this point to this point then this point\nthis point to this point then this point\nthis point to this point then this point to this point this point to this point\nto this point this point to this point\nto this point this point to this point this point to this point this point to\nthis point to this point this point to\nthis point to this point this point to this point every point I'm actually\nthis point every point I'm actually\nthis point every point I'm actually going to compute the distance once this\ngoing to compute the distance once this\ngoing to compute the distance once this point is done we will go ahead with the\npoint is done we will go ahead with the\npoint is done we will go ahead with the next point and we'll try to compute the\nnext point and we'll try to compute the\nnext point and we'll try to compute the distance and once we get all this\ndistance and once we get all this\ndistance and once we get all this particular distance what we are going to\nparticular distance what we are going to\nparticular distance what we are going to do we are going to do the average of\ndo we are going to do the average of\ndo we are going to do the average of them average\nthem average\nthem average now tell me if I try to find out the\nnow tell me if I try to find out the\nnow tell me if I try to find out the relationship between a of I and B of I\nrelationship between a of I and B of I\nrelationship between a of I and B of I if my cluster model is good will a of\nif my cluster model is good will a of\nif my cluster model is good will a of I will be greater than b of I or\nI will be greater than b of I or\nI will be greater than b of I or will B of I will be greater than a ofi\nwill B of I will be greater than a ofi\nwill B of I will be greater than a ofi if I have a good clustering model if I\nif I have a good clustering model if I\nif I have a good clustering model if I have a good clustering model will a of I\nhave a good clustering model will a of I\nhave a good clustering model will a of I is greater than b of I will be greater\nis greater than b of I will be greater\nis greater than b of I will be greater than b of I or whether B of I will be\nthan b of I or whether B of I will be\nthan b of I or whether B of I will be greater than a of I out of this if we\ngreater than a of I out of this if we\ngreater than a of I out of this if we have a really good model obviously the\nhave a really good model obviously the\nhave a really good model obviously the distance between B of I will be greater\ndistance between B of I will be greater\ndistance between B of I will be greater than a of I in a good model that\nthan a of I in a good model that\nthan a of I in a good model that basically means if I talk about sloid\nbasically means if I talk about sloid\nbasically means if I talk about sloid clustering the values will be between -1\nclustering the values will be between -1\nclustering the values will be between -1 to +1 the more the value is towards +1\nto +1 the more the value is towards +1\nto +1 the more the value is towards +1 that basically means the good the model\nthat basically means the good the model\nthat basically means the good the model is the good the clustering model is the\nis the good the clustering model is the\nis the good the clustering model is the more the values towards negative one\nmore the values towards negative one\nmore the values towards negative one that basically means this condition is\nthat basically means this condition is\nthat basically means this condition is getting applied now what does this\ngetting applied now what does this\ngetting applied now what does this condition basically say that basically\ncondition basically say that basically\ncondition basically say that basically means that this distance is far than the\nmeans that this distance is far than the\nmeans that this distance is far than the cluster distance this is what this\ncluster distance this is what this\ncluster distance this is what this information is getting portrayed and\ninformation is getting portrayed and\ninformation is getting portrayed and this is the importance of CID\nthis is the importance of CID\nthis is the importance of CID clustering finally when we apply the\nclustering finally when we apply the\nclustering finally when we apply the formula of CID clustering you'll be able\nformula of CID clustering you'll be able\nformula of CID clustering you'll be able to see that sloid clustering is nothing\nto see that sloid clustering is nothing\nto see that sloid clustering is nothing but let me rub this everything guys for\nbut let me rub this everything guys for\nbut let me rub this everything guys for you let me just show you what is CID\nyou let me just show you what is CID\nyou let me just show you what is CID clustering CID clustering formula will\nclustering CID clustering formula will\nclustering CID clustering formula will be something like this this B of I so\nbe something like this this B of I so\nbe something like this this B of I so here you have solid clustering this is\nhere you have solid clustering this is\nhere you have solid clustering this is the formula B of I minus a of I Max of a\nthe formula B of I minus a of I Max of a\nthe formula B of I minus a of I Max of a of I comma B of I if C of I is greater\nof I comma B of I if C of I is greater\nof I comma B of I if C of I is greater than one right so by this you will be\nthan one right so by this you will be\nthan one right so by this you will be getting the value between -1 to + 1 and\ngetting the value between -1 to + 1 and\ngetting the value between -1 to + 1 and more the value is towards + one the more\nmore the value is towards + one the more\nmore the value is towards + one the more good your model is more the values\ngood your model is more the values\ngood your model is more the values towards minus1 more bad your model is\ntowards minus1 more bad your model is\ntowards minus1 more bad your model is because if it is towards minus1 that\nbecause if it is towards minus1 that\nbecause if it is towards minus1 that basically means your a of I is obviously\nbasically means your a of I is obviously\nbasically means your a of I is obviously greater than b of I so this is the\ngreater than b of I so this is the\ngreater than b of I so this is the outcome with respect to cot crust string\noutcome with respect to cot crust string\noutcome with respect to cot crust string if s is equal to zero that basically\nif s is equal to zero that basically\nif s is equal to zero that basically means still your model needs to be uh\nmeans still your model needs to be uh\nmeans still your model needs to be uh per basically the clustering needs to be\nper basically the clustering needs to be\nper basically the clustering needs to be improved what is I over here I is\nimproved what is I over here I is\nimproved what is I over here I is nothing but one data point you you can\nnothing but one data point you you can\nnothing but one data point you you can just read this guys data point in I in\njust read this guys data point in I in\njust read this guys data point in I in the cluster C of I so I hope everybody's\nthe cluster C of I so I hope everybody's\nthe cluster C of I so I hope everybody's understood this now let's go ahead and\nunderstood this now let's go ahead and\nunderstood this now let's go ahead and let's discuss about the next topic we\nlet's discuss about the next topic we\nlet's discuss about the next topic we have obviously finished up solart\nhave obviously finished up solart\nhave obviously finished up solart clustering over here let's discuss about\nclustering over here let's discuss about\nclustering over here let's discuss about something called as DB\nsomething called as DB\nsomething called as DB scan so for DB scan clustering this is\nscan so for DB scan clustering this is\nscan so for DB scan clustering this is an amazing clustering algorithm we'll\nan amazing clustering algorithm we'll\nan amazing clustering algorithm we'll try to understand how to actually do DB\ntry to understand how to actually do DB\ntry to understand how to actually do DB clustering and probably you'll be able\nclustering and probably you'll be able\nclustering and probably you'll be able to understand a lot of things from this\nto understand a lot of things from this\nto understand a lot of things from this now in DB scan clustering what are the\nnow in DB scan clustering what are the\nnow in DB scan clustering what are the important things so let's start with\nimportant things so let's start with\nimportant things so let's start with respect to DB scan clustering and let's\nrespect to DB scan clustering and let's\nrespect to DB scan clustering and let's understand some of the important points\nunderstand some of the important points\nunderstand some of the important points over here the first point that you\nover here the first point that you\nover here the first point that you really need to remember is something\nreally need to remember is something\nreally need to remember is something called as score point points I'll also\ncalled as score point points I'll also\ncalled as score point points I'll also talk about when do you say core points\ntalk about when do you say core points\ntalk about when do you say core points or when do you say other points as such\nor when do you say other points as such\nor when do you say other points as such so the first point that I will probably\nso the first point that I will probably\nso the first point that I will probably discuss about is something called as Min\ndiscuss about is something called as Min\ndiscuss about is something called as Min points the second point that I will\npoints the second point that I will\npoints the second point that I will probably discuss about is something\nprobably discuss about is something\nprobably discuss about is something called as score points the third thing\ncalled as score points the third thing\ncalled as score points the third thing that I will probably discuss about is\nthat I will probably discuss about is\nthat I will probably discuss about is something called as border points and\nsomething called as border points and\nsomething called as border points and the fourth point that I will definitely\nthe fourth point that I will definitely\nthe fourth point that I will definitely talk about is something called as noise\ntalk about is something called as noise\ntalk about is something called as noise Point okay guys now tell me in C's\nPoint okay guys now tell me in C's\nPoint okay guys now tell me in C's clustering\nclustering\nclustering if I have this kind of groups don't you\nif I have this kind of groups don't you\nif I have this kind of groups don't you think with the help of two different\nthink with the help of two different\nthink with the help of two different clusters I may combine this two like\nclusters I may combine this two like\nclusters I may combine this two like this with the help of two different\nthis with the help of two different\nthis with the help of two different clusters I may combine something like\nclusters I may combine something like\nclusters I may combine something like this right but understand over here what\nthis right but understand over here what\nthis right but understand over here what what problem is basically happening with\nwhat problem is basically happening with\nwhat problem is basically happening with the second clustering this is actually\nthe second clustering this is actually\nthe second clustering this is actually an outliers let's say that let's say one\nan outliers let's say that let's say one\nan outliers let's say that let's say one thing very nicely I will put okay let's\nthing very nicely I will put okay let's\nthing very nicely I will put okay let's say I have one point over here I have\nsay I have one point over here I have\nsay I have one point over here I have one point over here here so if I do\none point over here here so if I do\none point over here here so if I do clustering probably I will get one\nclustering probably I will get one\nclustering probably I will get one cluster\ncluster\ncluster here and I may get another cluster which\nhere and I may get another cluster which\nhere and I may get another cluster which is somewhere here now understand one\nis somewhere here now understand one\nis somewhere here now understand one thing this point is definitely an\nthing this point is definitely an\nthing this point is definitely an outlier even though this is an outlier\noutlier even though this is an outlier\noutlier even though this is an outlier with the help of K means what I'm\nwith the help of K means what I'm\nwith the help of K means what I'm actually doing I'm actually grouping\nactually doing I'm actually grouping\nactually doing I'm actually grouping this into another group so can we have a\nthis into another group so can we have a\nthis into another group so can we have a scenario wherein a kind of clustering\nscenario wherein a kind of clustering\nscenario wherein a kind of clustering algorithm is there where we can leave\nalgorithm is there where we can leave\nalgorithm is there where we can leave the outlier separately and this outlier\nthe outlier separately and this outlier\nthe outlier separately and this outlier in this particular algorithm and this is\nin this particular algorithm and this is\nin this particular algorithm and this is B basically uh we will be using DB scan\nB basically uh we will be using DB scan\nB basically uh we will be using DB scan to relieve the outlier and this point\nto relieve the outlier and this point\nto relieve the outlier and this point will be called as a noisy Point noisy\nwill be called as a noisy Point noisy\nwill be called as a noisy Point noisy point or I can also say it as an outlier\npoint or I can also say it as an outlier\npoint or I can also say it as an outlier so this will be a noise point for this\nso this will be a noise point for this\nso this will be a noise point for this kind of algorithm where you want to skip\nkind of algorithm where you want to skip\nkind of algorithm where you want to skip the outliers we can definitely use DB\nthe outliers we can definitely use DB\nthe outliers we can definitely use DB scan that is density based spatial\nscan that is density based spatial\nscan that is density based spatial clustering of application with noise a\nclustering of application with noise a\nclustering of application with noise a very amazing algorithm and definitely I\nvery amazing algorithm and definitely I\nvery amazing algorithm and definitely I have tried using this a lot nowadays I\nhave tried using this a lot nowadays I\nhave tried using this a lot nowadays I don't use K means or Hier means instead\ndon't use K means or Hier means instead\ndon't use K means or Hier means instead use this kind of algorithm now see this\nuse this kind of algorithm now see this\nuse this kind of algorithm now see this what are the important things over here\nwhat are the important things over here\nwhat are the important things over here first of all you need to go ahead with\nfirst of all you need to go ahead with\nfirst of all you need to go ahead with Min points Min points so first thing is\nMin points Min points so first thing is\nMin points Min points so first thing is that you need to have Min points this\nthat you need to have Min points this\nthat you need to have Min points this Min points is a kind of\nMin points is a kind of\nMin points is a kind of hyperparameter this basically says what\nhyperparameter this basically says what\nhyperparameter this basically says what does hyper parameter says and there is\ndoes hyper parameter says and there is\ndoes hyper parameter says and there is also a value which is called as\nalso a value which is called as\nalso a value which is called as Epsilon which I forgot I will write it\nEpsilon which I forgot I will write it\nEpsilon which I forgot I will write it down over here this is called as Epsilon\ndown over here this is called as Epsilon\ndown over here this is called as Epsilon now what does epsilon mean Epsilon\nnow what does epsilon mean Epsilon\nnow what does epsilon mean Epsilon basically means if I have a point like\nbasically means if I have a point like\nbasically means if I have a point like this\nthis\nthis and if I take Epsilon this is nothing\nand if I take Epsilon this is nothing\nand if I take Epsilon this is nothing but the radius of that specific Circle\nbut the radius of that specific Circle\nbut the radius of that specific Circle radius of that specific Circle okay so\nradius of that specific Circle okay so\nradius of that specific Circle okay so Epsilon is nothing but radius over here\nEpsilon is nothing but radius over here\nEpsilon is nothing but radius over here in this specific T what does minimum\nin this specific T what does minimum\nin this specific T what does minimum points is equal to 4 mean let's say that\npoints is equal to 4 mean let's say that\npoints is equal to 4 mean let's say that I have I have taken a point over here\nI have I have taken a point over here\nI have I have taken a point over here let's say that this is my\nlet's say that this is my\nlet's say that this is my point and I have drawn a circle which\npoint and I have drawn a circle which\npoint and I have drawn a circle which looks like this and let's say that this\nlooks like this and let's say that this\nlooks like this and let's say that this is my Epsilon\nis my Epsilon\nis my Epsilon value okay this is my Epsilon value if I\nvalue okay this is my Epsilon value if I\nvalue okay this is my Epsilon value if I say my Min point point is equal to 4\nsay my Min point point is equal to 4\nsay my Min point point is equal to 4 which is again a hyper\nwhich is again a hyper\nwhich is again a hyper parameter that basically means I can if\nparameter that basically means I can if\nparameter that basically means I can if I have four at least four points over\nI have four at least four points over\nI have four at least four points over here near to this particular Circle\nhere near to this particular Circle\nhere near to this particular Circle based on this Epsilon value then what\nbased on this Epsilon value then what\nbased on this Epsilon value then what will happen is that this point this red\nwill happen is that this point this red\nwill happen is that this point this red point will actually become a core\npoint will actually become a core\npoint will actually become a core point a core point which is basically\npoint a core point which is basically\npoint a core point which is basically given over here if it has at least that\ngiven over here if it has at least that\ngiven over here if it has at least that many number of Min points inside or near\nmany number of Min points inside or near\nmany number of Min points inside or near to this particular within this\nto this particular within this\nto this particular within this Epsilon okay within this particular\nEpsilon okay within this particular\nEpsilon okay within this particular cluster suppose this is my cluster with\ncluster suppose this is my cluster with\ncluster suppose this is my cluster with the help of Epsilon I have actually\nthe help of Epsilon I have actually\nthe help of Epsilon I have actually created it is there a particular unit of\ncreated it is there a particular unit of\ncreated it is there a particular unit of Epsilon or we simply take the unit of\nEpsilon or we simply take the unit of\nEpsilon or we simply take the unit of distance no Epsilon value will also get\ndistance no Epsilon value will also get\ndistance no Epsilon value will also get selected through some way I I'll show\nselected through some way I I'll show\nselected through some way I I'll show you I'll show you in the practical\nyou I'll show you in the practical\nyou I'll show you in the practical application don't worry now the next\napplication don't worry now the next\napplication don't worry now the next thing is that let's say let's say I have\nthing is that let's say let's say I have\nthing is that let's say let's say I have another another point over here let's\nanother another point over here let's\nanother another point over here let's say that I have another point over here\nsay that I have another point over here\nsay that I have another point over here and this is my circle with respect to\nand this is my circle with respect to\nand this is my circle with respect to Epsilon I have created it let's say that\nEpsilon I have created it let's say that\nEpsilon I have created it let's say that here I have only one\nhere I have only one\nhere I have only one point I have only one point inside this\npoint I have only one point inside this\npoint I have only one point inside this particular cluster at that point this\nparticular cluster at that point this\nparticular cluster at that point this point becomes something called as border\npoint becomes something called as border\npoint becomes something called as border Point border Point border point also we\nPoint border Point border point also we\nPoint border Point border point also we have discussed over here right so border\nhave discussed over here right so border\nhave discussed over here right so border point is also there so here I'm saying\npoint is also there so here I'm saying\npoint is also there so here I'm saying that at least one at least one if it is\nthat at least one at least one if it is\nthat at least one at least one if it is only one it is present then it will\nonly one it is present then it will\nonly one it is present then it will become a border point if it has Force\nbecome a border point if it has Force\nbecome a border point if it has Force definitely this will become a core Point\ndefinitely this will become a core Point\ndefinitely this will become a core Point core Point like how we have this red\ncore Point like how we have this red\ncore Point like how we have this red color so and there will be one more\ncolor so and there will be one more\ncolor so and there will be one more scenario suppose I have this one cluster\nscenario suppose I have this one cluster\nscenario suppose I have this one cluster let's say this is my Epsilon and suppose\nlet's say this is my Epsilon and suppose\nlet's say this is my Epsilon and suppose if I don't have any points near this\nif I don't have any points near this\nif I don't have any points near this then this will definitely become my\nthen this will definitely become my\nthen this will definitely become my noise point and this noise point will\nnoise point and this noise point will\nnoise point and this noise point will nothing be but this will be a\nnothing be but this will be a\nnothing be but this will be a cluster okay so here I have actually\ncluster okay so here I have actually\ncluster okay so here I have actually discussed about the noise point also so\ndiscussed about the noise point also so\ndiscussed about the noise point also so I hope everybody is able to understand\nI hope everybody is able to understand\nI hope everybody is able to understand the key terms now what is basically\nthe key terms now what is basically\nthe key terms now what is basically happening is that whenever we have a\nhappening is that whenever we have a\nhappening is that whenever we have a noise Point like in this particular\nnoise Point like in this particular\nnoise Point like in this particular scenario we have a noise point and we\nscenario we have a noise point and we\nscenario we have a noise point and we don't find any points inside this any\ndon't find any points inside this any\ndon't find any points inside this any core point or border point if you don't\ncore point or border point if you don't\ncore point or border point if you don't find inside this then it is going to\nfind inside this then it is going to\nfind inside this then it is going to just get neglected that basically means\njust get neglected that basically means\njust get neglected that basically means this is basically treated as an outlier\nthis is basically treated as an outlier\nthis is basically treated as an outlier I hope everybody is able to understand\nI hope everybody is able to understand\nI hope everybody is able to understand here this point will be treated as an\nhere this point will be treated as an\nhere this point will be treated as an outlier or it can also be treated as a\noutlier or it can also be treated as a\noutlier or it can also be treated as a noise point and this will never be taken\nnoise point and this will never be taken\nnoise point and this will never be taken inside a group okay it will never never\ninside a group okay it will never never\ninside a group okay it will never never be taken inside a group suppose I have\nbe taken inside a group suppose I have\nbe taken inside a group suppose I have this set of points which you see\nthis set of points which you see\nthis set of points which you see basically over here red core and all and\nbasically over here red core and all and\nbasically over here red core and all and there is also a border Point by making\nthere is also a border Point by making\nthere is also a border Point by making multiple circles over here here you can\nmultiple circles over here here you can\nmultiple circles over here here you can definitely say that how we are defining\ndefinitely say that how we are defining\ndefinitely say that how we are defining core points and the Border points and\ncore points and the Border points and\ncore points and the Border points and this can be combined into a single group\nthis can be combined into a single group\nthis can be combined into a single group okay this can be combined into a single\nokay this can be combined into a single\nokay this can be combined into a single group because how the connection is now\ngroup because how the connection is now\ngroup because how the connection is now see this this yellow line is basically\nsee this this yellow line is basically\nsee this this yellow line is basically created by one sorry this yellow point\ncreated by one sorry this yellow point\ncreated by one sorry this yellow point is basically created by one Epsilon and\nis basically created by one Epsilon and\nis basically created by one Epsilon and we have one One Core point over here\nwe have one One Core point over here\nwe have one One Core point over here remember over here it should be at least\nremember over here it should be at least\nremember over here it should be at least one core Point okay not one point but\none core Point okay not one point but\none core Point okay not one point but one core point at least if it is having\none core point at least if it is having\none core point at least if it is having one core point then it will become a\none core point then it will become a\none core point then it will become a border point this will become a border\nborder point this will become a border\nborder point this will become a border point that basically means yes this can\npoint that basically means yes this can\npoint that basically means yes this can be the part of this specific group so\nbe the part of this specific group so\nbe the part of this specific group so what we are doing Whenever there is a\nwhat we are doing Whenever there is a\nwhat we are doing Whenever there is a noise we are going to neglect it\nnoise we are going to neglect it\nnoise we are going to neglect it wherever there is a broader and core\nwherever there is a broader and core\nwherever there is a broader and core points we are going to combine it so\npoints we are going to combine it so\npoints we are going to combine it so I'll show you one more diagram which is\nI'll show you one more diagram which is\nI'll show you one more diagram which is an amazing diagram which will help you\nan amazing diagram which will help you\nan amazing diagram which will help you understand more in this a k means\nunderstand more in this a k means\nunderstand more in this a k means clustering and Hier mean clustering now\nclustering and Hier mean clustering now\nclustering and Hier mean clustering now see this everybody now the right hand\nsee this everybody now the right hand\nsee this everybody now the right hand side of diagram that you see is based on\nside of diagram that you see is based on\nside of diagram that you see is based on DB scan clustering and the left hand\nDB scan clustering and the left hand\nDB scan clustering and the left hand side is basically your traditional\nside is basically your traditional\nside is basically your traditional clustering method let's say that this is\nclustering method let's say that this is\nclustering method let's say that this is K means which one do you think is better\nK means which one do you think is better\nK means which one do you think is better over here you see this these all\nover here you see this these all\nover here you see this these all outliers are not combined inside a group\noutliers are not combined inside a group\noutliers are not combined inside a group But whichever are nearer as a core point\nBut whichever are nearer as a core point\nBut whichever are nearer as a core point and the broader point separate separate\nand the broader point separate separate\nand the broader point separate separate groups are actually\ngroups are actually\ngroups are actually created right so this is how amazing a\ncreated right so this is how amazing a\ncreated right so this is how amazing a DB scan clustering is a DB scan\nDB scan clustering is a DB scan\nDB scan clustering is a DB scan clustering is pretty much amazing that\nclustering is pretty much amazing that\nclustering is pretty much amazing that is basically the outcome of this here in\nis basically the outcome of this here in\nis basically the outcome of this here in C's clustering you can see this all\nC's clustering you can see this all\nC's clustering you can see this all these points has also been taken as blue\nthese points has also been taken as blue\nthese points has also been taken as blue color as one group because I'll be\ncolor as one group because I'll be\ncolor as one group because I'll be considering this as one group but here\nconsidering this as one group but here\nconsidering this as one group but here we are able to determine this in a\nwe are able to determine this in a\nwe are able to determine this in a amazing groups so in I'm saying you guys\namazing groups so in I'm saying you guys\namazing groups so in I'm saying you guys directly use DB scan with without\ndirectly use DB scan with without\ndirectly use DB scan with without worrying about anything so now let's\nworrying about anything so now let's\nworrying about anything so now let's focus on the Practical part uh I'm just\nfocus on the Practical part uh I'm just\nfocus on the Practical part uh I'm just going to give you a GitHub link\ngoing to give you a GitHub link\ngoing to give you a GitHub link everybody download the code guys I've\neverybody download the code guys I've\neverybody download the code guys I've given you the GitHub link quickly\ngiven you the GitHub link quickly\ngiven you the GitHub link quickly download and keep your file ready I'm\ndownload and keep your file ready I'm\ndownload and keep your file ready I'm going to open my anaconda prompt\ngoing to open my anaconda prompt\ngoing to open my anaconda prompt probably open my jupyter notbook we'll\nprobably open my jupyter notbook we'll\nprobably open my jupyter notbook we'll do one practical problem I've given you\ndo one practical problem I've given you\ndo one practical problem I've given you the link guys please open it so this is\nthe link guys please open it so this is\nthe link guys please open it so this is what we are going to do today this will\nwhat we are going to do today this will\nwhat we are going to do today this will be amazing here you'll be able to see\nbe amazing here you'll be able to see\nbe amazing here you'll be able to see amazing things how do you come to know\namazing things how do you come to know\namazing things how do you come to know that over fitting or underfitting is\nthat over fitting or underfitting is\nthat over fitting or underfitting is happening you don't know the real value\nhappening you don't know the real value\nhappening you don't know the real value right so in in clustering there will not\nright so in in clustering there will not\nright so in in clustering there will not be any underfitting or overfitting so uh\nbe any underfitting or overfitting so uh\nbe any underfitting or overfitting so uh what all things we'll be importing first\nwhat all things we'll be importing first\nwhat all things we'll be importing first is that we'll try cin clustering we'll\nis that we'll try cin clustering we'll\nis that we'll try cin clustering we'll do silot scoring and then probably we'll\ndo silot scoring and then probably we'll\ndo silot scoring and then probably we'll see the output and um and we'll do DB\nsee the output and um and we'll do DB\nsee the output and um and we'll do DB scan Also let's say DB scan is also\nscan Also let's say DB scan is also\nscan Also let's say DB scan is also there so uh what are the things we have\nthere so uh what are the things we have\nthere so uh what are the things we have basically imported one is the cin\nbasically imported one is the cin\nbasically imported one is the cin clustering one is the Sout samples and\nclustering one is the Sout samples and\nclustering one is the Sout samples and Sout scores these all are present in the\nSout scores these all are present in the\nSout scores these all are present in the SK learn and it is present in metrics\nSK learn and it is present in metrics\nSK learn and it is present in metrics that basically means we use this\nthat basically means we use this\nthat basically means we use this specific parameter to validate\nspecific parameter to validate\nspecific parameter to validate clustering models okay now we'll try to\nclustering models okay now we'll try to\nclustering models okay now we'll try to execute this and apart from that mat\nexecute this and apart from that mat\nexecute this and apart from that mat plot lib we are just trying to import\nplot lib we are just trying to import\nplot lib we are just trying to import numai we are trying to import and all\nnumai we are trying to import and all\nnumai we are trying to import and all here we are executing it perfectly the\nhere we are executing it perfectly the\nhere we are executing it perfectly the next thing is that here the next step is\nnext thing is that here the next step is\nnext thing is that here the next step is that generating the sample data from\nthat generating the sample data from\nthat generating the sample data from make underscore blobs first of all we\nmake underscore blobs first of all we\nmake underscore blobs first of all we are just trying to generate some samples\nare just trying to generate some samples\nare just trying to generate some samples with some two features and we are saying\nwith some two features and we are saying\nwith some two features and we are saying that okay should have four centroids or\nthat okay should have four centroids or\nthat okay should have four centroids or C centroids itself with some features\nC centroids itself with some features\nC centroids itself with some features I'm trying to generate some X and Y data\nI'm trying to generate some X and Y data\nI'm trying to generate some X and Y data randomly and this particular data set\nrandomly and this particular data set\nrandomly and this particular data set will basically be used in performing\nwill basically be used in performing\nwill basically be used in performing clustering algorithms okay forget about\nclustering algorithms okay forget about\nclustering algorithms okay forget about range undor ncore clusters because we\nrange undor ncore clusters because we\nrange undor ncore clusters because we need to try with different different\nneed to try with different different\nneed to try with different different clusters and try to find out the solid\nclusters and try to find out the solid\nclusters and try to find out the solid score so right now I just initialized\nscore so right now I just initialized\nscore so right now I just initialized with 2 3 4 5 6 values it is very simple\nwith 2 3 4 5 6 values it is very simple\nwith 2 3 4 5 6 values it is very simple so if I go and probably see my X data so\nso if I go and probably see my X data so\nso if I go and probably see my X data so my X data will look something like this\nmy X data will look something like this\nmy X data will look something like this so this is my X data with two features\nso this is my X data with two features\nso this is my X data with two features and this is my Y data with one feature\nand this is my Y data with one feature\nand this is my Y data with one feature which is my output which belongs to a\nwhich is my output which belongs to a\nwhich is my output which belongs to a specific class okay so that you can\nspecific class okay so that you can\nspecific class okay so that you can actually do with the help of make\nactually do with the help of make\nactually do with the help of make underscore blobs let's say how to apply\nunderscore blobs let's say how to apply\nunderscore blobs let's say how to apply kin's clustering algorithm so as I said\nkin's clustering algorithm so as I said\nkin's clustering algorithm so as I said that I will be using W CSS W CSS\nthat I will be using W CSS W CSS\nthat I will be using W CSS W CSS basically means within cluster sum of\nbasically means within cluster sum of\nbasically means within cluster sum of square so I'm going to import K means\nsquare so I'm going to import K means\nsquare so I'm going to import K means over here for I in range 1A 11 that\nover here for I in range 1A 11 that\nover here for I in range 1A 11 that basically means I'm going to use\nbasically means I'm going to use\nbasically means I'm going to use different different K values or centroid\ndifferent different K values or centroid\ndifferent different K values or centroid values and try to C which is having the\nvalues and try to C which is having the\nvalues and try to C which is having the minimal wcss value and I'll try to draw\nminimal wcss value and I'll try to draw\nminimal wcss value and I'll try to draw that graph which I had actually shown\nthat graph which I had actually shown\nthat graph which I had actually shown you with respect to Elbow method so here\nyou with respect to Elbow method so here\nyou with respect to Elbow method so here I will basically be also using K means\nI will basically be also using K means\nI will basically be also using K means number of clusters will be I and\nnumber of clusters will be I and\nnumber of clusters will be I and initialization technique I will will be\ninitialization technique I will will be\ninitialization technique I will will be using K means Plus+ so that the points\nusing K means Plus+ so that the points\nusing K means Plus+ so that the points the centroids that are initialized those\nthe centroids that are initialized those\nthe centroids that are initialized those those points are very very far and then\nthose points are very very far and then\nthose points are very very far and then you have random state is equal to zero\nyou have random state is equal to zero\nyou have random state is equal to zero then we do fit and finally we do wcss do\nthen we do fit and finally we do wcss do\nthen we do fit and finally we do wcss do upend cins doin inertia okay this dot\nupend cins doin inertia okay this dot\nupend cins doin inertia okay this dot inertia will give you the distance\ninertia will give you the distance\ninertia will give you the distance between the centroids and all the other\nbetween the centroids and all the other\nbetween the centroids and all the other points and this is what I'm going to\npoints and this is what I'm going to\npoints and this is what I'm going to append in this wcss value and finally\nappend in this wcss value and finally\nappend in this wcss value and finally I'll just plot it now here you can see\nI'll just plot it now here you can see\nI'll just plot it now here you can see that I'm just plotting it obviously by\nthat I'm just plotting it obviously by\nthat I'm just plotting it obviously by seeing this graph this graph looks like\nseeing this graph this graph looks like\nseeing this graph this graph looks like an elbow okay this graph looks like an\nan elbow okay this graph looks like an\nan elbow okay this graph looks like an elbow so the point that I'm actually\nelbow so the point that I'm actually\nelbow so the point that I'm actually going to consider over here see which is\ngoing to consider over here see which is\ngoing to consider over here see which is the last abrupt change so if I talk\nthe last abrupt change so if I talk\nthe last abrupt change so if I talk about the last abrupt change here I have\nabout the last abrupt change here I have\nabout the last abrupt change here I have the specific value with respect to this\nthe specific value with respect to this\nthe specific value with respect to this okay I have one specific value with\nokay I have one specific value with\nokay I have one specific value with respect to this this is my abrupt change\nrespect to this this is my abrupt change\nrespect to this this is my abrupt change from here the changes are normal so I'm\nfrom here the changes are normal so I'm\nfrom here the changes are normal so I'm going to basically select K is equal to\ngoing to basically select K is equal to\ngoing to basically select K is equal to 4 now what I'm actually going to do with\n4 now what I'm actually going to do with\n4 now what I'm actually going to do with the help of sart with the help of s CL\nthe help of sart with the help of s CL\nthe help of sart with the help of s CL score we are going to compare whether K\nscore we are going to compare whether K\nscore we are going to compare whether K is equal to 4 is valid or not so that is\nis equal to 4 is valid or not so that is\nis equal to 4 is valid or not so that is what we are going to do valid or not so\nwhat we are going to do valid or not so\nwhat we are going to do valid or not so here we are going to do this now let's\nhere we are going to do this now let's\nhere we are going to do this now let's go ahead and let's try to see it how we\ngo ahead and let's try to see it how we\ngo ahead and let's try to see it how we are going to do it so here you can see n\nare going to do it so here you can see n\nare going to do it so here you can see n clusters is equal to 4 then I'm actually\nclusters is equal to 4 then I'm actually\nclusters is equal to 4 then I'm actually able to find out the prediction and this\nable to find out the prediction and this\nable to find out the prediction and this is specifically my output okay this is\nis specifically my output okay this is\nis specifically my output okay this is done now see this code okay this code is\ndone now see this code okay this code is\ndone now see this code okay this code is a huge code I have actually taken this\na huge code I have actually taken this\na huge code I have actually taken this code directly from the SK learn page of\ncode directly from the SK learn page of\ncode directly from the SK learn page of Silo if you go and see this this code is\nSilo if you go and see this this code is\nSilo if you go and see this this code is directly given over there but I'm just\ndirectly given over there but I'm just\ndirectly given over there but I'm just going to talk about like what are the\ngoing to talk about like what are the\ngoing to talk about like what are the important things we need to see over\nimportant things we need to see over\nimportant things we need to see over here with respect to different different\nhere with respect to different different\nhere with respect to different different clusters see see this clusters 2 3 4 5 6\nclusters see see this clusters 2 3 4 5 6\nclusters see see this clusters 2 3 4 5 6 I'm going to basically compare whether\nI'm going to basically compare whether\nI'm going to basically compare whether the K value should be four or not with\nthe K value should be four or not with\nthe K value should be four or not with the help of solid scoring so let's go\nthe help of solid scoring so let's go\nthe help of solid scoring so let's go here and here you can see that I'm\nhere and here you can see that I'm\nhere and here you can see that I'm applying this one first I will go with\napplying this one first I will go with\napplying this one first I will go with respect to for Loop for ncore clusters\nrespect to for Loop for ncore clusters\nrespect to for Loop for ncore clusters in range underscore clusters different\nin range underscore clusters different\nin range underscore clusters different different cluster values are there first\ndifferent cluster values are there first\ndifferent cluster values are there first we'll start with two so here you can see\nwe'll start with two so here you can see\nwe'll start with two so here you can see initialize the cluster with and cluster\ninitialize the cluster with and cluster\ninitialize the cluster with and cluster value and a random generator seed of 10\nvalue and a random generator seed of 10\nvalue and a random generator seed of 10 for reproducibility so ncore clusters\nfor reproducibility so ncore clusters\nfor reproducibility so ncore clusters first I take took it as two and then I\nfirst I take took it as two and then I\nfirst I take took it as two and then I did fit predict on X after I did fit\ndid fit predict on X after I did fit\ndid fit predict on X after I did fit predictor on X I'm using this score on X\npredictor on X I'm using this score on X\npredictor on X I'm using this score on X comma cluster label now what this is\ncomma cluster label now what this is\ncomma cluster label now what this is going to do understand in Solo what did\ngoing to do understand in Solo what did\ngoing to do understand in Solo what did we discuss it will it will try to find\nwe discuss it will it will try to find\nwe discuss it will it will try to find out all the Clusters the Clusters over\nout all the Clusters the Clusters over\nout all the Clusters the Clusters over here like this and it'll try to\nhere like this and it'll try to\nhere like this and it'll try to calculate the distance between them\ncalculate the distance between them\ncalculate the distance between them which is the a of I then it'll try to\nwhich is the a of I then it'll try to\nwhich is the a of I then it'll try to compute the B of I then finally it'll\ncompute the B of I then finally it'll\ncompute the B of I then finally it'll try to compute the score and if the\ntry to compute the score and if the\ntry to compute the score and if the value is between minus1 to +1 the more\nvalue is between minus1 to +1 the more\nvalue is between minus1 to +1 the more the Valu is towards + one the more\nthe Valu is towards + one the more\nthe Valu is towards + one the more better it is right so these all things\nbetter it is right so these all things\nbetter it is right so these all things we have already discussed and that is\nwe have already discussed and that is\nwe have already discussed and that is what this specific function will do and\nwhat this specific function will do and\nwhat this specific function will do and this will give my solo average value\nthis will give my solo average value\nthis will give my solo average value over here solid value will be over here\nover here solid value will be over here\nover here solid value will be over here okay this we have done and then we can\nokay this we have done and then we can\nokay this we have done and then we can continuously do it for another another\ncontinuously do it for another another\ncontinuously do it for another another things you can actually find it over\nthings you can actually find it over\nthings you can actually find it over here and this value that you see this\nhere and this value that you see this\nhere and this value that you see this code that you see is nothing nothing so\ncode that you see is nothing nothing so\ncode that you see is nothing nothing so complex okay this is just to display the\ncomplex okay this is just to display the\ncomplex okay this is just to display the data properly in the form of graphs okay\ndata properly in the form of graphs okay\ndata properly in the form of graphs okay in the form of graphs so again I'm\nin the form of graphs so again I'm\nin the form of graphs so again I'm telling you I did not write this code\ntelling you I did not write this code\ntelling you I did not write this code I've directly taken it from the uh SK\nI've directly taken it from the uh SK\nI've directly taken it from the uh SK learn page of solid okay so just try to\nlearn page of solid okay so just try to\nlearn page of solid okay so just try to see this particular uh plotting diagrams\nsee this particular uh plotting diagrams\nsee this particular uh plotting diagrams and all that you can definitely figure\nand all that you can definitely figure\nand all that you can definitely figure out but let's see I will try to execute\nout but let's see I will try to execute\nout but let's see I will try to execute it and try to find out the output now\nit and try to find out the output now\nit and try to find out the output now see for ncore cluster is equal to 2 the\nsee for ncore cluster is equal to 2 the\nsee for ncore cluster is equal to 2 the average solid score is 70 I told you the\naverage solid score is 70 I told you the\naverage solid score is 70 I told you the value will be between -1 to +1 and I'm\nvalue will be between -1 to +1 and I'm\nvalue will be between -1 to +1 and I'm actually getting 704 which is very very\nactually getting 704 which is very very\nactually getting 704 which is very very good and then for ncore cluster is equal\ngood and then for ncore cluster is equal\ngood and then for ncore cluster is equal to 3 588 then ncore cluster is equal to\nto 3 588 then ncore cluster is equal to\nto 3 588 then ncore cluster is equal to 4 I'm getting 65 which is pretty much\n4 I'm getting 65 which is pretty much\n4 I'm getting 65 which is pretty much amazing and then for ncore cluster equal\namazing and then for ncore cluster equal\namazing and then for ncore cluster equal to 5 the average score is 563 and ncore\nto 5 the average score is 563 and ncore\nto 5 the average score is 563 and ncore cluster is equal to 6 you are saying\ncluster is equal to 6 you are saying\ncluster is equal to 6 you are saying .45 here directly you can actually say\n.45 here directly you can actually say\n.45 here directly you can actually say that fine for _ cluster equal to 2 I'm\nthat fine for _ cluster equal to 2 I'm\nthat fine for _ cluster equal to 2 I'm getting an amazing score of\ngetting an amazing score of\ngetting an amazing score of 704 obviously you're you're getting the\n704 obviously you're you're getting the\n704 obviously you're you're getting the highest value over this so should we\nhighest value over this so should we\nhighest value over this so should we select ncore cluster isal to two Okay we\nselect ncore cluster isal to two Okay we\nselect ncore cluster isal to two Okay we should not directly conclude from it\nshould not directly conclude from it\nshould not directly conclude from it because here we need to also see that\nbecause here we need to also see that\nbecause here we need to also see that any feature value or any cluster value\nany feature value or any cluster value\nany feature value or any cluster value is also coming as negative value that\nis also coming as negative value that\nis also coming as negative value that also we need to check so here we will go\nalso we need to check so here we will go\nalso we need to check so here we will go down over here you will see the first\ndown over here you will see the first\ndown over here you will see the first one over here with respect to the first\none over here with respect to the first\none over here with respect to the first one you see that I'm get getting the\none you see that I'm get getting the\none you see that I'm get getting the value from 0 to 1 it is not going going\nvalue from 0 to 1 it is not going going\nvalue from 0 to 1 it is not going going to Min -.1 so definitely two clusters\nto Min -.1 so definitely two clusters\nto Min -.1 so definitely two clusters was able to solve the problem so I'll\nwas able to solve the problem so I'll\nwas able to solve the problem so I'll keep it like this with me I definitely\nkeep it like this with me I definitely\nkeep it like this with me I definitely have a chance that this may this may\nhave a chance that this may this may\nhave a chance that this may this may perform well I may have a chance that\nperform well I may have a chance that\nperform well I may have a chance that this K uh K is equal to 2 May perform\nthis K uh K is equal to 2 May perform\nthis K uh K is equal to 2 May perform well okay so I may have a chance let's\nwell okay so I may have a chance let's\nwell okay so I may have a chance let's see to the next one to the next one over\nsee to the next one to the next one over\nsee to the next one to the next one over here you can see that for one of the\nhere you can see that for one of the\nhere you can see that for one of the cluster the value is negative if the\ncluster the value is negative if the\ncluster the value is negative if the value is negative that basically means\nvalue is negative that basically means\nvalue is negative that basically means the AI is obviously greater than b ofi\nthe AI is obviously greater than b ofi\nthe AI is obviously greater than b ofi so I'm not going to prer this because it\nso I'm not going to prer this because it\nso I'm not going to prer this because it is having some negative values even\nis having some negative values even\nis having some negative values even though my cluster looks better but again\nthough my cluster looks better but again\nthough my cluster looks better but again understand what is the problem with\nunderstand what is the problem with\nunderstand what is the problem with respect to this cluster is that if I\nrespect to this cluster is that if I\nrespect to this cluster is that if I take this cluster and probably compute\ntake this cluster and probably compute\ntake this cluster and probably compute the distance between this point to this\nthe distance between this point to this\nthe distance between this point to this point and if I probably compute from\npoint and if I probably compute from\npoint and if I probably compute from this point to this point or this point\nthis point to this point or this point\nthis point to this point or this point to this point this point is obviously\nto this point this point is obviously\nto this point this point is obviously nearer to this right it is obviously\nnearer to this right it is obviously\nnearer to this right it is obviously nearer to this so that is the reason why\nnearer to this so that is the reason why\nnearer to this so that is the reason why I'm getting a negative value over here\nI'm getting a negative value over here\nI'm getting a negative value over here okay negative value over here this is my\nokay negative value over here this is my\nokay negative value over here this is my uh output my score this point that you\nuh output my score this point that you\nuh output my score this point that you see dotted points this is my score 58\nsee dotted points this is my score 58\nsee dotted points this is my score 58 what whatever it is this is basically my\nwhat whatever it is this is basically my\nwhat whatever it is this is basically my score so obviously this basically\nscore so obviously this basically\nscore so obviously this basically indicates that this point is near the\nindicates that this point is near the\nindicates that this point is near the other cluster point is nearer to this so\nother cluster point is nearer to this so\nother cluster point is nearer to this so I'm actually getting a negative value\nI'm actually getting a negative value\nI'm actually getting a negative value right so this you really need to\nright so this you really need to\nright so this you really need to understand okay now similarly if I go\nunderstand okay now similarly if I go\nunderstand okay now similarly if I go with respect to ncore Cluster is equal\nwith respect to ncore Cluster is equal\nwith respect to ncore Cluster is equal to 4 this looks good because here I\nto 4 this looks good because here I\nto 4 this looks good because here I don't have any negative value and here\ndon't have any negative value and here\ndon't have any negative value and here you can see how cooly it has basically\nyou can see how cooly it has basically\nyou can see how cooly it has basically divided the points amazing inly with the\ndivided the points amazing inly with the\ndivided the points amazing inly with the help of k equal to 4 right and similarly\nhelp of k equal to 4 right and similarly\nhelp of k equal to 4 right and similarly if I go with five obviously you can see\nif I go with five obviously you can see\nif I go with five obviously you can see some negative values are here some\nsome negative values are here some\nsome negative values are here some dotted line negative value are there\ndotted line negative value are there\ndotted line negative value are there with respect to six you also have some\nwith respect to six you also have some\nwith respect to six you also have some negative values so definitely I'll not\nnegative values so definitely I'll not\nnegative values so definitely I'll not go with six I may either go with four or\ngo with six I may either go with four or\ngo with six I may either go with four or I may either go with two now whenever\nI may either go with two now whenever\nI may either go with two now whenever you have this options always take a\nyou have this options always take a\nyou have this options always take a bigger number instead of two take four\nbigger number instead of two take four\nbigger number instead of two take four because four is greater than two because\nbecause four is greater than two because\nbecause four is greater than two because it will be able to create a generalized\nit will be able to create a generalized\nit will be able to create a generalized model so from this I'm actually going to\nmodel so from this I'm actually going to\nmodel so from this I'm actually going to take and is equal to 4 K is equal to 4\ntake and is equal to 4 K is equal to 4\ntake and is equal to 4 K is equal to 4 now should we compare with this with the\nnow should we compare with this with the\nnow should we compare with this with the elbow method here also I got four right\nelbow method here also I got four right\nelbow method here also I got four right so both are actually matching so this\nso both are actually matching so this\nso both are actually matching so this indicates that with the help of this\nindicates that with the help of this\nindicates that with the help of this clustering this siluette score we can\nclustering this siluette score we can\nclustering this siluette score we can definitely come to a conclusion and\ndefinitely come to a conclusion and\ndefinitely come to a conclusion and validate our clustering model in an\nvalidate our clustering model in an\nvalidate our clustering model in an amazing way so I hope everybody is able\namazing way so I hope everybody is able\namazing way so I hope everybody is able to understand and this way you basically\nto understand and this way you basically\nto understand and this way you basically validate a model and definitely you can\nvalidate a model and definitely you can\nvalidate a model and definitely you can try it out you can understand this code\ntry it out you can understand this code\ntry it out you can understand this code definitely I but till here you have\ndefinitely I but till here you have\ndefinitely I but till here you have understood that here I'm going to get\nunderstood that here I'm going to get\nunderstood that here I'm going to get the average value then for iore clusters\nthe average value then for iore clusters\nthe average value then for iore clusters whatever cluster this is matching it is\nwhatever cluster this is matching it is\nwhatever cluster this is matching it is just mapping over there and it is\njust mapping over there and it is\njust mapping over there and it is basically giving so this was the session\nbasically giving so this was the session\nbasically giving so this was the session and uh yes in today's session we\nand uh yes in today's session we\nand uh yes in today's session we efficiently covered many topics we\nefficiently covered many topics we\nefficiently covered many topics we covered kin hierle clustering solid\ncovered kin hierle clustering solid\ncovered kin hierle clustering solid score DB clustering in tomorrow's\nscore DB clustering in tomorrow's\nscore DB clustering in tomorrow's session the topics that are probably\nsession the topics that are probably\nsession the topics that are probably pending is first I'll start with svm and\npending is first I'll start with svm and\npending is first I'll start with svm and svr second I will go ahead with XG boost\nsvr second I will go ahead with XG boost\nsvr second I will go ahead with XG boost and and third I will cover up PCA let's\nand and third I will cover up PCA let's\nand and third I will cover up PCA let's see whether I'll be able to complete\nsee whether I'll be able to complete\nsee whether I'll be able to complete this session uh one one amazing thing\nthis session uh one one amazing thing\nthis session uh one one amazing thing that I want to teach you guys because\nthat I want to teach you guys because\nthat I want to teach you guys because many people ask me the definition of\nmany people ask me the definition of\nmany people ask me the definition of bias and variance so guys uh many people\nbias and variance so guys uh many people\nbias and variance so guys uh many people get confused when we talk about bias and\nget confused when we talk about bias and\nget confused when we talk about bias and variance you know because let's say that\nvariance you know because let's say that\nvariance you know because let's say that uh I have a model for the training data\nuh I have a model for the training data\nuh I have a model for the training data set it gives us somewhere around 90%\nset it gives us somewhere around 90%\nset it gives us somewhere around 90% accuracy let's say I'm getting a 90%\naccuracy let's say I'm getting a 90%\naccuracy let's say I'm getting a 90% accuracy for the test data I may\naccuracy for the test data I may\naccuracy for the test data I may probably getting somewhere around 70%\nprobably getting somewhere around 70%\nprobably getting somewhere around 70% accuracy now tell me which scenario is\naccuracy now tell me which scenario is\naccuracy now tell me which scenario is basically this most of the people will\nbasically this most of the people will\nbasically this most of the people will be saying that okay fine it is\nbe saying that okay fine it is\nbe saying that okay fine it is overfitting now when I say overfitting I\noverfitting now when I say overfitting I\noverfitting now when I say overfitting I basically mention overfitting by low\nbasically mention overfitting by low\nbasically mention overfitting by low bias and high\nbias and high\nbias and high variance right so many people get\nvariance right so many people get\nvariance right so many people get confused Krish tell me just the exact\nconfused Krish tell me just the exact\nconfused Krish tell me just the exact definition of bias and variance low bias\ndefinition of bias and variance low bias\ndefinition of bias and variance low bias obviously you are saying that because\nobviously you are saying that because\nobviously you are saying that because the training is performed like the model\nthe training is performed like the model\nthe training is performed like the model is performing well with the help of\nis performing well with the help of\nis performing well with the help of training data set but with respect to\ntraining data set but with respect to\ntraining data set but with respect to the test data set the model is not\nthe test data set the model is not\nthe test data set the model is not performing well with respect to training\nperforming well with respect to training\nperforming well with respect to training data set why do we always say bias and\ndata set why do we always say bias and\ndata set why do we always say bias and with respect to test data set why do we\nwith respect to test data set why do we\nwith respect to test data set why do we always say variance so for this you need\nalways say variance so for this you need\nalways say variance so for this you need to understand the definition of bias so\nto understand the definition of bias so\nto understand the definition of bias so let me write down the definition of bias\nlet me write down the definition of bias\nlet me write down the definition of bias over here so here I can definitely write\nover here so here I can definitely write\nover here so here I can definitely write that bias it is a\nthat bias it is a\nthat bias it is a phenomena that\nphenomena that\nphenomena that skews the\nskews the\nskews the result of an\nalgorithm in\nalgorithm in favor in favor or against an\nfavor in favor or against an\nfavor in favor or against an idea against an idea I'll make you\nidea against an idea I'll make you\nidea against an idea I'll make you understand the definition uh um but\nunderstand the definition uh um but\nunderstand the definition uh um but understand the understand understand\nunderstand the understand understand\nunderstand the understand understand what I have actually written over here\nwhat I have actually written over here\nwhat I have actually written over here it is a phenomena that skewes the result\nit is a phenomena that skewes the result\nit is a phenomena that skewes the result of an algorithm in favor or against an\nof an algorithm in favor or against an\nof an algorithm in favor or against an idea whenever I say this specific idea\nidea whenever I say this specific idea\nidea whenever I say this specific idea this idea I will just talk about the\nthis idea I will just talk about the\nthis idea I will just talk about the training data set initially now when we\ntraining data set initially now when we\ntraining data set initially now when we train a specific model suppose if I have\ntrain a specific model suppose if I have\ntrain a specific model suppose if I have this specific model over\nthis specific model over\nthis specific model over here and I'm training with this specific\nhere and I'm training with this specific\nhere and I'm training with this specific training data set so this is my training\ntraining data set so this is my training\ntraining data set so this is my training data set now based on the definition\ndata set now based on the definition\ndata set now based on the definition what does it basically say it is a\nwhat does it basically say it is a\nwhat does it basically say it is a phenomenon that skews the result of an\nphenomenon that skews the result of an\nphenomenon that skews the result of an algorithm in favor or against an idea or\nalgorithm in favor or against an idea or\nalgorithm in favor or against an idea or a this specific training data set so\na this specific training data set so\na this specific training data set so even though I'm training this particular\neven though I'm training this particular\neven though I'm training this particular model with this training data set\nmodel with this training data set\nmodel with this training data set with this data set it may it may be in\nwith this data set it may it may be in\nwith this data set it may it may be in favor of that or it may be against of\nfavor of that or it may be against of\nfavor of that or it may be against of that that basically means it may perform\nthat that basically means it may perform\nthat that basically means it may perform well it may not perform well if it is\nwell it may not perform well if it is\nwell it may not perform well if it is not performing well that basically means\nnot performing well that basically means\nnot performing well that basically means the accuracy is down if the accuracy is\nthe accuracy is down if the accuracy is\nthe accuracy is down if the accuracy is better at that point of time what will\nbetter at that point of time what will\nbetter at that point of time what will say see if the accuracy is better that\nsay see if the accuracy is better that\nsay see if the accuracy is better that time what we'll say we we'll come up\ntime what we'll say we we'll come up\ntime what we'll say we we'll come up with two terms from here obviously you\nwith two terms from here obviously you\nwith two terms from here obviously you understand okay there are two scenarios\nunderstand okay there are two scenarios\nunderstand okay there are two scenarios of bias now here if it is in favor that\nof bias now here if it is in favor that\nof bias now here if it is in favor that basically means it is performing well\nbasically means it is performing well\nbasically means it is performing well with respect to the training data set I\nwith respect to the training data set I\nwith respect to the training data set I will basically say that it has high bu\nwill basically say that it has high bu\nwill basically say that it has high bu if it is not able to perform well with\nif it is not able to perform well with\nif it is not able to perform well with the training data set then here I will\nthe training data set then here I will\nthe training data set then here I will say it as low\nsay it as low\nsay it as low bias I hope everybody is able to\nbias I hope everybody is able to\nbias I hope everybody is able to understand in this specific thing\nunderstand in this specific thing\nunderstand in this specific thing because many many many people has this\nbecause many many many people has this\nbecause many many many people has this kind kind of confusion now similarly if\nkind kind of confusion now similarly if\nkind kind of confusion now similarly if I talk about variance let's say about\nI talk about variance let's say about\nI talk about variance let's say about variance because you need to understand\nvariance because you need to understand\nvariance because you need to understand the definition a definition is very much\nthe definition a definition is very much\nthe definition a definition is very much important okay if I if I just talk about\nimportant okay if I if I just talk about\nimportant okay if I if I just talk about the definition of variance I'm just\nthe definition of variance I'm just\nthe definition of variance I'm just going to refer like this the variance\ngoing to refer like this the variance\ngoing to refer like this the variance refers to the changes in the model when\nrefers to the changes in the model when\nrefers to the changes in the model when using when using different\nusing when using different\nusing when using different portion of the\nportion of the\nportion of the training or test\ntraining or test\ntraining or test data now let's understand this\ndata now let's understand this\ndata now let's understand this particular\nparticular\nparticular definition variance refers to the\ndefinition variance refers to the\ndefinition variance refers to the changes in the model when using\nchanges in the model when using\nchanges in the model when using different proportion of the test\ndifferent proportion of the test\ndifferent proportion of the test training data or test data we obviously\ntraining data or test data we obviously\ntraining data or test data we obviously know that whenever initially if I have a\nknow that whenever initially if I have a\nknow that whenever initially if I have a model understand from the definition\nmodel understand from the definition\nmodel understand from the definition everything will make sense I am\neverything will make sense I am\neverything will make sense I am basically training initially with the\nbasically training initially with the\nbasically training initially with the training\ntraining\ntraining data okay because we divide our data set\ndata okay because we divide our data set\ndata okay because we divide our data set see our data set whenever we are working\nsee our data set whenever we are working\nsee our data set whenever we are working with we divide this into two parts one\nwith we divide this into two parts one\nwith we divide this into two parts one is our train data and test data okay\nis our train data and test data okay\nis our train data and test data okay because this is a tra test data is a\nbecause this is a tra test data is a\nbecause this is a tra test data is a part of that particular data set right\npart of that particular data set right\npart of that particular data set right and suppose in this particular training\nand suppose in this particular training\nand suppose in this particular training data it gets trained and performs well\ndata it gets trained and performs well\ndata it gets trained and performs well here I'm actually talking about bias but\nhere I'm actually talking about bias but\nhere I'm actually talking about bias but when we come with respect to the\nwhen we come with respect to the\nwhen we come with respect to the prediction of the specific model at that\nprediction of the specific model at that\nprediction of the specific model at that point of time I can use other training\npoint of time I can use other training\npoint of time I can use other training data that basically means that training\ndata that basically means that training\ndata that basically means that training data may not be similar or I can also\ndata may not be similar or I can also\ndata may not be similar or I can also use test data now in this test data what\nuse test data now in this test data what\nuse test data now in this test data what we do we do some kind of predictions\nwe do we do some kind of predictions\nwe do we do some kind of predictions these are my predictions and in this\nthese are my predictions and in this\nthese are my predictions and in this prediction again I may get two\nprediction again I may get two\nprediction again I may get two scenario I may get two scenario which is\nscenario I may get two scenario which is\nscenario I may get two scenario which is basically mentioned by variance it\nbasically mentioned by variance it\nbasically mentioned by variance it refers to the changes in the model when\nrefers to the changes in the model when\nrefers to the changes in the model when using when using different portion of\nusing when using different portion of\nusing when using different portion of the training or test data refers to the\nthe training or test data refers to the\nthe training or test data refers to the changes basically means whether it is\nchanges basically means whether it is\nchanges basically means whether it is able to give a good prediction or wrong\nable to give a good prediction or wrong\nable to give a good prediction or wrong predictions that's it so in this\npredictions that's it so in this\npredictions that's it so in this particular scenario if it gives a good\nparticular scenario if it gives a good\nparticular scenario if it gives a good prediction I may definitely say it as\nprediction I may definitely say it as\nprediction I may definitely say it as low variance that basically means the\nlow variance that basically means the\nlow variance that basically means the accuracy with the accuracy with respect\naccuracy with the accuracy with respect\naccuracy with the accuracy with respect to the test data is also very good if I\nto the test data is also very good if I\nto the test data is also very good if I probably get a bad if I probably get a\nprobably get a bad if I probably get a\nprobably get a bad if I probably get a bad accuracy at that time I basically\nbad accuracy at that time I basically\nbad accuracy at that time I basically say it as high variance so if I talk\nsay it as high variance so if I talk\nsay it as high variance so if I talk about three scenarios over here let's\nabout three scenarios over here let's\nabout three scenarios over here let's say this is my model one and this is my\nsay this is my model one and this is my\nsay this is my model one and this is my model\nmodel\nmodel two and this is my model\ntwo and this is my model\ntwo and this is my model three now in this scenario let's\nthree now in this scenario let's\nthree now in this scenario let's consider that my model one has the\nconsider that my model one has the\nconsider that my model one has the training\ntraining\ntraining accuracy of 90% and test accuracy of\naccuracy of 90% and test accuracy of\naccuracy of 90% and test accuracy of 75% similarly I have here as my train\n75% similarly I have here as my train\n75% similarly I have here as my train accuracy of 60% and my test accuracy\naccuracy of 60% and my test accuracy\naccuracy of 60% and my test accuracy of\nof\nof 55% now similarly if I have my train\n55% now similarly if I have my train\n55% now similarly if I have my train accuracy of 90% And my test accuracy of\naccuracy of 90% And my test accuracy of\naccuracy of 90% And my test accuracy of 92% now tell me what what things you\n92% now tell me what what things you\n92% now tell me what what things you will be getting here obviously you can\nwill be getting here obviously you can\nwill be getting here obviously you can directly say that fine your training\ndirectly say that fine your training\ndirectly say that fine your training accuracy is better now you're talking\naccuracy is better now you're talking\naccuracy is better now you're talking about bias so this basically indicates\nabout bias so this basically indicates\nabout bias so this basically indicates that this has low\nthat this has low\nthat this has low bias and since your test accuracy is bad\nbias and since your test accuracy is bad\nbias and since your test accuracy is bad because it is when compared to the train\nbecause it is when compared to the train\nbecause it is when compared to the train accuracy it is less so here you are\naccuracy it is less so here you are\naccuracy it is less so here you are basically going to say high\nbasically going to say high\nbasically going to say high variance understand with respect to the\nvariance understand with respect to the\nvariance understand with respect to the definition similarly over here what\ndefinition similarly over here what\ndefinition similarly over here what you'll say high\nyou'll say high\nyou'll say high bias High variance because obviously it\nbias High variance because obviously it\nbias High variance because obviously it is not performing\nis not performing\nis not performing well this is another scenario last the\nwell this is another scenario last the\nwell this is another scenario last the last scenario is that this is the\nlast scenario is that this is the\nlast scenario is that this is the scenario that we want because it is low\nscenario that we want because it is low\nscenario that we want because it is low bias and low variance\nbias and low variance\nbias and low variance okay many many people have basically\nokay many many people have basically\nokay many many people have basically asked me the definition with respect to\nasked me the definition with respect to\nasked me the definition with respect to bias and variance and here I've actually\nbias and variance and here I've actually\nbias and variance and here I've actually discussed and this indicates this gives\ndiscussed and this indicates this gives\ndiscussed and this indicates this gives me a generalized model and this is what\nme a generalized model and this is what\nme a generalized model and this is what is our aim when we are working as a data\nis our aim when we are working as a data\nis our aim when we are working as a data scientist so I hope you have understood\nscientist so I hope you have understood\nscientist so I hope you have understood the basic difference between V bias and\nthe basic difference between V bias and\nthe basic difference between V bias and variance and I was able to give you lot\nvariance and I was able to give you lot\nvariance and I was able to give you lot of examples lot of understanding with\nof examples lot of understanding with\nof examples lot of understanding with respect to this so I hope you have\nrespect to this so I hope you have\nrespect to this so I hope you have actually got this particular uh\nactually got this particular uh\nactually got this particular uh understanding of this uh two terms which\nunderstanding of this uh two terms which\nunderstanding of this uh two terms which we specifically talk about high bias low\nwe specifically talk about high bias low\nwe specifically talk about high bias low bias High variance low variance right so\nbias High variance low variance right so\nbias High variance low variance right so this was it from my side guys uh and uh\nthis was it from my side guys uh and uh\nthis was it from my side guys uh and uh I hope you have understood\nI hope you have understood\nI hope you have understood this\nokay so let's take let's consider a data\nokay so let's take let's consider a data set credit\nset credit\nset credit and let's say this is a\nand let's say this is a\nand let's say this is a approval so we are going to take this\napproval so we are going to take this\napproval so we are going to take this sample data set and understand how does\nsample data set and understand how does\nsample data set and understand how does XG boost work suppose salary is less\nXG boost work suppose salary is less\nXG boost work suppose salary is less than or equal to 50 and the credit is\nthan or equal to 50 and the credit is\nthan or equal to 50 and the credit is bad so approval the loan approval will\nbad so approval the loan approval will\nbad so approval the loan approval will be zero that basically means he he or\nbe zero that basically means he he or\nbe zero that basically means he he or she will not get if it is less than or\nshe will not get if it is less than or\nshe will not get if it is less than or equal to 50 if the credit score is good\nequal to 50 if the credit score is good\nequal to 50 if the credit score is good then probably approval will be one if it\nthen probably approval will be one if it\nthen probably approval will be one if it is less than or equal to 50 if it is\nis less than or equal to 50 if it is\nis less than or equal to 50 if it is good\ngood\ngood again then it is going to get one if it\nagain then it is going to get one if it\nagain then it is going to get one if it is greater than\nis greater than\nis greater than 50 and if it is bad then obviously\n50 and if it is bad then obviously\n50 and if it is bad then obviously approval will be\nzero if it is greater than\nzero if it is greater than 50 if it is good we are going to get it\n50 if it is good we are going to get it\n50 if it is good we are going to get it as one if it is greater than\nas one if it is greater than\nas one if it is greater than 50k and probably if it is normal then\n50k and probably if it is normal then\n50k and probably if it is normal then also we are going to get\nalso we are going to get\nalso we are going to get it so this is this is my data set so how\nit so this is this is my data set so how\nit so this is this is my data set so how does XG boost classifier work understand\ndoes XG boost classifier work understand\ndoes XG boost classifier work understand the full form of XG boost is\nthe full form of XG boost is\nthe full form of XG boost is Extreme gradient\nExtreme gradient\nExtreme gradient boosting extreme gradient boosting so we\nboosting extreme gradient boosting so we\nboosting extreme gradient boosting so we will basically understand about extreme\nwill basically understand about extreme\nwill basically understand about extreme gradient boosting now extreme gradient\ngradient boosting now extreme gradient\ngradient boosting now extreme gradient boosting uh will be actually used to\nboosting uh will be actually used to\nboosting uh will be actually used to solve both classification and the\nsolve both classification and the\nsolve both classification and the regression problem statement so first of\nregression problem statement so first of\nregression problem statement so first of all let's understand how it is basically\nall let's understand how it is basically\nall let's understand how it is basically exib basically how it actually if you if\nexib basically how it actually if you if\nexib basically how it actually if you if you just talk about XG boost you\nyou just talk about XG boost you\nyou just talk about XG boost you understand that it is a boosting\nunderstand that it is a boosting\nunderstand that it is a boosting technique and internally it tries to use\ntechnique and internally it tries to use\ntechnique and internally it tries to use decision tree so how does this decision\ndecision tree so how does this decision\ndecision tree so how does this decision Tre is basically getting constructed in\nTre is basically getting constructed in\nTre is basically getting constructed in the case of XV boost and how it is\nthe case of XV boost and how it is\nthe case of XV boost and how it is basically solved we are going to discuss\nbasically solved we are going to discuss\nbasically solved we are going to discuss about it so whenever we start exib boost\nabout it so whenever we start exib boost\nabout it so whenever we start exib boost classifier understand that first of all\nclassifier understand that first of all\nclassifier understand that first of all we create a specific base model suppose\nwe create a specific base model suppose\nwe create a specific base model suppose if I say this is my base model and this\nif I say this is my base model and this\nif I say this is my base model and this base model will be a weak learner okay\nbase model will be a weak learner okay\nbase model will be a weak learner okay and this base model will always give an\nand this base model will always give an\nand this base model will always give an output of probability of 0.5 in the case\noutput of probability of 0.5 in the case\noutput of probability of 0.5 in the case of classification problem so suppose if\nof classification problem so suppose if\nof classification problem so suppose if I say this is probability 0.5 then I\nI say this is probability 0.5 then I\nI say this is probability 0.5 then I will try to create a field over here\nwill try to create a field over here\nwill try to create a field over here this field is called as residual field\nthis field is called as residual field\nthis field is called as residual field so first base model what I'm going to do\nso first base model what I'm going to do\nso first base model what I'm going to do any data set that you give from here to\nany data set that you give from here to\nany data set that you give from here to train it will always give you the output\ntrain it will always give you the output\ntrain it will always give you the output as 0.5 so this is just a dummy base\nas 0.5 so this is just a dummy base\nas 0.5 so this is just a dummy base model now tell me if my probability\nmodel now tell me if my probability\nmodel now tell me if my probability output is is 0.5 if I want to calculate\noutput is is 0.5 if I want to calculate\noutput is is 0.5 if I want to calculate the residual that basically means I need\nthe residual that basically means I need\nthe residual that basically means I need to subtract approval minus this\nto subtract approval minus this\nto subtract approval minus this particular value so what will be the\nparticular value so what will be the\nparticular value so what will be the value over here 0 -.5 will be\nvalue over here 0 -.5 will be\nvalue over here 0 -.5 will be -.5 1 -.5 will be5 1 -.5 will\n-.5 1 -.5 will be5 1 -.5 will\n-.5 1 -.5 will be5 1 -.5 will be5 and 0 -.5 will be -.5 and this 1 -.5\nbe5 and 0 -.5 will be -.5 and this 1 -.5\nbe5 and 0 -.5 will be -.5 and this 1 -.5 will\nwill\nwill be uh 0.5 and this will also be 0.5\nbe uh 0.5 and this will also be 0.5\nbe uh 0.5 and this will also be 0.5 let's consider that I have one more\nlet's consider that I have one more\nlet's consider that I have one more record uh and this specific record can\nrecord uh and this specific record can\nrecord uh and this specific record can be anything uh because I want to keep\nbe anything uh because I want to keep\nbe anything uh because I want to keep some more records over here so let's\nsome more records over here so let's\nsome more records over here so let's consider that I have one more record\nconsider that I have one more record\nconsider that I have one more record which is less than or equal to 50K and\nwhich is less than or equal to 50K and\nwhich is less than or equal to 50K and if the credit scod is normal you're\nif the credit scod is normal you're\nif the credit scod is normal you're going to get zero so here also if I try\ngoing to get zero so here also if I try\ngoing to get zero so here also if I try to find out the residual it will be\nto find out the residual it will be\nto find out the residual it will be minus5 now the first step I hope\nminus5 now the first step I hope\nminus5 now the first step I hope everybody's understood we have to create\neverybody's understood we have to create\neverybody's understood we have to create a base model okay this base model is\na base model okay this base model is\na base model okay this base model is very much important because we have to\nvery much important because we have to\nvery much important because we have to create all the decision Tree in a\ncreate all the decision Tree in a\ncreate all the decision Tree in a sequential manner so the first\nsequential manner so the first\nsequential manner so the first sequential base tree which is again this\nsequential base tree which is again this\nsequential base tree which is again this is also a decision tree kind of thing\nis also a decision tree kind of thing\nis also a decision tree kind of thing you can consider but this is a base\nyou can consider but this is a base\nyou can consider but this is a base model which takes any inputs and gives\nmodel which takes any inputs and gives\nmodel which takes any inputs and gives by default the probability as 05 now\nby default the probability as 05 now\nby default the probability as 05 now let's go ahead and understand what are\nlet's go ahead and understand what are\nlet's go ahead and understand what are the steps in constructing decision tree\nthe steps in constructing decision tree\nthe steps in constructing decision tree after creating the base model the first\nafter creating the base model the first\nafter creating the base model the first step is that create uh binary decision\nstep is that create uh binary decision\nstep is that create uh binary decision tree so I'm going to write it down all\ntree so I'm going to write it down all\ntree so I'm going to write it down all the steps please make sure that you note\nthe steps please make sure that you note\nthe steps please make sure that you note it down so so create a binary tree\nit down so so create a binary tree\nit down so so create a binary tree binary decision tree using the features\nbinary decision tree using the features\nbinary decision tree using the features second step we basically Define we we we\nsecond step we basically Define we we we\nsecond step we basically Define we we we say it as okay Second Step what we do we\nsay it as okay Second Step what we do we\nsay it as okay Second Step what we do we actually calculate the similarity weight\nactually calculate the similarity weight\nactually calculate the similarity weight we calculate the similarity weight I'll\nwe calculate the similarity weight I'll\nwe calculate the similarity weight I'll talk about this similarity weight what\ntalk about this similarity weight what\ntalk about this similarity weight what exactly it is if I want to use this a\nexactly it is if I want to use this a\nexactly it is if I want to use this a formula it is summation of residual\nformula it is summation of residual\nformula it is summation of residual Square\nSquare\nSquare divided\ndivided\ndivided by summation of probability 1 minus\nby summation of probability 1 minus\nby summation of probability 1 minus probability plus Lambda I'll talk about\nprobability plus Lambda I'll talk about\nprobability plus Lambda I'll talk about this what is exactly Lambda it is the\nthis what is exactly Lambda it is the\nthis what is exactly Lambda it is the kind of hyperparameter again so that it\nkind of hyperparameter again so that it\nkind of hyperparameter again so that it does not overfit the third thing is that\ndoes not overfit the third thing is that\ndoes not overfit the third thing is that we calculate the Information Gain okay\nwe calculate the Information Gain okay\nwe calculate the Information Gain okay Information Gain so these are the steps\nInformation Gain so these are the steps\nInformation Gain so these are the steps we basically use in constructing or in\nwe basically use in constructing or in\nwe basically use in constructing or in solving uh in creating an HD boost\nsolving uh in creating an HD boost\nsolving uh in creating an HD boost classifier the first step is that we\nclassifier the first step is that we\nclassifier the first step is that we create a inary decision tree using the\ncreate a inary decision tree using the\ncreate a inary decision tree using the feature then we go ahead with\nfeature then we go ahead with\nfeature then we go ahead with calculating the similarity weight and\ncalculating the similarity weight and\ncalculating the similarity weight and finally we go ahead and calculate the\nfinally we go ahead and calculate the\nfinally we go ahead and calculate the information gain so how does it go ahead\ninformation gain so how does it go ahead\ninformation gain so how does it go ahead let's understand over here and let's try\nlet's understand over here and let's try\nlet's understand over here and let's try to find out okay now let's go ahead and\nto find out okay now let's go ahead and\nto find out okay now let's go ahead and let's try to construct the decision tree\nlet's try to construct the decision tree\nlet's try to construct the decision tree as I said that let's consider that I'm\nas I said that let's consider that I'm\nas I said that let's consider that I'm considering salary feature So based on\nconsidering salary feature So based on\nconsidering salary feature So based on using salary feature what I'm actually\nusing salary feature what I'm actually\nusing salary feature what I'm actually going to do I am going to take this as\ngoing to do I am going to take this as\ngoing to do I am going to take this as my node and I'm going to split this up\nmy node and I'm going to split this up\nmy node and I'm going to split this up and remember whenever we are creating\nand remember whenever we are creating\nand remember whenever we are creating decision Tree in this particular case it\ndecision Tree in this particular case it\ndecision Tree in this particular case it will be a binary decision tree let's say\nwill be a binary decision tree let's say\nwill be a binary decision tree let's say that in salary one is less than or equal\nthat in salary one is less than or equal\nthat in salary one is less than or equal to one is greater than 50 so this two\nto one is greater than 50 so this two\nto one is greater than 50 so this two you obviously have in the case of binary\nyou obviously have in the case of binary\nyou obviously have in the case of binary in case of credit where there are three\nin case of credit where there are three\nin case of credit where there are three categories I'll also show you how that\ncategories I'll also show you how that\ncategories I'll also show you how that further split will happen and how that\nfurther split will happen and how that\nfurther split will happen and how that will get converted into a binary team so\nwill get converted into a binary team so\nwill get converted into a binary team so here you have less than or equal to 50K\nhere you have less than or equal to 50K\nhere you have less than or equal to 50K and greater than 50k now let's go ahead\nand greater than 50k now let's go ahead\nand greater than 50k now let's go ahead and understand how many vales are there\nand understand how many vales are there\nand understand how many vales are there in this salary so if I see before the\nin this salary so if I see before the\nin this salary so if I see before the split you can definitely see that I'm\nsplit you can definitely see that I'm\nsplit you can definitely see that I'm going to use this residual and probably\ngoing to use this residual and probably\ngoing to use this residual and probably train this entire model now if I really\ntrain this entire model now if I really\ntrain this entire model now if I really wanted to find out the residual\nwanted to find out the residual\nwanted to find out the residual initially these are my residuals over\ninitially these are my residuals over\ninitially these are my residuals over here so one resid is -.5 then I have 0.5\nhere so one resid is -.5 then I have 0.5\nhere so one resid is -.5 then I have 0.5 over here then I have .5 then again I\nover here then I have .5 then again I\nover here then I have .5 then again I have -.5 then again I have 0.5 then\nhave -.5 then again I have 0.5 then\nhave -.5 then again I have 0.5 then again I have 0.5 and finally I have\nagain I have 0.5 and finally I have\nagain I have 0.5 and finally I have minus .5 so these are my total residuals\nminus .5 so these are my total residuals\nminus .5 so these are my total residuals that are there suppose if I make this\nthat are there suppose if I make this\nthat are there suppose if I make this split less than or equal to 50 First\nsplit less than or equal to 50 First\nsplit less than or equal to 50 First less than or equal to 50 the residuals\nless than or equal to 50 the residuals\nless than or equal to 50 the residuals what are things are there so here I'm\nwhat are things are there so here I'm\nwhat are things are there so here I'm going to have minus5 then less than or\ngoing to have minus5 then less than or\ngoing to have minus5 then less than or equal to 50 again I'm going to have 05\nequal to 50 again I'm going to have 05\nequal to 50 again I'm going to have 05 then again less than or equal to 50 I'm\nthen again less than or equal to 50 I'm\nthen again less than or equal to 50 I'm going to have 0.5 and less than or equal\ngoing to have 0.5 and less than or equal\ngoing to have 0.5 and less than or equal to again one more 0.5 is there I'm just\nto again one more 0.5 is there I'm just\nto again one more 0.5 is there I'm just going to remove this the last5 which is\ngoing to remove this the last5 which is\ngoing to remove this the last5 which is nothing but Min -.5 so I hope you\nnothing but Min -.5 so I hope you\nnothing but Min -.5 so I hope you understood this split so half of the\nunderstood this split so half of the\nunderstood this split so half of the things came over here the remaining half\nthings came over here the remaining half\nthings came over here the remaining half will be greater than or equal to greater\nwill be greater than or equal to greater\nwill be greater than or equal to greater than 50 so you have one value here one\nthan 50 so you have one value here one\nthan 50 so you have one value here one value here one value here so it will be\nvalue here one value here so it will be\nvalue here one value here so it will be Min -.5 then you have 0.5 and then\nMin -.5 then you have 0.5 and then\nMin -.5 then you have 0.5 and then finally you have 0.5 residuals how do we\nfinally you have 0.5 residuals how do we\nfinally you have 0.5 residuals how do we get it guys see from the base model\nget it guys see from the base model\nget it guys see from the base model which is by default giving 0.5 first my\nwhich is by default giving 0.5 first my\nwhich is by default giving 0.5 first my data goes over here by default\ndata goes over here by default\ndata goes over here by default probability I'm going to get 0.5 so\nprobability I'm going to get 0.5 so\nprobability I'm going to get 0.5 so residual is basically calculated from\nresidual is basically calculated from\nresidual is basically calculated from this probability and approval so this\nthis probability and approval so this\nthis probability and approval so this probability minus approval so if you\nprobability minus approval so if you\nprobability minus approval so if you subtract 0 -.5 sorry I'm just going to\nsubtract 0 -.5 sorry I'm just going to\nsubtract 0 -.5 sorry I'm just going to rub this so if you subtract 0 -.5 you're\nrub this so if you subtract 0 -.5 you're\nrub this so if you subtract 0 -.5 you're going to get -.5 1 -.5 you're going to\ngoing to get -.5 1 -.5 you're going to\ngoing to get -.5 1 -.5 you're going to get .5 1 -.5 you're going to get .5 so\nget .5 1 -.5 you're going to get .5 so\nget .5 1 -.5 you're going to get .5 so everybody I hope is very much clear with\neverybody I hope is very much clear with\neverybody I hope is very much clear with respect to this so this is the first\nrespect to this so this is the first\nrespect to this so this is the first step we constructed a binary tree now in\nstep we constructed a binary tree now in\nstep we constructed a binary tree now in the second step it says calculate the\nthe second step it says calculate the\nthe second step it says calculate the similarity weight now how to calculate\nsimilarity weight now how to calculate\nsimilarity weight now how to calculate the similarity weight similarity weight\nthe similarity weight similarity weight\nthe similarity weight similarity weight formula is sum of residual Square now\nformula is sum of residual Square now\nformula is sum of residual Square now what is residual Square let's say that\nwhat is residual Square let's say that\nwhat is residual Square let's say that I'm going to calculate the the the uh\nI'm going to calculate the the the uh\nI'm going to calculate the the the uh I'm going to calculate for this okay\nI'm going to calculate for this okay\nI'm going to calculate for this okay similarity weight now in this particular\nsimilarity weight now in this particular\nsimilarity weight now in this particular case if I go and calculate my similarity\ncase if I go and calculate my similarity\ncase if I go and calculate my similarity weight it will be summation of residual\nweight it will be summation of residual\nweight it will be summation of residual Square this is my residual values this\nSquare this is my residual values this\nSquare this is my residual values this is my residual Valu so I'm going to do\nis my residual Valu so I'm going to do\nis my residual Valu so I'm going to do the summation of this Square okay this\nthe summation of this Square okay this\nthe summation of this Square okay this value square you can see over here sum\nvalue square you can see over here sum\nvalue square you can see over here sum of residual Square everybody you can see\nof residual Square everybody you can see\nof residual Square everybody you can see sum of of residual squares so what do\nsum of of residual squares so what do\nsum of of residual squares so what do you think sum of residual squares will\nyou think sum of residual squares will\nyou think sum of residual squares will be in this particular case how I have to\nbe in this particular case how I have to\nbe in this particular case how I have to do it I will just take up this all\ndo it I will just take up this all\ndo it I will just take up this all values like\nvalues like\nvalues like -.5\n-.5\n-.5 +5\n+5\n+5 +5 and\n+5 and\n+5 and -.5 whole square right I'm just going to\n-.5 whole square right I'm just going to\n-.5 whole square right I'm just going to do the squaring of this divided by\ndo the squaring of this divided by\ndo the squaring of this divided by understand what it is divided by it is\nunderstand what it is divided by it is\nunderstand what it is divided by it is divided by probability of 1 minus\ndivided by probability of 1 minus\ndivided by probability of 1 minus probability now where do we get this\nprobability now where do we get this\nprobability now where do we get this probability value where do we get this\nprobability value where do we get this\nprobability value where do we get this probability value value we get this\nprobability value value we get this\nprobability value value we get this probability value from our base model\nprobability value from our base model\nprobability value from our base model right so here I'm basically going to say\nright so here I'm basically going to say\nright so here I'm basically going to say that we are going to do the summation of\nthat we are going to do the summation of\nthat we are going to do the summation of probability of 1 minus probability 1\nprobability of 1 minus probability 1\nprobability of 1 minus probability 1 minus probability that basically means\nminus probability that basically means\nminus probability that basically means for each and every point for each and\nfor each and every point for each and\nfor each and every point for each and every Point what is the probability see\nevery Point what is the probability see\nevery Point what is the probability see probability is basically coming from the\nprobability is basically coming from the\nprobability is basically coming from the base model so for each Pro each point\nbase model so for each Pro each point\nbase model so for each Pro each point I'm going to come compute two things one\nI'm going to come compute two things one\nI'm going to come compute two things one is the probability and then 1 minus\nis the probability and then 1 minus\nis the probability and then 1 minus probability and this I'm going to do the\nprobability and this I'm going to do the\nprobability and this I'm going to do the summ\nsumm\nsumm like this I will do it four times 1 -.5\nlike this I will do it four times 1 -.5\nlike this I will do it four times 1 -.5 then .5 * 1 -.5 and finally you'll be\nthen .5 * 1 -.5 and finally you'll be\nthen .5 * 1 -.5 and finally you'll be able to see one more will be there which\nable to see one more will be there which\nable to see one more will be there which is\nis\nis +5 1 -.5 so this will be your total\n+5 1 -.5 so this will be your total\n+5 1 -.5 so this will be your total things with respect to this so I hope\nthings with respect to this so I hope\nthings with respect to this so I hope you have understood till here uh where\nyou have understood till here uh where\nyou have understood till here uh where you are able to understand that what we\nyou are able to understand that what we\nyou are able to understand that what we have done this is summation of uh\nhave done this is summation of uh\nhave done this is summation of uh residual square and this is the\nresidual square and this is the\nresidual square and this is the remaining probability multiplied by 1\nremaining probability multiplied by 1\nremaining probability multiplied by 1 minus probability now tell me what are\nminus probability now tell me what are\nminus probability now tell me what are you able to find out from this if you\nyou able to find out from this if you\nyou able to find out from this if you cancel this and this this and this this\ncancel this and this this and this this\ncancel this and this this and this this value is going to become zero so this\nvalue is going to become zero so this\nvalue is going to become zero so this entire value is going to become Zer\nentire value is going to become Zer\nentire value is going to become Zer because 0 divided by anything is 0er so\nbecause 0 divided by anything is 0er so\nbecause 0 divided by anything is 0er so here I hope everybody is understood what\nhere I hope everybody is understood what\nhere I hope everybody is understood what is the similarity weight of this\nis the similarity weight of this\nis the similarity weight of this specific node if I want to write it is\nspecific node if I want to write it is\nspecific node if I want to write it is nothing but zero now you may be\nnothing but zero now you may be\nnothing but zero now you may be considering where is Lambda\nconsidering where is Lambda\nconsidering where is Lambda value okay we will initially initialize\nvalue okay we will initially initialize\nvalue okay we will initially initialize Lambda by 1 I'll talk about this hyper\nLambda by 1 I'll talk about this hyper\nLambda by 1 I'll talk about this hyper parameter let's consider it as 1 so here\nparameter let's consider it as 1 so here\nparameter let's consider it as 1 so here + 1 or plus 0 let's let's consider\n+ 1 or plus 0 let's let's consider\n+ 1 or plus 0 let's let's consider Lambda value 0 let's say for right now\nLambda value 0 let's say for right now\nLambda value 0 let's say for right now okay I'm just going to make it Lambda is\nokay I'm just going to make it Lambda is\nokay I'm just going to make it Lambda is equal to0 I'm just going to talk about\nequal to0 I'm just going to talk about\nequal to0 I'm just going to talk about it because it is a kind of hyper\nit because it is a kind of hyper\nit because it is a kind of hyper parameter by Z -.5 -.5 +5 +5 if I do the\nparameter by Z -.5 -.5 +5 +5 if I do the\nparameter by Z -.5 -.5 +5 +5 if I do the summation if I do the summation here you\nsummation if I do the summation here you\nsummation if I do the summation here you will be able to see that I'm going to\nwill be able to see that I'm going to\nwill be able to see that I'm going to get zero so this calculation we have\nget zero so this calculation we have\nget zero so this calculation we have done and we have got uh the sumission of\ndone and we have got uh the sumission of\ndone and we have got uh the sumission of weight is equal to Z and let's go ahead\nweight is equal to Z and let's go ahead\nweight is equal to Z and let's go ahead and calculate the sumission of the\nand calculate the sumission of the\nand calculate the sumission of the weight of the next node no no no it's\nweight of the next node no no no it's\nweight of the next node no no no it's not first Square it is whole squar so\nnot first Square it is whole squar so\nnot first Square it is whole squar so here also if I do so it is5 +5 now let's\nhere also if I do so it is5 +5 now let's\nhere also if I do so it is5 +5 now let's do it for this if I want to find out the\ndo it for this if I want to find out the\ndo it for this if I want to find out the similarity weight again see I'm going to\nsimilarity weight again see I'm going to\nsimilarity weight again see I'm going to repeat it .5 +5 whole squ and since\nrepeat it .5 +5 whole squ and since\nrepeat it .5 +5 whole squ and since there are three points so I'm going to\nthere are three points so I'm going to\nthere are three points so I'm going to basically use probability 1 minus\nbasically use probability 1 minus\nbasically use probability 1 minus probability for one point then plus\nprobability for one point then plus\nprobability for one point then plus probability 1 minus probability second\nprobability 1 minus probability second\nprobability 1 minus probability second point and then probability and 1 minus\npoint and then probability and 1 minus\npoint and then probability and 1 minus probability for the third point and\nprobability for the third point and\nprobability for the third point and Lambda is zero so I'm not going to write\nLambda is zero so I'm not going to write\nLambda is zero so I'm not going to write anything now go let's go and do the\nanything now go let's go and do the\nanything now go let's go and do the calculation for this node so - 5 - 5 it\ncalculation for this node so - 5 - 5 it\ncalculation for this node so - 5 - 5 it becomes zero then .5 whole square right\nbecomes zero then .5 whole square right\nbecomes zero then .5 whole square right so here I'm going to get 0.25 here if\nso here I'm going to get 0.25 here if\nso here I'm going to get 0.25 here if you do the calculation here you are\nyou do the calculation here you are\nyou do the calculation here you are going to get 75 so this value is going\ngoing to get 75 so this value is going\ngoing to get 75 so this value is going to be 1x3 and which is nothing at33 so\nto be 1x3 and which is nothing at33 so\nto be 1x3 and which is nothing at33 so the similarity weight for this node for\nthe similarity weight for this node for\nthe similarity weight for this node for this node\nthis node\nthis node is33 so here you can see probability of\nis33 so here you can see probability of\nis33 so here you can see probability of multiplied by 1 minus\nmultiplied by 1 minus\nmultiplied by 1 minus probability okay now the next step that\nprobability okay now the next step that\nprobability okay now the next step that we do is that calculate the information\nwe do is that calculate the information\nwe do is that calculate the information gain now you know how to calculate the\ngain now you know how to calculate the\ngain now you know how to calculate the information gain but before that let's\ninformation gain but before that let's\ninformation gain but before that let's do the computation for this also for\ndo the computation for this also for\ndo the computation for this also for this root node also go ahead and\nthis root node also go ahead and\nthis root node also go ahead and calculate the similarity weight of\ncalculate the similarity weight of\ncalculate the similarity weight of this okay they\nthis okay they\nthis okay they why the base model probability is5\nwhy the base model probability is5\nwhy the base model probability is5 because it is just understand that it is\nbecause it is just understand that it is\nbecause it is just understand that it is a dummy dummy model I have just put a if\na dummy dummy model I have just put a if\na dummy dummy model I have just put a if condition there saying that it is going\ncondition there saying that it is going\ncondition there saying that it is going to give 0.5 now do it for this one guys\nto give 0.5 now do it for this one guys\nto give 0.5 now do it for this one guys root node what it will be see I can\nroot node what it will be see I can\nroot node what it will be see I can calculate from here only minus1 gone\ncalculate from here only minus1 gone\ncalculate from here only minus1 gone this is also gone this is also gone this\nthis is also gone this is also gone this\nthis is also gone this is also gone this will be .25 divided by something now\nwill be .25 divided by something now\nwill be .25 divided by something now tell me guys what should be for the root\ntell me guys what should be for the root\ntell me guys what should be for the root node what is the similarity similarity\nnode what is the similarity similarity\nnode what is the similarity similarity weight what is the similarity weight for\nweight what is the similarity weight for\nweight what is the similarity weight for for this do this calculation everyone up\nfor this do this calculation everyone up\nfor this do this calculation everyone up one I know it will be. 25 divided by\none I know it will be. 25 divided by\none I know it will be. 25 divided by this will be 1.75 are you getting this\nthis will be 1.75 are you getting this\nthis will be 1.75 are you getting this similarity weight which will be nothing\nsimilarity weight which will be nothing\nsimilarity weight which will be nothing but 1 by 7 and if I divide 1 by 7 if I\nbut 1 by 7 and if I divide 1 by 7 if I\nbut 1 by 7 and if I divide 1 by 7 if I say what is 1 by 7 it\nsay what is 1 by 7 it\nsay what is 1 by 7 it is42 so it is nothing but .14 if I want\nis42 so it is nothing but .14 if I want\nis42 so it is nothing but .14 if I want to calculate the root node similarity\nto calculate the root node similarity\nto calculate the root node similarity weight over here\nweight over here\nweight over here is4 so I know 0.14 here 0 here 33 now\nis4 so I know 0.14 here 0 here 33 now\nis4 so I know 0.14 here 0 here 33 now see over here we calculate the\nsee over here we calculate the\nsee over here we calculate the Information Gain Next Step the third\nInformation Gain Next Step the third\nInformation Gain Next Step the third step what we do is that we calculate the\nstep what we do is that we calculate the\nstep what we do is that we calculate the information gain now Information Gain is\ninformation gain now Information Gain is\ninformation gain now Information Gain is nothing but in this particular case the\nnothing but in this particular case the\nnothing but in this particular case the root node similarity weight we'll try to\nroot node similarity weight we'll try to\nroot node similarity weight we'll try to add up so I will be getting\nadd up so I will be getting\nadd up so I will be getting 0.33 minus this particular Top Root node\n0.33 minus this particular Top Root node\n0.33 minus this particular Top Root node whatever split has happened that\nwhatever split has happened that\nwhatever split has happened that similarity weight I'll take 0 +33\nsimilarity weight I'll take 0 +33\nsimilarity weight I'll take 0 +33 -14 so Point\n-14 so Point\n-14 so Point -14 and if I do it it is nothing but\n-14 and if I do it it is nothing but\n-14 and if I do it it is nothing but just open your calculator again and\njust open your calculator again and\njust open your calculator again and 33\n33\n33 -14 so it is nothing but .19 I'm getting\n-14 so it is nothing but .19 I'm getting\n-14 so it is nothing but .19 I'm getting .19 as my information gain the\n.19 as my information gain the\n.19 as my information gain the information gain of this specific tree I\ninformation gain of this specific tree I\ninformation gain of this specific tree I got it\ngot it\ngot it as19 obviously you know how the features\nas19 obviously you know how the features\nas19 obviously you know how the features will get selected based on the\nwill get selected based on the\nwill get selected based on the Information Gain but let's say that the\nInformation Gain but let's say that the\nInformation Gain but let's say that the highest Information Gain that is given\nhighest Information Gain that is given\nhighest Information Gain that is given by salary okay now we will go ahead and\nby salary okay now we will go ahead and\nby salary okay now we will go ahead and do the further split let's go ahead and\ndo the further split let's go ahead and\ndo the further split let's go ahead and do the further split so I I know my\ndo the further split so I I know my\ndo the further split so I I know my information gain now it is1 n and\ninformation gain now it is1 n and\ninformation gain now it is1 n and Information Gain is basically used to\nInformation Gain is basically used to\nInformation Gain is basically used to select that specific node through which\nselect that specific node through which\nselect that specific node through which the split will happen now I'll further\nthe split will happen now I'll further\nthe split will happen now I'll further go and do the split let's say that I'm\ngo and do the split let's say that I'm\ngo and do the split let's say that I'm going to do the further split with the\ngoing to do the further split with the\ngoing to do the further split with the next feature that is which one credit so\nnext feature that is which one credit so\nnext feature that is which one credit so I'm going to take credit over here I'm\nI'm going to take credit over here I'm\nI'm going to take credit over here I'm going to take credit over here and again\ngoing to take credit over here and again\ngoing to take credit over here and again I have to do a binary split again but\nI have to do a binary split again but\nI have to do a binary split again but you may be considering chish here are\nyou may be considering chish here are\nyou may be considering chish here are only three categories how we are going\nonly three categories how we are going\nonly three categories how we are going to basically do this particular split\nto basically do this particular split\nto basically do this particular split right because we don't know how to do\nright because we don't know how to do\nright because we don't know how to do the split because we have three\nthe split because we have three\nthe split because we have three categories over here so in this case\ncategories over here so in this case\ncategories over here so in this case what I will do is that we what we can\nwhat I will do is that we what we can\nwhat I will do is that we what we can definitely do is that in this particular\ndefinitely do is that in this particular\ndefinitely do is that in this particular case the split that we are probably\ncase the split that we are probably\ncase the split that we are probably going to do is that let's consider two\ngoing to do is that let's consider two\ngoing to do is that let's consider two categories like good and normal at one\ncategories like good and normal at one\ncategories like good and normal at one side bad at one side so here it becomes\nside bad at one side so here it becomes\nside bad at one side so here it becomes a binary split again now let's go ahead\na binary split again now let's go ahead\na binary split again now let's go ahead and let's try to see that how many data\nand let's try to see that how many data\nand let's try to see that how many data points will fall here and how many data\npoints will fall here and how many data\npoints will fall here and how many data points will fall here so for writing\npoints will fall here so for writing\npoints will fall here so for writing down the data points let's say if it is\ndown the data points let's say if it is\ndown the data points let's say if it is less than or see go to the path if it is\nless than or see go to the path if it is\nless than or see go to the path if it is less than or equal to 50 it'll go this\nless than or equal to 50 it'll go this\nless than or equal to 50 it'll go this path and if it is B then we are probably\npath and if it is B then we are probably\npath and if it is B then we are probably going to get how much is the residual we\ngoing to get how much is the residual we\ngoing to get how much is the residual we are going to get one residual over here\nare going to get one residual over here\nare going to get one residual over here first of all so this is my one residual\nfirst of all so this is my one residual\nfirst of all so this is my one residual that is -.5 then similarly if I see less\nthat is -.5 then similarly if I see less\nthat is -.5 then similarly if I see less than or equal to 50 good is there right\nthan or equal to 50 good is there right\nthan or equal to 50 good is there right good or normal is there so here again 0.\ngood or normal is there so here again 0.\ngood or normal is there so here again 0. five will come I hope everybody is able\nfive will come I hope everybody is able\nfive will come I hope everybody is able to understand see the second record less\nto understand see the second record less\nto understand see the second record less than or equal to 50 we go in this path\nthan or equal to 50 we go in this path\nthan or equal to 50 we go in this path but it is good we come over here again\nbut it is good we come over here again\nbut it is good we come over here again less than or equal to 50 good again we\nless than or equal to 50 good again we\nless than or equal to 50 good again we are going to get 1\nare going to get 1\nare going to get 1 more5 then go with respect to greater\nmore5 then go with respect to greater\nmore5 then go with respect to greater than or equal to 50 which is coming over\nthan or equal to 50 which is coming over\nthan or equal to 50 which is coming over here we'll not worry about it right now\nhere we'll not worry about it right now\nhere we'll not worry about it right now again less than or equal to 50 normal\nagain less than or equal to 50 normal\nagain less than or equal to 50 normal again it is\nagain it is\nagain it is -.5 right so this many records\n-.5 right so this many records\n-.5 right so this many records definitely coming over here only one\ndefinitely coming over here only one\ndefinitely coming over here only one record is basically coming over here\nrecord is basically coming over here\nrecord is basically coming over here then again we will start the same\nthen again we will start the same\nthen again we will start the same process again we will start the same\nprocess again we will start the same\nprocess again we will start the same process now for the same process what we\nprocess now for the same process what we\nprocess now for the same process what we are going to do again try to calculate\nare going to do again try to calculate\nare going to do again try to calculate the similarity weight now in order to\nthe similarity weight now in order to\nthe similarity weight now in order to calculate the similarity weight what I\ncalculate the similarity weight what I\ncalculate the similarity weight what I will do I will basically say this is my\nwill do I will basically say this is my\nwill do I will basically say this is my similarity weight this will become .25\nsimilarity weight this will become .25\nsimilarity weight this will become .25 divided 025 why because this whole\ndivided 025 why because this whole\ndivided 025 why because this whole square right this whole Square residual\nsquare right this whole Square residual\nsquare right this whole Square residual square right summation of residual\nsquare right summation of residual\nsquare right summation of residual square but here I have only one residual\nsquare but here I have only one residual\nsquare but here I have only one residual so this Square it will become and then\nso this Square it will become and then\nso this Square it will become and then what I'm actually going to do I'm going\nwhat I'm actually going to do I'm going\nwhat I'm actually going to do I'm going to basically write .5 - 1 -.5 this is\nto basically write .5 - 1 -.5 this is\nto basically write .5 - 1 -.5 this is nothing for only for one data point so\nnothing for only for one data point so\nnothing for only for one data point so this is nothing but .5 * .5 which is\nthis is nothing but .5 * .5 which is\nthis is nothing but .5 * .5 which is nothing but 0.25 right now in this\nnothing but 0.25 right now in this\nnothing but 0.25 right now in this particular case I will get similarity\nparticular case I will get similarity\nparticular case I will get similarity weight as I hope everybody I'm getting\nweight as I hope everybody I'm getting\nweight as I hope everybody I'm getting it as one now what about this similarity\nit as one now what about this similarity\nit as one now what about this similarity weight if you want to compute it is\nweight if you want to compute it is\nweight if you want to compute it is again very very simple this and this\nagain very very simple this and this\nagain very very simple this and this will get cancelled then again it will be\nwill get cancelled then again it will be\nwill get cancelled then again it will be 025 divided by um if I say one like this\n025 divided by um if I say one like this\n025 divided by um if I say one like this .25 then again it will be 75 then this\n.25 then again it will be 75 then this\n.25 then again it will be 75 then this will also be 1 by3 that is nothing but\nwill also be 1 by3 that is nothing but\nwill also be 1 by3 that is nothing but 33 so similarity weight will\n33 so similarity weight will\n33 so similarity weight will be33 then again I have to calculate the\nbe33 then again I have to calculate the\nbe33 then again I have to calculate the information gain of this node what I\ninformation gain of this node what I\ninformation gain of this node what I will do I will add this up see 1\nwill do I will add this up see 1\nwill do I will add this up see 1 +33 I'll add like 1\n+33 I'll add like 1\n+33 I'll add like 1 +33 minus 0 why zero because the\n+33 minus 0 why zero because the\n+33 minus 0 why zero because the information gain the similarity weight\ninformation gain the similarity weight\ninformation gain the similarity weight of this uh the up one is basically 0\nof this uh the up one is basically 0\nof this uh the up one is basically 0 right for this particular credit node\nright for this particular credit node\nright for this particular credit node similarity weight is zero so 1\nsimilarity weight is zero so 1\nsimilarity weight is zero so 1 +33 minus 0 this will be 1.33 so like\n+33 minus 0 this will be 1.33 so like\n+33 minus 0 this will be 1.33 so like this further split will again happen\nthis further split will again happen\nthis further split will again happen over here with different different node\nover here with different different node\nover here with different different node and we will only be getting a binary\nand we will only be getting a binary\nand we will only be getting a binary split but we will be comparing based on\nsplit but we will be comparing based on\nsplit but we will be comparing based on Information Gain which one is coming\nInformation Gain which one is coming\nInformation Gain which one is coming good now let's say that I have created\ngood now let's say that I have created\ngood now let's say that I have created this path I have I have designed I have\nthis path I have I have designed I have\nthis path I have I have designed I have developed my entire binary decision tree\ndeveloped my entire binary decision tree\ndeveloped my entire binary decision tree which is a speciality in XG boost now\nwhich is a speciality in XG boost now\nwhich is a speciality in XG boost now what I'm going to do over here is that\nwhat I'm going to do over here is that\nwhat I'm going to do over here is that see everybody what I'm going to do let's\nsee everybody what I'm going to do let's\nsee everybody what I'm going to do let's consider the inferencing part let's say\nconsider the inferencing part let's say\nconsider the inferencing part let's say this record is going to go how we are\nthis record is going to go how we are\nthis record is going to go how we are going to calculate the output so this\ngoing to calculate the output so this\ngoing to calculate the output so this first of all went to this base model now\nfirst of all went to this base model now\nfirst of all went to this base model now let's go ahead and see how the\nlet's go ahead and see how the\nlet's go ahead and see how the inferencing will happen suppose This\ninferencing will happen suppose This\ninferencing will happen suppose This Record is going right so first of all\nRecord is going right so first of all\nRecord is going right so first of all this record will go to this base model\nthis record will go to this base model\nthis record will go to this base model the base model is giving the probability\nthe base model is giving the probability\nthe base model is giving the probability as 0.5 so the first base model is\nas 0.5 so the first base model is\nas 0.5 so the first base model is basically giving 0.5 now base based on\nbasically giving 0.5 now base based on\nbasically giving 0.5 now base based on this 05 how do we calculate the real\nthis 05 how do we calculate the real\nthis 05 how do we calculate the real probability how do we calculate the real\nprobability how do we calculate the real\nprobability how do we calculate the real probability in this okay so we apply\nprobability in this okay so we apply\nprobability in this okay so we apply something called as logs so we basically\nsomething called as logs so we basically\nsomething called as logs so we basically say log of P / 1us P so this is the\nsay log of P / 1us P so this is the\nsay log of P / 1us P so this is the formula we basically apply in only the\nformula we basically apply in only the\nformula we basically apply in only the case of base model so if we try to see\ncase of base model so if we try to see\ncase of base model so if we try to see this it is nothing but log\nthis it is nothing but log\nthis it is nothing but log of5 / .5 which is nothing but zero log\nof5 / .5 which is nothing but zero log\nof5 / .5 which is nothing but zero log of one is nothing but zero so in the\nof one is nothing but zero so in the\nof one is nothing but zero so in the first case whenever any record goes I\nfirst case whenever any record goes I\nfirst case whenever any record goes I will be getting the zero value over here\nwill be getting the zero value over here\nwill be getting the zero value over here okay zero value over here then plus why\nokay zero value over here then plus why\nokay zero value over here then plus why plus I'm doing because it will now go to\nplus I'm doing because it will now go to\nplus I'm doing because it will now go to the binary decision tree now this record\nthe binary decision tree now this record\nthe binary decision tree now this record will go to my binary decision Tre\nwill go to my binary decision Tre\nwill go to my binary decision Tre whatever value I'm getting from this I'm\nwhatever value I'm getting from this I'm\nwhatever value I'm getting from this I'm actually adding that up and now it will\nactually adding that up and now it will\nactually adding that up and now it will go over here now when it goes over here\ngo over here now when it goes over here\ngo over here now when it goes over here first of all let's see which branch it\nfirst of all let's see which branch it\nfirst of all let's see which branch it is following it is following less than\nis following it is following less than\nis following it is following less than or equal to 50 Branch first Branch over\nor equal to 50 Branch first Branch over\nor equal to 50 Branch first Branch over here then this is bad it'll go and\nhere then this is bad it'll go and\nhere then this is bad it'll go and follow here so here I can see that the\nfollow here so here I can see that the\nfollow here so here I can see that the similarity weight is one now the\nsimilarity weight is one now the\nsimilarity weight is one now the similarity weight is basically one in\nsimilarity weight is basically one in\nsimilarity weight is basically one in this case so what we do in the case of\nthis case so what we do in the case of\nthis case so what we do in the case of this we pass it to a learning rate\nthis we pass it to a learning rate\nthis we pass it to a learning rate parameter so this specifically is my\nparameter so this specifically is my\nparameter so this specifically is my learning rate multiplied by 1 one\nlearning rate multiplied by 1 one\nlearning rate multiplied by 1 one because why similarity weight is one\nbecause why similarity weight is one\nbecause why similarity weight is one over here so this will basically be my\nover here so this will basically be my\nover here so this will basically be my first references and Alpha over here is\nfirst references and Alpha over here is\nfirst references and Alpha over here is my learning rate it can be a very small\nmy learning rate it can be a very small\nmy learning rate it can be a very small value based on the learning parameter\nvalue based on the learning parameter\nvalue based on the learning parameter that we use like how we have defined\nthat we use like how we have defined\nthat we use like how we have defined learning parameters elsewhere on top of\nlearning parameters elsewhere on top of\nlearning parameters elsewhere on top of this we apply an activation function\nthis we apply an activation function\nthis we apply an activation function which is called as sigmoid since this is\nwhich is called as sigmoid since this is\nwhich is called as sigmoid since this is a classification problem we apply an\na classification problem we apply an\na classification problem we apply an activation function which is called as\nactivation function which is called as\nactivation function which is called as sigmoid and I hope you know what is the\nsigmoid and I hope you know what is the\nsigmoid and I hope you know what is the use of sigmoid based on this based on\nuse of sigmoid based on this based on\nuse of sigmoid based on this based on the alpha value based on this the output\nthe alpha value based on this the output\nthe alpha value based on this the output will be between 0 to 1 now I hope you\nwill be between 0 to 1 now I hope you\nwill be between 0 to 1 now I hope you getting it guys this is how the entire\ngetting it guys this is how the entire\ngetting it guys this is how the entire inferencing will probably happen now\ninferencing will probably happen now\ninferencing will probably happen now similarly what I will do I will try to\nsimilarly what I will do I will try to\nsimilarly what I will do I will try to construct this kind of decision tree\nconstruct this kind of decision tree\nconstruct this kind of decision tree parall so we we can also write our\nparall so we we can also write our\nparall so we we can also write our entire function will look something like\nentire function will look something like\nentire function will look something like this Alpha 0 + alpha 1 and this will be\nthis Alpha 0 + alpha 1 and this will be\nthis Alpha 0 + alpha 1 and this will be your decision tree 1 output then Alpha 2\nyour decision tree 1 output then Alpha 2\nyour decision tree 1 output then Alpha 2 your decision tree output Alpha 3 your\nyour decision tree output Alpha 3 your\nyour decision tree output Alpha 3 your decision 3 output like this Alpha 4 your\ndecision 3 output like this Alpha 4 your\ndecision 3 output like this Alpha 4 your decision 3 output fourth decision tree\ndecision 3 output fourth decision tree\ndecision 3 output fourth decision tree like this it will be alpha n your\nlike this it will be alpha n your\nlike this it will be alpha n your decision tree n output and this will be\ndecision tree n output and this will be\ndecision tree n output and this will be your output finally when you're trying\nyour output finally when you're trying\nyour output finally when you're trying to inference from any new\nto inference from any new\nto inference from any new record now the reason why we say this as\nrecord now the reason why we say this as\nrecord now the reason why we say this as boosting because see understand we are\nboosting because see understand we are\nboosting because see understand we are going to add each and every decision\ngoing to add each and every decision\ngoing to add each and every decision tree output slowly to finally get our\ntree output slowly to finally get our\ntree output slowly to finally get our output with respect to the working of\noutput with respect to the working of\noutput with respect to the working of the decision tree this is how XG boost\nthe decision tree this is how XG boost\nthe decision tree this is how XG boost actually work don't credit further needs\nactually work don't credit further needs\nactually work don't credit further needs to be simplified yes see like this\nto be simplified yes see like this\nto be simplified yes see like this similarly we can split credit with the\nsimilarly we can split credit with the\nsimilarly we can split credit with the help of like we can make blue green one\nhelp of like we can make blue green one\nhelp of like we can make blue green one side normal at one side But whichever\nside normal at one side But whichever\nside normal at one side But whichever will be giving the information gain more\nwill be giving the information gain more\nwill be giving the information gain more that will be taken into consideration\nthat will be taken into consideration\nthat will be taken into consideration right and this is how your entire X\nright and this is how your entire X\nright and this is how your entire X boost classifier works it is very very\nboost classifier works it is very very\nboost classifier works it is very very difficult to basically calculate all\ndifficult to basically calculate all\ndifficult to basically calculate all those things so that is the reason we\nthose things so that is the reason we\nthose things so that is the reason we say that XG boost is also a blackbox\nsay that XG boost is also a blackbox\nsay that XG boost is also a blackbox model so this is basically a blackb\nmodel so this is basically a blackb\nmodel so this is basically a blackb model it is it prone to overfitting see\nmodel it is it prone to overfitting see\nmodel it is it prone to overfitting see at one stage we also need to perform\nat one stage we also need to perform\nat one stage we also need to perform hyperparameter tuning and this we\nhyperparameter tuning and this we\nhyperparameter tuning and this we specifically say pre- pruning we tend to\nspecifically say pre- pruning we tend to\nspecifically say pre- pruning we tend to do pre pruning and since we are\ndo pre pruning and since we are\ndo pre pruning and since we are combining multiple decision trees no no\ncombining multiple decision trees no no\ncombining multiple decision trees no no this decision tree this decision tree is\nthis decision tree this decision tree is\nthis decision tree this decision tree is this one this independent decision tree\nthis one this independent decision tree\nthis one this independent decision tree which I have created now parall after\nwhich I have created now parall after\nwhich I have created now parall after this what I'll do I'll create one more\nthis what I'll do I'll create one more\nthis what I'll do I'll create one more decision tree so it'll be looking like\ndecision tree so it'll be looking like\ndecision tree so it'll be looking like this see finally how it will look so\nthis see finally how it will look so\nthis see finally how it will look so this is my base model then my data then\nthis is my base model then my data then\nthis is my base model then my data then my data will go to this decision tree\nmy data will go to this decision tree\nmy data will go to this decision tree which I have actually done as a binary\nwhich I have actually done as a binary\nwhich I have actually done as a binary split on different different records\nsplit on different different records\nsplit on different different records then again we will make another decision\nthen again we will make another decision\nthen again we will make another decision tree which will again be a binary tree\ntree which will again be a binary tree\ntree which will again be a binary tree the splits will look like this then this\nthe splits will look like this then this\nthe splits will look like this then this is my base model where I'm getting the\nis my base model where I'm getting the\nis my base model where I'm getting the value as zero this will be alpha 1\nvalue as zero this will be alpha 1\nvalue as zero this will be alpha 1 multiplied by decision tree 1 which is\nmultiplied by decision tree 1 which is\nmultiplied by decision tree 1 which is this then this is Alpha 2 multiplied by\nthis then this is Alpha 2 multiplied by\nthis then this is Alpha 2 multiplied by decision tree 2 which is this and like\ndecision tree 2 which is this and like\ndecision tree 2 which is this and like this we will keep on continuously adding\nthis we will keep on continuously adding\nthis we will keep on continuously adding more decision trees unless and until\nmore decision trees unless and until\nmore decision trees unless and until this entire things becomes a very strong\nthis entire things becomes a very strong\nthis entire things becomes a very strong learner so this is how how we basically\nlearner so this is how how we basically\nlearner so this is how how we basically do the combination of all these things\ndo the combination of all these things\ndo the combination of all these things so I hope everybody is able to\nso I hope everybody is able to\nso I hope everybody is able to understand about the XG boost classifier\nunderstand about the XG boost classifier\nunderstand about the XG boost classifier now you may be thinking how does\nnow you may be thinking how does\nnow you may be thinking how does regressor work do you want a regressor\nregressor work do you want a regressor\nregressor work do you want a regressor problem statement also the decision tree\nproblem statement also the decision tree\nproblem statement also the decision tree will get constructed based on\nwill get constructed based on\nwill get constructed based on Independent features and again Lambda\nIndependent features and again Lambda\nIndependent features and again Lambda value is a hyperparameter we basically\nvalue is a hyperparameter we basically\nvalue is a hyperparameter we basically set up Lambda value with the help of\nset up Lambda value with the help of\nset up Lambda value with the help of cross validation now uh let's go ahead\ncross validation now uh let's go ahead\ncross validation now uh let's go ahead and discuss about ex boost regressor the\nand discuss about ex boost regressor the\nand discuss about ex boost regressor the second algorithm that we we will\nsecond algorithm that we we will\nsecond algorithm that we we will probably discuss about is something\nprobably discuss about is something\nprobably discuss about is something called as XG boost regressor and how\ncalled as XG boost regressor and how\ncalled as XG boost regressor and how does X boost regressor actually work\ndoes X boost regressor actually work\ndoes X boost regressor actually work some fundamental is follow in random\nsome fundamental is follow in random\nsome fundamental is follow in random Forest no in random Forest it is\nForest no in random Forest it is\nForest no in random Forest it is completely different there bagging\ncompletely different there bagging\ncompletely different there bagging happens bagging happens so over here\nhappens bagging happens so over here\nhappens bagging happens so over here let's go ahead with the regressor so\nlet's go ahead with the regressor so\nlet's go ahead with the regressor so here I'm going to take some example\nhere I'm going to take some example\nhere I'm going to take some example let's say that I have this many\nlet's say that I have this many\nlet's say that I have this many experience this many Gap and based on\nexperience this many Gap and based on\nexperience this many Gap and based on that we need to determine the salary my\nthat we need to determine the salary my\nthat we need to determine the salary my salary is my output feature let's say\nsalary is my output feature let's say\nsalary is my output feature let's say the experience is 2 2.5 3 4 4.5 okay now\nthe experience is 2 2.5 3 4 4.5 okay now\nthe experience is 2 2.5 3 4 4.5 okay now in this Gap let's say it is yes\nin this Gap let's say it is yes\nin this Gap let's say it is yes yes no no yes and let's say that the\nyes no no yes and let's say that the\nyes no no yes and let's say that the salary is somewhere around 40K it is\nsalary is somewhere around 40K it is\nsalary is somewhere around 40K it is 41k\n41k\n41k 52k and uh let's see some more data set\n52k and uh let's see some more data set\n52k and uh let's see some more data set over here 60k and 62k now the first step\nover here 60k and 62k now the first step\nover here 60k and 62k now the first step in classifier we created a base model\nin classifier we created a base model\nin classifier we created a base model here also we'll try to create a base\nhere also we'll try to create a base\nhere also we'll try to create a base model first of all this base model what\nmodel first of all this base model what\nmodel first of all this base model what output it will give it will give the\noutput it will give it will give the\noutput it will give it will give the average of all these values what is the\naverage of all these values what is the\naverage of all these values what is the average of all these values okay what is\naverage of all these values okay what is\naverage of all these values okay what is the average of all these value 40 81 52\nthe average of all these value 40 81 52\nthe average of all these value 40 81 52 60 62 if I just do the average it is\n60 62 if I just do the average it is\n60 62 if I just do the average it is nothing but 51k so by default I will\nnothing but 51k so by default I will\nnothing but 51k so by default I will create a base model which will take any\ncreate a base model which will take any\ncreate a base model which will take any input and just give the output as 51\ninput and just give the output as 51\ninput and just give the output as 51 this is the first step now based on this\nthis is the first step now based on this\nthis is the first step now based on this I will try to calculate my residual now\nI will try to calculate my residual now\nI will try to calculate my residual now how do I calculate my residual I will\nhow do I calculate my residual I will\nhow do I calculate my residual I will just subtract 40 by 51k so this will\njust subtract 40 by 51k so this will\njust subtract 40 by 51k so this will basically be - 11k\nbasically be - 11k\nbasically be - 11k and uh this will be 10 K - 10 K - 10 and\nand uh this will be 10 K - 10 K - 10 and\nand uh this will be 10 K - 10 K - 10 and this will be 1 this will be 9 and this\nthis will be 1 this will be 9 and this\nthis will be 1 this will be 9 and this will be 11 I hope everybody's able to\nwill be 11 I hope everybody's able to\nwill be 11 I hope everybody's able to get this let's say that I I make this as\nget this let's say that I I make this as\nget this let's say that I I make this as 42k okay for just making my calculation\n42k okay for just making my calculation\n42k okay for just making my calculation little bit easy so I have 9 over here so\nlittle bit easy so I have 9 over here so\nlittle bit easy so I have 9 over here so this is my residual then again the first\nthis is my residual then again the first\nthis is my residual then again the first step is that I construct my uh decision\nstep is that I construct my uh decision\nstep is that I construct my uh decision tree now let's say say that I'm going to\ntree now let's say say that I'm going to\ntree now let's say say that I'm going to use The Experience over here so this is\nuse The Experience over here so this is\nuse The Experience over here so this is my experience node and based on this\nmy experience node and based on this\nmy experience node and based on this experience node I have my features over\nexperience node I have my features over\nexperience node I have my features over here so here I will take up all my\nhere so here I will take up all my\nhere so here I will take up all my residuals - 11 99 1 99 11 and then how\nresiduals - 11 99 1 99 11 and then how\nresiduals - 11 99 1 99 11 and then how do I do the split based on experience\ndo I do the split based on experience\ndo I do the split based on experience this is a continuous feature so I have\nthis is a continuous feature so I have\nthis is a continuous feature so I have to basically do split with respect to\nto basically do split with respect to\nto basically do split with respect to continuous feature which I have already\ncontinuous feature which I have already\ncontinuous feature which I have already shown you in decision tree how do we do\nshown you in decision tree how do we do\nshown you in decision tree how do we do so here is my residual here it is 40\nso here is my residual here it is 40\nso here is my residual here it is 40 minus this\nminus this\nminus this is - 11 K - 9 K uh this is 1 K this is 9\nis - 11 K - 9 K uh this is 1 K this is 9\nis - 11 K - 9 K uh this is 1 K this is 9 K and\nK and\nK and 11k - 9k so now I will just create take\n11k - 9k so now I will just create take\n11k - 9k so now I will just create take up my first node here I'm going to use\nup my first node here I'm going to use\nup my first node here I'm going to use my experience feature I know my values\nmy experience feature I know my values\nmy experience feature I know my values what all things are going to come 11k in\nwhat all things are going to come 11k in\nwhat all things are going to come 11k in the root node - 9 1 9 and 11 now what we\nthe root node - 9 1 9 and 11 now what we\nthe root node - 9 1 9 and 11 now what we are going to do over here is that so I'm\nare going to do over here is that so I'm\nare going to do over here is that so I'm going to do again a binary split over\ngoing to do again a binary split over\ngoing to do again a binary split over here now the binary split will happen\nhere now the binary split will happen\nhere now the binary split will happen based on the continuous feature that is\nbased on the continuous feature that is\nbased on the continuous feature that is experienced so two types of Records I\nexperienced so two types of Records I\nexperienced so two types of Records I may get one is less than or equal to two\nmay get one is less than or equal to two\nmay get one is less than or equal to two and one is greater than 2 less than or\nand one is greater than 2 less than or\nand one is greater than 2 less than or equal to two and one is greater than two\nequal to two and one is greater than two\nequal to two and one is greater than two now less than or equal to two when I do\nnow less than or equal to two when I do\nnow less than or equal to two when I do the split let's see how many values we\nthe split let's see how many values we\nthe split let's see how many values we are getting less than or equal to two I\nare getting less than or equal to two I\nare getting less than or equal to two I will get only one value that is -1 and\nwill get only one value that is -1 and\nwill get only one value that is -1 and here I'm actually going to get all the\nhere I'm actually going to get all the\nhere I'm actually going to get all the other values - 9 1 9 11 now what we are\nother values - 9 1 9 11 now what we are\nother values - 9 1 9 11 now what we are going to do after this is that calculate\ngoing to do after this is that calculate\ngoing to do after this is that calculate the similarity weight now here the\nthe similarity weight now here the\nthe similarity weight now here the similarity weight will little bit the\nsimilarity weight will little bit the\nsimilarity weight will little bit the formula will change with respect to\nformula will change with respect to\nformula will change with respect to regression so similarity weight is\nregression so similarity weight is\nregression so similarity weight is nothing but summation of residual\nnothing but summation of residual\nnothing but summation of residual squares divided by number of residuals\nsquares divided by number of residuals\nsquares divided by number of residuals plus Lambda again here we are going to\nplus Lambda again here we are going to\nplus Lambda again here we are going to consider Lambda is zero because this is\nconsider Lambda is zero because this is\nconsider Lambda is zero because this is a hyper parameter tuning more the value\na hyper parameter tuning more the value\na hyper parameter tuning more the value of Lambda that basically means more more\nof Lambda that basically means more more\nof Lambda that basically means more more we are penalizing with respect to the\nwe are penalizing with respect to the\nwe are penalizing with respect to the residuals so this will be the formula\nresiduals so this will be the formula\nresiduals so this will be the formula that we are going to apply okay so let's\nthat we are going to apply okay so let's\nthat we are going to apply okay so let's see for the first number that that we\nsee for the first number that that we\nsee for the first number that that we want to apply so how this will get\nwant to apply so how this will get\nwant to apply so how this will get applied again I'm going to write this\napplied again I'm going to write this\napplied again I'm going to write this formula here it'll be better let's say\nformula here it'll be better let's say\nformula here it'll be better let's say here similarity weight is equal to\nhere similarity weight is equal to\nhere similarity weight is equal to summation of residual square and here\nsummation of residual square and here\nsummation of residual square and here you have number of residuals plus Lambda\nyou have number of residuals plus Lambda\nyou have number of residuals plus Lambda see previously we were using probability\nsee previously we were using probability\nsee previously we were using probability and then all those things we are using\nand then all those things we are using\nand then all those things we are using so if you want to calculate the\nso if you want to calculate the\nso if you want to calculate the similarity weight of this this will\nsimilarity weight of this this will\nsimilarity weight of this this will become 121 divided by number of residual\nbecome 121 divided by number of residual\nbecome 121 divided by number of residual is 1 plus Lambda is 0 so this is going\nis 1 plus Lambda is 0 so this is going\nis 1 plus Lambda is 0 so this is going to be 121 so here we are going to\nto be 121 so here we are going to\nto be 121 so here we are going to calculate the similarity weight which is\ncalculate the similarity weight which is\ncalculate the similarity weight which is nothing but 121 if if we probably take\nnothing but 121 if if we probably take\nnothing but 121 if if we probably take Alpha let's let's do one thing if we\nAlpha let's let's do one thing if we\nAlpha let's let's do one thing if we probably take uh if if we probably take\nprobably take uh if if we probably take\nprobably take uh if if we probably take Alpha is equal to 1 then what will\nAlpha is equal to 1 then what will\nAlpha is equal to 1 then what will happen if you take Alpha is equal to 1\nhappen if you take Alpha is equal to 1\nhappen if you take Alpha is equal to 1 just think over here what will what may\njust think over here what will what may\njust think over here what will what may happen we may directly penalize the\nhappen we may directly penalize the\nhappen we may directly penalize the similarity weight right by just adding\nsimilarity weight right by just adding\nsimilarity weight right by just adding one okay so let's do that also suppose I\none okay so let's do that also suppose I\none okay so let's do that also suppose I say I'm going to take Alpha is equal to\nsay I'm going to take Alpha is equal to\nsay I'm going to take Alpha is equal to 1 so what will happen this will not be\n1 so what will happen this will not be\n1 so what will happen this will not be the formula now now what will become 121\nthe formula now now what will become 121\nthe formula now now what will become 121 divided number of residual is 1 + 1 this\ndivided number of residual is 1 + 1 this\ndivided number of residual is 1 + 1 this is nothing but 65.5 let's say that I now\nis nothing but 65.5 let's say that I now\nis nothing but 65.5 let's say that I now have 65.5 as my similarity weight now\nhave 65.5 as my similarity weight now\nhave 65.5 as my similarity weight now similarly I will go ahead and compute\nsimilarly I will go ahead and compute\nsimilarly I will go ahead and compute the similarity weight for the next one\nthe similarity weight for the next one\nthe similarity weight for the next one so here it will become - 9 + 9 + 9 + 11\nso here it will become - 9 + 9 + 9 + 11\nso here it will become - 9 + 9 + 9 + 11 whole Square divided 4 + 1 so this and\nwhole Square divided 4 + 1 so this and\nwhole Square divided 4 + 1 so this and this will get subtracted 12 squ is\nthis will get subtracted 12 squ is\nthis will get subtracted 12 squ is nothing but 14 4 144 divid 5 so if I go\nnothing but 14 4 144 divid 5 so if I go\nnothing but 14 4 144 divid 5 so if I go ahead and calculate 144 ID 5 it is\nahead and calculate 144 ID 5 it is\nahead and calculate 144 ID 5 it is nothing but 28.5 so here I get\nnothing but 28.5 so here I get\nnothing but 28.5 so here I get 28.5 so the similarity weight for this\n28.5 so the similarity weight for this\n28.5 so the similarity weight for this is\nis\nis 28.5 similarly I can go ahead and\n28.5 similarly I can go ahead and\n28.5 similarly I can go ahead and calculate the similarity weight for this\ncalculate the similarity weight for this\ncalculate the similarity weight for this for the top one so it'll be nothing but\nfor the top one so it'll be nothing but\nfor the top one so it'll be nothing but what it will be 11 + sorry - 11 - 11 - 9\nwhat it will be 11 + sorry - 11 - 11 - 9\nwhat it will be 11 + sorry - 11 - 11 - 9 + + 1 + 9 + 11 divided 1 2 3 4 5 5 + 1\n+ + 1 + 9 + 11 divided 1 2 3 4 5 5 + 1\n+ + 1 + 9 + 11 divided 1 2 3 4 5 5 + 1 is 6 so this is getting subtracted this\nis 6 so this is getting subtracted this\nis 6 so this is getting subtracted this will be 1X 6 anyhow this will be whole\nwill be 1X 6 anyhow this will be whole\nwill be 1X 6 anyhow this will be whole square right so anyhow it will be 1X 6\nsquare right so anyhow it will be 1X 6\nsquare right so anyhow it will be 1X 6 only so 1X 6 will be my similarity\nonly so 1X 6 will be my similarity\nonly so 1X 6 will be my similarity weight over here okay 28.8 hits okay now\nweight over here okay 28.8 hits okay now\nweight over here okay 28.8 hits okay now finally The Information Gain that we\nfinally The Information Gain that we\nfinally The Information Gain that we need to compute will be very much simple\nneed to compute will be very much simple\nneed to compute will be very much simple what will be the Information Gain 65.5 +\nwhat will be the Information Gain 65.5 +\nwhat will be the Information Gain 65.5 + 28.8\n28.8\n28.8 minus 1X 6 so try to get it whatever we\nminus 1X 6 so try to get it whatever we\nminus 1X 6 so try to get it whatever we are trying to get it over here just tell\nare trying to get it over here just tell\nare trying to get it over here just tell me what will be the output is it 98.34%\n60.5 60.5 + 28 88 then this will change\n60.5 60.5 + 28 88 then this will change just a second 89.1 3 understand you\njust a second 89.1 3 understand you\njust a second 89.1 3 understand you don't have to worry about calculation\ndon't have to worry about calculation\ndon't have to worry about calculation automatically that things will be doing\nautomatically that things will be doing\nautomatically that things will be doing it okay so you don't have to worry now\nit okay so you don't have to worry now\nit okay so you don't have to worry now see we have now further the decision\nsee we have now further the decision\nsee we have now further the decision tree can be splitted into any number of\ntree can be splitted into any number of\ntree can be splitted into any number of times probably the next split what we\ntimes probably the next split what we\ntimes probably the next split what we can do is that we can we can do next\ncan do is that we can we can do next\ncan do is that we can we can do next split something like this this will be\nsplit something like this this will be\nsplit something like this this will be my experience the two splits that may\nmy experience the two splits that may\nmy experience the two splits that may happen with respect to less than or\nhappen with respect to less than or\nhappen with respect to less than or equal to 2.5 less than or equal to 2.5\nequal to 2.5 less than or equal to 2.5\nequal to 2.5 less than or equal to 2.5 or greater than 2.5 now if this probably\nor greater than 2.5 now if this probably\nor greater than 2.5 now if this probably gives the Information Gain better then\ngives the Information Gain better then\ngives the Information Gain better then the split will happen like this\nthe split will happen like this\nthe split will happen like this otherwise whichever gives the better\notherwise whichever gives the better\notherwise whichever gives the better information again the split will\ninformation again the split will\ninformation again the split will basically happen like this I hope like\nbasically happen like this I hope like\nbasically happen like this I hope like let's say that this is this is the split\nlet's say that this is this is the split\nlet's say that this is this is the split that is required - 11 - 11 is 9 is over\nthat is required - 11 - 11 is 9 is over\nthat is required - 11 - 11 is 9 is over here and then we have 1 comma 9A 11 okay\nhere and then we have 1 comma 9A 11 okay\nhere and then we have 1 comma 9A 11 okay because less than or equal to 2.5 this\nbecause less than or equal to 2.5 this\nbecause less than or equal to 2.5 this two records will definitely go over here\ntwo records will definitely go over here\ntwo records will definitely go over here and this two This Record will definitely\nand this two This Record will definitely\nand this two This Record will definitely go over here now if I try to calculate\ngo over here now if I try to calculate\ngo over here now if I try to calculate the similarity weight for this it will\nthe similarity weight for this it will\nthe similarity weight for this it will be nothing but - 11 - 9 - 11 - 9 whole S\nbe nothing but - 11 - 9 - 11 - 9 whole S\nbe nothing but - 11 - 9 - 11 - 9 whole S ided 2 + 1 right now in this particular\nided 2 + 1 right now in this particular\nided 2 + 1 right now in this particular case it will be - 20 s / 3 which is\ncase it will be - 20 s / 3 which is\ncase it will be - 20 s / 3 which is nothing but 400 2 20 into 20 is 400\nnothing but 400 2 20 into 20 is 400\nnothing but 400 2 20 into 20 is 400 which is nothing but 3 so if I go and\nwhich is nothing but 3 so if I go and\nwhich is nothing but 3 so if I go and probably use a\nprobably use a\nprobably use a calculator and show it to you\ncalculator and show it to you\ncalculator and show it to you 400 / 3 which is nothing but\n400 / 3 which is nothing but\n400 / 3 which is nothing but 133.33 so the similarity weight for this\n133.33 so the similarity weight for this\n133.33 so the similarity weight for this is\nis\nis 133.33 similarly I can go ahead and\n133.33 similarly I can go ahead and\n133.33 similarly I can go ahead and compute for this it will be 1 + 9 + 11\ncompute for this it will be 1 + 9 + 11\ncompute for this it will be 1 + 9 + 11 whole s / 3 + 1 right so it will be 10 +\nwhole s / 3 + 1 right so it will be 10 +\nwhole s / 3 + 1 right so it will be 10 + 11 10 + 11 is nothing but 21 whole s/ 4\n11 10 + 11 is nothing but 21 whole s/ 4\n11 10 + 11 is nothing but 21 whole s/ 4 so what it is 21 whole square if I open\nso what it is 21 whole square if I open\nso what it is 21 whole square if I open my calculator 21 s 21 * 21 which is\nmy calculator 21 s 21 * 21 which is\nmy calculator 21 s 21 * 21 which is nothing but 441 divid by 4 divid by 4 so\nnothing but 441 divid by 4 divid by 4 so\nnothing but 441 divid by 4 divid by 4 so this will probably 110 110.\nthis will probably 110 110.\nthis will probably 110 110. 2.25 and similarly I can go ahead and\n2.25 and similarly I can go ahead and\n2.25 and similarly I can go ahead and compute for this so if I want to compute\ncompute for this so if I want to compute\ncompute for this so if I want to compute for this what it will be the same thing\nfor this what it will be the same thing\nfor this what it will be the same thing that we have got over here that is 1x 6\nthat we have got over here that is 1x 6\nthat we have got over here that is 1x 6 so this will basically be 1X 6 so\nso this will basically be 1X 6 so\nso this will basically be 1X 6 so finally if I compute the information\nfinally if I compute the information\nfinally if I compute the information again it will be what it will be 133\nagain it will be what it will be 133\nagain it will be what it will be 133 1333 +\n1333 +\n1333 + 1.25 - 1X 6 obviously this value will be\n1.25 - 1X 6 obviously this value will be\n1.25 - 1X 6 obviously this value will be greater than the previous one what we\ngreater than the previous one what we\ngreater than the previous one what we have got that is\nhave got that is\nhave got that is 8913 so definitely we are going to use\n8913 so definitely we are going to use\n8913 so definitely we are going to use this split which is better than the\nthis split which is better than the\nthis split which is better than the previous split right let's say that this\nprevious split right let's say that this\nprevious split right let's say that this split has been considered finally how do\nsplit has been considered finally how do\nsplit has been considered finally how do we see the output okay I hope everybody\nwe see the output okay I hope everybody\nwe see the output okay I hope everybody is able to understand right let's say\nis able to understand right let's say\nis able to understand right let's say that this split has worked well so I'm\nthat this split has worked well so I'm\nthat this split has worked well so I'm going to rub all these things\ngoing to rub all these things\ngoing to rub all these things 11.25 is there now suppose I want to do\n11.25 is there now suppose I want to do\n11.25 is there now suppose I want to do the inferencing how the inferencing will\nthe inferencing how the inferencing will\nthe inferencing how the inferencing will be done\nbe done\nbe done 11.25 here 110.2 now suppose any record\n11.25 here 110.2 now suppose any record\n11.25 here 110.2 now suppose any record comes from here first of all any record\ncomes from here first of all any record\ncomes from here first of all any record that will go it will go to the base\nthat will go it will go to the base\nthat will go it will go to the base model so the base model whenever it goes\nmodel so the base model whenever it goes\nmodel so the base model whenever it goes the value is 51 51 plus alpha 1 this is\nthe value is 51 51 plus alpha 1 this is\nthe value is 51 51 plus alpha 1 this is my learning rate one suppose if it goes\nmy learning rate one suppose if it goes\nmy learning rate one suppose if it goes in this route then what we have we have\nin this route then what we have we have\nin this route then what we have we have - 11 - 9 whenever we go in this rote\n- 11 - 9 whenever we go in this rote\n- 11 - 9 whenever we go in this rote which has - 11 and - 9 the average of\nwhich has - 11 and - 9 the average of\nwhich has - 11 and - 9 the average of both these numbers will be considered\nboth these numbers will be considered\nboth these numbers will be considered what is average of both these numbers -\nwhat is average of both these numbers -\nwhat is average of both these numbers - 11 - 1 9/ 2 this is nothing but - 10\n11 - 1 9/ 2 this is nothing but - 10\n11 - 1 9/ 2 this is nothing but - 10 right so - 10 will get multiplied here\nright so - 10 will get multiplied here\nright so - 10 will get multiplied here suppose if it goes in this route then\nsuppose if it goes in this route then\nsuppose if it goes in this route then here what will happen here will 1 + 9 +\nhere what will happen here will 1 + 9 +\nhere what will happen here will 1 + 9 + 11 divide by 3 average will be taken so\n11 divide by 3 average will be taken so\n11 divide by 3 average will be taken so 21 divid 3 7 will be there so this will\n21 divid 3 7 will be there so this will\n21 divid 3 7 will be there so this will get replaced by 7 so similarly anything\nget replaced by 7 so similarly anything\nget replaced by 7 so similarly anything that you are doing this is with respect\nthat you are doing this is with respect\nthat you are doing this is with respect to decision tree 1 like this we will\nto decision tree 1 like this we will\nto decision tree 1 like this we will again construct decision tree separately\nagain construct decision tree separately\nagain construct decision tree separately and again it will become Alpha 2 by\nand again it will become Alpha 2 by\nand again it will become Alpha 2 by decision Tre 2 Alpha 3 by decision 3 3\ndecision Tre 2 Alpha 3 by decision 3 3\ndecision Tre 2 Alpha 3 by decision 3 3 and like this you will be doing till\nand like this you will be doing till\nand like this you will be doing till Alpha and decision 3 n and once you\nAlpha and decision 3 n and once you\nAlpha and decision 3 n and once you calculate this this will be your\ncalculate this this will be your\ncalculate this this will be your specific output in a regression tree so\nspecific output in a regression tree so\nspecific output in a regression tree so in this particular case what will happen\nin this particular case what will happen\nin this particular case what will happen you're just trying to play with\nyou're just trying to play with\nyou're just trying to play with parameters and you're trying to use in a\nparameters and you're trying to use in a\nparameters and you're trying to use in a different way to compute all this things\ndifferent way to compute all this things\ndifferent way to compute all this things everybody clear but again it is a\neverybody clear but again it is a\neverybody clear but again it is a blackbox model you cannot visualize all\nblackbox model you cannot visualize all\nblackbox model you cannot visualize all this things now let's go to the third\nthis things now let's go to the third\nthis things now let's go to the third algorithm which is called as s VM see\nalgorithm which is called as s VM see\nalgorithm which is called as s VM see svm is almost like decision uh logistic\nsvm is almost like decision uh logistic\nsvm is almost like decision uh logistic regression okay so the major aim of svm\nregression okay so the major aim of svm\nregression okay so the major aim of svm is\nis\nis that major aim of svm is that suppose if\nthat major aim of svm is that suppose if\nthat major aim of svm is that suppose if I have a do data points like this okay\nI have a do data points like this okay\nI have a do data points like this okay we obviously use uh logistic regression\nwe obviously use uh logistic regression\nwe obviously use uh logistic regression to split this data points right like\nto split this data points right like\nto split this data points right like this we try to create a best fit line\nthis we try to create a best fit line\nthis we try to create a best fit line which looks like this and probably based\nwhich looks like this and probably based\nwhich looks like this and probably based on this best fit line we try to divide\non this best fit line we try to divide\non this best fit line we try to divide the point now in svm what we do is that\nthe point now in svm what we do is that\nthe point now in svm what we do is that we not only create a best fit line but\nwe not only create a best fit line but\nwe not only create a best fit line but instead we also create a point which is\ninstead we also create a point which is\ninstead we also create a point which is called as marginal\ncalled as marginal\ncalled as marginal planes so like this we create some\nplanes so like this we create some\nplanes so like this we create some marginal\nmarginal\nmarginal plane so this is your hyper plane and\nplane so this is your hyper plane and\nplane so this is your hyper plane and this is your marginal plane and\nthis is your marginal plane and\nthis is your marginal plane and whichever plane has this maximum\nwhichever plane has this maximum\nwhichever plane has this maximum distance will be able to divide the\ndistance will be able to divide the\ndistance will be able to divide the points more efficiently but usually in\npoints more efficiently but usually in\npoints more efficiently but usually in in a normal scenario you know whenever\nin a normal scenario you know whenever\nin a normal scenario you know whenever we talk about hyper plane or whenever we\nwe talk about hyper plane or whenever we\nwe talk about hyper plane or whenever we talk about marginal plane there will be\ntalk about marginal plane there will be\ntalk about marginal plane there will be lot of overlapping of points right\nlot of overlapping of points right\nlot of overlapping of points right suppose if I have some specific points I\nsuppose if I have some specific points I\nsuppose if I have some specific points I have one point which looks like this I\nhave one point which looks like this I\nhave one point which looks like this I may also have another points which may\nmay also have another points which may\nmay also have another points which may overlap so it is very difficult to get\noverlap so it is very difficult to get\noverlap so it is very difficult to get an exact straight marginal planes and\nan exact straight marginal planes and\nan exact straight marginal planes and split the point based on this now this\nsplit the point based on this now this\nsplit the point based on this now this specific marginal plane should be\nspecific marginal plane should be\nspecific marginal plane should be maximum because we can create any type\nmaximum because we can create any type\nmaximum because we can create any type best fit line and probably\nbest fit line and probably\nbest fit line and probably uh use this marginal plane now if we\nuh use this marginal plane now if we\nuh use this marginal plane now if we have this overlapping right if for what\nhave this overlapping right if for what\nhave this overlapping right if for what do we call for this kind of plane this\ndo we call for this kind of plane this\ndo we call for this kind of plane this kind of plane is basically called as\nkind of plane is basically called as\nkind of plane is basically called as hard marginal plane so this is basically\nhard marginal plane so this is basically\nhard marginal plane so this is basically called as hardge marginal plane okay and\ncalled as hardge marginal plane okay and\ncalled as hardge marginal plane okay and similarly if any points are overlapping\nsimilarly if any points are overlapping\nsimilarly if any points are overlapping suppose this yellow points can also get\nsuppose this yellow points can also get\nsuppose this yellow points can also get overlapped over here and there may be\noverlapped over here and there may be\noverlapped over here and there may be some kind of Errors so for this\nsome kind of Errors so for this\nsome kind of Errors so for this particular case we basically say as soft\nparticular case we basically say as soft\nparticular case we basically say as soft marginal plane because here we will be\nmarginal plane because here we will be\nmarginal plane because here we will be able to see that errors will be there\nable to see that errors will be there\nable to see that errors will be there now in asvm what we focus on doing is\nnow in asvm what we focus on doing is\nnow in asvm what we focus on doing is that we focus on creating this marginal\nthat we focus on creating this marginal\nthat we focus on creating this marginal plane with maximum distance even though\nplane with maximum distance even though\nplane with maximum distance even though there are some errors we consider it in\nthere are some errors we consider it in\nthere are some errors we consider it in solving it by providing some kind of\nsolving it by providing some kind of\nsolving it by providing some kind of hyper parameter now how do we go ahead\nhyper parameter now how do we go ahead\nhyper parameter now how do we go ahead and basically create this all marginal\nand basically create this all marginal\nand basically create this all marginal planes and how do we go ahead with this\nplanes and how do we go ahead with this\nplanes and how do we go ahead with this it's very much simple uh just imagine in\nit's very much simple uh just imagine in\nit's very much simple uh just imagine in this specific way that initially let's\nthis specific way that initially let's\nthis specific way that initially let's consider that I have this data point\nconsider that I have this data point\nconsider that I have this data point suppose this is my\nsuppose this is my\nsuppose this is my best fit line how do we give this best\nbest fit line how do we give this best\nbest fit line how do we give this best fit line as equation we basically say\nfit line as equation we basically say\nfit line as equation we basically say yal mx + C right we we basically say\nyal mx + C right we we basically say\nyal mx + C right we we basically say this equation as y mx + C no hard hard\nthis equation as y mx + C no hard hard\nthis equation as y mx + C no hard hard marginal it is impossible in a normal\nmarginal it is impossible in a normal\nmarginal it is impossible in a normal data set obviously you'll not be able to\ndata set obviously you'll not be able to\ndata set obviously you'll not be able to get it but definitely we go ahead with\nget it but definitely we go ahead with\nget it but definitely we go ahead with creating a soft marginal plan now Y is\ncreating a soft marginal plan now Y is\ncreating a soft marginal plan now Y is equal to MX plus C what does this m\nequal to MX plus C what does this m\nequal to MX plus C what does this m indicate m is nothing but slope and C\nindicate m is nothing but slope and C\nindicate m is nothing but slope and C indicates nothing but intercept\nindicates nothing but intercept\nindicates nothing but intercept can I say that this both equations are\ncan I say that this both equations are\ncan I say that this both equations are same ax + b y + C isal 0 can I also say\nsame ax + b y + C isal 0 can I also say\nsame ax + b y + C isal 0 can I also say that this is the equation of a straight\nthat this is the equation of a straight\nthat this is the equation of a straight line can I say that this is also the\nline can I say that this is also the\nline can I say that this is also the equation of straight line I will say\nequation of straight line I will say\nequation of straight line I will say that both of them are equal can I say\nthat both of them are equal can I say\nthat both of them are equal can I say both of them are equal see if I try to\nboth of them are equal see if I try to\nboth of them are equal see if I try to prove this to you if I take this\nprove this to you if I take this\nprove this to you if I take this equation and try to find out y it will\nequation and try to find out y it will\nequation and try to find out y it will be nothing but minus C Min - c\nbe nothing but minus C Min - c\nbe nothing but minus C Min - c minus a sorry - a x and this will be\nminus a sorry - a x and this will be\nminus a sorry - a x and this will be divided by B this will be divided by\ndivided by B this will be divided by\ndivided by B this will be divided by B this will be divided by B so here you\nB this will be divided by B so here you\nB this will be divided by B so here you can see that it is almost the same in\ncan see that it is almost the same in\ncan see that it is almost the same in this particular case my M value will be\nthis particular case my M value will be\nthis particular case my M value will be - A by B and my C will basically be\n- A by B and my C will basically be\n- A by B and my C will basically be minus C by B so both the equation are\nminus C by B so both the equation are\nminus C by B so both the equation are almost same\nalmost same\nalmost same so let's consider that this is my\nso let's consider that this is my\nso let's consider that this is my equation and I am actually and whenever\nequation and I am actually and whenever\nequation and I am actually and whenever I say Y is equal to mx + C can I also\nI say Y is equal to mx + C can I also\nI say Y is equal to mx + C can I also write something like this Y is equal to\nwrite something like this Y is equal to\nwrite something like this Y is equal to W1\nW1\nW1 X1 + W2 X2 plus like this plus C or plus\nX1 + W2 X2 plus like this plus C or plus\nX1 + W2 X2 plus like this plus C or plus b same thing no so here also we can\nb same thing no so here also we can\nb same thing no so here also we can write y w transpose x + B same equation\nwrite y w transpose x + B same equation\nwrite y w transpose x + B same equation right we are basically using same\nright we are basically using same\nright we are basically using same equation yes we can also write it in a\nequation yes we can also write it in a\nequation yes we can also write it in a different way but at the end of the day\ndifferent way but at the end of the day\ndifferent way but at the end of the day we are also treating something like this\nwe are also treating something like this\nwe are also treating something like this let's say that this slope is in this\nlet's say that this slope is in this\nlet's say that this slope is in this direction if this slope is in this\ndirection if this slope is in this\ndirection if this slope is in this direction then I can basically say that\ndirection then I can basically say that\ndirection then I can basically say that let's consider that the slope is minus\nlet's consider that the slope is minus\nlet's consider that the slope is minus one\none\none let's say that this slope is minus one\nlet's say that this slope is minus one\nlet's say that this slope is minus one see it is in the negative Direction\nsee it is in the negative Direction\nsee it is in the negative Direction let's say that this slope is minus one\nlet's say that this slope is minus one\nlet's say that this slope is minus one I'm just trying to prove that this slope\nI'm just trying to prove that this slope\nI'm just trying to prove that this slope is negative value let's consider this\nis negative value let's consider this\nis negative value let's consider this now suppose this is one of my point - 4a\nnow suppose this is one of my point - 4a\nnow suppose this is one of my point - 4a 0 and obviously this particular equation\n0 and obviously this particular equation\n0 and obviously this particular equation is given by this particular line is\nis given by this particular line is\nis given by this particular line is given by this equation now if I really\ngiven by this equation now if I really\ngiven by this equation now if I really want to find out the Y value let's say\nwant to find out the Y value let's say\nwant to find out the Y value let's say that this is my\nthat this is my\nthat this is my X1 this is my X1 and this is my X2 let's\nX1 this is my X1 and this is my X2 let's\nX1 this is my X1 and this is my X2 let's say that\nsay that\nsay that I want to find out I want to find out\nI want to find out I want to find out\nI want to find out I want to find out this W transpose x + b the Y value based\nthis W transpose x + b the Y value based\nthis W transpose x + b the Y value based on this line if I want to compute the y-\non this line if I want to compute the y-\non this line if I want to compute the y- value based on this line how will I\nvalue based on this line how will I\nvalue based on this line how will I compute W transpose X basically means\ncompute W transpose X basically means\ncompute W transpose X basically means what w value what all things will be\nwhat w value what all things will be\nwhat w value what all things will be there one value is B right B is\nthere one value is B right B is\nthere one value is B right B is intercept right now intercept is passing\nintercept right now intercept is passing\nintercept right now intercept is passing from origin can I say my B will be zero\nfrom origin can I say my B will be zero\nfrom origin can I say my B will be zero obviously I can assume that b will be\nobviously I can assume that b will be\nobviously I can assume that b will be zero now in this particular case if I\nzero now in this particular case if I\nzero now in this particular case if I talk about w w in this case is minus one\ntalk about w w in this case is minus one\ntalk about w w in this case is minus one which I have initialized over here so if\nwhich I have initialized over here so if\nwhich I have initialized over here so if I want to do this matrix multiplication\nI want to do this matrix multiplication\nI want to do this matrix multiplication it will be W transpose can be written as\nit will be W transpose can be written as\nit will be W transpose can be written as like this and this x value can be\nlike this and this x value can be\nlike this and this x value can be written as -4 comma - 4 and 0 -4 and 0\nwritten as -4 comma - 4 and 0 -4 and 0\nwritten as -4 comma - 4 and 0 -4 and 0 right so I can basically write like this\nright so I can basically write like this\nright so I can basically write like this now if I do this multiplication what\nnow if I do this multiplication what\nnow if I do this multiplication what will my value I get I will basically get\nwill my value I get I will basically get\nwill my value I get I will basically get four right so this is a positive\nfour right so this is a positive\nfour right so this is a positive value this is a positive value Now\nvalue this is a positive value Now\nvalue this is a positive value Now understand since this is a positive\nunderstand since this is a positive\nunderstand since this is a positive value any points that are below this\nvalue any points that are below this\nvalue any points that are below this line any points that I consider below\nline any points that I consider below\nline any points that I consider below this line and if I try to calculate the\nthis line and if I try to calculate the\nthis line and if I try to calculate the Y can I say that it will always be\nY can I say that it will always be\nY can I say that it will always be positive yes or no similarly if I could\npositive yes or no similarly if I could\npositive yes or no similarly if I could probably consider one point over here as\nprobably consider one point over here as\nprobably consider one point over here as 4A 4A 4 now tell me in this 4A 4 if I\n4A 4A 4 now tell me in this 4A 4 if I\n4A 4A 4 now tell me in this 4A 4 if I calculate the Y value what will you get\ncalculate the Y value what will you get\ncalculate the Y value what will you get whether you'll get a positive value or a\nwhether you'll get a positive value or a\nwhether you'll get a positive value or a negative value if I try to calculate the\nnegative value if I try to calculate the\nnegative value if I try to calculate the Y value in this case because here only\nY value in this case because here only\nY value in this case because here only positive values will'll be getting right\npositive values will'll be getting right\npositive values will'll be getting right so if I calculate the Y value will the Y\nso if I calculate the Y value will the Y\nso if I calculate the Y value will the Y value be negative or positive just try\nvalue be negative or positive just try\nvalue be negative or positive just try to calculate how do you calculate again\nto calculate how do you calculate again\nto calculate how do you calculate again I will use y equation this time again my\nI will use y equation this time again my\nI will use y equation this time again my slope is minus1 my intercept is zero and\nslope is minus1 my intercept is zero and\nslope is minus1 my intercept is zero and here I will have 4 comma\nhere I will have 4 comma\nhere I will have 4 comma 4 now here Min\n4 now here Min\n4 now here Min -4 and then this is + 0 this will be Min\n-4 and then this is + 0 this will be Min\n-4 and then this is + 0 this will be Min -4 right so this will be a negative\n-4 right so this will be a negative\n-4 right so this will be a negative value negative value guys negative see -\nvalue negative value guys negative see -\nvalue negative value guys negative see - 4 + 0 negative so any point that I will\n4 + 0 negative so any point that I will\n4 + 0 negative so any point that I will probably have in top of this any\nprobably have in top of this any\nprobably have in top of this any points Above This Plane right and if I\npoints Above This Plane right and if I\npoints Above This Plane right and if I try to calculate the Y value it will\ntry to calculate the Y value it will\ntry to calculate the Y value it will always be negative so what two things\nalways be negative so what two things\nalways be negative so what two things you are able to get positive and\nyou are able to get positive and\nyou are able to get positive and negative so you can consider this\nnegative so you can consider this\nnegative so you can consider this entirely one category this another\nentirely one category this another\nentirely one category this another category at least these two things you\ncategory at least these two things you\ncategory at least these two things you can basically\ncan basically\ncan basically consider guys I hope everybody is able\nconsider guys I hope everybody is able\nconsider guys I hope everybody is able to understand this so this will be my\nto understand this so this will be my\nto understand this so this will be my one\none\none category and this will be my another\ncategory and this will be my another\ncategory and this will be my another category obviously so that basically\ncategory obviously so that basically\ncategory obviously so that basically means I can definitely use a plane and\nmeans I can definitely use a plane and\nmeans I can definitely use a plane and split this point I hope everybody is\nsplit this point I hope everybody is\nsplit this point I hope everybody is able to understand now let's go ahead\nable to understand now let's go ahead\nable to understand now let's go ahead and let's see how this marginal plane\nand let's see how this marginal plane\nand let's see how this marginal plane will get created and what is the cost\nwill get created and what is the cost\nwill get created and what is the cost function to basically do this or what is\nfunction to basically do this or what is\nfunction to basically do this or what is the cost function in making sure that\nthe cost function in making sure that\nthe cost function in making sure that the marginal plane will definitely work\nthe marginal plane will definitely work\nthe marginal plane will definitely work right it becomes difficult right so\nright it becomes difficult right so\nright it becomes difficult right so suppose let's consider an\nsuppose let's consider an\nsuppose let's consider an example suppose I say that this is my\nexample suppose I say that this is my\nexample suppose I say that this is my lines let's say uh I want to basically\nlines let's say uh I want to basically\nlines let's say uh I want to basically create a kind of I have two variety of\ncreate a kind of I have two variety of\ncreate a kind of I have two variety of points one is this point let's say I\npoints one is this point let's say I\npoints one is this point let's say I have all this points like this and the\nhave all this points like this and the\nhave all this points like this and the other points I have somewhere here let's\nother points I have somewhere here let's\nother points I have somewhere here let's consider I am just using directly good\nconsider I am just using directly good\nconsider I am just using directly good number of points so that I can split it\nnumber of points so that I can split it\nnumber of points so that I can split it okay because I will try to talk about it\nokay because I will try to talk about it\nokay because I will try to talk about it what I'm actually trying to prove so\nwhat I'm actually trying to prove so\nwhat I'm actually trying to prove so obviously this is my best fit line that\nobviously this is my best fit line that\nobviously this is my best fit line that splits and apart from that what I will\nsplits and apart from that what I will\nsplits and apart from that what I will do is that I'll also create a marginal\ndo is that I'll also create a marginal\ndo is that I'll also create a marginal points so in order to create the\npoints so in order to create the\npoints so in order to create the marginal point I may use some different\nmarginal point I may use some different\nmarginal point I may use some different color let's see which color this will be\ncolor let's see which color this will be\ncolor let's see which color this will be my one marginal point remember it will\nmy one marginal point remember it will\nmy one marginal point remember it will be to the nearest point over here and\nbe to the nearest point over here and\nbe to the nearest point over here and basically we will construct like like\nbasically we will construct like like\nbasically we will construct like like this and similarly here we will be\nthis and similarly here we will be\nthis and similarly here we will be constructing like this I've already told\nconstructing like this I've already told\nconstructing like this I've already told you guys this equation can be mentioned\nyou guys this equation can be mentioned\nyou guys this equation can be mentioned at w transpose x + B = 0 right I can\nat w transpose x + B = 0 right I can\nat w transpose x + B = 0 right I can definitely say this because ax + b y + C\ndefinitely say this because ax + b y + C\ndefinitely say this because ax + b y + C is equal to 0 so this I can also write\nis equal to 0 so this I can also write\nis equal to 0 so this I can also write it as W transpose x equal to 0 sorry\nit as W transpose x equal to 0 sorry\nit as W transpose x equal to 0 sorry plus b plus b equal to 0 so both are\nplus b plus b equal to 0 so both are\nplus b plus b equal to 0 so both are same okay this I don't have to prove it\nsame okay this I don't have to prove it\nsame okay this I don't have to prove it I hope everybody's clear with this now\nI hope everybody's clear with this now\nI hope everybody's clear with this now what I'm going to do let's represent\nwhat I'm going to do let's represent\nwhat I'm going to do let's represent this line also with some equation so\nthis line also with some equation so\nthis line also with some equation so this line if I want to represent this\nthis line if I want to represent this\nthis line if I want to represent this will be W transpose x + B what value\nwill be W transpose x + B what value\nwill be W transpose x + B what value will come over here positive or negative\nwill come over here positive or negative\nwill come over here positive or negative C from this line anything above this\nC from this line anything above this\nC from this line anything above this plane right any any any distance that we\nplane right any any any distance that we\nplane right any any any distance that we try to find out it will always be\ntry to find out it will always be\ntry to find out it will always be negative so let's say that I'm using it\nnegative so let's say that I'm using it\nnegative so let's say that I'm using it as minus one to just read as it is a\nas minus one to just read as it is a\nas minus one to just read as it is a negative value and this line that I am\nnegative value and this line that I am\nnegative value and this line that I am going to mention it it will be W\ngoing to mention it it will be W\ngoing to mention it it will be W transpose x + B is equal to + 1 Min -1\ntranspose x + B is equal to + 1 Min -1\ntranspose x + B is equal to + 1 Min -1 above + 1 because we have already\nabove + 1 because we have already\nabove + 1 because we have already discussed from this point if you're\ndiscussed from this point if you're\ndiscussed from this point if you're trying to calculate the Y value it is\ntrying to calculate the Y value it is\ntrying to calculate the Y value it is always going to be + one this is going\nalways going to be + one this is going\nalways going to be + one this is going to be minus one here I should definitely\nto be minus one here I should definitely\nto be minus one here I should definitely say this as K okay but I'm not\nsay this as K okay but I'm not\nsay this as K okay but I'm not mentioning K in many articles you'll see\nmentioning K in many articles you'll see\nmentioning K in many articles you'll see it as minus one uh many research paper\nit as minus one uh many research paper\nit as minus one uh many research paper also they use it as minus one but I\nalso they use it as minus one but I\nalso they use it as minus one but I would like to specify uh minus and plus\nwould like to specify uh minus and plus\nwould like to specify uh minus and plus K but here let's go and write minus1 and\nK but here let's go and write minus1 and\nK but here let's go and write minus1 and plus now my aim is to increase this\nplus now my aim is to increase this\nplus now my aim is to increase this distance okay this distance I really\ndistance okay this distance I really\ndistance okay this distance I really want to increase this distance now in\nwant to increase this distance now in\nwant to increase this distance now in order to increase this if I increase\norder to increase this if I increase\norder to increase this if I increase this distance that basically means my\nthis distance that basically means my\nthis distance that basically means my model is performing well so let's say I\nmodel is performing well so let's say I\nmodel is performing well so let's say I want to find this distance first of all\nwant to find this distance first of all\nwant to find this distance first of all so if I write w transpose X Plus Bal to\nso if I write w transpose X Plus Bal to\nso if I write w transpose X Plus Bal to 1 and here I will write w transpose x +\n1 and here I will write w transpose x +\n1 and here I will write w transpose x + B isal minus1 so what I'm going to do\nB isal minus1 so what I'm going to do\nB isal minus1 so what I'm going to do I'm going to do the computation and\nI'm going to do the computation and\nI'm going to do the computation and subtract it like this so here obviously\nsubtract it like this so here obviously\nsubtract it like this so here obviously this will be my X1 this will be my X2\nthis will be my X1 this will be my X2\nthis will be my X1 this will be my X2 okay because these are my another points\nokay because these are my another points\nokay because these are my another points X2 and X1 so I can write w transpose X1\nX2 and X1 so I can write w transpose X1\nX2 and X1 so I can write w transpose X1 -\n-\n- X2 B and B will get cancell and here I\nX2 B and B will get cancell and here I\nX2 B and B will get cancell and here I will be writing two right so from here\nwill be writing two right so from here\nwill be writing two right so from here we can definitely write two different\nwe can definitely write two different\nwe can definitely write two different things let's see what all things we can\nthings let's see what all things we can\nthings let's see what all things we can write so here this is nothing but the\nwrite so here this is nothing but the\nwrite so here this is nothing but the difference between my this plane and\ndifference between my this plane and\ndifference between my this plane and this plane which is given by like this\nthis plane which is given by like this\nthis plane which is given by like this okay now always understand whenever we\nokay now always understand whenever we\nokay now always understand whenever we consider any any vector vors right any\nconsider any any vector vors right any\nconsider any any vector vors right any vectors right it also has something\nvectors right it also has something\nvectors right it also has something called as\ncalled as\ncalled as magnitude so if I want to remove this\nmagnitude so if I want to remove this\nmagnitude so if I want to remove this magnitude I can divide this by W this\nmagnitude I can divide this by W this\nmagnitude I can divide this by W this magnitude of w then only my Vector will\nmagnitude of w then only my Vector will\nmagnitude of w then only my Vector will remain which is indicated like this so\nremain which is indicated like this so\nremain which is indicated like this so I'm going to basically divide by this\nI'm going to basically divide by this\nI'm going to basically divide by this particular operation both both the side\nparticular operation both both the side\nparticular operation both both the side I'm dividing by this magnitude of w and\nI'm dividing by this magnitude of w and\nI'm dividing by this magnitude of w and I don't care about the directions over\nI don't care about the directions over\nI don't care about the directions over here right now we just care about the\nhere right now we just care about the\nhere right now we just care about the vectors now when I write like this what\nvectors now when I write like this what\nvectors now when I write like this what is our aim our aim is to can I say our\nis our aim our aim is to can I say our\nis our aim our aim is to can I say our aim is to our aim is to\naim is to our aim is to\naim is to our aim is to maximize 2 byw can I say this guys yes\nmaximize 2 byw can I say this guys yes\nmaximize 2 byw can I say this guys yes or\nno what is our aim our aim is to\nno what is our aim our aim is to basically maximize this right by\nbasically maximize this right by\nbasically maximize this right by updating W comma B value I need to\nupdating W comma B value I need to\nupdating W comma B value I need to maximize this yes everybody's clear with\nmaximize this yes everybody's clear with\nmaximize this yes everybody's clear with this can I say that yes I want to\nthis can I say that yes I want to\nthis can I say that yes I want to maximize this yes or no everybody I want\nmaximize this yes or no everybody I want\nmaximize this yes or no everybody I want to maximize this if I maximize this that\nto maximize this if I maximize this that\nto maximize this if I maximize this that basically means my marginal plane will\nbasically means my marginal plane will\nbasically means my marginal plane will become bigger my marginal plane will be\nbecome bigger my marginal plane will be\nbecome bigger my marginal plane will be bigger okay now can I write along with\nbigger okay now can I write along with\nbigger okay now can I write along with this that such that y of I my output\nthis that such that y of I my output\nthis that such that y of I my output will be dependent on two different\nwill be dependent on two different\nwill be dependent on two different things one is I can say that my y y of I\nthings one is I can say that my y y of I\nthings one is I can say that my y y of I is plus of uh is + one when w transpose\nis plus of uh is + one when w transpose\nis plus of uh is + one when w transpose x + B is greater than or equal to 1\nx + B is greater than or equal to 1\nx + B is greater than or equal to 1 everybody see in this equation what I'm\neverybody see in this equation what I'm\neverybody see in this equation what I'm actually trying to specify such that y\nactually trying to specify such that y\nactually trying to specify such that y of I is + 1 when w transpose x + B is\nof I is + 1 when w transpose x + B is\nof I is + 1 when w transpose x + B is greater than 1 and when it is minus 1\ngreater than 1 and when it is minus 1\ngreater than 1 and when it is minus 1 that basically means w transpose of X is\nthat basically means w transpose of X is\nthat basically means w transpose of X is B is less than or equal to minus now\nB is less than or equal to minus now\nB is less than or equal to minus now what does this basically mean see all my\nwhat does this basically mean see all my\nwhat does this basically mean see all my values whenever I compute W transpose x\nvalues whenever I compute W transpose x\nvalues whenever I compute W transpose x + B is greater than or equal to 1 I'm\n+ B is greater than or equal to 1 I'm\n+ B is greater than or equal to 1 I'm obviously going to get this + one when w\nobviously going to get this + one when w\nobviously going to get this + one when w transpose X+ B is less than or equal to\ntranspose X+ B is less than or equal to\ntranspose X+ B is less than or equal to 1 I'm always going to get the output as\n1 I'm always going to get the output as\n1 I'm always going to get the output as minus one I hope that is the reason why\nminus one I hope that is the reason why\nminus one I hope that is the reason why I have actually written like this so\nI have actually written like this so\nI have actually written like this so this two we have already discussed why\nthis two we have already discussed why\nthis two we have already discussed why we are specifically writing we want to\nwe are specifically writing we want to\nwe are specifically writing we want to increase the marginal plane which is\nincrease the marginal plane which is\nincrease the marginal plane which is this this is my marginal plane and I'm\nthis this is my marginal plane and I'm\nthis this is my marginal plane and I'm writing one condition that my Yi value\nwriting one condition that my Yi value\nwriting one condition that my Yi value will be+ one when w transpose X plus b\nwill be+ one when w transpose X plus b\nwill be+ one when w transpose X plus b is greater than or equal to 1 otherwise\nis greater than or equal to 1 otherwise\nis greater than or equal to 1 otherwise it when it is less than or equal to\nit when it is less than or equal to\nit when it is less than or equal to minus one it is going to be very much\nminus one it is going to be very much\nminus one it is going to be very much clear with this transpose condition we\nclear with this transpose condition we\nclear with this transpose condition we have already done it everybody clear\nhave already done it everybody clear\nhave already done it everybody clear with this now on top of it we can add\nwith this now on top of it we can add\nwith this now on top of it we can add one more very important Point instead of\none more very important Point instead of\none more very important Point instead of writing such that and all you can also\nwriting such that and all you can also\nwriting such that and all you can also say that our major\nsay that our major\nsay that our major aim our major aim is that if I multiply\naim our major aim is that if I multiply\naim our major aim is that if I multiply y i multiplied by W transpose X of I + B\ny i multiplied by W transpose X of I + B\ny i multiplied by W transpose X of I + B If I multiply this two this will always\nIf I multiply this two this will always\nIf I multiply this two this will always be able greater than or equal to 1 for\nbe able greater than or equal to 1 for\nbe able greater than or equal to 1 for correct points right for correct points\ncorrect points right for correct points\ncorrect points right for correct points because understand if it is minus one if\nbecause understand if it is minus one if\nbecause understand if it is minus one if I'm multiplying with this and if it is a\nI'm multiplying with this and if it is a\nI'm multiplying with this and if it is a correct Point minus into minus will\ncorrect Point minus into minus will\ncorrect Point minus into minus will obviously be greater than or equal to\nobviously be greater than or equal to\nobviously be greater than or equal to one only right similarly for this it\none only right similarly for this it\none only right similarly for this it will be greater than 1 so I can also\nwill be greater than 1 so I can also\nwill be greater than 1 so I can also definitely say that my major M If I\ndefinitely say that my major M If I\ndefinitely say that my major M If I multiply y of I with this it will be\nmultiply y of I with this it will be\nmultiply y of I with this it will be always greater than or equal to + 1 U\nalways greater than or equal to + 1 U\nalways greater than or equal to + 1 U which is definitely saying that it will\nwhich is definitely saying that it will\nwhich is definitely saying that it will be a positive value so this is just a\nbe a positive value so this is just a\nbe a positive value so this is just a representation guys but understand what\nrepresentation guys but understand what\nrepresentation guys but understand what is the minimized cost function this is\nis the minimized cost function this is\nis the minimized cost function this is my minimized cost function maximized\nmy minimized cost function maximized\nmy minimized cost function maximized cost function now I'm going to again\ncost function now I'm going to again\ncost function now I'm going to again write it down\nwrite it down\nwrite it down maximize W comma B maximize W comma b 2\nmaximize W comma B maximize W comma b 2\nmaximize W comma B maximize W comma b 2 by magnitude of w I can also write\nby magnitude of w I can also write\nby magnitude of w I can also write something like this minimize W comma B\nsomething like this minimize W comma B\nsomething like this minimize W comma B and I can just inverse this which looks\nand I can just inverse this which looks\nand I can just inverse this which looks like this are these both are same or not\nlike this are these both are same or not\nlike this are these both are same or not because always understand in machine\nbecause always understand in machine\nbecause always understand in machine learning algorithm why do we write\nlearning algorithm why do we write\nlearning algorithm why do we write minimize things because we are trying to\nminimize things because we are trying to\nminimize things because we are trying to minimize something okay both are\nminimize something okay both are\nminimize something okay both are equivalent these both are equivalent and\nequivalent these both are equivalent and\nequivalent these both are equivalent and why we specifically write minimization\nwhy we specifically write minimization\nwhy we specifically write minimization because in the back propagation when we\nbecause in the back propagation when we\nbecause in the back propagation when we we are continuously updating the weights\nwe are continuously updating the weights\nwe are continuously updating the weights of w and B so we can definitely write\nof w and B so we can definitely write\nof w and B so we can definitely write like this so here my main target is to\nlike this so here my main target is to\nlike this so here my main target is to minimize this particular value by\nminimize this particular value by\nminimize this particular value by changing W and B and I will start adding\nchanging W and B and I will start adding\nchanging W and B and I will start adding some more parameters over here this is\nsome more parameters over here this is\nsome more parameters over here this is fine till here I think everybody has got\nfine till here I think everybody has got\nfine till here I think everybody has got it this is our aim and we are going to\nit this is our aim and we are going to\nit this is our aim and we are going to do this but I'm going to add two more\ndo this but I'm going to add two more\ndo this but I'm going to add two more parameters in this Optimizer one is C of\nparameters in this Optimizer one is C of\nparameters in this Optimizer one is C of I and one is summation of I equal 1 to n\nI and one is summation of I equal 1 to n\nI and one is summation of I equal 1 to n and here I will use something called as\nand here I will use something called as\nand here I will use something called as EA EA of I first of all I'll tell what\nEA EA of I first of all I'll tell what\nEA EA of I first of all I'll tell what is C of I see if I have this specific\nis C of I see if I have this specific\nis C of I see if I have this specific data point let's say if some of my\ndata point let's say if some of my\ndata point let's say if some of my points are over here then is it a right\npoints are over here then is it a right\npoints are over here then is it a right right prediction or wrong prediction if\nright prediction or wrong prediction if\nright prediction or wrong prediction if some of my points are over here is it a\nsome of my points are over here is it a\nsome of my points are over here is it a right prediction or wrong prediction\nright prediction or wrong prediction\nright prediction or wrong prediction obviously it is a wrong prediction if my\nobviously it is a wrong prediction if my\nobviously it is a wrong prediction if my points are somewhere here is it a WR\npoints are somewhere here is it a WR\npoints are somewhere here is it a WR prediction wrong wrong incorrect\nprediction wrong wrong incorrect\nprediction wrong wrong incorrect prediction right so this C value\nprediction right so this C value\nprediction right so this C value basically says that how many errors we\nbasically says that how many errors we\nbasically says that how many errors we can have how many errors we can have if\ncan have how many errors we can have if\ncan have how many errors we can have if it says that fine we can have six errors\nit says that fine we can have six errors\nit says that fine we can have six errors or seven errors how many errors we can\nor seven errors how many errors we can\nor seven errors how many errors we can have even though we are using the\nhave even though we are using the\nhave even though we are using the marginal plane how many errors we can\nmarginal plane how many errors we can\nmarginal plane how many errors we can have so here I'm specifically writing\nhave so here I'm specifically writing\nhave so here I'm specifically writing how many errors we can have this is what\nhow many errors we can have this is what\nhow many errors we can have this is what is specified by C ofi EA of I basically\nis specified by C ofi EA of I basically\nis specified by C ofi EA of I basically says that what is the summation of I'm\nsays that what is the summation of I'm\nsays that what is the summation of I'm going to write it down since we are\ngoing to write it down since we are\ngoing to write it down since we are doing the sumission this entire term\ndoing the sumission this entire term\ndoing the sumission this entire term basically mentions that sumission\nbasically mentions that sumission\nbasically mentions that sumission of the distance of the values distance\nof the distance of the values distance\nof the distance of the values distance of the wrong points and how do we\nof the wrong points and how do we\nof the wrong points and how do we calculate the distance from here to here\ncalculate the distance from here to here\ncalculate the distance from here to here suppose this is a wrong point I will try\nsuppose this is a wrong point I will try\nsuppose this is a wrong point I will try to calculate the distance from here to\nto calculate the distance from here to\nto calculate the distance from here to here I will do the sumission of this\nhere I will do the sumission of this\nhere I will do the sumission of this I'll do the sumission of this I will do\nI'll do the sumission of this I will do\nI'll do the sumission of this I will do the sumission of this similarly for the\nthe sumission of this similarly for the\nthe sumission of this similarly for the Green Point another sumission will\nGreen Point another sumission will\nGreen Point another sumission will happen from here to here like this here\nhappen from here to here like this here\nhappen from here to here like this here to here and we going to do that specific\nto here and we going to do that specific\nto here and we going to do that specific sumission so we are telling that fine if\nsumission so we are telling that fine if\nsumission so we are telling that fine if you are not able to fit properly try to\nyou are not able to fit properly try to\nyou are not able to fit properly try to apply this two hyperparameters and try\napply this two hyperparameters and try\napply this two hyperparameters and try to make sure that this many errors are\nto make sure that this many errors are\nto make sure that this many errors are also there it is well and good no\nalso there it is well and good no\nalso there it is well and good no problem we will go ahead with that try\nproblem we will go ahead with that try\nproblem we will go ahead with that try to do the submission of the data points\nto do the submission of the data points\nto do the submission of the data points and based on that try to construct the\nand based on that try to construct the\nand based on that try to construct the best fit line along with the marginal\nbest fit line along with the marginal\nbest fit line along with the marginal plane like this even though there are\nplane like this even though there are\nplane like this even though there are some errors over here or errors over\nsome errors over here or errors over\nsome errors over here or errors over here we are good to go with respect one\nhere we are good to go with respect one\nhere we are good to go with respect one more thing is there which is called as\nmore thing is there which is called as\nmore thing is there which is called as Al svr svr only one thing is getting\nAl svr svr only one thing is getting\nAl svr svr only one thing is getting changed in svr only this value will get\nchanged in svr only this value will get\nchanged in svr only this value will get changed so I want you all to explore and\nchanged so I want you all to explore and\nchanged so I want you all to explore and just let me know this will be one\njust let me know this will be one\njust let me know this will be one assignment for you only this value will\nassignment for you only this value will\nassignment for you only this value will be changing remaining everything are\nbe changing remaining everything are\nbe changing remaining everything are same so just try to if you change this\nsame so just try to if you change this\nsame so just try to if you change this particular value that becomes an svr\nparticular value that becomes an svr\nparticular value that becomes an svr just try to explore and just try to find\njust try to explore and just try to find\njust try to explore and just try to find out and just try to let me know so\nout and just try to let me know so\nout and just try to let me know so overall uh did you like the entire\noverall uh did you like the entire\noverall uh did you like the entire session everyone okay in this one more\nsession everyone okay in this one more\nsession everyone okay in this one more thing is there which is called as kernel\nthing is there which is called as kernel\nthing is there which is called as kernel Matrix svm kernel we say it as svm\nMatrix svm kernel we say it as svm\nMatrix svm kernel we say it as svm kernel now in s VM kernel what happens\nkernel now in s VM kernel what happens\nkernel now in s VM kernel what happens suppose if I have a specific data points\nsuppose if I have a specific data points\nsuppose if I have a specific data points which looks like this which looks like\nwhich looks like this which looks like\nwhich looks like this which looks like this so we obviously cannot use a\nthis so we obviously cannot use a\nthis so we obviously cannot use a straight line and try to divide it so\nstraight line and try to divide it so\nstraight line and try to divide it so what we do we convert this two Dimension\nwhat we do we convert this two Dimension\nwhat we do we convert this two Dimension into three dimensions and then probably\ninto three dimensions and then probably\ninto three dimensions and then probably we push our Point like this one point\nwe push our Point like this one point\nwe push our Point like this one point will go like this and the white point\nwill go like this and the white point\nwill go like this and the white point will go down and then we can basically\nwill go down and then we can basically\nwill go down and then we can basically use a plane to split it so I uploaded a\nuse a plane to split it so I uploaded a\nuse a plane to split it so I uploaded a video around uh around that and uh you\nvideo around uh around that and uh you\nvideo around uh around that and uh you can definitely have a look onto that and\ncan definitely have a look onto that and\ncan definitely have a look onto that and I have also shown you practically how to\nI have also shown you practically how to\nI have also shown you practically how to do it that is the reason I've created\ndo it that is the reason I've created\ndo it that is the reason I've created that specific video so great uh this was\nthat specific video so great uh this was\nthat specific video so great uh this was it from my side I hope you like this\nit from my side I hope you like this\nit from my side I hope you like this session so thank you everyone have a\nsession so thank you everyone have a\nsession so thank you everyone have a great day keep on rocking keep on\ngreat day keep on rocking keep on\ngreat day keep on rocking keep on learning and never give up"
  },
  {
    "id": 258990554,
    "timestamp": "2026-02-05T01:42:48.582Z",
    "title": "All Machine Learning algorithms explained in 17 min",
    "url": "https://www.youtube.com/watch?v=E0Hmnixke2g&t=108s",
    "text": "in the next 17 minutes I will give you\nin the next 17 minutes I will give you an overview of the most important\nan overview of the most important\nan overview of the most important machine learning algorithms to help you\nmachine learning algorithms to help you\nmachine learning algorithms to help you decide which one is right for your\ndecide which one is right for your\ndecide which one is right for your problem my name is Tim and I have been a\nproblem my name is Tim and I have been a\nproblem my name is Tim and I have been a data scientist for over 10 years and\ndata scientist for over 10 years and\ndata scientist for over 10 years and taught all of these algorithms to\ntaught all of these algorithms to\ntaught all of these algorithms to hundreds of students in real life\nhundreds of students in real life\nhundreds of students in real life machine learning boot camps there is a\nmachine learning boot camps there is a\nmachine learning boot camps there is a simple strategy for picking the right\nsimple strategy for picking the right\nsimple strategy for picking the right algorithm for your problem in 17 minutes\nalgorithm for your problem in 17 minutes\nalgorithm for your problem in 17 minutes you will know how to pick the right one\nyou will know how to pick the right one\nyou will know how to pick the right one for any problem and get a basic\nfor any problem and get a basic\nfor any problem and get a basic intuition of each algorithm and how they\nintuition of each algorithm and how they\nintuition of each algorithm and how they relate to each other my goal is to give\nrelate to each other my goal is to give\nrelate to each other my goal is to give as many of you as possible an intuitive\nas many of you as possible an intuitive\nas many of you as possible an intuitive understanding of the major machine\nunderstanding of the major machine\nunderstanding of the major machine learning algorithms to make you stop\nlearning algorithms to make you stop\nlearning algorithms to make you stop feeling overwhelmed according to\nfeeling overwhelmed according to\nfeeling overwhelmed according to Wikipedia machine learning is a field of\nWikipedia machine learning is a field of\nWikipedia machine learning is a field of study in artificial intelligence\nstudy in artificial intelligence\nstudy in artificial intelligence concerned with the development and study\nconcerned with the development and study\nconcerned with the development and study of statistical algorithms that can learn\nof statistical algorithms that can learn\nof statistical algorithms that can learn from data and generalize to unseen data\nfrom data and generalize to unseen data\nfrom data and generalize to unseen data and thus perform tasks without explicit\nand thus perform tasks without explicit\nand thus perform tasks without explicit instructions much of the recent\ninstructions much of the recent\ninstructions much of the recent advancements in AI are driven by neural\nadvancements in AI are driven by neural\nadvancements in AI are driven by neural networks which I hope to give you an\nnetworks which I hope to give you an\nnetworks which I hope to give you an intuitive understanding of by the end of\nintuitive understanding of by the end of\nintuitive understanding of by the end of this video Let's divide machine learning\nthis video Let's divide machine learning\nthis video Let's divide machine learning into its subfields generally machine\ninto its subfields generally machine\ninto its subfields generally machine learning is divided into two areas\nlearning is divided into two areas\nlearning is divided into two areas supervised learning and unsupervised\nsupervised learning and unsupervised\nsupervised learning and unsupervised learning supervised learning is when we\nlearning supervised learning is when we\nlearning supervised learning is when we have a data set with any number of\nhave a data set with any number of\nhave a data set with any number of independent variables also called\nindependent variables also called\nindependent variables also called features or input variables and a\nfeatures or input variables and a\nfeatures or input variables and a dependent variable also called Target or\ndependent variable also called Target or\ndependent variable also called Target or output variable that is supposed to be\noutput variable that is supposed to be\noutput variable that is supposed to be predicted we have a so-called training\npredicted we have a so-called training\npredicted we have a so-called training data set where we know the True Values\ndata set where we know the True Values\ndata set where we know the True Values for the output variable also called\nfor the output variable also called\nfor the output variable also called labels that we can train our algorithm\nlabels that we can train our algorithm\nlabels that we can train our algorithm on to later predict the output variable\non to later predict the output variable\non to later predict the output variable for new unknown data examples could be\nfor new unknown data examples could be\nfor new unknown data examples could be predicting the price of a house the\npredicting the price of a house the\npredicting the price of a house the output variable based on features of the\noutput variable based on features of the\noutput variable based on features of the house say square footage location year\nhouse say square footage location year\nhouse say square footage location year of construction Etc categorizing an\nof construction Etc categorizing an\nof construction Etc categorizing an object as a cat or a dog the output\nobject as a cat or a dog the output\nobject as a cat or a dog the output variable or label based on features of\nvariable or label based on features of\nvariable or label based on features of the object say height weight size of the\nthe object say height weight size of the\nthe object say height weight size of the ears color of the eyes Etc unsupervised\nears color of the eyes Etc unsupervised\nears color of the eyes Etc unsupervised learning is basically any learning\nlearning is basically any learning\nlearning is basically any learning problem that is not supervised so where\nproblem that is not supervised so where\nproblem that is not supervised so where no truth about the data is known so\nno truth about the data is known so\nno truth about the data is known so where a supervised algorithm would be\nwhere a supervised algorithm would be\nwhere a supervised algorithm would be like showing a little kid what a typical\nlike showing a little kid what a typical\nlike showing a little kid what a typical cat looks like and what a typical dog\ncat looks like and what a typical dog\ncat looks like and what a typical dog looks like and then giving it a new\nlooks like and then giving it a new\nlooks like and then giving it a new picture and asking it what animal it\npicture and asking it what animal it\npicture and asking it what animal it sees an unsupervised algorithm would be\nsees an unsupervised algorithm would be\nsees an unsupervised algorithm would be giving a kid with no idea of what cats\ngiving a kid with no idea of what cats\ngiving a kid with no idea of what cats and dogs are a pile of pictures of\nand dogs are a pile of pictures of\nand dogs are a pile of pictures of animals and asking it to group by\nanimals and asking it to group by\nanimals and asking it to group by similarity without any further\nsimilarity without any further\nsimilarity without any further instructions examples of unsupervised\ninstructions examples of unsupervised\ninstructions examples of unsupervised problems might be to sort all of your\nproblems might be to sort all of your\nproblems might be to sort all of your emails into three unspecified categories\nemails into three unspecified categories\nemails into three unspecified categories which you can then later inspect and\nwhich you can then later inspect and\nwhich you can then later inspect and name as you wish the algorithm will\nname as you wish the algorithm will\nname as you wish the algorithm will decide on its own how it will create\ndecide on its own how it will create\ndecide on its own how it will create those categories also called clusters\nthose categories also called clusters\nthose categories also called clusters let's start with supervised learning\nlet's start with supervised learning\nlet's start with supervised learning arguably the bigger and more important\narguably the bigger and more important\narguably the bigger and more important branch of machine learning there are\nbranch of machine learning there are\nbranch of machine learning there are broadly two subcategories in regression\nbroadly two subcategories in regression\nbroadly two subcategories in regression we want to predict a continuous numeric\nwe want to predict a continuous numeric\nwe want to predict a continuous numeric Target variable for a given input\nTarget variable for a given input\nTarget variable for a given input variable using the example from before\nvariable using the example from before\nvariable using the example from before it could be predicting the price of a\nit could be predicting the price of a\nit could be predicting the price of a house given any number features of a\nhouse given any number features of a\nhouse given any number features of a house and determining their relationship\nhouse and determining their relationship\nhouse and determining their relationship to the final price of the house we might\nto the final price of the house we might\nto the final price of the house we might for example find out that square footage\nfor example find out that square footage\nfor example find out that square footage is directly proportional to the price\nis directly proportional to the price\nis directly proportional to the price linear dependence but that the age of\nlinear dependence but that the age of\nlinear dependence but that the age of the house has no influence on the price\nthe house has no influence on the price\nthe house has no influence on the price of the house in classification we try to\nof the house in classification we try to\nof the house in classification we try to assign a discrete categorical label also\nassign a discrete categorical label also\nassign a discrete categorical label also called a class to a data point for\ncalled a class to a data point for\ncalled a class to a data point for example we may want to assign the label\nexample we may want to assign the label\nexample we may want to assign the label spam or no spam to an email based on its\nspam or no spam to an email based on its\nspam or no spam to an email based on its content sender and so on but we could\ncontent sender and so on but we could\ncontent sender and so on but we could also have more than two classes for\nalso have more than two classes for\nalso have more than two classes for example junk primary social promotions\nexample junk primary social promotions\nexample junk primary social promotions and updates as Gmail does by default now\nand updates as Gmail does by default now\nand updates as Gmail does by default now let's dive into the actual algorithms\nlet's dive into the actual algorithms\nlet's dive into the actual algorithms starting with the mother of all machine\nstarting with the mother of all machine\nstarting with the mother of all machine learning algorithms linear regression in\nlearning algorithms linear regression in\nlearning algorithms linear regression in general supervised learning algorithms\ngeneral supervised learning algorithms\ngeneral supervised learning algorithms try to determine the relationship\ntry to determine the relationship\ntry to determine the relationship between two variables we try to find the\nbetween two variables we try to find the\nbetween two variables we try to find the function that Maps one to the other\nfunction that Maps one to the other\nfunction that Maps one to the other linear regression in its simplest form\nlinear regression in its simplest form\nlinear regression in its simplest form is trying to determine a linear\nis trying to determine a linear\nis trying to determine a linear relationship between two variables\nrelationship between two variables\nrelationship between two variables namely the input and the output we want\nnamely the input and the output we want\nnamely the input and the output we want to fit a linear equation to the data by\nto fit a linear equation to the data by\nto fit a linear equation to the data by minimizing the sum of squares of the\nminimizing the sum of squares of the\nminimizing the sum of squares of the distances between data points and the\ndistances between data points and the\ndistances between data points and the regression Line This simply minimizes\nregression Line This simply minimizes\nregression Line This simply minimizes the average distance of the real data to\nthe average distance of the real data to\nthe average distance of the real data to our predictive model in this case the\nour predictive model in this case the\nour predictive model in this case the regression line and should therefore\nregression line and should therefore\nregression line and should therefore minimize prediction errors for new data\nminimize prediction errors for new data\nminimize prediction errors for new data points a simple example of a linear\npoints a simple example of a linear\npoints a simple example of a linear relationship might be the height and\nrelationship might be the height and\nrelationship might be the height and shoe size of a person where the\nshoe size of a person where the\nshoe size of a person where the regression fit might tell us that for\nregression fit might tell us that for\nregression fit might tell us that for every one unit of shoe size increase the\nevery one unit of shoe size increase the\nevery one unit of shoe size increase the person will be on average 2 Ines taller\nperson will be on average 2 Ines taller\nperson will be on average 2 Ines taller you can make your model more complex and\nyou can make your model more complex and\nyou can make your model more complex and fit multi-dimensional data to an output\nfit multi-dimensional data to an output\nfit multi-dimensional data to an output variable in the example of the shoe size\nvariable in the example of the shoe size\nvariable in the example of the shoe size you might for example want to include\nyou might for example want to include\nyou might for example want to include the gender age and ethnicity of the\nthe gender age and ethnicity of the\nthe gender age and ethnicity of the person to get an even better model many\nperson to get an even better model many\nperson to get an even better model many of the very fancy machine learning\nof the very fancy machine learning\nof the very fancy machine learning algorithms including neural networks are\nalgorithms including neural networks are\nalgorithms including neural networks are just extensions of this very simple idea\njust extensions of this very simple idea\njust extensions of this very simple idea as I will show you later in the video\nas I will show you later in the video\nas I will show you later in the video logistic regression is a variant of\nlogistic regression is a variant of\nlogistic regression is a variant of linear regression and probably the most\nlinear regression and probably the most\nlinear regression and probably the most basic classification algorithm instead\nbasic classification algorithm instead\nbasic classification algorithm instead of fitting a line to two numerical\nof fitting a line to two numerical\nof fitting a line to two numerical variables with a presumably linear\nvariables with a presumably linear\nvariables with a presumably linear relationship you now try to predict a\nrelationship you now try to predict a\nrelationship you now try to predict a categorical output variable using\ncategorical output variable using\ncategorical output variable using categorical or numerical input variables\ncategorical or numerical input variables\ncategorical or numerical input variables let's look at an example we now want to\nlet's look at an example we now want to\nlet's look at an example we now want to predict one of two classes for example\npredict one of two classes for example\npredict one of two classes for example the gender of a person based on height\nthe gender of a person based on height\nthe gender of a person based on height and weight so a linear regression\nand weight so a linear regression\nand weight so a linear regression wouldn't make much sense anymore instead\nwouldn't make much sense anymore instead\nwouldn't make much sense anymore instead of fitting a line to the data we now fit\nof fitting a line to the data we now fit\nof fitting a line to the data we now fit a so-called sigmoid function to the data\na so-called sigmoid function to the data\na so-called sigmoid function to the data which looks like this the equation will\nwhich looks like this the equation will\nwhich looks like this the equation will now not tell us about a linear\nnow not tell us about a linear\nnow not tell us about a linear relationship between two variables but\nrelationship between two variables but\nrelationship between two variables but will now conveniently tell us the\nwill now conveniently tell us the\nwill now conveniently tell us the probability of a data point falling into\nprobability of a data point falling into\nprobability of a data point falling into a certain class given the value of the\na certain class given the value of the\na certain class given the value of the input variable so for example the\ninput variable so for example the\ninput variable so for example the likelihood of an adult person with a\nlikelihood of an adult person with a\nlikelihood of an adult person with a height of 180 cm being a man would be\nheight of 180 cm being a man would be\nheight of 180 cm being a man would be 80% this is completely made up of course\n80% this is completely made up of course\n80% this is completely made up of course the K nearest neighbors algorithm or KNN\nthe K nearest neighbors algorithm or KNN\nthe K nearest neighbors algorithm or KNN is a very simple and intuitive algorithm\nis a very simple and intuitive algorithm\nis a very simple and intuitive algorithm that can be used for both regression and\nthat can be used for both regression and\nthat can be used for both regression and class ification it is a so-called\nclass ification it is a so-called\nclass ification it is a so-called non-parametric algorithm the name means\nnon-parametric algorithm the name means\nnon-parametric algorithm the name means that we don't try to fit any equations\nthat we don't try to fit any equations\nthat we don't try to fit any equations and thus find any parameters of a model\nand thus find any parameters of a model\nand thus find any parameters of a model so no true model fitting is necessary\nso no true model fitting is necessary\nso no true model fitting is necessary the idea of KNN is simply that for any\nthe idea of KNN is simply that for any\nthe idea of KNN is simply that for any given new data point we will predict the\ngiven new data point we will predict the\ngiven new data point we will predict the target to be the average of its K\ntarget to be the average of its K\ntarget to be the average of its K nearest neighbors while this might seem\nnearest neighbors while this might seem\nnearest neighbors while this might seem very simple this is actually a very\nvery simple this is actually a very\nvery simple this is actually a very powerful predictive algorithm especially\npowerful predictive algorithm especially\npowerful predictive algorithm especially when relationships are more complicated\nwhen relationships are more complicated\nwhen relationships are more complicated than a simple linear relationship in a\nthan a simple linear relationship in a\nthan a simple linear relationship in a classification example we might say that\nclassification example we might say that\nclassification example we might say that the gender of a person will be the same\nthe gender of a person will be the same\nthe gender of a person will be the same as the majority of the five people\nas the majority of the five people\nas the majority of the five people closest in weight and height to the\nclosest in weight and height to the\nclosest in weight and height to the person in question in a regression\nperson in question in a regression\nperson in question in a regression example we might say that the weight of\nexample we might say that the weight of\nexample we might say that the weight of a person is the average weight of the\na person is the average weight of the\na person is the average weight of the three people closest in height and of\nthree people closest in height and of\nthree people closest in height and of chest circumference this makes a ton of\nchest circumference this makes a ton of\nchest circumference this makes a ton of intuitive sense you might realize that\nintuitive sense you might realize that\nintuitive sense you might realize that the number three seems a bit arbitrary\nthe number three seems a bit arbitrary\nthe number three seems a bit arbitrary and it is K is called a hyperparameter\nand it is K is called a hyperparameter\nand it is K is called a hyperparameter of the algorithm and choosing the right\nof the algorithm and choosing the right\nof the algorithm and choosing the right K is an art choosing a very small number\nK is an art choosing a very small number\nK is an art choosing a very small number of K say one or two will lead to your\nof K say one or two will lead to your\nof K say one or two will lead to your model predicting your training data set\nmodel predicting your training data set\nmodel predicting your training data set very well but not generalizing well to\nvery well but not generalizing well to\nvery well but not generalizing well to unseen data this is called overfitting\nunseen data this is called overfitting\nunseen data this is called overfitting choosing a very large number say 1,000\nchoosing a very large number say 1,000\nchoosing a very large number say 1,000 will lead to a worst fit over overall\nwill lead to a worst fit over overall\nwill lead to a worst fit over overall this is called underfitting the best\nthis is called underfitting the best\nthis is called underfitting the best number is somewhere in between and\nnumber is somewhere in between and\nnumber is somewhere in between and Depends a lot on the problem at hand\nDepends a lot on the problem at hand\nDepends a lot on the problem at hand methods for finding the right\nmethods for finding the right\nmethods for finding the right hyperparameters include cross validation\nhyperparameters include cross validation\nhyperparameters include cross validation but are beyond the scope of this video\nbut are beyond the scope of this video\nbut are beyond the scope of this video support Vector machine is a supervised\nsupport Vector machine is a supervised\nsupport Vector machine is a supervised machine learning algorithm originally\nmachine learning algorithm originally\nmachine learning algorithm originally designed for classification tasks but it\ndesigned for classification tasks but it\ndesigned for classification tasks but it can also be used for regression tasks\ncan also be used for regression tasks\ncan also be used for regression tasks the core concept of the algorithm is to\nthe core concept of the algorithm is to\nthe core concept of the algorithm is to draw a decision boundary between data\ndraw a decision boundary between data\ndraw a decision boundary between data points that separates data points of the\npoints that separates data points of the\npoints that separates data points of the training set as well as possible as the\ntraining set as well as possible as the\ntraining set as well as possible as the name suggests a new unseen data point\nname suggests a new unseen data point\nname suggests a new unseen data point will be classified according to where it\nwill be classified according to where it\nwill be classified according to where it falls with respect to the decision\nfalls with respect to the decision\nfalls with respect to the decision boundary let's take this arbitrary\nboundary let's take this arbitrary\nboundary let's take this arbitrary example of trying to classify animals by\nexample of trying to classify animals by\nexample of trying to classify animals by their weight and the length of their\ntheir weight and the length of their\ntheir weight and the length of their nose in this simple case of trying to\nnose in this simple case of trying to\nnose in this simple case of trying to classify cats and elephants the decision\nclassify cats and elephants the decision\nclassify cats and elephants the decision boundary is a straight line the svm\nboundary is a straight line the svm\nboundary is a straight line the svm algorithm tries to find the line that\nalgorithm tries to find the line that\nalgorithm tries to find the line that separates the classes with the largest\nseparates the classes with the largest\nseparates the classes with the largest margin possible that is maximizing the\nmargin possible that is maximizing the\nmargin possible that is maximizing the space between the different classes this\nspace between the different classes this\nspace between the different classes this makes the decision boundary generalize\nmakes the decision boundary generalize\nmakes the decision boundary generalize well and less sensitive to noise and\nwell and less sensitive to noise and\nwell and less sensitive to noise and outliers in the training data the\noutliers in the training data the\noutliers in the training data the so-called support vectors are the data\nso-called support vectors are the data\nso-called support vectors are the data points that sit on the edge of the\npoints that sit on the edge of the\npoints that sit on the edge of the margin knowing the support vectors is\nmargin knowing the support vectors is\nmargin knowing the support vectors is enough to classify new data points which\nenough to classify new data points which\nenough to classify new data points which often makes the algorithm very memory\noften makes the algorithm very memory\noften makes the algorithm very memory efficient one of the benefits of SPM is\nefficient one of the benefits of SPM is\nefficient one of the benefits of SPM is that it is very powerful in high\nthat it is very powerful in high\nthat it is very powerful in high Dimensions that is if the number of\nDimensions that is if the number of\nDimensions that is if the number of features is large compared to the size\nfeatures is large compared to the size\nfeatures is large compared to the size of the data in those higher dimensional\nof the data in those higher dimensional\nof the data in those higher dimensional cases the decision boundary is called a\ncases the decision boundary is called a\ncases the decision boundary is called a hyperplane another feature that makes\nhyperplane another feature that makes\nhyperplane another feature that makes svms extremely powerful is the use of\nsvms extremely powerful is the use of\nsvms extremely powerful is the use of so-called kernel functions which allow\nso-called kernel functions which allow\nso-called kernel functions which allow for the identification of Highly complex\nfor the identification of Highly complex\nfor the identification of Highly complex nonlinear decision boundaries kernel\nnonlinear decision boundaries kernel\nnonlinear decision boundaries kernel functions are an implicit way to turn\nfunctions are an implicit way to turn\nfunctions are an implicit way to turn your original features into new more\nyour original features into new more\nyour original features into new more complex features using the so-called\ncomplex features using the so-called\ncomplex features using the so-called kernel trick which is beyond the scope\nkernel trick which is beyond the scope\nkernel trick which is beyond the scope of this video this allows for efficient\nof this video this allows for efficient\nof this video this allows for efficient creation of nonlinear decision\ncreation of nonlinear decision\ncreation of nonlinear decision boundaries by creating complex new\nboundaries by creating complex new\nboundaries by creating complex new features such as weight divided by\nfeatures such as weight divided by\nfeatures such as weight divided by height squared also called the BMI this\nheight squared also called the BMI this\nheight squared also called the BMI this is called implicit feature engineering\nis called implicit feature engineering\nis called implicit feature engineering neural networks take the idea of\nneural networks take the idea of\nneural networks take the idea of implicit feature engineering to the next\nimplicit feature engineering to the next\nimplicit feature engineering to the next level as I will explain later possible\nlevel as I will explain later possible\nlevel as I will explain later possible kernel functions for svms are the linear\nkernel functions for svms are the linear\nkernel functions for svms are the linear the polinomial the RBF and the sigmoid\nthe polinomial the RBF and the sigmoid\nthe polinomial the RBF and the sigmoid kernel another fairly simple classifier\nkernel another fairly simple classifier\nkernel another fairly simple classifier is the naive Bas classifier that gets\nis the naive Bas classifier that gets\nis the naive Bas classifier that gets its name from B theorem which looks like\nits name from B theorem which looks like\nits name from B theorem which looks like this I believe it's easiest to\nthis I believe it's easiest to\nthis I believe it's easiest to understand naive Bay with an example use\nunderstand naive Bay with an example use\nunderstand naive Bay with an example use case that it is often used for spam\ncase that it is often used for spam\ncase that it is often used for spam filters we can train our algorithm with\nfilters we can train our algorithm with\nfilters we can train our algorithm with a number of spam and non-spam emails and\na number of spam and non-spam emails and\na number of spam and non-spam emails and count the occurrences of different words\ncount the occurrences of different words\ncount the occurrences of different words in each class and thereby calculate the\nin each class and thereby calculate the\nin each class and thereby calculate the probability of certain words appearing\nprobability of certain words appearing\nprobability of certain words appearing in spam emails and non-spam emails we\nin spam emails and non-spam emails we\nin spam emails and non-spam emails we can then quickly classify a new email\ncan then quickly classify a new email\ncan then quickly classify a new email based on the words it contains by by\nbased on the words it contains by by\nbased on the words it contains by by using base theorem we simply multiply\nusing base theorem we simply multiply\nusing base theorem we simply multiply the different probabilities of all words\nthe different probabilities of all words\nthe different probabilities of all words in the email together this algorithm\nin the email together this algorithm\nin the email together this algorithm makes the false assumption that the\nmakes the false assumption that the\nmakes the false assumption that the probabilities of the different words\nprobabilities of the different words\nprobabilities of the different words appearing are independent of each other\nappearing are independent of each other\nappearing are independent of each other which is why we call this classifier\nwhich is why we call this classifier\nwhich is why we call this classifier naive this makes it very computation\nnaive this makes it very computation\nnaive this makes it very computation Ally efficient while still being a good\nAlly efficient while still being a good\nAlly efficient while still being a good approximation for many use cases such as\napproximation for many use cases such as\napproximation for many use cases such as spam classification and other text-based\nspam classification and other text-based\nspam classification and other text-based classification tasks decision trees are\nclassification tasks decision trees are\nclassification tasks decision trees are the basis of a number of more complex\nthe basis of a number of more complex\nthe basis of a number of more complex supervised learning algorithms in its\nsupervised learning algorithms in its\nsupervised learning algorithms in its simplest form a decision tree looks\nsimplest form a decision tree looks\nsimplest form a decision tree looks somewhat like this the decision tree is\nsomewhat like this the decision tree is\nsomewhat like this the decision tree is basically a series of yes no questions\nbasically a series of yes no questions\nbasically a series of yes no questions that allow us to partition a data set in\nthat allow us to partition a data set in\nthat allow us to partition a data set in several Dimensions here is an example\nseveral Dimensions here is an example\nseveral Dimensions here is an example decision tree for classifying people\ndecision tree for classifying people\ndecision tree for classifying people into high and lowrisk patients for heart\ninto high and lowrisk patients for heart\ninto high and lowrisk patients for heart attacks the goal of the decision tree\nattacks the goal of the decision tree\nattacks the goal of the decision tree algorithm is to create so-called Leaf\nalgorithm is to create so-called Leaf\nalgorithm is to create so-called Leaf nodes at the bottom of the tree that are\nnodes at the bottom of the tree that are\nnodes at the bottom of the tree that are as pure as possible meaning instead of\nas pure as possible meaning instead of\nas pure as possible meaning instead of randomly splitting the data we try to\nrandomly splitting the data we try to\nrandomly splitting the data we try to find splits that lead to the resulting\nfind splits that lead to the resulting\nfind splits that lead to the resulting groups or leaves to be as pure as\ngroups or leaves to be as pure as\ngroups or leaves to be as pure as possible which is to say that as few\npossible which is to say that as few\npossible which is to say that as few data points as possible are\ndata points as possible are\ndata points as possible are misclassified while this might seem like\nmisclassified while this might seem like\nmisclassified while this might seem like a very basic and simple algorithm which\na very basic and simple algorithm which\na very basic and simple algorithm which it is we can turn it into a very\nit is we can turn it into a very\nit is we can turn it into a very powerful algorithm by combining many\npowerful algorithm by combining many\npowerful algorithm by combining many decision trees together combining many\ndecision trees together combining many\ndecision trees together combining many simple models to a more powerful complex\nsimple models to a more powerful complex\nsimple models to a more powerful complex model is called an ensemble algorithm\nmodel is called an ensemble algorithm\nmodel is called an ensemble algorithm one form of ensembling is bagging where\none form of ensembling is bagging where\none form of ensembling is bagging where we train multiple models on different\nwe train multiple models on different\nwe train multiple models on different subsets of the training data using a\nsubsets of the training data using a\nsubsets of the training data using a method called bootstrap\nmethod called bootstrap\nmethod called bootstrap a famous version of this idea is called\na famous version of this idea is called\na famous version of this idea is called a random Forest where many decision\na random Forest where many decision\na random Forest where many decision trees vote on the classification of your\ntrees vote on the classification of your\ntrees vote on the classification of your data by majority vote of the different\ndata by majority vote of the different\ndata by majority vote of the different trees in the random Forest random\ntrees in the random Forest random\ntrees in the random Forest random forests are very powerful estimators\nforests are very powerful estimators\nforests are very powerful estimators that can be used both for classification\nthat can be used both for classification\nthat can be used both for classification and regression the randomness comes from\nand regression the randomness comes from\nand regression the randomness comes from randomly excluding features for\nrandomly excluding features for\nrandomly excluding features for different trees in the forest which\ndifferent trees in the forest which\ndifferent trees in the forest which prevents overfitting and makes it much\nprevents overfitting and makes it much\nprevents overfitting and makes it much more robust because it removes\nmore robust because it removes\nmore robust because it removes correlation between the trees another\ncorrelation between the trees another\ncorrelation between the trees another type of Ensemble method is called\ntype of Ensemble method is called\ntype of Ensemble method is called boosting where instead of running many\nboosting where instead of running many\nboosting where instead of running many decision trees in parallel like for\ndecision trees in parallel like for\ndecision trees in parallel like for random forests we train models in\nrandom forests we train models in\nrandom forests we train models in sequence where each model focuses on\nsequence where each model focuses on\nsequence where each model focuses on fixing the errors made by the previous\nfixing the errors made by the previous\nfixing the errors made by the previous model we combine a series of weak models\nmodel we combine a series of weak models\nmodel we combine a series of weak models in sequence thus becoming a strong model\nin sequence thus becoming a strong model\nin sequence thus becoming a strong model because each sequential model tries to\nbecause each sequential model tries to\nbecause each sequential model tries to fix the errors of the previous model\nfix the errors of the previous model\nfix the errors of the previous model boosted trees often get to higher\nboosted trees often get to higher\nboosted trees often get to higher accuracies than random forests but are\naccuracies than random forests but are\naccuracies than random forests but are also more prone to overfitting its\nalso more prone to overfitting its\nalso more prone to overfitting its sequential nature makes it slower to\nsequential nature makes it slower to\nsequential nature makes it slower to train than random forests famous\ntrain than random forests famous\ntrain than random forests famous examples of boosted trees are Ada boost\nexamples of boosted trees are Ada boost\nexamples of boosted trees are Ada boost gradient boosting and XG boost the\ngradient boosting and XG boost the\ngradient boosting and XG boost the details of which are beyond the scope of\ndetails of which are beyond the scope of\ndetails of which are beyond the scope of this video now let's get to the reigning\nthis video now let's get to the reigning\nthis video now let's get to the reigning king of AI neural networks to to\nking of AI neural networks to to\nking of AI neural networks to to understand neural networks let's look at\nunderstand neural networks let's look at\nunderstand neural networks let's look at logistic regression again say we have a\nlogistic regression again say we have a\nlogistic regression again say we have a number of features and are trying to\nnumber of features and are trying to\nnumber of features and are trying to predict a target class the features\npredict a target class the features\npredict a target class the features might be pixel intensities of a digital\nmight be pixel intensities of a digital\nmight be pixel intensities of a digital image and the target might be\nimage and the target might be\nimage and the target might be classifying the image as one of the\nclassifying the image as one of the\nclassifying the image as one of the digits from 0 to 9 now for this\ndigits from 0 to 9 now for this\ndigits from 0 to 9 now for this particular case you might see why this\nparticular case you might see why this\nparticular case you might see why this might be difficult to do with logistic\nmight be difficult to do with logistic\nmight be difficult to do with logistic regression because say the number one\nregression because say the number one\nregression because say the number one doesn't look the same when different\ndoesn't look the same when different\ndoesn't look the same when different people write it and even if the same\npeople write it and even if the same\npeople write it and even if the same person writes it several times it will\nperson writes it several times it will\nperson writes it several times it will look slightly different each time and it\nlook slightly different each time and it\nlook slightly different each time and it won't be the exact same pixels\nwon't be the exact same pixels\nwon't be the exact same pixels illuminated for every instance of the\nilluminated for every instance of the\nilluminated for every instance of the number one all of the instances of the\nnumber one all of the instances of the\nnumber one all of the instances of the number one have commonality however like\nnumber one have commonality however like\nnumber one have commonality however like they all have a dominating vertical line\nthey all have a dominating vertical line\nthey all have a dominating vertical line and usually no Crossing Lines as other\nand usually no Crossing Lines as other\nand usually no Crossing Lines as other digits might have and usually there are\ndigits might have and usually there are\ndigits might have and usually there are no circular shapes in the number one as\nno circular shapes in the number one as\nno circular shapes in the number one as there would be in the number eight or or\nthere would be in the number eight or or\nthere would be in the number eight or or nine however the computer doesn't\nnine however the computer doesn't\nnine however the computer doesn't initially know about these more complex\ninitially know about these more complex\ninitially know about these more complex features but only the pixel intensities\nfeatures but only the pixel intensities\nfeatures but only the pixel intensities we could manually engineer these\nwe could manually engineer these\nwe could manually engineer these features by measuring some of these\nfeatures by measuring some of these\nfeatures by measuring some of these things and explicitly adding them as new\nthings and explicitly adding them as new\nthings and explicitly adding them as new features but artificial neural networks\nfeatures but artificial neural networks\nfeatures but artificial neural networks similarly to using a kernel function\nsimilarly to using a kernel function\nsimilarly to using a kernel function with a support Vector machine are\nwith a support Vector machine are\nwith a support Vector machine are designed to implicitly and automatically\ndesigned to implicitly and automatically\ndesigned to implicitly and automatically design these features for us without any\ndesign these features for us without any\ndesign these features for us without any guidance from humans we do this by\nguidance from humans we do this by\nguidance from humans we do this by adding additional layers of unknown\nadding additional layers of unknown\nadding additional layers of unknown variables between the input and output\nvariables between the input and output\nvariables between the input and output variables in its simplest form this is\nvariables in its simplest form this is\nvariables in its simplest form this is called a single layer percep chop which\ncalled a single layer percep chop which\ncalled a single layer percep chop which is basically just a multi-feature\nis basically just a multi-feature\nis basically just a multi-feature regression task now if we add a hidden\nregression task now if we add a hidden\nregression task now if we add a hidden layer the hidden variables in the middle\nlayer the hidden variables in the middle\nlayer the hidden variables in the middle layer represent some hidden unknown\nlayer represent some hidden unknown\nlayer represent some hidden unknown features and instead of predicting the\nfeatures and instead of predicting the\nfeatures and instead of predicting the target variable directly we try to\ntarget variable directly we try to\ntarget variable directly we try to predict these hidden features with our\npredict these hidden features with our\npredict these hidden features with our input features and then try to predict\ninput features and then try to predict\ninput features and then try to predict the target variables with our new hidden\nthe target variables with our new hidden\nthe target variables with our new hidden features in our specific example we\nfeatures in our specific example we\nfeatures in our specific example we might be able to say that every time\nmight be able to say that every time\nmight be able to say that every time several pixels are illuminated next to\nseveral pixels are illuminated next to\nseveral pixels are illuminated next to each other they represent a horizontal\neach other they represent a horizontal\neach other they represent a horizontal line which can be a new feature to try\nline which can be a new feature to try\nline which can be a new feature to try and predict the digit in question even\nand predict the digit in question even\nand predict the digit in question even though we never explicitly defined a\nthough we never explicitly defined a\nthough we never explicitly defined a feature called horizontal line This is a\nfeature called horizontal line This is a\nfeature called horizontal line This is a much simplified view of what is actually\nmuch simplified view of what is actually\nmuch simplified view of what is actually going on but hopefully this gets the\ngoing on but hopefully this gets the\ngoing on but hopefully this gets the point across we don't usually know what\npoint across we don't usually know what\npoint across we don't usually know what the hidden features represent we just\nthe hidden features represent we just\nthe hidden features represent we just train the neural network to predict the\ntrain the neural network to predict the\ntrain the neural network to predict the final Target as well as possible the\nfinal Target as well as possible the\nfinal Target as well as possible the hidden features we can Design This Way\nhidden features we can Design This Way\nhidden features we can Design This Way are limited in the case of the single\nare limited in the case of the single\nare limited in the case of the single hidden layer but what if we add a layer\nhidden layer but what if we add a layer\nhidden layer but what if we add a layer and have the hidden layer predict\nand have the hidden layer predict\nand have the hidden layer predict another hidden layer what if we now had\nanother hidden layer what if we now had\nanother hidden layer what if we now had even more layers this is called Deep\neven more layers this is called Deep\neven more layers this is called Deep learning and can result in very complex\nlearning and can result in very complex\nlearning and can result in very complex hidden features so that might represent\nhidden features so that might represent\nhidden features so that might represent all kinds of complex information in the\nall kinds of complex information in the\nall kinds of complex information in the pictures like the fact that there is a\npictures like the fact that there is a\npictures like the fact that there is a face in the picture however we will\nface in the picture however we will\nface in the picture however we will usually not know what the hidden\nusually not know what the hidden\nusually not know what the hidden features mean we just know that they\nfeatures mean we just know that they\nfeatures mean we just know that they result in good predictions all we have\nresult in good predictions all we have\nresult in good predictions all we have talked about so far is supervised\ntalked about so far is supervised\ntalked about so far is supervised learning where we wanted to predict a\nlearning where we wanted to predict a\nlearning where we wanted to predict a specific Target variable using some\nspecific Target variable using some\nspecific Target variable using some input variables however sometimes we\ninput variables however sometimes we\ninput variables however sometimes we don't have anything specific to predict\ndon't have anything specific to predict\ndon't have anything specific to predict and just want to find some underlying\nand just want to find some underlying\nand just want to find some underlying structure in our data that's where\nstructure in our data that's where\nstructure in our data that's where unsupervised learning comes in a very\nunsupervised learning comes in a very\nunsupervised learning comes in a very common unsupervised problem is\ncommon unsupervised problem is\ncommon unsupervised problem is clustering it's easy to confuse\nclustering it's easy to confuse\nclustering it's easy to confuse clustering with classification but they\nclustering with classification but they\nclustering with classification but they are conceptually very different\nare conceptually very different\nare conceptually very different classification is when we know the\nclassification is when we know the\nclassification is when we know the classes we want to predict and have\nclasses we want to predict and have\nclasses we want to predict and have training data with true labels available\ntraining data with true labels available\ntraining data with true labels available shown as colors here like pictures of\nshown as colors here like pictures of\nshown as colors here like pictures of cats and dogs clustering is when we\ncats and dogs clustering is when we\ncats and dogs clustering is when we don't have any labels and want to find\ndon't have any labels and want to find\ndon't have any labels and want to find unknown clusters just by looking at the\nunknown clusters just by looking at the\nunknown clusters just by looking at the overall structure of the data and trying\noverall structure of the data and trying\noverall structure of the data and trying to find potential clusters in the data\nto find potential clusters in the data\nto find potential clusters in the data for example we might look at a\nfor example we might look at a\nfor example we might look at a two-dimensional data set that looks like\ntwo-dimensional data set that looks like\ntwo-dimensional data set that looks like this any human will probably easily see\nthis any human will probably easily see\nthis any human will probably easily see three clusters here but it's not always\nthree clusters here but it's not always\nthree clusters here but it's not always as straightforward as your data might\nas straightforward as your data might\nas straightforward as your data might might also look like this we don't know\nmight also look like this we don't know\nmight also look like this we don't know how many clusters there are because the\nhow many clusters there are because the\nhow many clusters there are because the problem is unsupervised the most famous\nproblem is unsupervised the most famous\nproblem is unsupervised the most famous clustering algorithm is called K means\nclustering algorithm is called K means\nclustering algorithm is called K means clustering just like for KNN K is a\nclustering just like for KNN K is a\nclustering just like for KNN K is a hyperparameter and stands for the number\nhyperparameter and stands for the number\nhyperparameter and stands for the number of clusters you are looking for finding\nof clusters you are looking for finding\nof clusters you are looking for finding the right number of clusters again is an\nthe right number of clusters again is an\nthe right number of clusters again is an art and has a lot to do with your\nart and has a lot to do with your\nart and has a lot to do with your specific problem and some trial and\nspecific problem and some trial and\nspecific problem and some trial and error in domain knowledge might be\nerror in domain knowledge might be\nerror in domain knowledge might be required this is beyond the scope of\nrequired this is beyond the scope of\nrequired this is beyond the scope of this video K means is very simple you\nthis video K means is very simple you\nthis video K means is very simple you start by randomly selecting centers for\nstart by randomly selecting centers for\nstart by randomly selecting centers for your K clusters and assigning all data\nyour K clusters and assigning all data\nyour K clusters and assigning all data points to the cluster center closest to\npoints to the cluster center closest to\npoints to the cluster center closest to them the Clusters here are shown in blue\nthem the Clusters here are shown in blue\nthem the Clusters here are shown in blue and green you then recalculate the\nand green you then recalculate the\nand green you then recalculate the cluster centers based on the data points\ncluster centers based on the data points\ncluster centers based on the data points now assigned to them you can see the\nnow assigned to them you can see the\nnow assigned to them you can see the centers moving closer to the actual\ncenters moving closer to the actual\ncenters moving closer to the actual clusters you then assign the data points\nclusters you then assign the data points\nclusters you then assign the data points again to the new cluster centers\nagain to the new cluster centers\nagain to the new cluster centers followed by recalculating the cluster\nfollowed by recalculating the cluster\nfollowed by recalculating the cluster centers you repeat this process until\ncenters you repeat this process until\ncenters you repeat this process until the centers of the Clusters have\nthe centers of the Clusters have\nthe centers of the Clusters have stabilized while K means is the most\nstabilized while K means is the most\nstabilized while K means is the most famous and most common clustering\nfamous and most common clustering\nfamous and most common clustering algorithm other algorithms exist\nalgorithm other algorithms exist\nalgorithm other algorithms exist including some where you don't need to\nincluding some where you don't need to\nincluding some where you don't need to specify the number of clusters like\nspecify the number of clusters like\nspecify the number of clusters like hierarchical clustering and DB scan\nhierarchical clustering and DB scan\nhierarchical clustering and DB scan which can find clusters of arbitrary\nwhich can find clusters of arbitrary\nwhich can find clusters of arbitrary shape but I won't discuss them here the\nshape but I won't discuss them here the\nshape but I won't discuss them here the last type of algorithm I will leave you\nlast type of algorithm I will leave you\nlast type of algorithm I will leave you with is dimensionality reduction the\nwith is dimensionality reduction the\nwith is dimensionality reduction the idea of dimensionality reduction is to\nidea of dimensionality reduction is to\nidea of dimensionality reduction is to reduce the number of features or\nreduce the number of features or\nreduce the number of features or dimensions of your data set keeping as\ndimensions of your data set keeping as\ndimensions of your data set keeping as much information as possible usually\nmuch information as possible usually\nmuch information as possible usually this group of algorithms does this by\nthis group of algorithms does this by\nthis group of algorithms does this by finding correlations between existing\nfinding correlations between existing\nfinding correlations between existing features and removing potentially\nfeatures and removing potentially\nfeatures and removing potentially redundant Dimensions without losing much\nredundant Dimensions without losing much\nredundant Dimensions without losing much information for example do you really\ninformation for example do you really\ninformation for example do you really need a picture in high resolution to\nneed a picture in high resolution to\nneed a picture in high resolution to recognize the airplane in the picture or\nrecognize the airplane in the picture or\nrecognize the airplane in the picture or can you reduce the number of pixels in\ncan you reduce the number of pixels in\ncan you reduce the number of pixels in the image as such dimensionality\nthe image as such dimensionality\nthe image as such dimensionality reduction will give you information\nreduction will give you information\nreduction will give you information about the relationships within your\nabout the relationships within your\nabout the relationships within your existing features and it can also be\nexisting features and it can also be\nexisting features and it can also be used as a pre-processing step in your\nused as a pre-processing step in your\nused as a pre-processing step in your supervised learning algorithm to reduce\nsupervised learning algorithm to reduce\nsupervised learning algorithm to reduce the number of features in your data set\nthe number of features in your data set\nthe number of features in your data set and make the algorithm more efficient\nand make the algorithm more efficient\nand make the algorithm more efficient and robust an example algorithm is\nand robust an example algorithm is\nand robust an example algorithm is principal component analysis or PCA\nprincipal component analysis or PCA\nprincipal component analysis or PCA let's say we are trying to predict types\nlet's say we are trying to predict types\nlet's say we are trying to predict types of fish based on several features like\nof fish based on several features like\nof fish based on several features like length height color and number of teeth\nlength height color and number of teeth\nlength height color and number of teeth when looking at the correlations of the\nwhen looking at the correlations of the\nwhen looking at the correlations of the different features we might find that\ndifferent features we might find that\ndifferent features we might find that height and length are strongly\nheight and length are strongly\nheight and length are strongly correlated and including both both won't\ncorrelated and including both both won't\ncorrelated and including both both won't help the algorithm much and might in\nhelp the algorithm much and might in\nhelp the algorithm much and might in fact hurt it by introducing noise we can\nfact hurt it by introducing noise we can\nfact hurt it by introducing noise we can simply include a shape feature that is a\nsimply include a shape feature that is a\nsimply include a shape feature that is a combination of the two this is actually\ncombination of the two this is actually\ncombination of the two this is actually extremely common in large data sets and\nextremely common in large data sets and\nextremely common in large data sets and allows us to reduce the number of\nallows us to reduce the number of\nallows us to reduce the number of features dramatically and still get good\nfeatures dramatically and still get good\nfeatures dramatically and still get good results PCA does this by finding the\nresults PCA does this by finding the\nresults PCA does this by finding the directions in which most variance in the\ndirections in which most variance in the\ndirections in which most variance in the data set is retained in this example the\ndata set is retained in this example the\ndata set is retained in this example the direction of most variant is a diagonal\ndirection of most variant is a diagonal\ndirection of most variant is a diagonal this is called the first principal\nthis is called the first principal\nthis is called the first principal component or PC and can become our new\ncomponent or PC and can become our new\ncomponent or PC and can become our new shape feature the second principal\nshape feature the second principal\nshape feature the second principal component is orthogonal to the first and\ncomponent is orthogonal to the first and\ncomponent is orthogonal to the first and only explains a small fr C of the\nonly explains a small fr C of the\nonly explains a small fr C of the variant of the data set and can thus be\nvariant of the data set and can thus be\nvariant of the data set and can thus be excluded from our data set in this case\nexcluded from our data set in this case\nexcluded from our data set in this case in large data sets we can do this for\nin large data sets we can do this for\nin large data sets we can do this for all features and rank them by explain\nall features and rank them by explain\nall features and rank them by explain variants and exclude any principal\nvariants and exclude any principal\nvariants and exclude any principal components that don't contribute much to\ncomponents that don't contribute much to\ncomponents that don't contribute much to the variant and thus wouldn't help much\nthe variant and thus wouldn't help much\nthe variant and thus wouldn't help much in our ml model this was all common\nin our ml model this was all common\nin our ml model this was all common machine learning algorithms explained if\nmachine learning algorithms explained if\nmachine learning algorithms explained if you are overwhelmed and don't know which\nyou are overwhelmed and don't know which\nyou are overwhelmed and don't know which algorithm you need here is a great cheat\nalgorithm you need here is a great cheat\nalgorithm you need here is a great cheat sheet by syit learn that will help you\nsheet by syit learn that will help you\nsheet by syit learn that will help you decide which algorithm is right for\ndecide which algorithm is right for\ndecide which algorithm is right for which type of problem if you want a road\nwhich type of problem if you want a road\nwhich type of problem if you want a road map on how to learn machine learning\nmap on how to learn machine learning\nmap on how to learn machine learning check out my video on that"
  },
  {
    "id": 258990560,
    "timestamp": "2026-02-05T01:42:48.675Z",
    "title": "Essential Machine Learning and AI Concepts Animated",
    "url": "https://www.youtube.com/watch?v=PcbuKRNtCUc",
    "text": "Learn about all the most important terms\nLearn about all the most important terms and concepts related to machine learning\nand concepts related to machine learning\nand concepts related to machine learning and AI. In this course, Vladimir from\nand AI. In this course, Vladimir from\nand AI. In this course, Vladimir from Touring Time Machine will give quick and\nTouring Time Machine will give quick and\nTouring Time Machine will give quick and simple explanations with animations for\nsimple explanations with animations for\nsimple explanations with animations for all the machine learning concepts you\nall the machine learning concepts you\nall the machine learning concepts you need to know. So, you love AI, data\nneed to know. So, you love AI, data\nneed to know. So, you love AI, data science, and machine learning, but find\nscience, and machine learning, but find\nscience, and machine learning, but find it all a bit overwhelming. Wish someone\nit all a bit overwhelming. Wish someone\nit all a bit overwhelming. Wish someone could break down the big ideas into\ncould break down the big ideas into\ncould break down the big ideas into simple bite-sized explanations with\nsimple bite-sized explanations with\nsimple bite-sized explanations with visuals that actually make sense? you're\nvisuals that actually make sense? you're\nvisuals that actually make sense? you're in the right place. This video is your\nin the right place. This video is your\nin the right place. This video is your quick guide to the must know concepts in\nquick guide to the must know concepts in\nquick guide to the must know concepts in this field. We'll keep it clear. Skip\nthis field. We'll keep it clear. Skip\nthis field. We'll keep it clear. Skip the confusing jargon and use\nthe confusing jargon and use\nthe confusing jargon and use eye-catching graphics to turn complex\neye-catching graphics to turn complex\neye-catching graphics to turn complex ideas into aha moments. No fancy terms,\nideas into aha moments. No fancy terms,\nideas into aha moments. No fancy terms, no headaches, just the cool stuff made\nno headaches, just the cool stuff made\nno headaches, just the cool stuff made easy. Ready? Let's jump in. Variance is\neasy. Ready? Let's jump in. Variance is\neasy. Ready? Let's jump in. Variance is a statistical measure of the dispersion\na statistical measure of the dispersion\na statistical measure of the dispersion of a set of values often used in machine\nof a set of values often used in machine\nof a set of values often used in machine learning to understand data\nlearning to understand data\nlearning to understand data distribution.\ndistribution.\ndistribution. Unsupervised learning is a type of\nUnsupervised learning is a type of\nUnsupervised learning is a type of machine learning where the algorithm\nmachine learning where the algorithm\nmachine learning where the algorithm learns from unlabelled data often used\nlearns from unlabelled data often used\nlearns from unlabelled data often used for clustering or anomaly\ndetection. Time series analysis is the\ndetection. Time series analysis is the study of ordered often temporal data\nstudy of ordered often temporal data\nstudy of ordered often temporal data used for forecasting and trend analysis.\nused for forecasting and trend analysis.\nused for forecasting and trend analysis. [Music]\n[Music]\n[Music] Transfer learning is the practice of\nTransfer learning is the practice of\nTransfer learning is the practice of applying knowledge gained from one task\napplying knowledge gained from one task\napplying knowledge gained from one task to a different but related task often\nto a different but related task often\nto a different but related task often used to improve model performance.\nused to improve model performance.\nused to improve model performance. [Music]\n[Music]\n[Music] Gradient descent is an optimization\nGradient descent is an optimization\nGradient descent is an optimization algorithm used to find the local minimum\nalgorithm used to find the local minimum\nalgorithm used to find the local minimum of a function by iteratively adjusting\nof a function by iteratively adjusting\nof a function by iteratively adjusting its parameters in the opposite direction\nits parameters in the opposite direction\nits parameters in the opposite direction of the gradient.\nof the gradient.\nof the gradient. [Music]\nStochastic gradient descent is a variant\nStochastic gradient descent is a variant of gradient descent that updates the\nof gradient descent that updates the\nof gradient descent that updates the model's parameters using only a single\nmodel's parameters using only a single\nmodel's parameters using only a single data point at each\niteration. Sentiment analysis is the use\niteration. Sentiment analysis is the use of natural language processing to\nof natural language processing to\nof natural language processing to identify and categorize opinions\nidentify and categorize opinions\nidentify and categorize opinions expressed in text.\nexpressed in text.\nexpressed in text. [Music]\n[Music]\n[Music] Regression is a statistical method used\nRegression is a statistical method used\nRegression is a statistical method used to model the relationship between a\nto model the relationship between a\nto model the relationship between a dependent variable and one or more\ndependent variable and one or more\ndependent variable and one or more independent\nindependent\nindependent [Music]\n[Music]\n[Music] variables. Regularization is a technique\nvariables. Regularization is a technique\nvariables. Regularization is a technique used to prevent overfitting by adding a\nused to prevent overfitting by adding a\nused to prevent overfitting by adding a penalty term to the loss function.\n[Music]\n[Music] Logistic regression is a statistical\nLogistic regression is a statistical\nLogistic regression is a statistical method used for binary classification\nmethod used for binary classification\nmethod used for binary classification problems modeling the probability of a\nproblems modeling the probability of a\nproblems modeling the probability of a particular class.\nLinear regression is a statistical\nLinear regression is a statistical method used for modeling and analyzing\nmethod used for modeling and analyzing\nmethod used for modeling and analyzing linear relationships between a dependent\nlinear relationships between a dependent\nlinear relationships between a dependent variable and one or more independent\nvariable and one or more independent\nvariable and one or more independent variables. Reinforcement learning is a\nvariables. Reinforcement learning is a\nvariables. Reinforcement learning is a type of machine learning where an agent\ntype of machine learning where an agent\ntype of machine learning where an agent learns to make decisions by interacting\nlearns to make decisions by interacting\nlearns to make decisions by interacting with an environment to achieve a goal.\nDecision trees are a type of supervised\nDecision trees are a type of supervised learning algorithm used for both\nlearning algorithm used for both\nlearning algorithm used for both classification and regression tasks that\nclassification and regression tasks that\nclassification and regression tasks that make decisions based on splitting data\nmake decisions based on splitting data\nmake decisions based on splitting data along feature values.\nalong feature values.\nalong feature values. Random forest is an ensemble learning\nRandom forest is an ensemble learning\nRandom forest is an ensemble learning method that consists of multiple\nmethod that consists of multiple\nmethod that consists of multiple decision trees and outputs the average\ndecision trees and outputs the average\ndecision trees and outputs the average prediction of the individual trees for\nprediction of the individual trees for\nprediction of the individual trees for regression tasks or the class that\nregression tasks or the class that\nregression tasks or the class that receives the most votes for\nreceives the most votes for\nreceives the most votes for classification\nclassification\nclassification [Music]\n[Music]\n[Music] tasks. Truncation is the process of\ntasks. Truncation is the process of\ntasks. Truncation is the process of limiting the number of elements in a\nlimiting the number of elements in a\nlimiting the number of elements in a data set or the number of nodes in a\ndata set or the number of nodes in a\ndata set or the number of nodes in a neural network.\nPrincipal component analysis or PCA is a\nPrincipal component analysis or PCA is a dimensionality reduction technique that\ndimensionality reduction technique that\ndimensionality reduction technique that transforms the original variables into a\ntransforms the original variables into a\ntransforms the original variables into a new set of uncorrelated\nvariables. Pre-training is the practice\nvariables. Pre-training is the practice of training a machine learning model on\nof training a machine learning model on\nof training a machine learning model on a large data set before fine-tuning it\na large data set before fine-tuning it\na large data set before fine-tuning it on a specific task.\n[Music]\n[Music] Object detection is a computer vision\nObject detection is a computer vision\nObject detection is a computer vision task that identifies and locates objects\ntask that identifies and locates objects\ntask that identifies and locates objects within images or\nwithin images or\nwithin images or [Music]\n[Music]\n[Music] video.\n[Music]\n[Music] Oversampling is a technique used to\nOversampling is a technique used to\nOversampling is a technique used to balance class distribution by randomly\nbalance class distribution by randomly\nbalance class distribution by randomly duplicating minority class\nduplicating minority class\nduplicating minority class [Music]\n[Music]\n[Music] instances. Outlier is a data point that\ninstances. Outlier is a data point that\ninstances. Outlier is a data point that deviates significantly from the rest of\ndeviates significantly from the rest of\ndeviates significantly from the rest of the data set. often considered as noise\nthe data set. often considered as noise\nthe data set. often considered as noise or\nor\nor [Music]\n[Music]\n[Music] anomaly. Overfitting is a modeling error\nanomaly. Overfitting is a modeling error\nanomaly. Overfitting is a modeling error that occurs when a machine learning\nthat occurs when a machine learning\nthat occurs when a machine learning algorithm captures noise in the training\nalgorithm captures noise in the training\nalgorithm captures noise in the training data.\ndata.\ndata. [Music]\n[Music]\n[Music] One hot encoding is a representation of\nOne hot encoding is a representation of\nOne hot encoding is a representation of categorical variables as binary vectors\ncategorical variables as binary vectors\ncategorical variables as binary vectors commonly used in machine learning\nalgorithms. Nearest neighbor search is\nalgorithms. Nearest neighbor search is an algorithm used to find the data\nan algorithm used to find the data\nan algorithm used to find the data points in a data set that are closest to\npoints in a data set that are closest to\npoints in a data set that are closest to a given point.\na given point.\na given point. [Music]\n[Music]\n[Music] Normal distribution is a probability\nNormal distribution is a probability\nNormal distribution is a probability distribution characterized by a\ndistribution characterized by a\ndistribution characterized by a bellshaped curve commonly used in\nbellshaped curve commonly used in\nbellshaped curve commonly used in statistics and machine\nstatistics and machine\nstatistics and machine [Music]\n[Music]\n[Music] learning. Normalization is the process\nlearning. Normalization is the process\nlearning. Normalization is the process of scaling the features to a standard\nof scaling the features to a standard\nof scaling the features to a standard range commonly used in machine learning\nrange commonly used in machine learning\nrange commonly used in machine learning to improve algorithm performance.\nNatural language processing or NLP is a\nNatural language processing or NLP is a field of AI that focuses on the\nfield of AI that focuses on the\nfield of AI that focuses on the interaction between computers and human\nlanguage. Matrix factorization is a\nlanguage. Matrix factorization is a technique used to decompose a matrix\ntechnique used to decompose a matrix\ntechnique used to decompose a matrix into multiple matrices commonly used in\ninto multiple matrices commonly used in\ninto multiple matrices commonly used in recommendation systems.\nrecommendation systems.\nrecommendation systems. Mark of chain is a stochcastic model\nMark of chain is a stochcastic model\nMark of chain is a stochcastic model representing a sequence of possible\nrepresenting a sequence of possible\nrepresenting a sequence of possible events where the probability of each\nevents where the probability of each\nevents where the probability of each event depends solely on the state\nevent depends solely on the state\nevent depends solely on the state attained in the previous event. Often\nattained in the previous event. Often\nattained in the previous event. Often used in machine learning and data\nused in machine learning and data\nused in machine learning and data science for simulating sampling from\nscience for simulating sampling from\nscience for simulating sampling from complex probability distributions and\ncomplex probability distributions and\ncomplex probability distributions and studying systems over time.\nstudying systems over time.\nstudying systems over time. Model selection is the process of\nModel selection is the process of\nModel selection is the process of choosing the most appropriate machine\nchoosing the most appropriate machine\nchoosing the most appropriate machine learning algorithm for a particular\ntask. Model evaluation is the process of\ntask. Model evaluation is the process of assessing the performance of a machine\nassessing the performance of a machine\nassessing the performance of a machine learning model using specific metrics.\nlearning model using specific metrics.\nlearning model using specific metrics. Jupyter Notebook is an open-source web\nJupyter Notebook is an open-source web\nJupyter Notebook is an open-source web application that allows for the creation\napplication that allows for the creation\napplication that allows for the creation and sharing of documents containing live\nand sharing of documents containing live\nand sharing of documents containing live code, equations, and\ncode, equations, and\ncode, equations, and visualizations. Knowledge transfer is\nvisualizations. Knowledge transfer is\nvisualizations. Knowledge transfer is the process of applying knowledge gained\nthe process of applying knowledge gained\nthe process of applying knowledge gained from one domain to another. Often used\nfrom one domain to another. Often used\nfrom one domain to another. Often used in machine learning to improve model\nin machine learning to improve model\nin machine learning to improve model performance.\nKnowledge graphs is a structured\nKnowledge graphs is a structured representation of facts and\nrepresentation of facts and\nrepresentation of facts and relationships commonly used in semantic\nrelationships commonly used in semantic\nrelationships commonly used in semantic search and recommendation\nsystems. Joint probability is the\nsystems. Joint probability is the probability of multiple events occurring\nprobability of multiple events occurring\nprobability of multiple events occurring together often used in probabilistic\ntogether often used in probabilistic\ntogether often used in probabilistic models.\nmodels.\nmodels. Inductive bias is the set of assumptions\nInductive bias is the set of assumptions\nInductive bias is the set of assumptions a machine learning algorithm makes to\na machine learning algorithm makes to\na machine learning algorithm makes to generalize from training data to unseen\ndata. Information extraction is the\ndata. Information extraction is the process of automatically extracting\nprocess of automatically extracting\nprocess of automatically extracting useful information from unstructured\nuseful information from unstructured\nuseful information from unstructured data sources.\n[Music]\n[Music] Inference is the process of making\nInference is the process of making\nInference is the process of making predictions using a trained machine\npredictions using a trained machine\npredictions using a trained machine learning\nlearning\nlearning [Music]\nmodel. Imbalanced data is a data set\nmodel. Imbalanced data is a data set where the class distribution is not\nwhere the class distribution is not\nwhere the class distribution is not equal, often requiring special handling\nequal, often requiring special handling\nequal, often requiring special handling techniques.\ntechniques.\ntechniques. Human in the loop is a machine learning\nHuman in the loop is a machine learning\nHuman in the loop is a machine learning approach that involves human interaction\napproach that involves human interaction\napproach that involves human interaction commonly used to improve model\ncommonly used to improve model\ncommonly used to improve model [Music]\naccuracy. Graphics processing unit or\naccuracy. Graphics processing unit or GPU is a specialized hardware used for\nGPU is a specialized hardware used for\nGPU is a specialized hardware used for rapid computation commonly used to\nrapid computation commonly used to\nrapid computation commonly used to accelerate machine learning algorithms.\naccelerate machine learning algorithms.\naccelerate machine learning algorithms. Vanishing gradient is a problem in\nVanishing gradient is a problem in\nVanishing gradient is a problem in training neural networks where gradients\ntraining neural networks where gradients\ntraining neural networks where gradients become too small for effective weight\n[Music]\n[Music] updates. Generalization is the ability\nupdates. Generalization is the ability\nupdates. Generalization is the ability of machine learning model to perform\nof machine learning model to perform\nof machine learning model to perform well on unseen data.\nGenerative adversarial networks or GANs\nGenerative adversarial networks or GANs is a class of machine learning\nis a class of machine learning\nis a class of machine learning frameworks where two neural networks the\nframeworks where two neural networks the\nframeworks where two neural networks the generator and the discriminator are\ngenerator and the discriminator are\ngenerator and the discriminator are trained together commonly used for image\ntrained together commonly used for image\ntrained together commonly used for image generation\ngeneration\ngeneration tasks. Assemble methods are techniques\ntasks. Assemble methods are techniques\ntasks. Assemble methods are techniques that combine multiple machine learning\nthat combine multiple machine learning\nthat combine multiple machine learning models to improve overall performance.\nmodels to improve overall performance.\nmodels to improve overall performance. Multiclass classification is a\nMulticlass classification is a\nMulticlass classification is a classification task where each sample\nclassification task where each sample\nclassification task where each sample can belong to one of three or more\nclasses. Data prep-processing is an\nclasses. Data prep-processing is an umbrella term for the initial steps of\numbrella term for the initial steps of\numbrella term for the initial steps of preparing and cleaning data before\npreparing and cleaning data before\npreparing and cleaning data before fitting it into a machine learning\nfitting it into a machine learning\nfitting it into a machine learning model.\nmodel.\nmodel. Regression analysis is a statistical\nRegression analysis is a statistical\nRegression analysis is a statistical process for estimating the relationships\nprocess for estimating the relationships\nprocess for estimating the relationships among variables commonly used in\namong variables commonly used in\namong variables commonly used in predictive\npredictive\npredictive [Music]\n[Music]\n[Music] modeling. The sigmoid function is an\nmodeling. The sigmoid function is an\nmodeling. The sigmoid function is an activation function that outputs values\nactivation function that outputs values\nactivation function that outputs values between zero and one commonly used in\nbetween zero and one commonly used in\nbetween zero and one commonly used in logistic regression and neural networks.\nlogistic regression and neural networks.\nlogistic regression and neural networks. Evolutionary algorithms are optimization\nEvolutionary algorithms are optimization\nEvolutionary algorithms are optimization algorithms inspired by the process of\nalgorithms inspired by the process of\nalgorithms inspired by the process of natural selection used in machine\nnatural selection used in machine\nnatural selection used in machine learning for parameter tuning. Language\nlearning for parameter tuning. Language\nlearning for parameter tuning. Language models are statistical models that\nmodels are statistical models that\nmodels are statistical models that predict the likelihood of a sequence of\npredict the likelihood of a sequence of\npredict the likelihood of a sequence of words commonly used in natural language\nwords commonly used in natural language\nwords commonly used in natural language processing. Back propagation is an\nprocessing. Back propagation is an\nprocessing. Back propagation is an optimization algorithm used to minimize\noptimization algorithm used to minimize\noptimization algorithm used to minimize the loss function by adjusting the\nthe loss function by adjusting the\nthe loss function by adjusting the model's weights. fundamental to the\nmodel's weights. fundamental to the\nmodel's weights. fundamental to the training of artificial neural\n[Music]\n[Music] networks. Bugging is an ensemble\nnetworks. Bugging is an ensemble\nnetworks. Bugging is an ensemble learning technique that improves\nlearning technique that improves\nlearning technique that improves stability and accuracy by training\nstability and accuracy by training\nstability and accuracy by training multiple instances of the same model on\nmultiple instances of the same model on\nmultiple instances of the same model on different subsets of the training\ndifferent subsets of the training\ndifferent subsets of the training data. Dense vector is a type of vector\ndata. Dense vector is a type of vector\ndata. Dense vector is a type of vector in which most of the elements are non\nin which most of the elements are non\nin which most of the elements are non zero. Commonly used in machine learning\nzero. Commonly used in machine learning\nzero. Commonly used in machine learning and data science for feature\nand data science for feature\nand data science for feature representation and various\nrepresentation and various\nrepresentation and various computations. Feature engineering is the\ncomputations. Feature engineering is the\ncomputations. Feature engineering is the process of transforming raw data into a\nprocess of transforming raw data into a\nprocess of transforming raw data into a format that makes it easier for machine\nformat that makes it easier for machine\nformat that makes it easier for machine learning algorithms to\nlearning algorithms to\nlearning algorithms to interpret. Support vector machines or\ninterpret. Support vector machines or\ninterpret. Support vector machines or SVMs are supervised learning algorithms\nSVMs are supervised learning algorithms\nSVMs are supervised learning algorithms used for classification and regression\nused for classification and regression\nused for classification and regression tasks that find the hyper plane that\ntasks that find the hyper plane that\ntasks that find the hyper plane that best separates different classes in the\nbest separates different classes in the\nbest separates different classes in the feature space.\nfeature space.\nfeature space. Cross validation is a technique for\nCross validation is a technique for\nCross validation is a technique for assessing the performance of a machine\nassessing the performance of a machine\nassessing the performance of a machine learning model by dividing the data set\nlearning model by dividing the data set\nlearning model by dividing the data set into multiple subsets and evaluating the\ninto multiple subsets and evaluating the\ninto multiple subsets and evaluating the model on different combinations of these\nmodel on different combinations of these\nmodel on different combinations of these subsets.\nLoss function is a mathematical function\nLoss function is a mathematical function that quantifies the difference between\nthat quantifies the difference between\nthat quantifies the difference between the predicted and actual outcomes in\nthe predicted and actual outcomes in\nthe predicted and actual outcomes in machine learning algorithms.\nmachine learning algorithms.\nmachine learning algorithms. [Music]\nP value is a measure used in hypothesis\nP value is a measure used in hypothesis testing to indicate the probability of\ntesting to indicate the probability of\ntesting to indicate the probability of observing a test statistic as extreme as\nobserving a test statistic as extreme as\nobserving a test statistic as extreme as the one computed given that the null\nthe one computed given that the null\nthe one computed given that the null hypothesis is\nhypothesis is\nhypothesis is true. Test is a statistical test used to\ntrue. Test is a statistical test used to\ntrue. Test is a statistical test used to compare the means of two groups and\ncompare the means of two groups and\ncompare the means of two groups and determine if they are significantly\ndetermine if they are significantly\ndetermine if they are significantly different from each other.\ndifferent from each other.\ndifferent from each other. Cosine similarity is a metric used to\nCosine similarity is a metric used to\nCosine similarity is a metric used to measure the cosine of the angle between\nmeasure the cosine of the angle between\nmeasure the cosine of the angle between two nonzero vectors in an inner product\ntwo nonzero vectors in an inner product\ntwo nonzero vectors in an inner product space. Often used to measure document\nspace. Often used to measure document\nspace. Often used to measure document similarity. Dropout is a regularization\nsimilarity. Dropout is a regularization\nsimilarity. Dropout is a regularization technique in neural networks where\ntechnique in neural networks where\ntechnique in neural networks where randomly selected neurons are ignored\nrandomly selected neurons are ignored\nrandomly selected neurons are ignored during training helping to prevent\nduring training helping to prevent\nduring training helping to prevent overfitting.\noverfitting.\noverfitting. [Music]\n[Music]\n[Music] Softmax function is an activation\nSoftmax function is an activation\nSoftmax function is an activation function that turns a vector of raw\nfunction that turns a vector of raw\nfunction that turns a vector of raw scores into probabilities often used in\nscores into probabilities often used in\nscores into probabilities often used in the output layer of a classification\nthe output layer of a classification\nthe output layer of a classification neural\nneural\nneural network. Base theorem is a principle in\nnetwork. Base theorem is a principle in\nnetwork. Base theorem is a principle in probability theory and statistics that\nprobability theory and statistics that\nprobability theory and statistics that describes the probability of an event\ndescribes the probability of an event\ndescribes the probability of an event based on prior knowledge of related\nbased on prior knowledge of related\nbased on prior knowledge of related conditions.\nconditions.\nconditions. Tan function is an activation function\nTan function is an activation function\nTan function is an activation function used in neural networks that scales the\nused in neural networks that scales the\nused in neural networks that scales the output to be in the range between\noutput to be in the range between\noutput to be in the range between negative -1 and positive\nnegative -1 and positive\nnegative -1 and positive one. A relu function or rectified linear\none. A relu function or rectified linear\none. A relu function or rectified linear unit is a nonlinear activation function\nunit is a nonlinear activation function\nunit is a nonlinear activation function used in neural networks that output the\nused in neural networks that output the\nused in neural networks that output the input if it is positive and zero\ninput if it is positive and zero\ninput if it is positive and zero otherwise. Mean squared error is a loss\notherwise. Mean squared error is a loss\notherwise. Mean squared error is a loss function used in regression problems\nfunction used in regression problems\nfunction used in regression problems that measures the average of the squares\nthat measures the average of the squares\nthat measures the average of the squares of the errors between predicted and\nof the errors between predicted and\nof the errors between predicted and actual values. Root mean square error is\nactual values. Root mean square error is\nactual values. Root mean square error is the square root of the mean squared\nthe square root of the mean squared\nthe square root of the mean squared error providing a measure of the average\nerror providing a measure of the average\nerror providing a measure of the average magnitude of the errors between\nmagnitude of the errors between\nmagnitude of the errors between predicted and actual observations.\npredicted and actual observations.\npredicted and actual observations. R squar or the coefficient of\nR squar or the coefficient of\nR squar or the coefficient of determination is a statistical measure\ndetermination is a statistical measure\ndetermination is a statistical measure indicating the proportion of the\nindicating the proportion of the\nindicating the proportion of the dependent variables variance that is\ndependent variables variance that is\ndependent variables variance that is explained by the independent variable or\nexplained by the independent variable or\nexplained by the independent variable or variables in a regression model.\nvariables in a regression model.\nvariables in a regression model. L1 and L2 regularization are techniques\nL1 and L2 regularization are techniques\nL1 and L2 regularization are techniques that add penalty terms to the loss\nthat add penalty terms to the loss\nthat add penalty terms to the loss function to prevent overfitting with L1\nfunction to prevent overfitting with L1\nfunction to prevent overfitting with L1 leading to sparse solutions and L2\nleading to sparse solutions and L2\nleading to sparse solutions and L2 simply shrinking the\nsimply shrinking the\nsimply shrinking the weights. Learning rate is a tuning\nweights. Learning rate is a tuning\nweights. Learning rate is a tuning parameter in an optimization algorithm\nparameter in an optimization algorithm\nparameter in an optimization algorithm that determines the step size at each\nthat determines the step size at each\nthat determines the step size at each iteration while moving towards a minimum\niteration while moving towards a minimum\niteration while moving towards a minimum of a loss function. It metaphorically\nof a loss function. It metaphorically\nof a loss function. It metaphorically represents the speed at which a machine\nrepresents the speed at which a machine\nrepresents the speed at which a machine learning model learns, influencing the\nlearning model learns, influencing the\nlearning model learns, influencing the extent to which newly acquired\nextent to which newly acquired\nextent to which newly acquired information overrides old information.\n[Music]\n[Music] Naive base classifier is a probabilistic\nNaive base classifier is a probabilistic\nNaive base classifier is a probabilistic classifier based on applying base\nclassifier based on applying base\nclassifier based on applying base theorem assuming that all features are\ntheorem assuming that all features are\ntheorem assuming that all features are independent of each other given the\nindependent of each other given the\nindependent of each other given the category of the object. Cost function is\ncategory of the object. Cost function is\ncategory of the object. Cost function is a function that measures the cost or\na function that measures the cost or\na function that measures the cost or loss of the model's prediction compared\nloss of the model's prediction compared\nloss of the model's prediction compared to the true value and the optimization\nto the true value and the optimization\nto the true value and the optimization task is to minimize this function.\ntask is to minimize this function.\ntask is to minimize this function. Confusion matrix, also known as an error\nConfusion matrix, also known as an error\nConfusion matrix, also known as an error matrix, is a specific table layout that\nmatrix, is a specific table layout that\nmatrix, is a specific table layout that allows visualization of the performance\nallows visualization of the performance\nallows visualization of the performance of an algorithm, typically a supervised\nof an algorithm, typically a supervised\nof an algorithm, typically a supervised learning one. Each row of the matrix\nlearning one. Each row of the matrix\nlearning one. Each row of the matrix represents the instances in an actual\nrepresents the instances in an actual\nrepresents the instances in an actual class, while each column represents the\nclass, while each column represents the\nclass, while each column represents the instances in a predicted class.\ninstances in a predicted class.\ninstances in a predicted class. Precision is the fraction of relevant\nPrecision is the fraction of relevant\nPrecision is the fraction of relevant instances among the retrieved instances\ninstances among the retrieved instances\ninstances among the retrieved instances reflecting the accuracy of the model in\nreflecting the accuracy of the model in\nreflecting the accuracy of the model in classifying positive instances. Recall,\nclassifying positive instances. Recall,\nclassifying positive instances. Recall, also known as sensitivity, is the\nalso known as sensitivity, is the\nalso known as sensitivity, is the fraction of relevant instances that were\nfraction of relevant instances that were\nfraction of relevant instances that were retrieved, indicating the ability of the\nretrieved, indicating the ability of the\nretrieved, indicating the ability of the model to identify all relevant\nmodel to identify all relevant\nmodel to identify all relevant instances. Both metrics are crucial for\ninstances. Both metrics are crucial for\ninstances. Both metrics are crucial for evaluating the performance of\nevaluating the performance of\nevaluating the performance of classification models, especially in\nclassification models, especially in\nclassification models, especially in imbalanced data sets where one class\nimbalanced data sets where one class\nimbalanced data sets where one class significantly outnumbers the other. The\nsignificantly outnumbers the other. The\nsignificantly outnumbers the other. The area under the curve of the receiver\narea under the curve of the receiver\narea under the curve of the receiver operating characteristic curve is a\noperating characteristic curve is a\noperating characteristic curve is a performance measurement for\nperformance measurement for\nperformance measurement for classification problems representing the\nclassification problems representing the\nclassification problems representing the probability that a model will rank a\nprobability that a model will rank a\nprobability that a model will rank a randomly chosen positive instance higher\nrandomly chosen positive instance higher\nrandomly chosen positive instance higher than a randomly chosen negative one\nthan a randomly chosen negative one\nthan a randomly chosen negative one providing an aggregate measure of model\nproviding an aggregate measure of model\nproviding an aggregate measure of model performance across all possible\nperformance across all possible\nperformance across all possible classification thresholds. Train test\nclassification thresholds. Train test\nclassification thresholds. Train test split refers to the process of dividing\nsplit refers to the process of dividing\nsplit refers to the process of dividing the data set into two subsets. One used\nthe data set into two subsets. One used\nthe data set into two subsets. One used for training the model known as the\nfor training the model known as the\nfor training the model known as the training set and the other used for\ntraining set and the other used for\ntraining set and the other used for evaluating the model's performance known\nevaluating the model's performance known\nevaluating the model's performance known as the test set. This method helps in\nas the test set. This method helps in\nas the test set. This method helps in assessing how well the model will\nassessing how well the model will\nassessing how well the model will generalize to new unseen data. Grid\ngeneralize to new unseen data. Grid\ngeneralize to new unseen data. Grid search or a parameter sweep is\nsearch or a parameter sweep is\nsearch or a parameter sweep is traditionally used for hyperparameter\ntraditionally used for hyperparameter\ntraditionally used for hyperparameter optimization in machine learning and\noptimization in machine learning and\noptimization in machine learning and involves exhaustively searching through\ninvolves exhaustively searching through\ninvolves exhaustively searching through a manually specified subset of the\na manually specified subset of the\na manually specified subset of the hyperparameter space of a learning\nhyperparameter space of a learning\nhyperparameter space of a learning algorithm. It must be guided by some\nalgorithm. It must be guided by some\nalgorithm. It must be guided by some performance metric typically measured by\nperformance metric typically measured by\nperformance metric typically measured by cross validation on the training set or\ncross validation on the training set or\ncross validation on the training set or evaluation on a hold out a validation\nevaluation on a hold out a validation\nevaluation on a hold out a validation set. Grid search evaluates different\nset. Grid search evaluates different\nset. Grid search evaluates different combinations of hyperparameter values\ncombinations of hyperparameter values\ncombinations of hyperparameter values and outputs the settings that achieved\nand outputs the settings that achieved\nand outputs the settings that achieved the highest score in the validation\nthe highest score in the validation\nthe highest score in the validation procedure. Anomaly detection refers to\nprocedure. Anomaly detection refers to\nprocedure. Anomaly detection refers to the identification of rare items, events\nthe identification of rare items, events\nthe identification of rare items, events or observations which significantly\nor observations which significantly\nor observations which significantly deviate from the majority of the data\ndeviate from the majority of the data\ndeviate from the majority of the data and do not conform to a well-defined\nand do not conform to a well-defined\nand do not conform to a well-defined notion of normal behavior. finding\nnotion of normal behavior. finding\nnotion of normal behavior. finding applications in various domains\napplications in various domains\napplications in various domains including cyber security, medicine and\nincluding cyber security, medicine and\nincluding cyber security, medicine and financial fraud detection. Missing\nfinancial fraud detection. Missing\nfinancial fraud detection. Missing values refer to the absence of data\nvalues refer to the absence of data\nvalues refer to the absence of data values in expected data points which can\nvalues in expected data points which can\nvalues in expected data points which can significantly impact the analysis and\nsignificantly impact the analysis and\nsignificantly impact the analysis and conclusions drawn from the data\nconclusions drawn from the data\nconclusions drawn from the data necessitating the use of various\nnecessitating the use of various\nnecessitating the use of various handling techniques such as imputation\nhandling techniques such as imputation\nhandling techniques such as imputation emission or direct analysis methods to\nemission or direct analysis methods to\nemission or direct analysis methods to mitigate biases and inaccuracies.\nmitigate biases and inaccuracies.\nmitigate biases and inaccuracies. Ukidian distance between two points in\nUkidian distance between two points in\nUkidian distance between two points in uklidian space is the length of the line\nuklidian space is the length of the line\nuklidian space is the length of the line segment between those two points which\nsegment between those two points which\nsegment between those two points which can be calculated using the Pythagorean\ncan be calculated using the Pythagorean\ncan be calculated using the Pythagorean theorem and is often used to measure the\ntheorem and is often used to measure the\ntheorem and is often used to measure the similarity between objects or data\nsimilarity between objects or data\nsimilarity between objects or data points.\npoints.\npoints. Manhattan distance also known as taxi\nManhattan distance also known as taxi\nManhattan distance also known as taxi cap or L1 distance is a metric in which\ncap or L1 distance is a metric in which\ncap or L1 distance is a metric in which the distance between two points is the\nthe distance between two points is the\nthe distance between two points is the sum of the absolute differences of their\nsum of the absolute differences of their\nsum of the absolute differences of their cartisian coordinates. It is named after\ncartisian coordinates. It is named after\ncartisian coordinates. It is named after the grid layout of Manhattan streets\nthe grid layout of Manhattan streets\nthe grid layout of Manhattan streets reflecting the path a taxi would take\nreflecting the path a taxi would take\nreflecting the path a taxi would take between two points on the island.\nbetween two points on the island.\nbetween two points on the island. Humming distance is a metric used to\nHumming distance is a metric used to\nHumming distance is a metric used to measure the difference between two\nmeasure the difference between two\nmeasure the difference between two strings of equal length by counting the\nstrings of equal length by counting the\nstrings of equal length by counting the number of positions at which the\nnumber of positions at which the\nnumber of positions at which the corresponding symbols are different. It\ncorresponding symbols are different. It\ncorresponding symbols are different. It is particularly useful in error\nis particularly useful in error\nis particularly useful in error detection and error correction in coding\ndetection and error correction in coding\ndetection and error correction in coding theory. Jakart similarity quantifies the\ntheory. Jakart similarity quantifies the\ntheory. Jakart similarity quantifies the degree of similarity between two sets by\ndegree of similarity between two sets by\ndegree of similarity between two sets by computing the ratio of the size of their\ncomputing the ratio of the size of their\ncomputing the ratio of the size of their intersection to the size of their union.\nK means clustering is a method in\nK means clustering is a method in machine learning and data science that\nmachine learning and data science that\nmachine learning and data science that partitions n observations into K\npartitions n observations into K\npartitions n observations into K clusters where each observation belongs\nclusters where each observation belongs\nclusters where each observation belongs to the cluster with the nearest mean\nto the cluster with the nearest mean\nto the cluster with the nearest mean serving as a prototype of the cluster\nserving as a prototype of the cluster\nserving as a prototype of the cluster thus minimizing within cluster\nthus minimizing within cluster\nthus minimizing within cluster variances.\nvariances.\nvariances. Bootstrapping is a form of hypothesis\nBootstrapping is a form of hypothesis\nBootstrapping is a form of hypothesis testing that involves resampling a\ntesting that involves resampling a\ntesting that involves resampling a single data set to create a multitude of\nsingle data set to create a multitude of\nsingle data set to create a multitude of simulated samples. Those samples are\nsimulated samples. Those samples are\nsimulated samples. Those samples are used to calculate standard errors,\nused to calculate standard errors,\nused to calculate standard errors, confidence intervals, and for hypothesis\nconfidence intervals, and for hypothesis\nconfidence intervals, and for hypothesis testing. Hierarchal clustering is a\ntesting. Hierarchal clustering is a\ntesting. Hierarchal clustering is a method of clustering in which objects\nmethod of clustering in which objects\nmethod of clustering in which objects are grouped into hierarchal structure\nare grouped into hierarchal structure\nare grouped into hierarchal structure allowing researchers to explore data at\nallowing researchers to explore data at\nallowing researchers to explore data at various levels of granularity.\nvarious levels of granularity.\nvarious levels of granularity. Matrix multiplication in the context of\nMatrix multiplication in the context of\nMatrix multiplication in the context of machine learning and data science is a\nmachine learning and data science is a\nmachine learning and data science is a fundamental algebraic operation where\nfundamental algebraic operation where\nfundamental algebraic operation where two matrices are combined to produce a\ntwo matrices are combined to produce a\ntwo matrices are combined to produce a new matrix and is crucial to performing\nnew matrix and is crucial to performing\nnew matrix and is crucial to performing transformations solving linear equations\ntransformations solving linear equations\ntransformations solving linear equations and optimizing models. Jacobian matrix\nand optimizing models. Jacobian matrix\nand optimizing models. Jacobian matrix is a matrix composed of all first\nis a matrix composed of all first\nis a matrix composed of all first partial derivatives of a multivariable\npartial derivatives of a multivariable\npartial derivatives of a multivariable function. And in the context of machine\nfunction. And in the context of machine\nfunction. And in the context of machine learning and data science, it is often\nlearning and data science, it is often\nlearning and data science, it is often used for optimization and gradient\nused for optimization and gradient\nused for optimization and gradient computation. Hessen matrix is a square\ncomputation. Hessen matrix is a square\ncomputation. Hessen matrix is a square matrix of second order partial\nmatrix of second order partial\nmatrix of second order partial derivatives. And in the context of\nderivatives. And in the context of\nderivatives. And in the context of machine learning and data science, it is\nmachine learning and data science, it is\nmachine learning and data science, it is often used for optimization allowing for\noften used for optimization allowing for\noften used for optimization allowing for the assessment of the convexity or\nthe assessment of the convexity or\nthe assessment of the convexity or concavity of a loss function.\nconcavity of a loss function.\nconcavity of a loss function. Measures of central tendency refer to a\nMeasures of central tendency refer to a\nMeasures of central tendency refer to a central or typical value for a\ncentral or typical value for a\ncentral or typical value for a probability distribution often calculate\nprobability distribution often calculate\nprobability distribution often calculate called averages. The most common\ncalled averages. The most common\ncalled averages. The most common measures of central tendency are the\nmeasures of central tendency are the\nmeasures of central tendency are the arithmetic mean, the median, and the\narithmetic mean, the median, and the\narithmetic mean, the median, and the mode. These measures can be applied to\nmode. These measures can be applied to\nmode. These measures can be applied to one-dimensional data and can be used to\none-dimensional data and can be used to\none-dimensional data and can be used to understand the central position around\nunderstand the central position around\nunderstand the central position around which data values are distributed.\nwhich data values are distributed.\nwhich data values are distributed. Activation function determines the\nActivation function determines the\nActivation function determines the output value of a node based on its\noutput value of a node based on its\noutput value of a node based on its input values and weights and nonlinear\ninput values and weights and nonlinear\ninput values and weights and nonlinear activation functions enable the model to\nactivation functions enable the model to\nactivation functions enable the model to approximate complex nonlinear\napproximate complex nonlinear\napproximate complex nonlinear relationships.\nrelationships.\nrelationships. [Music]\n[Music]\n[Music] Artificial neural network is a system\nArtificial neural network is a system\nArtificial neural network is a system inspired by the structure and\ninspired by the structure and\ninspired by the structure and functioning of biological neural\nfunctioning of biological neural\nfunctioning of biological neural networks capable of learning, adapting,\nnetworks capable of learning, adapting,\nnetworks capable of learning, adapting, and recognizing complex patterns using\nand recognizing complex patterns using\nand recognizing complex patterns using multiple layers of artificial neurons.\nmultiple layers of artificial neurons.\nmultiple layers of artificial neurons. Each processing information and passing\nEach processing information and passing\nEach processing information and passing it forward.\nit forward.\nit forward. Perceptron is an algorithm for\nPerceptron is an algorithm for\nPerceptron is an algorithm for supervised learning of binary\nsupervised learning of binary\nsupervised learning of binary classifiers. A function which can decide\nclassifiers. A function which can decide\nclassifiers. A function which can decide whether an input represented by a vector\nwhether an input represented by a vector\nwhether an input represented by a vector of numbers belongs to some specific\nof numbers belongs to some specific\nof numbers belongs to some specific class. It is a type of linear classifier\nclass. It is a type of linear classifier\nclass. It is a type of linear classifier meaning a classification algorithm that\nmeaning a classification algorithm that\nmeaning a classification algorithm that makes its predictions based on a linear\nmakes its predictions based on a linear\nmakes its predictions based on a linear predictor function combining a set of\npredictor function combining a set of\npredictor function combining a set of weights with a feature vector.\nweights with a feature vector.\nweights with a feature vector. Convolutional neural network or CNN is a\nConvolutional neural network or CNN is a\nConvolutional neural network or CNN is a type of artificial neural network\ntype of artificial neural network\ntype of artificial neural network specifically designed for processing,\nspecifically designed for processing,\nspecifically designed for processing, recognizing, and classifying images\nrecognizing, and classifying images\nrecognizing, and classifying images utilizing convolution to automatically\nutilizing convolution to automatically\nutilizing convolution to automatically and adaptively extract features from\nand adaptively extract features from\nand adaptively extract features from input data. Enabling the network to\ninput data. Enabling the network to\ninput data. Enabling the network to efficiently handle visual information\nefficiently handle visual information\nefficiently handle visual information and find applications in various domains\nand find applications in various domains\nand find applications in various domains such as image recognition, medical image\nsuch as image recognition, medical image\nsuch as image recognition, medical image analysis, and natural language\nanalysis, and natural language\nanalysis, and natural language processing.\nprocessing.\nprocessing. [Music]\nRecurrent neural network or RNN is a\nRecurrent neural network or RNN is a type of neural network well suited for\ntype of neural network well suited for\ntype of neural network well suited for processing sequences of inputs\nprocessing sequences of inputs\nprocessing sequences of inputs characterized by its ability to maintain\ncharacterized by its ability to maintain\ncharacterized by its ability to maintain internal state called memory to process\ninternal state called memory to process\ninternal state called memory to process arbitrary sequences of inputs making it\narbitrary sequences of inputs making it\narbitrary sequences of inputs making it applicable to tasks such as handwriting\napplicable to tasks such as handwriting\napplicable to tasks such as handwriting recognition or speech recognition. It\nrecognition or speech recognition. It\nrecognition or speech recognition. It allows the input from some nodes to\nallows the input from some nodes to\nallows the input from some nodes to affect subsequent input to the same\naffect subsequent input to the same\naffect subsequent input to the same nodes enabling the network to exhibit\nnodes enabling the network to exhibit\nnodes enabling the network to exhibit temporal dynamic behavior and be\ntemporal dynamic behavior and be\ntemporal dynamic behavior and be theoretically during complete.\ntheoretically during complete.\ntheoretically during complete. Long short-term memory or LSTM is a type\nLong short-term memory or LSTM is a type\nLong short-term memory or LSTM is a type of recurrent neural network designed to\nof recurrent neural network designed to\nof recurrent neural network designed to address the vanishing gradient problem\naddress the vanishing gradient problem\naddress the vanishing gradient problem inherent in traditional RNNs allowing\ninherent in traditional RNNs allowing\ninherent in traditional RNNs allowing the network to learn long-term\nthe network to learn long-term\nthe network to learn long-term dependencies and effectively handle\ndependencies and effectively handle\ndependencies and effectively handle sequences of data making it applicable\nsequences of data making it applicable\nsequences of data making it applicable in fields like speech recognition\nin fields like speech recognition\nin fields like speech recognition machine translation and time series\nmachine translation and time series\nmachine translation and time series prediction.\nprediction.\nprediction. Transformer model is a deep learning\nTransformer model is a deep learning\nTransformer model is a deep learning architecture introduced in the paper\narchitecture introduced in the paper\narchitecture introduced in the paper attention is all you need that relies on\nattention is all you need that relies on\nattention is all you need that relies on self-attention mechanisms and is known\nself-attention mechanisms and is known\nself-attention mechanisms and is known for its parallel processing capabilities\nfor its parallel processing capabilities\nfor its parallel processing capabilities allowing it to require less training\nallowing it to require less training\nallowing it to require less training time compared to previous recurrent\ntime compared to previous recurrent\ntime compared to previous recurrent neural architectures like LSDM. It has\nneural architectures like LSDM. It has\nneural architectures like LSDM. It has been widely adopted for training large\nbeen widely adopted for training large\nbeen widely adopted for training large language models and has applications in\nlanguage models and has applications in\nlanguage models and has applications in natural language processing, computer\nnatural language processing, computer\nnatural language processing, computer vision, and other domains. Padding\nvision, and other domains. Padding\nvision, and other domains. Padding refers to the technique of adding extra\nrefers to the technique of adding extra\nrefers to the technique of adding extra data points or placeholders to a data\ndata points or placeholders to a data\ndata points or placeholders to a data set or an array often used in processing\nset or an array often used in processing\nset or an array often used in processing input data for convolutional neural\ninput data for convolutional neural\ninput data for convolutional neural networks to ensure that the convolution\nnetworks to ensure that the convolution\nnetworks to ensure that the convolution kernels fit properly over the input data\nkernels fit properly over the input data\nkernels fit properly over the input data maintaining the spatial dimensions of\nmaintaining the spatial dimensions of\nmaintaining the spatial dimensions of the input. pooling is a dimensionality\nthe input. pooling is a dimensionality\nthe input. pooling is a dimensionality reduction method that serves to decrease\nreduction method that serves to decrease\nreduction method that serves to decrease the number of parameters and\nthe number of parameters and\nthe number of parameters and computations in the network control\ncomputations in the network control\ncomputations in the network control overfitting and progressively reduce the\noverfitting and progressively reduce the\noverfitting and progressively reduce the spatial size of the representation known\nspatial size of the representation known\nspatial size of the representation known as down\nas down\nas down [Music]\nsampling. Variational autoenccoder is a\nsampling. Variational autoenccoder is a generative model that leverages\ngenerative model that leverages\ngenerative model that leverages variational basin methods to encode and\nvariational basin methods to encode and\nvariational basin methods to encode and decode input data aiming to generate new\ndecode input data aiming to generate new\ndecode input data aiming to generate new data similar to the input and it is\ndata similar to the input and it is\ndata similar to the input and it is particularly useful in unsupervised\nparticularly useful in unsupervised\nparticularly useful in unsupervised learning scenarios.\nQuantum machine learning is the\nQuantum machine learning is the intersection of quantum computing and\nintersection of quantum computing and\nintersection of quantum computing and machine learning, aiming to solve\nmachine learning, aiming to solve\nmachine learning, aiming to solve complex problems more\ncomplex problems more\ncomplex problems more efficiently. And if you like this video,\nefficiently. And if you like this video,\nefficiently. And if you like this video, I bet you will love this one, too. So,\nI bet you will love this one, too. So,\nI bet you will love this one, too. So, watch it next. That was V and I'll see\nwatch it next. That was V and I'll see\nwatch it next. That was V and I'll see you in a bite.\nyou in a bite.\nyou in a bite. [Music]"
  },
  {
    "id": 258990563,
    "timestamp": "2026-02-05T01:42:50.889Z",
    "title": "Machine Learning Full Course - Learn Machine Learning 10 Hours | Machine Learning Tutorial | Edureka",
    "url": "https://www.youtube.com/watch?v=GwIo3gDZCVQ",
    "text": "I'm sure you all agree that machine learning is one\nof the hottest Trend in today's market right Gartner predicts\nthat by 2022 there would be at least 40%\nof new application development project going on in the market\nthat would be requiring machine learning co-developers\non their team.\nIt's expected that these project will generate a revenue\nof around three point nine trillion dollar,\nisn't it cute so looking at the huge?\nUpcoming demand of machine learning around the world.\nWe guys at Eureka have come up\nand designed a well-structured machine learning full course\nfor you guys.\nBut before we actually drill down over there,\nlet me just introduce myself.\nHello all I am Atul from Edureka.\nAnd today I'll be guiding you\nthrough this entire machine learning course.\nWell, this course has been designed in a way\nthat you get the most out of it.\nSo we'll slowly and gradually start\nwith a beginner level and then move towards the advanced topic.\nSo without delaying any further,\nlet's start with the agenda of today's Action\non machine learning course has been segregated\ninto six different module will start our first module\nwith introduction to machine learning here.\nWe'll discuss things.\nLike what exactly is machine learning\nhow it differs from artificial intelligence and the planning\nwhat is various types or dead space application\nand finally we'll end up first module\nwith a basic demo and python.\nOkay a second module focuses on starts\nand probability here will cover things\nlike descriptive statistics and inferential statistics to Bob.\nRarity Theory and so\non our third module is unsupervised learning.\nWell supervised learning is one of a type of machine learning\nwhich focuses mainly\non regression and classification type of problem.\nIt deals with label data sets and the algorithm\nwhich are a part of it are linear regression\nlogistic regression Napier's random Forest decision tree\nand so on.\nOur fourth module is on unsupervised learning.\nWell this module focuses mainly on dealing\nwith unlabeled data sets\nand the algorithm which are a part.\nOffered or k-means algorithm\nand a priori algorithm as a part of fifth module.\nWe have reinforcement learning here.\nWe are going to discuss about reinforcement learning\nand depth on also\nabout Q learning algorithm finally in the end.\nIt's all about to make you industry ready.\nOkay.\nSo here we are going to discuss about three different projects\nwhich are based on supervised learning\nand unsupervised learning\nand reinforcement learning finally in the end.\nI tell you about some of the skills\nthat you need to become a machine learnings and Jean.\nNia okay, and also I am discussing about some\nof the important questions\nthat are asked in a machine-learning interview fine\nwith this we come to the end of this agenda\nbefore you move ahead\ndon't forget to subscribe to a dareka and press\nthe Bell icon to never miss any update from us.\nHello everyone.\nThis is a toll from Eureka\nand welcome to today's session on what is machine learning.\nAs you know,\nwe are living in a world of humans\nand machines humans have been evolving\nand learning from the past experience since millions\nof years on the other hand the era of machines\nand robots have just begun in today's world.\nThese machines are the rewards are\nlike they need to be program\nbefore they actually follow your instructions.\nBut what if the machine started to learn\non their own and this is\nwhere machine learning comes\ninto picture machine learning is the core\nof many futuristic technology advancement in our world.\nAnd today you can see various examples\nor implementation of machine learning around us\nsuch as Tesla's self-driving car Apple Siri, Sophia.\nI do bot and many more are there.\nSo what exactly is machine learning?\nWell Machine learning is a subfield\nof artificial intelligence\nthat focuses on the design of system\nthat can learn from and make decisions\nand predictions based on the experience\nwhich is data in the case of machines machine learning\nenables computer to act\nand make data-driven decisions rather than\nBeing explicitly programmed\nto carry out a certain task these programs\nare designed to learn\nand improve over time\nwhen exposed to new data.\nLet's move on and discuss one\nof the biggest confusion of the people in the world.\nThey think that all the three of them\nthe AI the machine learning and the Deep learning all are same,\nyou know, what they are wrong.\nLet me clarify things\nfor you artificial intelligence is a broader concept\nof machines being able to carry out tasks in a smarter way.\nIt covers anything which enables the computer to be.\nHave like humans think of a famous Turing test to determine\nwhether a computer is capable of thinking\nlike a human being or not.\nIf you are talking to Siri on your phone\nand you get an answer you're already very close to it.\nSo this was about the artificial intelligence now coming\nto the machine learning part.\nSo as I already said machine learning is a subset\nor a current application of AI it is based on the idea\nthat we should be able to give machine the access\nto data and let them learn from done cells.\nIt's a subset of artificial intelligence.\nIs that deals\nwith the extraction of pattern from data set?\nThis means that the machine can not only find the rules\nfor optimal Behavior,\nbut also can adapt to the changes in the world many\nof the algorithms involved have been known\nfor decades centuries even thanks to the advances\nin the computer science and parallel Computing.\nThey can now scale up to massive data volumes.\nSo this was about the machine learning part now coming over\nto deep learning deep learning is a subset of machine learning\nwhere similar machine learning.\nTamar used to train deep neural network.\nSo as to achieve better accuracy in those cases\nwhere former was not performing up to the mark, right?\nI hope now you understood that machine learning Ai\nand deep learning all three are different.\nOkay moving on ahead.\nLet's see in general how a machine learning work.\nOne of the approaches is\nwhere the machine learning algorithm is strained\nusing a labeled or unlabeled training data\nset to produce a model\nnew input data is introduced to the machine learning algorithm\nand it make prediction based on the model.\nThe prediction is evaluated for accuracy.\nAnd if the accuracy is acceptable the machine\nlearning algorithm is deployed.\nNow if the accuracy is not acceptable\nthe machine learning algorithm is strained again,\nand again with an argument a training data set.\nThis was just in high-level example\nas they are many more factor and other steps involved in it.\nNow, let's move on and subcategorize the Machine\nlearning into three different types the supervised learning\nand unsupervised learning and reinforcement\nlearning and let's see what each of them are how they work.\nWork and how each\nof them is used in the field of banking Healthcare retail\nand other domains.\nDon't worry.\nI'll make sure\nthat I use enough examples and implementation of all three\nof them to give you a proper understanding of it.\nSo starting with supervised learning.\nWhat is it?\nSo let's see a mathematical definition\nof supervised learning supervised learning is\nwhere you have input variables X and an output variable Y\nand you use an algorithm to learn the mapping function\nfrom the input to the output.\nThat is y Affects\nthe goal is to approximate the mapping function.\nSo well that whenever you have a new input data\nX you could predict the output variable.\nThat is why for that data, right?\nI think this was confusing for you.\nLet me simplify the definition of supervised learning\nso we can rephrase the understanding\nof the mathematical definition as a machine learning method\nwhere each instances of a training data set is composed\nof different input attribute\nand an expected output the input attributes\nof a training data set can be of any End of data it can be\na pixel of the image.\nIt can be a value of a data base row\nor it can even be an audio frequency histogram right\nfor each input instance\nand expected output values\nAssociated value can be discreet representing a category\nor can be a real or continuous value in either case.\nThe algorithm learns the input pattern\nthat generate the expected output now\nonce the algorithm is strain,\nit can be used to predict the correct output\nof a never seen input.\nYou can see I image on your screen right\nin this image.\nAnd see that we are feeding raw inputs as image of Apple\nto the algorithm as a part of the algorithm.\nWe have a supervisor who keeps on correcting\nthe machine or who keeps on training the machine.\nIt keeps on telling him that yes, it is a Apple.\nNo, it is not an apple things like that.\nSo this process keeps\non repeating until we get a final train model.\nOnce the model is ready.\nIt can easily predict the correct output\nof a never seen input in this slide.\nYou can see\nthat we are giving an image of a green apple to the machine\nand the Machine can easily identify it as yes,\nit is an apple and it is giving the correct result right?\nLet me make things more clearer to you.\nLet's discuss another example of it.\nSo in this Slide,\nthe image shows an example\nof a supervised learning process used to produce a model\nwhich is capable of recognizing the ducks in the image.\nThe training data set is composed of labeled picture\nof ducks and non Ducks.\nThe result of supervised learning process is\na predictor model\nwhich is capable of associating a label duck.\nOr not duck to the new image presented to the model.\nNow one strain,\nthe resulting predictive model can be deployed\nto the production environment.\nYou can see a mobile app.\nFor example once deployed it is ready to recognize\nthe new pictures right now.\nYou might be wondering why this category\nof machine learning is named as supervised learning.\nWell, it is called a supervised learning\nbecause the process of an algorithm learning\nfrom the training data set can be thought\nof as a teacher supervising the learning process\nif we know the correct answers.\nI will go Rhythm iteratively makes\nwhile predicting on the training data\nand is corrected by the teacher the learning stops\nwhen the algorithm achieves an acceptable level of performance.\nNow, let's move on and see some\nof the popular supervised learning algorithm.\nSo we have linear regression random forest\nand support Vector machines.\nThese are just for your information.\nWe will discuss about these algorithms\nin our next video.\nNow, let's see some of the popular use cases\nof supervised learning\nso we have Donna codon or any other speech\nAutomation in your mobile phone trains using your voice\nand one strain it start working based on the training.\nThis is an application of supervised learning suppose.\nYou are telling OK Google call Sam\nor you say Hey Siri call Sam you get an answer to it\nand action is performed\nand automatically a call goes to Sam.\nSo these are just an example\nof supervised learning next comes the weather up\nbased on some of the prior knowledge\nlike when it is sunny the temperature is high.\nFire when it is cloudy humidity is higher any kind of that they\npredict the parameters for a given time.\nSo this is also an example of supervised learning\nas we are feeding the data to the machine and telling\nthat whenever it is sunny.\nThe temperature should be higher whenever it is cloudy.\nThe humidity should be higher.\nSo it's an example of supervised learning.\nAnother example is biometric attendance\nwhere you train the machine and after couple of inputs\nof your biometric identity beat your thumb your iris\nor yellow or anything\nonce trained Machine gun validate your future input\nand can identify you next comes in the field of banking sector\nin banking sector\nsupervised learning is used to predict the credit worthiness\nof a credit card holder\nby building a machine learning model to look\nfor faulty attributes by providing it\nwith a data on deliquent\nand non-delinquent customers.\nNext comes the healthcare sector in the healthcare sector.\nIt is used to predict the patient's readmission rates\nby building a regression model\nby providing data\non the patients treatment Administration and readmissions\nto show variables\nthat best correlate with readmission.\nNext comes the retail sector and Retail sector.\nIt is used to analyze the product\nthat a customer by together.\nIt does this by building a supervised model\nto identify frequent itemsets\nand Association rule from the transactional data now,\nlets learn about the next category\nof machine learning the unsupervised part mathematically\nunsupervised learning is\nwhere you only\nhave Put data X and no corresponding output variable.\nThe goal for unsupervised learning is to model\nthe underlying structure\nor distribution in the data\nin order to learn more about the data.\nSo let me rephrase you this in simple terms\nin unsupervised learning approach the data instances\nof a training data set do not have\nan expected output Associated\nto them instead unsupervised\nlearning algorithm detects pattern based\non innate characteristics\nof the input data an example of machine learning tasks.\nAsk that applies unsupervised learning is clustering\nin this task similar data instances are grouped together\nin order to identify clusters of data in this slide.\nYou can see that initially we have different varieties\nof fruits as input.\nNow these set of fruits as input X are given to the model.\nNow, what is the model is trained using\nunsupervised learning algorithm.\nThe model will create clusters on the basis of its training.\nIt will grip the similar fruits and make their cluster.\nLet me make things more clearer to you.\nLet's take another example of it.\nSo in this Slide the image below shows an example\nof unsupervised learning process this algorithm processes\nan unlabeled training data set\nand based on the characteristics.\nIt grips the picture\ninto three different clusters of data despite the ability\nof grouping similar data into clusters.\nThe algorithm is not capable to add labels to the crow.\nThe algorithm only knows which data instances are similar,\nbut it cannot identify the meaning of this group.\nSo, Now you might be wondering why this category\nof machine learning is named as unsupervised learning.\nSo these are called as\nunsupervised learning because unlike supervised learning ever.\nThere are no correct answer\nand there is no teacher algorithms are left\non their own to discover\nand present the interesting structure in the data.\nLet's move on and see some\nof the popular unsupervised learning algorithm.\nSo we have here k-means apriori algorithm\nand hierarchical clustering now,\nlet's move on and see some of the examples\nof Is learning suppose a friend invites you to his party\nand where you meet totally strangers.\nNow, you will classify them using unsupervised learning\nas you don't have any prior knowledge about them\nand this classification can be done on the basis\nof gender age group dressing education qualification\nor whatever way you might like now why\nthis learning is different from supervised learning\nsince you didn't use any pasta prior knowledge\nabout the people you kept on classifying them on the go\nas they kept on coming you kept on classifying them.\nYeah, this category of people belong to this group\nthis category of people belong to that group and so on.\nOkay, let's see one more example.\nLet's suppose you have never seen a football match before\nand by chance you watch a video on the internet.\nNow, you can easily classify the players on the basis\nof different Criterion,\nlike player wearing the same kind of Jersey are\nin one class player wearing different kind\nof Jersey aren't different class\nor you can classify them on the basis\nof their playing style like the guys are attacker.\nSo he's in one class.\nHe's a Defender he's Another class\nor you can classify them.\nWhatever Way You observe the things\nso this was also an example of unsupervised learning.\nLet's move on and see\nhow unsupervised learning is used in the sectors\nof banking Healthcare undertale.\nSo starting at banking sector.\nSo in banking sector it is used to segment customers\nby behavioral characteristic by surveying prospects\nand customers to develop multiple segments\nusing clustering and Healthcare sector.\nIt is used to categorize the MRI data by normal or abnormal.\nAges it uses deep learning techniques to build a model\nthat learns from different features of images to recognize\na different pattern.\nNext is the retail sector and Retail sector.\nIt is used to recommend the products to customer\nbased on their past purchases.\nIt does this by building a collaborative filtering model\nbased on the past purchases by them.\nI assume you guys\nnow have a proper idea of what unsupervised learning means\nif you have any slightest doubt\ndon't hesitate and add your doubt to the I'm in section.\nSo let's discuss the third\nand the last type of machine learning\nthat is reinforcement learning.\nSo what is reinforcement learning?\nWell reinforcement learning\nis a type of machine learning algorithm\nwhich allows software agents\nand machine to automatically determine the ideal Behavior\nwithin a specific context to maximize its performance.\nThe reinforcement learning is about interaction\nbetween two elements\nthe environment and the learning agent\nthe learning agent leverages to mechanism namely exploration.\nAnd exploitation when learning agent acts on trial\nand error basis,\nit is termed as exploration\nand when it acts based on the knowledge gained\nfrom the environment,\nit is referred to as exploitation.\nNow this environment rewards the agent for correct actions,\nwhich is reinforcement signal leveraging the rewards\nobtain the agent\nimproves its environment knowledge to select\nthe next action in this image.\nYou can see that the machine is confused\nwhether it is an apple or it's not an apple\nthen the Sheena's chain using reinforcement learning.\nIf it makes correct decision.\nIt get rewards point for it\nand in case of wrong it gets a penalty for that.\nOnce the training is done.\nNow.\nThe machine can easily identify which one of them is an apple.\nLet's see an example here.\nWe can see that we have an agent\nwho has to judge from the environment to find out\nwhich of the two is a duck the first task\nhe did is to observe the environment next.\nWe select some action using some policy.\nIt seems that the machine has made a wrong decision.\nBye.\nChoosing a bunny as a duck.\nSo the machine will get penalty for it.\nFor example - 50.4 a wrong answer right now.\nThe machine will update its policy\nand this will continue\ntill the machine gets an optimal policy\nfrom the next time machine will know that bunny is not a duck.\nLet's see some of the use cases of reinforcement learning\nbut before that lets see\nhow Pavlo trained his dog using reinforcement learning\nor how he applied\nthe reinforcement method to train his dog.\nBabu integrated learning\nin four stages initially Pavlo gave me to his dog\nand in response to the meet the dog started salivating next\nwhat he did he created a sound\nwith the bell for this the dog did not respond anything\nin the third part it tried to condition the dog\nby using the bell\nand then giving him the food seeing the food\nthe dog started salivating eventually a situation came\nwhen the dog started salivating just after hearing the Bell even\nif the food was not given to him as the The dog was reinforced\nthat whenever the master will ring the bell he\nwill get the food now.\nLet's move on and see\nhow reinforcement learning is applied in the field\nof banking Healthcare and Retail sector.\nSo starting with the banking sector\nin banking sector reinforcement learning is used to create\na next best offer model\nfor a call center by building a predictive model\nthat learns over time\nas user accept or reject offer made by the sales staff fine now\nin healthcare sector it is used to allocate the scars.\nResources to handle different type of er\ncases by building a Markov decision process\nthat learns treatment strategies for each type of er case next\nand the last comes in retail sector.\nSo let's see\nhow reinforcement learning is applied to retail sector\nand Retail sector.\nIt can be used to reduce excess stock\nwith Dynamic pricing by building a dynamic pricing model\nthat are just the price based\non customer response to the offers.\nI hope by now you have attained some understanding of\nwhat is machine learning and you are ready to move.\nMove ahead.\nWelcome to today's topic of discussion on AI\nversus machine learning versus deep learning.\nThese are the term which have confused a lot\nof people and if you two are one among them,\nlet me resolve it for you.\nWell artificial intelligence is a broader umbrella\nunder which machine learning\nand deep learning come you can also see in the diagram\nthat even deep learning\nis a subset of machine learning so you can say\nthat all three of them The AI and machine learning\nand deep learning are just the subset of each other.\nSo let's move on and understand\nhow exactly the differ from each other.\nSo let's start with artificial intelligence.\nThe term artificial intelligence\nwas first coined in the year 1956.\nThe concept is pretty old,\nbut it has gained its popularity recently.\nBut why well,\nthe reason is earlier we had very small amount of data\nthe data we had was not enough to predict the Turret result\nbut now there's a tremendous increase\nin the amount of data statistics\nsuggest that by 2020 the accumulated volume\nof data will increase\nfrom 4.4 zettabyte stew roughly around 44 zettabytes\nor 44 trillion jeebies\nof data along with such enormous amount of data.\nNow, we have more advanced algorithm\nand high-end computing power and storage\nthat can deal with such large amount of data as a result.\nIt is expected\nthat 70% of The price will Implement a i\nover the next 12 months\nwhich is up from 40 percent in 2016 and 51 percent in 2017.\nJust for your understanding.\nWhat does AI well,\nit's nothing but a technique\nthat enables the machine to act like humans\nby replicating the behavior and nature with AI\nit is possible\nfor machine to learn from the experience.\nThe machines are just their responses based\non new input there\nby performing human-like tasks artificial intelligence can be\nand to accomplish\nspecific tasks by processing large amount of data\nand recognizing pattern in them.\nYou can consider\nthat building an artificial intelligence is like Building\na Church the first church took generations to finish.\nSo most of the workers were working in it never saw\nthe final outcome those working\non it took pride in their craft building bricks\nand chiseling stone\nthat was going to be placed into the great structure.\nSo as AI researchers,\nwe should think of ourselves as humble brick makers was job.\nIt's just study\nhow to build components example Parts is planners\nor learning algorithm or Etc anything\nthat someday someone and somewhere will integrate\ninto the intelligent systems some of the examples\nof artificial intelligence from our day-to-day life\nare Apple series chess-playing computer Tesla self-driving car\nand many more these examples are based on deep learning\nand natural language processing.\nWell, this was about what is AI and how it gains its hype.\nSo moving on ahead.\nLet's Gus about machine learning and see what it is\nand why it was the when introduced well\nMachine learning came into existence in the late 80s\nand the early 90s,\nbut what were the issues with the people\nwhich made the machine learning come into existence let\nus discuss them one by one in the field of Statistics.\nThe problem was\nhow to efficiently train large complex model in the field\nof computer science and artificial intelligence.\nThe problem was how to train more robust version\nof AI system while in the case of Neuroscience.\nProblem faced by the researchers was\nhow to design operation model of the brain.\nSo these were some of the issues\nwhich had the largest influence and led to the existence\nof the machine learning.\nNow this machine learning shifted its focus\nfrom the symbolic approaches.\nIt had inherited from the AI and move\ntowards the methods and model.\nIt had borrowed from statistics and probability Theory.\nSo let's proceed and see\nwhat exactly is machine learning.\nWell Machine learning is a subset of AI\nwhich enables the computer to act\nand make data-driven decisions to carry out a certain task.\nThese programs are algorithms are designed in a way\nthat they can learn and improve over time\nwhen exposed to new data.\nLet's see an example of machine learning.\nLet's say you want to create a system\nwhich tells the expected weight of a person based on its side.\nThe first thing you do is you collect the data.\nLet's see there is\nhow your data looks like now each point\non the graph represent one data point to start\nwith we can draw a simple line to predict the weight based\non the height for Sample a simple line W equal x\nminus hundred with W is waiting kgs and edges hide\nand centimeter this line can help us to make the prediction.\nOur main goal is to reduce the difference\nbetween the estimated value and the actual value.\nSo in order to achieve it,\nwe try to draw a straight line that fits through all\nthese different points and minimize the error.\nSo our main goal is to minimize the error\nand make them as small as possible decreasing the error\nor the difference between the actual value and estimated.\nValue increases the performance of the model further\non the more data points.\nWe collect the better.\nOur model will become we can also improve our model\nby adding more variables\nand creating different production lines for them.\nOnce the line is created.\nSo from the next time if we feed a new data,\nfor example height of a person to the model,\nit would easily predict the data for you and it will tell you\nwhat has predicted weight could be.\nI hope you got a clear understanding\nof machine learning.\nSo moving on ahead.\nLet's learn about deep learning now what is deep learning?\nYou can consider deep learning model as a rocket engine\nand its fuel is its huge amount of data\nthat we feed to these algorithms the concept\nof deep learning is not new,\nbut recently it's hype as increase\nand deep learning is getting more attention.\nThis field is a particular kind of machine learning\nthat is inspired by\nthe functionality of our brain cells called neurons\nwhich led to the concept of artificial neural network.\nIt simply takes\nthe data connection between all the artificial neurons\nand adjust them according to the data pattern.\nMore neurons are added\nat the size of the data is large it automatically features\nlearning at multiple levels of abstraction.\nThereby allowing a system\nto learn complex function mapping without depending\non any specific algorithm.\nYou know, what no one actually knows what happens\ninside a neural network and why it works so well,\nso currently you can call it as a black box.\nLet us discuss some of the example of deep learning\nand understand it in a better way.\nLet me start with in simple example\nand explain you how things And at a conceptual level,\nlet us try and understand\nhow you would recognize a square from other shapes.\nThe first thing you do is you check\nwhether there are four lines associated with a figure\nor not simple concept, right?\nIf yes, we further check\nif they are connected and closed again a few years.\nWe finally check whether it is perpendicular\nand all its sides are equal, correct.\nIf everything fulfills.\nYes, it is a square.\nWell, it is nothing but a nested hierarchy of Concepts.\nWhat we did here we took a complex task\nof identifying a square\nand this case and broken into simpler tasks.\nNow this deep learning also does the same thing\nbut at a larger scale,\nlet's take an example of machine which recognizes\nthe animal the task of the machine is to recognize\nwhether the given image is of a cat or a dog.\nWhat if we were asked to resolve the same issue using the concept\nof machine learning what we would do first.\nWe would Define the features such as\ncheck whether the animal has whiskers or not a check.\nThe animal has pointed ears\nor not or whether its tail is straight or curved in short.\nWe will Define the facial features and let\nthe system identify which features are more important\nin classifying a particular animal now\nwhen it comes to deep learning it takes this to one step ahead\ndeep learning automatically finds are the feature\nwhich are most important for classification compare\ninto machine learning\nwhere we had to manually give out that features by now.\nI guess you have understood that AI is the bigger picture\nand machine learning and deep learning are it's apart.\nSo let's move on\nand focus our discussion on machine learning\nand deep learning the easiest way to understand the difference\nbetween the machine learning and deep learning is to know\nthat deep learning is machine learning more specifically.\nIt is the next evolution of machine learning.\nLet's take few important parameter\nand compare machine learning with deep learning.\nSo starting with data dependencies,\nthe most important difference between deep learning\nand machine learning is its performance as the volume\nof the data gets From the below graph.\nYou can see\nthat when the size of the data is small deep learning algorithm\ndoesn't perform that well,\nbut why well,\nthis is because deep learning algorithm needs\na large amount of data to understand it perfectly\non the other hand the machine learning algorithm can easily\nwork with smaller data set fine.\nNext comes the hardware dependencies deep learning\nalgorithms are heavily dependent on high-end machines\nwhile the machine learning algorithm can work\non low and machines as Well,\nthis is because the requirement\nof deep learning algorithm include gpus\nwhich is an integral part\nof its working the Deep learning algorithm requires gpus\nas they do a large\namount of matrix multiplication operations,\nand these operations\ncan only be efficiently optimized using a GPU\nas it is built for this purpose.\nOnly our third parameter\nwill be feature engineering well feature engineering is a process\nof putting the domain knowledge to reduce the complexity\nof the data.\nMake patterns more visible to learning algorithms.\nThis process is difficult and expensive in terms of time\nand expertise in case of machine learning\nmost other features are needed to be identified by an expert\nand then hand coded as per the domain\nand the data type.\nFor example, the features\ncan be a pixel value shapes texture position orientation\nor anything fine the performance\nof most of the machine learning algorithm depends\non how accurately the features are identified and stood\nwhere as in case\nof deep learning algorithms it try to learn high level features\nfrom the data.\nThis is a very distinctive part of deep learning\nwhich makes it way ahead\nof traditional machine learning deep learning reduces the task\nof developing new feature extractor for every problem\nlike in the case\nof CNN algorithm it first try to learn the low-level features\nof the image such as edges and lines\nand then it proceeds to the parts of faces of people\nand then finally to the high-level representation\nof the face.\nI hope that things Getting clearer to you.\nSo let's move on ahead and see the next parameter.\nSo our next parameter is problem solving approach\nwhen we are solving a problem using traditional machine\nlearning algorithm.\nIt is generally recommended\nthat we first break down the problem\ninto different sub parts solve them individually\nand then finally combine them to get the desired result.\nThis is how the machine learning algorithm handles the problem\non the other hand the Deep learning algorithm\nsolves the problem from end to end.\nLet's take an example.\nTo understand this suppose you have a task\nof multiple object detection.\nAnd your task is to identify.\nWhat is the object and where it is present in the image.\nSo, let's see and compare.\nHow will you tackle this issue using the concept\nof machine learning\nand deep learning starting with machine learning\nin a typical machine learning approach.\nYou would first divide the problem into two step\nfirst object detection and then object recognization.\nFirst of all,\nyou would use a bounding box detection algorithm\nlike grab could fight.\nSample to scan through the image\nand find out all the possible objects.\nNow, once the objects are recognized you would use\nobject recognization algorithm,\nlike svm with hog to recognize relevant objects.\nNow, finally,\nwhen you combine the result you would be able to identify.\nWhat is the object and where it is present\nin the image on the other hand in deep learning approach.\nYou would do the process from end to end for example\nin a yellow net\nwhich is a type of deep learning algorithm you would pass.\nAn image and it would give out the location along with the name\nof the object.\nNow, let's move on to our fifth comparison parameter\nits execution time.\nUsually a deep learning algorithm takes a long time\nto train this is\nbecause there's so\nmany parameter in a deep learning algorithm\nthat makes the training longer\nthan usual the training might even last for two weeks\nor more than that.\nIf you are training completely from the scratch,\nwhereas in the case of machine learning,\nit relatively takes much less time to train ranging\nfrom a few weeks.\nToo few Arts.\nNow.\nThe execution time is completely reversed\nwhen it comes to the testing of data during testing\nthe Deep learning algorithm takes much less time to run.\nWhereas if you compare it with a KNN algorithm,\nwhich is a type of machine learning algorithm the test\ntime increases as the size of the data increase last\nbut not the least we have interpretability as\na factor for comparison of machine learning\nand deep learning.\nThis fact is the main reason why deep learning is still\nthought ten times before anyone knew.\nUses it in the industry.\nLet's take an example suppose.\nWe use deep learning to give\nautomated scoring two essays the performance it gives\nand scoring is quite excellent and is near\nto the human performance,\nbut there's an issue with it.\nIt does not reveal white\nhas given that score indeed mathematically.\nIt is possible to find out\nthat which node of a deep neural network were activated,\nbut we don't know\nwhat the neurons are supposed to model\nand what these layers of neurons are doing collectively.\nSo if To interpret the result\non the other hand machine learning algorithm,\nlike decision tree gives us a crisp rule for void chose\nand watered chose.\nSo it is particularly easy to interpret the reasoning\nbehind therefore the algorithms like decision tree\nand linear or logistic\nregression are primarily used in industry for interpretability.\nLet me summarize things\nfor you machine learning uses algorithm to parse\nthe data learn from the data\nand make informed decision based on what it has learned fine.\nin this deep learning structures algorithms in layers to create\nartificial neural network\nthat can learn\nand make Intelligent Decisions on their own finally\ndeep learning is a subfield of machine learning\nwhile both fall under the broad category\nof artificial intelligence deep learning is usually\nwhat's behind the most\nhuman-like artificial intelligence now\nin early days scientists used to have a lab notebook\nto Test progress results\nand conclusions now Jupiter is a modern-day\nto that allows data scientists\nto record the complete analysis process much\nin the same way other scientists use a lab notebook.\nNow, the Jupiter product was originally developed as a part\nof IPython project the iPad\nand project was used to provide interactive online access\nto python over time.\nIt became useful to interact\nwith other data analysis tools such as are in the same manner\nwith the split from python\nthe tool crew in in his current manifestation of Jupiter.\nNow IPython is still an active tool\nthat's available for use.\nThe name Jupiter itself is derived from the combination\nof Julia Python.\nAnd our while Jupiter runs code\nin many programming languages python is a requirement\nfor installing the jupyter notebook itself now\nto download jupyter notebook.\nThere are a few ways in their official website.\nIt is strongly recommended installing Python and Jupiter\nusing Anaconda distribution,\nwhich includes python Don't know what book\nand other commonly used packages\nfor scientific Computing as well as data science.\nAlthough one can also\ndo so using the pipe installation method personally.\nWhat I would suggest is downloading an app\non a navigator, which is\na desktop graphical user interface included in Anaconda.\nNow, this allows you to launch application\nand easily manage conda packages environments\nand channels without the need to use command line commands.\nSo all you need to do is go to another Corner dot orgy\nand inside you go.\nTo Anaconda Navigators.\nSo as you can see here,\nwe have the conda installation code which you're going\nto use to install it in your particular PC.\nSo either you can use these installers.\nSo once you download the Anaconda Navigator,\nit looks something like this.\nSo as you can see here,\nwe have Jupiter lab jupyter notebook you have QT console,\nwhich is IPython console.\nWe have spider which is somewhat similar to a studio\nin terms of python again,\nwe have a studio so we have orange three\nWe have glue is and we have VSC code.\nOur Focus today would be on this jupyter notebook itself.\nNow when you launch the Navigator,\nyou can see there are many options available\nfor launching python as well.\nAs our instances Now by definition are jupyter.\nNotebook is fundamentally\na Json file with a number of annotations.\nNow, it has three main parts\nwhich are the metadata The Notebook format and the list\nof cells now you\nshould get yourself acquainted with the environment\nthat Jupiter user interface has a number of components.\nSo it's important to know\nwhat our components you should be using\non a daily basis and you should get acquainted with it.\nSo as you can see here\nour Focus today will be on the jupyter notebook.\nSo let me just launched the Japan and notebook.\nNow what it does is creates a online python instance\nfor you to use it over the web.\nSo let's launch now\nas you can see we have Jupiter on the top left\nas expected and this acts as a button to go\nto your home page whenever you click\non this you get back to your particular home paste.\nIs the dashboard now there are three tabs displayed\nwith other files running and clusters.\nNow, what will do is will understand all\nof these three and understand\nwhat are the importance\nof these three tabs other file tab shows the list\nof the current files in the directory.\nSo as you can see we have so many files here.\nNow the running tab\npresents another screen of the currently running processes\nand the notebooks now the drop-down list for the terminals\nand notebooks are populated with there.\nRunning numbers.\nSo as you can see inside,\nwe do not have any running terminals\nor there no running notebooks as of now\nand the cluster tab\npresents another screen to display the list\nof clusters available see in the top right corner of the screen.\nThere are three buttons which are upload new\nand the refresh button.\nLet me go back so you can see here.\nWe have the upload new and the refresh button.\nNow the upload button is used to add files\nto The Notebook space and you may also just drag and drop\nas you would when handling files.\nSimilarly, you can drag\nand drop notebooks into specific folders as well.\nNow the menu with the new in the top residents\nof further many of text file folders terminal\nand Python 3.\nNow, the test file option is used to add a text file\nto the current directory Jupiter will open a new browser window\nfor you for the running new text editor.\nNow, the text entered is automatically saved\nand will be displayed in your notebooks files display.\nNow the folder option\nwhat it does is creates a new folder.\nWith the name Untitled folder and remember all the files\nand folder names are editable.\nNow the terminal option allows you to start\nand IPython session.\nThe node would options available will be activated\nwhen additional note books are available in your environment.\nThe Python 3 option is used to begin pythons recession\ninteractively in your note.\nThe interface looks like the following screen shot.\nNow what you have is full file editing capabilities\nfor your script including saving as new file.\nYou also have a complete ID\nfor your python script now we come to the refresh button.\nThe refresh button is used to update the display.\nIt's not really necessary as a display is reactive\nto any changes in the underlying file structure.\nI had a talk with the files tab item.\nThere is a check box drop\ndown menu and a home button as you can see here.\nWe have the checkbox the drop-down menu\nand the home button.\nNow the check box is used to toggle all the checkboxes\nin the item list.\nSo as you can see you can select all of these when either move\nor either delete all of the file selected,\nIt or what you can do is select all\nand deselect some of the files\nas your wish now the drop down menu presents a list\nof choices available,\nwhich are the folders all notebooks running\nand files to the folder section\nwill select all the folders in the display\nand present account of the folders in the small box.\nSo as you can see here,\nwe have 18 number of folders now all the notebooks section\nwill change the count of the number of nodes\nand provide you with three option\nso you can see here.\nIt has selected all the given notebooks\nwhich are a In a number\nand you get the option to either duplicate the current notebook.\nYou need to move it view it edit it or delete.\nNow, the writing section will select any running scripts\nas you can see here.\nWe have zero running scripts\nand update the count to the number selected.\nNow the file section will select all the files\nin the notebook display and update the counts accordingly.\nSo if you select the files here,\nwe are seven files as you can see here.\nWe have seven files some datasets CSV files\nand text files now the home button.\nBrings you back to the home screen of the notebook.\nSo on you to do is click on the jupyter.\nNotebook lower.\nIt will bring you back to the Jupiter notebook dashboard.\nNow, as you can see on the left hand side\nof every item is a checkbox and I can and the items name.\nThe checkbox is used to build a set of files to operate\nupon and the icon is indicated of of the type of the item.\nAnd in this case,\nall of the items are folder here coming down.\nWe have the ring notebooks.\nAnd finally we have certain files which are the text files\nand the As we files now a typical workflow\nof any jupyter.\nNotebook is to first of all create a notebook\nfor the project or your data analysis.\nAdd your analysis step coding\nand output and Surround your analysis with organization\nand presentation mark down to communicate\nand entire story now interactive notebooks\nthat include widgets and display modules\nwill then be used by others by modifying parameters\nand the data to note the effects of the changes now\nif we talk about security jupyter notebooks are created\nin order to Be shared with other users in many cases\nover the Internet.\nHowever, jupyter notebook can execute arbitrary code\nand generate arbitrary code.\nThis can be a problem.\nIf malicious aspects have been placed\nin the note Now the default security mechanism for Japan\nor notebooks include raw HTML,\nwhich is always sanitized and check for malicious coding.\nAnother aspect is you cannot run external Java scripts.\nNow the cell contents,\nespecially the HTML and the JavaScript are not trusted\nit requires user value.\nNation to continue\nand the output from any cell is not trusted all other HTML\nor JavaScript is never trusted\nand clearing the output will cause the notebook\nto become trusted\nwhen save now notebooks can also use a security digest\nto ensure the correct user is modifying the contents.\nSo for that what you need to do is a digest\nwhat it does is takes into the account\nthe entire contents of the notebook and a secret\nwhich is only known by The Notebook Creator\nand this combination ensures\nthat malicious coding is is not going to be added\nto the notebook\nso you can add security to address to notebook\nusing the following command which I have given here.\nSo it's Jupiter the profile\nwhat you have selected and inside you\nwhat you need to do is security and notebook secret.\nSo what you can do is replace the notebooks\nsecret with your putter secret\nand that will act as a key for the particular notebook.\nSo what you need to do is share that particular key\nwith all your colleagues\nor whoever you want to share that particular notebook\nwith and in that case,\nit keeps the notebooks.\nGeode and away from other malicious coders\nand all other aspect of Jupiter is configuration.\nSo you can configure some of the display parameters used\nand presenting notebooks.\nNow, these aren't configurable due to the use of product known\nas code mirror to present and modify the notebook.\nSo cold mirror water basically is it is a JavaScript\nbased editor for the u.s.\nWithin the web pages and notebooks.\nSo what you do is what you do code mirror,\nso as you can see here\ncode mirror is a versatile text editor implemented.\nIn JavaScript for the browser.\nSo what it does is\nallow you to configure the options for Jupiter.\nSo now let's execute some python code\nand understand the notebook in A Better Way Jupiter\ndoes not interact\nwith your scripts as much as it executes your script\nand request the result.\nSo I think this is how jupyter notebooks\nhave been extended to other languages besides python\nas it just takes\na script runs it against a particular language engine\nand across the output from the engine all\nthe while not Really knowing what kind\nof a script is being executed now the new windows shows\nand empty cell\nfor you to enter the python code know\nwhat you need to do is under new you select the Python 3 and\nwhat I will do is open a new notebook.\nNow this notebook is Untitled.\nSo let's give the new work area and name python code.\nSo as you can see we have renamed this particular cell\nnow order save option should be on the next to the title\nas you can see last.\nCheckpoint a few days ago, unsaved changes.\nThe autosave option is always on what we do is\nwith an accurate name.\nWe can find the selection\nand this particular notebook very easily\nfrom The Notebook home page.\nSo if you select your browser's Home tab\nand refresh you will find\nthis new window name displayed here again.\nSo if you just go to a notebook home\nand as you can see,\nI mentioned it by then quotes and under running.\nAlso, you have the pilot and quotes here.\nSo let's get back to the Particular page\nor the notebook one thing to note here\nthat it has\nand does an item icon versus a folder icon\nthough automatically assigned extension\nas you can see here is ipy and be the IPython note and says\nthe item is in a browser in a Jupiter environment.\nIt is marked as running answer\nis a file by that name in this directory as well.\nSo if you go to your directory,\nlet me go and check it.\nSo as you can see if you go into the users are you\ncan see we have the in class projects\nthat Python codes like the series automatically\nhave that particular IPython notebook created\nin our working environment\nand the local disk space also.\nSo if you open the IP y + B file in a text editor,\nyou will see basic context of a Jupiter code as you can see\nif I'm opening it.\nThe cells are empty.\nNothing is there so let's type in some code here.\nFor example, I'm going to put in name equals edgy Rekha.\nNext what I'm going to do is provide subscribers\nthat equals seven hundred gay and to run this particular cell.\nWhat you need to do is click on the run Icon\nand it will see here we have one.\nSo this is the first set to be executed in the second cell.\nWe enter python code\nthat references the variables from the first cell.\nSo as you can see here,\nwe have friend named has strings subscribers.\nSo let me just run this particular.\nSo as you can see here note.\nNow that we have an output here\nthat Erica has 700k YouTube subscriber now\nsince more than 700 K now to know more about Jupiter\nand other Technologies,\nwhat you can do is subscribe to our Channel and get\nupdates on the latest trending Technologies.\nSo note that Jupiter color codes\nyour python just as decent editor vote\nand we have empty braces\nto the left of each code block such as you can see here.\nIf we execute the cell\nthe results are displayed in line now, it's interesting\nthat Jupiter keeps.\nThe output last generated in the saved version of the file\nand it's a save checkpoints.\nNow, if we were to rerun your cells using the rerun\nor the run all the output would be generated\nand c8y autosave now,\nthe cell number is incremented and as you can see\nif I rerun this you see the cell number change\nfrom one to three\nand if I rerun this the Selma will change from 2 to 4.\nSo what Jupiter does is keeps a track of the latest version\nof each cell so similarly\nif you are to close the browser tab It's the display\nin the Home tab.\nYou will find a new item we created\nwhich is the python code your notebook saved autosaved\nas you can see here in the bracket has autosaved.\nSo if we close this in the home button,\nyou can see here.\nWe have python codes.\nSo as you can see if we click that it opens the same notebook.\nIt has the previously\ndisplayed items will be always there showing the output sweat\nthat we generated in the last run now\nthat we have seen how python Works\nin Jupiter including the underlying encoding then\nhow this python.\nThis allows data set or data set Works in Jupiter.\nSo let me create another new python notebook.\nSo what I'm going to do is name this as pandas.\nSo from here,\nwhat we will do is read in last dataset\nand compute some standard statistics of data.\nNow what we are interested in in seeing\nhow to use the pandas in Jupiter\nhow well the script performs\nand what information is stored in the metadata,\nespecially if it's a large dataset\nso our Python script accesses the iris dataset here\nthat's built into one of the Python packages.\nNow.\nAll we are looking in to do is to read in slightly large number\nof items and calculate some basic operations\non the data set.\nSo first of all,\nwhat we need to do is\nfrom sklearn import the data set so sklearn\nis scikit-learn and it is another library of python.\nIt contains a lot of data sets for machine learning\nand all the algorithms\nwhich are present for machine learning\nand the data sets which are there so,\nSo import was successful.\nSo what we're going to do now is pull in the IRS data.\nWhat we're going to do is Iris underscore data set equals\nand the load on the screen now that should do and I'm sorry,\nit's data set start lower.\nSo so as you can see here,\nthe number here is considered three now\nbecause in the second drawer\nand we encountered an error it was data set.\nHe's not data set.\nSo so what we're going\nto do is grab the first two corner of the data.\nSo let's pretend x equals.\nIf you press the tab, it automatically detects\nwhat you're going to write as Todd datasets dot data.\nAnd what we're going to do is take the first two rows comma\nnot to run it from your keyboard.\nAll you need to do is press shift + enter.\nSo next what we're going to do is calculate\nsome basic statistics.\nSo what we're going to do is X underscore.\nCount equals x I'm going to use the length function and said\nthat we're going to use x dot flat similarly.\nWe going to see X-Men\nand X Max and the Min our display our results.\nWhat we're going to do is you just play the results now, so\nas you can see the counter 300 the minimum value is 3.8 m/s.\nAnd what is 0.4\nand the mean is five point eight four three three three.\nSo let me connect you to the real life\nand tell you what all are the things\nwhich you can easily do using the concepts of machine learning\nso you can easily get answer to the questions\nlike which types of house lies in this segment\nor what is the market value of this house or is this\na male as spam or not spam?\nIs there any fraud?\nWell, these are some of the question you could ask\nto the machine\nbut for getting an answer to these you need some algorithm\nthe machine need to train on the basis of some algorithm.\nOkay, but how will you decide which algorithm\nto choose and when?\nOkay.\nSo the best option for us is to explore them one by one.\nSo the first is classification algorithm\nwhere the categories predicted using the data\nif you have some question,\nlike is this person a male\nor a female or is this male a Spam or not?\nSpam then these category of question would fall\nunder the classification algorithm classification is\na supervised learning approach\nin which the computer program learns from the input\ngiven to it\nand then uses\nthis learning to classify new observation some examples\nof classification problems\nare speech organization handwriting recognized.\nShouldn't biometric identification document\nclassification Etc.\nSo next is the anomaly detection algorithm\nwhere you identify the unusual data point.\nSo what is an anomaly detection.\nWell, it's a technique\nthat is used to identify unusual pattern\nthat does not conform to expected Behavior\nor you can say the outliers.\nIt has many application\nin business like intrusion detection,\nlike identifying strange patterns in the network traffic\nthat could signal a hack or system Health monitoring\nthat is sporting a deadly tumor in the MRI scan\nor you can even use it\nfor fraud detection credit card transaction\nor to deal with fault detection in operating environment.\nSo next comes the clustering algorithm,\nyou can use this clustering algorithm to group the data\nbased on some similar condition.\nNow you can get answer to which type of houses lies\nin this segment or what type of customer buys this product.\nThe clustering is a task of dividing the population\nor data points into number of groups such\nthat the data point and the same groups are more.\nHello to other data points in the same group than those\nin the other groups in simple words.\nThe aim is to segregate groups with similar trait\nand assigning them into cluster.\nNow this clustering is a task of dividing the population\nor data points into a number of groups such\nthat the data points\nin the X group is more similar to the other data points\nin the same group rather than those in the other group.\nIn other words.\nThe aim is to segregate the groups with similar traits\nand assigning them into different clusters.\nLet's understand this\nwith an example Suppose you are the head of a rental store\nand you wish to understand the preference of your customer\nto scale up your business.\nSo is it possible\nfor you to look at the detail of each customer and design\na unique business strategy for each of them?\nDefinitely not right?\nBut what you can do is to Cluster all your customer saying\nto 10 different groups based on their purchasing habit\nand you can use a separate strategy\nfor customers in each of these ten different groups.\nAnd this is what we call clustering.\nNext we have regression algorithm where the data\nitself is predicted question.\nYou may ask to this type of model is like what is\nthe market value of this house\nor is it going to rain tomorrow or not?\nSo regression is one of the most important and broadly\nused machine learning and statistics tool.\nIt allows you to make prediction from data by learning\nthe relationship between the features of your data\nand some observe continuous valued response regulation\nis used in a massive number of application.\nYou know, what stock Isis prediction can be done\nusing regression now,\nyou know about different machine learning algorithm.\nHow will you decide which algorithm to choose\nand when so let's cover this part using a demo.\nSo in this demo part\nwhat we will do will create six different machine learning model\nand pick the best model and build the confidence such\nthat it has the most reliable accuracy.\nSo far our demo part will be using the IRS data set.\nThis data set is quite very famous\nand is considered one of the best small project to start\nwith you can consider\nthis as a hello world data set for machine learning.\nSo this data set consists\nof 150 observation of Iris flower.\nTherefore Columns of measurement of flowers in centimeters\nthe fifth column being the species\nof the flower observe all the observed flowers belong\nto one of the three species of Iris setosa Iris virginica\nand Iris versicolor.\nWell, this is a good good project\nbecause it is so well to understand\nthe attributes are numeric.\nSo you have to figure out how to load and handle the data.\nIt is a classification problem.\nThereby allowing you to practice\nwith perhaps an easier type of supervised learning algorithm.\nIt has only four attributes and 150 rose.\nMeaning it is very small\nand can easily fit into the memory and even all\nof the numeric attributes are in same unit\nand the same scale means you do not require any special scaling\nor transformation to get started.\nSo let's start coding and as I told earlier for the\nBut I'll be using Anaconda with python 3.0 install on it.\nSo when you install Anaconda\nhow your Navigator would look like.\nSo there's my home page of my anaconda navigator on this.\nI'll be using the jupyter notebook,\nwhich is a web-based interactive Computing notebook environment,\nwhich will help me to write and execute my python codes on it.\nSo let's hit the launch button and execute\nour jupyter notebook.\nSo as you can see\nthat my jupyter notebook is starting on localhost\ndouble eight nine zero.\nOkay, so there's my jupyter notebook\nwhat I'll do here.\nI'll select new.\nbook Python 3 Does my environment where I can write\nand execute all my python codes on it?\nSo let's start by checking the version\nof the libraries in order to make this video short\nand more interactive and more informative.\nI've already written the set of code.\nSo let me just copy and paste it down.\nI'll explain you then one by one.\nSo let's start\nby checking the version of the Python libraries.\nOkay, so there is the code let's just copy\nit copied and let's paste it.\nOkay first let\nme summarize things for you what we are doing here.\nWe are just checking the version\nof the different libraries starting\nwith python will first check what version\nof python we are working on then we'll check\nwhat are the version of sci-fi we are using\nthe numpy matplotlib then Panda then scikit-learn.\nOkay.\nSo let's execute the Run button and see\nwhat are the various versions of libraries\nwhich we are using it the run.\nSo we are working on Python 3 point 6 point 4 PSI by 1.0 now.\nBy 1.1 for matplotlib 2.12 pandas 0.22 and scikit-learn\nor version 0.19.\nOkay.\nSo these are the version\nwhich I'm using ideally your version should be more recent\nor it should match but don't worry\nif you lack a few versions behind\nas the API is do not change so quickly everything\nin this tutorial will very likely still work for you.\nOkay, but in case you\nare getting an error stop and try to fix that error\nin case you are unable to find the solution for the error,\nfeel free to reach out at Eureka even after the This class.\nLet me tell you this\nif you are not able to run the script properly,\nyou will not be able to complete this tutorial.\nOkay, so whenever you get a doubt reach out\nto a deal-breaker and just resolve it now,\neverything is working smoothly then now is the time\nto load the data set.\nSo as I said,\nI'll be using the iris flower data set for this tutorial\nbut before loading the data set,\nlet's import all the modules function and the object\nwhich we are going to use in this tutorial same\nI've already written the set of code.\nSo let's just copy and paste them.\nLet's load all the libraries.\nSo these are the various libraries\nwhich will be using in our tutorial.\nSo everything should work fine without an error.\nIf you get an error just stop you need to work\non your cyber environment before you continue any further.\nSo I guess everything should work fine.\nLet's hit the Run button and see.\nOkay, it worked.\nSo let's now move ahead and load the data.\nWe can load the data direct\nfrom the UCI machine learning repository.\nFirst of all,\nlet me tell you we are using Panda to load the data.\nOkay.\nSo let's say my URL.\nIs this so This is\nMy URL for the use your machine learning repository\nfrom where I will be downloading the data set.\nOkay.\nNow what I'll do,\nI'll specify the name of each column\nwhen loading the data.\nThis will help me later to explore the data.\nOkay, so I'll just copy and paste it down.\nOkay, so I'm defining a variable names\nwhich consists of various parameters\nincluding sepal length sepal width petal length battle\nwith and class.\nSo these are just the name of column from the data set.\nOkay.\nNow let's define the data set.\nSo data set equals Panda dot read underscore CSV inside\nthat we are defining URL and the names\nthat is equal to name.\nAs I already said we'll be using Panda to load the data.\nAlright, so we are using Panda dot read CSV,\nso we are reading.\nThe CSV file and inside that\nfrom where that CSV is coming from the URL which you are.\nSo there's my URL.\nOkay name sequel names.\nIt's just specifying the names of the various columns\nin that particular CSV file.\nOkay.\nSo let's move forward and execute it.\nSo even our data set is loaded.\nIn case you have some network issues just go ahead\nand download the iris data file into your working directory\nand loaded using the same method but your make sure\nthat you change the url to the local name\nor else you might get an error.\nOkay.\nYeah, our data set is loaded.\nSo let's move ahead and check out data set.\nLet's see how many columns or rows we have in our data set.\nOkay.\nSo let's print the number\nof rows and columns in our data set.\nSo our data set is data set dot shape\nwhat this will do.\nIt will just give you the numbers of total number\nof rows and 2.\nLittle more of column or you can say the total number\nof instances are attributes in your data set fine.\nSo print data set dot shape audio getting 150 and 500.\nSo 150 is the total number of rows in your data set\nand five is the total number of columns fine.\nSo moving on ahead.\nWhat if I want to see the sample data set?\nOkay.\nSo let me just print the first certain instances\nof the data set.\nOkay, so print data set.\nHead.\nWhat I want is the first 30 instances fine.\nThis will give me the first 30 result of my data set.\nOkay.\nSo when I hit the Run button\nwhat I am getting is the first 30 result,\nokay 0 to 29.\nSo this is\nhow my sample data set looks like sepal length sepal\nwidth petal and petal width and the class, okay.\nSo this is how our data set looks like now,\nlet's move on\nand look at the summary of each attribute.\nWhat if I want to find out the count mean the minimum\nand the maximum values and some other percentiles as well.\nSo what should I do then\nfor that print data set dot described.\nWhat did we give let's see.\nSo you can see\nthat all the numbers are the same scales of similar range\nbetween 0 to 8 centimeters,\nright the mean value the standard deviation\nthe minimum value the 25 percentile\n50 percentile 75 percentile\nthe maximum value all these values lies\nin the range between 0 to 8 centimeter.\nOkay.\nSo what we just did is we just took a summary\nof each attribute.\nNow, let's look at the number of instances\nthat belong to each class.\nSo for that what we'll do print data set.\nFirst of all,\nso let's print data set\nand I want to group it Group by using class\nand I want the size of it size of each class fine,\nand let's hit the Run.\nOkay.\nSo what I want to do,\nI want to print print out data set.\nHowever want to get it.\nI want it by class.\nSo Group by class.\nOkay.\nNow I want the size\nof each class find the size of each class.\nSo Group by class dot size and skewed the run\nso you can see\nthat I have 50 instances of Iris setosa 50 instances\nof Iris versicolor\nand 50 instances of Iris virginica.\nOkay, all our of data type integer of base64 fine.\nSo now we have a basic idea of Data, now,\nlet's move ahead and create some visualization for it.\nSo for this we are going to create two different types\nof plot first would be the univariate plot\nand the next would be the multivariate plot.\nSo we'll be creating univariate plots to better understand\nabout each attribute\nand the next will be creating the multivariate plot to better\nunderstand the relationship between different attributes.\nOkay.\nSo we start with some univariate plot\nthat is plot of each individual variable.\nSo given that the input variables are numeric\nwe can create box and whiskers plot for it.\nOkay.\nSo let's move ahead and create a box and whiskers plot\nso data set Dot Plot.\nWhat kind I want it's a box.\nOkay, I'm do I need a subplot?\nYeah, I need subplots for that.\nSo subplots equal to what type\nof layout do I won't so my layout structure is\n2 cross 2 next do I want to share my coordinates X\nand Y coordinates.\nNo, I don't want to share it.\nSo share x equal false\nand even share why that 2 equals false?\nOkay.\nSo we have our data set Dot Plot kind equal box.\nMy subplots is to lay out to Us too and then\nwhat I want to do it,\nI want to see so Plot show whatever I created short.\nOkay, execute it.\nNot just gives us a much clearer idea\nabout the distribution of the input attribute.\nNow what if I had given the layout to 2 cross 2 instead\nof that I would have given it for cross for so\nwhat it will result just see fine.\nEverything would be printed in just one single row.\nHold on guys area is a doubt.\nHe's asking that why we're using the sheriff's\nand share y values.\nWhat are these why we have assigned false values to it?\nOkay Ariel.\nSo in order to resolve this query,\nI need to show you what will happen\nif I give True Values to them.\nOkay, so be with me so share its go.\nPull through and share why that equals true.\nSo let's see what result will get.\nYou're getting it the X\nand y-coordinates are just shared among all the\nfor visualization.\nRight?\nSo are you can see\nthat the sepal length and sepal width has\ny values ranging from zero point zero two seven point five\nwhich are being shared\namong both the visualization so is with the petal length.\nIt has shared value\nbetween zero point zero two seven point five.\nOkay, so that is why I don't want to share\nthe value of X and Y,\nso it's just giving us a cluttered visualization.\nSo Aria why I'm doing this.\nI'm just doing it\ncause I don't want my X and Y coordinates To be shared\namong any visualization.\nOkay.\nThat is why my share X and share by value are false.\nOkay, let's execute it.\nSo this is a pretty much Clear visualization\nwhich gives a clear idea about the distribution\nof the input attribute.\nNow if you want you can also create a histogram\nof each input variable to get a clear idea\nof the distribution.\nSo let's create a histogram for it.\nSo data set dot his okay.\nI would need to see it.\nSo plot dot show.\nLet's see.\nSo there's my histogram and it seems\nthat we have two input variables that have a go.\nAnd distribution so this is useful to note\nas we can use the algorithms\nthat can exploit this assumption.\nOkay.\nSo next comes the multivariate lat now\nthat we have created the univariate plot to understand\nabout each attribute.\nLet's move on and look at the multivariate plot and see\nthe interaction between the different variables.\nSo first, let's look at the scatter plot\nof all the attribute this can be helpful\nto spot structured relationship between input variables.\nOkay.\nSo let's create a scatter Matrix.\nSo for creating a scatter plot, we need scatter Matrix,\nand we need to pass our data set into It okay.\nAnd then what I want I want to see it.\nSo plot dot show.\nSo this is how my scatter Matrix looks\nlike it's like\nthat the diagonal grouping of some pear, right?\nSo this suggests a high correlation\nand a predictable relationship.\nAll right.\nThis was our multivariate plot.\nNow, let's move on and evaluate some algorithm\nthat's time to create some model of the data\nand estimate the accuracy on the basis of unseen data.\nOkay.\nSo now we know all about our data set, right?\nWe know how many instances\nand attributes are there in our data set.\nWe know the summary of each attribute.\nSo I guess we have seen much about our data set.\nNow.\nLet's move on and create some algorithm\nand estimate their accuracy based on the Unseen data.\nOkay.\nNow what we'll do we'll create some model of the data\nand estimate the accuracy based on the some unseen data.\nOkay.\nSo for that first of all,\nlet's create a validation data set.\nWhat is the validation data set validation data set is\nyour training data set\nthat will be using it to trainer model fine.\nAll right.\nSo how will create a validation data\nset for creating a validation data set?\nWhat we are going to do is we are going to split our data set\ninto two point.\nOkay.\nSo the very first thing we'll do is to create\na validation data set.\nSo why do we even need a validation data set?\nSo we need a validation data set know\nthat the model we created is any good later.\nWhat we'll do we'll use the statistical method\nto estimate the accuracy of the model that we create\non the Unseen data.\nWe also want a more concrete estimate of the accuracy\nof the best model\non unseen data by evaluating it on the actual unseen data.\nOkay confused.\nLet me simplify this for you.\nWhat we'll do we'll split the loaded data\ninto two parts the first 80 percent of the data.\nUser to train our model and the rest 20% will hold back\nas the validation data set\nthat will use it to verify our trained model.\nOkay fine.\nSo let's define an array.\nThis is my ra water it will consist of will consist\nof all the values from the data set.\nSo data set dot values.\nOkay next.\nI'll Define a variable X\nwhich will consist of all the column\nfrom the array from 0 to 4 starting from 0 to 4\nand the next variable Y\nwhich would consist of of the array starting from this.\nSo first of all,\nwe will Define a variable X that will consist of the values\nin the array starting from the beginning 0 Del for\nokay.\nSo these are the column which will include\nin the X variable\nand for a y variable I'll Define it as a class or the output.\nSo what I need,\nI just need the fourth column that is my class column.\nSo I'll start it from the beginning\nand I just want the fourth column.\nOkay now I'll Define the my validation size.\nValidation underscore sighs,\nI'll Define it as 0.20 and our use a seed I\nDefine CD equals 6.\nSo this method seed sets the integers starting value used\nin generating random number.\nOkay, I'll Define the value of C R equals x.\nI'll tell you what is the importance of that later on?\nOkay.\nSo let me Define first few variables such as X\nunderscore train test why underscore train\nand why underscore test Okay,\nso What do you want to do is Select some model.\nOkay, so module underscore selection.\nBut before doing\nthat what we have to do is split our training data set\ninto two halves.\nOkay, so dot train underscore test underscore split\nwhat you want to split is a value of X and Y.\nOkay and my test size is equals to validation size,\nwhich is a 0.20 correct and my random state.\nIs equal to seed so what the city is doing?\nIt's helping me to keep the same Randomness in the training\nand testing data set fine.\nSo let's execute it and see what is our result.\nIt's executed next.\nWe'll create a test harness for this.\nWe'll use 10-fold cross-validation to\nestimate the accuracy.\nSo what it will do it will split a data set\ninto 10 parts crane on the nine part\nand test on the one part\nand this will repeat for all combination of train\nand test pilots.\nOkay.\nSo for that,\nlet's define again my CD that was six already\nDefine and scoring equals accuracy fine.\nSo we are using the metric of accuracy to evaluate the model.\nSo what is this?\nThis is a ratio of number of correctly predicted instances\ndivided by the total number of instances in the data set x\nhundred giving a percentage example.\nIt's 98% accurate or 99% accurate things like that.\nOkay, so we'll be In the scoring variable\nwhen we run the build\nand evaluate each model in the next step.\nThe next part is building model till now.\nWe don't know which algorithm would be good for this problem\nor what configuration to use.\nSo let's begin with six different algorithm.\nI'll be using\nlogistic regression linear discriminant analysis,\nk-nearest neighbor classification and\nregression trees neighbor buys.\nAnd what Vector machine well\nthese algorithms chime using is a good mixture of simple linear\nor non-linear algorithms in simple linear switch.\nIncluded the logistic regression\nand the linear discriminant analysis or the nonlinear part\nwhich included the KNN algorithm the card algorithm\nthat the neighbor buys and the support Vector machines.\nOkay.\nSo we reset the random number seed\nbefore each run to ensure that evaluation\nof each algorithm\nis performed using exactly the same data spreads.\nIt ensures the result are directly comparable.\nOkay, so, let me just copy and paste it.\nOkay.\nSo what we're doing here,\nwe are building five different types of model.\nWe are building logistic regression\nlinear discriminant analysis,\nk-nearest neighbor decision tree ghajini buys\nand the support Vector machine.\nOkay next what we'll do we'll evaluate model in each turn.\nOkay.\nSo what is this?\nSo we have six different model\nand accuracy estimation for each one of them\nnow we need to compare the model to each other\nand select the most accurate of them all.\nSo running the script we saw the following result\nso we can see some of the results on the screen.\nWhat is It is just\nthe accuracy score using different set of algorithms.\nOkay, when we are using logistic regression,\nwhat is the accuracy rate\nwhen we are using linear discriminant algorithm?\nWhat is the accuracy and so-and-so?\nOkay.\nSo from the output with seems\nthat LD algorithm was the most accurate model\nthat we tested now,\nwe want to get an idea of the accuracy of the model\non our validation set or the testing data set.\nSo this will give us an independent final check\non the accuracy of the best model.\nIt is always valuable to keep our testing data set\nfor just in case you made a our overfitting\nto the testing data set\nor you made a data leak both will result\nin an overly optimistic result.\nOkay, you can run the ldo model directly on the validation set\nand summarize the result as a final score a confusion Matrix\nand a classification statistics and probability are essential\nbecause these disciples form the basic Foundation\nof all machine learning algorithms deep learning.\nSocial intelligence and data science,\nin fact mathematics and probability is\nbehind everything around us from shapes patterns\nand colors to the count\nof petals in a flower mathematics is embedded\nin each and every aspect of our lives.\nSo I'm going to go ahead and discuss the agenda\nfor today with you all we're going to begin\nthe session by understanding\nwhat is data after that.\nWe'll move on and look at the different categories\nof data like quantitative and Qualitative data,\nthen we'll discuss what exactly statistics is\nthe basic terminologies in statistics and a couple\nof sampling techniques.\nOnce we're done with that.\nWe'll discuss a different types of Statistics\nwhich involve descriptive and inferential statistics.\nThen in the next session,\nwe will mainly be focusing on descriptive statistics\nhere will understand the different measures\nof center measures of spread Information Gain\nand entropy will also\nunderstand all of these measures with the help of a user.\nAnd finally, we'll discuss what exactly a confusion Matrix is.\nOnce we've covered\nthe entire descriptive statistics module will discuss\nthe probability module\nhere will understand what exactly probability\nis the different terminologies in probability.\nWe will also study the different probability distributions,\nthen we'll discuss the types of probability which include\nmarginal probability joint and conditional probability.\nThen we move on\nand discuss a use case wherein we will see examples\nthat show us\nhow the different types of probability work\nand to better understand Bayes theorem.\nWe look at a small example.\nAlso, I forgot to mention\nthat at the end of the descriptive statistics module\nwill be running a small demo in the our language.\nSo for those of you who don't know much\nabout our I'll be explaining every line in depth,\nbut if you want to have a more in-depth understanding\nabout our I'll leave a couple of blocks.\nAnd a couple of videos in the description box\nyou all can definitely check out that content.\nNow after we've completed the probability module will discuss\nthe inferential statistics module will start this module\nby understanding\nwhat is point estimation will discuss\nwhat is confidence interval and how you can estimate\nthe confidence interval will also discuss margin of error\nand will understand all of these concepts by looking\nat a small use case.\nWe finally end the inferential Real statistic module by looking\nat what hypothesis testing is hypothesis.\nTesting is a very important part of inferential statistics.\nSo we'll end the session by looking at a use case\nthat discusses how hypothesis testing works\nand to sum everything up.\nWe'll look at a demo\nthat explains how inferential statistics works.\nRight?\nSo guys, there's a lot to cover today.\nSo let's move ahead and take a look at our first topic\nwhich is what is data.\nNow, this is a quite simple question\nif I ask any of You what is data?\nYou'll see that it's a set of numbers\nor some sort of documents\nthat have stored in my computer now data is actually everything.\nAll right, look around you there is data everywhere each click\non your phone generates more data than you know,\nnow this generated data provides insights for analysis\nand helps us make Better Business decisions.\nThis is why data is so important to give you\na formal definition data refers to facts and statistics.\nCollected together for reference or analysis.\nAll right.\nThis is the definition of data in terms\nof statistics and probability.\nSo as we know data can be collected it\ncan be measured and analyzed\nit can be visualized by using statistical models\nand graphs now data is divided into two major subcategories.\nAlright, so first we have qualitative data\nand quantitative data.\nThese are the two different types of data\nunder qualitative data.\nI'll be have nominal and ordinal data\nand under quantitative data.\nWe have discrete and continuous data.\nNow, let's focus on qualitative data.\nNow this type of data deals with characteristics and descriptors\nthat can't be easily measured\nbut can be observed subjectively\nnow qualitative data is further divided\ninto nominal and ordinal data.\nSo nominal data is any sort of data\nthat doesn't have any order or ranking?\nOkay.\nAn example of nominal data is gender.\nNow.\nThere is no ranking in gender.\nThere's only male female or other right?\nThere is no one two,\nthree four or any sort of ordering in gender race is\nanother example of nominal data.\nNow ordinal data is basically an ordered series of information.\nOkay, let's say that you went to a restaurant.\nOkay.\nYour information is stored in the form of customer ID.\nAll right.\nSo basically you are represented with a customer ID.\nNow you would have rated their service as\neither good or average.\nAll right, that's how no ordinal data is\nand similarly they'll have a record of other customers\nwho visit the restaurant along with their ratings.\nAll right.\nSo any data which has some sort of sequence\nor some sort of order to it is known as ordinal data.\nAll right, so guys,\nthis is pretty simple to understand now,\nlet's move on and look at quantitative data.\nSo quantitative data basically these He's\nwith numbers and things.\nOkay, you can understand\nthat by the word quantitative itself quantitative is\nbasically quantity.\nRight Saudis with numbers a deals with anything\nthat you can measure objectively, right?\nSo there are two types\nof quantitative data there is discrete and continuous data\nnow discrete data is also known as categorical data\nand it can hold a finite number of possible values.\nNow, the number of students in a class is a finite Number.\nAll right, you can't have infinite number\nof students in a class.\nLet's say in your fifth grade.\nThere were a hundred students in your class.\nAll right, there weren't infinite number but there was\na definite finite number of students in your class.\nOkay, that's discrete data.\nNext.\nWe have continuous data.\nNow this type of data can hold infinite number\nof possible values.\nOkay.\nSo when you say weight of a person is an example\nof continuous data\nwhat I mean to see is my weight can be 50 kgs\nor it Can be 50.1 kgs\nor it can be 50.00 one kgs or 50.000 one or is 50.0 2 3\nand so on right?\nThere are infinite number of possible values, right?\nSo this is what I mean by continuous data.\nAll right.\nThis is the difference between discrete and continuous data.\nAnd also I would like to mention a few other things over here.\nNow, there are a couple of types of variables as well.\nWe have a discrete variable\nand we have a continuous variable discrete variable\nis also known as a categorical variable\nor and it can hold values of different categories.\nLet's say that you have a variable called message\nand there are two types of values that this variable\ncan hold let's say\nthat your message can either be a Spam message\nor a non spam message.\nOkay, that's when you call a variable as discrete\nor categorical variable.\nAll right, because it can hold values\nthat represent different categories of data\nnow continuous variables are basically variables\nthat can store in finite number of values.\nSo the weight of a person can be denoted as\na continuous variable.\nAll right, let's say there is a variable called weight\nand it can store infinite number of possible values.\nThat's why we'll call it a continuous variable.\nSo guys basically variable is anything\nthat can store a value right?\nSo if you associate any sort of data with a A table,\nthen it will become either discrete variable\nor continuous variable.\nThere is also dependent and independent type of variables.\nNow, we won't discuss all with that in depth because\nthat's pretty understandable.\nI'm sure all of you know,\nwhat is independent variable and dependent variable right?\nDependent variable is any variable whose value\ndepends on any other independent variable?\nSo guys that much knowledge I expect or\nif you do have all right.\nSo now let's move on and look at our next topic which Which is\nwhat is statistics now coming to the formal definition\nof statistics statistics is an area of Applied Mathematics,\nwhich is concerned\nwith data collection analysis interpretation\nand presentation now usually\nwhen I speak about statistics people think statistics is\nall about analysis\nbut statistics has other path toward it has data collection is\nalso part of Statistics data interpretation presentation.\nAll of this comes\ninto statistics already are going to use statistical methods\nto visualize data to collect data to interpret data.\nAlright, so the area of mathematics deals\nwith understanding\nhow data can be used to solve complex problems.\nOkay.\nNow I'll give you a couple of examples\nthat can be solved by using statistics.\nOkay, let's say\nthat your company has created a new drug\nthat may cure cancer.\nHow would you conduct a test to confirm\nthe As Effectiveness now,\neven though this sounds like a biology problem.\nThis can be solved with Statistics.\nAll right, you will have to create a test\nwhich can confirm the effectiveness of the drum\nor a this is a common problem\nthat can be solved using statistics.\nLet me give you another example you\nand a friend are at a baseball game and out of the blue.\nHe offers you a bet\nthat neither team will hit a home run in that game.\nShould you take the BET?\nAll right here you just discuss the probability\nof I know you'll win or lose.\nAll right, this is another problem\nthat comes under statistics.\nLet's look at another example.\nThe latest sales data has just come in\nand your boss wants you to prepare a report\nfor management on places\nwhere the company could improve its business.\nWhat should you look for?\nAnd what should you not look for now?\nThis problem involves a lot\nof data analysis will have to look at the different variables\nthat are causing your business to go down\nor the you have to look at a few variables.\nThat are increasing the performance of your models\nand does growing your business.\nAlright, so this involves a lot of data analysis\nand the basic idea\nbehind data analysis is to use statistical techniques\nin order to figure out the relationship\nbetween different variables\nor different components in your business.\nOkay.\nSo now let's move on and look\nat our next topic which is basic terminologies and statistics.\nNow before you dive deep into statistics, it is important\nthat you understand the basic terminologies\nused in statistics.\nThe two most important terminologies in statistics\nare population and Sample.\nSo throughout the statistics course or throughout any problem\nthat you're trying to stall with Statistics.\nYou will come across these two words,\nwhich is population and Sample Now population is a collection\nor a set of individuals or objects or events.\nEvents whose properties are to be analyzed.\nOkay.\nSo basically you can refer to population as a subject\nthat you're trying to analyze now a sample is just\nlike the word suggests.\nIt's a subset of the population.\nSo you have to make sure that you choose the sample\nin such a way\nthat it represents the entire population.\nAll right.\nIt shouldn't Focus add one part of the population instead.\nIt should represent the entire population.\nThat's how your sample should be chosen.\nSo Well chosen sample will contain most\nof the information about a particular population parameter.\nNow, you must be wondering how can one choose a sample\nthat best represents the entire population now\nsampling is a statistical method\nthat deals with the selection of individual observations\nwithin a population.\nSo sampling is performed\nin order to infer statistical knowledge about a population.\nAll right, if you want to understand\nthe different statistics of a population\nlike the mean\nthe median Median the mode or the standard deviation\nor the variance of a population.\nThen you're going to perform sampling.\nAll right,\nbecause it's not reasonable for you to study a large population\nand find out the mean median and everything else.\nSo why is sampling performed you might ask?\nWhat is the point of sampling?\nWe can just study the entire population now guys,\nthink of a scenario\nwhere in you're asked to perform a survey\nabout the eating habits of teenagers in the US.\nSo at present there are over 42 million teens in the US\nand this number is growing\nas we are speaking right now, correct.\nIs it possible to survey each of these 42 million individuals\nabout their health?\nIs it possible?\nWell, it might be possible\nbut this will take forever to do now.\nObviously, it's not it's not reasonable to go around\nknocking each door\nand asking for what does your teenage son eat\nand all of that right?\nThis is not very reasonable.\nThat's Why sampling is used?\nIt's a method wherein a sample of the population is studied\nin order to draw inferences about the entire population.\nSo it's basically a shortcut to starting\nthe entire population instead of taking the entire population\nand finding out all the solutions.\nYou just going to take a part of the population\nthat represents the entire population\nand you're going to perform all your statistical analysis\nyour inferential statistics on that small sample.\nAll right,\nand that sample basically here Presents the entire population.\nAll right, so I'm sure have made this clear\nto you all what is sample and what is population now?\nThere are two main types of sampling techniques\nthat are discussed today.\nWe have probability sampling and non-probability\nsampling now in this video\nwill only be focusing on probability sampling techniques\nbecause non-probability sampling is not within the scope\nof this video.\nAll right will only discuss the probability part\nbecause we're focusing\non Statistics and probability correct.\nNow again under probability sampling.\nWe have three different types.\nWe have random sampling systematic\nand stratified sampling.\nAll right, and just to mention the different types\nof non-probability\nsampling zwi have no ball Kota judgment\nand convenience sampling.\nAll right now guys in this session.\nI'll only be focusing on probability.\nSo let's move on\nand look at the different types of probability sampling.\nSo what is Probability sampling.\nIt is a sampling technique\nin which samples from a large population\nare chosen by using the theory of probability.\nAll right, so there are three types\nof probability sampling.\nAll right first we have the random sampling now\nin this method each member\nof the population has an equal chance\nof being selected in the sample.\nAll right.\nSo each and every individual or each and every object\nin the population has an equal chance\nof being a A part of the sample.\nThat's what random sampling is all about.\nOkay, you are randomly going to select any individual\nor any object.\nSo this Bay each individual has\nan equal chance of being selected.\nCorrect?\nNext.\nWe have systematic sampling now\nin systematic sampling every nth record is chosen\nfrom the population to be a part of the sample.\nAll right.\nNow refer this image\nthat I've shown over here out of these six groups\nevery Skinned group is chosen as a sample.\nOkay.\nSo every second record is chosen here and this is\nour systematic sampling works.\nOkay, you're randomly selecting the nth record\nand you're going to add that to your sample.\nNext.\nWe have stratified sampling.\nNow in this type\nof technique a stratum is used to form samples\nfrom a large population.\nSo what is a stratum a stratum is basically a subset\nof the population\nthat shares at least one comment.\nCharacteristics so let's say\nthat your population has a mix of both male and female\nso you can create to straightens\nout of this one will have only the male subset\nand the other will have the female subset\nor a this is what stratum is it is basically a subset\nof the population\nthat shares at least one common characteristics.\nAll right in our example, it is gender.\nSo after you've created\na stratum you're going to use random sampling\non the stratums and you're going to choose a final Samba.\nBut so random sampling meaning\nthat all of the individuals\nin each of the stratum will have an equal chance\nof being selected in the sample, correct.\nSo Guys, these were the three different types\nof sampling techniques.\nNow, let's move on and look at our next topic\nwhich is the different types of Statistics.\nSo after this,\nwe'll be looking at the more advanced concepts of Statistics,\nright so far we discuss the basics of Statistics,\nwhich is basically\nwhat is statistics the different sampling.\nTechniques and the terminologies and statistics.\nAll right.\nNow we look at the different types of Statistics.\nSo there are two major types of Statistics\ndescriptive statistics\nand inferential statistics in today's session.\nWe will be discussing both of these types\nof Statistics in depth.\nAll right, we'll also be looking at a demo\nwhich I'll be running in the our language\nin order to make you understand what exactly\ndescriptive and inferential statistics is so guys,\nwhich is going to look at the 600 don't worry,\nif you don't have much knowledge,\nI'm explaining everything from the basic level.\nAll right, so guys descriptive statistics is a method\nwhich is used to describe and understand the features\nof specific data set by giving a short summary of the data.\nOkay, so it is mainly\nfocused upon the characteristics of data.\nIt also provides a graphical summary of the data now\nin order to make you understand what descriptive statistics is,\nlet's suppose.\nPose that you want to gift all your classmates or t-shirt.\nSo to study the average shirt size of a student\nin a classroom.\nSo if you were to use descriptive statistics to study\nthe average shirt size of students in your classroom,\nthen what you would do is you would record the shirt size\nof all students in the class\nand then you would find out the maximum minimum and average\nshirt size of the club.\nOkay.\nSo coming to inferential statistics inferential\nstatistics makes Is\nand predictions about a population based\non the sample of data taken from the population?\nOkay.\nSo in simple words,\nit generalizes a large data set and it applies probability\nto draw a conclusion.\nOkay.\nSo it allows you to infer data parameters\nbased on a statistical model by using sample data.\nSo if we consider the same example of finding\nthe average shirt size of students in a class\nin infinite shal statistics,\nyou will take a sample.\nAll set of the class\nwhich is basically a few people from the entire class.\nAll right, you already have had grouped the class\ninto large medium and small.\nAll right in this method you basically build\na statistical model\nand expand it for the entire population in the class.\nSo guys, there was a brief understanding of descriptive\nand inferential statistics.\nSo that's the difference between descriptive\nand inferential now in the next section,\nwe will go in depth about descriptive statistics.\nAll right, so,\nThat's a discuss more about descriptive statistics.\nSo like I mentioned\nearlier descriptive statistics is a method\nthat is used to describe and understand the features\nof a specific data set by giving short summaries about the sample\nand measures of the data.\nThere are two important measures in descriptive statistics.\nWe have measure of central tendency,\nwhich is also known as measure\nof center and we have measures of variability.\nThis is also known as measures of spread.\nEd so measures of center include mean median and mode now\nwhat is measures\nof center measures of the center are statistical measures\nthat represent the summary of a data set?\nOkay, the three main measures of center are mean median\nand mode coming to measures of variability\nor measures of spread.\nWe have range interquartile range variance\nand standard deviation.\nAll right.\nSo now let's discuss each\nof these measures in a little more.\nUp starting with the measures of center.\nNow.\nI'm sure all of you know what the mean is mean is basically\nthe measure of the average of all the values in a sample.\nOkay, so it's basically the average of all\nthe values in a sample.\nHow do you measure the mean I hope all of you know\nhow the main is measured\nif there are 10 numbers\nand you want to find the mean of these 10 numbers.\nAll you have to do is you have to add up all the 10 numbers\nand you have to divide it by 10 then here\nrepresents the Number of samples in your data set.\nAll right, since we have 10 numbers,\nwe're going to divide this by 10.\nAll right, this will give us the average\nor the mean so to better understand the measures\nof central tendency.\nLet's look at an example.\nNow the data set over here is basically the cars data set\nand it contains a few variables.\nAll right, it has something known as cars.\nIt has mileage\nper gallon cylinder type displacement horsepower\nand roll axle ratio.\nAll right, all of these measures are related to cars.\nOkay.\nSo what you're going to do is you're going\nto use descriptive analysis\nand you're going to analyze each of the variables\nin the sample data set\nfor the mean standard deviation median mode and so on.\nSo let's say that you want to find out the mean\nor the average horsepower\nof the cars among the population of cards.\nLike I mentioned earlier\nwhat you'll do is you will check the average of all the values.\nSo in this case,\nwe will take the sum of the horizontal.\nHorsepower of each car\nand we'll divide that by the total number of cards.\nOkay, that's exactly\nwhat I've done here in the calculation part.\nSo this hundred and ten basically\nrepresents the horsepower for the first car.\nAlright, similarly.\nI've just added up all the values of horsepower\nfor each of the cars\nand I've divided it by 8 now 8 is basically the number\nof cars in our data set.\nAll right, so hundred and three point six two five is what\nour mean is or Average of horsepower is all right.\nNow, let's understand what median is with an example?\nOkay.\nSo to Define median median is basically a measure\nof the central value\nof the sample set is called the median.\nAll right, you can see that it is a middle value.\nSo if we want to find out the center value\nof the mileage per gallon among the population\nof cars first,\nwhat we'll do is we'll arrange the MGP values in ascending\nor descending order\nand Choose a middle value right in this case\nsince we have eight values, right?\nWe have eight values which is an even entry.\nSo whenever you have even number of data points\nor samples in your data set,\nthen you're going to take the average\nof the two middle values.\nIf we had nine values over here.\nWe can easily figure out the middle value\nand you know choose that as a median.\nBut since they're even number of values we're going\nto take the average of the two middle values.\nAll right, so,\nEight and twenty three are my two middle values\nand I'm taking the mean of those 2\nand hence I get twenty two point nine,\nwhich is my median.\nAll right.\nLastly let's look at how mode is calculated.\nSo what is mode the value\nthat is most recurrent\nin the sample set is known as mode or basically the value\nthat occurs most often.\nOkay, that is known as mode.\nSo let's say\nthat we want to find out the most common type of cylinder\namong the population\nof cards all we have to Do is we will check the value\nwhich is repeated the most number of times here.\nWe can see that the cylinders come in two types.\nWe have cylinder of Type 4 and cylinder of type 6, right?\nSo take a look at the data set.\nYou can see that the most recurring value is 6 right.\nWe have one two, three four and five.\nWe have five six and we have one two, three.\nYeah, we have three four types of lenders and 5/6.\nCylinders.\nSo basically we have\nthree four type cylinders and we have five six type cylinders.\nAll right.\nSo our mode is going to be 6 since 6 is more\nrecurrent than 4 so guys\nthose were the measures\nof the center or the measures of central tendency.\nNow, let's move on and look at the measures of the spread.\nAll right.\nNow, what is the measure of spread a measure of spread?\nSometimes also called\nas measure of dispersion is used to describe the The variability\nin a sample or population.\nOkay, you can think of it as some sort\nof deviation in the sample.\nAll right.\nSo you measure this\nwith the help of the different measure of spreads.\nWe have range interquartile range variance\nand standard deviation.\nNow range is pretty self-explanatory, right?\nIt is the given measure of how spread apart the values\nin a data set are the range can be calculated\nas shown in this formula.\nSo you're basically going to The maximum value\nin your data set\nfrom the minimum value in your data set.\nThat's how you calculate the range of the data.\nAlright, next we have interquartile range.\nSo before we discuss interquartile range,\nlet's understand.\nWhat a quartile is red.\nSo quartiles basically tell us about the spread of a data set\nby breaking the data set into different quarters.\nOkay, just like\nhow the median breaks the data into two parts.\nThe quartile will break it.\nIn two different quarters,\nso to better understand how quartile and\ninterquartile are calculated.\nLet's look at a small example.\nNow this data set basically represents the marks\nof hundred students ordered from the lowest\nto the highest scores red.\nSo the quartiles lie\nin the following ranges the first quartile,\nwhich is also known as q1 it\nlies between the 25th and 26th observation.\nAll right.\nSo if you look at this I've highlighted the 25th\nand the Six observation.\nSo how you can calculate Q 1 or first quartile is\nby taking the average of these two values.\nAlright, since both the values are 45\nwhen you add them up and divide them by two\nyou'll still get 45 now the second quartile\nor Q 2 is between the 50th and the fifty first observation.\nSo you're going to take the average of 58\nand 59 and you will get a value of 58.5 now,\nthis is my second quarter the third quartile Q3.\nIs between the 75th\nand the 76th observation here again will take the average\nof the two values\nwhich is the 75th value and the 76 value right\nand you'll get a value of 71.\nAll right, so guys this is exactly\nhow you calculate the different quarters.\nNow, let's look at what is interquartile range.\nSo IQR or the interquartile range is a measure\nof variability based on dividing a data set into quartiles.\nNow, the interquartile range is Calculated\nby subtracting the q1 from Q3.\nSo basically Q3\nminus q1 is your IQ are so your IQR is your Q3 minus q1?\nAll right.\nNow this is how each\nof the quartiles are each core tile represents a quarter,\nwhich is 25% All right.\nSo guys, I hope all of you are clear\nwith the interquartile range and what our quartiles now,\nlet's look at variance covariance is\nbasically a measure that shows how much a\nI'm variable the first from its expected value.\nOkay.\nIt's basically the variance in any variable now variance\ncan be calculated by using this formula right here x\nbasically represents any data point in your data set\nn is the total number of data points in your data set\nand X bar is basically the mean of data points.\nAll right.\nThis is how you calculate variance variance is\nbasically a Computing the squares of deviations.\nOkay.\nThat's why it says s Square there now.\nLook at what is deviation deviation is just the difference\nbetween each element from the mean.\nOkay, so it can be calculated by using this simple formula\nwhere X I basically represents a data point\nand mu is the mean of the population\nor add this is exactly\nhow you calculate the deviation Now population variance\nand Sample variance are very specific to\nwhether you're calculating the variance\nin your population data set or in your sample data\nset now the only difference between Elation\nand Sample variance.\nSo the formula\nfor population variance is pretty explanatory.\nSo X is basically each data point mu is the mean\nof the population\nn is the number of samples in your data set.\nAll right.\nNow, let's look at sample.\nVariance Now sample variance\nis the average of squared differences from the mean.\nAll right here x i is any data point\nor any sample in your data set X bar is the mean\nof your sample.\nAll right.\nIt's not the main of your population.\nIt's the If your sample and\nif you notice any here is a smaller n is the number\nof data points in your sample.\nAnd this is basically the difference between sample\nand population variance.\nI hope that is clear coming\nto standard deviation is the measure of dispersion\nof a set of data from its mean.\nAll right, so it's basically the deviation from your mean.\nThat's what standard deviation is now to better understand\nhow the measures of spread are calculated.\nLet's look at a small use case.\nSo let's say the Daenerys has 20 dragons.\nThey have the numbers nine to five four and so on\nas shown on the screen,\nwhat you have to do is you have to work out\nthe standard deviation or at\nin order to calculate the standard deviation.\nYou need to know the mean right?\nSo first you're going to find out the mean of your sample set.\nSo how do you calculate the mean you add all the numbers\nin your data set\nand divided by the total number of samples in your data set\nso you get a value of 7 here\nthen you I'll clear the rhs of your standard deviation formula.\nAll right, so from each data point you're going\nto subtract the mean and you're going to square that.\nAll right.\nSo when you do that,\nyou will get the following result.\nYou'll basically get this 425 for 925\nand so on so finally you will just find the mean\nof the squared differences.\nAll right.\nSo your standard deviation\nwill come up to two point nine eight three\nonce you take the square root.\nSo guys, this is pretty simple.\nIt's a simple mathematic technique.\nAll you have to do is you have to substitute the values\nin the formula.\nAll right.\nI hope this was clear to all of you.\nNow let's move on\nand discuss the next topic which is Information Gain\nand entropy now.\nThis is one of my favorite topics in statistics.\nIt's very interesting and this topic is mainly involved\nin machine learning algorithms,\nlike decision trees and random forest.\nAll right, it's very important\nfor you to know\nhow Information Gain and entropy really work and why they are\nso essential in building machine learning models.\nWe focus on the statistic parts of Information Gain and entropy\nand after that we'll discuss As a use case and see\nhow Information Gain\nand entropy is used in decision trees.\nSo for those of you\nwho don't know what a decision tree is it is\nbasically a machine learning algorithm.\nYou don't have to know anything about this.\nI'll explain everything in depth.\nSo don't worry.\nNow.\nLet's look at what exactly entropy and Information Gain Is\nAs entropy is basically the measure\nof any sort of uncertainty that is present in the data.\nAll right, so it can be measured by using this formula.\nSo here s is the set of all instances in the data set\nor although data items in the data set\nn is the different type\nof classes in your data set Pi is the event probability.\nNow this might seem a little confusing\nto you all but when we go to the use case,\nyou'll understand all of these terms even better.\nAll right cam.\nTo Information Gain\nas the word suggests Information Gain indicates\nhow much information a particular feature\nor a particular variable gives us about the final outcome.\nOkay, it can be measured by using this formula.\nSo again here hedge of s is the entropy\nof the whole data set s SJ is the number\nof instances with the J value\nof an attribute a s is the total number of instances\nin the data set V is the set\nof distinct values of an attribute a hedge\nof SJ is the entropy of subsets of instances\nand hedge of a comma s is the entropy of an attribute\na even though this seems confusing.\nI'll clear out the confusion.\nAll right, let's discuss a small problem statement\nwhere we will understand\nhow Information Gain\nand entropy is used to study the significance of a model.\nSo like I said Information Gain\nand entropy are very important statistical measures\nthat let us understand\nthe significance of a predictive model.\nOkay to get a more clear understanding.\nLet's look at a use case.\nAll right now suppose we are given a problem statement.\nAll right, the statement is that you have to predict\nwhether a match can be played\nor Not by studying the weather conditions.\nSo the predictor variables here are outlook humidity wind day\nis also a predictor variable.\nThe target variable is basically played already.\nThe target variable is the variable\nthat you're trying to protect.\nOkay.\nNow the value of the target variable will decide\nwhether or not a game can be played.\nAll right, so that's why The play has two values.\nIt has no and yes, no,\nmeaning that the weather conditions are not good.\nAnd therefore you cannot play the game.\nYes, meaning that the weather conditions are good and suitable\nfor you to play the game.\nAlright, so that was a problem statement.\nI hope the problem statement is clear to all of you now\nto solve such a problem.\nWe make use of something known as decision trees.\nSo guys think of an inverted tree\nand each branch of the tree denotes some decision.\nAll right, each branch is Is known as the branch node\nand at each branch node,\nyou're going to take a decision in such a manner\nthat you will get an outcome at the end of the branch.\nAll right.\nNow this figure here basically shows\nthat out of 14 observations 9 observations result in a yes,\nmeaning that out of 14 days.\nThe match can be played only on nine days.\nAlright, so here\nif you see on day 1 Day 2 Day 8 day 9 and 11.\nThe Outlook has been Alright,\nso basically we try to plaster a data set\ndepending on the Outlook.\nSo when the Outlook is sunny,\nthis is our data set when the Outlook is overcast.\nThis is what we have\nand when the Outlook is the rain this is what we have.\nAll right, so\nwhen it is sunny we have two yeses and three nodes.\nOkay, when the Outlook is overcast.\nWe have all four as yes has meaning\nthat on the four days when the Outlook was overcast.\nWe can play the game.\nAll right.\nNow when it comes to drain,\nwe have three yeses and two nodes.\nAll right.\nSo if you notice here,\nthe decision is being made by choosing the Outlook variable\nas the root node.\nOkay.\nSo the root node is\nbasically the topmost node in a decision tree.\nNow, what we've done here is we've created a decision tree\nthat starts with the Outlook node.\nAll right, then you're splitting the decision tree further\ndepending on other parameters like Sunny overcast and rain.\nAll right now like we know that Outlook has three values.\nSunny overcast and brain so let me explain this\nin a more in-depth manner.\nOkay.\nSo what you're doing here is you're making\nthe decision Tree by choosing the Outlook variable\nat the root node.\nThe root note is basically the topmost node\nin a decision tree.\nNow the Outlook node has three branches coming out from it,\nwhich is sunny overcast and rain.\nSo basically Outlook\ncan have three values either it can be sunny.\nIt can be overcast or it can be rainy.\nOkay now these three values Use are assigned\nto the immediate Branch nodes and for each\nof these values the possibility\nof play is equal to yes is calculated.\nSo the sunny\nand the rain branches will give you an impure output.\nMeaning that there is a mix of yes and no right.\nThere are two yeses here three nodes here.\nThere are three yeses here and two nodes over here,\nbut when it comes to the overcast variable,\nit results in a hundred percent pure subset.\nAll right, this shows that the overcast baby.\nWill result in a definite and certain output.\nThis is exactly what entropy is used to measure.\nAll right, it calculates the impurity or the uncertainty.\nAlright, so the lesser the uncertainty or the entropy\nof a variable more significant is that variable?\nSo when it comes to overcast there's literally no impurity\nin the data set.\nIt is a hundred percent pure subset, right?\nSo be want variables like these in order to build a model.\nAll right now,\nwe don't always Ways get lucky and we don't always find\nvariables that will result in pure subsets.\nThat's why we have the measure entropy.\nSo the lesser the entropy of a particular variable the most\nsignificant that variable will be so in a decision tree.\nThe root node is assigned the best attribute\nso that the decision tree can predict the most\nprecise outcome meaning that on the root note.\nYou should have the most significant variable.\nAll right, that's why we've chosen Outlook\nor and now some of you might ask me why haven't you chosen\novercast Okay is overcast is not a variable.\nIt is a value of the Outlook variable.\nAll right.\nThat's why we've chosen outlook here because it has\na hundred percent pure subset which is overcast.\nAll right.\nNow the question in your head is how do I decide which variable\nor attribute best Blitz the data now right now,\nI know I looked at the data\nand I told you that,\nyou know here we have a hundred percent pure subset,\nbut what if it's a more complex problem\nand you're not able to understand which variable\nwill best split the data,\nso guys when it comes\nto decision tree Information and gain\nand entropy will help\nyou understand which variable will best split the data set.\nAll right, or which variable you have to assign to the root node\nbecause whichever variable is assigned to the dude node.\nIt will best let the data set\nand it has to be the most significant variable.\nAll right.\nSo how we can do this is we need to use\nInformation Gain and entropy.\nSo from the total of the 14 instances\nthat we saw nine of them said yes\nand 5 of the instances said know\nthat you cannot play on that particular day.\nAll right.\nSo how do you calculate the entropy?\nSo this is the formula you just substitute\nthe values in the formula.\nSo when you substitute the values in the formula,\nyou will get a value of 0.9940.\nAll right.\nThis is the entropy\nor this is the uncertainty of the data present in a sample.\nNow in order to ensure\nthat we choose the best variable for the root node.\nLet us look at all the possible combinations\nthat you can use on the root node.\nOkay, so these are All the possible combinations\nyou can either have Outlook you can have\nwindy humidity or temperature.\nOkay, these are four variables and you can have any one\nof these variables as your root node.\nBut how do you select\nwhich variable best fits the root node?\nThat's what we are going to see by using\nInformation Gain and entropy.\nSo guys now the task at hand is to find the information gain\nfor each of these attributes.\nAll right.\nSo for Outlook for windy for humidity and for temperature,\nwe're going to find out the information.\nNation gained right now a point to remember is\nthat the variable\nthat results in the highest Information Gain must be chosen\nbecause it will give us the most precise and output information.\nAll right.\nSo the information gain for attribute windy will calculate\nthat first here.\nWe have six instances of true and eight instances of false.\nOkay.\nSo when you substitute all the values in the formula,\nyou will get a value of zero point zero four eight.\nSo we get a value of You 2.0 for it.\nNow.\nThis is a very low value for Information Gain.\nAll right, so the information\nthat you're going to get from Windy attribute is pretty low.\nSo let's calculate the information gain\nof attribute Outlook.\nAll right, so from the total of 14 instances,\nwe have five instances with say Sunny for instances,\nwhich are overcast and five instances,\nwhich are rainy.\nAll right for Sonny.\nWe have three yeses and to nose for overcast we have All the\nfor as yes for any we have three years and two nodes.\nOkay.\nSo when you calculate the information gain\nof the Outlook variable will get a value\nof zero point 2 4 7 now compare this to the information gain\nof the windy attribute.\nThis value is actually pretty good.\nRight we have zero point 2 4 7 which is a pretty good value\nfor Information Gain.\nNow, let's look at the information gain\nof attribute humidity now over here.\nWe have seven instances with say hi and seven instances\nwith say normal.\nRight and under the high Branch node.\nWe have three instances with say yes,\nand the rest for instances would say no similarly\nunder the normal Branch.\nWe have one two, three,\nfour, five six seven instances would say yes\nand one instance with says no.\nAll right.\nSo when you calculate the information gain\nfor the humidity variable,\nyou're going to get a value of 0.15 one.\nNow.\nThis is also a pretty decent value,\nbut when you compare it to the Information Gain,\nOf the attribute Outlook it is less right now.\nLet's look at the information gain of attribute temperature.\nAll right, so the temperature can hold repeat.\nSo basically the temperature attribute can hold\nhot mild and cool.\nOkay under hot.\nWe have two instances with says yes and two instances\nfor no under mild.\nWe have four instances of yes and two instances of no\nand under col we have three instances of yes\nand one instance of no.\nAll right.\nWhen you calculate\nthe information gain for this attribute,\nyou will get a value of zero point zero to nine,\nwhich is again very less.\nSo what you can summarize from here is if we look\nat the information gain for each of these variable will see\nthat for Outlook.\nWe have the maximum gain.\nAll right, we have zero point two four seven,\nwhich is the highest Information Gain value\nand you must always choose a variable with the highest\nInformation Gain to split the data at the root node.\nSo that's why we assign The Outlook variable\nat the root node.\nAll right, so guys.\nI hope this use case with clear if any of you have doubts.\nPlease keep commenting those doubts now,\nlet's move on and look at what exactly a confusion Matrix is\nthe confusion Matrix is the last topic\nfor descriptive statistics read after this.\nI'll be running a short demo where I'll be showing you\nhow you can calculate mean median mode\nand standard deviation variance and all of those values\nby using our okay.\nSo let's talk about confusion Matrix now guys.\nWhat is the confusion Matrix now don't get confused.\nThis is not any complex topic now confusion.\nMatrix is a matrix\nthat is often used to describe the performance of a model.\nAll right, and this is specifically used\nfor classification models\nor a classifier\nand what it does is it will calculate the accuracy\nor it will calculate the performance of your classifier\nby comparing your actual results and Your predicted results.\nAll right.\nSo this is what it looks\nlike to positive to negative and all of that.\nNow this is a little confusing.\nI'll get back to what exactly true positive\nto negative and all of this stands for for now.\nLet's look at an example and let's try and understand what\nexactly confusion Matrix is.\nSo guys have made sure\nthat I put examples after each and every topic\nbecause it's important you\nunderstand the Practical part of Statistics.\nAll right statistics has literally nothing to do\nwith Theory you need to understand how Calculations\nare done in statistics.\nOkay.\nSo here what I've done is now let's look at a small use case.\nOkay, let's consider\nthat your given data about a hundred and sixty five\npatients out of which hundred and five patients have a disease\nand the remaining 50 patients don't have a disease.\nOkay.\nSo what you're going to do is you will build a classifier\nthat predicts by using\nthese hundred and sixty five observations.\nYou'll feed all\nof these 165 observations to your classifier\nand it will predict the output every time\na new patients detail is fed to the classifier right now\nout of these 165 cases.\nLet's say that the classifier predicted.\nYes hundred and ten times and no 55 times.\nAlright, so yes basically stands for yes.\nThe person has a disease and no stands for know.\nThe person does not have a disease.\nAll right, that's pretty self-explanatory.\nBut yeah, so it predicted that a hundred and ten times.\nPatient has a disease and 55 times\nthat know the patient doesn't have a disease.\nHowever in reality only hundred and five patients\nin the sample have the disease and 60 patients\nwho do not have the disease, right?\nSo how do you calculate the accuracy of your model?\nYou basically build the confusion Matrix?\nAll right.\nThis is how the Matrix looks\nlike and basically denotes the total number of observations\nthat you have\nwhich is 165 in our case actual denotes the actual use\nin the data set\nand predicted denotes the predicted values\nby the classifier.\nSo the actual value is no here\nand the predicted value is no here.\nSo your classifier was correctly able\nto classify 50 cases as no.\nAll right, since both of these are no so 50\nit was correctly able to classify but 10\nof these cases it incorrectly classified meaning\nthat your actual value here is no but you classifier\npredicted it as yes\nor I that's why this And over here similarly\nit wrongly predicted\nthat five patients do not have diseases\nwhereas they actually did have diseases\nand it correctly predicted hundred patients,\nwhich have the disease.\nAll right.\nI know this is a little bit confusing.\nBut if you look at these values no,\nno 50 meaning\nthat it correctly predicted 50 values No\nYes means that it wrongly predicted.\nYes for the values are it was supposed to predict.\nNo.\nAll right.\nNow what exactly is?\nIs this true positive to negative and all of that?\nI'll tell you what exactly it is.\nSo true positive are the cases in which we predicted a yes\nand they do not actually have the disease.\nAll right, so it is basically this value\nalready predicted a yes here,\neven though they did not have the disease.\nSo we have 10 true positives right similarly true-\nis we predicted know\nand they don't have the disease meaning\nthat this is correct.\nFalse positive is be predicted.\nYes, but they do not actually have the disease\nor at this is also known as type 1 error falls- is we predicted.\nNo, but they actually do not have the disease.\nSo guys basically falls-\nand true negatives are basically correct classifications.\nAll right.\nSo this was confusion Matrix\nand I hope this concept is clear again guys.\nIf you have doubts,\nplease comment your doubt in the comment section.\nSo guys, that was the entire descriptive.\nX module and now we will discuss about probability.\nOkay.\nSo before we understand what exactly probability is,\nlet me clear out a very common misconception people\noften tend to ask me this question.\nWhat is the relationship between statistics and probability?\nSo probability and statistics are related fields.\nAll right.\nSo probability is a mathematical method used\nfor statistical analysis.\nTherefore we can say\nthat a probability and statistics are interconnected.\nLaunches of mathematics\nthat deal with analyzing the relative frequency of events.\nSo they're very interconnected feels\nand probability makes use of statistics\nand statistics makes use\nof probability or a they're very interconnected Fields.\nSo that is the relationship between statistics\nand probability.\nNow, let's understand what exactly is probability.\nSo probability is the measure\nof How likely an event will occur to be more precise.\nIt is the ratio.\nOf desired outcome to the total outcomes.\nNow, the probability\nof all outcomes always sum up to 1 the probability will always\nsum up to 1 probability cannot go beyond one.\nOkay.\nSo either your probability can be 0 or it can be 1\nor it can be in the form of decimals like 0.5\nto or 0.55 or it can be in the form of 0.5 0.7 0.9.\nBut it's valuable always stay between the range 0 and 1.\nOkay at the famous example\nof probability is rolling a dice example.\nSo when you roll a dice you get six possible outcomes, right?\nYou get one two,\nthree four and five six phases of a dice now\neach possibility only has one outcome.\nSo what is the probability that on rolling a dice?\nYou will get 3 the probability is 1 by 6, right\nbecause there's only one phase\nwhich has the number 3 on it out of six phases.\nThere's only one phase which has the number three.\nSo the probability of getting 3\nwhen you roll a dice is 1 by 6 similarly,\nif you want to find the probability of getting\na number 5 again,\nthe probability is going to be 1 by 6.\nAll right, so all of this will sum up to 1.\nAll right, so guys this is exactly what probability is.\nIt's a very simple concept we all learnt it\nin 8 standard onwards right now.\nLet's understand the different terminologies\nthat are related to probability.\nNow the three terminologies that you often come across\nwhen We talk about probability.\nWe have something known as the random experiment.\nOkay, it's basically an experiment or a process\nfor which the outcomes cannot be predicted with certainty.\nAll right.\nThat's why you use probability.\nYou're going to use probability in order to predict the outcome\nwith some sort\nof certainty sample space is the entire possible set\nof outcomes of a random experiment an event is\none or more outcomes of an experiment.\nSo if you consider the example Love rolling a dice.\nNow.\nLet's say that you want to find out the probability\nof getting a to when you roll the dice.\nOkay.\nSo finding this probability is the random experiment\nthe sample space is basically your entire possibility.\nOkay.\nSo one two, three, four,\nfive six phases are there and out of that you need\nto find the probability of getting a 2, right.\nSo all the possible outcomes\nwill basically represent your sample space.\nOkay.\nSo 1 to 6 are all your possible outcomes this represents.\nSample space event is one or more outcome\nof an experiment.\nSo in this case my event is to get a to\nwhen I roll a dice, right?\nSo my event is the probability of getting a to\nwhen I roll a dice.\nSo guys, this is basically what random experiment sample space\nand event really means alright.\nNow, let's discuss the different types of events.\nThere are two types of events that you should know about there\nis disjoint and non disjoint events disjoint events.\nThese are events\nthat do not have any common outcome.\nFor example,\nif you draw a single card from a deck of cards,\nit cannot be a king and a queen correct.\nIt can either be king or it can be Queen.\nNow a non disjoint events are events\nthat have common outcomes.\nFor example, a student can get hundred marks\nin statistics and hundred marks in probability.\nAll right, and also the outcome\nof a ball delibird can be a no ball\nand it can be a 6 right.\nSo this is Non disjoint events are or n.\nThese are very simple to understand right now.\nLet's move on and look at the different types\nof probability distribution.\nAll right, I'll be discussing\nthe three main probability distribution functions.\nI'll be talking about probability density\nfunction normal distribution and Central limit theorem.\nOkay probability density function also known\nas PDF is concerned\nwith the relative likelihood for a continuous random variable.\nTo take on a given value.\nAll right.\nSo the PDF gives the probability\nof a variable that lies between the range A and B.\nSo basically what you're trying to do is you're going to try\nand find the probability of a continuous random variable\nover a specified range.\nOkay.\nNow this graph denotes the PDF of a continuous variable.\nNow, this graph is also known as the bell curve right?\nIt's famously called the bell curve because of\nits shape and there are three important properties\nthat you To know about a probability density function.\nNow the graph of a PDF will be continuous over a range.\nThis is because you're finding the probability\nthat a continuous variable lies between the ranges A and B,\nright the second property is\nthat the area bounded by the curve of a density function\nand the x-axis is equal to 1 basically the area\nbelow the curve is equal to 1 all right,\nbecause it denotes probability again\nthe probability cannot arrange.\nMore than one it has to be\nbetween 0 and 1 property number three is that the probability\nthat our random variable assumes a value between A\nand B is equal to the area\nunder the PDF bounded by A and B. Okay.\nNow what this means is\nthat the probability value is denoted by the area\nof the graph.\nAll right, so whatever value that you get here,\nwhich basically one is the probability\nthat a random variable will lie between the range A and B.\nAll right, so I hope\nIf you have understood the probability density function,\nit's basically the probability of finding the value\nof a continuous random variable between the range A and B.\nAll right.\nNow, let's look at our next distribution,\nwhich is normal distribution now normal distribution,\nwhich is also known as\nthe gaussian distribution is a probability distribution\nthat denotes the symmetric property\nof the mean right meaning\nthat the idea behind this function is\nthat The data near the mean\noccurs more frequently than the data away from the mean.\nSo what it means to say is\nthat the data around the mean represents the entire data set.\nOkay.\nSo if you just take a sample of data\naround the mean it can represent the entire data set now similar\nto the probability density function the normal distribution\nappears as a bell curve.\nAll right.\nNow when it comes to normal distribution,\nthere are two important factors.\nAll right, we have the mean of the population.\nAnd the standard deviation.\nOkay, so the mean and the graph determines the location\nof the center of the graph,\nright and the standard deviation determines the height\nof the graph.\nOkay.\nSo if the standard deviation is large the curve is going\nto look something like this.\nAll right, it'll be short and wide and\nif the standard deviation is small the curve\nis tall and narrow.\nAll right.\nSo this was it about normal distribution.\nNow, let's look at the central limit theorem.\nNow the central limit theorem states\nthat the sampling distribution\nof the mean of any independent random variable will be normal\nor nearly normal\nif the sample size is large enough now,\nthat's a little confusing.\nOkay.\nLet me break it down for you now in simple terms\nif we had a large population\nand we divided it into many samples.\nThen the mean of all the samples\nfrom the population will be almost equal\nto the mean of the entire population right meaning\nthat each of the sample is normally distributed.\nRight.\nSo if you compare the mean of each of the sample,\nit will almost be equal to the mean of the population.\nRight?\nSo this graph basically shows a more clear understanding\nof the central limit theorem red you can see each sample here\nand the mean\nof each sample is almost along the same line, right?\nOkay.\nSo this is exactly\nwhat the central limit theorem States now the accuracy\nor the resemblance\nto the normal distribution depends on two main factors.\nRight.\nSo the first is the number of sample points\nthat you consider.\nAll right,\nand the second is a shape of the underlying population.\nNow the shape obviously depends on the standard deviation\nand the mean of a sample, correct.\nSo guys the central limit theorem basically states\nthat each sample will be normally distributed\nin such a way\nthat the mean of each sample will coincide with the mean\nof the actual population.\nAll right in short terms.\nThat's what central limit theorem States.\nAlright, and this holds true only for a large.\nIs it mostly for a small data set\nand there are more deviations\nwhen compared to a large data set is because of\nthe scaling Factor, right?\nThe small is deviation\nin a small data set will change the value very drastically,\nbut in a large data set a small deviation\nwill not matter at all.\nNow, let's move on and look at our next topic\nwhich is the different types of probability.\nNow, this is a important topic\nbecause most of your problems can be solved by understanding\nwhich type of probability should I use to solve?\nThis problem right?\nSo we have three important types of probability.\nWe have marginal joint and conditional probability.\nSo let's discuss each\nof these now the probability of an event occurring unconditioned\non any other event is known as marginal probability\nor unconditional probability.\nSo let's say that you want to find the probability\nthat a card drawn is a heart.\nAll right.\nSo if you want to find the probability\nthat a card drawn is a heart the prophet.\nB13 by 52 since there are 52 cards in a deck\nand there are 13 hearts in a deck of cards.\nRight and there are 52 cards in a turtleneck.\nSo your marginal probability will be 13 by 52.\nThat's about marginal probability.\nNow, let's understand.\nWhat is joint probability.\nNow joint probability\nis a measure of two events happening at the same time.\nOkay.\nLet's say that the two events are A and B.\nSo the probability of event A\nand B occurring is the dissection of A and B.\nSo for example,\nif you want to find the probability\nthat a card is a four and a red that would be joint probability.\nAll right, because you're finding a card\nthat is 4 and the card has to be red in color.\nSo for the answer,\nthis will be 2 by 52 because we have 1/2\nin heart and we have 1/2 and diamonds correct.\nSo both of these are red and color therefore.\nOur probability is to by 52\nand if you further down it Is 1 by 26, right?\nSo this is what joint probability is all\nabout moving on.\nLet's look at what exactly conditional probability is.\nSo if the probability\nof an event or an outcome is based on the occurrence\nof a previous event or an outcome,\nthen you call it as a conditional probability.\nOkay.\nSo the conditional probability of an event B is the probability\nthat the event will occur given\nthat an event a has already occurred, right?\nSo if a and b are dependent events,\nthen the expression\nfor conditional probability is given by this.\nNow this first term on the left hand side,\nwhich is p b of a is basically the probability\nof event B occurring\ngiven that event a has already occurred.\nAll right.\nSo like I said,\nif a and b are dependent events,\nthen this is the expression but if a\nand b are independent events,\nand the expression for conditional probability is\nlike this, right?\nSo guys P of A and B of B is obviously the probability of A\nand probability of B right now.\nLet's move on now in order\nto understand conditional probability joint probability\nand marginal probability.\nLet's look at a small use case.\nOkay now basically we're going to take a data set\nwhich examines the salary package and training\nundergone my candidates.\nOkay.\nNow in this there are 60 candidates without training\nand forty five candidates,\nwhich have enrolled for Adder a curse training.\nRight.\nNow the task here is you have to assess the training\nwith a salary package.\nOkay, let's look at this in a little more depth.\nSo in total,\nwe have hundred and five candidates out of which 60\nof them have not enrolled Frederick has training\nand 45 of them have enrolled for a deer Acres training\nor this is a small survey\nthat was conducted\nand this is the rating of the package or the salary\nthat they got right?\nSo if you read through the data,\nyou can understand there were five candidates.\nIt's without education\nor training who got a very poor salary package.\nOkay.\nSimilarly, there are\n30 candidates with Ed Eureka training\nwho got a good package, right?\nSo guys basically you're comparing the salary package\nof a person depending on\nwhether or not they've enrolled for a director training, right?\nThis is our data set.\nNow, let's look at our problem statement find the probability\nthat a candidate has undergone a Drake\nhas training quite simple,\nwhich type of probability is this Is this is\nmarginal probability?\nRight?\nSo the probability\nthat a candidate has undergone edger Acres training is\nobviously 45 divided by a hundred and five\nsince 45 is the number\nof candidates with Eddie record raining\nand hundred and five is the total number of candidates.\nSo you get a value of approximately 0.4\nto all right,\nthat's the probability of a candidate\nthat has undergone educate a girl straining next question\nfind the probability\nthat a candidate has attended edger Acres training.\nAlso has good package.\nNow.\nThis is obviously a joint probability problem, right?\nSo how do you calculate this now?\nSince our table is quite formatted we can directly find\nthat people who have gotten a good package\nalong with Eddie record raining or 30, right?\nSo out of hundred and five people 30 people\nhave education training and a good package, right?\nThey specifically asking for people\nwith Eddie record raining.\nRemember that night.\nThe question is find the probability that a gang Today,\nit has attended editor Acres training\nand also has a good package.\nAll right, so we need to consider two factors\nthat is a candidate\nwho's addenda deaderick has training and\nwho has a good package.\nSo clearly that number\nis 30 30 divided by total number of candidates,\nwhich is 1:05, right?\nSo here you get the answer clearly next.\nWe have find the probability\nthat a candidate has a good package given\nthat he has not undergone training.\nOkay.\nNow this is Early conditional probability\nbecause here you're defining a condition you're saying\nthat you want to find the probability of a candidate\nwho has a good package given that he's not undergone.\nAny training, right?\nThe condition is that he's not undergone any training.\nAll right.\nSo the number of people\nwho have not undergone training are 60 and out\nof that five of them have got a good package\nthat so that's why this is Phi by 60 and not five\nby a hundred and five\nbecause here they have clearly mentioned has a good pack.\nGiven that he has not undergone training.\nSo you have to only\nconsider people who have not undergone training, right?\nSo any five people\nwho have not undergone training have gotten\na good package, right?\nSo 5 divided by 60 you get a probability of around 0.08\nwhich is pretty low, right?\nOkay.\nSo this was all\nabout the different types of probability now,\nlet's move on and look at our last Topic in probability,\nwhich is base theorem.\nNow guys base.\nYour room is a very important concept when it comes\nto statistics and probability.\nIt is majorly used in knife bias algorithm.\nThose of you who aren't aware.\nNow I've bias is a supervised learning classification\nalgorithm and it is mainly used in Gmail spam filtering right?\nA lot of you might have noticed that if you open up Gmail,\nyou'll see that you have a folder called spam right\nor that is carried out through machine learning\nand And the algorithm use there is knife bias, right?\nSo now let's discuss what exactly the Bayes theorem is\nand what it denotes the bias theorem is used\nto show the relation between one conditional probability\nand it's inverse.\nAll right.\nBasically it's nothing but the probability\nof an event occurring based on prior knowledge of conditions\nthat might be related to the same event.\nOkay.\nSo mathematically the bell's theorem is represented\nlike this right now.\nShown in this equation.\nThe left-hand term is referred to as the likelihood ratio\nwhich measures the probability\nof occurrence of event be given an event a okay\non the left hand side is\nwhat is known as the posterior right\nis referred to as posterior,\nwhich means that the probability\nof occurrence of a given an event be right.\nThe second term is referred\nto as the likelihood Ratio or at this measures the probability\nof occurrence of B given an event.\nA now P of a is also known as the prior\nwhich refers to the actual probability distribution of A\nand P of B is again,\nthe probability of B, right.\nThis is the bias theorem\nand in order to better understand the base theorem.\nLet's look at a small example.\nLet's say that we have three bowels we have bow is\na bow will be and bouncy.\nOkay barley contains two blue balls\nand for red balls bowel B\ncontains eight blue balls and for red balls.\nWow Zeke.\nGames one blue ball and three red balls now\nif we draw one ball from each Bowl,\nwhat is the probability to draw a blue ball\nfrom a bowel a if we know\nthat we drew exactly a total of two blue balls, right?\nIf you didn't understand the question,\nplease read it I shall pause for a second or two.\nRight.\nSo I hope all of you have understood the question.\nOkay.\nNow what I'm going to do is I'm going to draw\na blueprint for you\nand tell you how exactly to solve the problem.\nBut I want you all to give me the solution\nto this problem, right?\nI'll draw a blueprint.\nI'll tell you what exactly the steps are\nbut I want you to come up with a solution\non your own right the formula is also given to you.\nEverything is given to you.\nAll you have to do is come up with the final answer.\nRight?\nLet's look at how you can solve this problem.\nSo first of all,\nwhat we will do is Let's consider a all right,\nlet a be the event\nof picking a blue ball from bag in and let\nX be the event of picking exactly two blue balls,\nright because these are the two events\nthat we need to calculate the probability of now\nthere are two probabilities that you need to consider here.\nOne is the event of picking a blue ball from bag a\nand the other is the event of picking exactly two blue balls.\nOkay.\nSo these two are represented by a and X respectively\nand so what we want is the probability of occurrence\nof event a given X,\nwhich means that given\nthat we're picking exactly two blue balls.\nWhat is the probability\nthat we are picking a blue ball from bag?\nSo by the definition of conditional probability,\nthis is exactly what our equation will look like.\nCorrect.\nThis is basically a occurrence of event a given element X\nand this is the probability of a and x\nand this is the probability of X alone, correct.\nWhat we need to do is we need to find these two probabilities\nwhich is probability of a and X occurring together\nand probability of X. Okay.\nThis is the entire solution.\nSo how do you find P probability\nof X this you can do in three ways.\nSo first is white ball from a either white from be\nor read from see now first is to find the probability of x x\nbasically represents the event\nof picking exactly two blue balls.\nRight.\nSo these are the three ways in which it is possible.\nSo you'll pick one blue ball from bowel a and one from bowel\nbe in the second case.\nYou can pick one\nfrom a and another blue ball from see in the third case.\nYou can pick a blue ball from Bagby\nand a blue ball from bagsy.\nRight?\nThese are the three ways in which it is possible.\nSo you need to find the probability of each\nof this step do is\nthat you need to find the probability of a\nand X occurring together.\nThis is the sum of terms one and two.\nOkay, this is\nbecause in both of these events,\nyou're picking a ball from bag, correct?\nSo there is find out this probability and let\nme know your answer in the comment section.\nAll right.\nWe'll see if you get the answer right?\nI gave you the entire solution to this.\nAll you have to do is substitute the value right?\nIf you want a second or two,\nI'm going to pause on the screen so that you can go through this\nin a more clearer way right?\nRemember that you need to calculate two.\nHe's the first probability\nthat you need to calculate is the event of picking a blue ball\nfrom bag a given\nthat you're picking exactly two blue balls.\nOkay, II probability you need to calculate is the event\nof picking exactly to bluebirds.\nAll right.\nThese are the two probabilities.\nYou need to calculate so remember that and this\nis the solution.\nAll right, so guys,\nmake sure you mention your answers\nin the comment section for now.\nLet's move on and Get our next topic,\nwhich is the inferential statistics.\nSo guys, we just completed the probability module right now.\nWe will discuss inferential statistics,\nwhich is the second type of Statistics.\nWe discussed descriptive statistics earlier.\nAll right.\nSo like I mentioned earlier inferential statistics also\nknown as statistical inference is a branch of Statistics\nthat deals with forming inferences and predictions\nabout a population based on a sample of data.\nTaken from the population.\nAll right, and the question you should ask is\nhow does one form inferences or predictions on a sample?\nThe answer is you use Point estimation?\nOkay.\nNow you must be wondering\nwhat is point estimation one estimation is concerned\nwith the use of the sample data to measure a single value\nwhich serves as an approximate value\nor the best estimate of an unknown population parameter.\nThat's a little confusing.\nLet me break it down to you for Camping\nin order to calculate the mean of a huge population.\nWhat we do is we first draw out the sample of the population\nand then we find the sample mean\nright the sample mean is then used to estimate\nthe population mean this is basically Point estimate,\nyou're estimating the value of one of the parameters\nof the population, right?\nBasically the main\nyou're trying to estimate the value of the mean.\nThis is what point estimation is the two main terms\nin point estimation.\nThere's something known as\nas the estimator and the something known\nas the estimate estimator is a function of the sample\nthat is used to find out the estimate.\nAlright in this example.\nIt's basically the sample mean right so a function\nthat calculates the sample mean is known as the estimator\nand the realized value\nof the estimator is the estimate right?\nSo I hope Point estimation is clear.\nNow, how do you find the estimates?\nThere are four common ways in which you can do this.\nThe first one is method of Moment yo,\nwhat you do is you form an equation\nin the sample data set\nand then you analyze the similar equation\nin the population data set\nas well like the population mean population variance and so on.\nSo in simple terms,\nwhat you're doing is you're taking down some known facts\nabout the population\nand you're extending those ideas to the sample.\nAlright, once you do that,\nyou can analyze the sample and estimate more\nessential or more complex values right next.\nWe have maximum likelihood.\nThis method basically uses a model to estimate a value.\nAll right.\nNow a maximum likelihood is majorly based on probability.\nSo there's a lot of probability involved in this method next.\nWe have the base estimator this works by minimizing\nthe errors or the average risk.\nOkay, the base estimator has a lot to do\nwith the Bayes theorem.\nAll right, let's not get into the depth\nof these estimation methods.\nFinally.\nWe have the best unbiased estimators in this method.\nThere are seven unbiased estimators that can be used\nto approximate a parameter.\nOkay.\nSo Guys these were a couple of methods\nthat are used to find the estimate\nbut the most well-known method to find the estimate is known as\nthe interval estimation.\nOkay.\nThis is one\nof the most important estimation methods right?\nThis is where confidence interval also comes\ninto the picture right apart from interval estimation.\nWe also have something known as margin of error.\nSo I'll be discussing all of this.\nIn the upcoming slides.\nSo first let's understand.\nWhat is interval estimate?\nOkay, an interval or range of values,\nwhich are used to estimate a population parameter is known as\nan interval estimation, right?\nThat's very understandable.\nBasically what they're trying to see is you're going to estimate\nthe value of a parameter.\nLet's say you're trying to find the mean of a population.\nWhat you're going to do is you're going to build a range\nand your value will lie in that range or in that interval.\nAlright, so this way your output is going to be more accurate\nbecause you've not predicted a point estimation instead.\nYou have estimated an interval\nwithin which your value might occur, right?\nOkay.\nNow this image clearly shows\nhow Point estimate and interval estimate or different\nso guys interval estimate is obviously more accurate\nbecause you are not just focusing on a particular value\nor a particular point\nin order to predict the probability instead.\nYou're saying that the value might be\nwithin this range between the lower confidence limit\nand the upper confidence limit.\nAll right, this is denotes the range or the interval.\nOkay, if you're still confused about interval estimation,\nlet me give you a small example\nif I stated that I will take 30 minutes to reach the theater.\nThis is known as Point estimation.\nOkay, but if I stated\nthat I will take between 45 minutes\nto an hour to reach the theater.\nThis is an example of into Estimation.\nAll right.\nI hope it's clear.\nNow now interval estimation gives rise to two important\nstatistical terminologies one is known as confidence interval\nand the other is known as margin of error.\nAll right.\nSo there's it's important\nthat you pay attention\nto both of these terminologies confidence interval is one\nof the most significant measures\nthat are used to check\nhow essential machine learning model is.\nAll right.\nSo what is confidence interval confidence interval is\nthe measure of your confidence\nthat the interval estimated contains\nthe population parameter or the population mean\nor any of those parameters right now statisticians\nuse confidence interval to describe the amount\nof uncertainty associated\nwith the sample estimate of a population parameter now guys,\nthis is a lot of definition.\nLet me just make you understand confidence interval\nwith a small example.\nOkay.\nLet's say that you perform\na survey and you survey a group of cat owners.\nThe see how many cans of cat food they purchase\nin one year.\nOkay, you test\nyour statistics at the 99 percent confidence level\nand you get a confidence interval\nof hundred comma 200 this means\nthat you think\nthat the cat owners\nby between hundred to two hundred cans in a year and also\nsince the confidence level is 99% shows\nthat you're very confident that the results are, correct.\nOkay.\nI hope all of you are clear with that.\nAlright, so your confidence interval here will be\na hundred and two hundred\nand your confidence level will be 99% Right?\nThat's the difference between confidence interval\nand confidence level So within your confidence interval\nyour value is going to lie and your confidence level will show\nhow confident you are about your estimation, right?\nI hope that was clear.\nLet's look at margin of error.\nNo margin of error\nfor a given level of confidence is a greatest possible distance\nbetween the Point estimate\nand the value of the parameter\nthat it is estimating you can say\nthat it is a deviation from the actual point estimate right.\nNow.\nThe margin of error can be calculated\nusing this formula now zc her denotes the critical value\nor the confidence interval\nand this is X standard deviation divided by root\nof the sample size.\nAll right, n is basically the sample size now,\nlet's understand how you can estimate\nthe confidence intervals.\nSo guys the level of confidence\nwhich is denoted by C is the probability\nthat the interval estimate contains a population parameter.\nLet's say that you're trying to estimate the mean.\nAll right.\nSo the level of confidence is the probability\nthat the interval estimate contains\nthe population parameter.\nSo this interval between minus Z and z\nor the area beneath this curve is nothing but the probability\nthat the interval estimate contains a population parameter.\nYou don't all right.\nIt should basically contain the value\nthat you are predicting right.\nNow.\nThese are known as critical values.\nThis is basically your lower limit\nand your higher limit confidence level.\nAlso, there's something known as the Z score now.\nThis court can be calculated by using the standard normal table.\nAll right, if you look it up anywhere on Google\nyou'll find the z-score table\nor the standard normal table to understand\nhow this is done.\nLet's look at a small example.\nOkay, let's say that the level of confidence.\nVince is 90% This means that you are 90% confident\nthat the interval contains the population mean.\nOkay, so the remaining 10% which is out of hundred percent.\nThe remaining 10% is equally distributed\non these tail regions.\nOkay, so you have 0.05 here and 0.05 over here, right?\nSo on either side\nof see you will distribute the other leftover percentage\nnow these Z scores are calculated from the table\nas I mentioned before.\nAll right one.\nI'm 6 4 5 is get collated from the standard normal table.\nOkay, so guys how you estimate the level of confidence?\nSo to sum it up.\nLet me tell you the steps that are involved in constructing\na confidence interval first.\nYou would start by identifying a sample statistic.\nOkay.\nThis is the statistic\nthat you will use to estimate a population parameter.\nThis can be anything like the mean\nof the sample next you will select a confidence level\nnow the confidence level describes the uncertainty\nof a Sampling method right\nafter that you'll find something known as the margin\nof error right?\nWe discussed margin of error earlier.\nSo you find this based on the equation\nthat I explained in the previous slide,\nthen you'll finally specify the confidence interval.\nAll right.\nNow, let's look at a problem statement\nto better understand this concept a random sample\nof 32 textbook prices is taken from a local College Bookstore.\nThe mean of the sample is so so\nand so and the sample standard deviation is\nThis use a 95% confident level\nand find the margin of error for the mean price\nof all text books in the bookstore.\nOkay.\nNow, this is a very straightforward question.\nIf you want you can read the question again.\nAll you have to do is you have to just substitute the values\ninto the equation.\nAll right, so guys,\nwe know the formula for margin of error you take the Z score\nfrom the table.\nAfter that we have deviation Madrid's 23.4 for right\nand that's standard deviation and n stands for the number\nof samples here.\nThe number of samples is 32 basically 32 textbooks.\nSo approximately your margin of error is going to be\naround 8.1 to this is a pretty simple question.\nAll right.\nI hope all of you understood this now\nthat you know,\nthe idea behind confidence interval.\nLet's move ahead to one\nof the most important topics in statistical inference,\nwhich is hypothesis testing, right?\nSo Ugly statisticians use hypothesis testing\nto formally check\nwhether the hypothesis is accepted or rejected.\nOkay, hypothesis.\nTesting is an inferential statistical technique\nused to determine\nwhether there is enough evidence in a data sample to infer\nthat a certain condition holds true for an entire population.\nSo to understand\nthe characteristics of a general population,\nwe take a random sample,\nand we analyze the properties of the sample right we test.\nWhether or not the identified conclusion represents\nthe population accurately\nand finally we interpret the results now\nwhether or not to accept the hypothesis depends\nupon the percentage value that we get from the hypothesis.\nOkay, so to better understand this,\nlet's look at a small example before that.\nThere are a few steps that are followed\nin hypothesis testing you begin by stating the null\nand the alternative hypothesis.\nAll right.\nI'll tell you what exactly these terms are\nand then you formulate.\nAnalysis plan right after that you analyze the sample data\nand finally you can interpret the results\nright now to understand the entire hypothesis testing.\nWe look at a good example.\nOkay now consider for boys Nick jean-bob\nand Harry these boys were caught bunking a class\nand they were asked to stay back at school\nand clean the classroom as a punishment, right?\nSo what John did is he decided\nthat four of them would take turns to clean their classrooms.\nHe came up with a plan of writing each\nof their names on chits\nand putting them\nin a bowl now every day they had to pick up a name from the bowel\nand that person had to play in the clock, right?\nThat sounds pretty fair enough now it is been three days\nand everybody's name has come up except John's assuming\nthat this event is completely random\nand free of bias.\nWhat is a probability\nof John not cheating right or is the probability\nthat he's not actually cheating this can Solved\nby using hypothesis testing.\nOkay.\nSo we'll Begin by calculating the probability of John\nnot being picked for a day.\nAlright, so we're going to assume\nthat the event is free of bias.\nSo we need to find out the probability\nof John not cheating right first we will find the probability\nthat John is not picked for a day, right?\nWe get 3 out of 4,\nwhich is basically 75% 75% is fairly high.\nSo if John is not picked for three days in a row\nthe Probability will drop down to approximately 42% Okay.\nSo three days in a row meaning\nthat is the probability drops down to 42 percent.\nNow, let's consider a situation\nwhere John is not picked for 12 days in a row\nthe probability drops down to three point two percent.\nOkay.\nThat's the probability of John cheating becomes\nfairly high, right?\nSo in order\nfor statisticians to come to a conclusion,\nthey Define what is known as a threshold value.\nRight considering the above situation\nif the threshold value is set to 5 percent.\nIt would indicate\nthat if the probability lies below 5% then John is cheating\nhis way out of detention.\nBut if the probability is about threshold value then John\nit just lucky and his name isn't getting picked.\nSo the probability\nand hypothesis testing give rise to two important components\nof hypothesis testing,\nwhich is null hypothesis and alternative hypothesis.\nNull.\nHypothesis is based.\nBasically approving\nthe Assumption alternate hypothesis is\nwhen your result disapproves the Assumption right therefore\nin our example,\nif the probability of an event occurring\nis less than 5% which it is then the event is biased hence.\nIt proves the alternate hypothesis.\nSo guys with this we come to the end of this session.\nLet's go ahead and understand what exactly is.\nWas learning so supervised learning is\nwhere you have the input variable X\nand the output variable Y and use an algorithm\nto learn the map Egg function from the input to the output\nas I mentioned earlier\nwith the example of face detection.\nSo it is called supervised learning\nbecause the process of an algorithm learning\nfrom the training data set can be thought\nof as a teacher supervising the learning process.\nSo if we have a look at the supervised learning steps\nor What would rather say the workflow?\nSo the model is used as you can see here.\nWe have the historic data.\nThen we again we have the random sampling.\nWe split the data into train your asset\nand the testing data set using the training data set.\nWe with the help of machine learning\nwhich is supervised machine learning.\nWe create statistical model then\nafter we have a mod which is being generated\nwith the help of the training data set.\nWhat we do is use the testing data set\nfor production and testing.\nWhat we do is get the output\nand finally we have the model validation outcome.\nThat was the training and testing.\nSo if we have a look at the prediction part\nof any particular supervised learning algorithm,\nso the model is used for operating outcome\nof a new data set.\nSo whenever performance\nof the model degraded the model is retrained\nor if there are any performance issues,\nthe model is retained with the help of the new data now\nwhen we talk about supervisor\nin there not just one but quite a few algorithms here.\nSo we have linear regression logistic regression.\nThis is entry.\nWe have random Forest.\nWe have made by classifiers.\nSo linear regression is used to estimate real values.\nFor example, the cost of houses.\nThe number of calls the total sales based\non the continuous variables.\nSo that is what reading regression is.\nNow when we talk about logistic regression,\nwhich is used to estimate discrete values, for example,\nwhich are binary values like 0 and 1 yes,\nor no true.\nFalse based on the given set of independent variables.\nSo for example,\nwhen you are talking about something like the chances\nof winning or if you talk\nabout winning which can be either true or false\nif will it rain today with it can be the yes or no,\nso it cannot be\nlike when the output of a particular algorithm\nor the particular question is either.\nYes.\nNo or Banner e then\nonly we use a large stick regression the next\nwe have decision trees.\nSo now these are used for classification problems it work.\nX for both categorical and continuous\ndependent variables and\nif we talk about random Forest So Random Forest is an M symbol\nof a decision tree,\nit gives better prediction accuracy than decision tree.\nSo that is another type of supervised learning algorithm.\nAnd finally we have the need based classifier.\nSo it was\na classification technique based on the Bayes theorem\nwith an assumption of Independence between predictors.\nA linear regression is one of the easiest algorithm\nin machine learning.\nIt is a statistical model\nthat attempts to show the relationship\nbetween two variables with a linear equation.\nBut before we drill down\nto linear regression algorithm in depth,\nI'll give you a quick overview of today's agenda.\nSo we'll start a session\nwith a quick overview of what is regression\nas linear regression is one of a type\nof regression algorithm.\nOnce we learn about regression,\nits use case the various types of it next.\nWe'll learn about the algorithm from scratch.\nEach where I'll teach\nyou it's mathematical implementation first,\nthen we'll drill down to the coding part\nand Implement linear regression using python\nin today's session will deal\nwith linear regression algorithm using least Square method check\nits goodness of fit\nor how close the data is\nto the fitted regression line using the R square method.\nAnd then finally\nwhat will do will optimize it using the gradient decent method\nin the last part on the coding session.\nI'll teach you to implement linear regression using Python\nand Coding session\nwould be divided into two parts the first part would consist\nof linear regression using python from scratch\nwhere you will use the mathematical algorithm\nthat you have learned in this session.\nAnd in the next part of the coding session\nwill be using scikit-learn for direct implementation\nof linear regression.\nSo let's begin our session with what is regression.\nWell regression analysis is\na form of predictive modeling technique\nwhich investigates the relationship between a dependent\nand independent variable a regression analysis.\nVols graphing a line over a set of data points\nthat most closely fits the overall shape of the data\nor regression shows the changes in a dependent variable\non the y-axis\nto the changes\nin the explanation variable on the x-axis fine.\nNow you would ask what are the uses of regression?\nWell, there are major three uses of regression analysis the first\nbeing determining the strength of predicates errs,\nthe regression might be used to identify the strength\nof the effect\nthat the independent variables have on the dependent variable\nor But you can ask question.\nLike what is the strength of relationship between sales\nand marketing spending or what is the relationship between age\nand income second is forecasting\nan effect in this the regression can be used to forecast effects\nor impact of changes.\nThat is the regression analysis help us to understand\nhow much the dependent variable changes with the change\nand one or more independent variable fine.\nFor example, you can ask question like how much\nadditional say Lancome will I get for each?\nThousand dollars spent on marketing.\nSo it is Trend forecasting\nin this the regression analysis predict Trends\nand future values.\nThe regression analysis can be used to get Point estimates\nin this you can ask questions.\nLike what will be the price of Bitcoin\nand next six months, right?\nSo next topic is linear versus logistic regression by now.\nI hope that you know, what a regression is.\nSo let's move on and understand its type.\nSo there are various kinds of regression like linear\nregression logistic regression polynomial regression.\nOthers only but for this session\nwill be focusing on linear and logistic regression.\nSo let's move on and let me tell you what is linear regression.\nAnd what is logistic regression\nthen what we'll do we'll compare both of them.\nAll right.\nSo starting with linear regression\nin simple linear regression,\nwe are interested in things like y equal MX plus C.\nSo what we are trying to find is the correlation between X\nand Y variable this means\nthat every value of x\nhas a corresponding value of y and it\nif it is continuous.\nAll right, however in logistic regression,\nwe are not fitting our data to a straight line\nlike linear regression instead what we are doing.\nWe are mapping Y versus X\nto a sigmoid function in logistic regression.\nWhat we find out is is y 1 or 0 for this particular value of x\nso thus we are essentially deciding true or false value\nfor a given value of x fine.\nSo as a core concept of linear regression,\nyou can say that the data is modeled using a straight.\nBut in the case of logistic regression\nthe data is module using a sigmoid function.\nThe linear regression is used with continuous variables\non the other hand the logistic regression.\nIt is used with categorical variable the output\nor the prediction of a linear regression\nis a value of the variable\non the other hand the output of production\nof a logistic regression is the probability\nof occurrence of the event.\nNow, how will you check the accuracy\nand goodness of fit in case of linear regression?\nWe are various methods like measured by loss R square.\nAre adjusted r squared Etc\nwhile in the case of logistic regression you\nhave accuracy precision recall F1 score,\nwhich is nothing but the harmonic mean of precision\nand recall next is Roc curve\nfor determining the probability threshold for classification\nor the confusion Matrix Etc.\nThere are many all right.\nSo summarizing the difference\nbetween linear and logistic regression.\nYou can say\nthat the type of function you are mapping to is the main point\nof difference between linear and logistic regression\na linear regression model.\nThe Continuous X2 a continuous file on the other hand\na logistic regression Maps a continuous x\nto the bindery why\nso we can use logistic regression to make category\nor true false decisions from the data find\nso let's move on ahead next is linear\nregression selection criteria,\nor you can say when will you use linear regression?\nSo the first is classification\nand regression capabilities regression models predict\na continuous variable such as the sales made on a day\nor predict the temperature\nof a city T their Reliance on a polynomial\nlike a straight line to fit a data set\nposes a real challenge\nwhen it comes towards building a classification capability.\nLet's imagine that you fit a line with a train points\nthat you have now imagine you add some more data points to it.\nBut in order to fit it, what do you have to do?\nYou have to change your existing model\nthat is maybe you have to change the threshold itself.\nSo this will happen with each new data point you are\nto the model hence.\nThe linear regression is not good for classification models.\nFine.\nNext is data quality.\nEach missing value removes one data point\nthat could optimize\nthe regression and simple linear regression.\nThe outliers can significantly\ndisrupt the outcome just for now.\nYou can know that if you remove the outliers your model\nwill become very good.\nAll right.\nSo this is about data quality.\nNext is computational complexity a linear regression is often\nnot computationally expensive as compared to the decision tree\nor the clustering algorithm the order of complexity\nfor n training example and X features usually Falls\nin either Big O of x\nOr bigger of xn next is comprehensible\nand transparent the linear regression are\neasily comprehensible and transparent in nature.\nThey can be represented by a simple mathematical notation\nto anyone and can be understood very easily.\nSo these are some of the criteria based\non which you will select the linear regression algorithm.\nAll right.\nNext is where is linear regression used first\nis evaluating trans and sales estimate.\nWell linear regression can be used in business\nto evaluate Trends and make estimates.\nForecast for example,\nif a company sales have increased steadily every month\nfor past few years then conducting a linear analysis\non the sales data with monthly sales on the y axis\nand time on the x axis.\nThis will give you a line\nthat predicts the upward Trends in the sale after creating\nthe trendline the company could use the slope\nof the lines to focused sale in future months.\nNext is analyzing.\nThe impact of price changes will linear regression\ncan be used to analyze the effect of pricing\non Omer behavior for instance,\nif a company changes\nthe price on a certain product several times,\nthen it can record the quantity itself for each price level\nand then perform a linear regression\nwith sold quantity as\na dependent variable and price as the independent variable.\nThis would result in a line that depicts the extent\nto which the customer reduce their consumption of the product\nas the prices increasing.\nSo this result would help us in future pricing decisions.\nNext is assessment of risk in financial services\nand insurance domain.\nLinear regression can be used to analyze the risk,\nfor example health insurance company might conduct\na linear regression algorithm\nhow it can do it can do it by plotting the number of claims\nper customer against its age and they might discover\nthat the old customers\ntend to make more health insurance claim.\nWell the result of such analysis might guide\nimportant business decisions.\nAll right, so by now you have just a rough idea of\nwhat linear regression algorithm as like\nwhat it does where it is used when You should use it early.\nNow.\nLet's move on and understand the algorithm\nand depth so suppose you have independent variable\non the x-axis and dependent variable on the y-axis.\nAll right suppose.\nThis is the data point on the x axis.\nThe independent variable is increasing on the x-axis.\nAnd so does the dependent variable on the y-axis?\nSo what kind of linear regression line you would get\nyou would get a positive linear regression line.\nAll right as the slope would be positive next is suppose.\nYou have an independent variable on the X axis\nwhich is increasing\nand on the other hand the dependent variable on the y-axis\nthat is decreasing.\nSo what kind of line will you get in that case?\nYou will get a negative regression line.\nIn this case as the slope of the line is negative\nand this particular line\nthat is line of y equal MX\nplus C is a line of linear regression\nwhich shows the relationship between independent variable\nand dependent variable\nand this line is only known as line of linear regression.\nOkay.\nSo let's add some data points, too.\nOur graph so these are some observation\nor data points on our graph.\nSo let's plot some more.\nOkay.\nNow all our data points are plotted now our task is\nto create a regression line or the best fit line.\nAll right now\nonce our regression line is drawn now,\nit's the task of production now suppose.\nThis is our estimated value or the predicted value\nand this is our actual value.\nOkay.\nSo what we have to do our main goal is to reduce this error\nthat is to reduce the distance\nbetween the Estimated or the predicted value\nand the actual value the best fit line would be the one\nwhich had the least error\nor the least difference in estimated value\nand the actual value.\nAll right, and other words we have to minimize the error.\nThis was a brief understanding\nof linear regression algorithm soon.\nWe'll jump towards mathematical implementation.\nBut for then let me tell you this suppose you draw a graph\nwith speed on the x-axis\nand distance covered\non the y axis with the time domain in constant.\nIf you plot a graph between the speed travel\nby the vehicle\nand the distance traveled in a fixed unit of time,\nthen you will get a positive relationship.\nAll right.\nSo suppose the equation of a line is y equal MX plus C.\nThen in this case Y is the distance traveled\nin a fixed duration of time x\nis the speed of vehicle m is the positive slope\nof the line and see is the y-intercept of the line.\nAll right suppose the distance remaining constant.\nYou have to plot a graph between the speed of the vehicle\nand the time taken to travel a fixed distance.\nThen in that case you will get a line\nwith a negative relationship.\nAll right, the slope of the line is negative here the equation\nof line changes to y equal minus of MX plus C\nwhere Y is the time taken to travel\na fixed distance X is the speed\nof vehicle m is the negative slope\nof the line and see is the y-intercept of the line.\nAll right.\nNow, let's get back\nto our independent and dependent variable.\nSo in that term, why is our dependent variable\nand X that is our independent variable now,\nlet's move on.\nAnd see them at the magical implementation of the things.\nAlright, so we have x\nequal 1 2 3 4 5 let's plot them on the x-axis.\nSo 0 1 2 3 4 5 6 align and we have y as 3 4 2 4 5.\nAll right.\nSo let's plot 1\n2 3 4 5 on the y-axis now,\nlet's plot our coordinates 1 by 1 so x equal 1 and y equal 3,\nso we have here x\nequal 1 and y equal\n3 So this is the point 1 comma 3 so similarly\nwe have 1 3 2 4 3 2 4 4 &amp; 5 5.\nAlright, so moving on ahead.\nLet's calculate the mean of X and Y and plot it on the graph.\nAll right, so mean of X is 1\nplus 2 plus 3 plus 4 plus 5 divided by 5.\nThat is 3.\nAll right, similarly mean of Y is 3 plus 4 plus 2\nplus 4 plus 5 that is 18.\nSo we 10 divided by 5.\nThat is nothing but 3.6.\nAlright, so next what we'll do we'll plot.\nI mean that is 3 comma 3 .6 on the graph.\nOkay.\nSo there's a point 3 comma 3 .6\nsee our goal is to find or predict the best fit line\nusing the least Square Method All right.\nSo in order to find\nthat we first need to find the equation of line,\nso let's find the equation of our regression line.\nAlright, so let's suppose this is our regression line\ny equal MX plus C.\nNow.\nWe have an equation of line.\nSo all we need to do is find the value of M\nand C. I wear m equals summation of x\nminus X bar X Y minus y bar upon the summation of x\nminus X bar whole Square don't get confused.\nLet me resolve it for you.\nAll right.\nSo moving on ahead as a part of formula.\nWhat we are going to do will calculate x minus X bar.\nSo we have X as 1 minus X bar as 3 so 1 minus 3\nthat is minus 2 next.\nWe have x equal to minus its mean 3\nthat is minus 1 similarly we 3 -\n3 0 4 minus 3 1 5 - 3 2.\nAll right, so x minus X bar.\nIt's nothing but the distance of all the point\nthrough the line y equal 3\nand what does this y\nminus y bar implies it implies the distance\nof all the point from the line\nx equal 3 .6 fine.\nSo let's calculate the value of y minus y bar.\nSo starting with y equal 3 -\nvalue of y bar that is 3.6.\nSo it is three minus three. .6.\nHow much -\nof 0.6 next is 4 minus 3.6 that is 0.4 next to minus 3.6\nthat is - of 1.6.\nNext is 4 minus 3.6 that is 0.4 again,\n5 minus 3.6 that is 1.4.\nAlright, so now we are done with Y minus y bar fine now next\nwe will calculate x minus X bar whole Square.\nSo let's calculate x minus X bar whole Square\nso it is -\n2 whole square that is 4 minus 1 whole square.\nThat is 1 0 squared is 0 1 Square 1 2 square for fine.\nSo now in our table we have x minus X bar y minus y bar\nand x minus X bar whole Square.\nNow what we need.\nWe need the product of x minus X bar X Y minus y bar.\nAlright, so let's see the product of x\nminus X bar X Y minus\ny bar that is minus of 2 x minus of 0.6.\nThat is 1.2 minus of 1 x 0 point 4.\nThat is minus.\n- of zero point 4 0 x minus of 1.6.\nThat is 0 1 multiplied by zero point four\nthat is 0.4.\nAnd next 2 multiplied by 1 point for that is 2.8.\nAll right.\nNow almost all the parts of our formula is done.\nSo now what we need to do is get the summation\nof last two columns.\nAll right, so the summation of x minus X bar whole square is 10\nand the summation of x\nminus X bar X Y minus y bar is\nfor So the value of M will be equal to 4 by 10 fine.\nSo let's put this value\nof m equals zero point 4 and our line y equal MX plus C.\nSo let's file all the points into the equation\nand find the value of C.\nSo we have y as 3.6 remember the mean by m as 0.4\nwhich we calculated just now X as the mean value of x\nthat is 3 and we\nhave the equation as 3 point 6 equals 0 .4\nApplied by 3 plus C. Alright\nthat is 3.6 equal 1 Point 2 plus C.\nSo what is the value of C that is 3.6 minus 1.2.\nThat is 2.4.\nAll right.\nSo what we had we had m equals zero point four C as 2.4.\nAnd then finally\nwhen we calculate the equation of the regression line,\nwhat we get is y equal zero point four times of X\nplus two point four.\nSo this is the regression line.\nAll right, so there is\nhow you are plotting your points this Actual point.\nAll right now for given m equals zero point four and SQL 2.4.\nLet's predict the value of y for x equal 1 2 3 4 &amp; 5.\nSo when x equal 1 the predicted value\nof y will be zero point four x\none plus two point four that is 2.8.\nSimilarly when x equal to predicted value\nof y will be zero point 4 x\n2 + 2 point 4 that equals to 3 point 2 similarly x\nequal 3 y will be 3. .6.\nX equals 4 y will be 4 point 0 x\nequal 5 y will be four point four.\nSo let's plot them on the graph\nand the line passing through all these predicting point\nand cutting y-axis at 2.4 as the line of regression.\nNow your task is to calculate the distance between the actual\nand the predicted value\nand your job is to reduce the distance.\nAll right, or in other words,\nyou have to reduce the error between the actual\nand the predicted value the line\nwith the least error will be the line of linear regression.\nChicken or regression line\nand it will also be the best fit line.\nAll right.\nSo this is how things work in computer.\nSo what it do it performs n number of iteration\nfor different values of M for different values of M.\nIt will calculate the equation of line\nwhere y equals MX plus C.\nRight?\nSo as the value of M changes the line\nis changing so iteration will start from one.\nAll right, and it will perform a number of iteration.\nSo after every iteration\nwhat it will do it will calculate the predicted.\nValue according to the line and compare the distance\nof actual value to the predicted value\nand the value of M\nfor which the distance between the actual\nand the predicted value is minimum will be selected\nas the best fit line.\nAll right.\nNow that we have calculated the best fit line now,\nit's time to check the goodness of fit or to check\nhow good a model is performing.\nSo in order to do that,\nwe have a method called R square method.\nSo what is this R square?\nWell r-squared value is a statistical measure\nof how close the data are\nto the fitted regression line in general.\nIt is considered\nthat a high r-squared value model is a good model,\nbut you can also have a lower squared value\nfor a good model as well\nor a higher squared value for a model\nthat does not fit at all.\nI like it is also known as coefficient of determination\nor the coefficient of multiple determination.\nLet's move on and see how a square is calculated.\nSo these are our actual values plotted on the graph.\nWe had calculated the predicted values\nof Y as 2.8 3.2 3.6 4.0 4.4.\nRemember when we calculated the predicted values\nof Y for the equation Y predicted equals 0 1 4 x\nof X plus two point four for every x\nequal 1 2 3 4 &amp; 5 from there.\nWe got the Ed values of Phi all right.\nSo let's plot it on the graph.\nSo these are point and the line passing\nthrough these points are nothing but the regression line.\nAll right.\nNow what you need to do is\nyou have to check and compare the distance of actual -\nmean versus the distance of predicted - mean alike.\nSo basically what you are doing you are calculating the distance\nof actual value to the mean to distance of predicted value\nto the mean I like\nso there is nothing\nbut a square in mathematically you can represent our school.\nWhereas summation of Y predicted values minus y\nbar whole Square divided by summation of Y minus\ny bar whole Square\nwhere Y is the actual value y p is the predicted value\nand Y Bar is the mean value of y that is nothing but 3.6.\nRemember, this is our formula.\nSo next what we'll do we'll calculate y minus.\nY1.\nSo we have y is 3y bar as 3 point 6,\nso we'll calculate it as 3 minus 3.6\nthat is nothing but minus of 0.6 similarly for y equal 4\nand Y Bar equal 3.6.\nWe have y minus y bar as zero point 4 then 2 minus 3.6.\nIt is 1 point 6 4 minus 3.6 again zero point four\nand five minus 3.6\nit is 1.4.\nSo we got the value of y minus y bar.\nNow what we have to do we have to take it Square.\nSo we have minus 0.6 Square as 0.36 0.4 Square as 0.16 -\nof 1.6 Square as 2.56 0.4 Square as 0.16 and 1.4 squared\nis 1.96 now is a part of formula what we need.\nWe need our YP minus y BAR value.\nSo these are VIP values and we have to subtract it from the No,\nwhy so 2 .8 minus 3.6 that is minus 0.8.\nSimilarly.\nWe will get 3.2 minus 3.6 that is 0.4 and 3.6 minus 3.6.\nThat is 0 for 1 0 minus 3.6 that is 0.4.\nThen 4 .4 minus 3.6 that is 0.8.\nSo we calculated the value of YP minus y bar now,\nit's our turn to calculate the value of y b minus\ny bar whole Square next.\nWe have -\nof 0.8 Square as 0.64 - of Point four square as 0.160 Square\n0 0 point 4 Square as again 0.16 and 0.8 Square as 0.64.\nAll right.\nNow as a part of formula\nwhat it suggests it suggests me to take the summation of Y P\nminus y bar whole square\nand summation of Y minus y bar whole Square.\nAll right.\nLet's see.\nSo in submitting y minus y bar whole Square\nwhat you get is five point two and summation of Y P minus\ny bar whole Square you get one point six.\nSo the value of R square can be calculated as\n1 point 6 upon 5.2 fine.\nSo the result which will get is approximately equal to 0.3.\nWell, this is not a good fit.\nAll right, so it suggests\nthat the data points are far away from the regression line.\nAlright, so this is\nhow your graph will look like when R square is 0.3\nwhen you increase the value of R square to 0.7.\nSo you'll see\nthat the actual value would like closer to the regression line\nwhen it reaches to 0.9 it comes.\nMore clothes and when the value of approximately equals\nto 1 then the actual values lies on the regression line itself,\nfor example, in this case.\nIf you get a very low value of R square suppose 0.02.\nSo in that case what will see\nthat the actual values are very far away\nfrom the regression line or you can say\nthat there are too many outliers in your data.\nYou cannot focus and thing from the data.\nAll right.\nSo this was all about the calculation of our Square now,\nyou might get a question like are low values\nof Square always bad.\nWell in some field it is entirely expected that I ask\nwhere value will be low.\nFor example any field\nthat attempts to predict human behavior such as psychology\ntypically has r-squared values lower than around 50%\nthrough which you can conclude\nthat humans are simply harder\nto predict the under physical process furthermore.\nIf you ask what value is low,\nbut you have statistically significant predictors,\nthen you can still draw important conclusion\nabout how changes in the predicator values associated.\nCreated with the changes in the response value regardless\nof the r-squared\nthe significant coefficient still represent the mean change\nin the response for one unit of change in the predicator\nwhile holding other predicated in the model constant.\nObviously this type\nof information can be extremely valuable.\nAll right.\nAll right.\nSo this was all about the theoretical concept now,\nlet's move on to the coding part\nand understand the code in depth.\nSo for implementing linear regression using python,\nI will be using Anaconda\nwith jupyter notebook installed on it.\nSo I like there's a jupyter notebook\nand we are using python 3.0 on it.\nAlright, so we are going to use a data set consisting\nof head size and human brain of different people.\nAll right.\nSo let's import our data set percent matplotlib and line.\nWe are importing numpy\nas NP pandas as speedy and matplotlib and from matplotlib.\nWe are importing pipe lot of that as PLT.\nAlright next we will import our data had brain dot CSV\nand store it in the database table.\nLet's execute the Run button and see the armor.\nBut so this task symbol it symbolizes\nthat it still executing.\nSo there's a output our data set consists\nof two thirty seven rows and 4 columns.\nWe have columns as gender age range head size\nin centimeter Cube\nand brain weights and Graham fine.\nSo there's our sample data set.\nThis is how it looks it consists of all these data set.\nSo now that we have imported our data,\nso as you can see they are 237 values in the training set\nso we can find a linear.\nRelationship between the head size and the Brain weights.\nSo now what we'll do we'll collect X &amp; Y\nthe X would consist of the head size values\nand the Y would consist of brain with values.\nSo collecting X and Y. Let's execute the Run.\nDone next what we'll do we need to find the values of b 1\nor B not or you can say m and C.\nSo we'll need the mean of X and Y values first of all\nwhat we'll do we'll calculate the mean of X and Y so mean x\nequal NP dot Min X.\nSo mean is a predefined function of Numb by similarly mean\nunderscore y equal NP dot mean of Y,\nso what it will return\nif you'll return the mean values of Y\nnext we'll check the total number of values.\nSo m equals.\nWell length of X. Alright,\nthen we'll use the formula to calculate the values\nof b 1 and B naught or MNC.\nAll right, let's execute the Run button and see\nwhat is the result.\nSo as you can see here on the screen,\nwe have got d 1 as 0 point 2 6 3\nand be not as three twenty five point five seven.\nAlright, so now that we have a coefficient.\nSo comparing it with the equation y equal MX plus C.\nYou can say that brain weight equals\nzero point 2 6 3 X Head size plus three twenty five point\nfive seven so you can say\nthat the value of M here is zero point\n2 6 3 and the value of C.\nHere is three twenty five point five seven.\nAll right, so there's our linear model now,\nlet's plot it and see graphically.\nLet's execute it.\nSo this is how our plot looks like this model is not so bad.\nBut we need to find out how good our model has.\nSo in order to find it the many methods\nlike root mean Square method the coefficient of determination\nor the a square method.\nSo in this tutorial,\nI have told you about our score method.\nSo let's focus on that and see how good our model is.\nSo let's calculate the R square value.\nAll right here SS underscore T is the total sum of square SS.\nI is the total sum of square of residuals and R square\nas the formula is 1 minus total sum\nof squares upon total sum of square of the residuals.\nAll right next when you execute it,\nyou will get the value of R square as 0.63\nwhich is pretty very good.\nNow that you have implemented simple linear regression model\nusing least Square method,\nlet's move on and see\nhow will you implement the model using machine learning library\ncalled scikit-learn.\nAll right.\nSo this scikit-learn is a simple machine.\nOwning library in Python welding machine learning model are\nvery easy using scikit-learn.\nSo suppose there's a python code.\nSo using the scikit-learn libraries your code shortens\nto this length\nlike so let's execute\nthe Run button and see you will get the same our to score.\nSo today we'll be discussing logistic regression.\nSo let's move forward\nand understand the what and by of logistic regression.\nNow this algorithm is most widely used\nwhen the dependent variable\nor you can see the output is in the binary format.\nAnd so here you need to predict the outcome\nof a categorical dependent variable.\nSo the outcome should be always discreet or categorical\nin nature Now by discrete.\nI mean the value should be binary\nor you can say you just have two values it can either be 0\nor 1 you can either be yes\nor a no either be true or false or high or low.\nSo only these can be the outcomes so the value\nwhich you need to protect should be discrete\nor you can say categorical in nature.\nWhereas in linear regression.\nWe have the value of by or you can say the value.\nTwo predictors in a Range\nthat is how there's a difference between linear regression\nand logistic regression.\nWe must be having question.\nWhy not linear regression now guys in linear regression\nthe value of buyer\nor the value which you need to predict is in a range,\nbut in our case as in the logistic regression,\nwe just have two values it can be either 0\nor it can be one.\nIt should not entertain the values which is\nbelow zero or above one.\nBut in linear regression,\nwe have the value of y in the range so here in order\nto implement logic regression.\nWe need to clip this This part so we don't need the value\nthat is below zero or we don't need the value\nwhich is above 1\nso since the value of y will be between only 0 and 1\nthat is the main rule of logistic regression.\nThe linear line has to be clipped at zero and one now.\nOnce we clip this graph it would look somewhat like this.\nSo here you are getting the curve\nwhich is nothing but three different straight lines.\nSo here we need to make a new way to solve this problem.\nSo this has to be formulated into equation\nand hence we come up with logistic regression.\nSo here the outcome is either 0 or 1.\nWhich is the main rule of logistic regression.\nSo with this our resulting curve cannot be formulated.\nSo hence our main aim to bring the values to 0\nand 1 is fulfilled.\nSo that is how we came up with large stick regression now here\nonce it gets formulated into an equation.\nIt looks somewhat like this.\nSo guys, this is nothing but an S curve\nor you can say the sigmoid curve a sigmoid function curve.\nSo this sigmoid function basically converts any value\nfrom minus infinity to Infinity pure discrete values,\nwhich a Logitech regression wants or you can say the Values\nwhich are in binary format either 0 or 1.\nSo if you see here the values and either 0\nor 1 and this is nothing but just a transition of it,\nbut guys there's a catch over here.\nSo let's say I have a data point that is 0.8.\nNow, how can you decide\nwhether your value is 0\nor 1 now here you have the concept\nof threshold which basically divides your line.\nSo here threshold value\nbasically indicates the probability of either winning\nor losing so here by winning.\nI mean the value is equals to 1.\nAm I losing I mean the values equal to 0\nbut how does it do that?\nLet's have a data point which is over here.\nLet's say my cursor is at 0.8.\nSo here I check\nwhether this value is less than the threshold value or not.\nLet's say if it is more than the threshold value.\nIt should give me the result as 1 if it is less than that,\nthen should give me the result is zero.\nSo here my threshold value is 0.5.\nI need to Define that if my value let's is 0.8.\nIt is a more than 0.5.\nThen the value should be rounded of to 1.\nLet's see if it is less than 0.5.\nLet's I have a value 0.2 then should reduce it to zero.\nSo here you can use the concept\nof threshold value to find output.\nSo here it should be discreet.\nIt should be either 0 or it should be one.\nSo I hope you caught this curve of logistic regression.\nSo guys, this is the sigmoid S curve.\nSo to make this curve we need to make an equation.\nSo let me address that part as well.\nSo let's see how an equation is formed to imitate\nthis functionality so over here,\nwe have an equation of a straight line.\nIt is y is equal to MX plus C.\nSo in this case,\nI just have only one independent variable but let's say\nif we have many independent variable then\nthe equation becomes m 1 x 1 plus m 2 x 2 plus m 3 x\n3 and so on till M NX n now,\nlet us put in B and X.\nSo here the equation becomes Y is equal to b 1 x\n1 plus beta 2 x\n2 plus b 3 x 3 and so on till be nxn plus C.\nSo guys the equation\nof the straight line has a range from minus infinity to Infinity.\nBut in our case or you can say largest equation the value\nwhich we need to predict or you can say the Y value\nit can have the range only from 0 to 1.\nSo in that case we need to transform this equation.\nSo to do that what we\nhad done we have just divide the equation by 1 minus y\nso now Y is equal to 0 so 0\nover 1 minus 0 which is equal to 1\nso 0 over 1 is again 0\nand if you take Y is equals to 1 then 1 over 1 minus 1\nwhich is 0\nso 1 over 0 is infinity.\nSo here my range is now between You know to Infinity,\nbut again, we want the range from minus infinity to Infinity.\nSo for that\nwhat we'll do we'll have the log of this equation.\nSo let's go ahead and have the logarithmic\nof this equation.\nSo here we have this transform it further to get the range\nbetween minus infinity to Infinity so over\nhere we have log of Y\nover 1 minus 1\nand this is your final logistic regression equation.\nSo guys, don't worry.\nYou don't have to write this formula or memorize\nthis formula in Python.\nYou just need to call this function\nwhich is logistic regression\nand everything will be be automatically for you.\nSo I don't want to scare you with the maths\nin the formulas behind it,\nwhich is always good to know how this formula was generated.\nSo I hope you guys are clear\nwith how logistic regression comes into the picture next.\nLet us see what are the major differences\nbetween linear regression was a logistic regression the first\nof all in linear regression,\nwe have the value\nof y as a continuous variable or the variable\nbetween need to predict are continuous in nature.\nWhereas in logistic regression.\nWe have the categorical variable so here the value\nwhich you need to predict should be Creating nature.\nIt should be either 0\nor 1 or should have just two values to it.\nFor example,\nwhether it is raining or it is not raining\nis it humid outside or it is not humid outside.\nNow, does it going to snow and it's not going to snow?\nSo these are the few example,\nwe need to predict\nwhere the values are discrete or you can just predict\nwhether this is happening or not.\nNext linear equation solves your regression problems.\nSo here you have a concept of independent variable\nand the dependent variable.\nSo here you can calculate the value of y\nwhich you need to predict using the A of X so\nhere your y variable or you can see the value\nthat you need to predict are in a range.\nBut whereas in logistic regression you\nhave discrete values.\nSo logistic regression basically solves a classification problem\nso it can basically classify it and it can just give you result\nwhether this event is happening or not.\nSo I hope it is\npretty much Clear till now next in linear equation.\nThe graph that you have seen is\na straight line graph so over here,\nyou can calculate the value of y with respect to the value\nof x where as in logistic regression because of that.\nThe got was a Escobar you can see the sigmoid curve.\nSo using the sigmoid function You can predict\nyour y-values moving the I let us see the various use cases\nwhere in logistic regression is implemented in real life.\nSo the very first is weather prediction now\nlargest aggression helps you to predict your weather.\nFor example, it is used to predict\nwhether it is raining or not whether it is sunny.\nIs it cloudy or not?\nSo all these things can be predicted\nusing logistic regression.\nWhere as you need to keep in mind\nthat both linear regression.\nAnd logistic regression can be used in predicting the weather.\nSo in that case linear equation helps you to predict\nwhat will be the temperature tomorrow\nwhereas logistic regression will only tell you\nwhich is going to rain or not or whether it's cloudy or not,\nwhich is going to snow or not.\nSo these values are discrete.\nWhereas if you apply linear regression you\nthe predicting things like what is the temperature tomorrow\nor what is the temperature day after tomorrow\nand all those thing?\nSo these are the slight differences\nbetween linear regression\nand logistic regression the moving ahead.\nWe have classification problem.\nSighs on performs multi-class classification.\nSo here it can help you tell whether it's a bird.\nIt's not a bird.\nThen you classify different kind of mammals.\nLet's say whether it's a dog or it's not a dog similarly.\nYou can check it for reptile\nwhether it's a reptile or not a reptile.\nSo in logistic regression,\nit can perform multi-class classification.\nSo this point I've already discussed\nthat it is used in classification problems next.\nIt also helps you to determine the illness as well.\nSo let me take an example.\nLet's say a patient goes for a routine check up in hospital.\nSo what doctor will do it,\nit will perform various tests on the patient and will check\nwhether the patient is actually l or not.\nSo what will be the features\nso doctor can check the sugar level\nthe blood pressure then what is the age of the patient?\nIs it very small or is it old person then?\nWhat is the previous medical history of the patient\nand all of these features will be recorded by the doctor\nand finally doctor checks the patient data and Data -\nthe outcome of an illness and the severity of illness.\nSo using all the data a doctor can identify\nwith A patient is ill or not.\nSo these are the various use cases\nin which you can use logistic regression now,\nI guess enough of theory part.\nSo let's move ahead and see some of the Practical implementation\nof logistic regression so over here,\nI be implementing two projects\nwhen I have the data set\nof Titanic so over here will predict what factors made\npeople more likely to survive the sinking of the Titanic ship\nand my second project will see the data analysis\non the SUV cars so over here we have the data of the SUV cars\nwho can purchase it.\nAnd what factors made people more interested in buying SUV?\nSo these will be the major questions as\nto why you should Implement\nlogistic regression and what output will you get by it?\nSo let's start by the very first project\nthat is Titanic data analysis.\nSo some of you might know\nthat there was a ship called as Titanic\nwith basically hit an iceberg\nand it sunk to the bottom of the ocean and it was\na big disaster at that time\nbecause it was the first voyage of the ship\nand it was supposed to be really really strongly built and one\nof the best ships of that time.\nSo it was a big Disaster\nof that time and of course there is a movie about this as well.\nSo many of you might have washed it.\nSo what we have we have data of the passengers those\nwho survived and those\nwho did not survive in this particular tragedy.\nSo what you have to do you have to look at this data\nand analyze which factors would have been contributed\nthe most to the chances\nof a person survival on the ship or not.\nSo using the logistic regression, we can predict\nwhether the person survived\nor the person died now apart from this.\nWe also have a look with the various features\nalong with that.\nSo first, let us explore The data set so over here.\nWe have the index value then the First Column\nis passenger ID.\nThen my next column is survived.\nSo over here,\nwe have two values a 0 and a 1 so 0 stands\nfor did not survive and one stands for survive.\nSo this column is categorical\nwhere the values are discrete next.\nWe have passenger class so over here,\nwe have three values 1 2 and 3.\nSo this basically tells you that\nwhether a passengers travelling in the first class second class\nor third class,\nthen we have the name of the We\nhave the six or you can see the gender of the passenger\nwhere the passenger is a male or female.\nThen we have the age we had sip SP.\nSo this basically means the number of siblings\nor the spouses aboard the Titanic so over here,\nwe have values such as 1 0 and so on then we have\nParts apart is basically the number of parents\nor children aboard the Titanic so over here,\nwe also have some values\nthen we have the ticket number.\nWe have the fair.\nWe have the table number and we have the embarked column.\nSo in my inbox column,\nwe have three values we have SC and Q.\nSo as basically stands\nfor Southampton C stands for Cherbourg\nand Q stands for Cubans down.\nSo these are the features\nthat will be applying our model on so here\nwe'll perform various steps\nand then we'll be implementing logistic regression.\nSo now these are the various steps\nwhich are required to implement any algorithm.\nSo now in our case we are implementing\nlogistic regression soft.\nVery first step is to collect your data\nor to import the libraries\nthat are used for collecting your data.\nAnd then taking it forward then my second step is to analyze\nyour data so over here I can go to the various fields\nand then I can analyze the data.\nI can check that the females\nor children survive better than the males\nor did the rich passenger survived more\nthan the poor passenger or did the money matter as in\nwho paid mode to get into the ship\nwith the evacuated first?\nAnd what about the workers does the worker survived\nor what is the survival rate\nif you were the worker in the ship and not just\na traveling passenger?\nSo all of these are very very and questions\nand you would be going through all of them one by one.\nSo in this stage,\nyou need to analyze our data\nand explore your data as much as you can then my third step is\nto Wrangle your data now\ndata wrangling basically means cleaning your data so over here,\nyou can simply remove the unnecessary items or\nif you have a null values in the data set.\nYou can just clear that data and then you can take it forward.\nSo in this step you can build your model\nusing the train data set\nand then you can test it\nusing a test so over here you will be performing a split\nwhich basically Get your data set into training\nand testing data set and find you will check the accuracy.\nSo as to ensure\nhow much accurate your values are.\nSo I hope you guys got these five steps\nthat you're going to implement in logistic regression.\nSo now let's go into all these steps in detail.\nSo number one.\nWe have to collect your data\nor you can say import the libraries.\nSo it may show you the implementation part as well.\nSo I just open my jupyter notebook\nand I just Implement all of these steps side by side.\nSo guys this is my jupyter notebook.\nSo first, let me just rename jupyter notebook to let's say\nTitanic data analysis.\nNow a full step was to import all the libraries\nand collect the data.\nSo let me just import all the library's first.\nSo first of all, I'll import pandas.\nSo pandas is used for data analysis.\nSo I'll say import pandas as PD then I will be importing numpy.\nSo I'll say import numpy as NP so number is a library in Python\nwhich basically stands for numerical Python\nand it is widely used to perform any scientific computation.\nNext.\nWe will be importing Seaborn.\nSo c 1 is a library\nfor statistical plotting so Say import Seaborn as SNS.\nI'll also import matplotlib.\nSo matplotlib library is again for plotting.\nSo I'll say import matplotlib dot Pi plot\nas PLT now to run this library in jupyter Notebook all I have\nto write in his percentage matplotlib in line.\nNext I will be importing one module as well.\nSo as to calculate the basic mathematical functions,\nso I'll say import maths.\nSo these are the libraries\nthat I will be needing in this Titanic data analysis.\nSo now let me just import my data set.\nSo I'll take a variable.\nLet's say Titanic data and using the pandas.\nI will just read my CSV or you can see the data set.\nI like the name of my data set that is Titanic dot CSV.\nNow.\nI have already showed you the data set so over here.\nLet me just bring the top 10 rows.\nSo for that I will just say\nI take the variable Titanic data dot head\nand I'll say the top ten rules.\nSo now I'll just run this\nso to run this style is have to press shift + enter\nor else you can just directly click on this cell so over here.\nI have the index.\nWe have the passenger ID, which is nothing.\nBut again the index\nwhich is starting from 1 then we have the survived column\nwhich has a category.\nCall values or you can say the discrete values,\nwhich is in the form of 0 or 1.\nThen we have the passenger class.\nWe have the name of the passenger sex age and so on.\nSo this is the data set\nthat I will be going forward\nwith next let us print the number of passengers\nwhich are there in this original data frame for that.\nI'll just simply type in print.\nI'll say a number of passengers.\nAnd using the length function,\nI can calculate the total length.\nSo I'll say length\nand inside this I'll be passing this variable\nbecause Titanic data, so I'll just copy it from here.\nI'll just paste it dot index\nand next set me just bring this one.\nSo here the number of passengers\nwhich are there\nin the original data set we have is 891 so around\nthis number would traveling in the Titanic ship so over here,\nmy first step is done\nwhere you have just collected data imported all the libraries\nand find out the total number of passengers,\nwhich are Titanic so now let me just go back\nto presentation and let's see.\nWhat is my next step.\nSo we're done with the collecting data.\nNext step is to analyze your data so over here\nwill be creating different plots to check the relationship\nbetween variables as\nin how one variable is affecting the other\nso you can simply explore your data set by making use\nof various columns\nand then you can plot a graph between them.\nSo you can either plot a correlation graph.\nYou can plot a distribution curve.\nIt's up to you guys.\nSo let me just go back\nto my jupyter notebook and let me analyze some of the data.\nOver here.\nMy second part is to analyze data.\nSo I just put this in headed to now to put this in here\nto I just have to go on code click on mark down\nand I just run this so first\nlet us plot account plot\nwhere you can pay between the passengers\nwho survived and who did not survive.\nSo for that I will be using the Seabourn Library so over\nhere I have imported Seaborn as SNS\nso I don't have to write the whole name.\nI'll simply say SNS dot count plot.\nI say axis with the survive and the data\nthat I'll be using is the Titanic data\nor you can say the name of variable in which you\nhave store your data set.\nSo now let me just run this\nso who were here as you can see I have survived column on my x\naxis and on the y axis.\nI have the count.\nSo zero basically stands for did not survive\nand one stands for the passengers\nwho did survive so over here,\nyou can see that around 550 of the passengers\nwho did not survive and they were around 350 passengers\nwho only survive so here you can basically conclude.\nThere are very less survivors than on survivors.\nSo this was the very first plot now there is not another plot\nto compare the sex as to whether out of all the passengers\nwho survived and who did not survive.\nHow many were men and how many were female\nso to do that?\nI'll simply say SNS dot count plot.\nI add the Hue as six so I want to know\nhow many females and how many male survive\nthen I'll be specifying the data.\nSo I'm using Titanic data set and let me just run\nthis you have done a mistake\nover here so over here you\ncan see I have survived column on the x-axis\nand I have the count on the why now.\nSo have you color stands for your male passengers\nand orange stands for your female?\nSo as you can see here the passengers\nwho did not survive\nthat has a value 0 so we can see that.\nMajority of males did not survive and if we see the people\nwho survived here,\nwe can see the majority of female survive.\nSo this basically concludes the gender of the survival rate.\nSo it appears on average women were more than three\ntimes more likely to survive than men next.\nLet us plot another plot\nwhere we have the Hue as the passenger class so over\nhere we can see which class at the passenger was traveling in\nwhether it was traveling in class 1 2 or 3.\nSo for that I just arrived the same command.\nI will say as soon as.com plot.\nI gave my x-axis as a family.\nI'll change my Hue to passenger class.\nSo my variable named as PE class.\nAnd the data said\nthat I'll be using this Titanic data.\nSo this is my result\nso over here you can see I have blue for first-class orange\nfor second class and green for the third class.\nSo here the passengers\nwho did not survive a majorly of the third class\nor you can say the lowest class\nor the cheapest class to get into the dynamic and the people\nwho did survive majorly belong to the higher classes.\nSo here 1 &amp; 2 has more eyes than the passenger\nwho were traveling in the third class.\nSo here we have computed that the passengers\nwho did not survive a majorly of third.\nOr you can see the lowest class\nand the passengers who were traveling\nin first and second class would tend to survive mode next.\nI just got a graph for the age distribution over here.\nI can simply use my data.\nSo we'll be using pandas library for this.\nI will declare an array and I'll pass in the column.\nThat is H.\nSo I plot and I want a histogram.\nSo I'll see plot da test.\nSo you can notice over here\nthat we have more of young passengers,\nor you can see the children between the ages 0 to 10\nand then we have the average people\nand if you go ahead Lester would be the population.\nSo this is the analysis on the age column.\nSo we saw that we have\nmore young passengers and more video courage passengers\nwhich are traveling in the Titanic.\nSo next let me plot a graph of fare as well.\nSo I'll say Titanic data.\nI say fair and again,\nI've got a histogram so I'll say hissed.\nSo here you can see the fair size is\nbetween zero to hundred now.\nLet me add the bin size.\nSo as to make it more clear over here,\nI'll say Ben is equals to let's\nsay 20 and I'll increase the figure size as well.\nSo I'll say fixed size.\nLet's say I'll give the dimensions as 10 by 5.\nSo it is bins.\nSo this is more clear now next.\nIt is analyzed the other columns as well.\nSo I'll just type in Titanic data\nand I want the information as to what all columns are left.\nSo here we have passenger ID,\nwhich I guess it's of no use then we have see\nhow many passengers survived and how many did not we\nalso do the analysis on the gender basis.\nWe saw with a female tend to survive more\nor the maintain to survive more then we saw the passenger class\nwhere the passenger is traveling in the first class second class\nor third class.\nThen we have the name.\nSo in name, we cannot do any analysis.\nWe saw the sex we saw the ages.\nWell, then we have sea bass P.\nSo this stands for the number of siblings or the spouse is\nwhich Are aboard the Titanic so let us do this as well.\nSo I'll say SNS dot count plot.\nI mentioned X SC SP.\nAnd I will be using the Titanic data\nso you can see the plot\nover here so over here you can conclude that.\nIt has the maximum value on zero so we can conclude\nthat neither children nor a spouse was\non board the Titanic now second most highest value is 1\nand then we have various values for 2 3 4 and so on next\nif I go above the store this column as well.\nSimilarly can do four parts.\nSo next we have part\nso you can see the number of parents or children\nwhich are both the Titanic so similarly can do.\nIsrael then we have the ticket number.\nSo I don't think so.\nAny analysis is required for Ticket.\nThen we have fears of a we have already discussed as\nin the people would tend to travel in the first class.\nYou will pay the highest view then we have the cable number\nand we have embarked.\nSo these are the columns\nthat will be doing data wrangling on\nso we have analyzed the data\nand we have seen quite a few graphs\nin which we can conclude which variable is better than another\nor what is the relationship the whole third step\nis my data wrangling so data wrangling basically\nmeans Cleaning your data.\nSo if you have a large data set,\nyou might be having some null values\nor you can say n values.\nSo it's very important\nthat you remove all the unnecessary items\nthat are present in your data set.\nSo removing this directly affects your accuracy.\nSo I just go ahead and clean my data\nby removing all the Nan values and unnecessary columns,\nwhich has a null value in the data set\nthe next time you're performing data wrangling.\nSupposed to fall I'll check\nwhether my dataset is null or not.\nSo I'll say Titanic data,\nwhich is the name of my data set and I'll say is null.\nSo this will basically tell me what all values are null\nand will return me a Boolean result.\nSo this basically checks the missing data\nand your result will be in Boolean format\nas in the result will be true or false so Falls mean\nif it is not null and true means\nif it is null,\nso let me just run this.\nOver here you can see the values as false or true.\nSo Falls is where the value is not null and true is\nwhere the value is none.\nSo over here you can see in the cabin column.\nWe have the very first value\nwhich is null so we have to do something on this so you can see\nthat we have a large data set.\nSo the counting does not stop\nand we can actually see the some of it.\nWe can actually print the number of passengers\nwho have the Nan value in each column.\nSo I say Titanic underscore data is null\nand I want the sum of it.\nThey've got some so this is basically print the number\nof passengers who have the n n values in each column\nso we can see\nthat we have missing values in each column that is 177.\nThen we have the maximum value in the cave in column\nand we have very Less in the Embark column.\nThat is 2 so here\nif you don't want to see this numbers,\nyou can also plot a heat map\nand then you can visually analyze it so let me just do\nthat as well.\nSo I'll say SNSD heat map.\nand say why tick labels False child has run this\nas we have already seen\nthat there were three columns\nin which missing data value was present.\nSo this might be age so over here almost 20% of each column\nhas a missing value then we have the caping columns.\nSo this is quite a large value\nand then we have two values for embark column as well.\nAdd a see map for color coding.\nSo I'll say see map.\nSo if I do this\nso the graph becomes more attractive so over here\nyellow stands for Drew or you can say the values are null.\nSo here we have computed\nthat we have the missing value of H. We have a lot\nof missing values in the cabin column\nand we have very less value,\nwhich is not even visible in the Embark column as well.\nSo to remove these missing values,\nyou can either replace the values and you can put in\nsome dummy values to it or you can simply drop the column.\nSo here let us suppose pick the age column.\nSo first, let me just plot a box plot\nand they will analyze with having a column as age\nso I'll say SNS dot box plot.\nI'll say x is equals to passenger class.\nSo it's PE class.\nI'll say Y is equal to H and the data set\nthat I'll be using is Titanic side.\nSo I'll say the data is goes to Titanic data.\nYou can see the edge\nin first class and second class tends to be more older rather\nthan we have it in the third place.\nWell that depends On the experience\nhow much you earn on might be there any number of reasons?\nSo here we concluded\nthat passengers\nwho were traveling in class one and class two a tend\nto be older than what we have in the class 3 so we have found\nthat we have some missing values in EM.\nNow one way is to either just drop the column\nor you can just simply fill in some values to them.\nSo this method is called as imputation now\nto perform data wrangling\nor cleaning it is for spring the head of the data set.\nSo I'll say Titanic not head so it's Titanic.\nFor data, let's say I just want the five rows.\nSo here we have survived which is again categorical.\nSo in this particular column,\nI can apply logic to progression.\nSo this can be my y value or the value\nthat you need to predict.\nThen we have the passenger class.\nWe have the name then we have ticket number Fair\ngiven so over here.\nWe have seen that in keeping.\nWe have a lot of null values or you can say that any invalid\nwhich is quite visible as well.\nSo first of all,\nwe'll just drop this column for dropping it.\nI'll just say Titanic underscore data.\nAnd I'll simply type in drop and the column\nwhich I need to drop so I have to drop the cable column.\nI mention the access equals to 1 and I'll say\nin place also to true.\nSo now again,\nI just print the head and a to see whether this column\nhas been removed from the data set or not.\nSo I'll say Titanic dot head.\nSo as you can see here,\nwe don't have given column anymore.\nNow, you can also drop the na values.\nSo I'll say Titanic data dot drop all the any values\nor you can say Nan\nwhich is not a number and I will say in place is equal to True.\nLet's Titanic.\nSo over here,\nlet me again plot the heat map and let's say what the values\nwhich will be for showing a lot of null values.\nHas it been removed or not.\nSo I'll say SNSD heat map.\nI'll pass in the data set.\nI'll check it is null I say why dick labels is equal to false.\nAnd I don't want color coding.\nSo again I say false.\nSo this will basically help me to check\nwhether my values has been removed\nfrom the data set or not.\nSo as you can see here, I don't have any null values.\nSo it's entirely black now.\nYou can actually know the some as well.\nSo I'll just go above So I'll just copy this part\nand I just use the sum function to calculate the sum.\nSo here the tells me\nthat data set is green as in the data set does not contain\nany null value or any n value.\nSo now we have R Angela data.\nYou can see cleaner data.\nSo here we have done just one step in data wrangling\nthat is just removing one column out of it.\nNow you can do a lot of things you can actually\nfill in the values with some other values\nor you can just calculate the mean\nand then you can just fit in the null values.\nBut now if I see my data set,\nso I'll say Titanic data dot head.\nBut now if I see you over here I have a lot of string values.\nSo this has to be converted to a categorical variables\nin order to implement logistic regression.\nSo what we will do we will convert this\nto categorical variable\ninto some dummy variables and this can be done using pandas\nbecause logistic regression just take two values.\nSo whenever you apply machine learning you need to make sure\nthat there are no string values present\nbecause it won't be taking these as your input variables.\nSo using string you don't have to predict anything but\nin my case I have the survived columns 2210 how many?\nPeople tend to survive\nand how men did not so 0 stands for did not survive\nand one stands for survive.\nSo now let me just convert these variables\ninto dummy variables.\nSo I'll just use pandas and I say PD not get dummies.\nYou can simply press tab to autocomplete\nand say Titanic data and I'll pass the sex\nso you can just simply click on shift + tab to get\nmore information on this.\nSo here we have the type data frame\nand we have the passenger ID survived and passenger class.\nSo if Run this you'll see\nthat 0 basically stands for not a female and once and\nfor it is a female similarly for male zero Stanford's not made\nand one Stanford main now,\nwe don't require both these columns\nbecause one column itself is enough to tell us\nwhether it's male or you can say female or not.\nSo let's say if I want to keep only mail I will say\nif the value of mail is 1\nso it is definitely a maid and is not a female.\nSo that is how you don't need both of these values.\nSo for that I just remove the First Column,\nlet's say a female so I'll say drop first.\nAndrew it has given me just one column\nwhich is male and has a value 0 and 1.\nLet me just set this as a variable hsx so over here\nI can say sex dot head.\nI'll just want to see the first pie Bros.\nSorry, it's Dot.\nSo this is how my data looks like now here.\nWe have done it for sex.\nThen we have the numerical values in age.\nWe have the numerical values in spouses.\nThen we have the ticket number.\nWe have the pair and we have embarked as well.\nSo in Embark, the values are in SC and Q.\nSo here also we can apply this get dummy function.\nSo let's say I will take a variable.\nLet's say Embark.\nI'll use the pandas Library.\nI need the column name that is embarked.\nLet me just print the head of it.\nSo I'll say Embark dot head so over here.\nWe have c q and s now here also we can drop the First Column\nbecause these two values are enough\nwith the passenger is either traveling for Q\nthat is toonstone S4 sound time\nand if both the values are 0 then definitely\nthe passenger is from Cherbourg.\nThat is the third value\nso you can again drop the first value so I'll say drop.\nLet me just run this\nso this is how my output looks like now similarly.\nYou can do it for passenger class as well.\nSo here also we have three classes one two,\nand three so I'll just copy the whole statement.\nSo let's say I want the variable name.\nLet's say PCL.\nI'll pass in the column name\nthat is PE class and I'll just drop the First Column.\nSo here also the values will be 1 2 or 3\nand I'll just remove the First Column.\nSo here we just left with two and three so\nif both the values are 0 then definitely the passengers\ntraveling the first class now,\nwe have made the values as categorical now,\nmy next step would be to concatenate all\nthese new rows into a data set.\nWe can see Titanic data using the pandas will just concatenate\nall these columns.\nSo I'll say p Dot.\nOne cat and then say we have to concatenate sex.\nWe have to concatenate Embark and PCL\nand then I will mention the access to one.\nI'll just run this can you\nto print the head so over here you can see\nthat these columns have been added over here.\nSo we have the mail column with basically tells\nwhere the person is male\nor it's a female then we have the Embark\nwhich is basically q\nand s so if it's traveling\nfrom Queenstown value would be one else it\nwould be 0 and If both of these values are zeroed,\nit is definitely traveling from Cherbourg.\nThen we have the passenger class as 2 and 3.\nSo the value of both\nthese is 0 then passengers travelling in class one.\nSo I hope you got this till now now these are\nthe irrelevant columns\nthat we have done over here\nso we can just drop these columns will drop\nin PE class the embarked column and the sex column.\nSo I'll just type\nin Titanic data dot drop and mention the columns\nthat I want to drop.\nSo I say And even lead the passenger ID\nbecause it's nothing but just the index value\nwhich is starting from one.\nSo I'll drop this as well then I don't want name as well.\nSo I'll delete name as well.\nThen what else we can drop we can drop the ticket as well.\nAnd then I'll just mention the axis L say\nin place is equal to True.\nOkay, so the my column name starts uppercase.\nSo these has been dropped now,\nlet me just bring my data set again.\nSo this is my final leadership guys.\nWe have the survived column\nwhich has the value zero and one then we have the passenger class\nor we forgot to drop this as well.\nSo no worries.\nI'll drop this again.\nSo now let me just run this.\nSo over here we have the survive.\nWe have the H we have the same SP.\nWe have the parts.\nWe have Fair mail and these we have just converted.\nSo here we have just performed data angling\nfor you can see clean the data\nand then we have just converted the values of gender\nto male then embarked to qns\nand the passenger Class 2 2 &amp; 3.\nSo this was all about my data wrangling\nor just cleaning the data then my next up is training\nand testing your data.\nSo here we will split the data set into train subset\nand test steps.\nAnd then what we'll do we'll build a model\non the train data\nand then predict the output on your test data set.\nSo let me just go back to Jupiter\nand it is implement this as well over here.\nI need to train my data set.\nSo I'll just put this indeed heading 3.\nSo over you need to Define your dependent variable\nand independent variable.\nSo here my Y is the output for you can say the value\nthat I need to predict so over here,\nI will write Titanic data.\nI'll take the column which is survive.\nSo basically I have to predict this column\nwhether the passenger survived or not.\nAnd as you can see we have the discrete outcome,\nwhich is in the form of 0 and 1 and rest all the things we\ncan take it as a features or you can say independent variable.\nSo I'll say Titanic data.\nNot a drop,\nso we just simply drop the survive\nand all the other columns will be my independent variable.\nSo everything else are the features which leads\nto the survival rate.\nSo once we have defined the independent variable\nand the dependent variable next step is to split\nyour data into training and testing subset.\nSo for that we will be using SK loan.\nI just type in from sklearn dot cross validation.\nimport train display Now here\nif you just click on shift and tab,\nyou can go to the documentation\nand you can just see the examples over here.\nAnd she can blast open it\nand then I just go to examples and see\nhow you can split your data.\nSo over here you have extra next test why drain\nwhy test and then using the string test platelet\nand just passing\nyour independent variable and dependent variable\nand just Define a size and a random straight to it.\nSo, let me just copy this and I'll just paste over here.\nOver here, we'll train test.\nThen we have the dependent variable train and test\nand using the split function will pass in the independent\nand dependent variable and then we'll set a split size.\nSo let's say I'll put it up 0.3.\nSo this basically means\nthat your data set is divided in 0.3\nthat is in 70/30 ratio.\nAnd then I can add any random straight to it.\nSo let's say I'm applying one this is not necessary.\nIf you want the same result as that of mine,\nyou can add the random stream.\nSo this would basically take exactly the same sample\nevery Next I have to train and predict by creating a model.\nSo here logistic regression will graph\nfrom the linear regression.\nSo next I'll just type in\nfrom SK loan dot linear model import logistic regression.\nNext I'll just create\nthe instance of this logistic regression model.\nSo I'll say log model is equals to largest aggression now.\nI just need to fit my model.\nSo I'll say log model dot fit\nand I'll just pass in my ex train.\nAnd why it rain?\nIt gives me all the details of logistic regression.\nSo here it gives me the class made dual fit intercept\nand all those things then what I need to do,\nI need to make prediction.\nSo I will take a variable insect addictions and I'll pass\non the model to it.\nSo I'll say log model dot protect\nand I'll pass in the value that is X test.\nSo here we have just created a model fit\nthat model and then we had made predictions.\nSo now to evaluate how my model has been performing.\nSo you can simply calculate the accuracy\nor you can also calculate a classification report.\nSo don't worry guys.\nI'll be showing both of these methods.\nSo I'll say\nfrom sklearn dot matrix input classification report.\nAre you start fishing report?\nAnd inside this I'll be passing in why test and the predictions?\nSo guys this is my classification report.\nSo over here, I have the Precision.\nI have the recall.\nWe have the advanced code and then we have support.\nSo here we have the value of decision as 75 72 and 73,\nwhich is not that bad now\nin order to calculate the accuracy as well.\nYou can also use the concept of confusion Matrix.\nSo if you want to print the confusion Matrix,\nI will simply say\nfrom sklearn dot matrix import confusion Matrix first of all,\nand then we'll just print this So\nhow am I function has been imported successfully\nso is a confusion Matrix.\nAnd I'll again passing the same variables\nwhich is why test and predictions.\nSo I hope you guys already know the concept of confusion Matrix.\nSo can you guys give me a quick confirmation as\nto whether you guys\nremember this confusion Matrix concept or not?\nSo if not,\nI can just quickly summarize this as well.\nOkay charged with you say\nso yes.\nOkay.\nSo what is not clear with this?\nSo I'll just tell you in a brief what\nconfusion Matrix is all about?\nSo confusion Matrix is nothing but a 2 by 2 Matrix\nwhich has a four outcomes this basic tells us\nthat how accurate your values are.\nSo here we have the column as predicted.\nNo predicted Y and we have actual know an actual.\nYes.\nSo this is the concept of confusion Matrix.\nSo here let me just fade in these values\nwhich we have just calculated.\nSo here we have 105.\n105 2125 and 63 So as you can see here,\nwe have got four outcomes now\n105 is the value where a model has predicted.\nNo, and in reality.\nIt was also a no so\nwhere we have predicted know an actual know similarly.\nWe have 63 as a predicted.\nYes.\nSo here the model predicted.\nYes, and actually also it was yes.\nSo in order to calculate the accuracy,\nyou just need to add the sum\nof these two values and divide the whole by the some.\nSo here these two values tells me where the order has.\nWe predicted the correct output.\nSo this value is also called as true-\nThis is called as false positive.\nThis is called as true positive\nand this is called as false negative.\nNow in order to calculate the accuracy.\nYou don't have to do it manually.\nSo in Python,\nyou can just import accuracy score function\nand you can get the results from that.\nSo I'll just do that as well.\nSo I'll say from sklearn dot-matrix import accuracy score\nand I'll simply print the accuracy.\nI'm passing the same variables.\nThat is why I test and predictions so over here.\nIt tells me the accuracy as 78 which is quite good so over here\nif you want to do it manually we have 2 plus these two numbers,\nwhich is 105 263.\nSo this comes out to almost 168 and then you have to divide\nby the sum of all the phone numbers.\nSo 105 plus 63 plus 21 plus 25,\nso this gives me a result of to 1/4.\nSo now if you divide\nthese two number you'll get the same accuracy that is 98%\nor you can say .78.\nSo that is how you can calculate the accuracy.\nSo now let me just go back to my presentation and let's see\nwhat all we have covered till now.\nSo here we have First Data data into train and test subset then\nwe have build a model on the train data\nand then predicted the output on the test data set\nand then my fifth step is to check the accuracy.\nSo here we have\ncalculator accuracy to almost seventy eight percent,\nwhich is quite good.\nYou cannot say that accuracy is bad.\nSo here tells me how accurate your results.\nSo him accuracy skoda finds\nthat enhanced got a good accuracy.\nSo now moving ahead.\nLet us see the second project that is SUV data analysis.\nSo in this a car company has released new SUV in the market\nand using the previous data about the sales of their SUV.\nThey want to predict the category of people\nwho might be interested in buying this.\nSo using the logistic regression,\nyou need to find what factors made people more interested\nin buying this SUV.\nSo for this let us hear data set where I have user ID I have\nOf gender as male and female then we have the age.\nWe have the estimated salary\nand then we have the purchased column.\nSo this is my discreet column\nor you can see the categorical column.\nSo here we just have the value\nthat is 0 and 1 and this column we need to predict\nwhether a person can actually purchase a SUV or Not.\nSo based on these factors, we will be deciding\nwhether a person can actually purchase SUV or not.\nSo we know the salary of a person we know the age\nand using these we can predict\nwhether person can actually purchase SUV\non Let me just go to my jupyter.\nNotebook and has implemented a logistic regression.\nSo guys, I will not be going through all the details\nof data cleaning and analyzing the part start part.\nI'll just leave it on you.\nSo just go ahead and practice as much as you can.\nAlright, so the second project is SUV predictions.\nAlright, so first of all,\nI have to import all the libraries\nso I say import numpy SNP and similarly.\nI'll do the rest of it.\nAlright, so now let me just bring the head\nof this data set.\nSo this give already seen that we have columns as user ID.\nWe have gender.\nWe have the age.\nWe have the salary and then we have to calculate\nwhether person can actually purchase a SUV or not.\nSo now let us just simply go on to the algorithm part.\nSo we'll directly start off with the logistic regression\nhow you can train a model so for doing all those things\nwe first need to Define an independent variable\nand a dependent variable.\nSo in this case,\nI want my ex at is an independent variable is\na data set.\nI lock so here I will specify sighing all the rows.\nSo cool and basically stands for that and in the columns,\nI want only two and three dot values.\nSo here we should fetch me all the rows\nand only the second and third column which is age\nand estimated salary.\nSo these are the factors\nwhich will be used to predict the dependent variable\nthat is purchase.\nSo here my dependent variable is purchase\nany dependent variable is of age and salary.\nSo I'll say later said dot\nI log I'll have all the rows and add just one for column.\nThat is my position.\nIs column values.\nAll right, so I just forgot\nwhen one square bracket over here.\nAlright so over here.\nI have defined my independent variable and dependent variable.\nSo here my independent variable is age and salary\nand dependent variable is the column purchase.\nNow, you must be wondering what is this?\nI lock function.\nSo I look function is basically an index of a panda's data frame\nand it is used for integer based indexing\nor you can also say selection by index now,\nlet me just bring these independent variables\nand dependent variable.\nSo if I bring the independent variable I have aged as\nwell as a salary next.\nLet me print the dependent variable as well.\nSo over here you can see I just have the values in 0\nand 1 so 0 stands for did not purchase next.\nLet me just divide my data set into training and test subset.\nSo I'll simply write in\nfrom SK loaned cross plate dot cross validation.\nimport rain test next I just press shift and tab\nand over here.\nI will go to the examples and just copy the same line.\nSo I'll just copy this.\nI'll move the points now.\nI want to text size to be let's see 25,\nso I have divided the trained and tested in 75/25 ratio.\nNow, let's say I'll take the random set of 0 So\nRandom State basically ensures the same result\nor you can say the same samples taken whenever you run the code.\nSo let me just run this now.\nYou can also scale your input values\nfor better performing\nand this can be done using standard scale.\nOh, so let me do that as well.\nSo I'll say from sklearn pre-processing.\nImport standard scalar now.\nWhy do we scale it now?\nIf you see a data set we are dealing with large numbers.\nWell, although we are using a very small data set.\nSo whenever you're working in a prod environment,\nyou'll be working with large data set\nwe will be using thousands\nand hundred thousands of do people's so they're scaling\ndown will definitely affect the performance\nby a large extent.\nSo here let me just show you\nhow you can scale down these input values and then\nthe pre-processing contains all your methods &amp; functionality,\nwhich is required to transform your data.\nSo now let us scale down\nfor tests as well as their training data set.\nSo else First Make an instance of it.\nSo I'll say standard scalar\nthen I'll have Xtreme sasc dot fit fit underscore transform.\nI'll pass in my Xtreme variable.\nAnd similarly I can do it for test wherein\nI'll pass the X test.\nAll right.\nNow my next step is to import logistic regression.\nSo I'll simply apply logistically creation\nby first importing it\nso I'll say from sklearn sklearn\nthe linear model import logistic regression over here.\nI'll be using classifier.\nSo is a classifier DOT is equals\nto largest aggression so over here,\nI just make an instance of it.\nSo I'll say logistic regression and over here.\nI just pass in the random state,\nwhich is 0 No, I simply fit the model.\nAnd I simply pass in X train and white rain.\nSo here it tells me all the details\nof logistic regression.\nThen I have to predict the value.\nSo I'll say why I prayed it's equals to classifier.\nThen predict function and then I just pass in X test.\nSo now we have created the model.\nWe have scale down our input values.\nThen we have applied logistic regression.\nWe have predicted the values\nand now we want to know the accuracy.\nSo now the accuracy first we need to import accuracy scores.\nSo I'll say from sklearn dot-matrix import\nactually see school\nand using this function we can calculate the accuracy\nor you can manually do\nthat by creating a confusion Matrix.\nSo I'll just pass.\nmy lightest and my y predicted All right.\nSo over here I get the accuracy is 89%\nSo we want to know the accuracy in percentage.\nSo I just have to multiply it by a hundred and if I run this\nso it gives me 89%\nSo I hope you guys are clear\nwith whatever I have taught you today.\nSo here I have taken my independent variables as age\nand salary and then we have calculated\nthat how many people can purchase the SUV\nand then we have calculated our model by checking\nthe accuracy so over here we get the accuracy is 89\nwhich is great.\nAlright guys that is it for today.\nSo I'll Discuss\nwhat we have covered in today's training.\nFirst of all,\nwe had a quick introduction to what is regression\nand where their aggression is actually use then\nwe have understood the types of regression\nand then got into the details\nof what and why of logistic regression\nof compared linear was in logistic regression.\nIf you've also seen the various use cases\nwhere you can Implement logistic regression in real life\nand then we have picked up two projects\nthat is Titanic data analysis\nand SUV prediction so over here we have seen\nhow you can collect your data analyze your data then perform.\nModeling on that date that train the data test\nthe data and then finally have calculated the accuracy.\nSo in your SUV prediction,\nyou can actually analyze clean your data\nand you can do a lot of things\nso you can just go ahead pick up any data set\nand explore it as much as you can.\nWhat is classification.\nI hope every one of you must have used Gmail.\nSo how do you think the male is getting classified as a Spam\nor not spam mail?\nWell, there's But classification So\nWhat It Is Well classification is the process\nof dividing the data set into different categories\nor groups by adding label.\nIn other way,\nyou can say that it is a technique\nof categorizing the observation into different category.\nSo basically what you are doing is you are taking\nthe data analyzing it\nand on the basis of some condition\nyou finely divided into various categories.\nNow, why do we classify it?\nWell, we classify it to perform predictive analysis\non it like when you get\nthe mail the machine predicts it Be a Spam\nor not spam mail\nand on the basis of that prediction it\nadd the irrelevant or spam mail\nto the respective folder in general this classification.\nAlgorithm handled questions.\nLike is this data belongs to a category or B category?\nLike is this a male or is this a female something like that?\nI getting it?\nOkay fine.\nNow the question arises where will you use it?\nWell, you can use this of protection order to check\nwhether the transaction is genuine or not suppose.\nI am using a credit.\nHere in India now due\nto some reason I had to fly to Dubai now.\nIf I'm using the credit card over there,\nI will get a notification alert regarding my transaction.\nThey would ask me to confirm about the transaction.\nSo this is also kind of predictive analysis\nas the machine predicts\nthat something fishy is\nin the transaction as very for our ago.\nI made the transaction using the same credit card and India\nand 24 hour later.\nThe same credit card is being used for the payment in Dubai.\nSo the machine texts that something fishy is going on\nin the transaction.\nSo in order to confirm it it sends you a notification alert.\nAll right.\nWell, this is one of the use case of classification\nyou can even use it to classify different items\nlike fruits on the base of its taste color size\nor weight a machine well trained using\nthe classification algorithm can easily predict the class\nor the type of fruit whenever new data is given to it.\nNot just the fruit.\nIt can be any item.\nIt can be a car.\nIt can be a house.\nIt can be a signboard.\nOr anything.\nHave you noticed\nthat while you visit some sites\nor you try to login into some you get\na picture capture for that right\nwhere you have to identify\nwhether the given image is of a car or its of a pole or not?\nYou have to select it for example that 10 images\nand you're selecting three Mages out of it.\nSo in a way you are training the machine,\nright you're telling\nthat these three are the picture of a car\nand rest are not so\nwho knows you are training at for something big\nright?\nSo moving on ahead.\nLet's discuss the types of education online.\nWell, there are several different ways\nto perform the same tasks like in order to predict\nwhether a given person is a male\nor a female the machine had to be trained first.\nAll right,\nbut there are multiple ways to train the machine and you\ncan choose any one of them just for Predictive Analytics.\nThere are many different techniques,\nbut the most common of them all is the decision tree,\nwhich we'll cover in depth in today's session.\nSo it's a part of classification algorithm.\nWe have decision tree random Forest name buys.\nK-nearest neighbor Lodge\nis Regression linear regression support Vector machines\nand so on there are many.\nAlright, so let me give you an idea about few\nof them starting with decision tree.\nWell decision tree is a graphical representation\nof all the possible solution\nto a decision the decisions\nwhich are made they can be explained very easily.\nFor example here is a task,\nwhich says that should I go to a restaurant\nor should I buy a hamburger you are confused on that.\nSo for the artboard you will do you will create\na dish entry for it starting\nwith the root node will be first of all,\nyou will check whether you are hungry or not.\nAll right,\nif you're not hungry then just go back to sleep.\nRight?\nIf you are hungry\nand you have $25 then you will decide to go to restaurant\nand if you're hungry and you don't have $25,\nthen you will just go and buy a hamburger.\nThat's it.\nAll right.\nSo there's about decision tree now moving on ahead.\nLet's see.\nWhat is a random Forest.\nWell random Forest build multiple decision trees\nand merges them together to get a more accurate\nand stable production.\nAll right, most of the time random Forest is trained\nwith a bagging method.\nThe bragging method is based on the idea\nthat the combination\nof learning module increases the overall result.\nIf you are combining the learning from different models\nand then clubbing it together\nwhat it will do it will Increase the overall result fine.\nJust one more thing.\nIf the size of your data set is huge.\nThen in that case one single decision tree would lead\nto our Offutt model same way\nlike a single person might have its own perspective\non the complete population as a population is very huge.\nRight?\nHowever, if we implement the voting system and ask\ndifferent individual to interpret the data,\nthen we would be able to cover the pattern\nin a much meticulous way even from the diagram.\nYou can see that in section A\nwe have Howard large training data set what we do.\nWe first divide our training data set\ninto n sub-samples on it\nand we create a decision tree for each cell sample.\nNow in the B part what we do we take the vote\nout of every decision made by every decision tree.\nAnd finally we Club the vote to get\nthe random Forest dition fine.\nLet's move on ahead.\nNext.\nWe have neighbor Buys.\nSo name bias is a classification technique,\nwhich is based on Bayes theorem.\nIt assumes that it's\nof any particular feature in a class is completely unrelated\nto the presence\nof any other feature named buys is simple\nand easy to implement algorithm and due to a Simplicity\nthis algorithm might out perform more complex model\nwhen the size of the data set is not large enough.\nAll right, a classical use case\nof Navy bias is a document classification.\nAnd that what you do you determine\nwhether a given text corresponds\nto one or more categories in the Texas case,\nthe features used might be the presence or absence.\nAbsence of any keyword.\nSo this was about Nev from the diagram.\nYou can see that using neighbor buys.\nWe have to decide\nwhether we have a disease or not.\nFirst what we do we check the probability\nof having a disease\nand not having the disease right probability\nof having a disease is 0.1\nwhile on the other hand probability of not having\na disease is 0.9.\nOkay first, let's see\nwhen we have disease and we go to the doctor.\nAll right, so when we visited the doctor\nand the test is positive Adjective so probability\nof having a positive test\nwhen you're having a disease is 0.8 0 and probability\nof a negative test\nwhen you already have a disease that is 0.20.\nThis is also a false negative statement as the test\nis detecting negative,\nbut you still have the disease, right?\nSo it's a false negative statement.\nNow, let's move ahead\nwhen you don't have the disease at all.\nSo probability of not having a disease is 0.9.\nAnd when you visit the doctor and the doctor is like, yes,\nyou have the disease.\nBut you already know that you don't have the disease.\nSo it's a false positive statement.\nSo probability of having a disease when you actually\nknow there is no disease is 0.1 and probability\nof not having a disease\nwhen you actually know there is no disease.\nSo and the probability of it is around 0.90 fine.\nIt is same as probability of not having a disease even\nthe test is showing the same results\na true positive statement.\nSo it is 0.9.\nAll right.\nSo let's move on ahead and discuss about kn n algorithm.\nSo this KNN algorithm or the k-nearest neighbor,\nit stores all the available cases\nand classifies new cases based on the similarity measure the K\nin the KNN algorithm as the nearest neighbor,\nwe wish to take vote from for example,\nif k equal 1 then the object is simply assigned to the class\nof that single nearest neighbor from the diagram.\nYou can see the difference in the image\nwhen k equal 1 k equal 3 and k equal 5, right?\nWell the And systems\nare now able to use the k-nearest neighbor\nfor visual pattern recognization to scan\nand detect hidden packages in the bottom bin\nof a shopping cart at the checkout\nif an object is detected\nwhich matches exactly to the object listed\nin the database.\nThen the price of the spotted product could even\nautomatically be added to the customers Bill\nwhile this automated billing practice is not used\nextensively at this time,\nbut the technology has been developed\nand is available for use\nif you want you can just use It and yeah,\none more thing k-nearest neighbor is also used\nin retail to detect patterns\nin the credit card users many new transaction scrutinizing\nsoftware application use\nCayenne algorithms to analyze register data\nand spot unusual pattern\nthat indicates suspicious activity.\nFor example, if register data indicates\nthat a lot of customers information\nis being entered manually rather than through automated scanning\nand swapping then in that case.\nThis could indicate\nthat the employees were using the register.\nIn fact stealing customers personal information or\nif I register data indicates\nthat a particular good is being returned\nor exchanged multiple times.\nThis could indicate\nthat employees are misusing the return policy\nor trying to make money from doing the fake returns, right?\nSo this was about KNN algorithm.\nSo starting with what is decision tree,\nbut first, let me tell you why did we choose\nthe Gentry to start with?\nWell, these decision tree are really very easy to read\nand understand it belongs to one of The few models\nthat interpretable\nwhere you can understand exactly why the classifier has made\nthat particular decision right?\nLet me tell you a fact that for a given data set.\nYou cannot say\nthat this algorithm performs better than that.\nIt's like you cannot say that decision trees\nbetter than a buys\nor name biases performing better than decision tree.\nIt depends on the data set,\nright you have to apply hit and trial method\nwith all the algorithms one\nby one and then compare the result the model\nwhich gives the best result as the Order\nwhich you can use at for better accuracy\nfor your data set.\nAll right, so let's start with what is decision tree.\nWell a decision tree is a graphical representation\nof all the possible solution\nto our decision based on certain conditions.\nNow, you might be wondering why this thing is called\nas decision tree.\nWell, it is called so\nbecause it starts with the root\nand then branches off to a number of solution just\nlike a tree right even the tree starts from a roux\nand it starts growing its branches.\nAs once it gets bigger\nand bigger similarly in a decision tree.\nIt has a roux\nwhich keeps on growing with increasing number of decision\nand the conditions now,\nlet me tell you a real life scenario.\nI won't say that all of you,\nbut most of you must have used it.\nRemember whenever you dial the toll-free number\nof your credit card company,\nit redirects you\nto his intelligent computerised assistant\nwhere it asks you questions like,\npress one for English or press 2 for Henry,\npress 3 for this press 4 for that right now\nonce you select one now again,\nIt redirects you to a certain set\nof questions like press 1 for this press 1 for that\nand similarly, right?\nSo this keeps on repeating\nuntil you finally get to the right person, right?\nYou might think\nthat you are caught in a voicemail hell\nbut what the company was actually doing it\nwas just using a decision tree to get you to the right person.\nI lied.\nI'd like you to focus on this particular image\nfor a moment on this particular slide.\nYou can see I image where the task is.\nShould I accept a new job offer or not?\nAlright, so you have to decide that for That\nwhat you did you created a decision tree starting\nwith the base condition or the root node.\nWas that the basic salary\nor the minimum salary should be $50,000\nif it is not $50,000.\nThen you are not at all accepting the offer.\nAll right.\nSo if your salary is greater than $50,000,\nthen you will further check\nwhether the commute is more than one hour or not.\nIf it is more than one are you will just decline the offer\nif it is less than one hour,\nthen you are getting closer to accepting the job offer then\nfurther what you will do.\nYou will check whether the company is offering.\nFree coffee or not,\nright if the company is not offering the free coffee,\nthen you will just decline the offer\nand have fit as offering the free coffee.\nAnd yeah, you will happily accept the offer right?\nThis is just an example of a decision tree.\nNow, let's move ahead and understand a decision tree.\nWell, here is a sample data set\nthat I will be using it to explain you\nabout the decision tree.\nAll right in this data set each row is an example.\nAnd the first two columns provide features or attributes\nthat describes the data\nand the last column gives the label\nor the class we want to predict\nand if you like you can just modify this data\nby adding additional features\nand more example\nand our program will work in exactly the same way fine.\nNow this data set is pretty straightforward\nexcept for one thing.\nI hope you have noticed that it is not perfectly separable.\nLet me tell you something more about that as\nin the second and fifth examples they have the same features.\nBut different labels both have yellow as a Colour\nand diameter as three,\nbut the labels are mango and lemon right?\nLet's move on and see\nhow our decision tree handles this case.\nAll right, in order to build a tree will use a decision tree\nalgorithm called card this card algorithm\nstands for classification\nand regression tree algorithm online.\nLet's see a preview of how it works.\nAll right to begin with We'll add a root node\nfor the tree and all the nodes receive a list\nof rows as a input\nand the route will receive the entire training data set now\neach node will ask true and false question\nabout one other feature.\nAnd in response to that question will split\nor partition the data set into two different subsets\nthese subsets then become input to child node.\nWe are to the tree\nand the goal of the question is to finally unmix the labels\nas we proceed down or in other words to produce\nthe purest possible distribution of the labels at each node.\nFor example, the input of this node contains only.\nOne single type of label so we could say\nthat it's perfectly unmixed.\nThere is no uncertainty about the type of label\nas it consists of only grapes right\non the other hand the labels in this node are still mixed up.\nSo we would ask another question to further drill it down,\nright but before that we need to understand which question to ask\nand when and to do\nthat we need to conduct\nby how much question helps to unmix the label\nand we can quantify the amount of Uncertainty\nat a single node using a metric called gini impurity\nand we can quantify\nhow much a question reduces\nthat uncertainty using a concept called information game will use\nthese to select the best question to ask at each point.\nAnd then what we'll do we'll iterate the steps\nwill recursively build the tree\non each of the new node will continue dividing the data\nuntil there are no further question to ask\nand finally we reach to our Leaf.\nAlright, alright, so this was about decision tree.\nSo in order to create a diversion First of all\nwhat you have to do you have to identify\ndifferent set of questions\nthat you can ask to a tree like is this color green\nand what will be these question this question will be decided by\nyour data set like as this colored green\nas the diameter greater than equal to 3 is the color\nyellow right questions resembles to your data set remember that?\nAll right.\nSo if my color is green,\nthen what it will do it will divide into two part first.\nThe Green Mango will be in the true while on the false.\nWe have lemon and the map all right.\nAnd if the color is green\nor the diameter is greater than equal to 3\nor the color is yellow.\nNow let's move on\nand understand about decision tree terminologies.\nAlright, so starting\nwith root node root node is a base node of a tree\nthe entire tree starts from a root node.\nIn other words.\nIt is the first node of a tree it represents\nthe entire population or sample\nand this entire population is further segregated\nor divided into two or more homogeneous set.\nFine.\nNext is the leaf node.\nWell, Leaf node is the one\nwhen you reach at the end of the tree,\nright that is you cannot further segregated down\nto any other level.\nThat is the leaf node.\nNext is splitting splitting is dividing your root node\nor node into different sub part on the basis of some condition.\nAll right, then comes the branch or the sub tree.\nWell, this Branch or subtree gets formed\nwhen you split the tree suppose when you split a root node,\nit gets divided into two branches\nor two subtrees right next.\nThe concept of pruning.\nWell, you can say that pruning is just opposite\nof splitting what we are doing here.\nWe are just removing the sub node of a decision tree\nwill see more about pruning later in this session.\nAll right, let's move on ahead.\nNext is parent or child node.\nWell, first of all root node is always the parent node\nand all other nodes\nassociated with that is known as child node.\nWell, you can understand it in a way that all the top node\nbelongs to a parent node and all the bottom node\nwhich are derived from a Top node zhi node the node\nproducing a further note is a child node and the node\nwhich is producing.\nIt is a parent node simple concept, right?\nLet's use the cartel Gotham and design a tree manually.\nSo first of all,\nwhat you do you decide which question to ask\nand when so how will you do that?\nSo let's first of all visualize the decision tree.\nSo there's the decision tree which will be creating manually\nor like first of all,\nlet's have a look at the Data set you have\nOutlook temperature humidity and windy\nas you have different attributes on the basis of\nthat you have to predict that whether you can play or not.\nSo which one among them should you pick first answer determine\nthe best attribute that classifies the training data?\nAll right.\nSo how will you choose the best attribute\nor how does a tree decide\nwhere to split or how the tree will decide its root node?\nWell before we move on\nand split a tree there are some terminologies\nthat you should know.\nAll right first being the gini index.\nX so what is this gini Index?\nThis gini index is the measure of impurity or Purity used\nin building a decision Tree in cartel Gotham.\nAll right.\nNext is Information Gain this Information Gain is\nthe decrease in entropy\nafter data set is split on the basis of an attribute\nconstructing a decision tree is all about finding an attribute\nthat Returns the highest Information Gain.\nAll right, so you will be selecting the node\nthat would give you the highest Information Gain.\nAlright next is reduction in variance.\nReduction in variance is an algorithm which is used\nfor continuous Target variable or regression problems.\nThe split with lower variance is selected as a criteria to let\nthe population see in general term.\nWhat do you mean by variance?\nVariance is how much your data is wearing?\nRight?\nSo if your data is less impure or is more pure\nthan in that case the variation would be less\nas all the data almost similar, right?\nSo there's also a way of setting a tree the split\nwith lower variance\nis selected as the criteria to split the population.\nAll right.\nNext is the chi Square t Square.\nIt is an algorithm\nwhich is used to find out these statistical significance\nbetween the differences between sub nodes\nand the parent nodes fine.\nLet's move ahead now the main question is\nhow will you decide the best attribute\nfor now just understand\nthat you need to calculate something known as\ninformation game the attribute\nwith the highest Information Gain is considered the best.\nYeah.\nI know your next question might be like what?\nThis information,\nbut before we move on and see\nwhat exactly Information Gain Is let me first introduce you\nto a term called entropy\nbecause this term\nwill be used in calculating the Information Gain.\nWell entropy is just a metric\nwhich measures the impurity of something or in other words.\nYou can say that as the first step to do\nbefore you solve the problem of a decision tree\nas I mentioned is something about impurity.\nSo let's move on and understand what is impurity suppose.\nYou are a basket full of apples\nand another Bowl Which is full of same label,\nwhich says Apple now\nif you are asked to pick one item\nfrom each basket and ball,\nthen the probability of getting the apple\nand it's correct label is 1 so in this case, you can say\nthat impurities zero.\nAll right.\nNow what if there are four different fruits\nin the basket and four different labels in the ball,\nthen the probability of matching the fruit\nto a label is obviously not one.\nIt's something less than that.\nWell, it could be possible\nthat I picked banana from the basket\nand when I randomly picked Level from the ball.\nIt says a cherry any random permutation\nand combination can be possible.\nSo in this case, I'd say that impurities is nonzero.\nI hope the concept of impurities here.\nSo coming back to entropy\nas I said entropy is the measure of impurity\nfrom the graph on your left.\nYou can see that\nas the probability is zero or one\nthat is either they are highly impure\nor they are highly pure than in that case the value\nof entropy is zero.\nAnd when the probability is 0.5 then the value of entropy.\nIs maximum.\nWell, what is impurity impurities the degree\nof Randomness how random data is\nso if the data is\ncompletely pure in that case the randomness equals zero or\nif the data is completely empty or even in that case\nthe value of impurity will be zero question.\nLike why is it\nthat the value of entropy is maximum\nat 0.5 might arise in a mine, right?\nSo let me discuss about that.\nLet me derive it mathematically\nas you can see here on the slide the mathematical formula\nof entropy is -\nof probability of yes,\nlet's move on and see\nwhat this graph has to say mathematically suppose s is\nour total sample space and it's divided into two parts.\nYes, and no like in our data set the result\nfor playing was divided into two parts.\nYes or no,\nwhich we have to predict either we have to play or not.\nRight?\nSo for that particular case,\nyou can Define the formula of entropy as entropy\nof total sample space equals negative\nof probability of e\nis multiplied by log of probability.\nWe of yes,\nwhether base 2 minus probability of no X log of probability of no\nwith base to where s is your total sample space\nand P of v s is the probability of e s--\nand p-- of know is the probability of no.\nWell, if the number of BS equal number of know\nthat is probability of s equals 0.5 right\nsince you have equal number of BS and know so\nin that case the value\nof entropy will be one just put the value over there.\nAll right.\nLet me just move to Next slide I'll show you this.\nAlright next is if it contains all Yes,\nor all know that is probability of a sample space is either 1\nor 0 then in that case entropy will be equal to 0\nLet's see the mathematically one by one.\nSo let's start with the first condition\nwhere the probability was 0.5.\nSo this is our formula for entropy, right?\nSo there's our first case right which will discuss the art\nwhen the probability of vs equal probability of node\nthat is in our data set we have Rule number of yes, and no.\nAll right.\nSo probability of yes equal probability of no\nand that equals 0.5 or in other words,\nyou can say that yes\nplus no equal to Total sample space.\nAll right, since the probability is 0.5.\nSo when you put the values\nin the formula you get something like this\nand when you calculate it,\nyou will get the entropy of the total sample space as one.\nAll right.\nLet's see for the next case.\nWhat is the next case either you have totally us\nor you have to No,\nso if you have total, yes,\nlet's see the formula when we have total.\nYes.\nSo you have all yes and 0 no fine.\nSo probability of e s equal one.\nAnd yes as the total sample space obviously.\nSo in the formula when you put that thing up here,\nyou get entropy\nof sample space equal negative X of 1 multiplied by log of 1\nas the value of log 1 equals 0.\nSo the total thing will result to 0 similarly is the case\nwith no even in that case you will get the entropy\nof total sample.\nCase as 0 so this was all about entropy.\nAll right.\nNext is what is Information Gain?\nWell Information Gain\nwhat it does is it measures the reduction in entropy.\nIt decides which attribute\nshould be selected as the decision node.\nIf s is our total collection\nthan Information Gain equals entropy,\nwhich we calculated just now that -\nweighted average multiplied by entropy of each feature.\nDon't worry.\nWe'll just see\nhow it to calculate it with an example.\nAll right.\nSo let's manually build a decision tree\nfor our data set.\nSo there's our data set\nwhich consists of 14 different instances\nout of which we have nine.\nYes and five know I like so we have the formula\nfor entropy just put over that since 9 years.\nSo total probability of e s equals 9\nby 14 and total probability of no equals Phi by 14\nand when you put up the value\nand calculate the result you will get the value.\nOh of entropy as 0.94.\nAll right.\nSo this was your first step\nthat is compute the entropy for the entire data set.\nAll right.\nNow you have to select\nthat out of Outlook temperature humidity and windy,\nwhich of the node should you select as the root node\nbig question, right?\nHow will you decide that?\nThis particular node should be chosen at the base note\nand on the basis of\nthat only I will be creating the entire tree.\nI will select that.\nLet's see so you have to do\nit one by one you have to calculate the entropy\nand Information Gain for all\nof the Front note so starting with Outlook.\nSo Outlook has\nthree different parameters Sunny overcast and rainy.\nSo first of all select how many number of years\nand no are there in the case of Sunny like when it is sunny\nhow many number of years\nand how many number of nodes are there?\nSo in total we have to yes\nand three Nos and case of sunny in case of overcast.\nWe have all yes.\nSo if it is overcast then will surely go to play.\nIt's like that.\nAlright and next it is rainy then total number of vs equal.\nThree and total number of no equals 2 fine next\nwhat we do we calculate the entropy\nfor each feature for here.\nWe are calculating the entropy when Outlook equals Sunny.\nFirst of all,\nwe are assuming that Outlook is our root node\nand for that we are calculating the information gain for it.\nAlright.\nSo in order to calculate the Information Gain remember\nthe formula it was entropy of the total sample space -\nweighted average X entropy of each feature.\nAll right.\nSo what we are doing here,\nwe are calculating the entropy of out.\nLook when it was sunny.\nSo total number of yes,\nwhen it was sunny was to and total number of know\nthat was three fine.\nSo let's put up in the formula\nsince the probability of yes is 2 by 5\nand the probability of no is 3 by 5.\nSo you will get something like this.\nAlright, so you are getting the entropy\nof sunny as zero point nine seven one fine.\nNext we will calculate the entropy for overcast\nwhen it was overcast.\nRemember it was all yes, right.\nSo the probability\nof yes is equal 1 and when you put over\nthat you will get the value of entropy as 0 fine\nand when it was rainy rainy has 3s and to nose.\nSo probability of e s in case of Sonny's 3 by 5\nand probability of know in case of Sonny's 2 by 5.\nAnd when you add the value of probability of vs\nand probability of no to the formula,\nyou get the entropy of sunny as zero point nine seven one point.\nNow, you have to calculate\nhow much information you are getting from Outlook\nthat equals weighted average.\nAll right.\nSo what was this?\nTo diverge total number of years and total number of no fine.\nSo information from Outlook equals 5 by 14 from\nwhere does this 5 came over?\nWe are calculating\nthe total number of sample space within that particular Outlook\nwhen it was sunny, right?\nSo in case of Sunny there was two years and three NOS.\nAll right.\nSo weighted average for Sonny would be equal to 5 by 14.\nAll right,\nsince the formula was five by 14 x entropy of each feature.\nAll right, so\nas calculated the entropy He for Sonny is zero point nine.\nSeven one, right?\nSo what we'll do we'll multiply 5 by 14 with 0.97 one.\nRight?\nWell, this was the calculation for information\nwhen Outlook equal sunny,\nbut Outlook even equals overcast and rainy for in that case.\nWhat we'll do again similarly will calculate for everything\nfor overcast and sunny\nfor overcast weighted averages\nfor by 14 multiplied by its entropy.\nThat is 0 and for Sonny it is same Phi by 14.\nYes, and to Knows X its entropy\nthat is zero point nine seven one.\nAnd finally we'll take the sum of all of them which equals\nto 0.693 right next.\nWe will calculate the information gained this\nwhat we did earlier was information taken from Outlook.\nNow, we are calculating.\nWhat is the information?\nWe are gaining from Outlook right.\nNow this Information Gain\nthat equals to Total entropy minus the information\nthat is taken from Outlook.\nAll right, so So total entropy we had 0.94 -\ninformation we took from Outlook as 0.693.\nSo the value of information gained from Outlook results\nto zero point two four seven.\nAll right.\nSo next what we have to do.\nLet's assume that Wendy is our root node.\nSo Wendy consists of two parameters false and true.\nLet's see how many years\nand how many nodes are there in case of true and false.\nSo when Wendy has Falls as its parameter,\nthen in that case it has six years and to knows.\nAnd when it as true as its parameter,\nit has 3 S and 3 nodes.\nAll right.\nSo let's move ahead\nand similarly calculate the information taken from Wendy\nand finally calculate the information gained from Wendy.\nAlright, so first of all,\nwhat we'll do we'll calculate the entropy\nof each feature starting with windy equal true.\nSo in case of true we had equal number of yes\nand equal number of no will remember the graph\nwhen we had the probability as\n0.5 as total number of years equal total number of know.\nFor that case the entropy equals 1\nso we can directly write entropy of room\nwhen it's windy is one\nas we had already proved it\nwhen probability equals 0.5 the entropy is the maximum\nthat equals to 1.\nAll right.\nNext is entropy of false when it is windy.\nAll right, so similarly just put the probability of yes\nand no in the formula and then calculate the result\nsince you have six years and two nodes.\nSo in total,\nyou'll get the probability of e S6 by 8 and probability\nof know Two by eight.\nAll right, so when you will calculate it,\nyou will get the entropy\nof false as zero point eight one one.\nAlright, now, let's calculate the information from windy.\nSo total information collected from Windy\nequals information taken\nwhen Wendy equal true plus information taken\nwhen when D equals false.\nSo we'll calculate the weighted average for each one of them\nand then we'll sum\nit up to finally get the total information taken from windy.\nSo in this case,\nit equals to 8 by 14 multiplied by 0.8 1 1 + 6 y 14 x 1\nwhat is this?\n8 it is total number of yes, and\nno in case when when D equals false, right?\nSo when it was false, so total number of BS\nthat equals to 6 and total more of know that equal to 2\nthat some herbs to 8.\nAll right.\nSo that is why the weighted average results to Aid by\n14 similarly information taken\nwhen windy equals true equals to 3 plus 3 that is 3 S\nand 3 no equal 6 divided by total number of sample space.\nThat is 14 x That is entropy of true.\nAll right, so it is a\nby 14 multiplied by 0.8 1 1 plus 6 by 14 x one\nwhich results to 0.89 to this\nis information taken from Windy.\nAll right.\nNow how much information you are gaining from Wendy.\nSo for that what you will do so total information gained\nfrom Windy that equals to Total entropy -\ninformation taken from Windy.\nAll right, that is 0.94 -\n0.89 to that equals to zero point zero four eight.\nAnd so 0.048 is the information gained from Windy.\nAll right.\nSimilarly we calculated for the rest to all right.\nSo for Outlook as you can see,\nthe information was 0.693.\nAnd it's Information Gain was zero point two four seven\nin case of temperature.\nThe information was around\nzero point nine one one and the Information Gain\nthat was equal to 0.02 9 in case of humidity.\nThe information gained was 0.15 to and in the case of windy.\nThe information gained was 0.048.\nSo what we'll do we'll select the attribute.\nWith a maximum fine.\nNow, we are selected Outlook as our root node,\nand it is further subdivided\ninto three different parts Sunny overcast and rain,\nso in case of overcast we have seen\nthat it consists of all.\nYes, so we can consider it as a leaf node,\nbut in case of sunny and rainy,\nit's doubtful as it consists of both.\nYes and both know\nso you need to recalculate the things right again\nfor this node.\nYou have to recalculate the things.\nAll right, you have to again select the attribute.\nIs having the maximum Information Gain.\nAll right, so there's\nhow your complete tree will look like.\nAll right.\nSo, let's see when you can play so you can play\nwhen Outlook is overcast.\nAll right, in that case.\nYou can always play if the Outlook is sunny.\nYou will further drill down to check the humidity condition.\nAll right, if the humidity is normal,\nthen you will play\nif the humidity is high then you won't play right\nwhen the Outlook predicts\nthat it's rainy then further you will check\nwhether it's windy or not.\nIf it is a week went then you will go and offer.\nSay but if it has strong wind, then you won't play right?\nSo this is how your entire decision tree would look\nlike at the end.\nNow comes the concept of pruning say is\nthat what should I do to play?\nWell you have to do pruning pruning will decide\nhow you will play.\nWhat is this pruning?\nWell, this pruning is nothing but cutting down the nodes\nand order to get the optimal solution.\nAll right.\nSo what pruning does it reduces the complexity?\nAll right as are you can see on the screen\nthat it showing only the result for you.\nThat is it showing all the result which says\nthat you can play.\nAll right before we drill down to a practical session\na common question might come in your mind.\nYou might think\nthat our tree base model better than cleaner model, right?\nYou can think like if I can use a logistic regression\nfor classification problem\nand linear regression for regression problem.\nThen why there is a need to use the tree.\nWell many of us have this In in their mind and well,\nthere's a valid question too.\nWell, actually as I said earlier,\nyou can use any algorithm.\nIt depends on the type of problem.\nYou're solving let's look at some key factor,\nwhich will help you to decide which algorithm to use and\nwhen so the first point being\nif the relationship between dependent and independent\nvariable as well approximated\nby a linear model then linear\nregression will outperform tree base model second case\nif there is a high non-linearity and complex\nrelationship between Lent\nand independent variables at remodel will outperform\na classical regression model in third case.\nIf you need to build a model\nwhich is easy to explain to people a decision tree model\nwill always do better than a linear model\nas the decision tree models\nare simpler to interpret then linear regression.\nAll right.\nNow, let's move on ahead and see\nhow you can write it as Gentry classifier from scratch\nand python using the card algorithm.\nAll right for this.\nI will be using jupyter notebook with python 3.0.\nOh install on it.\nAlright, so let's open the Anaconda\nand the jupyter notebook.\nWhereas that so this\nis a inner Corner Navigator and I will directly jump over\nto jupyter notebook and hit the launch button.\nI guess everyone knows that jupyter.\nNotebook is a web-based interactive Computing notebook\nenvironment where you can run your python codes.\nSo my jupyter notebook.\nIt opens on my Local Host double 8 9 1\nso I will be using this jupyter notebook\nin order to write my decision tree classifier\nusing python for this decision tree classifier.\nI have already written.\nSet of codes.\nLet me explain you just one by one.\nSo we'll start with initializing our training data set.\nSo there's our sample data set\nfor which each row is an example.\nThe last column is a label\nand the first two columns are the features.\nIf you want you can add some more features an example\nfor your practice interesting fact is\nthat this data set is designed in a way\nthat the second and fifth example have almost\nthe same features,\nbut they have different labels.\nAll right.\nSo let's move on and see how the tree handles this case\nas you can see here both.\nBoth of them the second\nand the fifth column have the same features.\nWhat did different is just their label?\nRight?\nSo let's move ahead.\nSo this is our training data set next what we are doing we\nare adding some column labels.\nSo they are used only to print the trees fine.\nSo what we'll do we'll add header to the columns\nlike the First Column is of color second is of diameter\nand third is a label column.\nAlright, next Road will do will Define\na function as unique values in which will pass the rows\nand the columns.\nSo this function what it will do.\nWe find the unique values for a column in the data set.\nSo this is an example for that.\nSo what we are doing here,\nwe are passing training data Hazard row\nand column number as 0 so\nwhat we are doing we are finding unique values in terms of color.\nAnd in this\nsince the row is training data and the column is 1\nso what you are doing here,\nso we are finding the unique values\nin terms of diameter fine.\nSo this is just an example next\nwhat we'll do we'll Define a function as class count\nand we'll pass zeros into it.\nSo what it does it counts the number of each type\nof Example within data set.\nSo in this function\nwhat we are basically doing we are counting the number\nof each type for example\nin the data set or what we are doing.\nWe are counting the unique values for the label\nin the data set as a sample.\nYou can see here.\nWe can pass that entire training data set\nto this particular function as class underscore count\nwhat it will do it will find all the different types of label\nwithin the training data set\nas you can see here the unique label consists of mango grape\nand lemon so next what we'll do we'll Define a function\nis numeric and we'll pass a value into it.\nSo what it Do it.\nWe'll just test\nif the value is numeric or not and it will return\nif the value is an integer or a float.\nFor example, you can see is numeric.\nWe are passing 7 so it is an integer\nso it will return in value and\nif we are passing red it's not a numeric value, right?\nSo moving on ahead\nwhere you define a class named as question.\nSo what this question does\nthis question is used to partition the data set.\nThis class voted does it just records a column number?\nFor example 0 for color a light and a column value for example,\ngreen Next what we are doing we are defining a match method\nwhich is used to compare the feature value\nin the example.\nThe feature values stored in the question.\nLet's see how first of all what you are doing.\nWe're defining an init function and inside\nthat we are passing the self column\nand the value as parameter.\nSo next what we do we Define a function\nas match what it does is it compares the feature value\nin an example to the feature value in this question\nwhen next we'll Define a function as re PR,\nwhich is just a helper method to print the question\nin a readable format.\nNext what we are doing we are defining a function partition.\nWell, this function is used to partition\nthe data set each row in the data set it checks\nif it matched the question or not\nif it does so it adds it to the true rose or if not,\nthen it adds to the false Rose.\nAll right, for example,\nas you can see, it's partition the training data set based on\nwhether the rows are ready or not here.\nWe are calling the function question\nand we are passing a value of zero and read to it.\nSo what did we do?\nIt will assign all the red rose to True underscore Rose.\nAnd everything else will be assigned\nto false underscore rose fine.\nNext what we'll do we'll Define a gini impurity function\nand inside that will pass the list of rows.\nSo what it will do it will just calculate the dream Purity\nfor the list of rows.\nNext what we are doing every defining a function\nas Information Gain.\nSo what this Information Gain function does it calculates\nThe Information Gain using the uncertainty\nof the starting node -\nthe weighted impurity of the child node.\nThe next function is find the best plate.\nWell, this function is used to find the best question to ask\nby iterating over every feature of value\nand then calculating the information game.\nFor the detail explanation on the code.\nYou can find the code in the description given below.\nAll right next we'll define a class as leave\nfor classifying the data.\nIt holds a dictionary of glass like mango for how many times\nit appears in the row from the training data\nthat reaches the sleeve.\nAlright next is the decision node.\nSo this decision node, it will ask a question.\nThis holds a reference to the question\nand the two child nodes on the base of\nthat you are deciding which node to add further to which branch.\nAlright so next video.\nWe're defining a function of Beltre and inside\nthat we are passing our number of rows.\nSo this is the function that is used to build the tree.\nSo initially what we did we Define all the various function\nthat we'll be using in order to build a tree.\nSo let's start\nby partitioning the data set for each unique attribute,\nthen we'll calculate the information gain\nand then return the question\nthat produces the highest gain\nand on the basis of that will split the tree.\nSo what we are doing here,\nwe are partitioning the data set calculating\nthe Information Gain.\nAnd then what this is returning it is returning the question\nthat is producing the highest gain.\nAll right.\nNow if gain equals 0 return Leaf Rose,\nso what it will do.\nSo if we are getting no for the gain\nthat is gain equals 0 then in that case\nsince no further question could be asked\nso what it will do it will return a leaf fine now true\nor underscore Rose\nor false underscore Rose equal partition with rose\nand the question.\nSo if we are reaching tell this position,\nthen you have already found a Value\nwhich will be used to partition the data set then\nwhat you will do you will recursively build\nthe true branch\nand similarly recursively build the false Branch.\nSo return Division and Discord node and side\nthat will be passing question true branch and false Branch.\nSo what it will do it will return a question node.\nThis question node this recalls the best feature\nor the value to ask at this point fine.\nNow that we have Builder tree next\nwhat we'll do we'll Define a print underscore tree function\nwhich will be used to print the tree fine.\nSo finally what we are doing in this particular function\nthat we are printing our tree next is the classify function\nwhich will use it to decide\nwhether to follow the true Branch or the false branch\nand then compared\nto the feature values stored in the node to the example.\nWe are considering and last what we'll do\nwe'll finally print the production at the leaf.\nSo let's execute it and see okay,\nso there's our testing data.\nOnline so we printed a leaf as well.\nNow that we have trained our algorithm is\nour training data set now it's time to test it.\nSo there's our testing data set.\nSo let's finally execute it and see what is the result.\nSo this is the result you will get so first question,\nwhich is asked by the algorithm is is diameter greater\nthan equal to 3,\nif it is true,\nthen it will further ask if the color is yellow again,\nif it is true,\nthen it will predict mango as one and lemon with one.\nAnd in case it is false,\nthen it will just predict the mango.\nNow.\nThis was the true part.\nNow next coming to diameter is not greater\nthan or equal to 3 then in that case it's false.\nAnd what did we do?\nIt'll just predict the grape vine.\nOkay.\nSo this was all about the coding part now,\nlet's conclude this session.\nBut before concluding let me just show you one more thing.\nNow.\nThere's a scikit-learn algorithm cheat sheet,\nwhich explains you\nwhich algorithm you should use and when all right,\nlet's build in a decision tree format.\nAt let's see how it is Big.\nSo first condition it will check\nwhether you have 50 samples or not.\nIf your samples are greater than 50,\nthen we'll move ahead if it is less than 50,\nthen you need to collect more data\nif your sample is greater than 50,\nthen you have to decide\nwhether you want to predict a category or not.\nIf you want to predict a category,\nthen further you will see\nthat whether you have labeled data or not.\nIf you have label data, then\nthat would be a classification algorithm problem.\nIf you don't have the label data,\nthen it would be a clustering problem.\nNow if you don't want to The category then\nwhat you want to protect predict a quantity.\nWell, if you want to predict a quantity,\nthen in that case,\nit would be a regression problem.\nIf you don't want to predict\na quantity and you want to keep looking further,\nthen in that case,\nyou should go for dimensionality reduction problems and still\nif you don't want to look\nand the predicting structure is not working.\nThen you have tough luck for that.\nI hope this doesn't recession clarifies all your doubt\nover decision tree algorithm.\nNow, we'll try to find out the answer to this particular\nquestion as to why we need random Forest fine.\nSo like human beings learn from the past experiences.\nSo unlike human beings a computer does not have\nexperiences then how does machine takes decisions?\nWhere does it learn from?\nWell a computer system actually\nlearns from the data which represents some past experiences\nof an application domain.\nSo now let's see,\nhow random Forest It's in building up in learning model\nwith a very simple use case of credit risk detection.\nNow needless to say\nthat credit card companies\nhave a very nested interest in identifying\nFinancial transactions\nthat are illegitimate and criminal in nature.\nAnd also I would like to mention this point\nthat according to the Federal Reserve payments\nstudy Americans used credit cards to pay\nfor twenty six point two million purchases in 2012\nand The estimated loss due to unauthorized transactions\nthat here was u.s.\n6 point 1 billion dollars now\nin the banking industry measuring risk is very critical\nbecause the stakes are too high.\nSo the overall goal is actually to figure out\nwho all can be fraudulent\nbefore too much Financial damage has been done.\nSo for this a credit card company receives thousands\nof applications for new cards\nand each application contains information.\nMission about an applicant, right?\nSo so here as you can see that from all those applications\nwhat we can actually figure out is\nthat predictor variables.\nLike what is the marital status of the person?\nWhat is the gender of the person?\nWhat is the age of the person and the status which is actually\nwhether it is a default pair or non-default pair.\nSo default payments are basically when payments\nare not made in time\nand according to the agreement signed by the cardholder.\nSo now that account is actually set to be in the default.\nSo you can easily figure out the history\nof the particular card holder from this then we can also look\nat the time of payment\nwhether he has been a regular pair\nor non regular one.\nWhat is the source of income for that particular person\nand so and so forth.\nSo to minimize loss the back actually needs\ncertain decision rule to predict\nwhether to approve\nParticular no one of that particular person or not.\nNow here is\nwhere the random Forest actually comes into the picture.\nAll right.\nNow, let's see how random Forest can actually help us\nin this particular scenario.\nNow, we have taken randomly\ntwo parameters out of all the predictive variables\nthat we saw previously now,\nwe have taken two predictor variables here.\nThe first one is the income\nand the second one is the H right\nand Hurley parallel\nit to decision trees have been implemented\nupon those predicted variables and let's first assume the case\nof the income variable right?\nSo here we have divided our income into three categories\nthe first one being the person earning over $35,000 second\nfrom 15 to 35 thousand dollars the third one running\nin the range of 0 to 15 thousand dollars.\nNow if a person is earning over $35,000,\nwhich is a pretty Good income pretty decent.\nSo now we'll check out for the credit history.\nAnd here the probability is\nthat if a person is earning a good amount then\nthere is very low risk\nthat he won't be able to pay back already earning good.\nSo the probability is\nthat his application of loan will get approved.\nRight?\nSo there is actually low risk or moderate risk,\nbut there's no real issue of higher risk as such.\nWe can approve the applicants request here.\nNow, let's move on and watch out for the second category\nwhere the person is actually earning\nfrom 15 to 35 thousand dollars right now here the person may\nor may not pay back.\nSo in such scenarios will look for the credit history as\nto what has been his previous history.\nNow if his previous history has been bad\nlike he has been a default ER in the previous transactions\nwill definitely not Consider approving his request\nand he will be\nat the high risk in which is not good for the bank.\nIf the previous history\nof that particular applicant is really good.\nThen we will just to clarify\na doubt will consider another parameter as well\nthat will be on depth.\nI have his already in really high dip then\nthe risks again increases and there are chances\nthat he might not pay repay in the future.\nSo here Will.\nNot accept the request of the person having high dipped\nif the person is in the low depth\nand he has been a good pair in his past history.\nThen there are chances\nthat he might be back and we can consider\napproving the request of this particular applicant.\nAlex look at the third category,\nwhich is a person earning from 0 to 15 thousand dollars.\nNow, this is something which actually raises I broke\nand this person will actually lie\nin the category of high risk.\nAll right.\nSo the probability is\nthat his application of loan would probably get rejected now,\nwe'll get one final outcome from this income parameter, right?\nNow let us look at our second variable\nthat is H which will lead into the second decision tree.\nNow.\nLet us say if the person is Young, right?\nSo now we will look forward to if it is a student now\nif it is a student then the chances are high\nthat he won't be able to repay back\nbecause he has no earning Source, right?\nSo here the risks are too high and probability is\nthat his application of loan will get rejected fine.\nNow if the person is Young\nand his Not the student then we'll probably go on\nand look for another variable.\nThat is pan balance.\nNow.\nLet's look if the bank balance is less than 5 lakhs.\nSo again the risk arises and the probabilities\nthat his application of loan will get rejected.\nNow if the person is Young is not a student\nand his bank balance so\nof greater than 5 lakhs is got a pretty good\nand stable and balanced then the probability is\nthat he is sort of application\nwill get approved of Now let us take another scenario\nif he's a senior, right?\nSo if he is a senior will probably go and check out\nfor this credit history.\nHow well has he been in his previous transactions?\nWhat kind of a person he is like\nwhether he's a defaulter or is Ananda falter.\nNow if he is a very fair kind of person\nin his previous transactions then again the risk arises\nand the probability of his application\ngetting rejected actually increases right now\nif he has An excellent person as\nper his transactions in the previous history.\nSo now again here there is least risk\nand the probabilities\nthat his application of loan will get approved.\nSo now here these two variables income and age have led\nto two different decision trees.\nRight and these two different decision trees actually led\nto two different results.\nNow what random forest does is it will actually compile\nthese two different results from these two different.\nGentry's and then finally,\nit will lead to a final outcome.\nThat is how random Forest actually works.\nRight?\nSo that is actually the motive of the random Forest.\nNow let us move forward and see what is random Forest right?\nYou can get an idea\nof the mechanism from the name itself random forests.\nSo a collection of trees is a fortress\nthat's why I called for is probably and here\nalso the trees are actually because being trained on subsets\nwhich are being selected at random.\nAnd therefore they are called random forests So Random forests\nis a collection or an insane.\nHumble of decision trees right here decision trees actually\nbuilt using the whole data set considering all features,\nbut actually in random Forest only a fraction of the number\nof rows is selected\nand that too at random\nand a particular number of features,\nwhich are actually selected at random are trained\nupon and that is\nhow the decision trees are built upon.\nRight?\nSo similarly number of decision trees will be grown\nand each decision tree will Salt into a certain final outcome\nand random Forest will do nothing\nbut actually just compiled the results\nof all those decision trees to bring up the final result.\nAs you can see in this particular figure\nthat a particular instance actually has resulted\ninto three different decision trees, right?\nSo not tree one results into a final outcome called Class A\nand tree to results\ninto class B. Similarly tree three results into class P\nSo Random Forest\nwill compile the results of all these Decision trees\nand it will go by the call of the majority voting now\nsince head to decision trees have actually voted\ninto the favor of the Class B that is decision tree 2 and 3.\nTherefore the final outcome will be in the favor of the Class B.\nAnd that is how random Forest actually works upon.\nNow one really\nbeautiful thing about this particular algorithm is\nthat it is one of the versatile algorithms\nwhich is capable of Performing both regression as well as Now,\nlet's try to understand random Forest further\nwith a very beautiful example or this is my favorite one.\nSo let's say you want to decide\nif you want to watch edge of tomorrow or not, right?\nSo in this particular scenario,\nyou will have two different actions to work Bond either.\nYou can just straight away go\nto your best friend asked him about.\nAll right,\nwhether should I go for Edge of Tomorrow not will I\nlike this movie or you can ask Your friends\nand take their opinion consideration and then based\non the final results\nwho can go out and watch Edge of Tomorrow, right?\nSo now let's just take the first scenario.\nSo where you go to your best friend asked about\nwhether you should go out to watch edge\nof tomorrow or not.\nSo your friend will probably ask you certain questions\nlike the first one being here Jonah So so let's say\nyour friend asks you\nif you really like The Adventurous kind\nof movies or not.\nSo you say yes,\ndefinitely I would love to watch it Venture kind of movie.\nSo the probabilities\nthat you will like edge of tomorrow as well.\nSince Age of Tomorrow is also a movie of Adventure\nand sci-fi kind of Journal right?\nSo let's say you do not like the adventure John a movie.\nSo then again the probability reduces\nthat you might really not like edge of Morrow right.\nSo from here you can come to a certain conclusion right?\nLet's say your best friend puts you into another situation\nwhere he'll ask you\nor a do you like Emily Blunt and you see definitely\nI like Emily Blunt and then he puts another question to you.\nDo you like Emily Blunt to be in the main lead\nand you say yes, then again,\nthe probability arises\nthat you will definitely like edge of tomorrow as\nwell because Edge of Tomorrow is Has the Emily plant\nin the main lead cast so\nand if you say oh I do not like Emily Blunt then again,\nthe probability reduces\nthat you would like Edge of Tomorrow to write.\nSo this is one way\nwhere you have one decision tree and your final outcome.\nYour final decision will be based on your one decision tree,\nor you can see your final outcome will be based\non just one friend.\nNo, definitely not really convinced.\nYou want to consider the options of your other friends also\nso that you can make very precise and crisp\ndecision right you go out\nand you approach some other bunch of friends of yours.\nSo now let's say you go to three of your friends\nand you ask them the same question\nwhether I would like to watch it off tomorrow or not.\nSo you go out and approach\nthree or four friends friend one friend twin friend three.\nNow, you will consider each of their Sport\nand then you will your decision now will be dependent\non the compiled results of all of your three friends, right?\nNow here, let's say you go to your first friend\nand you ask him\nwhether you would like to watch it just tomorrow\nnot and your first friend puts you to one question.\nDid you like Top Gun?\nAnd you say yes,\ndefinitely I did like the movie Top Gun then the probabilities\nthat you would like edge of tomorrow as\nwell because topgun is actually a military action drama,\nwhich is also Tom Cruise.\nSo now again the probability Rises that yes,\nyou will like edge of tomorrow as well and\nIf you say no I didn't like Top Gun then again.\nThe chances are\nthat you wouldn't like Edge of Tomorrow, right?\nAnd then another question that he puts you across is\nthat do you really like to watch action movies?\nAnd you say yes,\nI would love to watch them that again.\nThe chances are\nthat you would like to watch Edge of Tomorrow.\nSo from your friend\nwhen you can come to one conclusion now\nhere since the ratio\nof liking the movie to don't like is actually 2 is\nto 1 so the final result is Actually,\nyou would like Edge of Tomorrow.\nNow you go to your second friend and you ask the same question.\nSo now you are second friend asks you did you like far\nand away when we went out and did the last time\nwhen we washed it\nand you say no I really didn't like far and away\nthen you would say then you are definitely going\nto like Edge of Tomorrow.\nWhy does so because far and away is actually\nsince most of whom might not be knowing it so far\nin a ways Johner of romance\nand it revolves around a girl\nand a guy By falling in love with each other and so on.\nSo the probability is\nthat you wouldn't like edge of tomorrow.\nSo he ask you another question.\nDid you like Bolivian\nand to really like to watch Tom Cruise?\nAnd you say Yes, again.\nThe probability is\nthat you would like to watch Edge of Tomorrow.\nWhy because Oblivion again is a science fiction\ncasting Tom Cruise full of strange experiences.\nAnd where Tom Cruise is the savior of the masses.\nKind well,\nthat is the same kind of plot in edge of tomorrow as well.\nSo here it is pure yes\nthat you would like to watch edge of tomorrow.\nSo you get\nanother second decision from your second friend.\nNow you go to your third friend and ask him so\nprobably our third friend is not really interesting\nin having any sort of conversation with you say,\nit just simply asks you did you like Godzilla and you said\nno I didn't like Godzilla's\nwe said definitely you wouldn't like\nit's of tomorrow why so\nbecause Godzilla is also actually sign Fiction movie\nfrom the adventure Jonah.\nSo now you have got three results from\nthree different decision trees from three different friends.\nNow you compile the results of all those friends\nand then you make a final call that yes,\nwould you like to watch edge of tomorrow or not?\nSo this is some very real time and very interesting example\nwhere you can actually Implement random Forest\ninto ground reality right any questions so far.\nSo far, no,\nthat's good, and then we can move forward.\nNow let us look at various domains\nwhere random Forest is actually used.\nSo because of its diversity random Forest is actually used\nin various diverse to means\nlike so beat banking beat\nmedicine beat land use beat marketing name it\nand random Forest is there so in banking particularly\nrandom Forest is being actually used to make it out\nwhether the applicant will be a default a pair\nor it Will be non default of 1\nso that it can accordingly\napprove or reject the applications of loan,\nright?\nSo that is how random Forest is being used in banking\ntalking about medicine.\nRandom.\nForest is widely used\nin medicine field to predict beforehand.\nWhat is the probability\nif a person will actually have a particular disease or not?\nRight?\nSo it's actually used to look at the various disease Trends.\nLet's say you want to figure out what is the probability\nthat a person will have diabetes?\nNot and so what would you do?\nIt'd probably look at the medical history\nof the patient and then you will see or read.\nThis has been the glucose concentration.\nWhat was the BMI?\nWhat was the insulin levels\nin the patient in the past previous three months.\nWhat is the age of this particular person\nand will make a different decision trees based on each one\nof these predictor variables\nand then you'll finally compiled the results\nof all those variables and then you'll make a fine.\nFinal decision as\nto whether the person will have diabetes\nin the near future or not.\nThat is how random Forest will be used\nin medicine sector now move.\nRandom Forest is also actually used to find out the land use.\nFor example, I want to set up a particular industry\nin certain area.\nSo what would I probably look for a look for?\nWhat is the vegetation over there?\nWhat is the Urban population over there?\nRight and how much is the Is from the nearest modes\nof Transport like from the bus station\nor the railway station and accordingly.\nI will split my parameters\nand I will make decision on each one of these parameters\nand finally I'll compile my decision of all\nthese parameters in that will be my final outcome.\nSo that is how I am finally going to predict\nwhether I should put my industry\nat this particular location or not.\nRight?\nSo these three examples have actually been of majorly\naround classification problem\nbecause we are trying to classify\nwhether or not we're actually trying to answer this question\nwhether or not right now,\nlet's move forward and look\nhow marketing is revolving around random Forest.\nSo particularly in marketing\nwe try to identify the customer churn.\nSo this is particularly the regression kind\nof problem right now\nhow let's see so customer churn\nis nothing but actually the number of people\nwhich are actually The number of customers\nwho are losing out.\nSo we're going out of your market.\nNow you want to identify\nwhat will be your customer churn in near future.\nSo you'll most of them\neCommerce Industries are actually using this\nlike Amazon Flipkart Etc.\nSo they particularly look at your each Behavior as to\nwhat has been your past history.\nWhat has been your purchasing history.\nWhat do you like based on your activity\naround certain things around certain ads around certain?\nDiscounts or around certain kind of materials right?\nIf you like a particular top your activity will be more\naround that particular top.\nSo that is how they track each and every particular move\nof yours and then they try to predict\nwhether you will be moving out or not.\nSo that is how they identify the customer churn.\nSo these all are various domains\nwhere random Forest is used and this is\nnot the only list so there are numerous other examples\nwhich are Chile are using\nrandom forests that makes it so special actually.\nNow, let's move forward and see how random\nForest actually works.\nRight.\nSo let us start with the random Forest algorithm first.\nLet's just see it step\nby step as to how random Forest algorithm works.\nSo the first step is to actually select\ncertain M features from T.\nWhere m is less than T.\nSo here T is the total number of the predictor variables\nthat you have\nin your data set and out of those total predictor variables.\nYou will select some randomly some Features out of those now\nwhy we are actually selecting a few features only.\nThe reason is\nthat if you will select all the predictive variables\nor the total predictor variables then each of your decision tree\nwill be same.\nSo the model is not actually learning something new.\nIt is learning the same previous thing\nbecause all those decision trees will be similar,\nright if you actually split your predicted variables\nand you select randomly a few predicted variables only.\nLet's say there are 14 total number of variables and out\nof those you randomly pick just three right?\nSo every time you will get a new decision tree,\nso there will be variety.\nRight?\nSo the classification model will be actually\nmuch more intelligent than the previous one.\nNow.\nIt has got barrier to experiences.\nSo definitely it will make different decisions each time.\nAnd then when you will compile all those different decisions,\nit will be a new more accurate.\nAn efficient result right?\nSo the first important step is to select certain number\nof features out of all the features now,\nlet's move on to the second step.\nLet's say for any node D. Now.\nThe first step is to calculate the best plate at that point.\nSo, you know that decision tree\nhow decision trees actually implemented so\nyou pick up a the most significant variable right?\nAnd then you will split that particular node\ninto Other child nodes\nthat is how the split takes place, right?\nSo you will do it for M number of variables\nthat you have selected.\nLet's say you have selected three\nso you will implement the split at all.\nThose three nodes in one particular decision tree,\nright the third step is split up the node\ninto two daughter nodes.\nSo now you can split your root note\ninto as many notes\nas you want to put hair will split our node\ninto 2.2 notes as to this\nor that so it will be an answer in terms of You saw that right?\nOur fourth step will be to repeat all these 3 steps\nthat we've done previously\nand we'll repeat all this splitting\nuntil we have reached all the N number of nodes.\nRight?\nSo we need to repeat\nuntil we have reached till the leaf nodes\nof a decision tree.\nThat is how we will do it right now after these four steps.\nWe will have our one decision tree.\nBut random Forest is actually about multiple.\nAsian trees.\nSo here our fifth step will come into the picture\nwhich will actually repeat all these previous steps\nfor D number of times now hit these the D number\nof decision trees.\nLet's say I want to implement five decision trees.\nSo my first step\nwill be to implement all the previous steps 5 times.\nSo the head the eye tration is 4/5 number of times right now.\nOnce I have created\nthese five decision trees still my task is not complete yet.\nOn my final task will be to compile the results\nof all these five different decision trees\nand I will make a call\nin the majority voting right here.\nAs you can see in this picture.\nI had in different instances.\nThen I created n different decision trees.\nAnd finally I will compile the result of all these n\ndifferent decision trees\nand I will take my call on the majority voting right.\nSo whatever my majority vote says\nthat will be My final result.\nSo this is basically an overview of the random Forest algorithm\nhow it actually works.\nLet's just have a look at this example to get\nmuch better understanding of what we have learnt.\nSo let's say I have this data set\nwhich consists of four different instances, right?\nSo basically it consists of the weather information\nof previous 14 days right from D1 tildy 14,\nand this basically Outlook humidity and wind\nis Click gives me the better condition\nof those 14 days.\nAnd finally I have play\nwhich is my target variable weather match did take place\non that particular day or not right.\nNow.\nMy main goal is to find out\nwhether the match will actually take place\nif I have following these weather conditions\nwith me on any particular day.\nLet's say the Outlook is rainy that day\nand humidity is high and the wind is very weak.\nSo now I need to predict\nwhether I will be able to play in the match.\nThat they are not.\nAll right.\nSo this is a problem statement fine.\nNow, let's see how random Forest is used in this to sort it out.\nNow here the first step is to actually split\nmy entire data set into subsets here.\nI have split my entire 14 variables into further\nsmaller subsets right now these subsets may\nor may not overlap\nlike there is certain overlapping between d 1 till D3\nand D3 till D6 fine.\nIs an overlapping of D3\nso it might happen that there might be overlapping\nso you need not really worry about the overlapping\nbut you have to make sure\nthat all those subsets are actually different right?\nSo here I have taken three different subsets\nmy first subset consists of D1\ntill D3 Mexican subset consists of D3\ntill D6 and methods subset consists of D7 tildy.\nNow now I will first be focusing on my first upset now here,\nlet's say that particular day\nthe Outlook was Overcast fine if yes,\nit was overcast then the probabilities\nthat the match will take place.\nSo overcast is basically when your weather is too cloudy.\nSo if that is the condition then definitely the match\nwill take place and let's say it wasn't overcast.\nThen you will consider these second most probable option\nthat will be the wind\nand you will make a decision based on this now\nwhether wind was weak or strong if wind was weak,\nthen you will definitely go out and play them.\nJudge as you would not so now the final outcome\nout of this decision tree will be Play\nBecause here the ratio between the play\nand no play is to is to 1\nso we get to a certain decision from a first decision tree.\nNow, let us look at the second subset now\nsince second subset has different number of variables.\nSo that is why this decision trees absolutely different from\nwhat we saw in our four subsets.\nSo let's say if it was overcast then you will play the match\nif It isn't the overcast in you would go\nand look out for humidity.\nNow further.\nIt will get split into two whether it was high or normal.\nNow, we'll take the first case\nif the humidity was high and when it was week,\nthen you will play the match else\nif humidity was high but wind was too strong,\nthen you would not go out and play the match right now.\nLet us look at the second dot to node of humidity\nif the humidity was normal.\nThe wind was weak.\nThen you will definitely go out and play the match\nas you want go out and play the match.\nSo here if you look at the final result,\nthen the ratio of placed no play is 3 is to 2 then again.\nThe final outcome is actually play, right?\nSo from second subset,\nwe get the final decision of play now,\nlet us look at our third subset\nwhich consists of D7 till D9 here\nif again the overcast is yes, then you will play a match.\nEach else you will go and check out for humidity.\nAnd if the humidity is really high then you\nwon't play the match else.\nYou will play the match again the probability\nof playing the matches.\nYes, because the ratio of no play is Twist one, right?\nSo three different subsets three different decision trees\nthree different outcomes\nand one final outcome after compiling all the results\nfrom these three different decision trees are so I This\ngives a better perspective better understanding\nof random Forest like how it really works.\nAll right.\nSo now let's just have a look\nat various features of random Forest Ray.\nSo the first and the foremost feature is\nthat it is one\nof the most accurate learning algorithms, right?\nSo why it is so\nbecause single decision trees are actually prone\nto having high variance\nor Hive bias and on the contrary actually.\nM4s, it averages the entire variance\nacross the decision trees.\nSo let's say\nif the variances say X4 decision tree,\nbut for random Forest,\nlet's say we have implemented n number\nof decision trees parallely.\nSo my entire variance gets averaged to upon\nand my final variance actually becomes X upon n so\nthat is how the entire variance actually goes down\nas compared to other algorithms.\nNow second most important feature is\nthat it works well for both classification\nand regression problems\nand by far I have come across this is one\nand the only algorithm\nwhich works equally well for both of them\nthese classification kind of problem or a regression kind\nof problem, right?\nThen it's really runs efficient on large databases.\nSo basically it's really scalable.\nEven if you work for the lesser amount of database\nor if you work for a really huge volume of data, right?\nSo that's a very good part about it.\nThen the fourth most important point is\nthat it requires almost no input preparation.\nNow, why am I saying this is\nbecause it has got certain implicit methods,\nwhich actually take care and All the outliers\nand all the missing data\nand you really don't have to take care about all that thing\nwhile you are in the stages of input preparations.\nSo Random Forest is all here to take care\nof everything else and next.\nIs it performs implicit feature selection, right?\nSo while we are implementing multiple decision trees,\nso it has got implicit method\nwhich will automatically pick up some random features out.\nOf all your parameters and then it will go\non and implementing different decision trees.\nSo for example,\nif you just give one simple command\nthat all right,\nI want to implement 500 decision trees no matter\nhow so Random Forest will automatically take care\nand it will Implement all those 500 decision trees\nand those all 500 decision trees will be different\nfrom each other and this is\nbecause it has got implicit methods\nwhich will automatically collect different parameters.\nOut of all the variables that you have right?\nThen it can be easily grown in parallel why it is so\nbecause we are actually\nimplementing multiple decision trees and all\nthose decision trees are running\nor all those decisions trees are actually\ngetting implemented parallely.\nSo if you say I want thousand trees to be implemented.\nSo all those thousand trees are getting implemented parallely.\nSo that is how the computation time reduces down.\nRight, and the last point is\nthat it has got methods for balancing error\nin unbalanced it\nas it's now what exactly unbalanced data sets\nare let me just give you an example of that.\nSo let's say you're working on a data set fine\nand you create a random forest model and get\n90% accuracy immediately.\nFantastic you think right.\nSo now you start diving deep you go a little deeper.\nAnd you discovered\nthat 90% of that data actually belongs to just one class\ndamn your entire data set.\nYour entire decision is actually biased\nto just one particular class.\nSo Random Forest actually takes care of this thing\nand it is really not biased\ntowards any particular decision tree or any particular variable\nor any class.\nSo it has got methods which looks after it\nand they does is all the balance of errors in your data sets.\nSo that's pretty much\nabout the features of random forests.\nWhat is KNN algorithm\nwill K. Nearest neighbor is a simple algorithm\nthat stores all the available cases\nand classify the new data\nor case based on a similarity measure.\nIt suggests that\nif you are similar to your neighbors,\nthen you are one of them, right?\nFor example,\nif apple looks more similar to banana orange or Melon.\nRather than a monkey rat\nor a cat then most likely Apple belong to the group of fruits.\nAll right.\nWell in general Cayenne is used in Search application\nwhere you are looking for similar items\nthat is when your task is some form of fine items\nsimilar to this one.\nThen you call this search as a Cayenne search.\nBut what is this KN KN?\nWell this K denotes the number of nearest neighbor\nwhich are voting class of the new data\nor the testing data.\nFor example,\nif k equal 1 then the testing data are given the same label\nas a close this Ample in the training set similarly\nif k equal to 3 the labels\nof the three closes classes are checked and the most\ncommon label is assigned to then testing data.\nSo this is\nwhat a KN KN algorithm means so moving on ahead.\nLet's see some of the example of scenarios\nwhere KN is used in the industry.\nSo, let's see the industrial application\nof KNN algorithm starting with recommender system.\nWell the biggest use case\nof cayenne and search is a recommender system.\nThis recommended system is like an automated form\nof a shop counter guy when you asked him for a product.\nNot only shows you the product\nbut also suggest you or displays your relevant set of products,\nwhich are related to the item.\nYou're already interested in buying this KNN algorithm\napplies to recommending products like an Amazon\nor for recommending media,\nlike in case of Netflix or even for recommending advertisement\nto display to a user\nif I'm not wrong almost all of you must have used Amazon\nfor shopping, right?\nSo just to tell you more than 35% of amazon.com revenue\nis generated by its recommendation engine.\nSo what's their strategy Amazon uses?\nRecommendation as a targeted marketing tool\nin both the email campaigns\naround most of its website\nPages Amazon will recommend many products\nfrom different categories based on what you have browser\nand it will pull those products in front of you\nwhich you are likely to buy\nlike the frequently bought together option\nthat comes at the bottom of the product page to tempt you\ninto buying the combo.\nWell, this recommendation has just one main goal\nthat is increase average order value or to upsell\nand cross-sell customers by providing product suggestion\nbased on items in the shopping cart,\nor On the product they are currently looking at on site.\nSo next industrial application of KNN\nalgorithm is concept search\nor searching semantically similar documents\nand classifying documents containing similar topics.\nSo as you know,\nthe data on the Internet is increasing exponentially\nevery single second.\nThere are billions and billions of documents on the internet\neach document on the internet contains multiple Concepts,\nthat could be a potential concept.\nNow, this is a situation\nwhere the main problem is to extract concept\nfrom a set of documents\nas each page could have thousands of combination\nthat could be potential Concepts an average document could have\nmillions of concept combined\nthat the vast amount of data on the web.\nWell, we are talking about an enormous amount\nof data set and Sample.\nSo what we need is we need to find the concept\nfrom the enormous amount of data set and samples, right?\nSo for this purpose,\nwe'll be using KNN algorithm more advanced example\ncould include handwriting detection like an OCR\nor image recognization or even video recognization.\nAll right.\nSo now that you know various use cases\nof KNN algorithm,\nlet's proceed and see how does it work.\nSo how does a KNN algorithm work?\nLet's start by plotting these blue and orange\npoint on our graph.\nSo these Blue Points the belong to class A\nand the orange ones they belong to class B.\nNow you get a star as a new pony and your task is to predict\nwhether this new point it belongs to class A\nor it belongs to the class B.\nSo to start the production, the very first thing\nthat you have to do is select the value of K,\njust as I told you KN KN algorithm refers to the number\nof nearest neighbors that you want to select for example,\nin this case k equal to 3.\nSo what does it mean it means\nthat I am selecting three points\nwhich are the least distance to the new point\nor you can say I am selecting three different points\nwhich are closest to the star.\nWell at this point of time you can ask\nhow will you calculate the least distance?\nSo once you calculate the distance,\nyou will get one blue and two orange points\nwhich are closest to this star now since in this case\nas we have a majority of Inch point so you can see\nthat for k equal 3D star belongs to the class B,\nor you can say\nthat the star is more similar to the orange points\nmoving on ahead.\nWell, what if k equal to 6 well for this case,\nyou have to look for six different points\nwhich are closest to this star.\nSo in this case after calculating the distance,\nwe find that we have four blue points\nand two Orange Point\nwhich are closest to the star now,\nas you can see\nthat the blue points are in majority so you can say\nthat for k equals 6 this star belongs.\nThese two class A or the star is more similar to Blue Points.\nSo by now,\nI guess you know how a KNN algorithm work.\nAnd what is the significance of gain KNN algorithm.\nSo how will you choose the value of K?\nSo keeping in mind this case the most important parameter\nin KNN algorithm.\nSo, let's see when you build a k nearest neighbor classifier.\nHow will you choose a value of K?\nWell, you might have a specific value of K in mind\nor you could divide up your data and use something\nlike cross-validation technique to test several values of K\nin order to determine\nwhich works best for your data.\nExample if n equal 2,000 cases then\nin that case the optimal value of K lies somewhere\nin between 1 to 19.\nBut yes, unless you try it you cannot be sure of it.\nSo, you know how the algorithm is working on a higher level.\nLet's move on and see\nhow things are predicted using KNN algorithm.\nRemember I told you\nthe KNN algorithm uses the least distance measure\nin order to find its nearest neighbors.\nSo let's see how these distances calculated.\nWell, there are several distance measure\nwhich can be used.\nSo to start with Will mainly focus on euclidean distance\nin Manhattan distance in this session.\nSo what is this euclidean distance?\nWell, this euclidean distance is defined as the square root\nof the sum of difference between a new point x\nand an existing Point why\nso for example here we have Point P1 and P2 Point\nP. 1 is 1 1 and point B 2 is 5\nfor so what is the euclidean distance between both of them?\nSo you can say that euclidean distance is\na direct distance between two points.\nSo what is the distance between the point P1 and P2?\nSo we Calculate it as 5 minus 1 whole square\nplus 4 minus 1 whole square\nand we can route it over which results to 5.\nSo next is the Manhattan distance.\nWell, this Manhattan distance is used to calculate the distance\nbetween real Vector using\nthe sum of their absolute difference in this case.\nThe Manhattan distance between the point P1\nand P2 is mod of 5 minus 1 plus mod value of 4 minus 1,\nwhich results to 3 plus 4.\nThat is 7.\nSo this slide shows the difference between euclidean\nand Manhattan distance from point A to point B.\nSo euclidean distance is nothing but the direct\nor the least possible distance between A and B.\nWhereas the Manhattan distance is a distance between A\nand B measured along the axis at right angle.\nLet's take an example and see\nhow things are predicted using KNN algorithm\nor how the cannon algorithm is working suppose.\nWe have data set which consists of height weight\nand T-shirt size of some customers.\nNow when a new customer come we only have is height.\nAnd wait as the information now our task is to predict.\nWhat is the T-shirt size of that particular customer?\nSo for this will be using the KNN algorithm.\nSo the very first thing what we need to do,\nwe need to calculate the euclidean distance.\nSo now that you have a new data of height 160 one centimeter\nand weight as 61 kg.\nSo the very first thing\nthat we'll do is we'll calculate the euclidean distance,\nwhich is nothing but the square root\nof 160 1 minus 158 whole square\nplus 61 minus 58 whole square and square root of that is 4.24.\nLet's drag and drop it.\nSo these are the various euclidean distance\nof other points.\nNow, let's suppose k equal to 5 then the algorithm\nwhat it does is it searches for the five customer\nclosest to the new customer\nthat is most similar to the new data in terms\nof its attribute for k equal 5.\nLet's find the top five minimum euclidian distance.\nSo these are the distance\nwhich we are going to use one two,\nthree, four and five.\nSo let's rank them in the order first.\nThis is second.\nThis is third then this one is Forward and again,\nthis one is five.\nSo there's our order.\nSo for k equal 5 we have for t-shirts\nwhich come under size M and one t-shirt\nwhich comes under size l\nso obviously best guess for the best prediction\nfor the T-shirt size of white 161 centimeters and wait\n60 1 kg is M.\nOr you can say that a new customer fit\ninto size M. Well this was all about the theoretical session.\nBut before we drill down to the coding part,\nlet me just tell you why people call KN as a lazy learner.\nWell KN for classification.\nOcean is a very simple algorithm.\nBut that's not why they are called lazy KN is a lazy learner\nbecause it doesn't have a discriminative function\nfrom the training data.\nBut what it does it memorizes the training data,\nthere is no learning phase of the model and all\nof the work happens at the time.\nYour prediction is requested.\nSo as such there's the reason why KN is often referred\nto us lazy learning algorithm.\nSo this was all about the theoretical session now,\nlet's move on to the coding part.\nSo for the Practical\nimplementation of the Hands-On part,\nI'll be using the artists data set\nso This data set consists of 150 observation.\nWe have four features\nand one class label the four features include\nthe sepal length sepal width petal length and the petrol head\nwhereas the class label decides which flower belongs\nto which category.\nSo this was the description of the data set,\nwhich we are using now,\nlet's move on and see what are the step\nby step solution to perform a KNN algorithm.\nSo first, we'll start by handling the data\nwhat we have to do we have to open the data set\nfrom the CSV format\nand split the data set into train and test part next.\nWe'll take the Clarity where we have to calculate the distance\nbetween two data instances.\nOnce we calculate the distance next we'll look for the neighbor\nand select K Neighbors\nwhich are having the least distance from a new point.\nNow once we get our neighbor,\nthen we'll generate a response from a set of data instances.\nSo this will decide\nwhether the new Point belongs to class A or Class B.\nFinally will create the accuracy function\nand in the end.\nWe'll tie it all together in the main function.\nSo let's start with our code\nfor implementing KNN algorithm using python.\nI'll be using Java.\nOld book by Don 3.0 installed on it.\nNow.\nLet's move on and see\nhow can an algorithm can be implemented using python.\nSo there's my jupyter notebook,\nwhich is a web-based interactive Computing notebook environment\nwith python 3.0 installed on it.\nSo the launch its launching so there's our jupyter notebook\nand we'll be riding our python codes on it.\nSo the first thing\nthat we need to do is load our file our data\nis in CSV format without a header line\nor any code we can open the file the open function\nand read the data line using the reader function.\nIn the CSV module.\nSo let's write a code to load our data file.\nLet's execute the Run button.\nSo once you execute the Run button,\nyou can see the entire training data set as the output next.\nWe need to split the data into a training data set\nthat KN can use to make prediction and a test data set\nthat we can use to evaluate the accuracy of the model.\nSo we first need to convert the flower measure\nthat will load it as string into numbers\nthat we can work next.\nWe need to split the data set randomly to train and test.\nRatio 67's 233 for test is to train as a standard ratio,\nwhich is used for this purpose.\nSo let's define a function\nas load data set\nthat loads a CSV with the provided file\nnamed and split it randomly into training\nand test data set using the provided split ratio.\nSo this is our function load data set which is using filename\nsplit ratio training data set\nand testing data set as its input.\nAll right.\nSo let's execute the Run button and check for any errors.\nSo it's executed with zero errors.\nLet's test this function.\nSo there's our training set testing set load data set.\nSo this is our function load data set on inside\nthat we are passing.\nOur file is data with a split ratio of 0.66\nand training data set and test data set.\nLet's see what our training data set and test data set.\nIt's dividing into so it's giving a count\nof training data set and testing data set.\nThe total number of training data set\nas split into is 97 and total number\nof test data set we have is 53.\nSo total number of training data set we have here is 97 and total\nnumber of test data set we have here is 53.\nAll right.\nOkay, so Function load data set is performing.\nWell, so let's move on to step two\nwhich is similarity.\nSo in order to make prediction,\nwe need to calculate the similarity between\nany two given data instances.\nThis is needed\nso that we can locate the kamo similar data instances\nin the training data set are in turn make a prediction given\nthat all for flower measurement are numeric and have same unit.\nWe can directly use the euclidean distance measure.\nThis is nothing but the square root of the sum\nof squared differences between two areas\nof the number given\nthat all the for flower Are numeric and have\nsame unit we can directly use the euclidean distance measure\nwhich is nothing but the square root of the sum\nof squared difference between two areas\nor the number additionally we want to control\nwhich field to include in the distance calculation.\nSo specifically we only want to include first for attribute.\nSo our approach will be to limit the euclidean distance\nto a fixed length.\nAll right.\nSo let's define our euclidean function.\nSo this are euclidean distance function\nwhich takes instance one instance to and length as\nparameters instance 1 and ends.\nThese two are the two points\nof which you want to calculate the euclidean distance,\nwhereas this length and denote\nthat how many attributes you want to include?\nOkay.\nSo there's our euclidean function.\nLet's execute it.\nIt's executing fine without any errors.\nLet's test the function suppose the data one\nor the first instance consists of the data point has two to two\nand it belongs to class A\nand data to consist of four for four\nand it belongs to class P.\nSo when we calculate the euclidean distance\nof data one to data to and\nwhat we have to do we have to consider only\nfirst three features of them.\nAll right.\nSo let's print the distance as you can see here.\nThe distance comes out to be three point\nfour six four now like so this is nothing\nbut the square root of 4 minus 2 whole Square.\nSo this distance is nothing but the euclidean distance\nand it is calculated as square root of 4 minus 2 whole square\nplus 4 minus 2 whole square\nthat is nothing but 3 times of 4 minus 2 whole square\nthat is 12 + square root\nof 12 is nothing but 3.46 for all right.\nSo now that we have calculated the distance now we need to look\nfor K nearest.\nNeighbors now that we have a similarity measure\nwe can use it to collect the kamo similar instances\nfor a given unseen instance.\nWell, this is a straightforward process\nof calculating the distance for all the instances\nand selecting a subset with the smallest distance value.\nAnd now what we have to do we have to select\nthe smallest distance values.\nSo for that will be defining a function\nas get neighbors.\nSo for that\nwhat we will be doing will be defining a function\nas get neighbors\nwhat it will do it will return the K most similar Neighbors\nFrom the training set for a given test instance.\nAll right, so this is how our get neighbors In look\nlike it takes training data set\nand test instance and K as its input here.\nThe K is nothing but the number\nof nearest neighbor you want to check for.\nAll right.\nSo basically what you'll be getting\nfrom this get Mabel's function is K different points\nhaving least euclidean distance from the test instance.\nAll right, let's execute it.\nSo the function executed without any errors.\nSo let's test our function.\nSo suppose the training data set includes the data like to to\nto and it belongs to class A\nand other data includes four four four and it belongs\nto class P and at testing and Census 555 or now,\nwe have to predict\nwhether this test instance belongs to class A\nor it belongs to class be.\nAll right for k equal 1 we have to predict\nits nearest neighbor and predict\nwhether this test instance it will belong to class A\nor will it belong to class be?\nAlright.\nSo let's execute the Run button.\nAll right.\nSo an executing the Run button you can see\nthat we have output as for for for\nand be a new instance\n5 5 5 is closes 2.44 for which belongs to class be.\nAll right.\nNow once you have located the most similar neighbor\nfor a test instance next task is to predict a response based\non those neighbors.\nSo how we can do that.\nWell, we can do this\nby allowing each neighbor to vote for the class attribute\nand take the majority vote as a prediction.\nLet's see how we can do that.\nSo we are function\nas getresponse with takes neighbors as the input.\nWell, this neighbor was nothing but the output of this get me /\nfunction the output\nof get me were function will be fed to get response.\nAll right.\nLet's execute the Run button.\nIt's executed.\nLet's move ahead and test our function get response.\nSo we have a But as bun bun bun it belongs to class\nA 2 2 2 it belongs to class a33.\nIt belongs to class B.\nSo this response, that's what it will do.\nIt will store the value of get response by passing\nthis neighbor value.\nI like so what we want to check is we want to predict\nwhether that test instance final outcome will belongs\nto class A or Class B.\nWhen the neighbors are 1 1 1 a 2 2 A + 3 3 B.\nSo, let's check our response.\nNow that we have created all the different function\nwhich are required for a KNN algorithm.\nSo important main concern is\nhow do you evaluate the accuracy of the prediction\nand easy way to evaluate\nthe accuracy of the model is to calculate a ratio\nof the total correct prediction to all the protection made.\nSo for this I will be defining function\nas get accuracy and inside\nthat I'll be passing my test data set\nand the predictions get accuracy function check\nget executed without any error.\nLet's check it for a sample data set.\nSo we have our test data set as 1 1 1 It belongs to class A 2/2\nwhich again belongs to class 3 3 3 which belongs to class B\nand my predictions is for first test data.\nIt predicted latter belongs to class A which is true\nfor next it predicted that belongs to class C,\nwhich is again to and for the next again and predictive\nthat it belongs to class A which is false in this case\ncause the test data belongs to class be.\nAll right.\nSo in total we have to correct prediction out of three.\nAll right, so the ratio will be 2 by 3,\nwhich is nothing but 66.6.\nSo our accuracy rate is 66.6.\nIt's so now that you have created all the function\nthat are required for KNN algorithm.\nLet's compile them into one single main function.\nAlright, so this is our main function\nand we are using Iris data set\nwith a split of 0.67 and the value of K is 3 Let's see.\nWhat is the accuracy score of this check\nhow accurate are modulus so in training data set,\nwe have a hundred and thirteen values\nand then the test data set.\nWe have 37 values.\nThese are the predicted\nand the actual values of the output.\nOkay.\nSo in total we got an accuracy of 90s.\nIn point two nine percent, which is really very good.\nAlright, so I hope the concept of this KNN algorithm is\nhere device in a world full of machine learning\nand artificial intelligence surrounding almost everything\naround us classification\nand prediction is one\nof the most important aspects of machine learning.\nSo before moving forward,\nlet's have a quick look at the agenda.\nI'll start off this video by explaining you guys\nwhat exactly is Nave biased\nthen we'll and what is Bayes theorem\nwhich serves as a logic\nbehind the name pass algorithm going forward.\nI'll explain the steps involved\nin the neighbors algorithm one by one\nand finally add finish of this video with a demo\non the Nave bass using the SQL own package noun\na bass is a simple but surprisingly powerful algorithm\nfrom penetrative analysis.\nIt is a classification technique based on base theorem\nwith an assumption of Independence among predictors.\nIt comprises of two parts, which is name.\nAnd bias in simple terms neighbors classifier assumes\nthat the presence of a particular feature\nin a class is unrelated to the presence\nof any other feature,\neven if this features depend on each other\nor upon the existence of the other features,\nall of these properties independently contribute\nto the probability\nwhether a fruit is an apple or an orange or a banana.\nSo that is why it\nis known as naive now naive based model is easy to build\nand particularly useful for very large data sets.\nIn probability Theory and statistics based theorem,\nwhich is already known as the base law\nor the base rule describes the probability of an event\nbased on prior knowledge of the conditions\nthat might be related to the event now paste\ntheorem is a way to figure out conditional probability.\nThe conditional probability is the probability\nof an event happening given\nthat it has some relationship to one or more other events.\nFor example, your probability of getting a parking space\nis connected to the time of the day you pass.\nWhere you park\nand what conventions are you going on at that time\nbased Serum is slightly more nuanced in a nutshell.\nIt gives you an actual probability of an event given\ninformation about the tests.\nNow, if you look at the definition\nof Bayes theorem,\nwe can see that given a hypothesis H\nand the evidence e-base term states that\nthe relationship between the probability of the hypothesis\nbefore getting the evidence\nwhich is the P of H and the probability\nof the hypothesis after getting the evidence\nthat P of H given e is defined as probability\nof e given H into probability\nof H divided by probability of e it's rather confusing, right?\nSo let's take an example to understand this theorem.\nSo suppose I have a deck of cards and\nif a single card is drawn from the deck of playing cards,\nthe probability that the card is a king is for by 52\nsince there are four Kings in a standard deck of 52 cards.\nNow if King is an event, this card is a king.\nThe probability of King is given as 4 by 52\nthat is equal to 1 by 13.\nNow if the evidence is provided for instance someone looks\nas the That the single card is a face card the probability\nof King given\nthat it's a face can be calculated\nusing the base theorem by this formula.\nThe since every King is also a face card\nthe probability of face given\nthat it's a king is equal to 1\nand since there are three face cards in each suit.\nThat is the chat king and queen.\nThe probability of the face card is equal to 12 by 52.\nThat is 3 by 30.\nNow using Bayes theorem we can find out the probability\nof King given that it's a face\nso our final answer comes to 1 by 3,\nwhich is also true.\nSo if you have a deck of cards\nwhich has having only faces now there are three types of phases\nwhich are the chat king and queen so the probability\nthat it's the king is 1 by 3.\nNow.\nThis is the simple example of how based on works now\nif we look at the proof as in how this Bayes theorem Evolved.\nSo here we have probability of a given p\nand probability of B given a now for\na joint probability distribution over the sets A and B,\nthe probability of a intersection B,\nthe conditional probability of a given B is defined\nas the probability\nof a intersection B divided by probability of B,\nand similarly probability of B,\ngiven a is defined as probability of B intersection\na divided by probability of a now we can\nEquate probability of a intersection p and probability\nof B intersection a as both are the same thing now\nfrom this method\nas you can see,\nwe get our final base theorem proof,\nwhich is the probability of a given b equals probability of B,\ngiven a into probability\nof P divided by the probability of a now\nwhile this is the equation\nthat applies to any probability distribution\nover the events A and B.\nIt has a particular nice interpretation in case\nwhere a is represented as the hypothesis h\nand B is represented\nas some observed evidence e in that case the formula is p\nof H given e is equal\nto P of e given H\ninto probability of H divided by probability of e now\nthis relates the probability\nof hypotheses before getting the evidence,\nwhich is p of H to the probability\nof the hypothesis after getting the evidence\nwhich is p of H given e\nfor this reason P of H is known as the prior probability\nwhile P of Each given e is known as the posterior probability\nand the factor\nthat relates the two is known as the likelihood ratio Now using\nthis term space theorem can be rephrased\nas the posterior probability equals.\nThe prior probability times the likelihood ratio.\nSo now that we know the maths\nwhich is involved behind the baster.\nMm.\nLet's see how we can implement this in real life scenario.\nSo suppose we have a data set.\nIn which we have the Outlook the humidity\nand we need to find out\nwhether we should play or not on that day.\nSo the Outlook can be sunny overcast rain\nand the humidity high normal\nand the wind are categorized into two phases\nwhich are the weak and the strong winds.\nThe first of all will create a frequency table using\neach attribute of the data set.\nSo the frequency table for the Outlook looks\nlike this we have Sunny overcast and rainy the frequency table\nof humidity looks like this\nand Frequency table of when looks like this we have strong\nand weak for wind and high and normal ranges for humidity.\nSo for each frequency table,\nwe will generate a likelihood table now now\nthe likelihood table contains the probability\nof a particular day suppose we take the sunny\nand we take the play as yes\nand no so the probability of Sunny given\nthat we play yes is 3 by 10,\nwhich is 0.3 the probability of X,\nwhich is the probability of Sunny\nIs equal to 5 by 14 now,\nthese are all the terms\nwhich are just generated from the data\nwhich we have a\nand finally the probability of yes is 10 out of 14.\nSo if we have a look at the likelihood of yes given\nthat it's a sunny we can see using Bayes theorem.\nIt's the probability of Sunny given yes\ninto probability of s divided by the probability of Sunny.\nSo we have all the values here calculated.\nSo if you put that in our base serum equation,\nwe get the likelihood of yes.\nA 0.59 similarly the likelihood\nof no can also be calculated here is 0.40 now similarly.\nWe are going to create the likelihood table\nfor both the humidity\nand the win there's a\nfor humidity the likelihood for yes given the humidity\nis high is equal to 0.4 to and the probability\nof playing know given the vent is high is 0.58.\nThe similarly for table wind the probability of he has given\nthat the wind is week is 0.75 and the probability of no given\nthat the win is week is 0.25 now suppose we have of day\nwhich has high rain\nwhich has high humidity and the wind is weak.\nSo should we play or not?\nThat's our for that?\nWe use the base theorem here again the likelihood\nof yes on that day is equal\nto the probability of Outlook rain given\nthat it's a yes into probability of Magic given that say yes,\nand the probability of when that is we given\nthat it's we are playing yes into the probability of yes,\nwhich equals to zero point zero one nine\nand similarly the likelihood of know on that day is equal\nto zero point zero one six.\nNow if we look\nat the probability of yes for that day\nof playing we just need to divide it\nwith the likelihood some of both the yes\nand no so the probability of playing tomorrow,\nwhich is yes is 5\nwhereas the probability of not playing is equal to 0.45.\nNow.\nThis is based upon the data which we already have with us.\nSo now that you have an idea of what exactly is named bias\nhow it works and we have seen\nhow it can be implemented on a particular data set.\nLet's see where it is used in the industry.\nThe started with our first industrial use case,\nwhich is news categorization or we can use\nthe term text classification to broaden the spectrum\nof this algorithm news\nin the web are rapidly growing in the era of Information Age\nwhere each new site has its own different layout\nand categorization for grouping news.\nNow these heterogeneity\nof layout and categorization cannot always satisfy\nindividual users need to remove these heterogeneity\nand classifying the news articles.\nOwing to the user preference is a formidable task companies\nuse web crawler to extract useful text\nfrom HTML Pages the news articles\nand each of these news articles\nis then tokenized now these tokens are nothing\nbut the categories of the news now\nin order to achieve better classification result.\nWe remove the less significant Words,\nwhich are the stop was from the documents\nor the Articles\nand then we apply the Nave base classifier\nfor classifying the news contents based on the news.\nNow this is by far one\nof the best examples of Neighbors classifier,\nwhich is Spam filtering.\nNow.\nIt's the Nave Bayes classifier are\na popular statistical technique for email filtering.\nThey typically use bag of words features to identify\nat the spam email\nand approach commonly used in text classification as well.\nNow it works by correlating the use of tokens,\nbut the spam and non-spam emails and then the Bayes theorem,\nwhich I explained\nearlier is used to calculate the probability\nthat an email is\nor not a Spam so named by a Spam filtering is\na baseline technique for dealing with Spam\nthat container itself\nto the emails need of an individual user\nand give low false positive spam detection rates\nthat are generally acceptable to users.\nIt is one of the oldest ways of doing spam filtering\nwith its roots\nin the 1990s particular words have particular probabilities\nof occurring in spam.\nAnd and legitimate email as well for instance.\nMost emails users\nwill frequently encounter the world lottery\nor the lucky draw a spam email,\nbut we'll sell them see it in other emails.\nThe filter doesn't know these probabilities in advance\nand must be friends.\nSo it can build them up to train the filter.\nThe user must manually indicate\nwhether a new email is Spam or not for all the words\nin each straining email.\nThe filter will adjust the probability\nthat each word will appear in a Spam or legitimate.\nOwl in the database now\nafter training the word probabilities also known\nas the likelihood functions are used to compute the probability\nthat an email with a particular set of words as in in belongs\nto either category each word\nin the email contributes the email spam probability.\nThis contribution is called the posterior probability\nand is computed again using the base 0\nthen the email spam probability\nis computed over all the verse in the email\nand if the total exceeds a certain threshold say\nOr 95% the filter will Mark the email as spam.\nNow object detection is the process of finding instances\nof real-world objects such as faces bicycles\nand buildings in images\nor video now object detection\nalgorithm typically use extracted features\nand learning algorithm\nto recognize instance of an object category here again,\na bass plays an important role of categorization\nand classification of object now medical area.\nThis is increasingly voluminous amount of electronic data,\nwhich are becoming more and more complicated.\nThe produced medical data has certain characteristics\nthat make the analysis very challenging and attractive\nas well among all the different approaches.\nThe knave bias is used.\nIt is the most effective and efficient classification\nalgorithm and has been successfully applied\nto many medical problems empirical comparison\nof knave bias versus five popular classifiers\non Medical data sets shows\nthat may bias is well suited for medical application and has\nhigh performance in most of the examine medical problems.\nNow in the past various testicle methods have been used\nfor modeling in the area of disease diagnosis.\nThese methods require prior assumptions and are\nless capable of dealing\nwith massive and complicated nonlinear and dependent data one\nof the main advantages of neighbor as approach\nwhich is appealing to Physicians is\nthat all the available information is used?\nTo explain the decision this explanation seems\nto be natural for medical diagnosis and prognosis.\nThat is it is very close to the way\nhow physician diagnosed patients now weather is one\nof the most influential factor in our daily life to an extent\nthat it may affect the economy of a country\nthat depends on occupation like agriculture.\nTherefore as a countermeasure to reduce the damage\ncaused by uncertainty in whether Behavior,\nthere should be an efficient way to print the weather now\nwhether projecting has Challenging problem\nin the meteorological department\nsince ears even after the technology skill\nand scientific advancement the accuracy\nand protection of weather has never been sufficient even\nin current day this domain remains as a research topic\nin which scientists\nand mathematicians are working to produce a model\nor an algorithm\nthat will accurately predict the weather now\na bias in approach based model is created by\nwhere posterior probabilities are used to calculate\nthe likelihood of each class label for input.\nData instance and the one with the maximum likelihood\nis considered as the resulting output now earlier.\nWe saw a small implementation of this algorithm as well\nwhere we predicted\nwhether we should play or not based on the data,\nwhich we have collected earlier.\nNow, this is a python Library\nwhich is known as scikit-learn it helps to build in a bias\nand model in Python.\nNow, there are three types of named by ass model\nunder scikit-learn Library.\nThe first one is the caution.\nIt is used in classification and it Assumes\nthat the feature follow a normal distribution.\nThe next we have is multinomial.\nIt is used for discrete counts.\nFor example, let's say we have a text classification problem\nand here we consider bernouli trials,\nwhich is one step further\nand instead of word occurring in the document.\nWe have count\nhow often word occurs\nin the document you can think of it\nas a number of times outcomes number is observed\nin the given number of Trials.\nAnd finally we have the bernouli type.\nOf neighbors.\nThe binomial model is useful\nif your feature vectors are binary bag of words model\nwhere the once\nand the zeros are words occur in the document and the verse\nwhich do not occur\nin the document respectively based on their data set.\nYou can choose any of the given discussed model here,\nwhich is the gaussian the multinomial or the bernouli.\nSo let's understand how this algorithm works.\nAnd what are the different steps?\nOne can take to create a bison model and use knave bias\nto predict the output so here to understand better.\nWe are going to predict the onset of diabetes Now\nthis problem comprises\nof 768 observations of medical details\nfor Pima Indian patients.\nThe record describes instantaneous measurement taken\nfrom the patient such as the age the number\nof times pregnant\nand the blood work crew now all the patients are women aged 21\nand Older and all the attributes are numeric\nand the unit's vary from attribute to attribute.\nEach record has a class value that indicate\nwhether the patient suffered on onset of diabetes\nwithin five years are the measurements.\nNow.\nThese are classified as 0 now.\nI've broken the whole process down into the following steps.\nThe first step is handling the data\nin which we load the data from the CSV file and split it\ninto training and test it\nas it's the second step is summarizing the data.\nIn which we summarize\nthe properties in the training data sets so that we\ncan calculate the probabilities and make predictions.\nNow the third step comes is making a particular prediction.\nWe use the summaries\nof the data set to generate a single prediction.\nAnd after that we generate predictions given a test data\nset and a summarized training data sets.\nAnd finally we evaluate\nthe accuracy of the predictions made for a test data set\nas the percentage correct out of all the predictions made\nand finally We tied together and form.\nOur own model of nape is classifier.\nNow.\nThe first thing we need to do is load our data the data is\nin the CSV format without a header line\nor any codes.\nWe can open the file with the open function\nand read the data lines using the read functions\nin the CSV module.\nNow, we also need to convert the attributes\nthat were loaded as strings into numbers\nso that we can work with them.\nSo let me show you\nhow this can be implemented now for that you need to Tall python\non a system and use the jupyter notebook\nor the python shell.\nHey, I'm using the Anaconda Navigator\nwhich has all\nthe things required to do the programming in Python.\nWe have the Jupiter lab.\nWe have the notebook.\nWe have the QT console.\nEven we have a studio as well.\nSo what you need to do is just install the Anaconda Navigator\nit comes with the pre installed python also,\nso the moment you click launch on The jupyter Notebook.\nIt will take you to the Jupiter homepage\nin a local system and here you can do programming in Python.\nSo let me just rename it as by my India diabetes.\nSo first, we need to load the data set.\nSo I'm creating here a function load CSV now before that.\nWe need to import certain CSV the math\nand the random method.\nSo as you can see,\nI've created a load CSV function\nwhich will take the pie my Indian diabetes\ndata dot CSV file using the CSV dot read a method\nand then we are converting every element of that data set\ninto float originally all the ants are in string,\nbut we need to convert them into floor\nfor all calculation purposes.\nThe next we need to split the data into training data sets\nthat nay bias can use to make the prediction\nand this data set\nthat we can use to evaluate the accuracy of the model.\nWe need to split the data set randomly into training\nand testing data set in the ratio of usually\nwhich is 7230.\nBut for this example,\nI'm going to use 67\nand 33 now 70 and 30 is a Ratio for testing algorithms\nso you can play around with this number.\nSo this is our split data set function.\nNow the Navy base model is comprised\nof summary of the data in the training data set.\nNow this summary is then used while making predictions.\nNow the summary of the training data\ncollected involves the mean the standard deviation\nof each attribute by class value now, for example,\nif there are two class values and seven numerical attributes,\nthen we need a mean\nand the standard deviation for each of these seven attributes\nand the class value\nwhich makes The 14 attributes summaries\nso we can break the preparation\nof this summary down into the following sub tasks\nwhich are the separating data by class calculating mean\ncalculating standard deviation summarizing the data sets\nand summarizing attributes by class.\nSo the first task is to separate\nthe training data set instances by class value\nso that we can calculate statistics for each class.\nWe can do that by creating a map\nof each class value to a list of instances\nthat belong to the class.\nClass and sort the entire dataset of instances\ninto the appropriate list.\nNow the separate by class function just the same.\nSo as you can see the function assumes\nthat the last attribute is the class value\nthe function returns a map of class value to the list\nof data instances next.\nWe need to calculate the mean of each attribute\nfor a class value.\nNow, the mean is\nthe central middle or the central tendency of the data\nand we use it as a middle of our gaussian distribution\nwhen Calculating the probabilities.\nSo this is our function for mean now.\nWe also need to calculate the standard deviation\nof each attribute for a class value.\nThe standard deviation is calculated as a square root\nof the variance\nand the variance is calculated as the average\nof the squared differences\nfor each attribute value\nfrom the mean now one thing to note\nthat here is\nthat we are using n minus one method\nwhich subtracts one\nfrom the number of attributes values\nwhen calculating the variance.\nNow that we have the tools to summarize the data\nfor a given list of instances.\nWe can calculate the mean and standard deviation\nfor each attribute.\nNow that's if function groups the values for each attribute\nacross our data instances into their own lists\nso that we can compute the mean and standard deviation values\nfor each attribute.\nNow next comes the summarizing attributes by class.\nWe can pull it all together by first separating.\nOur training data sets into instances groped by class\nthen calculating the summaries for each a Should be now.\nWe are ready to make predictions using the summaries prepared\nfrom our training data\nmaking patients involved calculating the probability\nthat a given data instance belong to each class then\nselecting the class\nwith the largest probability as a prediction.\nNow we can divide this whole method into four tasks\nwhich are the calculating gaussian probability density\nfunction calculating class probability making a prediction\nand then estimating the accuracy\nnow to calculate the gaussian probability density function.\nWe use the gaussian function to estimate the probability\nof a given attribute value given the node mean\nand the standard deviation of the attribute estimated\nfrom the training data.\nAs you can see the parameters RX mean\nand the standard deviation now\nin the calculate probability function,\nwe calculate the exponent first then calculate the main division\nthis lets us fit the equation nicely into two lines.\nNow, the next task\nis calculating the class properties now\nthat we had can calculate the probability of an attribute\nbelonging to a class.\nWe can combine the probabilities of all the attributes values\nfor a data instance and come up with a probability\nof the entire.\nOur data instance belonging to the class.\nSo now that we have calculated the class properties.\nIt's time to finally make our first prediction now,\nwe can calculate the probability of the data instance belong\nto each class value\nand we can look for the largest probability\nand return the associated class\nand for that we are going to use this function predict\nwhich uses the summaries\nand the input Vector which is basically all the probabilities\nwhich are being input for a particular label\nnow finally we can An estimate the accuracy\nof the model by making predictions\nfor each data instances in our test data for that.\nWe use the get predictions method.\nNow this method is used\nto calculate the predictions based upon the test data sets\nand the summary of the training data set.\nNow, the predictions can be compared\nto the class values in our test data set\nand classification accuracy can be calculated as\nan accuracy ratio between the zeros\nand the hundred percent.\nNow the get accuracy method will calculate this accuracy ratio.\nNow finally to sum it all up.\nWe Define our main function we call all these methods\nwhich we have defined earlier one by one to get\nthe Courtesy of the model which we have created.\nSo as you can see,\nthis is our main function in which we have the file name.\nWe have defined the split ratio.\nWe have the data set.\nWe have the training and test data set.\nWe are using the split data set method next.\nWe are using the summarized by class function using\nthe get protection and the get accuracy method as well.\nSo guys as you can see the output of this one gives us\nthat we are splitting the 768 Rose into 514\nwhich is the training and 254\nwhich is the test data set rows and the accuracy of this model\nis 68% Now we can play with the amount of training\nand test data sets which are to be used\nso we can change the split ratio to seventies.\n238 is 220 to get different sort of accuracy.\nSo suppose I change the split ratio from 0.67 20.8.\nSo as you can see,\nwe get the accuracy of 62 percent.\nSo splitting it into 0.67 gave us a better result\nwhich was 68 percent.\nSo this is how you can Implement Navy bias caution classifier.\nThese are the step by step methods\nwhich you need to do in case of using the Nave Bayes classifier,\nbut don't worry.\nWe do not need to write all this many lines\nof code to make a model this with the second.\nAnd I really comes into picture the scikit-learn library has\na predefined method\nor as say a predefined function of nape bias,\nwhich converts all of these lines,\nof course into merely just two or three lines of codes.\nSo, let me just open another jupyter notebook.\nSo let me name it as sklearn a pass.\nNow here we are going to use the most famous data set\nwhich is the iris De Casa.\nNow, the iris flower data set is a multivariate\ndata set introduced by the British statistician\nand biologists Roland Fisher\nand based on this fish is linear discriminant model this data set\nbecame a typical test case\nfor many statistical classification techniques\nin machine learning.\nSo here we are going to use the caution NB model,\nwhich is already available in the sklearn.\nAs I mentioned earlier,\nthere were three types of Neighbors\nwhich are the question multinomial and the bernouli.\nSo here we are going to use the caution and be model\nwhich is already present in the SK loan Library,\nwhich is the cycle in library.\nSo first of all,\nwhat we need to do is import the sklearn data sets\nand the metrics and we also need to import the caution NB Now\nonce all these libraries\nare lowered we need to load the data set\nwhich is the iris dataset.\nThe next what we need to do is fit a Nave\nby a smaller to this data set.\nSo as you can see we have so easily defined the model\nwhich is the gaussian NB which contains\nall the programming\nwhich I just showed you earlier all the methods\nwhich are taking the input calculating the mean\nthe standard deviation separating it bike last\nand finally making predictions.\nCalculating the prediction accuracy.\nAll of this comes under the caution and be method\nwhich is inside already present in the sklearn library.\nWe just need to fit it according to the data set\nwhich we have so next\nif we print the model we see which is the gaussian NB model.\nThe next what we need to do is make the predictions.\nSo the expected output is data set dot Target\nand the projected is using the pretend model\nand the model we are using is the cause in N be here.\nNow to summarize the model\nwhich created we calculate the confusion Matrix\nand the classification report.\nSo guys, as you can see the classification to provide\nwe have the Precision of Point Ninety Six,\nwe have the recall of 0.96.\nWe have the F1 score\nand the support and finally if we print our confusion Matrix,\nas you can see it gives us this output.\nSo as you can see using the gaussian\nand we method just putting it in the model\nand using any of the data.\nfitting the model\nwhich you created into a particular data set\nand getting the desired output is so easy\nwith the scikit-learn library\nas we Mo support Vector machine is one\nof the most effective machine learning classifier\nand it has been used in various Fields\nsuch as face recognition cancer classification\nand so on today's session\nis dedicated to how svm works the various features of svm\nand how it Is used in the real world.\nAll right.\nOkay.\nNow let's move on and see what svm algorithm is all about.\nSo guys s VM\nor support Vector machine is a supervised learning algorithm,\nwhich is mainly used to classify data into different classes now\nunlike most algorithms svm makes use of a hyperplane\nwhich acts like a decision boundary\nbetween the various classes\nin general svm can be used to generate\nmultiple separating hyperplanes\nso that the data is Divided into segments.\nOkay, and each\nof these segments will contain only one kind of data.\nIt's mainly used\nfor classification purpose wearing you want to classify\nor data into two different segments depending\non the features of the data.\nNow before moving any further,\nlet's discuss a few features of svm.\nLike I mentioned earlier svm is a supervised learning algorithm.\nThis means that svm trains\non a set of labeled data svm studies the label training data\nand then classifies any new input Data,\ndepending on what it learned\nin the training phase a main advantage\nof support Vector machine is\nthat it can be used for both classification\nand regression problems.\nAll right.\nNow even though svm is mainly known for classification the svr\nwhich is the support Vector regressor is used\nfor regression problems.\nAll right, so svm can be used both for classification.\nAnd for regression.\nNow, this is one of the reasons why a lot of people prefer svm\nbecause it's a very good classifier and along\nThat it is also used for regression.\nOkay.\nAnother feature is the svm kernel functions svm can be used\nfor classifying nonlinear data\nby using the kernel trick the kernel trick basically\nmeans to transform your data into another dimension\nso that you can easily draw a hyperplane\nbetween the different classes of the data.\nAlright, nonlinear data is basically data\nwhich cannot be separated with a straight line.\nAlright, so svm can even be used on nonlinear data sets.\nYou just have to use a A kernel functions to do this.\nAll right.\nSo guys, I hope you all are clear\nwith the basic concepts of svm.\nNow, let's move on and look at how svm works\nso there's an order to understand how svm Works\nlet's consider a small scenario now for a second pretend\nthat you own a firm.\nOkay, and let's say that you have a problem\nand you want to set up a fence to protect your rabbits\nfrom the pack of wolves.\nOkay, but where do you\nbuild your films one way to get around?\nThe problem is to build a classifier based.\nOn the position of the rabbits and words in your pasture.\nSo what I'm telling you is you can classify the group\nof rabbits as one group\nand draw a decision boundary between the rabbits\nand the world correct.\nSo if I do that and if I try to draw a decision boundary\nbetween the rabbits and the Wolves,\nit looks something like this.\nOkay.\nNow you can clearly build a fence along this line\nin simple terms.\nThis is exactly\nhow SPM work it draws a decision boundary,\nwhich is a hyperplane\nbetween any New classes in order to separate them\nor classify them now.\nI know you're thinking how do you know\nwhere to draw a hyperplane\nthe basic principle behind svm is to draw a hyperplane\nthat best separates the two classes\nin our case the two glasses of the rabbits and the Wolves.\nSo you start off by drawing a random hyperplane\nand then you check the distance between the hyperplane\nand the closest data points\nfrom each Club these closes on your is data points\nto the hyperplane are known as support vectors.\nAnd that's where the name comes from support Vector machine.\nSo basically the hyperplane is drawn\nbased on these support vectors.\nSo guys an optimal hyperplane will have\na maximum distance from each of these support vectors.\nAll right.\nSo basically the hyperplane which has the maximum distance\nfrom the support vectors is the most optimal hyperplane\nand this distance between the hyperplane\nand the support vectors is known as the margin.\nAll right,\nso to sum it up svm is used to classify data.\nBy using a hyper plane such that the distance\nbetween the hyperplane and the support vectors is maximum.\nSo basically your margin has to be maximum.\nAll right, that way,\nyou know that you're actually separating your classes or add\nbecause the distance between the two classes is maximum.\nOkay.\nNow, let's try to solve a problem.\nOkay.\nSo let's say that I input a new data point.\nOkay.\nThis is a new data point\nand now I want to draw a hyper plane such\nthat it best separates the two classes.\nOkay, so I start off by drawing a hyperplane.\nLike this and then I check the distance\nbetween the hyperplane and the support vectors.\nOkay, so I'm trying to check\nif the margin is maximum for this hyper plane,\nbut what if I draw a hyperplane which is like this?\nAll right.\nNow I'm going to check the support vectors over here.\nThen I'm going to check the distance\nfrom the support vectors and for this hyperplane, it's clear\nthat the margin is more red.\nWhen you compare the margin of the previous one\nto this hyperplane.\nIt is more.\nSo the reason why I'm choosing this hyperplane is\nbecause the Distance between the support vectors\nand the hyperplane is maximum in this scenario.\nOkay.\nSo guys, this is how you choose a hyperplane.\nYou basically have to make sure\nthat the hyper plane has a maximum.\nMargin.\nAll right, it has to best separate the two classes.\nAll right.\nOkay so far it was quite easy.\nOur data was linearly separable\nwhich means that you could draw a straight line\nto separate the two classes.\nAll right, but what will you do?\nIf the data set is like this\nyou possibly can't draw a hyperplane like Is on it,\nit doesn't separate the two classes at all.\nSo what do you do\nin such situations now earlier in the session I mentioned\nhow a kernel can be used to transform data\ninto another dimension\nthat has a clear dividing margin between the classes of data.\nAlright, so kernel functions offer the user this option\nof transforming nonlinear spaces into linear ones.\nNonlinear data set is the one\nthat you can't separate using a straight line.\nAll right.\nIn order to deal with such data sets,\nyou're going to transform them into linear data sets\nand then use svm on them.\nOkay.\nSo simple trick would be to transform the two variables\nX and Y into a new feature space involving\na new variable called Z.\nAll right, so guys so far we were plotting our data\non two dimensional space.\nCorrect?\nWe will only using the X\nand the y axis so we had only those two variables X and Y now\nin order to deal with this kind of data a simple trick.\nBe to transform the two variables X\nand Y into a new feature space involving a new variable\ncalled Z. Okay,\nso we're basically visualizing the data\non a three-dimensional space.\nNow when you transform the 2D space into a 3D space\nyou can clearly see a dividing margin\nbetween the two classes of data right now.\nYou can go ahead and separate the two classes\nby drawing the best hyperplane between them.\nOkay, that's exactly\nwhat we discussed in the previous slides.\nSo guys, why don't you try this yourself dried.\nDrawing a hyperplane,\nwhich is the most Optimum for these two classes.\nAll right, so guys,\nI hope you have a good understanding\nabout nonlinear svm's now.\nLet's look at a real world use case\nif support Vector machines.\nSo guys s VM\nas a classifier has been used in cancer classification\nsince the early 2000s.\nSo there was an experiment held by a group of professionals\nwho applied svm in a colon cancer tissue classification.\nSo the data set consisted of about\nTransmembrane protein samples\nand only about 50 to 200 genes samples were input\nInto the svm classifier\nNow this sample\nwhich was input\ninto the svm classifier had both colon cancer tissue samples\nand normal colon tissue samples right now.\nThe main objective of this study was to classify Gene samples\nbased on whether they are cancerous or not.\nOkay, so svm was trained using the 50 to 200 samples\nin order to discriminate between non-tumor\nfrom A tumor specimens.\nSo the performance\nof the svm classifier was very accurate\nfor even a small data set.\nAll right, we had only 50 to 200 samples and even\nfor the small data set svm was pretty accurate\nwith this results.\nNot only that its performance was compared\nto other classification algorithms like naive Bayes\nand in each case svm outperform naive Bayes.\nSo after this experiment it was clear\nthat svm classified the data more effectively and it\nworked exceptionally good.\nSmall data sets.\nLet's go ahead\nand understand what exactly is unsupervised learning.\nSo sometimes the given data is unstructured and unlabeled\nso it becomes difficult to classify the data\ninto different categories.\nSo unsupervised learning helps to solve this problem.\nThis learning is used to Cluster the input data\nand classes on the basis of their statistical properties.\nSo example, we can cluster\nDifferent Bikes based upon the speed limit there.\nAcceleration or the average\nthat they are giving so\nand suppose learning is a type of machine learning algorithm\nused to draw inferences\nfrom beta sets consisting of input data\nwithout labeled responses.\nSo if you have a look at the workflow\nor the process flow of unsupervised learning,\nso the training data is collection of information\nwithout any label.\nWe have the machine learning algorithm\nand then we have the clustering malls.\nSo what it does is\nthat distributes the data into a different class.\nAnd again, if you provide any unreliable new data,\nit will make a prediction\nand find out to which cluster that particular data\nor the data set belongs\nto or the particular data point belongs to so one\nof the most important\nalgorithms in unsupervised learning is clustering.\nSo let's understand exactly what is clustering.\nSo a clustering\nbasically is the process of dividing the data sets\ninto groups consisting of similar data points.\nIt means grouping of objects based\non the information found in the data describing the object.\nObjects or their relationships\nso clustering malls focus on\nand defying groups of similar records\nand labeling records\naccording to the group to which they belong now this is done\nwithout the benefit of prior knowledge\nabout the groups and their characteristics.\nSo and in fact,\nwe may not even know exactly how many groups are\nthere to look for.\nNow.\nThese models are often referred to as\nunsupervised learning models,\nsince there's no external standard by which to judge.\nOne is classification performance.\nThere are no right or wrong answers to these model.\nAnd if we talk about why clustering is used\nso the goal of clustering is to determine\nthe intrinsic group in a set of unlabeled data sometime.\nThe partitioning is the goal\nor the purpose of clustering algorithm is to make sense\nof and exact value\nfrom the last set of structured and unstructured data.\nSo that is why clustering is used in the industry and\nif you have a look at the video,\nThese use cases of clustering in the industry.\nSo first of all, it's being used in marketing.\nSo discovering distinct groups\nin customer databases such as customers\nwho make a lot of long-distance calls customers\nwho use internet more\nthan cause they're also using insurance companies.\nSo like I need to find groups of Corporation insurance policy\nholders with high average claim rate Farmers crash cops,\nwhich is profitable.\nThey are using C Smith studies and defined problem areas of Oil\nor gas exploration Based on seesmic data,\nand they're also used in the recommendation of movies.\nIf you would say they are also used in Flickr photos.\nThey also used by Amazon\nfor recommending the product which category it lies in.\nSo basically if we talk\nabout clustering there are three types of clustering.\nSo first of all,\nwe have the exclusive clustering\nwhich is the hard clustering so here and item belongs\nexclusively to one cluster not several clusters\nand the data point.\nAlong exclusively to one cluster.\nSo an example of this is the k-means clustering\nso k-means clustering does this exclusive kind\nof clustering so secondly,\nwe have overlapping clustering\nso it is also known as soft clusters in this\nand item can belong\nto multiple clusters as its degree of association\nwith each cluster is shown and for example,\nwe have fuzzy or the c means clustering\nwhich is being used for overlapping clustering\nand finally we have The hierarchical clustering\nso when two clusters have a parent-child relationship\nor a tree-like structure,\nthen it is known as hierarchical cluster.\nSo as you can see here from the example,\nwe have a parent-child kind\nof relationship in the cluster given here.\nSo let's understand\nwhat exactly is K means clustering.\nSo k-means clustering is an algorithm whose main goal\nis to group similar elements of data points into a cluster\nand it is the process\nby which objects are classified into a predefined number.\nOf groups so that they are as much dissimilar as\npossible from one group to another group\nbut as much as similar or possible within each group now\nif you have a look at the algorithm working here, right?\nSo first of all,\nit starts with and defying the number of clusters,\nwhich is k then I can we find\nthe centroid we find the distance objects\nto the distance object\nto the centroid distance of object to the centroid\nthen we find the Dropping based\non the minimum distance has the centroid Converse\nif true then we make a cluster false.\nWe then I can't find the centroid repeat\nall of the steps again and again,\nso let me show you\nhow exactly clustering was with an example here.\nSo first we need to decide the number\nof clusters to be made now another important task here is\nhow to decide the important number of clusters\nor how to decide the number of clusters really get\ninto that later.\nSo first, let's assume\nthat the number Number of clusters we have decided\nis 3 so after that then we provide the centroids\nfor all the Clusters\nwhich is guessing\nand the algorithm calculates the euclidean distance\nof the point from each centroid\nand assigns the data point\nto the closest cluster now euclidean distance.\nAll of you know is the square root\nof the distance the square root of the square of the distance.\nSo next when the center is a calculated again,\nwe have our new clusters for each data point.\nAnd again the distance from the points\nto the new clusters are calculated and then again,\nthe points are assigned to the closest cluster.\nAnd then again,\nwe have the new centroid\nscattered and now these steps are repeated\nuntil we have a repetition the centroids\nor the new center eyes are very close to the very previous ones.\nSo antenna and less output gets repeated\nor the outputs are very very close enough.\nWe do not stop this process.\nWe keep on calculating the euclidean distance.\nIt's of all the points to the centroids.\nThen we calculate the new centroids\nand that is how clay means clustering Works basically,\nso an important part here is to understand\nhow to decide the value of K or the number of clusters\nbecause it does not make any sense.\nIf you do not know\nhow many classes are you going to make?\nSo to decide the number of clusters,\nwe have the elbow method.\nSo let's assume first\nof all compute the sum squared error,\nwhich is the sse4 some value.\nA for example,\nlet's take two four six and eight now the SS e\nwhich is the sum squared is defined as a sum\nof the squared distance between each number member\nof the cluster\nand its centroid mathematically and\nif you mathematically it is given by the equation\nwhich is provided here.\nAnd if you brought the key against the SSE,\nyou will see that the error decreases\nas K gets large now this is\nbecause the number of cluster increases\nthey should be smaller.\nSo does this torsion is also smaller know the idea\nof the elbow method is to choose the K at which\nthe SSC decreases abruptly.\nSo for example here\nif we have a look at the figure given here.\nWe see that the best number of cluster is at the elbow\nas you can see here the graph here changes abruptly\nafter number four.\nSo for this particular example,\nwe're going to use for as a number of cluster.\nSo first of all while working\nwith k-means clustering there are two key points,\nAs to know first of all be careful about various start.\nSo choosing the first center\nat random choosing the second center\nthat is far away from the first center similarly choosing\nthe NIH Center as far away as possible from the closest\nof the all the other centers\nand the second idea is to do as many runs\nof k-means each with different random starting points\nso that you get an idea of where exactly\nand how many clusters you need to make and\nwhere exactly the centroid lies.\nAnd how the data is getting confused\nnow k-means is not exactly a very good method.\nSo let's understand the pros and cons of clay means clusterings.\nWe know that k-means is simple and understandable.\nEveryone loves you\nthat the first go the items automatically assigned\nto the Clusters.\nNow if we have a look at the cons,\nso first of all one needs to define the number of clusters,\nthere's a very heavy task asks us\nif we have 3/4 or\nif we have 10 categories and if we do not know\nwhat the number of clusters are going to be.\nIt's Difficult for anyone to you know to guess the number\nof clusters not all items are forced into clusters\nwhether they are actually belong to any other cluster\nor any other category,\nthey are forced to to lie\nin that other category in which they are closest\nto this against happens because of the number\nof clusters with not defining the correct number of clusters\nor not being able to guess the correct number of clusters.\nSo and most of all it's unable\nto handle the noisy data and the outliners because anyway,\nAs machine learning engineers\nand data scientists have to clean the data.\nBut then again it comes down\nto the analysis watch they are doing and the method\nthat they are using so typically people do not clean the data\nfor k-means clustering even\nif the clean there's sometimes a now see noisy\nand outliners data which affect the whole model\nso that was all for k-means clustering.\nSo what we're going to do is now use k-means clustering\nfor the We data set\nso we have to find out the number of clusters\nand divide it accordingly.\nSo the use case is that first of all,\nwe have a data set of five thousand movies.\nAnd what you want to do is grip them\nif the movies into clusters based on the Facebook likes,\nso guys, let's have a look at the demo here.\nSo first of all,\nwhat we're going to do is import deep copy numpy pandas\nSeaborn the various libraries,\nwhich we're going to use now and from map popular videos.\nIn the use ply plot,\nand we're going to use this ggplot and next\nwhat we're going to do is import the data set\nand look at the shape of the data is it\nso if we have a look at the shape of the data set we can see\nthat it has 5043 rows with Twenty Eight columns.\nAnd if you have a look\nat the head of the data set we can see it has 5043 data points,\nso What we're going to do is place the data points\nin the plot me take the director Facebook likes\nand we have a look at the data columns face number\nand post cars total\nFacebook likes director Facebook likes.\nSo what we have done here\nnow is taking the director Facebook likes and the actor\nthree Facebook likes, right.\nSo we have five thousand forty three rows\nand two columns Now using the k-means from sklearn\nwhat we're going to do is import it.\nFirst we're going to import k-means\nfrom sklearn dot cluster.\nRemember guys Escalon is a very important library\nin Python for machine learning.\nSo and the number of cluster\nwhat we're going to do is provide as five now this again,\nthe number of cluster depends upon the SSE,\nwhich is the sum of squared errors\nor the we're going to use the elbow method.\nSo I'm not going to go into the details of that again.\nSo we're going to fit the data into the k-means to fit and\nif you find the cluster,\nUs then for the k-means and printed.\nSo what we find is is an array of five clusters\nand Fa print the label of the Caymans cluster.\nNow next what we're going to do is plot the data\nwhich we have with the Clusters with the new data clusters,\nwhich we have found and for this we're going\nto use the si bon and as you can see here,\nwe have plotted that car.\nWe have plotted the data into the grid\nand You can see here.\nWe have five clusters.\nSo probably what I would say is\nthat the cluster 3 and the cluster\nzero are very very close.\nSo it might depend see that's exactly\nwhat I was going to say.\nIs that initially the main Challenge\nand k-means clustering is to define the number of centers\nwhich are the K.\nSo as you can see here\nthat the third Center\nand the zeroth cluster the third cluster\nand the zeroth cluster up very very close to each other\nso It probably could have been in one another cluster\nand the another disadvantage was\nthat we do not exactly know\nhow the points are to be arranged.\nSo it's very difficult to force the data into any other cluster\nwhich makes our analysis a little different works fine.\nBut sometimes it might be difficult to code\nin the k-means clustering now,\nlet's understand what exactly is seems clustering.\nSo the fuzzy c means\nis an extension of the k-means clustering the popular simple.\nClustering technique so fuzzy clustering also referred\nas soft clustering is a form\nof clustering in which each data point can belong\nto more than one cluster.\nSo k-means tries to find the heart clusters\nwhere each point belongs to one cluster.\nWhereas the fuzzy c means discovers the soft clusters\nin a soft cluster any point can belong\nto more than one cluster\nat a time with a certain Affinity value\ntowards each 4zc means assigns the degree of membership,\nwhich Just from 0 to 1 to an object to a given cluster.\nSo there is a stipulation that the sum of the membership\nof an object to all the cluster.\nIt belongs to must be equal to 1 so the degree of membership\nof this particular point to pull of these clusters as 0.6 0.4.\nAnd if you add up we get 1\nso that is one of the logic behind the fuzzy c means\nso and and this Affinity is proportional to the distance\nfrom the point to the center of the cluster now then again\nNow we have the pros and cons of fuzzy see means.\nSo first of all,\nit allows a data point to be in multiple cluster.\nThat's a pro.\nIt's a more neutral representation of the behavior\nof jeans jeans usually are involved in multiple functions.\nSo it is a very good type of clustering\nwhen we're talking about genes First of and again,\nif we talk about the cons again,\nwe have to Define c which is the number\nof clusters same as K next.\nWe need to determine the membership cutoff value also,\nso that takes a lot of Time and it's time-consuming\nand the Clusters\nare sensitive to initial assignment of centroid.\nSo a slight change\nor deviation from the center has it's going to result\nin a very different kind of, you know,\na funny kind of output we get from the fuzzy c means and one\nof the major disadvantage of see means clustering is\nthat it's this are non deterministic algorithm.\nSo it does not give you\na particular output as in such that's\nthat now let's have a look.\nAt the third type\nof clustering which is the hierarchical clustering.\nSo hierarchical clustering is an alternative approach\nwhich builds a hierarchy from the bottom up\nor the top to bottom\nand does not require to specify the number\nof clusters beforehand.\nNow, the algorithm works as in first of all,\nwe put each data point in its own cluster and\nif I the closest to Cluster\nand combine them into one more cluster repeat the above step\ntill the data points are in a single cluster.\nNow, there are two types of hierarchical clustering one is\nI've number 80 plus string\nand the other one is division clustering.\nSo a commemorative clustering bills the dendogram\nfrom bottom level\nwhile the division clustering it starts all the data points\nin one cluster the fruit cluster now again\nhierarchical clustering also has some sort of pros and cons.\nSo in the pros don't know Assumption\nof a particular number of cluster is required\nand it may correspond to meaningful taxonomist.\nWhereas if we talk about the cons\nonce a decision is made to combine two clusters.\nHas it cannot be undone and one\nof the major disadvantage of these hierarchical clustering is\nthat it becomes very slow.\nIf we talked about very very large data sets and nowadays.\nI think every industry are using last year as its and collecting\nlarge amounts of data.\nSo hierarchical clustering is not the act\nor the best method someone might need to go for so there's\nthat now when we talk about unsupervised learning,\nso we have K means clustering and again,\nAnother important term\nwhich people usually Miss while talking about us was running and\nthere's one very important concept\nof Market Basket analysis.\nNow, it is one of the key techniques\nused by large retailers to uncover association\nbetween items now it works by looking\nfor combination of items\nthat occur together frequently\nin the transactions to put it in other way.\nIt allows retailers to identify the relationships\nbetween the items\nthat the People by for example people\nwho buy bread also tend to buy butter the marketing team\nat the retail stores should Target customers\nwho buy bread and butter and provide them and offer\nso that they buy a third item like an egg.\nSo if a customer buys bread and butter and sees a discount\nor an offer on X,\nhe will be encouraged to spend more money and buy the eggs.\nNow, this is what Market Basket analysis is all about now\nto find the association between the two items\nand make predictions about what the customers will buy.\nThere are two Cartoons which are the association rule Mining\nand the ebrary algorithms.\nSo let's discuss each of these algorithm\nwith an example.\nFirst of all,\nif we have a look at the association rule mining now,\nit's a technique that's shows\nhow items are associated to each other for example customers\nwho purchased spread have a 60 percent likelihood\nof also purchasing jam and customers\nwho purchase laptop are more likely to purchase laptop bags.\nNow if you take an example of an association rule\nif we have a look at the Example here a arrow B.\nIt means that\nif a person buys an atom a then he will also buy an atom P. Now.\nThere are three common ways to measure a particular Association\nbecause we have to find these rules not on the basis\nof some statistics, right?\nSo what we do is use\nsupport confidence and lift now these three common ways\nand the measures to have a look\nat the association rule Mining and know exactly\nhow good is that rule.\nSo first of all,\nwe have support So support gifts the fraction of the\nWhich contains an item A and B.\nSo it's basically the frequency of the item\nin the whole item set.\nWhere's confidence gifts\nhow often the item A and B occurred together\ngiven the number of item given the number\nof times a occur.\nSo it's frequency a comma B divided by\nthe frequency of a now left\nwhat indicates is the strength of the rule over the random\nco-occurrence of A and B.\nIf you have a close look at the denominator\nof the lift formula here,\nwe have support a into support be and now a major thing\nwhich can be noted from this is\nthat the support of A and B are independent here.\nSo if the value of lift\nor the denominator value of the lift is more it means\nthat the items are independently selling more not together.\nSo that in turn will decrease the value of lift.\nSo what happens is\nthat suppose the value of lift is more that implies\nthat the rule which we get.\nIt implies that the rule is strong and it\nAnd we used for later purposes because in that case the support\nin to support P value,\nwhich is the denominator of lift will be low\nwhich in turn means\nthat there is a relationship between the items in the and B.\nSo let's take an example of Association rule Mining\nand understand how exactly it works.\nSo let's suppose we have a set of items a b c d\nand e and we have the set of transactions\nwhich are T1 T2,\nT3, T4 and T5\nand what we need to do is create some sort of Rules,\nfor example, you can see a d\nwhich means that if a person buys a he buys D\nif a person by see he buys a if a person buys a he by C.\nAnd for the fourth one is\nif a person by B and C Hill in turn by a now\nwhat we need to do is calculate the support confidence and lift\nof these rules now here again,\nwe talked about a priority algorithm.\nSo a priori algorithm\nand the association rule mining go hand in hand.\nSo what a predator This algorithm.\nIt uses the frequent itemsets to generate the association rules\nand it is based on the concept\nthat a subset\nof a frequent itemsets must also be a frequent Isom set.\nSo let's understand\nwhat is a frequent item set and how all of these work together.\nSo if we take the following transactions of items,\nwe have transaction T 1 2 T 5 and the items are 1 3 4\n2 3 5 1 2 3 5 2 5 and 1 3 5 now.\nNow another more important thing about support\nwhich I forgot to mention was\nthat when talking\nabout Association rule mining there is a minimum support count\nwhat we need to do.\nNow.\nThe first step is to build a list of items\nthat of size 1 using this transaction data\nset and use the minimum support count to now,\nlet's see how we do that if we create the table see\nwhen you have a close look at the table c 1\nwe have the items at one which has support three\nbecause it appears in the transaction one.\nThree and five similarly\nif you have a look at the item set the single item 3.\nSo it has the support of for it appears\nin t 1 T 2 T 3 and T 5 but\nif we have a look at the item set for it only appears\nin the transaction once\nso it's support value is 1 now the item set with\nthe support value Which is less than the minimum support value\nthat is to have to be eliminated.\nSo the final table\nwhich is a table F1 has one two three.\nAnd five it does not contain the for now.\nWhat we're going to do is create the item list of the size\n2 and all the combination of the item sets in F1.\nI used in this iteration.\nSo we're left for behind.\nWe just have 1 2 3 &amp; 5.\nSo the possible item sets a 1 2 1 3\n1 5 2 3 2 5 &amp; 3 5 then again.\nWe will calculate the support So\nin this case if we have a closer look at the table\nc 2 we see that the items at once.\nWhat to do is having\na support value 1 which has to be eliminated.\nSo the final table f 2 does not contain 1 comma 2 similarly\nif we create the item sets\nof size 3 and calculate this support values,\nbut before calculating the support, let's perform\nthe puring on the data set.\nNow what's appearing?\nSo after all the combinations are made we divide the table\nc 3 items to check\nif there are another subset whose support is less\nthan the minimum support value.\nThis is a prairie algorithm.\nSo in the item sets one, two,\nthree what we can see that we have one two,\nand in the one to five again,\nwe have one too so build this cardboard of these item sets\nand we'll be left with 1 3 5 and 2 3 5.\nSo with one three five,\nwe have three subsets one five one, three three five,\nwhich are present in table F2.\nThen again.\nWe have two three to five and 3/5\nwhich are also present in t will f 2\nso we have 2 Move 1 comma 2 from the table c\n3 and create the table F3 now\nif you're using the items of C3 to create the atoms of C-4.\nSo what we find is\nthat we have the item set 1 2 3 5 the support value is\n1 Which is less than the minimum support value of 2.\nSo what we're going to do is stop here\nand we're going to return to the previous item set.\nThat is the table c 3 so the final table.\nWell, if three was one three five with\nthe support value of 2 and 2 3 5 with the support value of 2 now,\nwhat we're gonna do is generate all the subsets\nof each frequent itemsets.\nSo let's assume\nthat minimum confidence value is 60% So for every subset s\nof I the output rule is that s gives i2s\nis that s recommends i ns.\nIf the support of I / support of s is greater than or equal.\nEqual to the minimum confidence value,\nthen only will proceed further.\nSo keep in mind\nthat we have not used left till now.\nWe are only working with support and confidence.\nSo applying rules with item sets of F3\nwe get rule 1 which is 1 comma 3 which gives 1 3 5 and 1/3.\nIt means if you buy one and three there's a 66% chance\nthat you will buy item 5 also\nsimilarly the rule 1 comma 5 it means\nthat If you buy one and five,\nthere's a hundred percent chance\nthat you will buy three also similarly\nif we have a look at Rule 5 and 6 here\nthe confidence value is\nless than 60 percent which was the assumed confidence value.\nSo what we're going to do is with reject these files now\nan important thing to note here is\nthat have a closer look to the Rule 5 and root 3,\nyou see it has one five three one five three three point five.\nIt's very confusing.\nSo one thing to keep in Mine is\nthat the order of the item sets is also very important\nthat will help us allow create good rules\nand avoid any kind of confusion.\nSo that's that.\nSo now let's learn\nhow Association rule I used in Market Basket analysis problems.\nSo what we'll do is we will be using\nthe online transactions data\nof a retail store for generating Association rules.\nSo first of all,\nwhat you need to do is import pandas MSD ml.\nD&amp;D libraries from the imported and read the data.\nSo first of all, what we're going to do is read the data,\nwhat we're going to do is from M LX T\nand E dot frequent patterns.\nWe're going to improve the a priori and Association rules.\nAs you can see here.\nWe have the head of the data.\nYou can see we have invoice number stock code\nthe description quantity\nthe invoice dt8 unit price customer ID and the country.\nSo in the next step,\nwhat we will do is we will do the data cleanup\nwhich includes removing.\nHis from some of the descriptions given\nand what we're going to do is drop the rules\nthat do not have\nthe invoice numbers every move the crate transactions.\nSo hey, what what you're going to do is remove\nwhich do not have any invoice number\nif the string tight\nainst Epstein was a number then we're going to remove that.\nThose are the credits remove any kind of spaces\nfrom the descriptions.\nSo as you can see here,\nwe have like five hundred and thirty-two thousand rows\nwith eight columns.\nSo next one.\nWe're going to do is after the cleanup.\nWe need to consolidate the items into one transaction per row\nwith each product for the sake of keeping the data set small.\nWe're going to only look at the sales for France.\nSo we're going to use the only France and group\nby invoice number description\nwith the quantity sum up and see so\nwhich leaves us with three ninety two rows\nand one thousand five hundred sixty three columns.\nNow, there are a lot of zeros in the data,\nbut we also need to make sure Any positive values\nare converted to a 1 and anything less than 0 is set to 0\nso for that we're going to use this code defining\nand code units\nif x is less than 0 return 0 if x\nis greater than 1 returned one.\nSo what we're going to do is map\nand apply it to the whole data set we have here.\nSo now that we have structured data properly.\nSo the next step is to generate the frequent item set\nthat has support of at least seven percent.\nNow this number is chosen so that you can get close enough.\nNow, what we're going to do is generate the rules\nwith the corresponding support confidence and lift.\nSo we had given the minimum support a 0.7.\nThe metric is left frequent Island set\nand threshold is 1 so these are\nthe following rules now a few rules with a high lift value,\nwhich means that it occurs more frequently\nthan would be expected given the number of transaction\nthe product combinations most of the places the confidence.\nIs high as well.\nSo these are few to observations what we get here.\nIf we filter the data frame using the standard pandas code\nfor large lift six and high confidence 0.8.\nThis is what the output is going to look like.\nThese are 1 2 3 4 5 6 7 8.\nSo as you can see here,\nwe have the H rules which are the final rules\nwhich are given by the Association rule Mining\nand this is how all the industries are.\nAre any of these we've talked about largely retailers.\nThey tend to know\nhow their products are used and how exactly they\nshould rearrange and provide the offers on the product\nso that people spend more and more money\nand time in the shop.\nSo that was all about Association rule mining.\nSo so guys,\nthat's all for unsupervised learning.\nI hope you got to know about the different formulas\nhow unsupervised learning works because you know,\nwe did not provide any label to the data.\nAll we did was create some rules and not knowing what the data is\nand we did clusterings different types of clusterings\ncame in simi's hierarchical clustering.\nThe reinforcement learning is a part of machine learning\nwhere an agent is put in an environment\nand he learns to behave in this environment\nby performing certain actions.\nOkay, so it basically performs actions and it either gets\na rewards on the actions\nor It gets a punishment and observing the reward\nwhich it gets from those actions reinforcement learning is all\nabout taking an appropriate action in order\nto maximize the reward in a particular situation.\nSo guys in supervised learning the training data comprises\nof the input\nand the expected output\nand so the model is trained with the expected output itself,\nbut when it comes to reinforcement learning,\nthere is no expected output here the reinforcement agent decides.\nWhat actions to take in order to perform a given task.\nIn the absence of a training data set it is bound to learn\nfrom its experience itself.\nAll right.\nSo reinforcement learning is all about an agent\nwho's put in an unknown environment\nand he's going to use a hit and trial method in order\nto figure out the environment\nand then come up with an outcome.\nOkay.\nNow, let's look at reinforcement learning\nwithin an analogy.\nSo consider a scenario where in a baby is learning\nhow to walk the scenario can go about in two ways.\nNow in the first case the baby starts walking\nand makes it to the candy here.\nThe candy is basically the reward it's going to get so\nsince the candy is the end goal.\nThe baby is happy.\nIt's positive.\nOkay, so the baby is happy and it gets rewarded a set\nof candies now another way in which this could go is\nthat the baby starts walking\nbut Falls due to some hurdle in between the baby gets hurt\nand it doesn't get any candy and obviously the baby is sad.\nSo this is a negative reward.\nOkay, or you can say this is a setback.\nSo just like how we humans learn\nfrom our mistakes by trial and error.\nLearning is also similar.\nOkay, so we have an agent\nwhich is basically the baby and a reward\nwhich is the candy over here.\nOkay, and with many hurdles in between the agent is supposed\nto find the best possible path to read through the reward.\nSo guys, I hope you all are clear\nwith the reinforcement learning.\nNow.\nLet's look at the reinforcement learning process.\nSo generally a reinforcement learning system has\ntwo main components.\nAll right, the first is an agent and the second one is\nan environment now in the previous case,\nwe saw that the agent was a baby.\nB and the environment was the living room\nwhere in the baby was crawling.\nOkay.\nThe environment is the setting\nthat the agent is acting on and the agent over here\nrepresents the reinforcement learning algorithm.\nSo guys the reinforcement learning process starts\nwhen the environment sends a state to the agent\nand then the agent will take some actions based\non the observations\nin turn the environment will send the next state\nand the respective reward back to the agent.\nThe agent will update\nits knowledge with the reward returned by the I meant\nand it uses that to evaluate its previous action.\nSo guys this Loop keeps continuing\nuntil the environment sends a terminal state which means\nthat the agent has accomplished all his tasks\nand he finally gets the reward.\nOkay.\nThis is exactly\nwhat was depicted in this scenario.\nSo the agent keeps climbing up ladders\nuntil he reaches his reward to understand this better.\nLet's suppose that our agent is learning to play Counter-Strike.\nOkay, so let's break it down now initially the RL agent\nwhich is Only the player player 1 let's say it's the player\n1 who is trying to learn how to play the game.\nOkay.\nHe collects some state from the environment.\nOkay.\nThis could be the first state of Counter-Strike now based\non the state the agent will take some action.\nOkay, and this action can be anything\nthat causes a result.\nSo if the player moves left\nor right it's also considered as an action.\nOkay.\nSo initially the action is going to be random\nbecause obviously the first time you pick up Counter-Strike,\nyou're not going to be a master at it.\nSo you're going to try with different actions\nand you're just going to Up a random action\nin the beginning.\nNow the environment is going to give a new state.\nSo after clearing\nthat the environment is now going to give a new state\nto the agent or to the player.\nSo maybe he's across stage 1 now.\nHe's in stage 2.\nSo now the player\nwill get a reward our one from the environment\nbecause it cleared stage 1.\nSo this reward can be anything.\nIt can be additional points or coins or anything like that.\nOkay.\nSo basically this Loop keeps going on\nuntil the player is dead or reaches the destination.\nOkay, and it Continuously outputs a sequence\nof States actions and rewards.\nSo guys.\nThis was a small example to show\nyou how reinforcement learning process works.\nSo you start with an initial State\nand once a player clothes that state he gets a reward\nafter that the environment will give another stage to the player\nand after it clears that state it's going to get another reward\nand it's going to keep happening\nuntil the player reaches his destination.\nAll right, so guys, I hope this is clear now,\nlet's move on and look\nat the reinforcement learning definition.\nSo there are a few Concepts that you should be aware\nof while studying reinforcement learning.\nLet's look at those definitions over here.\nSo first we have the agent now an agent is basically\nthe reinforcement learning algorithm that learns\nfrom trial and error.\nOkay.\nSo an agent takes actions,\nlike for example a soldier in Counter-Strike navigating\nthrough the game.\nThat's also an action.\nOkay, if he moves left right or if he shoots at somebody\nthat's also an action.\nOkay.\nSo the agent is responsible\nfor taking actions in the environment.\nNow the environment is the whole Counter-Strike game.\nOkay.\nIt's basically the world through which the agent\nmoves the environment takes the agents current state\nand action as input\nand it Returns the agency reward and its next state as output.\nAlright, next we have action now all the possible steps\nthat an agent can take are called actions.\nSo like I said,\nit can be moving right left or shooting or any of that.\nAlright, then we have state now state is\nbasically the current condition returned by the environment.\nSo Double State you are in\nif you are in state 1 or if you're interested\nto that represents your current condition.\nAll right.\nNext we have reward a reward is basically an instant return\nfrom the environment to appraise Your Last Action.\nOkay, so it can be anything like coins\nor it can be additional points.\nSo basically a reward is given to an agent\nafter it clears.\nThe specific stages.\nNext we have policy policy is basically the strategy\nthat the agent uses to find out his next action.\nIn based on his current state policy is just\nthe strategy with which you approach the game.\nThen we have value.\nNow while you is the expected long-term return with discount\nso value and action value can be a little bit confusing\nfor you right now.\nBut as we move further,\nyou'll understand what I'm talking about.\nOkay, so value is basically the long-term return\nthat you get with discount.\nOkay discount, I'll explain in the further slides.\nThen we have action value\nnow action value is also known as Q value.\nOkay, it's very similar to what You except\nthat it takes an extra parameter,\nwhich is the current action.\nSo basically here you'll find out the Q value depending\non the particular action that you took.\nAll right.\nSo guys don't get confused with value and action value.\nWe look at examples\nin the further slides and you will understand this better.\nOkay, so guys make sure\nthat you're familiar with these terms\nbecause you'll be seeing a lot of these terms\nin the further slides.\nAll right.\nNow before we move any further,\nI'd like to discuss a few more Concepts.\nOkay.\nSo first we will discuss the reward maximization.\nSo if you haven't already realize the it the basic aim\nof the RL agent is to maximize the reward now,\nhow does that happen?\nLet's try to understand this in depth.\nSo the agent must be trained in such a way\nthat he takes the best action so that the reward is maximum\nbecause the end goal of reinforcement learning\nis to maximize your reward based on a set of actions.\nSo let me explain this with a small game now\nin the figure you can see there is a Forks there's some meat\nand there's a tiger So odd agent is basically the fox\nand his end goal is to eat the maximum amount of meat\nbefore being eaten by the tiger now\nsince the fox is a clever fellow\nhe eats the meat\nthat is closer to him rather than the meat\nwhich is closer to the tiger.\nNow this is because the closer he is to the tiger\nthe higher are his chances of getting killed.\nSo because of this the rewards which are near the tiger,\neven if they are bigger meat chunks,\nthey will be discounted.\nSo this is exactly what discounting means\nso our agent is not going to eat the meat chunks\nwhich are Closer to the tiger because of the risk.\nAll right now even\nthough the meat chunks might be larger.\nHe does not want to take the chances of getting killed.\nOkay.\nThis is called discounting.\nOkay.\nThis is where you discount\nbecause it improvised and you just eat the meat\nwhich are closer to you instead of taking risks\nand eating the meat\nwhich are closer to your opponent.\nAll right.\nNow the discounting of reward Works based\non a value called gamma will be discussing gamma\nin our further slides,\nbut in short the value of gamma is between 0 and 1.\nOkay.\nSo the Follow the gamma.\nThe larger is the discount value.\nOkay.\nSo if the gamma value is lesser,\nit means that the agent is not going to explore\nand he's not going to try and eat the meat chunks\nwhich are closer to the tiger.\nOkay, but if the gamma value is closer to 1 it means\nthat our agent is actually going to explore\nand it's going to dry and eat the meat chunks\nwhich are closer to the tiger.\nAll right now,\nI'll be explaining this in depth in the further slides.\nSo don't worry\nif you haven't got a clear concept yet,\nbut just understand that reward maximized.\nAtion is a very important step\nwhen it comes to reinforcement learning\nbecause the agent has to collect maximum rewards\nby the end of the game.\nAll right.\nNow, let's look at another concept\nwhich is called exploration and exploitation.\nSo exploration like the name suggests is about exploring\nand capturing more information about an environment\non the other hand exploitation is about using the already\nknown exploited information to hide in the rewards.\nSo guys consider the fox and tiger example\nthat we discussed now here\nthe foxy Only the meat chunks which are close to him,\nbut he does not eat the meat chunks\nwhich are closer to the tiger.\nOkay, even though they might give him more Awards.\nHe does not eat them\nif the fox only focuses on the closest rewards,\nhe will never reach the big chunks of meat.\nOkay, this is what exploitation is\nabout you just going to use the currently known information\nand you're going to try and get rewards based\non that information.\nBut if the fox decides to explore a bit,\nit can find the bigger award which is the big chunks of meat.\nThis is exactly what exploration is.\nSo the agent is not going to stick to one corner instead.\nHe's going to explore the entire environment and try\nand collect bigger rewards.\nAll right, so guys,\nI hope you all are clear with exploration and exploitation.\nNow, let's look at the markers decision process.\nSo guys, this is basically a mathematical approach\nfor mapping a solution in reinforcement learning in a way.\nThe purpose of reinforcement learning is to solve\na Markov decision process.\nOkay, so there are a few parameters.\nWas that I used to get to the solution.\nSo the parameters include the set of actions the set\nof states the rewards the policy\nthat you're taking to approach the problem and the value\nthat you get.\nOkay, so to sum it up the agent must take\nan action a to transition\nfrom a start state to the end State s\nwhile doing so the agent will receive a reward are\nfor each action that he takes.\nSo guys a series\nof actions taken by the agent Define the policy\nor a defines the approach.\nAnd the rewards\nthat are collected Define the value.\nSo the main goal here is to maximize the rewards\nby choosing the optimum policy.\nAll right.\nNow, let's try to understand this with the help\nof the shortest path problem.\nI'm sure a lot of you might have gone through this problem\nwhen you are in college,\nso guys look at the graph over here.\nSo our aim here is to find the shortest path\nbetween a and d with minimum possible cost.\nSo the value that you see on each of these edges\nbasically denotes the cost.\nSo if I want to go from A to see it's gonna cost me 15 points.\nOkay.\nSo let's look at how this is done.\nNow before we move and look at the problem\nin this problem the set of states are denoted by the nodes,\nwhich is ABCD\nand the action is to Traverse from one node to the other.\nSo if I'm going from A to B,\nthat's an action similarly a to see\nthat's an action.\nOkay, the reward is basically the cost\nwhich is represented by each Edge over here.\nAll right.\nNow the policy is basically the path that I choose\nto reach the destination,\nso Let's say I choose a seed be okay,\nthat's one policy in order to get to D\nand choosing a CD which is a policy.\nOkay.\nIt's basically how I'm approaching the problem.\nSo guys here you can start off at node a\nand you can take baby steps to your destination.\nNow initially you're clueless\nso you can just take the next possible node,\nwhich is visible to you.\nSo guys, if you're smart enough,\nyou're going to choose a to see instead of ABCD or ABD.\nAll right.\nSo now if you are at nodes see you want to drive.\nString note D.\nYou must again choose a weisbarth.\nAll right, you just have to calculate which path has\nthe highest cost\nor which path will give you the maximum rewards.\nSo guys, this is a simple problem.\nWe just trying to calculate the shortest path between a\nand d by traversing through these nodes.\nSo if I Traverse from a CD, it gives me the maximum reward.\nOkay, it gives me 65,\nwhich is more than any other policy would give me.\nOkay.\nSo if I go from ABD,\nit would be 40 when you compare this to a CD.\nIt gives me more reward.\nSo obviously I'm going to go with a CB.\nOkay, so guys was a simple problem\nin order to understand how Markov decision process works.\nAll right, so guys, I want to ask you a question.\nWhat do you think?\nI did hear did I perform exploration or did I\nperform exploitation now\nthe policy for the above example is of exploitation\nbecause we didn't explore the other nodes.\nOkay.\nWe just selected three notes and we travel through them.\nSo that's why this is called exploitation.\nWe must always explore the different notes\nso that we Find a more optimal policy.\nBut in this case,\nobviously a CD has the highest reward\nand we're going with a CD but generally it's not so simple.\nThere are a lot of nodes there hundreds of notes you Traverse\nand there are like 50 60 policies.\nOkay, 50 60 different policies.\nSo you make sure you explore through all the policies\nand then decide on an Optimum policy\nwhich will give you a maximum reward the for a robot\nand environment is a place\nwhere It has been put to use now.\nRemember this reward is itself\nthe agent for example an automobile Factory\nwhere a robot is used to move materials\nfrom one place to another now the task we discussed just\nnow have a property in common.\nNow, these tasks involve and environment and expect\nthe agent to learn from the environment.\nNow, this is where traditional machine learning phase\nand hence the need for reinforcement learning now.\nIt is good to have an established overview\nof the problem.\nThat is to be Of using the Q learning\nor the reinforcement learning\nso it helps to define the main components\nof a reinforcement learning solution.\nThat is the agent environment action rewards and States.\nSo let's suppose we are to build\na few autonomous robots for an automobile building Factory.\nNow, these robots will help the factory personal\nby conveying them the necessary parts\nthat they would need in order to pull the car.\nNow.\nThese different parts are located at\nnine different positions within the factory warehouse.\nThe car part include the chassis Wheels dashboard\nthe engine and so on\nand the factory workers have prioritized the location\nthat contains the body\nor the chassis to be the topmost but they\nhave provided the priorities for other locations as well,\nwhich will look into the moment.\nNow these locations\nwithin the factory look somewhat like this.\nSo as you can see here,\nwe have L1 L2 L3\nall of these stations now one thing you might notice here\nthat there Little obstacle prison in between the locations.\nSo L6 is the top priority location\nthat contains the chassis for preparing the car bodies.\nNow the task is to enable the robots\nso that they can find the shortest route\nfrom any given location to another location on their own.\nNow the agents in this case are the robots the environment\nis the automobile Factory warehouse.\nSo let's talk about the state's the states are the location\nin which a particular robot is\nAnd in the particular instance of time,\nwhich will denote it states the machines understand numbers\nrather than let us so let's map the location codes to number.\nSo as you can see here,\nwe have mapped location l 1 to this t 0 L 2 and 1\nand so on we have L8 as state 7 and L line at state.\nSo next what we're going to talk about are the actions.\nSo in our example,\nthe action will be the direct location\nthat a robot can go\nfrom a particular location right considering What\nthat is a tel to location\nand the Direct locations to which it can move rl5 L1 and L3.\nNow the figure here may come in handy to visualize this now\nas you might have already guessed the set of actions\nhere is nothing but the set\nof all possible states\nof the robot for each location the set of actions\nthat a robot can take will be different.\nFor example, the set of actions will change\nif the robot is in L1 rather than L2.\nSo if the robot is Is in L1 it can only go\nto L 4 and L 2 directly now\nthat we are done with the states and the actions.\nLet's talk about the rewards.\nSo the states are basically zero one two,\nthree four and the actions are also 0 1\n2 3 4 up to 8.\nNow.\nThe rewards now will be given to a robot.\nIf a location\nwhich is the state is directly reachable\nfrom a particular location.\nSo let's take an example suppose L line is directly reachable\nfrom L8, right?\nIf a robot goes from LA to align and vice versa,\nit will be rewarded by one and\nif I look a shin is not directly reachable\nfrom a particular equation.\nWe do not give any reward a reward of 0 now the reward\nis just a number\nand nothing else it enables the robots to make sense\nof the movements helping them\nin deciding what locations are directly reachable\nand what are not now\nwith this Q. We can construct a reward table\nwhich contains all the required values mapping\nbetween all possible States.\nSo as you can see here in the table the positions\nwhich are marked green have a positive reward.\nAnd as you can see here,\nwe have all the possible rewards that a robot can get by moving\nin between the different states.\nNow comes an interesting decision.\nNow remember that the factory administrator prioritized L6\nto be the topmost.\nSo how do we incorporate this fact in the above table.\nNow, this is done by associating the topmost priority location\nwith a very high reward than the usual ones.\nSo let's put 990.\nAnd in the cell L 6 comma\nand 6 now the table of rewards with a higher reward\nfor the topmost location looks something like this.\nWe have not formally defined all the vital components\nfor the solution.\nWe are aiming for the problem discussed.\nNow, you will shift gears a bit and study some\nof the fundamental concepts\nthat Prevail in the world of reinforcement learning\nand q-learning the first of all we'll start\nwith the Bellman equation now consider\nthe following Square rooms,\nwhich is analogous to the actual environment.\nAunt from our original problem,\nbut without the barriers now suppose a robot needs to go\nto the room marked\nin the green promise current position a using\nthe specified Direction now,\nhow can we enable the robot to do this programmatically\none idea would be introduced some kind of a footprint\nwhich the robot will be able to follow now here\na constant value is specified in each of the rooms\nwhich will come along the robots way\nif it follows the direction specified above now in this way\nif it starts at A it will be able to scan\nthrough this constant value\nand will move accordingly but this will only work\nif the direction is prefix\nand the robot always starts\nat the location a now consider the robot starts\nat this location rather than its previous one.\nNow the robot now sees Footprints\nin two different directions.\nIt is therefore unable to decide which way to go\nin order to get the destination which is the Green Room.\nIt happens primarily\nbecause the robot does not have a weight.\nRemember the directions to proceed so our job\nnow is to enable the robot with a memory.\nNow, this is where the Bellman equation comes into play.\nSo as you can see here,\nthe main reason of the Bellman equation\nis to enable the reward with the memory.\nThat's the thing we're going to use.\nSo the equation goes something like this V\nof s gives maximum a r of s comma a plus gamma of vs -\nwhere s is a particular state\nwhich is a ROM a is the Action Moving\nbetween the rooms as -\nis the state to which the robot goes from s\nand gamma is the discount Factor\nnow we'll get into it in a moment\nand obviously R of s comma a is a reward function\nwhich takes a state as an action a and outputs the reward now V\nof s is the value of being in a particular state\nwhich is the footprint\nnow we consider all the possible actions\nand take the one that yields the maximum value now,\nthere is one constraint however regarding the value Footprint,\nthat is the row marked\nin the yellow just below the Green Room.\nIt will always have the value of 1 to denote\nthat is one of the nearest room\nadjacent to the Green Room not this is also to ensure\nthat a robot gets a reward\nwhen it goes from a yellow room to The Green Room.\nLet's see how to make sense of the equation\nwhich we have here.\nSo let's assume a discount factor of 0.9\nas remember gamma is the discount value\nor the discount Factor.\nSo let's take a 0.9 now for the room,\nwhich is Just below the one\nor the yellow room, which is the Aztec Mark for this room.\nWhat will be the V of s\nthat is the value of being in a particular state?\nSo for this V of s would be something\nlike maximum of a will take 0\nwhich is the initial of our s comma.\nHey plus 0.9 which is gamma into 1\nthat gives us zero point nine now here the robot\nwill not get any reward\nfor going to a state marked in yellow.\nHence the ER s comma a is 0 here\nbut the robot knows the value of being in the yellow room.\nHence V of s Dash is one following this\nfor the other states.\nWe should get 0.9 then again,\nif we put 0.9 in this equation,\nwe get 0.81 than 0.7 to 9 and then we again reach\nthe starting point.\nSo this is\nhow the table looks with some value Footprints computed\nfrom the Bellman equation now a couple of things\nto It is here is\nthat the max function has the robot to always\nchoose the state\nthat gives it the maximum value of being in that state.\nNow the discount Factor gamma notifies the robot\nabout how far it is from the destination.\nThis is typically specified by the developer of the algorithm.\nThat would be installed in the robot.\nNow, the other states can also be given their respective values\nin a similar way.\nSo as you can see here the boxes adjacent\nto the green one have one and\nif we Move away from 1 we get 0.9 0.8 1 0 1 7 to 9\nand finally we reach 0.66.\nNow the robot now can precede its way\nthrough the Green Room utilizing these value Footprints event\nif it's dropped at any arbitrary room\nin the given location now,\nif a robot Lance up in the highlighted Sky Blue Area,\nit will still find two options to choose\nfrom but eventually either of the parts will be good enough\nfor the robot to take\nbecause Auto V the value for prints\nand only that out.\nNow one thing to note is\nthat the Bellman equation is one of the key equations\nin the world of reinforcement learning and Q learning.\nSo if we think realistically our surroundings do not always work\nin the way we expect there is always a bit\nof stochastic City involved in it.\nSo this applies to robot as well.\nSometimes it might so happen\nthat the robots Machinery got corrupted.\nSometimes the robot may come across some hindrance\non its way which it may not be known\nto it beforehand.\nRight and sometimes even if the robot knows\nthat it needs to take the right turn it will not so\nhow do we introduce this to cast a city\nin our case now here comes the Markov decision process.\nSo consider the robot is currently in the Red Room\nand it needs to go to the green room.\nNow.\nLet's now consider the robot has a slight chance\nof dysfunctioning and might take the left or the right\nor the bottom turn instead of digging the upper turn\nand are Get to the Green Room from where it is now,\nwhich is the Retro.\nNow the question is,\nhow do we enable the robot to handle this when it is out\nin the given environment right.\nNow, this is a situation\nwhere the decision making\nregarding which turn is to be taken is partly random\nand partly another control of the robot now partly random\nbecause we are not sure\nwhen exactly the robot mind dysfunctional and partly under\nthe control of the robot\nbecause it is still\nmaking a decision of taking a turn right on its own.\nAnd with the help of the program embedded into it.\nSo a Markov decision process\nis a discrete time stochastic Control process.\nIt provides a mathematical framework for modeling\ndecision-making in situations\nwhere the outcomes are partly random\nand partly under the control of the decision maker.\nNow we need to give this concept\na mathematical shape most likely an equation\nwhich then can be taken further.\nNow you might be surprised\nthat we can do this with the help of the Bellman equation.\nAction with a few minor tweaks.\nSo if we have a look at the original Bellman equation\nV of X is equal\nto maximum of our s comma a plus gamma V of s -\nwhat needs to be changed in the above equation\nso that we can introduce some amount of Randomness\nhere as long as we are not sure\nwhen the robot might not take the expected turn.\nWe are then also not sure in which room it might end up\nin which is nothing\nbut the ROM it moves\nfrom its current room at this point according.\nTo the equation.\nWe are not sure of the a stash\nwhich is the next state or the room,\nbut we do know all the probable turns the robot might take now\nin order to incorporate each\nof this probabilities into the above equation.\nWe need to associate a probability with each\nof the turns to quantify the robot.\nIf it has got any expertise chance of taking the stern know\nif we do so we get PS is equal to maximum\nof RS comma a plus gamma into summation of s -\nPS comma a comma s stash into V of his stash now the PS a--\nand a stash is the probability\nof moving from room s to establish with the action a\nand the submission here is the expectation\nof the situation.\nThat's a robot in curse,\nwhich is the randomness now, let's take a look\nat this example here.\nSo when we associate\nthe probabilities to each of these terms Owns,\nwe essentially mean that there is an 80% chance\nthat the robot will take the upper turn.\nNow, if you put all the required values\nin our equation,\nwe get V of s is equal to maximum of R of s comma a +\ncomma of 0.8 into V of room up\nplus zero point 1 into V of room down 0.03 into Rome\nof V of from left plus 0.03 into V of room right now note\nthat the value footprints.\nNot change due to the fact\nthat we are incorporating stochastically here.\nBut this time we will not calculate\nthose values Footprints instead.\nWe will let the robot to figure it out.\nNow up until this point.\nWe have not considered about rewarding the robot\nfor its action of going into a particular room.\nWe are only watering the robot\nwhen it gets to the destination now,\nideally there should be a reward for each action the robot takes\nto help it better assess the quality of the actions,\nbut the there was need not to be always be the same\nbut it is much better than having some amount\nof reward for the actions than having no rewards at all.\nRight and this idea is known as the living penalty in reality.\nThe reward system can be very complex\nand particularly modeling sparse rewards is an active area\nof research in the domain of reinforcement learning.\nSo by now we have got the equation which we have a so\nwhat we're going to do is now transition to Q learning.\nSo this equation gives us the value of going\nto a particular State taking the stochastic city\nof the environment into account.\nNow, we have also learned very briefly about the idea\nof living penalty\nwhich deals with associating each move of the robot\nwith a reward.\nSo Q learning processes\nand idea of assessing the quality of an action\nthat is taken to move to a state rather than determining\nthe possible value of the state\nwhich is being moved to so earlier.\nWe had 0.8 into V. E of s 1 0.03 into V\nof S 2 0 point 1 into V of S 3 and so on now\nif you incorporate the idea of assessing the quality\nof the action for moving to a certain state\nso the environment with the agent\nand the quality of the action will look something like this.\nSo instead of 0.8 V\nof s 1 will have q of s 1 comma a one will have q\nof S 2 comma 2 Q of S 3 now the robot now has food.\nIn states to choose from and along with that there are\nfour different actions also for the current state it is in so\nhow do we calculate Q of s comma\na that is the cumulative quality of the possible actions\nthe robot might take so let's break it down.\nNow from the equation V of s equals maximum a RS comma a +\ncomma summation s - PSAs - into V of s -\nif we discard them.\nMaximum function we have is of a plus gamma into summation p\nand v now essentially in the equation\nthat produces V of s.\nWe are considering all possible actions\nand all possible States from the current state\nthat the robot is\nin and then we are taking\nthe maximum value caused by taking a certain action\nand the equation produces a value footprint,\nwhich is for just one possible action.\nIn fact, we can think of it as the quality\nof the So Q of s comma a is equal to RS comma a +\ncomma of summation p and v now\nthat we have got an equation to quantify the quality\nof a particular action.\nWe are going to make a little adjustment\nin the equation we can now say\nthat V of s is the maximum of all the possible values\nof Q of s comma a right.\nSo let's utilize this fact\nand replace V of s Dash as a function of Q. So Q U.s.\nComma a becomes R of s comma a + comma of summation PSAs -\nand maximum of the que\nes - a -\nso the equation of V is now turned into an equation of Q,\nwhich is the quality.\nBut why would we do that now?\nThis is done to ease our calculations\nbecause now we have only one function Q\nwhich is also the core of the dynamic programming language.\nWe have only one.\nOcean Q to calculate\nand R of s comma a is a Quantified metric\nwhich produces reward of moving to a certain State.\nNow, the qualities\nof the actions are called The Q values\nand from now on we will refer to the value Footprints\nas the Q values an important piece\nof the puzzle is the temporal difference.\nNow temporal difference is the component\nthat will help the robot calculate the Q values\nwhich respect to the changes in the environment over time.\nSo consider Our robot is currently in the mark State\nand it wants to move to the Upper State.\nOne thing to note that here is\nthat the robot already knows the Q value of making the action\nthat is moving through the Upper State and we know\nthat the environment is stochastic in nature\nand the reward\nthat the robot will get after moving to the Upper State\nmight be different from an earlier observation.\nSo how do we capture this change the real difference?\nWe calculate the new q s comma a with the same formula\nand subtract the Previously known qsa from it.\nSo this will in turn give us the new QA.\nNow the equation\nthat we just derived gifts the temporal difference\nin the Q values\nwhich further helps to capture the random changes\nin the environment\nwhich may impose now the name q s comma a\nis updated as the following so Q T of s comma is equal\nto QT minus 1 s comma a\nplus Alpha D DT of a comma\ns now here Allah Alpha is the learning rate which controls\nhow quickly the robot adapts to the random changes imposed\nby the environment the qts comma is the current state q value\nand a QT minus 1 s comma is the previously recorded Q value.\nSo if we replace the TDS comma a with its full form equation,\nwe should get Q T of s comma is equal to QT -\n1 of s comma y\nplus Alpha into R of s comma a plus gamma maximum.\nQ s Dash a dash minus QT\nminus 1 s comma a now\nthat we have all the little pieces of q line together.\nLet's move forward to its implementation part.\nNow, this is the final equation of q-learning, right?\nSo, let's see\nhow we can implement this and obtain the best path\nfor any robot to take now to implement the algorithm.\nWe need to understand the warehouse location\nand how that can be mapped to different states.\nSo let's start by reconnecting the sample environment.\nSo as you can see here,\nwe have L1 L2 L3 to align and as you can see here,\nwe have certain borders also.\nSo first of all,\nlet's map each of the above locations in the warehouse\ntwo numbers or the states\nso that it will ease our calculations, right?\nSo what I'm going to do is create a new Python 3 file\nin the jupyter notebook and I'll name it as q-learning.\nNumber.\nOkay.\nSo let's define the states.\nBut before that what we need to do is import numpy\nbecause we're going to use numpy\nfor this purpose and let's initialize the parameters.\nThat is the gamma and Alpha parameters.\nSo gamma is 0.75\nwhich is the discount Factor whereas Alpha is 0.9,\nwhich is the learning rate.\nNow next what we're going to do is Define the states and map\nit to numbers.\nSo as I mentioned Earlier l 1 is 0 and Dylan line.\nWe have defined the states in the numerical form.\nNow.\nThe next step is to define the actions which is\nas mentioned above represents the transition\nto the next state.\nSo as you can see here,\nwe have an array of actions from 0 to 8.\nNow, what we're going to do is Define the reward table.\nSo as you can see, it's the same Matrix\nthat we created just now\nthat I showed you just now now if you understood it correctly,\nthere isn't any real Barrel limitation\nas depicted in the image,\nfor example, the transitional for tell one is allowed\nbut the reward will be zero to discourage\nthat path or in tough situation.\nWhat we do is add a minus 1 there\nso that it gets a negative reward.\nSo in the above code snippet as you can see here.\nI took each of the states and put once in the respective state\nthat are directly reachable from the certain State now,\nif you refer to that reward table, once again,\nwhat we created the above,\nour reconstruction will be easy to understand\nbut one thing to note here is\nthat we did not consider the top priority location L6 yet.\nWe would also need an inverse mapping\nfrom the state's back to its original location\nand it will be cleaner\nwhen we reach to the utter depths of the algorithms.\nSo for that what we're going to do Is have the inverse\nmap location State delegation.\nWe will take the distinct State and location\nand convert it back.\nNow.\nWhat we'll do is we will now Define a function get optimal\nwhich is the get optimal route,\nwhich will have a start location and an N location.\nDon't worry.\nThe code is pick\nbut I'll explain you each and every bit of the code.\nNow the get optimal route function will take\ntwo arguments the style location in the warehouse\nand the end location in the warehouse recipe lovely\nand it will return the optimal route\nfor reaching the end location\nfrom the starting location in the form of an ordered list\ncontaining the letters.\nSo we'll start by defining\nthe function by initializing the Q values to be all zeros.\nSo as you can see here,\nwe have given the Q value has to be 0 but For that\nwhat we need to do is copy the reward Matrix to a new one.\nSo this is the rewards new and next again.\nWhat we need to do is get the ending State corresponding\nto the ending location.\nAnd with this information automatically will set\nthe priority of the given ending stay to the highest one\nthat we are not defining it now,\nbut will automatically set the priority\nof the given ending State as nine nine nine.\nSo what we're going to do is initialize\nthe Q values to be 0 and in the queue learning process\nwhat you can see See here.\nWe are taking I in range 1,000 and we're going\nto pick up a state randomly.\nSo we're going to use the MP dot random r + NT\nand for traversing through the neighbor location\nin the same maze.\nWe're going to iterate through the new reward\nMatrix and get the actions\nwhich are greater than 0 and after that\nwhat we're going to do is pick an action randomly from the list\nof the playable actions\nin years to the next state\nwill going to compute the temporal difference,\nwhich is TD,\nwhich is the rewards plus gamma into The queue of next state\nand will take n p dot ARG Max\nof Q of next eight minus Q of the current state.\nWe going to then update\nthe Q values using the Bellman equation\nas you can see here,\nyou have the Bellman equation\nand we're going to update the Q values\nand after that we're going to initialize the optimal route\nwith a starting location now here we do not know\nwhat the next location yet.\nSo initialize it with the value of the starting location,\nwhich again is the random Shh now we do not know\nabout the exact number of iteration needed to reach\nto the final location.\nHence while loop will be a good choice for the iteration.\nSo when you're going to fetch the starting State fetch\nthe highest Q value penetrating\nto the starting State we go to the index\nor the next state,\nbut we need the corresponding letter.\nSo we're going to use that state to location function.\nWe just mentioned there\nand after that we're going to update the starting location\nfor the next iteration.\nFinally, we'll return the root.\nSo let's take the starting location of n line\nand and location of L1 and see what part do we actually get?\nSo as you can see here,\nwe get Airline l8l five L2 and L1.\nAnd if you have a look at the image here,\nwe have if we start from L9\nto L1 we got l8l 5 L 2 l 1 L HL 5 L2 L1.\nThat would yield us the maximum.\nMm value of the maximum reward for the robot.\nSo now we have come to the end\nof this Q learning session the past year has seen a lot\nof great examples for machine learning\nand many new high-impact\napplication of machine learning with discovered\nand brought to light especially in the healthcare Finance\nthe speech recognition augmented reality\nand much more complex 3D and video applications.\nThe natural language processing was easily\nthe most talked about domain\nwithin the community with the likes of you.\nLmf it and but being open sourced.\nSo let's have a look\nat some of the amazing machine learning projects\nwhich are open sourced the code is available for you.\nAnd those are discussed in this 2018 to nine in Spectrum.\nSo the first and the foremost is tensorflow dot DS now\nmachine learning in the browser\nor fictional thought a few years back.\nBack and a stunning reality.\nNow a lot of us in this field are welded to our favorite IDE,\nbut tells of not DOT JS has the potential to change your habits.\nIt's become a very popular released since its release\nearlier this year\nand continues to amaze with its flexibility.\nNow as a repository states,\nthere are primarily three major features\nof terms of rho dot J's develop machine learning\nand deep learning models\nin your process itself run pre-existent as\na flow models within the browser retrain our Gene\nthese prediction models as well.\nAnd if you are familiar with Kara's the high-level layers\nEPA will seem quite familiar,\nbut there are plenty of examples available on GitHub repository.\nSo do check out those legs to Quicken your learning curve.\nAnd as I mentioned earlier,\nI'll leave the links to all of these open\nsource machine learning projects in the description below.\nThe next what we not discuss is detector\non it is developed by Facebook and made a huge Splash\nwhen it was earlier launched in.\nAn 80 those developed by Facebook's AI research team,\nwhich is fa ir.\nAnd it implements the state\nof the art object detection frame was it\nis written in Python\nand as help enable multiple projects\nincluding the dance pose.\nNow, we'll know\nwhat exactly is then suppose after this example\nand this repository contains the code\nof over 70 preacher involves.\nSo it's a very good open source small guys.\nSo to check it out now the moment I talked\nabout then suppose.\nThat's the next one.\nI'm going to talk about so\nThat's supposed stents human pose estimation in the wild,\nbut the code to train and evaluate your own dance pose\nusing the our CNN model is included here\nand I've given the link to the open source code\nin the description below\nand there are notebooks available as well to visualize\ncertain Sports cocoa data set the next on our list.\nWe have D painterly harmonization.\nNow, I want you to take a moment to just admire\nthe above images.\nCan you tell which ones we're done by a human\nand which one by a machine?\nI certainly could not now here.\nThe first frame is the input image the original one\nand a third frame\nas you can see here has been generated by\nthis technique amazing, right?\nThe algorithm has an external object\nto your choosing to any image\nand manages it to make it look like nothing touched it now,\nmake sure you check out the code and try to implement it\non different sets of images yourself.\nIt is really really fun.\nBut talking about images.\nWe have image out painting now\nwhat if I give you an image and ask you to extend Its boundaries\nby imagining what it would look\nlike when the entire scene was captured.\nYou would understandably turn to some image editing software.\nBut here's the awesome news.\nYou can achieve it in few lines of code,\nwhich is the image out painting.\nNow.\nThis project is Akira's implementation of Stanford image\nout failing paper,\nwhich is incredibly cool and Illustrated paper.\nAnd this is how most research paper should be.\nI've given the links in the description\nbelow to check it out guys and see how you can.\nImplement it now.\nLet's talk about audio processing which is\nan another field\nwhere machine learning has started to make its mark.\nIt is not just limited to generate music.\nYou can do tasks like audio classification fingerprinting\nsegmentation tagging and much more and there is a lot\nthat's still yet to be explored\nand who knows perhaps you could use this project\nto Pioneer your way to the top.\nNow what if you want to discover your own planner now\nthat might perhaps be overstating things a bit,\nbut the astronaut repository will definitely get you close.\nThe Google brain team discovered\ntwo new planets in the summer 2017 by applying the astronaut.\nIt's a deep neural network meant for working\nwith astronomical data.\nIt goes to show the far-ranging application\nof machine learning and was a truly Monumental development.\nAnd now the team\nbehind the technology has open source the entire code,\nso go ahead and check out your own planet and\nwho knows you might even have a planet on your name now,\nI could not possibly let this section.\nPass by without mentioning the brt.\nThe Google AI is released has smashed record on his way\nto winning the hearts of NLP enthusiasts\nand experts alike following you.\nLmf it and he LMO brt really blew away the competition\nwith its performance.\nIt obtained a state of art result\non 11 and LP task apart from the official Google repository.\nThere is a python implementation of birth,\nwhich is worth checking out whether it makes a new era\nor not in natural language processing.\nThe thing we will soon find out now add on it.\nI'm sure you guys might have heard of it.\nIt is a framework for automatically learning\nhigh quality models without requiring programming expertise\nsince it's a Google invention.\nThe framework is based on tensorflow and you can build\nand simple models using a Danette\nand even extend it to use to train a neural network.\nNow the GitHub page contains the code and example\nthe API documentation\nand other things to get your hands dirty the trust me\nOtto ml is the next big thing.\nNG in our field now\nif you follow a few researchers on social media,\nyou must have come across some of the images.\nI am showing here\nin a video form a stick human running across the terrain\nor trying to stand up or some sort,\nbut that my friends is reinforcement learning\nand action now,\nhere's a signature example of it a framework to create\na simulated humanoid to imitate multiple motion skin.\nSo let's have a look at the top 10 skills.\nAre required to become\na successful machine learning engineer.\nSo starting with programming languages python\nis the lingua Franca of machine learning.\nYou may have had exposure to buy them.\nEven if you weren't previously\nin programming or in a computer science research field.\nHowever, it is important to have a solid understanding of glasses\nand data structures.\nSometimes python won't be enough often.\nYou'll encounter projects\nthat need to leverage hardware for Speed improvements.\nNow, make sure you are familiar with the basic algorithms as\nwell as the classes.\nMemory management and linking now\nif you want a job in machine learning,\nyou will probably have to learn all of these languages\nat some point C++ can help in speeding code up.\nWhereas our works great in statistics and plots\nand Hadoop is java-based.\nSo you probably need to implement mappers\nand reducers in Java.\nNow next we have linear algebra.\nYou need to be intimately familiar with mattresses vectors\nand matrix multiplication\nif you have an understanding of derivatives and integrals,\nYou should be in the clear.\nOtherwise even simple concept\nlike gradient descent will elude you statistic\nis going to come up a lot at least make sure you are familiar\nwith the caution distributions means standard deviation\nand much more every bit of statistical understanding\nBeyond this helps the theories help in learning\nabout algorithms great samples are naive buys\ngaussian mixture models and hidden Markov models.\nYou need to have a firm understanding of probability\nand stats to understand these these models just go nuts\nand study measure Theory\nand next we have advanced signal processing techniques.\nNow feature extraction is one of the most important parts\nof machine learning different types of problems\nneed various Solutions.\nYou may be able to utilize really cool Advanced\nsignal processing algorithms such as wavelets share.\nLet's go blades and bandless you need to learn\nabout the time-frequency analysis and try to apply it\nin your problems.\nNow, this skill will give you an edge over all\nthe other skills not this kid.\nWill give you an edge\nwhile you're applying\nfor a machine learning engine the job or others\nor next we have applied\nmaths a lot of machine learning techniques out.\nThere are just fancy types of functional approximation.\nNow these often get developed by theoretical mathematician\nand then get applied by people\nwho do not understand the theory at all.\nNow the result is\nthat many developers might have a hard time\nfinding the best techniques for the problem.\nSo even a basic understanding of numerical analysis will give\nyou a huge Edge having a firm understanding.\nEnding of algorithm Theory and knowing\nhow the algorithm works.\nYou can also discriminate models such as svm's now you will need\nto understand subjects such as\ngradient descent convex optimization LaGrange\nquadratic programming partial differentiation equations\nand much more now all this math might seem intimidating at first\nif you have been away from it for a while just\nmachine learning is much more math intensive\nthan something like front-end developer.\nJust like any other skill getting better at math is a man.\nOur Focus practice the next skill\nin our list is the neural network architectures.\nWe need machine learning for tasks that are too complex\nfor human to quote directly that is tasks\nthat are so complex\nthat it is Impractical now neural networks are a class\nof models within the general machine learning literature\nor neural networks are a specific set of algorithms\nthat have revolutionized machine learning.\nThey're inspired by biological neural networks,\nand the current so-called deep neural networks\nhave proven to work quite well.\nWell, the neural networks are themselves\nGeneral function approximations,\nwhich is why they\ncan be applied to almost any machine learning problem\nabout learning a complex mapping\nfrom the input to the output space.\nOf course, there are still good reason for the surge\nin the popularity of neural networks,\nbut neural networks have been by far the most accurate way of\napproaching many problems like translation speech recognition\nand image classification now coming to our next point\nwhich is the natural language processing now\nsince it combines computer science and Listed,\nthere are a bunch of libraries like the NLT K chances.\nMm and the techniques such as sentimental analysis\nand summarization\nthat are unique to NLP now audio and video processing\nhas a frequent overlap with the natural language processing.\nHowever, natural language processing can be applied\nto non audio data like text voice\nand audio analysis involves extracting useful information\nfrom the audio signals themselves being well-versed\nin math will get you far in this one\nand you should also be familiar.\nHer with the concepts such as the fast Fourier transforms.\nNow, these were the technical skills\nthat are required\nto become a successful machine learning engineer.\nSo next I'm going to discuss some of the non-technical skills\nor the soft skills,\nwhich are required to become a machine-learning engineer.\nSo first of all, we have the industry knowledge.\nNow the most successful machine learning projects out.\nThere are going to be those\nthat address real pain points whichever industry we\nare working for you should know\nhow that industry works\nand Will be beneficial for the business\nif a machine learning engineer does not have business Acumen\nand the know-how of the elements\nthat make up a successful business model\nor any particular algorithm.\nThen all those technical skills cannot be Channel productively,\nyou won't be able to discern the problems\nand potential challenges that need solving\nfor the business to sustain\nand grow you won't really be able to help\nyour organization explore new business opportunities.\nSo this is a must-have skill now next we\nhave effective communication.\nYou'll need to explain the machine learning\nConcepts to the people with little to no expertise\nin the field chances are you'll need to work\nwith a team of Engineers as well as many other teams.\nSo communication is going to make all of this much\nmore easier companies searching\nfor a strong machine learning engineer looking for someone\nwho can clearly\nand fluently translate their technical findings\nto a non technical team such as marketing\nor sales department and next on our list.\nWe have rapid prototyping so Iterating on ideas as quickly as\npossible is mandatory for finding one\nthat works in machine learning this applies to everything\nfrom picking up the right model\nto working on projects such as A/B Testing\nyou need to do a group\nof techniques used to quickly fabricate a scale model\nof a physical part\nor assembly using\nthe three-dimensional computer aided design,\nwhich is the cat so last\nbut not the least we have the final skill\nand that is to keep updated.\nYou must stay up to date with Any upcoming changes\nevery month new neural network models come out\nthat are performed the previous architecture.\nIt also means being aware\nof the news regarding the development of the tools\nthe changelog the conferences\nand much more you\nneed to know about the theories and algorithms.\nNow this you can achieve\nby reading the research papers blogs the conference's videos.\nAnd also you need to focus on the online community\nwith changes very quickly.\nSo expect and cultivate this change now,\nthis is not the Here we have certain skills the bonus skills,\nwhich will give you an edge over other competitors\nor the other persons\nwho are applying\nfor a machine-learning engineer position on the bonus point.\nWe have physics.\nNow, you might be in a situation\nwhere you're like to apply machine learning techniques\nto A system\nthat will interact with the real world having some knowledge\nof physics will take you far the next we\nhave reinforcement learning.\nSo this reinforcement learning has been a driver\nbehind many of the most exciting developments\nin the Deep learning\nand the AI community.\nT from the alphago zero to the open a is Dota 2 pot.\nThis will be a critical to understand\nif you want to go into robotics self-driving cars\nor other AI related areas.\nAnd finally we have\ncomputer vision out of all the disciplines out there.\nThere are by far\nthe most resources available for learning computer vision.\nThis field appears to have the lowest barriers to entry\nbut of course this\nlikely means you will face slightly more competition.\nSo having a good knowledge of computer vision\nhow it rolls will give you an edge.\nOther competitors now.\nI hope you got acquainted with all the skills\nwhich are required to become a successful\nmachine learning engineer.\nAs you know,\nwe are living in the worlds of humans and machines\nin today's world.\nThese machines are the robots have to be programmed\nbefore they start following your instructions.\nBut what if the machine started\nlearning on its own from their experience work like us\nand feel like us\nand do things more accurately than us now?\nWell his machine learning Angela comes into picture to make sure\neverything is working according to the procedures\nand the guidelines.\nSo in my opinion machine learning is one of the most\nrecent and And Technologies,\nthere is you probably use it at dozens of times every day\nwithout even knowing it.\nSo before we indulge\ninto the different roles the salary Trends\nand what should be there on the resume\nof a machine learning engineer\nwhile applying for a job.\nLet's understand\nwho exactly a machine learning engineering is so machine\nlearning Engineers are sophisticated programmers\nwho develop machines and systems\nthat can learn\nand apply knowledge without specific Direction artificial\nintelligence is the goal of a machine-learning engineer.\nThey are computer programmers\nbut their focus goes beyond specifically\nprogramming machines to perform specific tasks.\nThey create programs\nthat will enable machines to take actions\nwithout being specifically directed to perform those tasks.\nNow if we have a look at the job trends\nof machine learning in general.\nSo as you can see in Seattle itself,\nwe have 2,000 jobs in New York.\nWe have 1100 San Francisco.\nWe have 1100 in Bengaluru India,\nwe have 1100 and then we have Sunnyvale,\nCalifornia where we have If I were a number of jobs,\nso as you can see the number of jobs in the market is too much\nand probably with the emergence of machine learning\nand artificial intelligence.\nThis number is just going to get higher now.\nIf you have a look at the job opening salary-wise percentage,\nso you can see for the $90,000 per annum bracket.\nWe have 32.7 percentage and that's the maximum.\nSo be assured\nthat if you get a job as a machine-learning engineer,\nyou'll probably get around 90 thousand bucks a year.\nThat's safe to say.\nNow for the hundred and ten thousand dollars per year.\nWe have 25% $120,000.\nWe have 20 percent\nalmost then we have a hundred and thirty thousand dollars\nwhich are the senior machine learning and Jenna's\nthat's a 13 point 6 7% And finally,\nwe have the most senior machine learning engineer\nor we have the data scientist here,\nwhich have the salary\nof a hundred and forty thousand dollars per annum\nand the percentage for that one is really low.\nSo as you can see there is a great opportunity for people.\nWhat trying to go into machine learning field\nand get started with it?\nSo let's have a look at the machine learning\nin junior salary.\nSo the average salary in the u.s.\nIs around a hundred eleven thousand four hundred\nand ninety dollars\nand the average salary in India is around\nseven last nineteen thousand six hundred forty six rupees.\nThat's a very good average salary\nfor any particular profession.\nSo moving forward if we have a look\nat the salary of an entry-level machine learning.\nYou know, so the salary ranges from $76,000\nor seventy seven thousand dollars two hundred\nand fifty one thousand dollars per annum.\nThat's a huge salary.\nAnd if you talk about the bonus here,\nwe have like\nthree thousand dollars to twenty five thousand dollars depending\non the work YouTube and the project you are working on.\nLet's talk about the profit sharing now.\nSo it's around two thousand dollars\nto fifty thousand dollars.\nNow this again depends\nupon the project you are working the company you are working\nfor and the percentage\nthat Give to the in general or the developer\nfor that particular project.\nNow, the total pay comes around seventy six thousand dollars\nor seventy-five thousand dollars\ntwo hundred and sixty two thousand dollars\nand this is just for the entry level machine learning engineer.\nJust imagine if you become an experience machine\nlearning engineer your salary is going to go through the roof.\nSo now that we have understood\nwho exactly is a machine learning engineer\nthe various salary Trends the job Trends in the market\nand how it's rising.\nLet's understand.\nWhat skills it takes to become a machine learning engine.\nSo first of all,\nwe have programming languages now programming languages are\nbig deal when it comes to machine learning\nbecause you don't just need to have Proficiency\nin one language you might require Proficiency in Python.\nJava are or C++\nbecause you might be working in a Hadoop environment\nwhere you require\nJava programming to do the mapreduce Coatings\nand sometimes our is very great\nfor visualization purposes and python has you know,\nAnother favorite languages\nwhen comes to machine learning now next scale\nthat particular individual needs is calculus and statistics.\nSo a lot of machine learning algorithms are mostly\nmaths and statistics.\nSo and a lot of static\nis required majorly the matrix multiplication\nand all so good understanding\nof calculus as well as statistic is required.\nNow next we have\nsignal processing now Advanced signal processing is something\nthat will give you an upper Edge\nover other machine learning engine is\nif you are Applying for a job anywhere.\nNow the next kill we have is applied maths\nas I mentioned earlier many of the machine\nlearning algorithms here are purely mathematical formulas.\nSo a good understanding of maths and how the algorithm Works\nwill take you far ahead the next on our list.\nWe have neural networks.\nNo real networks are something\nthat has been emerging\nquite popularly in the recent years and due to its efficiency\nand the extent to which it can walk and get the results\nas soon as possible.\nNeural networks are a must\nfor machine learning engine now moving forward.\nWe have language processing.\nSo a lot of times machine learning Engineers have to deal\nwith text Data the voice data as well as video data now\nprocessing any kind of language audio\nor the video is something\nthat a machine-learning engineer has to do on a daily basis.\nSo one needs to be proficient in this area also now,\nthese are only some of the few skills\nwhich are absolutely necessary.\nI would say for any machine learning\nand Engineer so let's now discuss the job description\nor the roles\nand responsibilities\nof a particular machine learning engineer now\ndepending on their level\nof expertise machine learning Engineers may have\nto study and transform data science prototypes.\nThey need to design machine Learning Systems.\nThey also need to research and Implement appropriate machine\nlearning algorithms and tools\nas it's a very important part of the job.\nThey need to develop new machine learning application\naccording to the industry requirements the Select\nthe appropriate data sets\nand the data representation methods\nbecause if there is a slight deviation in the data set\nand the data representation\nthat's going to affect Model A lot.\nThey need to run machine learning tests and experiments.\nThey need to perform statistical analysis\nand fine-tuning using the test results.\nSo sometimes people ask\nwhat exactly is a difference between a data analyst\nand a machine learning engineer.\nSo so static analysis\njust a small part of of machine learning Engineers job.\nWhereas it is a major part\nor it probably covers a large part of a data analyst job\nrather than a machine learning Engineers job.\nSo machine learning Engineers might need to train\nand retrain the systems whenever necessary\nand they also need\nto extend the existing machine learning libraries\nand Frameworks to their full potential\nso that they could make the model Works superbly\nand finally they need to keep abreast of the developments\nin the field needless to say\nthat any machine.\nIn general or any particular individual has to stay updated\nto the technologies\nthat are coming in the market\nand every now and then a new technology arises\nwhich will overthrow the older one.\nSo you need to be up to date now coming\nto the resume part of a machine learning engineer.\nSo any resume of a particular machine learning Engineers\nshould consist like clear career objective skills,\nwhich a particular individual possesses\nthe educational qualification\ncertain certification the past experience\nif you are an experienced machine learning and Jen\nare the projects which you have worked on and that's it.\nSo let's have a look at the various elements\nthat are required\nin a machine-learning Engineers resume.\nSo first of all,\nyou need to have a clear career objective.\nSo here you will need not stretch it too much\nand keep it as precise as possible.\nSo next we have the skills required and these skills\ncan be technical as well as non technical.\nSo let's have a look\nat the various Technical and non-technical skills out here.\nSo starting with the technical skills.\nFirst of all,\nwe have programming languages as an our Java Python and C++.\nBut the first\nand the foremost requirement is to have a good grip\non any programming languages preferably python\nas it is easy to learn\nand it's applications are wider than any other language now,\nit is important to have a good understanding of topics\nlike data structures memory management and classes.\nAll the python is a very good language it\nalone cannot help you\nso you will probably have to learn all\nthese he's languages like C++ are python Java\nand also work on mapreduce\nat some point of time the next on our list.\nWe have calculus and linear algebra and statistics.\nSo you'll need to be intimately familiar\nwith matrices the vectors and the matrix multiplication.\nSo statistics is going to come up a lot\nand at least make sure you are familiar\nwith caution distribution means standard deviations\nand much more.\nSo you also need to have a firm understanding of probability.\nStats to understand the machine learning models the next\nas I mentioned earlier, it's signal processing techniques.\nSo feature extraction is one of the most important parts\nof machine learning\ndifferent types of problems need various Solutions.\nSo you may be able to utilize\nthe really cool Advanced signal processing algorithms such as\nwavelengths shallots curve.\nLet's and the ballast so try to learn about\nthe time-frequency analysis and try to apply it to your problems\nas it gives you an upper jaw.\nOur other machine learning Engineers,\nso just go for the next we\nhave mathematics and a lot of machine learning techniques out.\nThere are just fancy types of function approximation\nhaving a firm understanding of algorithm Theory and knowing\nhow the algorithm works is really necessary\nand understanding subjects\nlike gradient descent convex optimization\nquadratic programming\nand partial differentiation will help a lot the neural networks\nas I was talking earlier.\nSo we need machine learning for tasks that are too Flex\nfor humans to quote directly.\nSo that is the tasks that are so complex\nthat it is Impractical neural networks are a class\nof models within the general machine learning literature.\nThey are specific set of algorithms\nthat have revolutionized machine\nlearning deep neural networks have proven to work quite well\nand neural networks\nare themself General function approximations,\nwhich is why they can be applied\nto almost any machine learning problem out there\nand they help a lot about learning a complex mapping\nfrom the input\nto The output space now next we have language processing\nsince natural language processing combines two\nof the major areas of work\nthat are linguistic and computer science\nand chances are at some point you are going to work\nwith either text or audio or the video.\nSo it's necessary to have a control over libraries\nlike gents mm and ltk\nand techniques like what to wet sentimental analysis\nand text summarization Now voice\nand audio analysis involves extracting useful information\nfrom the Your signals themselves very well versed in maths\nand concept like Fourier transformation will get\nyou far in this one.\nThese were the technical skills that are required but be assured\nthat there are a lot of non technical skills.\nAlso that are required to land a good job\nin a machine learning industry.\nSo first of all,\nyou need to have an industry knowledge.\nSo the most successful machine learning projects out.\nThere are going to be those that address real pain points,\ndon't you agree?\nSo whichever industry are working for You should know\nhow that industry works\nand what will be beneficial for the industry.\nNow, if a machine learning engineer\ndoes not have business Acumen and the know-how of the elements\nthat make up a successful business model.\nAll those technical skills cannot be\nchanneled productively.\nYou won't be able to discern the problems\nand the potential challenges that need solving\nfor the business to sustain and grow the next on our list.\nWe have effective communication and not this is one\nof the most important parts in any job requirements.\nSo you'll need to In machine learning Concepts to people\nwith little to no expertise in the field a chances are\nyou will need to work with a team of Engineers\nas well as many other teams like marketing\nand the sales team.\nSo communication is going to make all of this much\neasier companies searching\nfor the strong machine learning engineer looking for someone\nwho can clearly and fluency translate technical findings\nto a non technical team.\nRapid prototyping is another skill,\nwhich is very much required for any machine learning engineer.\nSo iterating on ideas as quickly as possible is mandatory\nfor finding the one\nthat works in machine learning this applies to everything\nfrom picking the right model\nto working on projects such as a/b testing\nand much more now you need to do a group\nof techniques used to quickly fabricate a scale model\nof a physical part\nor assembly using\nthe three-dimensional computer aided design,\nwhich is the cat data now coming to the final skills,\nwhich will be required\nfor any machine learning agenda is to keep updated.\nSo you must stay up to date with any upcoming changes\nevery month new neural network models come out\nthat outperformed the previous architecture.\nIt also means being aware of the news regarding the development\nof the tools Theory\nand algorithms through research papers blocks conference videos\nand much more.\nNow another part of any machine learning engineer's resume is\nthe education qualification.\nSo a bachelor's\nor master's degree in computer science RIT economics statistics\nor even mathematics can help.\nUp you land a job in machine learning plus\nif you are an experienced machine learning engineer,\nso probably some standard company certifications\nwill help you a lot\nwhen Landing a good job in machine learning\nand finally coming to the professional experience.\nYou need to have experience in computer science statistics data\nas is if you are switching\nfrom any other profession into a machine learning engineer,\nor if you have a previous experience in machine learning\nthat is very well.\nNow finally if we talk about The projects\nso you need to have not just any project\nthat you have worked on you need to have working on machine\nlearning related projects\nthat involve a certain level of AI\nand working on neural networks to a certain degree\nto land a good job as a machine-learning engineer.\nNow if you have a look at the company's hiring machine\nlearning Engineers,\nso every other company is looking\nfor machine learning Engineers\nwho can modify the existing model to something\nthat did not need much more.\nOf Maintenance and cancel sustain so basically working\non artificial intelligence and new algorithms\nthat can work on their own is what every company deserves.\nSo Amazon Facebook.\nWe have Tech giants\nlike Microsoft IBM again in the gaming industry,\nwe have or the GPU industry Graphics industry.\nWe have Nvidia in banking industry.\nWe have JPMorgan Chase again,\nwe have LinkedIn and also we have Walmart.\nSo all of these companies require machine learning engine\nat some part of the time.\nSo be assured that\nif you are looking for a machine learning engineer post,\nevery other companies be it a big shot company or even\nthe new startups are looking for machine learning Engineers.\nSo be assured you will get a job now with this we come\nto an end of this video.\nSo I hope you've got a good understanding\nof who exactly are machine learning engineer is\nthe way just job Trends the salary Trends.\nWhat are the skills required to become machine learning engineer\nand once you become a machine-learning engineer,\nwhat are the roles and responsibilities\nor the Job description\nwhat appears to be on the resume or the job description\nwhat appears to be\non the job application of any machine learning engineers?\nAnd also I hope you got to know how to prepare your resume\nor how to prepare it in the correct format.\nAnd what on to keep their\nin the resume the career objectives the skills\nTechnical and non-technical\nprevious experience education qualification\nand certain projects\nwhich are related to it.\nSo that's it guys Ed Rica\nas you know provides a machine learning.\nEngineer master's program now that is aligned in such a way\nthat will get you acquainted in all the skills\nthat are required to become a machine\nlearning engine and that too in the correct form."
  }
]
