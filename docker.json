[
  {
    "id": 258994967,
    "timestamp": "2026-02-07T06:29:19.440Z",
    "title": "Get Docker | Docker Docs",
    "url": "https://docs.docker.com/get-started/get-docker/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nGet Docker\nGet Docker\nCopy as Markdown\n\nDocker is an open platform for developing, shipping, and running applications.\n\nDocker allows you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications.\n\nBy taking advantage of Docker‚Äôs methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.\n\nYou can download and install Docker on multiple platforms. Refer to the following section and choose the best installation path for you.\n\nDocker Desktop terms\n\nCommercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.\n\nDocker Desktop for Mac\n\nA native application using the macOS sandbox security model that delivers all Docker tools to your Mac.\n\nDocker Desktop for Windows\n\nA native Windows application that delivers all Docker tools to your Windows computer.\n\nDocker Desktop for Linux\n\nA native Linux application that delivers all Docker tools to your Linux computer.\n\nNote\n\nIf you're looking for information on how to install Docker Engine, see Docker Engine installation overview.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994970,
    "timestamp": "2026-02-07T06:29:19.446Z",
    "title": "What is Docker? | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-overview/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nWhat is Docker?\nWhat is Docker?\nCopy as Markdown\n\nDocker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker's methodologies for shipping, testing, and deploying code, you can significantly reduce the delay between writing code and running it in production.\n\nThe Docker platform\n\nDocker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security lets you run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you don't need to rely on what's installed on the host. You can share containers while you work, and be sure that everyone you share with gets the same container that works in the same way.\n\nDocker provides tooling and a platform to manage the lifecycle of your containers:\n\nDevelop your application and its supporting components using containers.\nThe container becomes the unit for distributing and testing your application.\nWhen you're ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two.\nWhat can I use Docker for?\nFast, consistent delivery of your applications\n\nDocker streamlines the development lifecycle by allowing developers to work in standardized environments using local containers which provide your applications and services. Containers are great for continuous integration and continuous delivery (CI/CD) workflows.\n\nConsider the following example scenario:\n\nYour developers write code locally and share their work with their colleagues using Docker containers.\nThey use Docker to push their applications into a test environment and run automated and manual tests.\nWhen developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation.\nWhen testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment.\nResponsive deployment and scaling\n\nDocker's container-based platform allows for highly portable workloads. Docker containers can run on a developer's local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments.\n\nDocker's portability and lightweight nature also make it easy to dynamically manage workloads, scaling up or tearing down applications and services as business needs dictate, in near real time.\n\nRunning more workloads on the same hardware\n\nDocker is lightweight and fast. It provides a viable, cost-effective alternative to hypervisor-based virtual machines, so you can use more of your server capacity to achieve your business goals. Docker is perfect for high density environments and for small and medium deployments where you need to do more with fewer resources.\n\nDocker architecture\n\nDocker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers.\n\nThe Docker daemon\n\nThe Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.\n\nThe Docker client\n\nThe Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.\n\nDocker Desktop\n\nDocker Desktop is an easy-to-install application for your Mac, Windows, or Linux environment that enables you to build and share containerized applications and microservices. Docker Desktop includes the Docker daemon (dockerd), the Docker client (docker), Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see Docker Desktop.\n\nDocker registries\n\nA Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker looks for images on Docker Hub by default. You can even run your own private registry.\n\nWhen you use the docker pull or docker run commands, Docker pulls the required images from your configured registry. When you use the docker push command, Docker pushes your image to your configured registry.\n\nDocker objects\n\nWhen you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.\n\nImages\n\nAn image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image that is based on the Ubuntu image but includes the Apache web server and your application, as well as the configuration details needed to make your application run.\n\nYou might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.\n\nContainers\n\nA container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.\n\nBy default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container's network, storage, or other underlying subsystems are from other containers or from the host machine.\n\nA container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that aren't stored in persistent storage disappear.\n\nExample docker run command\n\nThe following command runs an ubuntu container, attaches interactively to your local command-line session, and runs /bin/bash.\n\n$ docker run -i -t ubuntu /bin/bash\n\n\nWhen you run this command, the following happens (assuming you are using the default registry configuration):\n\nIf you don't have the ubuntu image locally, Docker pulls it from your configured registry, as though you had run docker pull ubuntu manually.\n\nDocker creates a new container, as though you had run a docker container create command manually.\n\nDocker allocates a read-write filesystem to the container, as its final layer. This allows a running container to create or modify files and directories in its local filesystem.\n\nDocker creates a network interface to connect the container to the default network, since you didn't specify any networking options. This includes assigning an IP address to the container. By default, containers can connect to external networks using the host machine's network connection.\n\nDocker starts the container and executes /bin/bash. Because the container is running interactively and attached to your terminal (due to the -i and -t flags), you can provide input using your keyboard while Docker logs the output to your terminal.\n\nWhen you run exit to terminate the /bin/bash command, the container stops but isn't removed. You can start it again or remove it.\n\nThe underlying technology\n\nDocker is written in the Go programming language and takes advantage of several features of the Linux kernel to deliver its functionality. Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, Docker creates a set of namespaces for that container.\n\nThese namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace.\n\nNext steps\nInstall Docker\nGet started with Docker\n\nEdit this page\n\nRequest changes\n\nTable of contents\nThe Docker platform\nWhat can I use Docker for?\nFast, consistent delivery of your applications\nResponsive deployment and scaling\nRunning more workloads on the same hardware\nDocker architecture\nThe Docker daemon\nThe Docker client\nDocker Desktop\nDocker registries\nDocker objects\nThe underlying technology\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994973,
    "timestamp": "2026-02-07T06:29:19.448Z",
    "title": "Introduction | Docker Docs",
    "url": "https://docs.docker.com/get-started/introduction/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nGet Docker Desktop\nDevelop with containers\nBuild and push your first image\nWhat's next\nDocker concepts\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nIntroduction\nIntroduction\nEmbark on a comprehensive learning path to understand Docker and containerization, beginning with foundational concepts and installation procedures. Progress through hands-on exercises that cover essential Docker commands, image creation, and container orchestration.\nSkill level\nBeginner\nTime to complete\n15 minutes\nPrerequisites\nNone\nAbout this series\n\nIn this guide series, you will gain hands-on experience with Docker, starting with installing and setting up Docker Desktop on your local machine. You will learn how to run your first container, understanding the basics of containerization and its benefits. This series guides you through building your first Docker image, providing insights into creating efficient and reusable images. Finally, you will explore how to publish your image on Docker Hub, enabling you to share your work with the broader community and leverage Docker's powerful ecosystem for collaborative development and deployment.\n\nWhat you'll learn\nSet up Docker Desktop\nRun your first container\nBuild your first image\nPublish your image on Docker Hub\nModules\n1. Get Docker Desktop\n\nGetting Docker Desktop up and running is the first crucial step for developers diving into containerization, offering a seamless and user-friendly interface for managing Docker containers. Docker Desktop simplifies the process of building, sharing, and running applications in containers, ensuring consistency across different environments.\n\nStart\n2. Develop with containers\n3. Build and push your first image\n4. What's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994976,
    "timestamp": "2026-02-07T06:29:19.464Z",
    "title": "Get Docker Desktop | Docker Docs",
    "url": "https://docs.docker.com/get-started/introduction/get-docker-desktop/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nGet Docker Desktop\nDevelop with containers\nBuild and push your first image\nWhat's next\nDocker concepts\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nIntroduction\n/\nGet Docker Desktop\nGet Docker Desktop\nCopy as Markdown\nExplanation\n\nDocker Desktop is the all-in-one package to build images, run containers, and so much more. This guide will walk you through the installation process, enabling you to experience Docker Desktop firsthand.\n\nDocker Desktop terms\n\nCommercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.\n\nDocker Desktop for Mac\n\nDownload (Apple Silicon) | Download (Intel) | Install instructions\n\nDocker Desktop for Windows\n\nDownload | Install instructions\n\nDocker Desktop for Linux\n\nInstall instructions\n\nOnce it's installed, complete the setup process and you're all set to run a Docker container.\n\nTry it out\n\nIn this hands-on guide, you will see how to run a Docker container using Docker Desktop.\n\nFollow the instructions to run a container using the CLI.\n\nRun your first container\n\nOpen your CLI terminal and start a container by running the docker run command:\n\n$ docker run -d -p 8080:80 docker/welcome-to-docker\n\nAccess the frontend\n\nFor this container, the frontend is accessible on port 8080. To open the website, visit http://localhost:8080 in your browser.\n\nManage containers using Docker Desktop\n\nOpen Docker Desktop and select the Containers field on the left sidebar.\n\nYou can view information about your container including logs, and files, and even access the shell by selecting the Exec tab.\n\nSelect the Inspect field to obtain detailed information about the container. You can perform various actions such as pause, resume, start or stop containers, or explore the Logs, Bind mounts, Exec, Files, and Stats tabs.\n\nDocker Desktop simplifies container management for developers by streamlining the setup, configuration, and compatibility of applications across different environments, thereby addressing the pain points of environment inconsistencies and deployment challenges.\n\nWhat's next?\n\nNow that you have Docker Desktop installed and ran your first container, it's time to start developing with containers.\n\nDevelop with containers\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nTry it out\nRun your first container\nAccess the frontend\nManage containers using Docker Desktop\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994979,
    "timestamp": "2026-02-07T06:29:19.471Z",
    "title": "Develop with containers | Docker Docs",
    "url": "https://docs.docker.com/get-started/introduction/develop-with-containers/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nGet Docker Desktop\nDevelop with containers\nBuild and push your first image\nWhat's next\nDocker concepts\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nIntroduction\n/\nDevelop with containers\nDevelop with containers\nCopy as Markdown\nExplanation\n\nNow that you have Docker Desktop installed, you are ready to do some application development. Specifically, you will do the following:\n\nClone and start a development project\nMake changes to the backend and frontend\nSee the changes immediately\nTry it out\n\nIn this hands-on guide, you'll learn how to develop with containers.\n\nStart the project\n\nTo get started, either clone or download the project as a ZIP file to your local machine.\n\n$ git clone https://github.com/docker/getting-started-todo-app\n\n\nAnd after the project is cloned, navigate into the new directory created by the clone:\n\n$ cd getting-started-todo-app\n\n\nOnce you have the project, start the development environment using Docker Compose.\n\nTo start the project using the CLI, run the following command:\n\n$ docker compose watch\n\n\nYou will see an output that shows container images being pulled down, containers starting, and more. Don't worry if you don't understand it all at this point. But, within a moment or two, things should stabilize and finish.\n\nOpen your browser to http://localhost to see the application up and running. It may take a few minutes for the app to run. The app is a simple to-do application, so feel free to add an item or two, mark some as done, or even delete an item.\n\nWhat's in the environment?\n\nNow that the environment is up and running, what's actually in it? At a high-level, there are several containers (or processes) that each serve a specific need for the application:\n\nReact frontend - a Node container that's running the React dev server, using Vite.\nNode backend - the backend provides an API that provides the ability to retrieve, create, and delete to-do items.\nMySQL database - a database to store the list of the items.\nphpMyAdmin - a web-based interface to interact with the database that is accessible at http://db.localhost.\nTraefik proxy - Traefik is an application proxy that routes requests to the right service. It sends all requests for localhost/api/* to the backend, requests for localhost/* to the frontend, and then requests for db.localhost to phpMyAdmin. This provides the ability to access all applications using port 80 (instead of different ports for each service).\n\nWith this environment, you as the developer don‚Äôt need to install or configure any services, populate a database schema, configure database credentials, or anything. You only need Docker Desktop. The rest just works.\n\nMake changes to the app\n\nWith this environment up and running, you‚Äôre ready to make a few changes to the application and see how Docker helps provide a fast feedback loop.\n\nChange the greeting\n\nThe greeting at the top of the page is populated by an API call at /api/greeting. Currently, it always returns \"Hello world!\". You‚Äôll now modify it to return one of three randomized messages (that you'll get to choose).\n\nOpen the backend/src/routes/getGreeting.js file in a text editor. This file provides the handler for the API endpoint.\n\nModify the variable at the top to an array of greetings. Feel free to use the following modifications or customize it to your own liking. Also, update the endpoint to send a random greeting from this list.\n\n 1\n\n 2\n\n 3\n\n 4\n\n 5\n\n 6\n 7\n 8\n\n 9\n\n10\n11\n\n\t\nconst GREETINGS = [\n\n    \"Whalecome!\",\n\n    \"All hands on deck!\",\n\n    \"Charting the course ahead!\",\n\n];\n\n\n\nmodule.exports = async (req, res) => {\n\n    res.send({\n\n        greeting: GREETINGS[ Math.floor( Math.random() * GREETINGS.length )],\n\n    });\n\n};\n\nIf you haven't done so yet, save the file. If you refresh your browser, you should see a new greeting. If you keep refreshing, you should see all of the messages appear.\n\nChange the placeholder text\n\nWhen you look at the app, you'll see the placeholder text is simply \"New Item\". You‚Äôll now make that a little more descriptive and fun. You‚Äôll also make a few changes to the styling of the app too.\n\nOpen the client/src/components/AddNewItemForm.jsx file. This provides the component to add a new item to the to-do list.\n\nModify the placeholder attribute of the Form.Control element to whatever you'd like to display.\n\n33\n34\n35\n36\n\n37\n\n38\n39\n\n\t\n<Form.Control\n\n    value={newItem}\n\n    onChange={(e) => setNewItem(e.target.value)}\n\n    type=\"text\"\n\n    placeholder=\"What do you need to do?\"\n\n    aria-label=\"New item\"\n\n/>\n\nSave the file and go back to your browser. You should see the change already hot-reloaded into your browser. If you don't like it, feel free to tweak it until it looks just right.\n\nChange the background color\n\nBefore you consider the application finalized, you need to make the colors better.\n\nOpen the client/src/index.scss file.\n\nAdjust the background-color attribute to any color you'd like. The provided snippet is a soft blue to go along with Docker's nautical theme.\n\nIf you're using an IDE, you can pick a color using the integrated color pickers. Otherwise, feel free to use an online Color Picker.\n\n3\n\n4\n\n5\n6\n7\n\n\t\nbody {\n\n    background-color: #99bbff;\n\n    margin-top: 50px;\n\n    font-family: 'Lato';\n\n}\n\nEach save should let you see the change immediately in the browser. Keep adjusting it until it's the perfect setup for you.\n\nAnd with that, you're done. Congrats on updating your website.\n\nRecap\n\nBefore you move on, take a moment and reflect on what happened here. Within a few moments, you were able to:\n\nStart a complete development project with zero installation effort. The containerized environment provided the development environment, ensuring you have everything you need. You didn't have to install Node, MySQL, or any of the other dependencies directly on your machine. All you needed was Docker Desktop and a code editor.\n\nMake changes and see them immediately. This was made possible because 1) the processes running in each container are watching and responding to file changes and 2) the files are shared with the containerized environment.\n\nDocker Desktop enables all of this and so much more. Once you start thinking with containers, you can create almost any environment and easily share it with your team.\n\nNext steps\n\nNow that the application has been updated, you‚Äôre ready to learn about packaging it as a container image and pushing it to a registry, specifically Docker Hub.\n\nBuild and push your first image\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nTry it out\nStart the project\nWhat's in the environment?\nMake changes to the app\nChange the greeting\nChange the placeholder text\nChange the background color\nRecap\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994982,
    "timestamp": "2026-02-07T06:29:19.477Z",
    "title": "Build and push your first image | Docker Docs",
    "url": "https://docs.docker.com/get-started/introduction/build-and-push-first-image/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nGet Docker Desktop\nDevelop with containers\nBuild and push your first image\nWhat's next\nDocker concepts\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nIntroduction\n/\nBuild and push your first image\nBuild and push your first image\nCopy as Markdown\nExplanation\n\nNow that you've updated the to-do list app, you‚Äôre ready to create a container image for the application and share it on Docker Hub. To do so, you will need to do the following:\n\nSign in with your Docker account\nCreate an image repository on Docker Hub\nBuild the container image\nPush the image to Docker Hub\n\nBefore you dive into the hands-on guide, the following are a few core concepts that you should be aware of.\n\nContainer images\n\nIf you‚Äôre new to container images, think of them as a standardized package that contains everything needed to run an application, including its files, configuration, and dependencies. These packages can then be distributed and shared with others.\n\nDocker Hub\n\nTo share your Docker images, you need a place to store them. This is where registries come in. While there are many registries, Docker Hub is the default and go-to registry for images. Docker Hub provides both a place for you to store your own images and to find images from others to either run or use as the bases for your own images.\n\nWhen choosing base images, Docker Hub offers two categories of trusted, Docker-maintained images:\n\nDocker Official Images (DOI) ‚Äì Curated images for popular software, following best practices and regularly updated.\nDocker Hardened Images (DHI) ‚Äì Minimal, secure, production-ready images with near-zero CVEs, designed to reduce attack surface and simplify compliance. DHI images are free and open source under Apache 2.0.\n\nIn Develop with containers, you used the following images that came from Docker Hub, each of which are Docker Official Images:\n\nnode - provides a Node environment and is used as the base of your development efforts. This image is also used as the base for the final application image.\nmysql - provides a MySQL database to store the to-do list items\nphpmyadmin - provides phpMyAdmin, a web-based interface to the MySQL database\ntraefik - provides Traefik, a modern HTTP reverse proxy and load balancer that routes requests to the appropriate container based on routing rules\n\nExplore the full catalog of trusted content on Docker Hub:\n\nDocker Official Images ‚Äì Curated images for popular software\nDocker Hardened Images ‚Äì Security-hardened, minimal production images (also available at dhi.io)\nDocker Verified Publishers ‚Äì Images from verified software vendors\nDocker Sponsored Open Source ‚Äì Images from sponsored OSS projects\nTry it out\n\nIn this hands-on guide, you'll learn how to sign in to Docker Hub and push images to Docker Hub repository.\n\nSign in with your Docker account\n\nTo push images to Docker Hub, you will need to sign in with a Docker account.\n\nOpen the Docker Dashboard.\n\nSelect Sign in at the top-right corner.\n\nIf needed, create an account and then complete the sign-in flow.\n\nOnce you're done, you should see the Sign in button turn into a profile picture.\n\nCreate an image repository\n\nNow that you have an account, you can create an image repository. Just as a Git repository holds source code, an image repository stores container images.\n\nGo to Docker Hub.\n\nSelect Create repository.\n\nOn the Create repository page, enter the following information:\n\nRepository name - getting-started-todo-app\nShort description - feel free to enter a description if you'd like\nVisibility - select Public to allow others to pull your customized to-do app\n\nSelect Create to create the repository.\n\nBuild and push the image\n\nNow that you have a repository, you are ready to build and push your image. An important note is that the image you are building extends the Node image, meaning you don't need to install or configure Node, yarn, etc. You can simply focus on what makes your application unique.\n\nWhat is an image/Dockerfile?\n\nWithout going too deep yet, think of a container image as a single package that contains everything needed to run a process. In this case, it will contain a Node environment, the backend code, and the compiled React code.\n\nAny machine that runs a container using the image, will then be able to run the application as it was built without needing anything else pre-installed on the machine.\n\nA Dockerfile is a text-based script that provides the instruction set on how to build the image. For this quick start, the repository already contains the Dockerfile.\n\nCLI VS Code\n\nTo get started, either clone or download the project as a ZIP file to your local machine.\n\n$ git clone https://github.com/docker/getting-started-todo-app\n\n\nAnd after the project is cloned, navigate into the new directory created by the clone:\n\n$ cd getting-started-todo-app\n\n\nBuild the project by running the following command, swapping out DOCKER_USERNAME with your username.\n\n$ docker build -t DOCKER_USERNAME/getting-started-todo-app .\n\n\nFor example, if your Docker username was mobydock, you would run the following:\n\n$ docker build -t mobydock/getting-started-todo-app .\n\n\nTo verify the image exists locally, you can use the docker image ls command:\n\n$ docker image ls\n\n\nYou will see output similar to the following:\n\nREPOSITORY                          TAG       IMAGE ID       CREATED          SIZE\n\nmobydock/getting-started-todo-app   latest    1543656c9290   2 minutes ago    1.12GB\n\n...\n\n\nTo push the image, use the docker push command. Be sure to replace DOCKER_USERNAME with your username:\n\n$ docker push DOCKER_USERNAME/getting-started-todo-app\n\n\nDepending on your upload speeds, this may take a moment to push.\n\nRecap\n\nBefore you move on, take a moment and reflect on what happened here. Within a few moments, you were able to build a container image that packages your application and push it to Docker Hub.\n\nGoing forward, you‚Äôll want to remember that:\n\nDocker Hub is the go-to registry for finding trusted content. Docker provides a collection of trusted content, composed of Docker Official Images, Docker Verified Publishers, and Docker Sponsored Open Source Software, to use directly or as bases for your own images.\n\nDocker Hub provides a marketplace to distribute your own applications. Anyone can create an account and distribute images. While you are publicly distributing the image you created, private repositories can ensure your images are accessible to only authorized users.\n\nUsage of other registries\n\nWhile Docker Hub is the default registry, registries are standardized and made interoperable through the Open Container Initiative. This allows companies and organizations to run their own private registries. Quite often, trusted content is mirrored (or copied) from Docker Hub into these private registries.\n\nNext steps\n\nNow that you‚Äôve built an image, it's time to discuss why you as a developer should learn more about Docker and how it will help you in your day-to-day tasks.\n\nWhat's Next\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nContainer images\nDocker Hub\nTry it out\nSign in with your Docker account\nCreate an image repository\nBuild and push the image\nRecap\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994985,
    "timestamp": "2026-02-07T06:29:19.484Z",
    "title": "What's next | Docker Docs",
    "url": "https://docs.docker.com/get-started/introduction/whats-next/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nGet Docker Desktop\nDevelop with containers\nBuild and push your first image\nWhat's next\nDocker concepts\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nIntroduction\n/\nWhat's next\nWhat's next\nCopy as Markdown\n\nThe following sections provide step-by-step guides to help you understand core Docker concepts, building images, and running containers.\n\nThe basics\n\nGet started learning the core concepts of containers, images, registries, and Docker Compose.\n\nWhat is a container?\n\nLearn how to run your first container.\n\nWhat is an image?\n\nLearn the basics of image layers.\n\nWhat is a registry?\n\nLearn about container registries, explore their interoperability, and interact with registries.\n\nWhat is Docker Compose?\n\nGain a better understanding of Docker Compose.\n\nBuilding images\n\nCraft optimized container images with Dockerfiles, build cache, and multi-stage builds.\n\nUnderstanding image layers\n\nLearn about the layers of container images.\n\nWriting a Dockerfile\n\nLearn how to create an image using a Dockerfile.\n\nBuild, tag and publish an image\n\nLearn how to build, tag, and publish an image to Docker Hub or any other registry.\n\nUsing the build cache\n\nLearn about the build cache, what changes invalidate the cache, and how to effectively use the build cache.\n\nMulti-stage builds\n\nGet a better understanding of multi-stage builds and their benefits.\n\nRunning containers\n\nMaster essential techniques for exposing ports, overriding defaults, persisting data, sharing files, and managing multi-container applications.\n\nPublishing ports\n\nUnderstand the significance of publishing and exposing ports in Docker.\n\nOverriding container defaults\n\nLearn how to override the container defaults using the docker run command.\n\nPersisting container data\n\nLearn the significance of data persistence in Docker.\n\nSharing local files with containers\n\nExplore the various storage options available in Docker and their common usage.\n\nMulti-container applications\n\nLearn the significance of multi-container applications and how they're different from single-container applications.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994988,
    "timestamp": "2026-02-07T06:29:19.494Z",
    "title": "What is a container? | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-container/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nWhat is a container?\nWhat is an image?\nWhat is a registry?\nWhat is Docker Compose?\nBuilding images\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nThe basics\n/\nWhat is a container?\nWhat is a container?\nCopy as Markdown\nExplanation\n\nImagine you're developing a killer web app that has three main components - a React frontend, a Python API, and a PostgreSQL database. If you wanted to work on this project, you'd have to install Node, Python, and PostgreSQL.\n\nHow do you make sure you have the same versions as the other developers on your team? Or your CI/CD system? Or what's used in production?\n\nHow do you ensure the version of Python (or Node or the database) your app needs isn't affected by what's already on your machine? How do you manage potential conflicts?\n\nEnter containers!\n\nWhat is a container? Simply put, containers are isolated processes for each of your app's components. Each component - the frontend React app, the Python API engine, and the database - runs in its own isolated environment, completely isolated from everything else on your machine.\n\nHere's what makes them awesome. Containers are:\n\nSelf-contained. Each container has everything it needs to function with no reliance on any pre-installed dependencies on the host machine.\nIsolated. Since containers run in isolation, they have minimal influence on the host and other containers, increasing the security of your applications.\nIndependent. Each container is independently managed. Deleting one container won't affect any others.\nPortable. Containers can run anywhere! The container that runs on your development machine will work the same way in a data center or anywhere in the cloud!\nContainers versus virtual machines (VMs)\n\nWithout getting too deep, a VM is an entire operating system with its own kernel, hardware drivers, programs, and applications. Spinning up a VM only to isolate a single application is a lot of overhead.\n\nA container is simply an isolated process with all of the files it needs to run. If you run multiple containers, they all share the same kernel, allowing you to run more applications on less infrastructure.\n\nUsing VMs and containers together\n\nQuite often, you will see containers and VMs used together. As an example, in a cloud environment, the provisioned machines are typically VMs. However, instead of provisioning one machine to run one application, a VM with a container runtime can run multiple containerized applications, increasing resource utilization and reducing costs.\n\nTry it out\n\nIn this hands-on, you will see how to run a Docker container using the Docker Desktop GUI.\n\nUsing the GUI Using the CLI\n\nUse the following instructions to run a container.\n\nOpen Docker Desktop and select the Search field on the top navigation bar.\n\nSpecify welcome-to-docker in the search input and then select the Pull button.\n\nOnce the image is successfully pulled, select the Run button.\n\nExpand the Optional settings.\n\nIn the Container name, specify welcome-to-docker.\n\nIn the Host port, specify 8080.\n\nSelect Run to start your container.\n\nCongratulations! You just ran your first container! üéâ\n\nView your container\n\nYou can view all of your containers by going to the Containers view of the Docker Desktop Dashboard.\n\nThis container runs a web server that displays a simple website. When working with more complex projects, you'll run different parts in different containers. For example, you might run a different container for the frontend, backend, and database.\n\nAccess the frontend\n\nWhen you launched the container, you exposed one of the container's ports onto your machine. Think of this as creating configuration to let you to connect through the isolated environment of the container.\n\nFor this container, the frontend is accessible on port 8080. To open the website, select the link in the Port(s) column of your container or visit http://localhost:8080 in your browser.\n\nExplore your container\n\nDocker Desktop lets you explore and interact with different aspects of your container. Try it out yourself.\n\nGo to the Containers view in the Docker Desktop Dashboard.\n\nSelect your container.\n\nSelect the Files tab to explore your container's isolated file system.\n\nStop your container\n\nThe docker/welcome-to-docker container continues to run until you stop it.\n\nGo to the Containers view in the Docker Desktop Dashboard.\n\nLocate the container you'd like to stop.\n\nSelect the Stop action in the Actions column.\n\nAdditional resources\n\nThe following links provide additional guidance into containers:\n\nRunning a container\nOverview of container\nWhy Docker?\nNext steps\n\nNow that you have learned the basics of a Docker container, it's time to learn about Docker images.\n\nWhat is an image?\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nContainers versus virtual machines (VMs)\nTry it out\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994991,
    "timestamp": "2026-02-07T06:29:19.499Z",
    "title": "What is an image? | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-an-image/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nWhat is a container?\nWhat is an image?\nWhat is a registry?\nWhat is Docker Compose?\nBuilding images\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nThe basics\n/\nWhat is an image?\nWhat is an image?\nCopy as Markdown\nExplanation\n\nSeeing as a container is an isolated process, where does it get its files and configuration? How do you share those environments?\n\nThat's where container images come in. A container image is a standardized package that includes all of the files, binaries, libraries, and configurations to run a container.\n\nFor a PostgreSQL image, that image will package the database binaries, config files, and other dependencies. For a Python web app, it'll include the Python runtime, your app code, and all of its dependencies.\n\nThere are two important principles of images:\n\nImages are immutable. Once an image is created, it can't be modified. You can only make a new image or add changes on top of it.\n\nContainer images are composed of layers. Each layer represents a set of file system changes that add, remove, or modify files.\n\nThese two principles let you to extend or add to existing images. For example, if you are building a Python app, you can start from the Python image and add additional layers to install your app's dependencies and add your code. This lets you focus on your app, rather than Python itself.\n\nFinding images\n\nDocker Hub is the default global marketplace for storing and distributing images. It has over 100,000 images created by developers that you can run locally. You can search for Docker Hub images and run them directly from Docker Desktop.\n\nDocker Hub provides a variety of Docker-supported and endorsed images known as Docker Trusted Content. These provide fully managed services or great starters for your own images. These include:\n\nDocker Official Images - a curated set of Docker repositories, serve as the starting point for the majority of users, and are some of the most secure on Docker Hub\nDocker Hardened Images - minimal, secure, production-ready images with near-zero CVEs, designed to reduce attack surface and simplify compliance. Free and open source under Apache 2.0\nDocker Verified Publishers - high-quality images from commercial publishers verified by Docker\nDocker-Sponsored Open Source - images published and maintained by open-source projects sponsored by Docker through Docker's open source program\n\nFor example, Redis and Memcached are a few popular ready-to-go Docker Official Images. You can download these images and have these services up and running in a matter of seconds. There are also base images, like the Node.js Docker image, that you can use as a starting point and add your own files and configurations. For production workloads requiring enhanced security, Docker Hardened Images offer minimal variants of popular images like Node.js, Python, and Go.\n\nTry it out\nUsing the GUI Using the CLI\n\nIn this hands-on, you will learn how to search and pull a container image using the Docker Desktop GUI.\n\nSearch for and download an image\n\nOpen the Docker Desktop Dashboard and select the Images view in the left-hand navigation menu.\n\nSelect the Search images to run button. If you don't see it, select the global search bar at the top of the screen.\n\nIn the Search field, enter \"welcome-to-docker\". Once the search has completed, select the docker/welcome-to-docker image.\n\nSelect Pull to download the image.\n\nLearn about the image\n\nOnce you have an image downloaded, you can learn quite a few details about the image either through the GUI or the CLI.\n\nIn the Docker Desktop Dashboard, select the Images view.\n\nSelect the docker/welcome-to-docker image to open details about the image.\n\nThe image details page presents you with information regarding the layers of the image, the packages and libraries installed in the image, and any discovered vulnerabilities.\n\nIn this walkthrough, you searched and pulled a Docker image. In addition to pulling a Docker image, you also learned about the layers of a Docker Image.\n\nAdditional resources\n\nThe following resources will help you learn more about exploring, finding, and building images:\n\nDocker trusted content\nExplore the Image view in Docker Desktop\nDocker Build overview\nDocker Hub\nNext steps\n\nNow that you have learned the basics of images, it's time to learn about distributing images through registries.\n\nWhat is a registry?\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nFinding images\nTry it out\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994994,
    "timestamp": "2026-02-07T06:29:19.510Z",
    "title": "What is a registry? | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-a-registry/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nWhat is a container?\nWhat is an image?\nWhat is a registry?\nWhat is Docker Compose?\nBuilding images\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nThe basics\n/\nWhat is a registry?\nWhat is a registry?\nCopy as Markdown\nExplanation\n\nNow that you know what a container image is and how it works, you might wonder - where do you store these images?\n\nWell, you can store your container images on your computer system, but what if you want to share them with your friends or use them on another machine? That's where the image registry comes in.\n\nAn image registry is a centralized location for storing and sharing your container images. It can be either public or private. Docker Hub is a public registry that anyone can use and is the default registry.\n\nWhile Docker Hub is a popular option, there are many other available container registries available today, including Amazon Elastic Container Registry (ECR), Azure Container Registry (ACR), and Google Container Registry (GCR). You can even run your private registry on your local system or inside your organization. For example, Harbor, JFrog Artifactory, GitLab Container registry etc.\n\nRegistry vs. repository\n\nWhile you're working with registries, you might hear the terms registry and repository as if they're interchangeable. Even though they're related, they're not quite the same thing.\n\nA registry is a centralized location that stores and manages container images, whereas a repository is a collection of related container images within a registry. Think of it as a folder where you organize your images based on projects. Each repository contains one or more container images.\n\nThe following diagram shows the relationship between a registry, repositories, and images.\n\nI\nI\nI\nI\nI\nm\nm\nm\nm\nm\na\na\na\na\na\ng\ng\ng\ng\ng\ne\ne\ne\ne\ne\nR\n:\n:\nR\n:\n:\n:\ne\ne\nR\np\np\np\np\np\np\np\ne\no\nr\nr\no\nr\nr\nr\ng\ns\no\no\ns\no\no\no\ni\ni\nj\nj\ni\nj\nj\nj\ns\nt\ne\ne\nt\ne\ne\ne\nt\no\nc\nc\no\nc\nc\nc\nr\nr\nt\nt\nr\nt\nt\nt\ny\ny\n-\n-\ny\n-\n-\n-\na\na\nb\nb\nb\nA\n:\n:\nB\n:\n:\n:\nv\nv\nv\nv\nv\n1\n2\n1\n1\n2\n.\n.\n.\n.\n.\n0\n0\n0\n1\n0\nNote\n\nYou can create one private repository and unlimited public repositories using the free version of Docker Hub. For more information, visit the Docker Hub subscription page.\n\nTry it out\n\nIn this hands-on, you will learn how to build and push a Docker image to the Docker Hub repository.\n\nSign up for a free Docker account\n\nIf you haven't created one yet, head over to the Docker Hub page to sign up for a new Docker account. Be sure to finish the verification steps sent to your email.\n\nYou can use your Google or GitHub account to authenticate.\n\nCreate your first repository\n\nSign in to Docker Hub.\n\nSelect the Create repository button in the top-right corner.\n\nSelect your namespace (most likely your username) and enter docker-quickstart as the repository name.\n\nSet the visibility to Public.\n\nSelect the Create button to create the repository.\n\nThat's it. You've successfully created your first repository. üéâ\n\nThis repository is empty right now. You'll now fix this by pushing an image to it.\n\nSign in with Docker Desktop\nDownload and install Docker Desktop, if not already installed.\nIn the Docker Desktop GUI, select the Sign in button in the top-right corner\nClone sample Node.js code\n\nIn order to create an image, you first need a project. To get you started quickly, you'll use a sample Node.js project found at github.com/dockersamples/helloworld-demo-node. This repository contains a pre-built Dockerfile necessary for building a Docker image.\n\nDon't worry about the specifics of the Dockerfile, as you'll learn about that in later sections.\n\nClone the GitHub repository using the following command:\n\ngit clone https://github.com/dockersamples/helloworld-demo-node\n\n\nNavigate into the newly created directory.\n\ncd helloworld-demo-node\n\n\nRun the following command to build a Docker image, swapping out YOUR_DOCKER_USERNAME with your username.\n\ndocker build -t YOUR_DOCKER_USERNAME/docker-quickstart .\n\nNote\n\nMake sure you include the dot (.) at the end of the docker build command. This tells Docker where to find the Dockerfile.\n\nRun the following command to list the newly created Docker image:\n\ndocker images\n\n\nYou will see output like the following:\n\nREPOSITORY                                 TAG       IMAGE ID       CREATED         SIZE\n\nYOUR_DOCKER_USERNAME/docker-quickstart   latest    476de364f70e   2 minutes ago   170MB\n\n\nStart a container to test the image by running the following command (swap out the username with your own username):\n\ndocker run -d -p 8080:8080 YOUR_DOCKER_USERNAME/docker-quickstart \n\n\nYou can verify if the container is working by visiting http://localhost:8080 with your browser.\n\nUse the docker tag command to tag the Docker image. Docker tags allow you to label and version your images.\n\ndocker tag YOUR_DOCKER_USERNAME/docker-quickstart YOUR_DOCKER_USERNAME/docker-quickstart:1.0 \n\n\nFinally, it's time to push the newly built image to your Docker Hub repository by using the docker push command:\n\ndocker push YOUR_DOCKER_USERNAME/docker-quickstart:1.0\n\n\nOpen Docker Hub and navigate to your repository. Navigate to the Tags section and see your newly pushed image.\n\nIn this walkthrough, you signed up for a Docker account, created your first Docker Hub repository, and built, tagged, and pushed a container image to your Docker Hub repository.\n\nAdditional resources\nDocker Hub Quickstart\nManage Docker Hub Repositories\nNext steps\n\nNow that you understand the basics of containers and images, you're ready to learn about Docker Compose.\n\nWhat is Docker Compose?\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nRegistry vs. repository\nTry it out\nSign up for a free Docker account\nCreate your first repository\nSign in with Docker Desktop\nClone sample Node.js code\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258994997,
    "timestamp": "2026-02-07T06:29:19.513Z",
    "title": "What is Docker Compose? | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-docker-compose/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nWhat is a container?\nWhat is an image?\nWhat is a registry?\nWhat is Docker Compose?\nBuilding images\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nThe basics\n/\nWhat is Docker Compose?\nWhat is Docker Compose?\nCopy as Markdown\nExplanation\n\nIf you've been following the guides so far, you've been working with single container applications. But, now you're wanting to do something more complicated - run databases, message queues, caches, or a variety of other services. Do you install everything in a single container? Run multiple containers? If you run multiple, how do you connect them all together?\n\nOne best practice for containers is that each container should do one thing and do it well. While there are exceptions to this rule, avoid the tendency to have one container do multiple things.\n\nYou can use multiple docker run commands to start multiple containers. But, you'll soon realize you'll need to manage networks, all of the flags needed to connect containers to those networks, and more. And when you're done, cleanup is a little more complicated.\n\nWith Docker Compose, you can define all of your containers and their configurations in a single YAML file. If you include this file in your code repository, anyone that clones your repository can get up and running with a single command.\n\nIt's important to understand that Compose is a declarative tool - you simply define it and go. You don't always need to recreate everything from scratch. If you make a change, run docker compose up again and Compose will reconcile the changes in your file and apply them intelligently.\n\nDockerfile versus Compose file\n\nA Dockerfile provides instructions to build a container image while a Compose file defines your running containers. Quite often, a Compose file references a Dockerfile to build an image to use for a particular service.\n\nTry it out\n\nIn this hands-on, you will learn how to use a Docker Compose to run a multi-container application. You'll use a simple to-do list app built with Node.js and MySQL as a database server.\n\nStart the application\n\nFollow the instructions to run the to-do list app on your system.\n\nDownload and install Docker Desktop.\n\nOpen a terminal and clone this sample application.\n\ngit clone https://github.com/dockersamples/todo-list-app \n\n\nNavigate into the todo-list-app directory:\n\ncd todo-list-app\n\n\nInside this directory, you'll find a file named compose.yaml. This YAML file is where all the magic happens! It defines all the services that make up your application, along with their configurations. Each service specifies its image, ports, volumes, networks, and any other settings necessary for its functionality. Take some time to explore the YAML file and familiarize yourself with its structure.\n\nUse the docker compose up command to start the application:\n\ndocker compose up -d --build\n\n\nWhen you run this command, you should see an output like this:\n\n[+] Running 5/5\n\n‚àö app 3 layers [‚£ø‚£ø‚£ø]      0B/0B            Pulled          7.1s\n\n  ‚àö e6f4e57cc59e Download complete                          0.9s\n\n  ‚àö df998480d81d Download complete                          1.0s\n\n  ‚àö 31e174fedd23 Download complete                          2.5s\n\n  ‚àö 43c47a581c29 Download complete                          2.0s\n\n[+] Running 4/4\n\n  ‚†∏ Network todo-list-app_default           Created         0.3s\n\n  ‚†∏ Volume \"todo-list-app_todo-mysql-data\"  Created         0.3s\n\n  ‚àö Container todo-list-app-app-1           Started         0.3s\n\n  ‚àö Container todo-list-app-mysql-1         Started         0.3s\n\n\nA lot happened here! A couple of things to call out:\n\nTwo container images were downloaded from Docker Hub - node and MySQL\nA network was created for your application\nA volume was created to persist the database files between container restarts\nTwo containers were started with all of their necessary config\n\nIf this feels overwhelming, don't worry! You'll get there!\n\nWith everything now up and running, you can open http://localhost:3000 in your browser to see the site. Note that the application may take 10-15 seconds to fully start. If the page doesn't load right away, wait a moment and refresh. Feel free to add items to the list, check them off, and remove them.\n\nIf you look at the Docker Desktop GUI, you can see the containers and dive deeper into their configuration.\n\nTear it down\n\nSince this application was started using Docker Compose, it's easy to tear it all down when you're done.\n\nIn the CLI, use the docker compose down command to remove everything:\n\ndocker compose down\n\n\nYou'll see output similar to the following:\n\n[+] Running 3/3\n\n‚àö Container todo-list-app-mysql-1  Removed        2.9s\n\n‚àö Container todo-list-app-app-1    Removed        0.1s\n\n‚àö Network todo-list-app_default    Removed        0.1s\n\n\nVolume persistence\n\nBy default, volumes aren't automatically removed when you tear down a Compose stack. The idea is that you might want the data back if you start the stack again.\n\nIf you do want to remove the volumes, add the --volumes flag when running the docker compose down command:\n\ndocker compose down --volumes\n\n[+] Running 1/0\n\n‚àö Volume todo-list-app_todo-mysql-data  Removed\n\n\nAlternatively, you can use the Docker Desktop GUI to remove the containers by selecting the application stack and selecting the Delete button.\n\nUsing the GUI for Compose stacks\n\nNote that if you remove the containers for a Compose app in the GUI, it's removing only the containers. You'll have to manually remove the network and volumes if you want to do so.\n\nIn this walkthrough, you learned how to use Docker Compose to start and stop a multi-container application.\n\nAdditional resources\n\nThis page was a brief introduction to Compose. In the following resources, you can dive deeper into Compose and how to write Compose files.\n\nOverview of Docker Compose\nOverview of Docker Compose CLI\nHow Compose works\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nTry it out\nStart the application\nTear it down\nAdditional resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995000,
    "timestamp": "2026-02-07T06:29:19.518Z",
    "title": "Building images | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/building-images/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nUnderstanding the image layers\nWriting a Dockerfile\nBuild, tag, and publish an image\nUsing the build cache\nMulti-stage builds\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nBuilding images\nBuilding images\nBuilding container images is both technical and an art. You want to keep the image small and focused to increase your security posture, but also need to balance potential tradeoffs, such as caching impacts. In this series, you‚Äôll deep dive into the secrets of images, how they are built and best practices.\nSkill level\nBeginner\nTime to complete\n25 minutes\nPrerequisites\nNone\nAbout this series\n\nLearn how to build production-ready images that are lean and efficient Docker images, essential for minimizing overhead and enhancing deployment in production environments.\n\nWhat you'll learn\nUnderstanding image layers\nWriting a Dockerfile\nBuild, tag and publish an image\nUsing the build cache\nMulti-stage builds\nModules\n1. Understanding the image layers\n\nHave you ever wondered how images work? This guide will help you to understand image layers - the fundamental building blocks of container images. You'll gain a comprehensive understanding of how layers are created, stacked, and utilized to ensure efficient and optimized containers.\n\nStart\n2. Writing a Dockerfile\n3. Build, tag, and publish an image\n4. Using the build cache\n5. Multi-stage builds\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995003,
    "timestamp": "2026-02-07T06:29:19.533Z",
    "title": "Understanding the image layers | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nUnderstanding the image layers\nWriting a Dockerfile\nBuild, tag, and publish an image\nUsing the build cache\nMulti-stage builds\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nBuilding images\n/\nUnderstanding the image layers\nUnderstanding the image layers\nCopy as Markdown\nExplanation\n\nAs you learned in What is an image?, container images are composed of layers. And each of these layers, once created, are immutable. But, what does that actually mean? And how are those layers used to create the filesystem a container can use?\n\nImage layers\n\nEach layer in an image contains a set of filesystem changes - additions, deletions, or modifications. Let‚Äôs look at a theoretical image:\n\nThe first layer adds basic commands and a package manager, such as apt.\nThe second layer installs a Python runtime and pip for dependency management.\nThe third layer copies in an application‚Äôs specific requirements.txt file.\nThe fourth layer installs that application‚Äôs specific dependencies.\nThe fifth layer copies in the actual source code of the application.\n\nThis example might look like:\n\nThis is beneficial because it allows layers to be reused between images. For example, imagine you wanted to create another Python application. Due to layering, you can leverage the same Python base. This will make builds faster and reduce the amount of storage and bandwidth required to distribute the images. The image layering might look similar to the following:\n\nLayers let you extend images of others by reusing their base layers, allowing you to add only the data that your application needs.\n\nStacking the layers\n\nLayering is made possible by content-addressable storage and union filesystems. While this will get technical, here‚Äôs how it works:\n\nAfter each layer is downloaded, it is extracted into its own directory on the host filesystem.\nWhen you run a container from an image, a union filesystem is created where layers are stacked on top of each other, creating a new and unified view.\nWhen the container starts, its root directory is set to the location of this unified directory, using chroot.\n\nWhen the union filesystem is created, in addition to the image layers, a directory is created specifically for the running container. This allows the container to make filesystem changes while allowing the original image layers to remain untouched. This enables you to run multiple containers from the same underlying image.\n\nTry it out\n\nIn this hands-on guide, you will create new image layers manually using the docker container commit command. Note that you‚Äôll rarely create images this way, as you‚Äôll normally use a Dockerfile. But, it makes it easier to understand how it‚Äôs all working.\n\nCreate a base image\n\nIn this first step, you will create your own base image that you will then use for the following steps.\n\nDownload and install Docker Desktop.\n\nIn a terminal, run the following command to start a new container:\n\n$ docker run --name=base-container -ti ubuntu\n\n\nOnce the image has been downloaded and the container has started, you should see a new shell prompt. This is running inside your container. It will look similar to the following (the container ID will vary):\n\nroot@d8c5ca119fcd:/#\n\n\nInside the container, run the following command to install Node.js:\n\n$ apt update && apt install -y nodejs\n\n\nWhen this command runs, it downloads and installs Node inside the container. In the context of the union filesystem, these filesystem changes occur within the directory unique to this container.\n\nValidate if Node is installed by running the following command:\n\n$ node -e 'console.log(\"Hello world!\")'\n\n\nYou should then see a ‚ÄúHello world!‚Äù appear in the console.\n\nNow that you have Node installed, you‚Äôre ready to save the changes you‚Äôve made as a new image layer, from which you can start new containers or build new images. To do so, you will use the docker container commit command. Run the following command in a new terminal:\n\n$ docker container commit -m \"Add node\" base-container node-base\n\n\nView the layers of your image using the docker image history command:\n\n$ docker image history node-base\n\n\nYou will see output similar to the following:\n\nIMAGE          CREATED          CREATED BY                                      SIZE      COMMENT\n\n9e274734bb25   10 seconds ago   /bin/bash                                       157MB     Add node\n\ncd1dba651b30   7 days ago       /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B\n\n<missing>      7 days ago       /bin/sh -c #(nop) ADD file:6089c6bede9eca8ec‚Ä¶   110MB\n\n<missing>      7 days ago       /bin/sh -c #(nop)  LABEL org.opencontainers.‚Ä¶   0B\n\n<missing>      7 days ago       /bin/sh -c #(nop)  LABEL org.opencontainers.‚Ä¶   0B\n\n<missing>      7 days ago       /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B\n\n<missing>      7 days ago       /bin/sh -c #(nop)  ARG RELEASE                  0B\n\n\nNote the ‚ÄúAdd node‚Äù comment on the top line. This layer contains the Node.js install you just made.\n\nTo prove your image has Node installed, you can start a new container using this new image:\n\n$ docker run node-base node -e \"console.log('Hello again')\"\n\n\nWith that, you should get a ‚ÄúHello again‚Äù output in the terminal, showing Node was installed and working.\n\nNow that you‚Äôre done creating your base image, you can remove that container:\n\n$ docker rm -f base-container\n\n\nBase image definition\n\nA base image is a foundation for building other images. It's possible to use any images as a base image. However, some images are intentionally created as building blocks, providing a foundation or starting point for an application.\n\nIn this example, you probably won‚Äôt deploy this node-base image, as it doesn‚Äôt actually do anything yet. But it‚Äôs a base you can use for other builds.\n\nBuild an app image\n\nNow that you have a base image, you can extend that image to build additional images.\n\nStart a new container using the newly created node-base image:\n\n$ docker run --name=app-container -ti node-base\n\n\nInside of this container, run the following command to create a Node program:\n\n$ echo 'console.log(\"Hello from an app\")' > app.js\n\n\nTo run this Node program, you can use the following command and see the message printed on the screen:\n\n$ node app.js\n\n\nIn another terminal, run the following command to save this container‚Äôs changes as a new image:\n\n$ docker container commit -c \"CMD node app.js\" -m \"Add app\" app-container sample-app\n\n\nThis command not only creates a new image named sample-app, but also adds additional configuration to the image to set the default command when starting a container. In this case, you are setting it to automatically run node app.js.\n\nIn a terminal outside of the container, run the following command to view the updated layers:\n\n$ docker image history sample-app\n\n\nYou‚Äôll then see output that looks like the following. Note the top layer comment has ‚ÄúAdd app‚Äù and the next layer has ‚ÄúAdd node‚Äù:\n\nIMAGE          CREATED              CREATED BY                                      SIZE      COMMENT\n\nc1502e2ec875   About a minute ago   /bin/bash                                       33B       Add app\n\n5310da79c50a   4 minutes ago        /bin/bash                                       126MB     Add node\n\n2b7cc08dcdbb   5 weeks ago          /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B\n\n<missing>      5 weeks ago          /bin/sh -c #(nop) ADD file:07cdbabf782942af0‚Ä¶   69.2MB\n\n<missing>      5 weeks ago          /bin/sh -c #(nop)  LABEL org.opencontainers.‚Ä¶   0B\n\n<missing>      5 weeks ago          /bin/sh -c #(nop)  LABEL org.opencontainers.‚Ä¶   0B\n\n<missing>      5 weeks ago          /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B\n\n<missing>      5 weeks ago          /bin/sh -c #(nop)  ARG RELEASE                  0B\n\n\nFinally, start a new container using the brand new image. Since you specified the default command, you can use the following command:\n\n$ docker run sample-app\n\n\nYou should see your greeting appear in the terminal, coming from your Node program.\n\nNow that you‚Äôre done with your containers, you can remove them using the following command:\n\n$ docker rm -f app-container\n\nAdditional resources\n\nIf you‚Äôd like to dive deeper into the things you learned, check out the following resources:\n\ndocker image history\ndocker container commit\nNext steps\n\nAs hinted earlier, most image builds don‚Äôt use docker container commit. Instead, you‚Äôll use a Dockerfile which automates these steps for you.\n\nWriting a Dockerfile\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nImage layers\nStacking the layers\nTry it out\nCreate a base image\nBuild an app image\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995006,
    "timestamp": "2026-02-07T06:29:19.548Z",
    "title": "Writing a Dockerfile | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/building-images/writing-a-dockerfile/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nUnderstanding the image layers\nWriting a Dockerfile\nBuild, tag, and publish an image\nUsing the build cache\nMulti-stage builds\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nBuilding images\n/\nWriting a Dockerfile\nWriting a Dockerfile\nCopy as Markdown\nExplanation\n\nA Dockerfile is a text-based document that's used to create a container image. It provides instructions to the image builder on the commands to run, files to copy, startup command, and more.\n\nAs an example, the following Dockerfile would produce a ready-to-run Python application:\n\nFROM python:3.13\n\nWORKDIR /usr/local/app\n\n\n\n# Install the application dependencies\n\nCOPY requirements.txt ./\n\nRUN pip install --no-cache-dir -r requirements.txt\n\n\n\n# Copy in the source code\n\nCOPY src ./src\n\nEXPOSE 8080\n\n\n\n# Setup an app user so the container doesn't run as the root user\n\nRUN useradd app\n\nUSER app\n\n\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\nCommon instructions\n\nSome of the most common instructions in a Dockerfile include:\n\nFROM <image> - this specifies the base image that the build will extend.\nWORKDIR <path> - this instruction specifies the \"working directory\" or the path in the image where files will be copied and commands will be executed.\nCOPY <host-path> <image-path> - this instruction tells the builder to copy files from the host and put them into the container image.\nRUN <command> - this instruction tells the builder to run the specified command.\nENV <name> <value> - this instruction sets an environment variable that a running container will use.\nEXPOSE <port-number> - this instruction sets configuration on the image that indicates a port the image would like to expose.\nUSER <user-or-uid> - this instruction sets the default user for all subsequent instructions.\nCMD [\"<command>\", \"<arg1>\"] - this instruction sets the default command a container using this image will run.\n\nTo read through all of the instructions or go into greater detail, check out the Dockerfile reference.\n\nTry it out\n\nJust as you saw with the previous example, a Dockerfile typically follows these steps:\n\nDetermine your base image\nInstall application dependencies\nCopy in any relevant source code and/or binaries\nConfigure the final image\n\nIn this quick hands-on guide, you'll write a Dockerfile that builds a simple Node.js application. If you're not familiar with JavaScript-based applications, don't worry. It isn't necessary for following along with this guide.\n\nSet up\n\nDownload this ZIP file and extract the contents into a directory on your machine.\n\nIf you'd rather not download a ZIP file, clone the https://github.com/docker/getting-started-todo-app project and checkout the build-image-from-scratch branch.\n\nCreating the Dockerfile\n\nNow that you have the project, you‚Äôre ready to create the Dockerfile.\n\nDownload and install Docker Desktop.\n\nExamine the project.\n\nExplore the contents of getting-started-todo-app/app/. You'll notice that a Dockerfile already exists. It is a simple text file that you can open in any text or code editor.\n\nDelete the existing Dockerfile.\n\nFor this exercise, you'll pretend you're starting from scratch and will create a new Dockerfile.\n\nCreate a file named Dockerfile in the getting-started-todo-app/app/ folder.\n\nDockerfile file extensions\n\nIt's important to note that the Dockerfile has no file extension. Some editors will automatically add an extension to the file (or complain it doesn't have one).\n\nIn the Dockerfile, define your base image by adding the following line:\n\nFROM node:22-alpine\n\nNow, define the working directory by using the WORKDIR instruction. This will specify where future commands will run and the directory files will be copied inside the container image.\n\nWORKDIR /app\n\nCopy all of the files from your project on your machine into the container image by using the COPY instruction:\n\nCOPY . .\n\nInstall the app's dependencies by using the yarn CLI and package manager. To do so, run a command using the RUN instruction:\n\nRUN yarn install --production\n\nFinally, specify the default command to run by using the CMD instruction:\n\nCMD [\"node\", \"./src/index.js\"]\n\nAnd with that, you should have the following Dockerfile:\n\nFROM node:22-alpine\n\nWORKDIR /app\n\nCOPY . .\n\nRUN yarn install --production\n\nCMD [\"node\", \"./src/index.js\"]\n\nThis Dockerfile isn't production-ready yet\n\nIt's important to note that this Dockerfile is not following all of the best practices yet (by design). It will build the app, but the builds won't be as fast, or the images as secure, as they could be.\n\nKeep reading to learn more about how to make the image maximize the build cache, run as a non-root user, and multi-stage builds.\n\nContainerize new projects quickly with docker init\n\nThe docker init command will analyze your project and quickly create a Dockerfile, a compose.yaml, and a .dockerignore, helping you get up and going. Since you're learning about Dockerfiles specifically here, you won't use it now. But, learn more about it here.\n\nAdditional resources\n\nTo learn more about writing a Dockerfile, visit the following resources:\n\nDockerfile reference\nDockerfile best practices\nBase images\nGetting started with Docker Init\nNext steps\n\nNow that you have created a Dockerfile and learned the basics, it's time to learn about building, tagging, and pushing the images.\n\nBuild, tag and publish the Image\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nCommon instructions\nTry it out\nSet up\nCreating the Dockerfile\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995009,
    "timestamp": "2026-02-07T06:29:19.548Z",
    "title": "Build, tag, and publish an image | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/building-images/build-tag-and-publish-an-image/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nUnderstanding the image layers\nWriting a Dockerfile\nBuild, tag, and publish an image\nUsing the build cache\nMulti-stage builds\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nBuilding images\n/\nBuild, tag, and publish an image\nBuild, tag, and publish an image\nCopy as Markdown\nExplanation\n\nIn this guide, you will learn the following:\n\nBuilding images - the process of building an image based on a Dockerfile\nTagging images - the process of giving an image a name, which also determines where the image can be distributed\nPublishing images - the process to distribute or share the newly created image using a container registry\nBuilding images\n\nMost often, images are built using a Dockerfile. The most basic docker build command might look like the following:\n\ndocker build .\n\nThe final . in the command provides the path or URL to the build context. At this location, the builder will find the Dockerfile and other referenced files.\n\nWhen you run a build, the builder pulls the base image, if needed, and then runs the instructions specified in the Dockerfile.\n\nWith the previous command, the image will have no name, but the output will provide the ID of the image. As an example, the previous command might produce the following output:\n\n$ docker build .\n\n[+] Building 3.5s (11/11) FINISHED                                              docker:desktop-linux\n\n => [internal] load build definition from Dockerfile                                            0.0s\n\n => => transferring dockerfile: 308B                                                            0.0s\n\n => [internal] load metadata for docker.io/library/python:3.12                                  0.0s\n\n => [internal] load .dockerignore                                                               0.0s\n\n => => transferring context: 2B                                                                 0.0s\n\n => [1/6] FROM docker.io/library/python:3.12                                                    0.0s\n\n => [internal] load build context                                                               0.0s\n\n => => transferring context: 123B                                                               0.0s\n\n => [2/6] WORKDIR /usr/local/app                                                                0.0s\n\n => [3/6] RUN useradd app                                                                       0.1s\n\n => [4/6] COPY ./requirements.txt ./requirements.txt                                            0.0s\n\n => [5/6] RUN pip install --no-cache-dir --upgrade -r requirements.txt                          3.2s\n\n => [6/6] COPY ./app ./app                                                                      0.0s\n\n => exporting to image                                                                          0.1s\n\n => => exporting layers                                                                         0.1s\n\n => => writing image sha256:9924dfd9350407b3df01d1a0e1033b1e543523ce7d5d5e2c83a724480ebe8f00    0.0s\n\n\nWith the previous output, you could start a container by using the referenced image:\n\ndocker run sha256:9924dfd9350407b3df01d1a0e1033b1e543523ce7d5d5e2c83a724480ebe8f00\n\n\nThat name certainly isn't memorable, which is where tagging becomes useful.\n\nTagging images\n\nTagging images is the method to provide an image with a memorable name. However, there is a structure to the name of an image. A full image name has the following structure:\n\n[HOST[:PORT_NUMBER]/]PATH[:TAG]\nHOST: The optional registry hostname where the image is located. If no host is specified, Docker's public registry at docker.io is used by default.\nPORT_NUMBER: The registry port number if a hostname is provided\nPATH: The path of the image, consisting of slash-separated components. For Docker Hub, the format follows [NAMESPACE/]REPOSITORY, where namespace is either a user's or organization's name. If no namespace is specified, library is used, which is the namespace for Docker Official Images.\nTAG: A custom, human-readable identifier that's typically used to identify different versions or variants of an image. If no tag is specified, latest is used by default.\n\nSome examples of image names include:\n\nnginx, equivalent to docker.io/library/nginx:latest: this pulls an image from the docker.io registry, the library namespace, the nginx image repository, and the latest tag.\ndocker/welcome-to-docker, equivalent to docker.io/docker/welcome-to-docker:latest: this pulls an image from the docker.io registry, the docker namespace, the welcome-to-docker image repository, and the latest tag\nghcr.io/dockersamples/example-voting-app-vote:pr-311: this pulls an image from the GitHub Container Registry, the dockersamples namespace, the example-voting-app-vote image repository, and the pr-311 tag\n\nTo tag an image during a build, add the -t or --tag flag:\n\ndocker build -t my-username/my-image .\n\n\nIf you've already built an image, you can add another tag to the image by using the docker image tag command:\n\ndocker image tag my-username/my-image another-username/another-image:v1\n\nPublishing images\n\nOnce you have an image built and tagged, you're ready to push it to a registry. To do so, use the docker push command:\n\ndocker push my-username/my-image\n\n\nWithin a few seconds, all of the layers for your image will be pushed to the registry.\n\nRequiring authentication\n\nBefore you're able to push an image to a repository, you will need to be authenticated. To do so, simply use the docker login command.\n\nTry it out\n\nIn this hands-on guide, you will build a simple image using a provided Dockerfile and push it to Docker Hub.\n\nSet up\n\nGet the sample application.\n\nIf you have Git, you can clone the repository for the sample application. Otherwise, you can download the sample application. Choose one of the following options.\n\nClone with git Download\n\nUse the following command in a terminal to clone the sample application repository.\n\n$ git clone https://github.com/docker/getting-started-todo-app\n\n\nDownload and install Docker Desktop.\n\nIf you don't have a Docker account yet, create one now. Once you've done that, sign in to Docker Desktop using that account.\n\nBuild an image\n\nNow that you have a repository on Docker Hub, it's time for you to build an image and push it to the repository.\n\nUsing a terminal in the root of the sample app repository, run the following command. Replace YOUR_DOCKER_USERNAME with your Docker Hub username:\n\n$ docker build -t YOUR_DOCKER_USERNAME/concepts-build-image-demo .\n\n\nAs an example, if your username is mobywhale, you would run the command:\n\n$ docker build -t mobywhale/concepts-build-image-demo .\n\n\nOnce the build has completed, you can view the image by using the following command:\n\n$ docker image ls\n\n\nThe command will produce output similar to the following:\n\nREPOSITORY                             TAG       IMAGE ID       CREATED          SIZE\n\nmobywhale/concepts-build-image-demo    latest    746c7e06537f   24 seconds ago   354MB\n\nYou can actually view the history (or how the image was created) by using the docker image history command:\n\n$ docker image history mobywhale/concepts-build-image-demo\n\n\nYou'll then see output similar to the following:\n\nIMAGE          CREATED         CREATED BY                                      SIZE      COMMENT\n\nf279389d5f01   8 seconds ago   CMD [\"node\" \"./src/index.js\"]                   0B        buildkit.dockerfile.v0\n\n<missing>      8 seconds ago   EXPOSE map[3000/tcp:{}]                         0B        buildkit.dockerfile.v0 \n\n<missing>      8 seconds ago   WORKDIR /app                                    8.19kB    buildkit.dockerfile.v0\n\n<missing>      4 days ago      /bin/sh -c #(nop)  CMD [\"node\"]                 0B\n\n<missing>      4 days ago      /bin/sh -c #(nop)  ENTRYPOINT [\"docker-entry‚Ä¶   0B\n\n<missing>      4 days ago      /bin/sh -c #(nop) COPY file:4d192565a7220e13‚Ä¶   20.5kB\n\n<missing>      4 days ago      /bin/sh -c apk add --no-cache --virtual .bui‚Ä¶   7.92MB\n\n<missing>      4 days ago      /bin/sh -c #(nop)  ENV YARN_VERSION=1.22.19     0B\n\n<missing>      4 days ago      /bin/sh -c addgroup -g 1000 node     && addu‚Ä¶   126MB\n\n<missing>      4 days ago      /bin/sh -c #(nop)  ENV NODE_VERSION=20.12.0     0B\n\n<missing>      2 months ago    /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B\n\n<missing>      2 months ago    /bin/sh -c #(nop) ADD file:d0764a717d1e9d0af‚Ä¶   8.42MB\n\nThis output shows the layers of the image, highlighting the layers you added and those that were inherited from your base image.\n\nPush the image\n\nNow that you have an image built, it's time to push the image to a registry.\n\nPush the image using the docker push command:\n\n$ docker push YOUR_DOCKER_USERNAME/concepts-build-image-demo\n\n\nIf you receive a requested access to the resource is denied, make sure you are both logged in and that your Docker username is correct in the image tag.\n\nAfter a moment, your image should be pushed to Docker Hub.\n\nAdditional resources\n\nTo learn more about building, tagging, and publishing images, visit the following resources:\n\nWhat is a build context?\ndocker build reference\ndocker image tag reference\ndocker push reference\nWhat is a registry?\nNext steps\n\nNow that you have learned about building and publishing images, it's time to learn how to speed up the build process using the Docker build cache.\n\nUsing the build cache\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nBuilding images\nTagging images\nPublishing images\nTry it out\nSet up\nBuild an image\nPush the image\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995012,
    "timestamp": "2026-02-07T06:29:19.564Z",
    "title": "Using the build cache | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/building-images/using-the-build-cache/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nUnderstanding the image layers\nWriting a Dockerfile\nBuild, tag, and publish an image\nUsing the build cache\nMulti-stage builds\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nBuilding images\n/\nUsing the build cache\nUsing the build cache\nCopy as Markdown\nExplanation\n\nConsider the following Dockerfile that you created for the getting-started app.\n\nFROM node:22-alpine\n\nWORKDIR /app\n\nCOPY . .\n\nRUN yarn install --production\n\nCMD [\"node\", \"./src/index.js\"]\n\nWhen you run the docker build command to create a new image, Docker executes each instruction in your Dockerfile, creating a layer for each command and in the order specified. For each instruction, Docker checks whether it can reuse the instruction from a previous build. If it finds that you've already executed a similar instruction before, Docker doesn't need to redo it. Instead, it‚Äôll use the cached result. This way, your build process becomes faster and more efficient, saving you valuable time and resources.\n\nUsing the build cache effectively lets you achieve faster builds by reusing results from previous builds and skipping unnecessary work. In order to maximize cache usage and avoid resource-intensive and time-consuming rebuilds, it's important to understand how cache invalidation works. Here are a few examples of situations that can cause cache to be invalidated:\n\nAny changes to the command of a RUN instruction invalidates that layer. Docker detects the change and invalidates the build cache if there's any modification to a RUN command in your Dockerfile.\n\nAny changes to files copied into the image with the COPY or ADD instructions. Docker keeps an eye on any alterations to files within your project directory. Whether it's a change in content or properties like permissions, Docker considers these modifications as triggers to invalidate the cache.\n\nOnce one layer is invalidated, all following layers are also invalidated. If any previous layer, including the base image or intermediary layers, has been invalidated due to changes, Docker ensures that subsequent layers relying on it are also invalidated. This keeps the build process synchronized and prevents inconsistencies.\n\nWhen you're writing or editing a Dockerfile, keep an eye out for unnecessary cache misses to ensure that builds run as fast and efficiently as possible.\n\nTry it out\n\nIn this hands-on guide, you will learn how to use the Docker build cache effectively for a Node.js application.\n\nBuild the application\n\nDownload and install Docker Desktop.\n\nOpen a terminal and clone this sample application.\n\n$ git clone https://github.com/dockersamples/todo-list-app\n\n\nNavigate into the todo-list-app directory:\n\n$ cd todo-list-app\n\n\nInside this directory, you'll find a file named Dockerfile with the following content:\n\nFROM node:22-alpine\n\nWORKDIR /app\n\nCOPY . .\n\nRUN yarn install --production\n\nEXPOSE 3000\n\nCMD [\"node\", \"./src/index.js\"]\n\nExecute the following command to build the Docker image:\n\n$ docker build .\n\n\nHere‚Äôs the result of the build process:\n\n[+] Building 20.0s (10/10) FINISHED\n\n\nThe first line indicates that the entire build process took 20.0 seconds. The first build may take some time as it installs dependencies.\n\nRebuild without making changes.\n\nNow, re-run the docker build command without making any change in the source code or Dockerfile as shown:\n\n$ docker build .\n\n\nSubsequent builds after the initial are faster due to the caching mechanism, as long as the commands and context remain unchanged. Docker caches the intermediate layers generated during the build process. When you rebuild the image without making any changes to the Dockerfile or the source code, Docker can reuse the cached layers, significantly speeding up the build process.\n\n[+] Building 1.0s (9/9) FINISHED                                                                            docker:desktop-linux\n\n => [internal] load build definition from Dockerfile                                                                        0.0s\n\n => => transferring dockerfile: 187B                                                                                        0.0s\n\n ...\n\n => [internal] load build context                                                                                           0.0s\n\n => => transferring context: 8.16kB                                                                                         0.0s\n\n => CACHED [2/4] WORKDIR /app                                                                                               0.0s\n\n => CACHED [3/4] COPY . .                                                                                                   0.0s\n\n => CACHED [4/4] RUN yarn install --production                                                                              0.0s\n\n => exporting to image                                                                                                      0.0s\n\n => => exporting layers                                                                                                     0.0s\n\n => => exporting manifest\n\n\nThe subsequent build was completed in just 1.0 second by leveraging the cached layers. No need to repeat time-consuming steps like installing dependencies.\n\nSteps\tDescription\tTime Taken (1st Run)\tTime Taken (2nd Run)\n1\tLoad build definition from Dockerfile\t0.0 seconds\t0.0 seconds\n2\tLoad metadata for docker.io/library/node:22-alpine\t2.7 seconds\t0.9 seconds\n3\tLoad .dockerignore\t0.0 seconds\t0.0 seconds\n4\tLoad build context\n\n(Context size: 4.60MB)\n\n\t0.1 seconds\t0.0 seconds\n5\tSet the working directory (WORKDIR)\t0.1 seconds\t0.0 seconds\n6\tCopy the local code into the container\t0.0 seconds\t0.0 seconds\n7\tRun yarn install --production\t10.0 seconds\t0.0 seconds\n8\tExporting layers\t2.2 seconds\t0.0 seconds\n9\tExporting the final image\t3.0 seconds\t0.0 seconds\n\nGoing back to the docker image history output, you see that each command in the Dockerfile becomes a new layer in the image. You might remember that when you made a change to the image, the yarn dependencies had to be reinstalled. Is there a way to fix this? It doesn't make much sense to reinstall the same dependencies every time you build, right?\n\nTo fix this, restructure your Dockerfile so that the dependency cache remains valid unless it really needs to be invalidated. For Node-based applications, dependencies are defined in the package.json file. You'll want to reinstall the dependencies if that file changes, but use cached dependencies if the file is unchanged. So, start by copying only that file first, then install the dependencies, and finally copy everything else. Then, you only need to recreate the yarn dependencies if there was a change to the package.json file.\n\nUpdate the Dockerfile to copy in the package.json file first, install dependencies, and then copy everything else in.\n\nFROM node:22-alpine\n\nWORKDIR /app\n\nCOPY package.json yarn.lock ./\n\nRUN yarn install --production \n\nCOPY . . \n\nEXPOSE 3000\n\nCMD [\"node\", \"src/index.js\"]\n\nCreate a file named .dockerignore in the same folder as the Dockerfile with the following contents.\n\nnode_modules\n\nBuild the new image:\n\n$ docker build .\n\n\nYou'll then see output similar to the following:\n\n[+] Building 16.1s (10/10) FINISHED\n\n=> [internal] load build definition from Dockerfile                                               0.0s\n\n=> => transferring dockerfile: 175B                                                               0.0s\n\n=> [internal] load .dockerignore                                                                  0.0s\n\n=> => transferring context: 2B                                                                    0.0s\n\n=> [internal] load metadata for docker.io/library/node:22-alpine                                  0.0s\n\n=> [internal] load build context                                                                  0.8s\n\n=> => transferring context: 53.37MB                                                               0.8s\n\n=> [1/5] FROM docker.io/library/node:22-alpine                                                    0.0s\n\n=> CACHED [2/5] WORKDIR /app                                                                      0.0s\n\n=> [3/5] COPY package.json yarn.lock ./                                                           0.2s\n\n=> [4/5] RUN yarn install --production                                                           14.0s\n\n=> [5/5] COPY . .                                                                                 0.5s\n\n=> exporting to image                                                                             0.6s\n\n=> => exporting layers                                                                            0.6s\n\n=> => writing image     \n\nsha256:d6f819013566c54c50124ed94d5e66c452325327217f4f04399b45f94e37d25        0.0s\n\n=> => naming to docker.io/library/node-app:2.0                                                 0.0s\n\n\nYou'll see that all layers were rebuilt. Perfectly fine since you changed the Dockerfile quite a bit.\n\nNow, make a change to the src/static/index.html file (like change the title to say \"The Awesome Todo App\").\n\nBuild the Docker image. This time, your output should look a little different.\n\n$ docker build -t node-app:3.0 .\n\n\nYou'll then see output similar to the following:\n\n[+] Building 1.2s (10/10) FINISHED \n\n=> [internal] load build definition from Dockerfile                                               0.0s\n\n=> => transferring dockerfile: 37B                                                                0.0s\n\n=> [internal] load .dockerignore                                                                  0.0s\n\n=> => transferring context: 2B                                                                    0.0s\n\n=> [internal] load metadata for docker.io/library/node:22-alpine                                  0.0s \n\n=> [internal] load build context                                                                  0.2s\n\n=> => transferring context: 450.43kB                                                              0.2s\n\n=> [1/5] FROM docker.io/library/node:22-alpine                                                    0.0s\n\n=> CACHED [2/5] WORKDIR /app                                                                      0.0s\n\n=> CACHED [3/5] COPY package.json yarn.lock ./                                                    0.0s\n\n=> CACHED [4/5] RUN yarn install --production                                                     0.0s\n\n=> [5/5] COPY . .                                                                                 0.5s \n\n=> exporting to image                                                                             0.3s\n\n=> => exporting layers                                                                            0.3s\n\n=> => writing image     \n\nsha256:91790c87bcb096a83c2bd4eb512bc8b134c757cda0bdee4038187f98148e2eda       0.0s\n\n=> => naming to docker.io/library/node-app:3.0                                                 0.0s\n\n\nFirst off, you should notice that the build was much faster. You'll see that several steps are using previously cached layers. That's good news; you're using the build cache. Pushing and pulling this image and updates to it will be much faster as well.\n\nBy following these optimization techniques, you can make your Docker builds faster and more efficient, leading to quicker iteration cycles and improved development productivity.\n\nAdditional resources\nOptimizing builds with cache management\nCache Storage Backend\nBuild cache invalidation\nNext steps\n\nNow that you understand how to use the Docker build cache effectively, you're ready to learn about Multi-stage builds.\n\nMulti-stage builds\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nTry it out\nBuild the application\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995018,
    "timestamp": "2026-02-07T06:29:19.569Z",
    "title": "Publishing and exposing ports | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/running-containers/publishing-ports/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nRunning containers\nPublishing and exposing ports\nOverriding container defaults\nPersisting container data\nSharing local files with containers\nMulti-container applications\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nRunning containers\n/\nPublishing and exposing ports\nPublishing and exposing ports\nCopy as Markdown\nExplanation\n\nIf you've been following the guides so far, you understand that containers provide isolated processes for each component of your application. Each component - a React frontend, a Python API, and a Postgres database - runs in its own sandbox environment, completely isolated from everything else on your host machine. This isolation is great for security and managing dependencies, but it also means you can‚Äôt access them directly. For example, you can‚Äôt access the web app in your browser.\n\nThat‚Äôs where port publishing comes in.\n\nPublishing ports\n\nPublishing a port provides the ability to break through a little bit of networking isolation by setting up a forwarding rule. As an example, you can indicate that requests on your host‚Äôs port 8080 should be forwarded to the container‚Äôs port 80. Publishing ports happens during container creation using the -p (or --publish) flag with docker run. The syntax is:\n\n$ docker run -d -p HOST_PORT:CONTAINER_PORT nginx\n\nHOST_PORT: The port number on your host machine where you want to receive traffic\nCONTAINER_PORT: The port number within the container that's listening for connections\n\nFor example, to publish the container's port 80 to host port 8080:\n\n$ docker run -d -p 8080:80 nginx\n\n\nNow, any traffic sent to port 8080 on your host machine will be forwarded to port 80 within the container.\n\nImportant\n\nWhen a port is published, it's published to all network interfaces by default. This means any traffic that reaches your machine can access the published application. Be mindful of publishing databases or any sensitive information. Learn more about published ports here.\n\nPublishing to ephemeral ports\n\nAt times, you may want to simply publish the port but don‚Äôt care which host port is used. In these cases, you can let Docker pick the port for you. To do so, simply omit the HOST_PORT configuration.\n\nFor example, the following command will publish the container‚Äôs port 80 onto an ephemeral port on the host:\n\n$ docker run -p 80 nginx\n\n\nOnce the container is running, using docker ps will show you the port that was chosen:\n\ndocker ps\n\nCONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS          PORTS                    NAMES\n\na527355c9c53   nginx         \"/docker-entrypoint.‚Ä¶\"   4 seconds ago    Up 3 seconds    0.0.0.0:54772->80/tcp    romantic_williamson\n\n\nIn this example, the app is exposed on the host at port 54772.\n\nPublishing all ports\n\nWhen creating a container image, the EXPOSE instruction is used to indicate the packaged application will use the specified port. These ports aren't published by default.\n\nWith the -P or --publish-all flag, you can automatically publish all exposed ports to ephemeral ports. This is quite useful when you‚Äôre trying to avoid port conflicts in development or testing environments.\n\nFor example, the following command will publish all of the exposed ports configured by the image:\n\n$ docker run -P nginx\n\nTry it out\n\nIn this hands-on guide, you'll learn how to publish container ports using both the CLI and Docker Compose for deploying a web application.\n\nUse the Docker CLI\n\nIn this step, you will run a container and publish its port using the Docker CLI.\n\nDownload and install Docker Desktop.\n\nIn a terminal, run the following command to start a new container:\n\n$ docker run -d -p 8080:80 docker/welcome-to-docker\n\n\nThe first 8080 refers to the host port. This is the port on your local machine that will be used to access the application running inside the container. The second 80 refers to the container port. This is the port that the application inside the container listens on for incoming connections. Hence, the command binds to port 8080 of the host to port 80 on the container system.\n\nVerify the published port by going to the Containers view of the Docker Desktop Dashboard.\n\nOpen the website by either selecting the link in the Port(s) column of your container or visiting http://localhost:8080 in your browser.\n\nUse Docker Compose\n\nThis example will launch the same application using Docker Compose:\n\nCreate a new directory and inside that directory, create a compose.yaml file with the following contents:\n\nservices:\n\n  app:\n\n    image: docker/welcome-to-docker\n\n    ports:\n\n      - 8080:80\n\nThe ports configuration accepts a few different forms of syntax for the port definition. In this case, you‚Äôre using the same HOST_PORT:CONTAINER_PORT used in the docker run command.\n\nOpen a terminal and navigate to the directory you created in the previous step.\n\nUse the docker compose up command to start the application.\n\nOpen your browser to http://localhost:8080.\n\nAdditional resources\n\nIf you‚Äôd like to dive in deeper on this topic, be sure to check out the following resources:\n\ndocker container port CLI reference\nPublished ports\nNext steps\n\nNow that you understand how to publish and expose ports, you're ready to learn how to override the container defaults using the docker run command.\n\nOverriding container defaults\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nPublishing ports\nPublishing to ephemeral ports\nPublishing all ports\nTry it out\nUse the Docker CLI\nUse Docker Compose\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995015,
    "timestamp": "2026-02-07T06:29:19.587Z",
    "title": "Multi-stage builds | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/building-images/multi-stage-builds/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nUnderstanding the image layers\nWriting a Dockerfile\nBuild, tag, and publish an image\nUsing the build cache\nMulti-stage builds\nRunning containers\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nBuilding images\n/\nMulti-stage builds\nMulti-stage builds\nCopy as Markdown\nExplanation\n\nIn a traditional build, all build instructions are executed in sequence, and in a single build container: downloading dependencies, compiling code, and packaging the application. All those layers end up in your final image. This approach works, but it leads to bulky images carrying unnecessary weight and increasing your security risks. This is where multi-stage builds come in.\n\nMulti-stage builds introduce multiple stages in your Dockerfile, each with a specific purpose. Think of it like the ability to run different parts of a build in multiple different environments, concurrently. By separating the build environment from the final runtime environment, you can significantly reduce the image size and attack surface. This is especially beneficial for applications with large build dependencies.\n\nMulti-stage builds are recommended for all types of applications.\n\nFor interpreted languages, like JavaScript or Ruby or Python, you can build and minify your code in one stage, and copy the production-ready files to a smaller runtime image. This optimizes your image for deployment.\nFor compiled languages, like C or Go or Rust, multi-stage builds let you compile in one stage and copy the compiled binaries into a final runtime image. No need to bundle the entire compiler in your final image.\n\nHere's a simplified example of a multi-stage build structure using pseudo-code. Notice there are multiple FROM statements and a new AS <stage-name>. In addition, the COPY statement in the second stage is copying --from the previous stage.\n\n# Stage 1: Build Environment\n\nFROM builder-image AS build-stage \n\n# Install build tools (e.g., Maven, Gradle)\n\n# Copy source code\n\n# Build commands (e.g., compile, package)\n\n\n\n# Stage 2: Runtime environment\n\nFROM runtime-image AS final-stage  \n\n#  Copy application artifacts from the build stage (e.g., JAR file)\n\nCOPY --from=build-stage /path/in/build/stage /path/to/place/in/final/stage\n\n# Define runtime configuration (e.g., CMD, ENTRYPOINT) \n\nThis Dockerfile uses two stages:\n\nThe build stage uses a base image containing build tools needed to compile your application. It includes commands to install build tools, copy source code, and execute build commands.\nThe final stage uses a smaller base image suitable for running your application. It copies the compiled artifacts (a JAR file, for example) from the build stage. Finally, it defines the runtime configuration (using CMD or ENTRYPOINT) for starting your application.\nTry it out\n\nIn this hands-on guide, you'll unlock the power of multi-stage builds to create lean and efficient Docker images for a sample Java application. You'll use a simple ‚ÄúHello World‚Äù Spring Boot-based application built with Maven as your example.\n\nDownload and install Docker Desktop.\n\nOpen this pre-initialized project to generate a ZIP file. Here‚Äôs how that looks:\n\nSpring Initializr is a quickstart generator for Spring projects. It provides an extensible API to generate JVM-based projects with implementations for several common concepts ‚Äî like basic language generation for Java, Kotlin, Groovy, and Maven.\n\nSelect Generate to create and download the zip file for this project.\n\nFor this demonstration, you‚Äôve paired Maven build automation with Java, a Spring Web dependency, and Java 21 for your metadata.\n\nNavigate the project directory. Once you unzip the file, you'll see the following project directory structure:\n\nspring-boot-docker\n\n‚îú‚îÄ‚îÄ HELP.md\n\n‚îú‚îÄ‚îÄ mvnw\n\n‚îú‚îÄ‚îÄ mvnw.cmd\n\n‚îú‚îÄ‚îÄ pom.xml\n\n‚îî‚îÄ‚îÄ src\n\n    ‚îú‚îÄ‚îÄ main\n\n    ‚îÇ   ‚îú‚îÄ‚îÄ java\n\n    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ com\n\n    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ example\n\n    ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ spring_boot_docker\n\n    ‚îÇ   ‚îÇ               ‚îî‚îÄ‚îÄ SpringBootDockerApplication.java\n\n    ‚îÇ   ‚îî‚îÄ‚îÄ resources\n\n    ‚îÇ       ‚îú‚îÄ‚îÄ application.properties\n\n    ‚îÇ       ‚îú‚îÄ‚îÄ static\n\n    ‚îÇ       ‚îî‚îÄ‚îÄ templates\n\n    ‚îî‚îÄ‚îÄ test\n\n        ‚îî‚îÄ‚îÄ java\n\n            ‚îî‚îÄ‚îÄ com\n\n                ‚îî‚îÄ‚îÄ example\n\n                    ‚îî‚îÄ‚îÄ spring_boot_docker\n\n                        ‚îî‚îÄ‚îÄ SpringBootDockerApplicationTests.java\n\n\n\n15 directories, 7 files\n\nThe src/main/java directory contains your project's source code, the src/test/java directory\ncontains the test source, and the pom.xml file is your project‚Äôs Project Object Model (POM).\n\nThe pom.xml file is the core of a Maven project's configuration. It's a single configuration file that\ncontains most of the information needed to build a customized project. The POM is huge and can seem\ndaunting. Thankfully, you don't yet need to understand every intricacy to use it effectively.\n\nCreate a RESTful web service that displays \"Hello World!\".\n\nUnder the src/main/java/com/example/spring_boot_docker/ directory, you can modify your\nSpringBootDockerApplication.java file with the following content:\n\npackage com.example.spring_boot_docker;\n\n\n\nimport org.springframework.boot.SpringApplication;\n\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\nimport org.springframework.web.bind.annotation.RequestMapping;\n\nimport org.springframework.web.bind.annotation.RestController;\n\n\n\n\n\n@RestController\n\n@SpringBootApplication\n\npublic class SpringBootDockerApplication {\n\n\n\n    @RequestMapping(\"/\")\n\n        public String home() {\n\n        return \"Hello World\";\n\n    }\n\n\n\n\tpublic static void main(String[] args) {\n\n\t\tSpringApplication.run(SpringBootDockerApplication.class, args);\n\n\t}\n\n\n\n}\n\nThe SpringbootDockerApplication.java file starts by declaring your com.example.spring_boot_docker package and importing necessary Spring frameworks. This Java file creates a simple Spring Boot web application that responds with \"Hello World\" when a user visits its homepage.\n\nCreate the Dockerfile\n\nNow that you have the project, you‚Äôre ready to create the Dockerfile.\n\nCreate a file named Dockerfile in the same folder that contains all the other folders and files (like src, pom.xml, etc.).\n\nIn the Dockerfile, define your base image by adding the following line:\n\nFROM eclipse-temurin:21.0.8_9-jdk-jammy\n\nNow, define the working directory by using the WORKDIR instruction. This will specify where future commands will run and the directory files will be copied inside the container image.\n\nWORKDIR /app\n\nCopy both the Maven wrapper script and your project's pom.xml file into the current working directory /app within the Docker container.\n\nCOPY .mvn/ .mvn\n\nCOPY mvnw pom.xml ./\n\nExecute a command within the container. It runs the ./mvnw dependency:go-offline command, which uses the Maven wrapper (./mvnw) to download all dependencies for your project without building the final JAR file (useful for faster builds).\n\nRUN ./mvnw dependency:go-offline\n\nCopy the src directory from your project on the host machine to the /app directory within the container.\n\nCOPY src ./src\n\nSet the default command to be executed when the container starts. This command instructs the container to run the Maven wrapper (./mvnw) with the spring-boot:run goal, which will build and execute your Spring Boot application.\n\nCMD [\"./mvnw\", \"spring-boot:run\"]\n\nAnd with that, you should have the following Dockerfile:\n\nFROM eclipse-temurin:21.0.8_9-jdk-jammy\n\nWORKDIR /app\n\nCOPY .mvn/ .mvn\n\nCOPY mvnw pom.xml ./\n\nRUN ./mvnw dependency:go-offline\n\nCOPY src ./src\n\nCMD [\"./mvnw\", \"spring-boot:run\"]\nBuild the container image\n\nExecute the following command to build the Docker image:\n\n$ docker build -t spring-helloworld .\n\n\nCheck the size of the Docker image by using the docker images command:\n\n$ docker images\n\n\nDoing so will produce output like the following:\n\nREPOSITORY          TAG       IMAGE ID       CREATED          SIZE\n\nspring-helloworld   latest    ff708d5ee194   3 minutes ago    880MB\n\n\nThis output shows that your image is 880MB in size. It contains the full JDK, Maven toolchain, and more. In production, you don‚Äôt need that in your final image.\n\nRun the Spring Boot application\n\nNow that you have an image built, it's time to run the container.\n\n$ docker run -p 8080:8080 spring-helloworld\n\n\nYou'll then see output similar to the following in the container log:\n\n[INFO] --- spring-boot:3.3.4:run (default-cli) @ spring-boot-docker ---\n\n[INFO] Attaching agents: []\n\n\n\n     .   ____          _            __ _ _\n\n    /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n\n   ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n\n    \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n\n     '  |____| .__|_| |_|_| |_\\__, | / / / /\n\n    =========|_|==============|___/=/_/_/_/\n\n\n\n    :: Spring Boot ::                (v3.3.4)\n\n\n\n2024-09-29T23:54:07.157Z  INFO 159 --- [spring-boot-docker] [           main]\n\nc.e.s.SpringBootDockerApplication        : Starting SpringBootDockerApplication using Java\n\n21.0.2 with PID 159 (/app/target/classes started by root in /app)\n\n ‚Ä¶.\n\nAccess your ‚ÄúHello World‚Äù page through your web browser at http://localhost:8080, or via this curl command:\n\n$ curl localhost:8080\n\nHello World\n\nUse multi-stage builds\n\nConsider the following Dockerfile:\n\nFROM eclipse-temurin:21.0.8_9-jdk-jammy AS builder\n\nWORKDIR /opt/app\n\nCOPY .mvn/ .mvn\n\nCOPY mvnw pom.xml ./\n\nRUN ./mvnw dependency:go-offline\n\nCOPY ./src ./src\n\nRUN ./mvnw clean install\n\n\n\nFROM eclipse-temurin:21.0.8_9-jre-jammy AS final\n\nWORKDIR /opt/app\n\nEXPOSE 8080\n\nCOPY --from=builder /opt/app/target/*.jar /opt/app/*.jar\n\nENTRYPOINT [\"java\", \"-jar\", \"/opt/app/*.jar\"]\n\nNotice that this Dockerfile has been split into two stages.\n\nThe first stage remains the same as the previous Dockerfile, providing a Java Development Kit (JDK) environment for building the application. This stage is given the name of builder.\n\nThe second stage is a new stage named final. It uses a slimmer eclipse-temurin:21.0.2_13-jre-jammy image, containing just the Java Runtime Environment (JRE) needed to run the application. This image provides a Java Runtime Environment (JRE) which is enough for running the compiled application (JAR file).\n\nFor production use, it's highly recommended that you produce a custom JRE-like runtime using jlink. JRE images are available for all versions of Eclipse Temurin, but jlink allows you to create a minimal runtime containing only the necessary Java modules for your application. This can significantly reduce the size and improve the security of your final image. Refer to this page for more information.\n\nWith multi-stage builds, a Docker build uses one base image for compilation, packaging, and unit tests and then a separate image for the application runtime. As a result, the final image is smaller in size since it doesn‚Äôt contain any development or debugging tools. By separating the build environment from the final runtime environment, you can significantly reduce the image size and increase the security of your final images.\n\nNow, rebuild your image and run your ready-to-use production build.\n\n$ docker build -t spring-helloworld-builder .\n\n\nThis command builds a Docker image named spring-helloworld-builder using the final stage from your Dockerfile file located in the current directory.\n\nNote\n\nIn your multi-stage Dockerfile, the final stage (final) is the default target for building. This means that if you don't explicitly specify a target stage using the --target flag in the docker build command, Docker will automatically build the last stage by default. You could use docker build -t spring-helloworld-builder --target builder . to build only the builder stage with the JDK environment.\n\nLook at the image size difference by using the docker images command:\n\n$ docker images\n\n\nYou'll get output similar to the following:\n\nspring-helloworld-builder latest    c5c76cb815c0   24 minutes ago      428MB\n\nspring-helloworld         latest    ff708d5ee194   About an hour ago   880MB\n\n\nYour final image is just 428 MB, compared to the original build size of 880 MB.\n\nBy optimizing each stage and only including what's necessary, you were able to significantly reduce the overall image size while still achieving the same functionality. This not only improves performance but also makes your Docker images more lightweight, more secure, and easier to manage.\n\nAdditional resources\nMulti-stage builds\nDockerfile best practices\nBase images\nSpring Boot Docker\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nTry it out\nCreate the Dockerfile\nBuild the container image\nRun the Spring Boot application\nUse multi-stage builds\nAdditional resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995024,
    "timestamp": "2026-02-07T06:29:19.591Z",
    "title": "Persisting container data | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/running-containers/persisting-container-data/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nRunning containers\nPublishing and exposing ports\nOverriding container defaults\nPersisting container data\nSharing local files with containers\nMulti-container applications\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nRunning containers\n/\nPersisting container data\nPersisting container data\nCopy as Markdown\nExplanation\n\nWhen a container starts, it uses the files and configuration provided by the image. Each container is able to create, modify, and delete files and does so without affecting any other containers. When the container is deleted, these file changes are also deleted.\n\nWhile this ephemeral nature of containers is great, it poses a challenge when you want to persist the data. For example, if you restart a database container, you might not want to start with an empty database. So, how do you persist files?\n\nContainer volumes\n\nVolumes are a storage mechanism that provide the ability to persist data beyond the lifecycle of an individual container. Think of it like providing a shortcut or symlink from inside the container to outside the container.\n\nAs an example, imagine you create a volume named log-data.\n\n$ docker volume create log-data\n\n\nWhen starting a container with the following command, the volume will be mounted (or attached) into the container at /logs:\n\n$ docker run -d -p 80:80 -v log-data:/logs docker/welcome-to-docker\n\n\nIf the volume log-data doesn't exist, Docker will automatically create it for you.\n\nWhen the container runs, all files it writes into the /logs folder will be saved in this volume, outside of the container. If you delete the container and start a new container using the same volume, the files will still be there.\n\nSharing files using volumes\n\nYou can attach the same volume to multiple containers to share files between containers. This might be helpful in scenarios such as log aggregation, data pipelines, or other event-driven applications.\n\nManaging volumes\n\nVolumes have their own lifecycle beyond that of containers and can grow quite large depending on the type of data and applications you‚Äôre using. The following commands will be helpful to manage volumes:\n\ndocker volume ls - list all volumes\ndocker volume rm <volume-name-or-id> - remove a volume (only works when the volume is not attached to any containers)\ndocker volume prune - remove all unused (unattached) volumes\nTry it out\n\nIn this guide, you'll practice creating and using volumes to persist data created by a Postgres container. When the database runs, it stores files into the /var/lib/postgresql directory. By attaching the volume here, you will be able to restart the container multiple times while keeping the data.\n\nUse volumes\n\nDownload and install Docker Desktop.\n\nStart a container using the Postgres image with the following command:\n\n$ docker run --name=db -e POSTGRES_PASSWORD=secret -d -v postgres_data:/var/lib/postgresql postgres:18\n\n\nThis will start the database in the background, configure it with a password, and attach a volume to the directory PostgreSQL will persist the database files.\n\nConnect to the database by using the following command:\n\n$ docker exec -ti db psql -U postgres\n\n\nIn the PostgreSQL command line, run the following to create a database table and insert two records:\n\nCREATE TABLE tasks (\n\n    id SERIAL PRIMARY KEY,\n\n    description VARCHAR(100)\n\n);\n\nINSERT INTO tasks (description) VALUES ('Finish work'), ('Have fun');\n\nVerify the data is in the database by running the following in the PostgreSQL command line:\n\nSELECT * FROM tasks;\n\nYou should get output that looks like the following:\n\n id | description\n\n----+-------------\n\n  1 | Finish work\n\n  2 | Have fun\n\n(2 rows)\n\nExit out of the PostgreSQL shell by running the following command:\n\n\\q\n\n\nStop and remove the database container. Remember that, even though the container has been deleted, the data is persisted in the postgres_data volume.\n\n$ docker stop db\n\n$ docker rm db\n\n\nStart a new container by running the following command, attaching the same volume with the persisted data:\n\n$ docker run --name=new-db -d -v postgres_data:/var/lib/postgresql postgres:18\n\n\nYou might have noticed that the POSTGRES_PASSWORD environment variable has been omitted. That‚Äôs because that variable is only used when bootstrapping a new database.\n\nVerify the database still has the records by running the following command:\n\n$ docker exec -ti new-db psql -U postgres -c \"SELECT * FROM tasks\"\n\nView volume contents\n\nThe Docker Desktop Dashboard provides the ability to view the contents of any volume, as well as the ability to export, import, empty, delete and clone volumes.\n\nOpen the Docker Desktop Dashboard and navigate to the Volumes view. In this view, you should see the postgres_data volume.\n\nSelect the postgres_data volume‚Äôs name.\n\nThe Stored Data tab shows the contents of the volume and provides the ability to navigate the files. The Container in-use tab displays the name of the container using the volume, the image name, the port number used by the container, and the target. A target is a path inside a container that gives access to the files in the volume. The Exports tab lets you export the volume. Double-clicking on a file will let you see the contents and make changes.\n\nRight-click on any file to save it or delete it.\n\nRemove volumes\n\nBefore removing a volume, it must not be attached to any containers. If you haven‚Äôt removed the previous container, do so with the following command (the -f will stop the container first and then remove it):\n\n$ docker rm -f new-db\n\n\nThere are a few methods to remove volumes, including the following:\n\nSelect the Delete Volume option on a volume in the Docker Desktop Dashboard.\n\nUse the docker volume rm command:\n\n$ docker volume rm postgres_data\n\n\nUse the docker volume prune command to remove all unused volumes:\n\n$ docker volume prune\n\nAdditional resources\n\nThe following resources will help you learn more about volumes:\n\nManage data in Docker\nVolumes\nVolume mounts\nNext steps\n\nNow that you have learned about persisting container data, it‚Äôs time to learn about sharing local files with containers.\n\nSharing local files with containers\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nContainer volumes\nManaging volumes\nTry it out\nUse volumes\nView volume contents\nRemove volumes\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995021,
    "timestamp": "2026-02-07T06:29:19.600Z",
    "title": "Overriding container defaults | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/running-containers/overriding-container-defaults/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nRunning containers\nPublishing and exposing ports\nOverriding container defaults\nPersisting container data\nSharing local files with containers\nMulti-container applications\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nRunning containers\n/\nOverriding container defaults\nOverriding container defaults\nCopy as Markdown\nExplanation\n\nWhen a Docker container starts, it executes an application or command. The container gets this executable (script or file) from its image‚Äôs configuration. Containers come with default settings that usually work well, but you can change them if needed. These adjustments help the container's program run exactly how you want it to.\n\nFor example, if you have an existing database container that listens on the standard port and you want to run a new instance of the same database container, then you might want to change the port settings the new container listens on so that it doesn‚Äôt conflict with the existing container. Sometimes you might want to increase the memory available to the container if the program needs more resources to handle a heavy workload or set the environment variables to provide specific configuration details the program needs to function properly.\n\nThe docker run command offers a powerful way to override these defaults and tailor the container's behavior to your liking. The command offers several flags that let you to customize container behavior on the fly.\n\nHere's a few ways you can achieve this.\n\nOverriding the network ports\n\nSometimes you might want to use separate database instances for development and testing purposes. Running these database instances on the same port might conflict. You can use the -p option in docker run to map container ports to host ports, allowing you to run the multiple instances of the container without any conflict.\n\n$ docker run -d -p HOST_PORT:CONTAINER_PORT postgres\n\nSetting environment variables\n\nThis option sets an environment variable foo inside the container with the value bar.\n\n$ docker run -e foo=bar postgres env\n\n\nYou will see output like the following:\n\nHOSTNAME=2042f2e6ebe4\n\nfoo=bar\n\nTip\n\nThe .env file acts as a convenient way to set environment variables for your Docker containers without cluttering your command line with numerous -e flags. To use a .env file, you can pass --env-file option with the docker run command.\n\n$ docker run --env-file .env postgres env\n\nRestricting the container to consume the resources\n\nYou can use the --memory and --cpus flags with the docker run command to restrict how much CPU and memory a container can use. For example, you can set a memory limit for the Python API container, preventing it from consuming excessive resources on your host. Here's the command:\n\n$ docker run -e POSTGRES_PASSWORD=secret --memory=\"512m\" --cpus=\"0.5\" postgres\n\n\nThis command limits container memory usage to 512 MB and defines the CPU quota of 0.5 for half a core.\n\nMonitor the real-time resource usage\n\nYou can use the docker stats command to monitor the real-time resource usage of running containers. This helps you understand whether the allocated resources are sufficient or need adjustment.\n\nBy effectively using these docker run flags, you can tailor your containerized application's behavior to fit your specific requirements.\n\nTry it out\n\nIn this hands-on guide, you'll see how to use the docker run command to override the container defaults.\n\nDownload and install Docker Desktop.\nRun multiple instances of the Postgres database\n\nStart a container using the Postgres image with the following command:\n\n$ docker run -d -e POSTGRES_PASSWORD=secret -p 5432:5432 postgres\n\n\nThis will start the Postgres database in the background, listening on the standard container port 5432 and mapped to port 5432 on the host machine.\n\nStart a second Postgres container mapped to a different port.\n\n$ docker run -d -e POSTGRES_PASSWORD=secret -p 5433:5432 postgres\n\n\nThis will start another Postgres container in the background, listening on the standard postgres port 5432 in the container, but mapped to port 5433 on the host machine. You override the host port just to ensure that this new container doesn't conflict with the existing running container.\n\nVerify that both containers are running by going to the Containers view in the Docker Desktop Dashboard.\n\nRun Postgres container in a controlled network\n\nBy default, containers automatically connect to a special network called a bridge network when you run them. This bridge network acts like a virtual bridge, allowing containers on the same host to communicate with each other while keeping them isolated from the outside world and other hosts. It's a convenient starting point for most container interactions. However, for specific scenarios, you might want more control over the network configuration.\n\nHere's where the custom network comes in. You create a custom network by passing --network flag with the docker run command. All containers without a --network flag are attached to the default bridge network.\n\nFollow the steps to see how to connect a Postgres container to a custom network.\n\nCreate a new custom network by using the following command:\n\n$ docker network create mynetwork\n\n\nVerify the network by running the following command:\n\n$ docker network ls\n\n\nThis command lists all networks, including the newly created \"mynetwork\".\n\nConnect Postgres to the custom network by using the following command:\n\n$ docker run -d -e POSTGRES_PASSWORD=secret -p 5434:5432 --network mynetwork postgres\n\n\nThis will start Postgres container in the background, mapped to the host port 5434 and attached to the mynetwork network. You passed the --network parameter to override the container default by connecting the container to custom Docker network for better isolation and communication with other containers. You can use docker network inspect command to see if the container is tied to this new bridge network.\n\nKey difference between default bridge and custom networks\n\nDNS resolution: By default, containers connected to the default bridge network can communicate with each other, but only by IP address. (unless you use --link option which is considered legacy). It is not recommended for production use due to the various technical shortcomings. On a custom network, containers can resolve each other by name or alias.\nIsolation: All containers without a --network specified are attached to the default bridge network, hence can be a risk, as unrelated containers are then able to communicate. Using a custom network provides a scoped network in which only containers attached to that network are able to communicate, hence providing better isolation.\nManage the resources\n\nBy default, containers are not limited in their resource usage. However, on shared systems, it's crucial to manage resources effectively. It's important not to let a running container consume too much of the host machine's memory.\n\nThis is where the docker run command shines again. It offers flags like --memory and --cpus to restrict how much CPU and memory a container can use.\n\n$ docker run -d -e POSTGRES_PASSWORD=secret --memory=\"512m\" --cpus=\".5\" postgres\n\n\nThe --cpus flag specifies the CPU quota for the container. Here, it's set to half a CPU core (0.5) whereas the --memory flag specifies the memory limit for the container. In this case, it's set to 512 MB.\n\nOverride the default CMD and ENTRYPOINT in Docker Compose\n\nSometimes, you might need to override the default commands (CMD) or entry points (ENTRYPOINT) defined in a Docker image, especially when using Docker Compose.\n\nCreate a compose.yml file with the following content:\n\nservices:\n\n  postgres:\n\n    image: postgres:18\n\n    entrypoint: [\"docker-entrypoint.sh\", \"postgres\"]\n\n    command: [\"-h\", \"localhost\", \"-p\", \"5432\"]\n\n    environment:\n\n      POSTGRES_PASSWORD: secret \n\nThe Compose file defines a service named postgres that uses the official Postgres image, sets an entrypoint script, and starts the container with password authentication.\n\nBring up the service by running the following command:\n\n$ docker compose up -d\n\n\nThis command starts the Postgres service defined in the Docker Compose file.\n\nVerify the authentication with Docker Desktop Dashboard.\n\nOpen the Docker Desktop Dashboard, select the Postgres container and select Exec to enter into the container shell. You can type the following command to connect to the Postgres database:\n\n# psql -U postgres\n\nNote\n\nThe PostgreSQL image sets up trust authentication locally so you may notice a password isn't required when connecting from localhost (inside the same container). However, a password will be required if connecting from a different host/container.\n\nOverride the default CMD and ENTRYPOINT with docker run\n\nYou can also override defaults directly using the docker run command with the following command:\n\n$ docker run -e POSTGRES_PASSWORD=secret postgres docker-entrypoint.sh -h localhost -p 5432\n\n\nThis command runs a Postgres container, sets an environment variable for password authentication, overrides the default startup commands and configures hostname and port mapping.\n\nAdditional resources\nWays to set environment variables with Compose\nWhat is a container\nNext steps\n\nNow that you have learned about overriding container defaults, it's time to learn how to persist container data.\n\nPersisting container data\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nOverriding the network ports\nSetting environment variables\nRestricting the container to consume the resources\nTry it out\nRun multiple instances of the Postgres database\nRun Postgres container in a controlled network\nManage the resources\nOverride the default CMD and ENTRYPOINT in Docker Compose\nOverride the default CMD and ENTRYPOINT with docker run\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995027,
    "timestamp": "2026-02-07T06:29:19.610Z",
    "title": "Sharing local files with containers | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/running-containers/sharing-local-files/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nRunning containers\nPublishing and exposing ports\nOverriding container defaults\nPersisting container data\nSharing local files with containers\nMulti-container applications\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nRunning containers\n/\nSharing local files with containers\nSharing local files with containers\nCopy as Markdown\nExplanation\n\nEach container has everything it needs to function with no reliance on any pre-installed dependencies on the host machine. Since containers run in isolation, they have minimal influence on the host and other containers. This isolation has a major benefit: containers minimize conflicts with the host system and other containers. However, this isolation also means containers can't directly access data on the host machine by default.\n\nConsider a scenario where you have a web application container that requires access to configuration settings stored in a file on your host system. This file may contain sensitive data such as database credentials or API keys. Storing such sensitive information directly within the container image poses security risks, especially during image sharing. To address this challenge, Docker offers storage options that bridge the gap between container isolation and your host machine's data.\n\nDocker offers two primary storage options for persisting data and sharing files between the host machine and containers: volumes and bind mounts.\n\nVolume versus bind mounts\n\nIf you want to ensure that data generated or modified inside the container persists even after the container stops running, you would opt for a volume. See Persisting container data to learn more about volumes and their use cases.\n\nIf you have specific files or directories on your host system that you want to directly share with your container, like configuration files or development code, then you would use a bind mount. It's like opening a direct portal between your host and container for sharing. Bind mounts are ideal for development environments where real-time file access and sharing between the host and container are crucial.\n\nSharing files between a host and container\n\nBoth -v (or --volume) and --mount flags used with the docker run command let you share files or directories between your local machine (host) and a Docker container. However, there are some key differences in their behavior and usage.\n\nThe -v flag is simpler and more convenient for basic volume or bind mount operations. If the host location doesn‚Äôt exist when using -v or --volume, a directory will be automatically created.\n\nImagine you're a developer working on a project. You have a source directory on your development machine where your code resides. When you compile or build your code, the generated artifacts (compiled code, executables, images, etc.) are saved in a separate subdirectory within your source directory. In the following examples, this subdirectory is /HOST/PATH. Now you want these build artifacts to be accessible within a Docker container running your application. Additionally, you want the container to automatically access the latest build artifacts whenever you rebuild your code.\n\nHere's a way to use docker run to start a container using a bind mount and map it to the container file location.\n\n$ docker run -v /HOST/PATH:/CONTAINER/PATH -it nginx\n\n\nThe --mount flag offers more advanced features and granular control, making it suitable for complex mount scenarios or production deployments. If you use --mount to bind-mount a file or directory that doesn't yet exist on the Docker host, the docker run command doesn't automatically create it for you but generates an error.\n\n$ docker run --mount type=bind,source=/HOST/PATH,target=/CONTAINER/PATH,readonly nginx\n\nNote\n\nDocker recommends using the --mount syntax instead of -v. It provides better control over the mounting process and avoids potential issues with missing directories.\n\nFile permissions for Docker access to host files\n\nWhen using bind mounts, it's crucial to ensure that Docker has the necessary permissions to access the host directory. To grant read/write access, you can use the :ro flag (read-only) or :rw (read-write) with the -v or --mount flag during container creation. For example, the following command grants read-write access permission.\n\n$ docker run -v HOST-DIRECTORY:/CONTAINER-DIRECTORY:rw nginx\n\n\nRead-only bind mounts let the container access the mounted files on the host for reading, but it can't change or delete the files. With read-write bind mounts, containers can modify or delete mounted files, and these changes or deletions will also be reflected on the host system. Read-only bind mounts ensures that files on the host can't be accidentally modified or deleted by a container.\n\nSynchronized File Share\n\nAs your codebase grows larger, traditional methods of file sharing like bind mounts may become inefficient or slow, especially in development environments where frequent access to files is necessary. Synchronized file shares improve bind mount performance by leveraging synchronized filesystem caches. This optimization ensures that file access between the host and virtual machine (VM) is fast and efficient.\n\nTry it out\n\nIn this hands-on guide, you‚Äôll practice how to create and use a bind mount to share files between a host and a container.\n\nRun a container\n\nDownload and install Docker Desktop.\n\nStart a container using the httpd image with the following command:\n\n$ docker run -d -p 8080:80 --name my_site httpd:2.4\n\n\nThis will start the httpd service in the background, and publish the webpage to port 8080 on the host.\n\nOpen the browser and access http://localhost:8080 or use the curl command to verify if it's working fine or not.\n\n$ curl localhost:8080\n\nUse a bind mount\n\nUsing a bind mount, you can map the configuration file on your host computer to a specific location within the container. In this example, you‚Äôll see how to change the look and feel of the webpage by using bind mount:\n\nDelete the existing container by using the Docker Desktop Dashboard:\n\nCreate a new directory called public_html on your host system.\n\n$ mkdir public_html\n\n\nNavigate into the newly created directory public_html and create a file called index.html with the following content. This is a basic HTML document that creates a simple webpage that welcomes you with a friendly whale.\n\n<!DOCTYPE html>\n\n<html lang=\"en\">\n\n<head>\n\n<meta charset=\"UTF-8\">\n\n<title> My Website with a Whale & Docker!</title>\n\n</head>\n\n<body>\n\n<h1>Whalecome!!</h1>\n\n<p>Look! There's a friendly whale greeting you!</p>\n\n<pre id=\"docker-art\">\n\n   ##         .\n\n  ## ## ##        ==\n\n ## ## ## ## ##    ===\n\n /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===\n\n{                       /  ===-\n\n\\______ O           __/\n\n\\    \\         __/\n\n \\____\\_______/\n\n\n\nHello from Docker!\n\n</pre>\n\n</body>\n\n</html>\n\nIt's time to run the container. The --mount and -v examples produce the same result. You can't run them both unless you remove the my_site container after running the first one.\n\n-v --mount\n$ docker run -d --name my_site -p 8080:80 -v .:/usr/local/apache2/htdocs/ httpd:2.4\n\nTip\n\nWhen using the -v or --mount flag in Windows PowerShell, you need to provide the absolute path to your directory instead of just ./. This is because PowerShell handles relative paths differently from bash (commonly used in Mac and Linux environments).\n\nWith everything now up and running, you should be able to access the site via http://localhost:8080 and find a new webpage that welcomes you with a friendly whale.\n\nAccess the file on the Docker Desktop Dashboard\n\nYou can view the mounted files inside a container by selecting the container's Files tab and then selecting a file inside the /usr/local/apache2/htdocs/ directory. Then, select Open file editor.\n\nDelete the file on the host and verify the file is also deleted in the container. You will find that the files no longer exist under Files in the Docker Desktop Dashboard.\n\nRecreate the HTML file on the host system and see that file re-appears under the Files tab under Containers on the Docker Desktop Dashboard. By now, you will be able to access the site too.\n\nStop your container\n\nThe container continues to run until you stop it.\n\nGo to the Containers view in the Docker Desktop Dashboard.\n\nLocate the container you'd like to stop.\n\nSelect the Stop action in the Actions column.\n\nAdditional resources\n\nThe following resources will help you learn more about bind mounts:\n\nManage data in Docker\nVolumes\nBind mounts\nRunning containers\nTroubleshoot storage errors\nPersisting container data\nNext steps\n\nNow that you have learned about sharing local files with containers, it‚Äôs time to learn about multi-container applications.\n\nMulti-container applications\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nVolume versus bind mounts\nSharing files between a host and container\nFile permissions for Docker access to host files\nTry it out\nRun a container\nUse a bind mount\nAccess the file on the Docker Desktop Dashboard\nStop your container\nAdditional resources\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995033,
    "timestamp": "2026-02-07T06:29:19.616Z",
    "title": "Docker workshop | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\nOverview of the Docker workshop\nCopy as Markdown\n\nThis 45-minute workshop contains step-by-step instructions on how to get started with Docker. This workshop shows you how to:\n\nBuild and run an image as a container.\nShare images using Docker Hub.\nDeploy Docker applications using multiple containers with a database.\nRun applications using Docker Compose.\nNote\n\nFor a quick introduction to Docker and the benefits of containerizing your applications, see Getting started.\n\nWhat is a container?\n\nA container is a sandboxed process running on a host machine that is isolated from all other processes running on that host machine. That isolation leverages kernel namespaces and cgroups, features that have been in Linux for a long time. Docker makes these capabilities approachable and easy to use. To summarize, a container:\n\nIs a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI.\nCan be run on local machines, virtual machines, or deployed to the cloud.\nIs portable (and can be run on any OS).\nIs isolated from other containers and runs its own software, binaries, configurations, etc.\n\nIf you're familiar with chroot, then think of a container as an extended version of chroot. The filesystem comes from the image. However, a container adds additional isolation not available when using chroot.\n\nWhat is an image?\n\nA running container uses an isolated filesystem. This isolated filesystem is provided by an image, and the image must contain everything needed to run an application - all dependencies, configurations, scripts, binaries, etc. The image also contains other configurations for the container, such as environment variables, a default command to run, and other metadata.\n\nNext steps\n\nIn this section, you learned about containers and images.\n\nNext, you'll containerize a simple application and get hands-on with the concepts.\n\nContainerize an application\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat is a container?\nWhat is an image?\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995030,
    "timestamp": "2026-02-07T06:29:19.627Z",
    "title": "Multi-container applications | Docker Docs",
    "url": "https://docs.docker.com/get-started/docker-concepts/running-containers/multi-container-applications/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nThe basics\nBuilding images\nRunning containers\nPublishing and exposing ports\nOverriding container defaults\nPersisting container data\nSharing local files with containers\nMulti-container applications\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nDocker concepts\n/\nRunning containers\n/\nMulti-container applications\nMulti-container applications\nCopy as Markdown\nExplanation\n\nStarting up a single-container application is easy. For example, a Python script that performs a specific data processing task runs within a container with all its dependencies. Similarly, a Node.js application serving a static website with a small API endpoint can be effectively containerized with all its necessary libraries and dependencies. However, as applications grow in size, managing them as individual containers becomes more difficult.\n\nImagine the data processing Python script needs to connect to a database. Suddenly, you're now managing not just the script but also a database server within the same container. If the script requires user logins, you'll need an authentication mechanism, further bloating the container size.\n\nOne best practice for containers is that each container should do one thing and do it well. While there are exceptions to this rule, avoid the tendency to have one container do multiple things.\n\nNow you might ask, \"Do I need to run these containers separately? If I run them separately, how shall I connect them all together?\"\n\nWhile docker run is a convenient tool for launching containers, it becomes difficult to manage a growing application stack with it. Here's why:\n\nImagine running several docker run commands (frontend, backend, and database) with different configurations for development, testing, and production environments. It's error-prone and time-consuming.\nApplications often rely on each other. Manually starting containers in a specific order and managing network connections become difficult as the stack expands.\nEach application needs its docker run command, making it difficult to scale individual services. Scaling the entire application means potentially wasting resources on components that don't need a boost.\nPersisting data for each application requires separate volume mounts or configurations within each docker run command. This creates a scattered data management approach.\nSetting environment variables for each application through separate docker run commands is tedious and error-prone.\n\nThat's where Docker Compose comes to the rescue.\n\nDocker Compose defines your entire multi-container application in a single YAML file called compose.yml. This file specifies configurations for all your containers, their dependencies, environment variables, and even volumes and networks. With Docker Compose:\n\nYou don't need to run multiple docker run commands. All you need to do is define your entire multi-container application in a single YAML file. This centralizes configuration and simplifies management.\nYou can run containers in a specific order and manage network connections easily.\nYou can simply scale individual services up or down within the multi-container setup. This allows for efficient allocation based on real-time needs.\nYou can implement persistent volumes with ease.\nIt's easy to set environment variables once in your Docker Compose file.\n\nBy leveraging Docker Compose for running multi-container setups, you can build complex applications with modularity, scalability, and consistency at their core.\n\nTry it out\n\nIn this hands-on guide, you'll first see how to build and run a counter web application based on Node.js, an Nginx reverse proxy, and a Redis database using the docker run commands. You‚Äôll also see how you can simplify the entire deployment process using Docker Compose.\n\nSet up\n\nGet the sample application. If you have Git, you can clone the repository for the sample application. Otherwise, you can download the sample application. Choose one of the following options.\n\nClone with git Download\n\nUse the following command in a terminal to clone the sample application repository.\n\n$ git clone https://github.com/dockersamples/nginx-node-redis\n\n\nNavigate into the nginx-node-redis directory:\n\n$ cd nginx-node-redis\n\n\nInside this directory, you'll find two sub-directories - nginx and web.\n\nDownload and install Docker Desktop.\n\nBuild the images\n\nNavigate into the nginx directory to build the image by running the following command:\n\n$ docker build -t nginx .\n\n\nNavigate into the web directory and run the following command to build the first web image:\n\n$ docker build -t web .\n\nRun the containers\n\nBefore you can run a multi-container application, you need to create a network for them all to communicate through. You can do so using the docker network create command:\n\n$ docker network create sample-app\n\n\nStart the Redis container by running the following command, which will attach it to the previously created network and create a network alias (useful for DNS lookups):\n\n$ docker run -d  --name redis --network sample-app --network-alias redis redis\n\n\nStart the first web container by running the following command:\n\n$ docker run -d --name web1 -h web1 --network sample-app --network-alias web1 web\n\n\nStart the second web container by running the following:\n\n$ docker run -d --name web2 -h web2 --network sample-app --network-alias web2 web\n\n\nStart the Nginx container by running the following command:\n\n$ docker run -d --name nginx --network sample-app  -p 80:80 nginx\n\nNote\n\nNginx is typically used as a reverse proxy for web applications, routing traffic to backend servers. In this case, it routes to the Node.js backend containers (web1 or web2).\n\nVerify the containers are up by running the following command:\n\n$ docker ps\n\n\nYou will see output like the following:\n\nCONTAINER ID   IMAGE     COMMAND                  CREATED              STATUS              PORTS                NAMES\n\n2cf7c484c144   nginx     \"/docker-entrypoint.‚Ä¶\"   9 seconds ago        Up 8 seconds        0.0.0.0:80->80/tcp   nginx\n\n7a070c9ffeaa   web       \"docker-entrypoint.s‚Ä¶\"   19 seconds ago       Up 18 seconds                            web2\n\n6dc6d4e60aaf   web       \"docker-entrypoint.s‚Ä¶\"   34 seconds ago       Up 33 seconds                            web1\n\n008e0ecf4f36   redis     \"docker-entrypoint.s‚Ä¶\"   About a minute ago   Up About a minute   6379/tcp             redis\n\nIf you look at the Docker Desktop Dashboard, you can see the containers and dive deeper into their configuration.\n\nWith everything up and running, you can open http://localhost in your browser to see the site. Refresh the page several times to see the host that‚Äôs handling the request and the total number of requests:\n\nweb2: Number of visits is: 9\n\nweb1: Number of visits is: 10\n\nweb2: Number of visits is: 11\n\nweb1: Number of visits is: 12\n\nNote\n\nYou might have noticed that Nginx, acting as a reverse proxy, likely distributes incoming requests in a round-robin fashion between the two backend containers. This means each request might be directed to a different container (web1 and web2) on a rotating basis. The output shows consecutive increments for both the web1 and web2 containers and the actual counter value stored in Redis is updated only after the response is sent back to the client.\n\nYou can use the Docker Desktop Dashboard to remove the containers by selecting the containers and selecting the Delete button.\n\nSimplify the deployment using Docker Compose\n\nDocker Compose provides a structured and streamlined approach for managing multi-container deployments. As stated earlier, with Docker Compose, you don‚Äôt need to run multiple docker run commands. All you need to do is define your entire multi-container application in a single YAML file called compose.yml. Let‚Äôs see how it works.\n\nNavigate to the root of the project directory. Inside this directory, you'll find a file named compose.yml. This YAML file is where all the magic happens. It defines all the services that make up your application, along with their configurations. Each service specifies its image, ports, volumes, networks, and any other settings necessary for its functionality.\n\nUse the docker compose up command to start the application:\n\n$ docker compose up -d --build\n\n\nWhen you run this command, you should see output similar to the following:\n\n ‚àö Network nginx-node-redis_default   Created                                                                                                   0.0s\n\n ‚àö Container nginx-node-redis-web2-1  Created                                                                                                   0.1s\n\n ‚àö Container nginx-node-redis-web1-1  Created                                                                                                   0.1s\n\n ‚àö Container nginx-node-redis-redis-1 Created                                                                                                   0.1s\n\n ‚àö Container nginx-node-redis-nginx-1 Created   \n\n\nIf you look at the Docker Desktop Dashboard, you can see the containers and dive deeper into their configuration.\n\nAlternatively, you can use the Docker Desktop Dashboard to remove the containers by selecting the application stack and selecting the Delete button.\n\nIn this guide, you learned how easy it is to use Docker Compose to start and stop a multi-container application compared to docker run which is error-prone and difficult to manage.\n\nAdditional resources\ndocker container run CLI reference\nWhat is Docker Compose\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExplanation\nTry it out\nSet up\nBuild the images\nRun the containers\nSimplify the deployment using Docker Compose\nAdditional resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved.\nBy clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.\nCookies Settings Reject All Accept All Cookies"
  },
  {
    "id": 258995039,
    "timestamp": "2026-02-07T06:29:19.631Z",
    "title": "Part 2: Update the application | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/03_updating_app/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\n/\nPart 2: Update the application\nUpdate the application\nCopy as Markdown\n\nIn part 1, you containerized a todo application. In this part, you'll update the application and image. You'll also learn how to stop and remove a container.\n\nUpdate the source code\n\nIn the following steps, you'll change the \"empty text\" when you don't have any todo list items to \"You have no todo items yet! Add one above!\"\n\nIn the src/static/js/app.js file, update line 56 to use the new empty text.\n\n- <p className=\"text-center\">No items yet! Add one above!</p>\n\n+ <p className=\"text-center\">You have no todo items yet! Add one above!</p>\n\n\nBuild your updated version of the image, using the docker build command.\n\n$ docker build -t getting-started .\n\n\nStart a new container using the updated code.\n\n$ docker run -dp 127.0.0.1:3000:3000 getting-started\n\n\nYou probably saw an error like this:\n\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint laughing_burnell \n\n(bb242b2ca4d67eba76e79474fb36bb5125708ebdabd7f45c8eaf16caaabde9dd): Bind for 127.0.0.1:3000 failed: port is already allocated.\n\n\nThe error occurred because you aren't able to start the new container while your old container is still running. The reason is that the old container is already using the host's port 3000 and only one process on the machine (containers included) can listen to a specific port. To fix this, you need to remove the old container.\n\nRemove the old container\n\nTo remove a container, you first need to stop it. Once it has stopped, you can remove it. You can remove the old container using the CLI or Docker Desktop's graphical interface. Choose the option that you're most comfortable with.\n\nCLI Docker Desktop\nRemove a container using the CLI\n\nGet the ID of the container by using the docker ps command.\n\n$ docker ps\n\n\nUse the docker stop command to stop the container. Replace <the-container-id> with the ID from docker ps.\n\n$ docker stop <the-container-id>\n\n\nOnce the container has stopped, you can remove it by using the docker rm command.\n\n$ docker rm <the-container-id>\n\nNote\n\nYou can stop and remove a container in a single command by adding the force flag to the docker rm command. For example: docker rm -f <the-container-id>\n\nStart the updated app container\n\nNow, start your updated app using the docker run command.\n\n$ docker run -dp 127.0.0.1:3000:3000 getting-started\n\n\nRefresh your browser on http://localhost:3000 and you should see your updated help text.\n\nSummary\n\nIn this section, you learned how to update and rebuild an image, as well as how to stop and remove a container.\n\nRelated information:\n\ndocker CLI reference\nNext steps\n\nNext, you'll learn how to share images with others.\n\nShare the application\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUpdate the source code\nRemove the old container\nStart the updated app container\nSummary\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995036,
    "timestamp": "2026-02-07T06:29:19.635Z",
    "title": "Part 1: Containerize an application | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/02_our_app/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\n/\nPart 1: Containerize an application\nContainerize an application\nCopy as Markdown\n\nFor the rest of this guide, you'll be working with a simple todo list manager that runs on Node.js. If you're not familiar with Node.js, don't worry. This guide doesn't require any prior experience with JavaScript.\n\nPrerequisites\nYou have installed the latest version of Docker Desktop.\nYou have installed a Git client.\nYou have an IDE or a text editor to edit files. Docker recommends using Visual Studio Code.\nGet the app\n\nBefore you can run the application, you need to get the application source code onto your machine.\n\nClone the getting-started-app repository using the following command:\n\n$ git clone https://github.com/docker/getting-started-app.git\n\n\nView the contents of the cloned repository. You should see the following files and sub-directories.\n\n‚îú‚îÄ‚îÄ getting-started-app/\n\n‚îÇ ‚îú‚îÄ‚îÄ .dockerignore\n\n‚îÇ ‚îú‚îÄ‚îÄ package.json\n\n‚îÇ ‚îú‚îÄ‚îÄ package-lock.json   \n\n‚îÇ ‚îú‚îÄ‚îÄ README.md\n\n‚îÇ ‚îú‚îÄ‚îÄ spec/\n\n‚îÇ ‚îú‚îÄ‚îÄ src/\nBuild the app's image\n\nTo build the image, you'll need to use a Dockerfile. A Dockerfile is simply a text-based file with no file extension that contains a script of instructions. Docker uses this script to build a container image.\n\nIn the getting-started-app directory, the same location as the package.json file, create a file named Dockerfile with the following contents:\n\n# syntax=docker/dockerfile:1\n\n\n\nFROM node:24-alpine\n\nWORKDIR /app\n\nCOPY . .\n\nRUN npm install --omit=dev\n\nCMD [\"node\", \"src/index.js\"]\n\nEXPOSE 3000\n\nThis Dockerfile does the following:\n\nUses node:24-alpine as the base image, a lightweight Linux image with Node.js pre-installed\nSets /app as the working directory\nCopies source code into the image\nInstalls the necessary dependencies\nSpecifies the command to start the application\nDocuments that the app listens on port 3000\n\nBuild the image using the following commands:\n\nIn the terminal, make sure you're in the getting-started-app directory. Replace /path/to/getting-started-app with the path to your getting-started-app directory.\n\n$ cd /path/to/getting-started-app\n\n\nBuild the image.\n\n$ docker build -t getting-started .\n\n\nThe docker build command uses the Dockerfile to build a new image. You might have noticed that Docker downloaded a lot of \"layers\". This is because you instructed the builder that you wanted to start from the node:24-alpine image. But, since you didn't have that on your machine, Docker needed to download the image.\n\nAfter Docker downloaded the image, the instructions from the Dockerfile copied in your application and used npm to install your application's dependencies.\n\nFinally, the -t flag tags your image. Think of this as a human-readable name for the final image. Since you named the image getting-started, you can refer to that image when you run a container.\n\nThe . at the end of the docker build command tells Docker that it should look for the Dockerfile in the current directory.\n\nStart an app container\n\nNow that you have an image, you can run the application in a container using the docker run command.\n\nRun your container using the docker run command and specify the name of the image you just created:\n\n$ docker run -d -p 127.0.0.1:3000:3000 getting-started\n\n\nThe -d flag (short for --detach) runs the container in the background. This means that Docker starts your container and returns you to the terminal prompt. Also, it does not display logs in the terminal.\n\nThe -p flag (short for --publish) creates a port mapping between the host and the container. The -p flag takes a string value in the format of HOST:CONTAINER, where HOST is the address on the host, and CONTAINER is the port on the container. The command publishes the container's port 3000 to 127.0.0.1:3000 (localhost:3000) on the host. Without the port mapping, you wouldn't be able to access the application from the host.\n\nAfter a few seconds, open your web browser to http://localhost:3000. You should see your app.\n\nAdd an item or two and see that it works as you expect. You can mark items as complete and remove them. Your frontend is successfully storing items in the backend.\n\nAt this point, you have a running todo list manager with a few items.\n\nIf you take a quick look at your containers, you should see at least one container running that's using the getting-started image and on port 3000. To see your containers, you can use the CLI or Docker Desktop's graphical interface.\n\nCLI Docker Desktop\n\nRun the docker ps command in a terminal to list your containers.\n\n$ docker ps\n\n\nOutput similar to the following should appear.\n\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                      NAMES\n\ndf784548666d        getting-started     \"docker-entrypoint.s‚Ä¶\"   2 minutes ago       Up 2 minutes        127.0.0.1:3000->3000/tcp   priceless_mcclintock\n\nSummary\n\nIn this section, you learned the basics about creating a Dockerfile to build an image. Once you built an image, you started a container and saw the running app.\n\nRelated information:\n\nDockerfile reference\ndocker CLI reference\nNext steps\n\nNext, you're going to make a modification to your app and learn how to update your running application with a new image. Along the way, you'll learn a few other useful commands.\n\nUpdate the application\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nGet the app\nBuild the app's image\nStart an app container\nSummary\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995045,
    "timestamp": "2026-02-07T06:29:19.646Z",
    "title": "Part 4: Persist the DB | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/05_persisting_data/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\n/\nPart 4: Persist the DB\nPersist the DB\nCopy as Markdown\n\nIn case you didn't notice, your todo list is empty every single time you launch the container. Why is this? In this part, you'll dive into how the container is working.\n\nThe container's filesystem\n\nWhen a container runs, it uses the various layers from an image for its filesystem. Each container also gets its own \"scratch space\" to create/update/remove files. Any changes won't be seen in another container, even if they're using the same image.\n\nSee this in practice\n\nTo see this in action, you're going to start two containers. In one container, you'll create a file. In the other container, you'll check whether that same file exists.\n\nStart an Alpine container and create a new file in it.\n\n$ docker run --rm alpine touch greeting.txt\n\nTip\n\nAny commands you specify after the image name (in this case, alpine) are executed inside the container. In this case, the command touch greeting.txt puts a file named greeting.txt on the container's filesystem.\n\nRun a new Alpine container and use the stat command to check whether the file exists.\n\n$ docker run --rm alpine stat greeting.txt\n\n\nYou should see output similar to the following that indicates the file does not exist in the new container.\n\nstat: can't stat 'greeting.txt': No such file or directory\n\n\nThe greeting.txt file created by the first container did not exist in the second container. That is because the writeable \"top layer\" of each container is isolated. Even though both containers shared the same underlying layers that make up the base image, the writable layer is unique to each container.\n\nContainer volumes\n\nWith the previous experiment, you saw that each container starts from the image definition each time it starts. While containers can create, update, and delete files, those changes are lost when you remove the container and Docker isolates all changes to that container. With volumes, you can change all of this.\n\nVolumes provide the ability to connect specific filesystem paths of the container back to the host machine. If you mount a directory in the container, changes in that directory are also seen on the host machine. If you mount that same directory across container restarts, you'd see the same files.\n\nThere are two main types of volumes. You'll eventually use both, but you'll start with volume mounts.\n\nPersist the todo data\n\nBy default, the todo app stores its data in a SQLite database at /etc/todos/todo.db in the container's filesystem. If you're not familiar with SQLite, no worries! It's simply a relational database that stores all the data in a single file. While this isn't the best for large-scale applications, it works for small demos. You'll learn how to switch this to a different database engine later.\n\nWith the database being a single file, if you can persist that file on the host and make it available to the next container, it should be able to pick up where the last one left off. By creating a volume and attaching (often called \"mounting\") it to the directory where you stored the data, you can persist the data. As your container writes to the todo.db file, it will persist the data to the host in the volume.\n\nAs mentioned, you're going to use a volume mount. Think of a volume mount as an opaque bucket of data. Docker fully manages the volume, including the storage location on disk. You only need to remember the name of the volume.\n\nCreate a volume and start the container\n\nYou can create the volume and start the container using the CLI or Docker Desktop's graphical interface.\n\nCLI Docker Desktop\n\nCreate a volume by using the docker volume create command.\n\n$ docker volume create todo-db\n\n\nStop and remove the todo app container once again with docker rm -f <id>, as it is still running without using the persistent volume.\n\nStart the todo app container, but add the --mount option to specify a volume mount. Give the volume a name, and mount it to /etc/todos in the container, which captures all files created at the path.\n\n$ docker run -dp 127.0.0.1:3000:3000 --mount type=volume,src=todo-db,target=/etc/todos getting-started\n\nNote\n\nIf you're using Git Bash, you must use different syntax for this command.\n\n$ docker run -dp 127.0.0.1:3000:3000 --mount type=volume,src=todo-db,target=//etc/todos getting-started\n\n\nFor more details about Git Bash's syntax differences, see Working with Git Bash.\n\nVerify that the data persists\n\nOnce the container starts up, open the app and add a few items to your todo list.\n\nStop and remove the container for the todo app. Use Docker Desktop or docker ps to get the ID and then docker rm -f <id> to remove it.\n\nStart a new container using the previous steps.\n\nOpen the app. You should see your items still in your list.\n\nGo ahead and remove the container when you're done checking out your list.\n\nYou've now learned how to persist data.\n\nDive into the volume\n\nA lot of people frequently ask \"Where is Docker storing my data when I use a volume?\" If you want to know, you can use the docker volume inspect command.\n\n$ docker volume inspect todo-db\n\n\nYou should see output like the following:\n\n[\n\n    {\n\n        \"CreatedAt\": \"2019-09-26T02:18:36Z\",\n\n        \"Driver\": \"local\",\n\n        \"Labels\": {},\n\n        \"Mountpoint\": \"/var/lib/docker/volumes/todo-db/_data\",\n\n        \"Name\": \"todo-db\",\n\n        \"Options\": {},\n\n        \"Scope\": \"local\"\n\n    }\n\n]\n\n\nThe Mountpoint is the actual location of the data on the disk. Note that on most machines, you will need to have root access to access this directory from the host.\n\nSummary\n\nIn this section, you learned how to persist container data.\n\nRelated information:\n\ndocker CLI reference\nVolumes\nNext steps\n\nNext, you'll learn how you can develop your app more efficiently using bind mounts.\n\nUse bind mounts\n\nEdit this page\n\nRequest changes\n\nTable of contents\nThe container's filesystem\nSee this in practice\nContainer volumes\nPersist the todo data\nCreate a volume and start the container\nVerify that the data persists\nDive into the volume\nSummary\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995042,
    "timestamp": "2026-02-07T06:29:19.648Z",
    "title": "Part 3: Share the application | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/04_sharing_app/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\n/\nPart 3: Share the application\nShare the application\nCopy as Markdown\n\nNow that you've built an image, you can share it. To share Docker images, you have to use a Docker registry. The default registry is Docker Hub and is where all of the images you've used have come from.\n\nDocker ID\n\nA Docker ID lets you access Docker Hub, which is the world's largest library and community for container images. Create a Docker ID for free if you don't have one.\n\nCreate a repository\n\nTo push an image, you first need to create a repository on Docker Hub.\n\nSign up or Sign in to Docker Hub.\n\nSelect the Create Repository button.\n\nFor the repository name, use getting-started. Make sure the Visibility is Public.\n\nSelect Create.\n\nIn the following image, you can see an example Docker command from Docker Hub. This command will push to this repository.\n\nPush the image\n\nLet's try to push the image to Docker Hub.\n\nIn the command line, run the following command:\n\ndocker push docker/getting-started\n\n\nYou'll see an error like this:\n\n$ docker push docker/getting-started\n\nThe push refers to repository [docker.io/docker/getting-started]\n\nAn image does not exist locally with the tag: docker/getting-started\n\n\nThis failure is expected because the image isn't tagged correctly yet. Docker is looking for an image name docker/getting-started, but your local image is still named getting-started.\n\nYou can confirm this by running:\n\ndocker image ls\n\n\nTo fix this, first sign in to Docker Hub using your Docker ID: docker login YOUR-USER-NAME.\n\nUse the docker tag command to give the getting-started image a new name. Replace YOUR-USER-NAME with your Docker ID.\n\n$ docker tag getting-started YOUR-USER-NAME/getting-started\n\n\nNow run the docker push command again. If you're copying the value from Docker Hub, you can drop the tagname part, as you didn't add a tag to the image name. If you don't specify a tag, Docker uses a tag called latest.\n\n$ docker push YOUR-USER-NAME/getting-started\n\nRun the image on a new instance\n\nNow that your image has been built and pushed into a registry, you can run your app on any machine that has Docker installed. Try pulling and running your image on another computer or a cloud instance.\n\nSummary\n\nIn this section, you learned how to share your images by pushing them to a registry. You then went to a brand new instance and were able to run the freshly pushed image. This is quite common in CI pipelines, where the pipeline will create the image and push it to a registry and then the production environment can use the latest version of the image.\n\nRelated information:\n\ndocker CLI reference\nMulti-platform images\nDocker Hub overview\nNext steps\n\nIn the next section, you'll learn how to persist data in your containerized application.\n\nPersist the DB\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCreate a repository\nPush the image\nRun the image on a new instance\nSummary\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995048,
    "timestamp": "2026-02-07T06:29:19.661Z",
    "title": "Part 5: Use bind mounts | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/06_bind_mounts/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\n/\nPart 5: Use bind mounts\nUse bind mounts\nCopy as Markdown\n\nIn part 4, you used a volume mount to persist the data in your database. A volume mount is a great choice when you need somewhere persistent to store your application data.\n\nA bind mount is another type of mount, which lets you share a directory from the host's filesystem into the container. When working on an application, you can use a bind mount to mount source code into the container. The container sees the changes you make to the code immediately, as soon as you save a file. This means that you can run processes in the container that watch for filesystem changes and respond to them.\n\nIn this chapter, you'll see how you can use bind mounts and a tool called nodemon to watch for file changes, and then restart the application automatically. There are equivalent tools in most other languages and frameworks.\n\nQuick volume type comparisons\n\nThe following are examples of a named volume and a bind mount using --mount:\n\nNamed volume: type=volume,src=my-volume,target=/usr/local/data\nBind mount: type=bind,src=/path/to/data,target=/usr/local/data\n\nThe following table outlines the main differences between volume mounts and bind mounts.\n\n\tNamed volumes\tBind mounts\nHost location\tDocker chooses\tYou decide\nPopulates new volume with container contents\tYes\tNo\nSupports Volume Drivers\tYes\tNo\nTrying out bind mounts\n\nBefore looking at how you can use bind mounts for developing your application, you can run a quick experiment to get a practical understanding of how bind mounts work.\n\nVerify that your getting-started-app directory is in a directory defined in Docker Desktop's file sharing setting. This setting defines which parts of your filesystem you can share with containers. For details about accessing the setting, see File sharing.\n\nNote\n\nThe File sharing tab is only available in Hyper-V mode, because the files are automatically shared in WSL 2 mode and Windows container mode.\n\nOpen a terminal and change directory to the getting-started-app directory.\n\nRun the following command to start bash in an ubuntu container with a bind mount.\n\nMac / Linux Command Prompt Git Bash PowerShell\n$ docker run -it --mount type=bind,src=.,target=/src ubuntu bash\n\n\nThe --mount type=bind option tells Docker to create a bind mount, where src is the current working directory on your host machine (getting-started-app), and target is where that directory should appear inside the container (/src).\n\nAfter running the command, Docker starts an interactive bash session in the root directory of the container's filesystem.\n\nroot@ac1237fad8db:/# pwd\n\n/\n\nroot@ac1237fad8db:/# ls\n\nbin   dev  home  media  opt   root  sbin  srv  tmp  var\n\nboot  etc  lib   mnt    proc  run   src   sys  usr\n\n\nChange directory to the src directory.\n\nThis is the directory that you mounted when starting the container. Listing the contents of this directory displays the same files as in the getting-started-app directory on your host machine.\n\nroot@ac1237fad8db:/# cd src\n\nroot@ac1237fad8db:/src# ls\n\nDockerfile  node_modules  package.json  package-lock.json  spec  src  \n\n\nCreate a new file named myfile.txt.\n\nroot@ac1237fad8db:/src# touch myfile.txt\n\nroot@ac1237fad8db:/src# ls\n\nDockerfile  myfile.txt  node_modules  package.json  package-lock.json  spec  src  \n\n\nOpen the getting-started-app directory on the host and observe that the myfile.txt file is in the directory.\n\n‚îú‚îÄ‚îÄ getting-started-app/\n\n‚îÇ ‚îú‚îÄ‚îÄ Dockerfile\n\n‚îÇ ‚îú‚îÄ‚îÄ myfile.txt\n\n‚îÇ ‚îú‚îÄ‚îÄ node_modules/\n\n‚îÇ ‚îú‚îÄ‚îÄ package.json\n\n‚îÇ ‚îú‚îÄ‚îÄ package-lock.json\n\n‚îÇ ‚îú‚îÄ‚îÄ spec/\n\n‚îÇ ‚îî‚îÄ‚îÄ src/\n\nFrom the host, delete the myfile.txt file.\n\nIn the container, list the contents of the app directory once more. Observe that the file is now gone.\n\nroot@ac1237fad8db:/src# ls\n\nDockerfile  node_modules  package.json  package-lock.json spec  src  \n\n\nStop the interactive container session with Ctrl + D.\n\nThat's all for a brief introduction to bind mounts. This procedure demonstrated how files are shared between the host and the container, and how changes are immediately reflected on both sides. Now you can use bind mounts to develop software.\n\nDevelopment containers\n\nUsing bind mounts is common for local development setups. The advantage is that the development machine doesn‚Äôt need to have all of the build tools and environments installed. With a single docker run command, Docker pulls dependencies and tools.\n\nRun your app in a development container\n\nThe following steps describe how to run a development container with a bind mount that does the following:\n\nMount your source code into the container\nInstall all dependencies\nStart nodemon to watch for filesystem changes\n\nYou can use the CLI or Docker Desktop to run your container with a bind mount.\n\nMac / Linux CLI PowerShell CLI Command Prompt CLI Git Bash CLI Docker Desktop\n\nMake sure you don't have any getting-started containers currently running.\n\nRun the following command from the getting-started-app directory.\n\n$ docker run -dp 127.0.0.1:3000:3000 \\\n\n    -w /app --mount type=bind,src=.,target=/app \\\n\n    node:24-alpine \\\n\n    sh -c \"npm install && npm run dev\"\n\n\nThe following is a breakdown of the command:\n\n-dp 127.0.0.1:3000:3000 - same as before. Run in detached (background) mode and create a port mapping\n-w /app - sets the \"working directory\" or the current directory that the command will run from\n--mount type=bind,src=.,target=/app - bind mount the current directory from the host into the /app directory in the container\nnode:24-alpine - the image to use. Note that this is the base image for your app from the Dockerfile\nsh -c \"npm install && npm run dev\" - the command. You're starting a shell using sh (alpine doesn't have bash) and running npm install to install packages and then running npm run dev to start the development server. If you look in the package.json, you'll see that the dev script starts nodemon.\n\nYou can watch the logs using docker logs <container-id>. You'll know you're ready to go when you see this:\n\n$ docker logs -f <container-id>\n\nnodemon -L src/index.js\n\n[nodemon] 2.0.20\n\n[nodemon] to restart at any time, enter `rs`\n\n[nodemon] watching path(s): *.*\n\n[nodemon] watching extensions: js,mjs,json\n\n[nodemon] starting `node src/index.js`\n\nUsing sqlite database at /etc/todos/todo.db\n\nListening on port 3000\n\n\nWhen you're done watching the logs, exit out by hitting Ctrl+C.\n\nDevelop your app with the development container\n\nUpdate your app on your host machine and see the changes reflected in the container.\n\nIn the src/static/js/app.js file, on line 109, change the \"Add Item\" button to simply say \"Add\":\n\n- {submitting ? 'Adding...' : 'Add Item'}\n\n+ {submitting ? 'Adding...' : 'Add'}\n\n\nSave the file.\n\nRefresh the page in your web browser, and you should see the change reflected almost immediately because of the bind mount. Nodemon detects the change and restarts the server. It might take a few seconds for the Node server to restart. If you get an error, try refreshing after a few seconds.\n\nFeel free to make any other changes you'd like to make. Each time you make a change and save a file, the change is reflected in the container because of the bind mount. When Nodemon detects a change, it restarts the app inside the container automatically. When you're done, stop the container and build your new image using:\n\n$ docker build -t getting-started .\n\nSummary\n\nAt this point, you can persist your database and see changes in your app as you develop without rebuilding the image.\n\nIn addition to volume mounts and bind mounts, Docker also supports other mount types and storage drivers for handling more complex and specialized use cases.\n\nRelated information:\n\ndocker CLI reference\nManage data in Docker\nNext steps\n\nIn order to prepare your app for production, you need to migrate your database from working in SQLite to something that can scale a little better. For simplicity, you'll keep using a relational database and switch your application to use MySQL. But, how should you run MySQL? How do you allow the containers to talk to each other? You'll learn about that in the next section.\n\nMulti container apps\n\nEdit this page\n\nRequest changes\n\nTable of contents\nQuick volume type comparisons\nTrying out bind mounts\nDevelopment containers\nRun your app in a development container\nDevelop your app with the development container\nSummary\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995054,
    "timestamp": "2026-02-07T06:29:19.679Z",
    "title": "Part 7: Use Docker Compose | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/08_using_compose/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\n/\nPart 7: Use Docker Compose\nUse Docker Compose\nCopy as Markdown\n\nDocker Compose is a tool that helps you define and share multi-container applications. With Compose, you can create a YAML file to define the services and with a single command, you can spin everything up or tear it all down.\n\nThe big advantage of using Compose is you can define your application stack in a file, keep it at the root of your project repository (it's now version controlled), and easily enable someone else to contribute to your project. Someone would only need to clone your repository and start the app using Compose. In fact, you might see quite a few projects on GitHub/GitLab doing exactly this now.\n\nCreate the Compose file\n\nIn the getting-started-app directory, create a file named compose.yaml.\n\n‚îú‚îÄ‚îÄ getting-started-app/\n\n‚îÇ ‚îú‚îÄ‚îÄ Dockerfile\n\n‚îÇ ‚îú‚îÄ‚îÄ compose.yaml\n\n‚îÇ ‚îú‚îÄ‚îÄ node_modules/\n\n‚îÇ ‚îú‚îÄ‚îÄ package.json\n\n‚îÇ ‚îú‚îÄ‚îÄ package-lock.json\n\n‚îÇ ‚îú‚îÄ‚îÄ spec/\n\n‚îÇ ‚îî‚îÄ‚îÄ src/\nDefine the app service\n\nIn part 6, you used the following command to start the application service.\n\n$ docker run -dp 127.0.0.1:3000:3000 \\\n\n  -w /app -v \".:/app\" \\\n\n  --network todo-app \\\n\n  -e MYSQL_HOST=mysql \\\n\n  -e MYSQL_USER=root \\\n\n  -e MYSQL_PASSWORD=secret \\\n\n  -e MYSQL_DB=todos \\\n\n  node:24-alpine \\\n\n  sh -c \"npm install && npm run dev\"\n\n\nYou'll now define this service in the compose.yaml file.\n\nOpen compose.yaml in a text or code editor, and start by defining the name and image of the first service (or container) you want to run as part of your application. The name will automatically become a network alias, which will be useful when defining your MySQL service.\n\nservices:\n\n  app:\n\n    image: node:24-alpine\n\nTypically, you will see command close to the image definition, although there is no requirement on ordering. Add the command to your compose.yaml file.\n\nservices:\n\n  app:\n\n    image: node:24-alpine\n\n    command: sh -c \"npm install && npm run dev\"\n\nNow migrate the -p 127.0.0.1:3000:3000 part of the command by defining the ports for the service.\n\nservices:\n\n  app:\n\n    image: node:24-alpine\n\n    command: sh -c \"npm install && npm run dev\"\n\n    ports:\n\n      - 127.0.0.1:3000:3000\n\nNext, migrate both the working directory (-w /app) and the volume mapping (-v \".:/app\") by using the working_dir and volumes definitions.\n\nOne advantage of Docker Compose volume definitions is you can use relative paths from the current directory.\n\nservices:\n\n  app:\n\n    image: node:24-alpine\n\n    command: sh -c \"npm install && npm run dev\"\n\n    ports:\n\n      - 127.0.0.1:3000:3000\n\n    working_dir: /app\n\n    volumes:\n\n      - ./:/app\n\nFinally, you need to migrate the environment variable definitions using the environment key.\n\nservices:\n\n  app:\n\n    image: node:24-alpine\n\n    command: sh -c \"npm install && npm run dev\"\n\n    ports:\n\n      - 127.0.0.1:3000:3000\n\n    working_dir: /app\n\n    volumes:\n\n      - ./:/app\n\n    environment:\n\n      MYSQL_HOST: mysql\n\n      MYSQL_USER: root\n\n      MYSQL_PASSWORD: secret\n\n      MYSQL_DB: todos\nDefine the MySQL service\n\nNow, it's time to define the MySQL service. The command that you used for that container was the following:\n\n$ docker run -d \\\n\n  --network todo-app --network-alias mysql \\\n\n  -v todo-mysql-data:/var/lib/mysql \\\n\n  -e MYSQL_ROOT_PASSWORD=secret \\\n\n  -e MYSQL_DATABASE=todos \\\n\n  mysql:8.0\n\n\nFirst define the new service and name it mysql so it automatically gets the network alias. Also specify the image to use as well.\n\n\n\nservices:\n\n  app:\n\n    # The app service definition\n\n  mysql:\n\n    image: mysql:8.0\n\nNext, define the volume mapping. When you ran the container with docker run, Docker created the named volume automatically. However, that doesn't happen when running with Compose. You need to define the volume in the top-level volumes: section and then specify the mountpoint in the service config. By simply providing only the volume name, the default options are used.\n\nservices:\n\n  app:\n\n    # The app service definition\n\n  mysql:\n\n    image: mysql:8.0\n\n    volumes:\n\n      - todo-mysql-data:/var/lib/mysql\n\n\n\nvolumes:\n\n  todo-mysql-data:\n\nFinally, you need to specify the environment variables.\n\nservices:\n\n  app:\n\n    # The app service definition\n\n  mysql:\n\n    image: mysql:8.0\n\n    volumes:\n\n      - todo-mysql-data:/var/lib/mysql\n\n    environment:\n\n      MYSQL_ROOT_PASSWORD: secret\n\n      MYSQL_DATABASE: todos\n\n\n\nvolumes:\n\n  todo-mysql-data:\n\nAt this point, your complete compose.yaml should look like this:\n\nservices:\n\n  app:\n\n    image: node:24-alpine\n\n    command: sh -c \"npm install && npm run dev\"\n\n    ports:\n\n      - 127.0.0.1:3000:3000\n\n    working_dir: /app\n\n    volumes:\n\n      - ./:/app\n\n    environment:\n\n      MYSQL_HOST: mysql\n\n      MYSQL_USER: root\n\n      MYSQL_PASSWORD: secret\n\n      MYSQL_DB: todos\n\n\n\n  mysql:\n\n    image: mysql:8.0\n\n    volumes:\n\n      - todo-mysql-data:/var/lib/mysql\n\n    environment:\n\n      MYSQL_ROOT_PASSWORD: secret\n\n      MYSQL_DATABASE: todos\n\n\n\nvolumes:\n\n  todo-mysql-data:\nRun the application stack\n\nNow that you have your compose.yaml file, you can start your application.\n\nMake sure no other copies of the containers are running first. Use docker ps to list the containers and docker rm -f <ids> to remove them.\n\nStart up the application stack using the docker compose up command. Add the -d flag to run everything in the background.\n\n$ docker compose up -d\n\n\nWhen you run the previous command, you should see output like the following:\n\nCreating network \"app_default\" with the default driver\n\nCreating volume \"app_todo-mysql-data\" with default driver\n\nCreating app_app_1   ... done\n\nCreating app_mysql_1 ... done\n\nYou'll notice that Docker Compose created the volume as well as a network. By default, Docker Compose automatically creates a network specifically for the application stack (which is why you didn't define one in the Compose file).\n\nLook at the logs using the docker compose logs -f command. You'll see the logs from each of the services interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues. The -f flag follows the log, so will give you live output as it's generated.\n\nIf you have run the command already, you'll see output that looks like this:\n\nmysql_1  | 2019-10-03T03:07:16.083639Z 0 [Note] mysqld: ready for connections.\n\nmysql_1  | Version: '8.0.31'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)\n\napp_1    | Connected to mysql db at host mysql\n\napp_1    | Listening on port 3000\n\nThe service name is displayed at the beginning of the line (often colored) to help distinguish messages. If you want to view the logs for a specific service, you can add the service name to the end of the logs command (for example, docker compose logs -f app).\n\nAt this point, you should be able to open your app in your browser on http://localhost:3000 and see it running.\n\nSee the app stack in Docker Desktop Dashboard\n\nIf you look at the Docker Desktop Dashboard, you'll see that there is a group named getting-started-app. This is the project name from Docker Compose and used to group the containers together. By default, the project name is simply the name of the directory that the compose.yaml was located in.\n\nIf you expand the stack, you'll see the two containers you defined in the Compose file. The names are also a little more descriptive, as they follow the pattern of <service-name>-<replica-number>. So, it's very easy to quickly see what container is your app and which container is the mysql database.\n\nTear it all down\n\nWhen you're ready to tear it all down, simply run docker compose down or hit the trash can on the Docker Desktop Dashboard for the entire app. The containers will stop and the network will be removed.\n\nWarning\n\nBy default, named volumes in your compose file are not removed when you run docker compose down. If you want to remove the volumes, you need to add the --volumes flag.\n\nThe Docker Desktop Dashboard does not remove volumes when you delete the app stack.\n\nSummary\n\nIn this section, you learned about Docker Compose and how it helps you simplify the way you define and share multi-service applications.\n\nRelated information:\n\nCompose overview\nCompose file reference\nCompose CLI reference\nNext steps\n\nNext, you'll learn about a few best practices you can use to improve your Dockerfile.\n\nImage-building best practices\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCreate the Compose file\nDefine the app service\nDefine the MySQL service\nRun the application stack\nSee the app stack in Docker Desktop Dashboard\nTear it all down\nSummary\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995051,
    "timestamp": "2026-02-07T06:29:19.682Z",
    "title": "Part 6: Multi-container apps | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/07_multi_container/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\n/\nPart 6: Multi-container apps\nMulti container apps\nCopy as Markdown\n\nUp to this point, you've been working with single container apps. But, now you will add MySQL to the application stack. The following question often arises - \"Where will MySQL run? Install it in the same container or run it separately?\" In general, each container should do one thing and do it well. The following are a few reasons to run the container separately:\n\nThere's a good chance you'd have to scale APIs and front-ends differently than databases.\nSeparate containers let you version and update versions in isolation.\nWhile you may use a container for the database locally, you may want to use a managed service for the database in production. You don't want to ship your database engine with your app then.\nRunning multiple processes will require a process manager (the container only starts one process), which adds complexity to container startup/shutdown.\n\nAnd there are more reasons. So, like the following diagram, it's best to run your app in multiple containers.\n\nContainer networking\n\nRemember that containers, by default, run in isolation and don't know anything about other processes or containers on the same machine. So, how do you allow one container to talk to another? The answer is networking. If you place the two containers on the same network, they can talk to each other.\n\nStart MySQL\n\nThere are two ways to put a container on a network:\n\nAssign the network when starting the container.\nConnect an already running container to a network.\n\nIn the following steps, you'll create the network first and then attach the MySQL container at startup.\n\nCreate the network.\n\n$ docker network create todo-app\n\n\nStart a MySQL container and attach it to the network. You're also going to define a few environment variables that the database will use to initialize the database. To learn more about the MySQL environment variables, see the \"Environment Variables\" section in the MySQL Docker Hub listing.\n\nMac / Linux / Git Bash PowerShell Command Prompt\n$ docker run -d \\\n\n    --network todo-app --network-alias mysql \\\n\n    -v todo-mysql-data:/var/lib/mysql \\\n\n    -e MYSQL_ROOT_PASSWORD=secret \\\n\n    -e MYSQL_DATABASE=todos \\\n\n    mysql:8.0\n\n\nIn the previous command, you can see the --network-alias flag. In a later section, you'll learn more about this flag.\n\nTip\n\nYou'll notice a volume named todo-mysql-data in the above command that is mounted at /var/lib/mysql, which is where MySQL stores its data. However, you never ran a docker volume create command. Docker recognizes you want to use a named volume and creates one automatically for you.\n\nTo confirm you have the database up and running, connect to the database and verify that it connects.\n\n$ docker exec -it <mysql-container-id> mysql -u root -p\n\n\nWhen the password prompt comes up, type in secret. In the MySQL shell, list the databases and verify you see the todos database.\n\nmysql> SHOW DATABASES;\n\n\nYou should see output that looks like this:\n\n+--------------------+\n\n| Database           |\n\n+--------------------+\n\n| information_schema |\n\n| mysql              |\n\n| performance_schema |\n\n| sys                |\n\n| todos              |\n\n+--------------------+\n\n5 rows in set (0.00 sec)\n\nExit the MySQL shell to return to the shell on your machine.\n\nmysql> exit\n\n\nYou now have a todos database and it's ready for you to use.\n\nConnect to MySQL\n\nNow that you know MySQL is up and running, you can use it. But, how do you use it? If you run another container on the same network, how do you find the container? Remember that each container has its own IP address.\n\nTo answer the questions above and better understand container networking, you're going to make use of the nicolaka/netshoot container, which ships with a lot of tools that are useful for troubleshooting or debugging networking issues.\n\nStart a new container using the nicolaka/netshoot image. Make sure to connect it to the same network.\n\n$ docker run -it --network todo-app nicolaka/netshoot\n\n\nInside the container, you're going to use the dig command, which is a useful DNS tool. You're going to look up the IP address for the hostname mysql.\n\n$ dig mysql\n\n\nYou should get output like the following.\n\n; <<>> DiG 9.18.8 <<>> mysql\n\n;; global options: +cmd\n\n;; Got answer:\n\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 32162\n\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n\n\n\n;; QUESTION SECTION:\n\n;mysql.\t\t\t\tIN\tA\n\n\n\n;; ANSWER SECTION:\n\nmysql.\t\t\t600\tIN\tA\t172.23.0.2\n\n\n\n;; Query time: 0 msec\n\n;; SERVER: 127.0.0.11#53(127.0.0.11)\n\n;; WHEN: Tue Oct 01 23:47:24 UTC 2019\n\n;; MSG SIZE  rcvd: 44\n\nIn the \"ANSWER SECTION\", you will see an A record for mysql that resolves to 172.23.0.2 (your IP address will most likely have a different value). While mysql isn't normally a valid hostname, Docker was able to resolve it to the IP address of the container that had that network alias. Remember, you used the --network-alias earlier.\n\nWhat this means is that your app only simply needs to connect to a host named mysql and it'll talk to the database.\n\nRun your app with MySQL\n\nThe todo app supports the setting of a few environment variables to specify MySQL connection settings. They are:\n\nMYSQL_HOST - the hostname for the running MySQL server\nMYSQL_USER - the username to use for the connection\nMYSQL_PASSWORD - the password to use for the connection\nMYSQL_DB - the database to use once connected\nNote\n\nWhile using env vars to set connection settings is generally accepted for development, it's highly discouraged when running applications in production. Diogo Monica, a former lead of security at Docker, wrote a fantastic blog post explaining why.\n\nA more secure mechanism is to use the secret support provided by your container orchestration framework. In most cases, these secrets are mounted as files in the running container. You'll see many apps (including the MySQL image and the todo app) also support env vars with a _FILE suffix to point to a file containing the variable.\n\nAs an example, setting the MYSQL_PASSWORD_FILE var will cause the app to use the contents of the referenced file as the connection password. Docker doesn't do anything to support these env vars. Your app will need to know to look for the variable and get the file contents.\n\nYou can now start your dev-ready container.\n\nSpecify each of the previous environment variables, as well as connect the container to your app network. Make sure that you are in the getting-started-app directory when you run this command.\n\nMac / Linux PowerShell Command Prompt Git Bash\n$ docker run -dp 127.0.0.1:3000:3000 \\\n\n  -w /app -v \".:/app\" \\\n\n  --network todo-app \\\n\n  -e MYSQL_HOST=mysql \\\n\n  -e MYSQL_USER=root \\\n\n  -e MYSQL_PASSWORD=secret \\\n\n  -e MYSQL_DB=todos \\\n\n  node:24-alpine \\\n\n  sh -c \"npm install && npm run dev\"\n\n\nIf you look at the logs for the container (docker logs -f <container-id>), you should see a message similar to the following, which indicates it's using the mysql database.\n\n[nodemon] 3.1.11\n\n[nodemon] to restart at any time, enter `rs`\n\n[nodemon] watching path(s): *.*\n\n[nodemon] watching extensions: js,mjs,cjs,json\n\n[nodemon] starting `node src/index.js`\n\nWaiting for mysql:3306.\n\nConnected!\n\nConnected to mysql db at host mysql\n\nListening on port 3000\n\n\nOpen the app in your browser and add a few items to your todo list.\n\nConnect to the mysql database and prove that the items are being written to the database. Remember, the password is secret.\n\n$ docker exec -it <mysql-container-id> mysql -p todos\n\n\nAnd in the mysql shell, run the following:\n\nmysql> select * from todo_items;\n\n+--------------------------------------+--------------------+-----------+\n\n| id                                   | name               | completed |\n\n+--------------------------------------+--------------------+-----------+\n\n| c906ff08-60e6-44e6-8f49-ed56a0853e85 | Do amazing things! |         0 |\n\n| 2912a79e-8486-4bc3-a4c5-460793a575ab | Be awesome!        |         0 |\n\n+--------------------------------------+--------------------+-----------+\n\n\nYour table will look different because it has your items. But, you should see them stored there.\n\nSummary\n\nAt this point, you have an application that now stores its data in an external database running in a separate container. You learned a little bit about container networking and service discovery using DNS.\n\nRelated information:\n\ndocker CLI reference\nNetworking overview\nNext steps\n\nThere's a good chance you are starting to feel a little overwhelmed with everything you need to do to start up this application. You have to create a network, start containers, specify all of the environment variables, expose ports, and more. That's a lot to remember and it's certainly making things harder to pass along to someone else.\n\nIn the next section, you'll learn about Docker Compose. With Docker Compose, you can share your application stacks in a much easier way and let others spin them up with a single, simple command.\n\nUse Docker Compose\n\nEdit this page\n\nRequest changes\n\nTable of contents\nContainer networking\nStart MySQL\nConnect to MySQL\nRun your app with MySQL\nSummary\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995057,
    "timestamp": "2026-02-07T06:29:19.738Z",
    "title": "Part 8: Image-building best practices | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/09_image_best/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\n/\nPart 8: Image-building best practices\nImage-building best practices\nCopy as Markdown\nImage layering\n\nUsing the docker image history command, you can see the command that was used to create each layer within an image.\n\nUse the docker image history command to see the layers in the getting-started image you created.\n\n$ docker image history getting-started\n\n\nYou should get output that looks something like the following.\n\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n\na78a40cbf866        18 seconds ago      /bin/sh -c #(nop)  CMD [\"node\" \"src/index.j‚Ä¶    0B                  \n\nf1d1808565d6        19 seconds ago      /bin/sh -c npm install --omit=dev               85.4MB              \n\na2c054d14948        36 seconds ago      /bin/sh -c #(nop) COPY dir:5dc710ad87c789593‚Ä¶   198kB               \n\n9577ae713121        37 seconds ago      /bin/sh -c #(nop) WORKDIR /app                  0B                  \n\nb95baba1cfdb        13 days ago         /bin/sh -c #(nop)  CMD [\"node\"]                 0B                  \n\n<missing>           13 days ago         /bin/sh -c #(nop)  ENTRYPOINT [\"docker-entry‚Ä¶   0B                  \n\n<missing>           13 days ago         /bin/sh -c #(nop) COPY file:238737301d473041‚Ä¶   116B                \n\n<missing>           13 days ago         /bin/sh -c apk add --no-cache --virtual .bui‚Ä¶   5.35MB              \n\n<missing>           13 days ago         /bin/sh -c addgroup -g 1000 node     && addu‚Ä¶   74.3MB              \n\n<missing>           13 days ago         /bin/sh -c #(nop)  ENV NODE_VERSION=12.14.1     0B                  \n\n<missing>           13 days ago         /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B                  \n\n<missing>           13 days ago         /bin/sh -c #(nop) ADD file:e69d441d729412d24‚Ä¶   5.59MB   \n\nEach of the lines represents a layer in the image. The display here shows the base at the bottom with the newest layer at the top. Using this, you can also quickly see the size of each layer, helping diagnose large images.\n\nYou'll notice that several of the lines are truncated. If you add the --no-trunc flag, you'll get the full output.\n\n$ docker image history --no-trunc getting-started\n\nLayer caching\n\nNow that you've seen the layering in action, there's an important lesson to learn to help decrease build times for your container images. Once a layer changes, all downstream layers have to be recreated as well.\n\nLook at the following Dockerfile you created for the getting started app.\n\n# syntax=docker/dockerfile:1\n\nFROM node:24-alpine\n\nWORKDIR /app\n\nCOPY . .\n\nRUN npm install --omit=dev\n\nCMD [\"node\", \"src/index.js\"]\n\nEXPOSE 3000\n\nGoing back to the image history output, you see that each command in the Dockerfile becomes a new layer in the image. You might remember that when you made a change to the image, the dependencies had to be reinstalled. It doesn't make much sense to ship around the same dependencies every time you build.\n\nTo fix it, you need to restructure your Dockerfile to help support the caching of the dependencies. For Node-based applications, those dependencies are defined in the package.json file. You can copy only that file in first, install the dependencies, and then copy in everything else. Then, you only recreate the dependencies if there was a change to the package.json.\n\nUpdate the Dockerfile to copy in the package.json first, install dependencies, and then copy everything else in.\n\n# syntax=docker/dockerfile:1\n\nFROM node:24-alpine\n\nWORKDIR /app\n\nCOPY package.json package-lock.json ./\n\nRUN npm install --omit=dev\n\nCOPY . .\n\nCMD [\"node\", \"src/index.js\"]\n\nBuild a new image using docker build.\n\n$ docker build -t getting-started .\n\n\nYou should see output like the following.\n\n[+] Building 16.1s (10/10) FINISHED\n\n=> [internal] load build definition from Dockerfile\n\n=> => transferring dockerfile: 175B\n\n=> [internal] load .dockerignore\n\n=> => transferring context: 2B\n\n=> [internal] load metadata for docker.io/library/node:24-alpine\n\n=> [internal] load build context\n\n=> => transferring context: 53.37MB\n\n=> [1/5] FROM docker.io/library/node:24-alpine\n\n=> CACHED [2/5] WORKDIR /app\n\n=> [3/5] COPY package.json package-lock.json ./\n\n=> [4/5] RUN npm install --omit=dev\n\n=> [5/5] COPY . .\n\n=> exporting to image\n\n=> => exporting layers\n\n=> => writing image     sha256:d6f819013566c54c50124ed94d5e66c452325327217f4f04399b45f94e37d25\n\n=> => naming to docker.io/library/getting-started\n\nNow, make a change to the src/static/index.html file. For example, change the <title> to \"The Awesome Todo App\".\n\nBuild the Docker image now using docker build -t getting-started . again. This time, your output should look a little different.\n\n[+] Building 1.2s (10/10) FINISHED\n\n=> [internal] load build definition from Dockerfile\n\n=> => transferring dockerfile: 37B\n\n=> [internal] load .dockerignore\n\n=> => transferring context: 2B\n\n=> [internal] load metadata for docker.io/library/node:24-alpine\n\n=> [internal] load build context\n\n=> => transferring context: 450.43kB\n\n=> [1/5] FROM docker.io/library/node:24-alpine\n\n=> CACHED [2/5] WORKDIR /app\n\n=> CACHED [3/5] COPY package.json package-lock.json ./\n\n=> CACHED [4/5] RUN npm install\n\n=> [5/5] COPY . .\n\n=> exporting to image\n\n=> => exporting layers\n\n=> => writing image     sha256:91790c87bcb096a83c2bd4eb512bc8b134c757cda0bdee4038187f98148e2eda\n\n=> => naming to docker.io/library/getting-started\n\nFirst off, you should notice that the build was much faster. And, you'll see that several steps are using previously cached layers. Pushing and pulling this image and updates to it will be much faster as well.\n\nMulti-stage builds\n\nMulti-stage builds are an incredibly powerful tool to help use multiple stages to create an image. There are several advantages for them:\n\nSeparate build-time dependencies from runtime dependencies\nReduce overall image size by shipping only what your app needs to run\nMaven/Tomcat example\n\nWhen building Java-based applications, you need a JDK to compile the source code to Java bytecode. However, that JDK isn't needed in production. Also, you might be using tools like Maven or Gradle to help build the app. Those also aren't needed in your final image. Multi-stage builds help.\n\n# syntax=docker/dockerfile:1\n\nFROM maven AS build\n\nWORKDIR /app\n\nCOPY . .\n\nRUN mvn package\n\n\n\nFROM tomcat\n\nCOPY --from=build /app/target/file.war /usr/local/tomcat/webapps \n\nIn this example, you use one stage (called build) to perform the actual Java build using Maven. In the second stage (starting at FROM tomcat), you copy in files from the build stage. The final image is only the last stage being created, which can be overridden using the --target flag.\n\nReact example\n\nWhen building React applications, you need a Node environment to compile the JS code (typically JSX), SASS stylesheets, and more into static HTML, JS, and CSS. If you aren't doing server-side rendering, you don't even need a Node environment for your production build. You can ship the static resources in a static nginx container.\n\n# syntax=docker/dockerfile:1\n\nFROM node:24-alpine AS build\n\nWORKDIR /app\n\nCOPY package* ./\n\nRUN npm install\n\nCOPY public ./public\n\nCOPY src ./src\n\nRUN npm run build\n\n\n\nFROM nginx:alpine\n\nCOPY --from=build /app/build /usr/share/nginx/html\n\nIn the previous Dockerfile example, it uses the node:24-alpine image to perform the build (maximizing layer caching) and then copies the output into an nginx container.\n\nTips\n\nThis React example is for illustration purposes. The getting-started todo app is a Node.js backend application, not a React frontend.\n\nSummary\n\nIn this section, you learned a few image building best practices, including layer caching and multi-stage builds.\n\nRelated information:\n\nDockerfile reference\nDockerfile best practices\nNext steps\n\nIn the next section, you'll learn about additional resources you can use to continue learning about containers.\n\nWhat next\n\nEdit this page\n\nRequest changes\n\nTable of contents\nImage layering\nLayer caching\nMulti-stage builds\nMaven/Tomcat example\nReact example\nSummary\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995063,
    "timestamp": "2026-02-07T06:29:19.738Z",
    "title": "Educational resources | Docker Docs",
    "url": "https://docs.docker.com/get-started/resources/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nEducational resources\nHome\n/\nGet started\n/\nEducational resources\nEducational resources\nCopy as Markdown\n\nDocker and the broader community of Docker experts have put together many different ways to get further training and hands-on experience with Docker. Expand your understanding of Docker and Kubernetes with these additional free and paid resources.\n\nDocker Training\n\nExpand your knowledge on all things Docker with basic to advanced trainings from Docker experts.\n\nYou can find recorded content at your own convenience, or register for a live session to participate in Q&A.\n\nHosted labs\n\nThese self-paced and hands-on workshops use a free, hosted environment (Play with Kubernetes) that doesn't require any installation. Follow along and learn more about Kubernetes.\n\nKubernetes Workshop\n\n* Labs are free but require registration with a Docker ID.\n\nSelf-guided tutorials\n\nCreated by experts in the Docker community, these free tutorials provide guided step-by-step workflows for working with the Docker platform.\n\nIntegrating Docker with Your IDE\n\nJava Development: Eclipse\nJava Development: IntelliJ\nJava Development: Netbeans\nLive Debugging Node.js with Docker and Visual Studio Code\n\nWindows Containers\n\nWindows Container Setup\nWindows Container Basics\nWindows Containers Multi-Container Applications\nBooks\n\nIf books are your preferred learning style, check out these written by the Docker Captains. Docker Captain is a distinction that Docker awards to select members of the community that are both experts in their field and are committed to sharing their Docker knowledge with others.\n\nLearn Docker in a Month of Lunches, Elton Stoneman. Use the code stonemanpc for a 40% discount.\nDocker on Windows: From 101 to Production with Docker on Windows, Elton Stoneman\nLearn Kubernetes in a Month of Lunches, Elton Stoneman. Use the code stonemanpc for a 40% discount.\nDocker in Action 2nd Edition Jeff Nickoloff, Oct 2019\nThe Kubernetes Book, Nigel Poulton, Nov 2018\nDocker Deep Dive, Nigel Poulton, 2024 Edition\n[Portuguese] Docker para desenvolvedores (2017) by Rafael Gomes\n[Spanish] √ârase una vez Docker, Manuel Morej√≥n, March 2023\n[Spanish] √ârase una vez Kubernetes, Manuel Morej√≥n, Jan 2022\nCLI cheat sheet\n\nThe Docker CLI cheat sheet features the common Docker CLI commands for easy reference. It covers working with Images, Containers, Docker Hub, and other general purpose commands.\n\nSelf-Paced online learning\n\nA number of Docker Captains have also created video courses on Docker and Kubernetes.\n\nBret Fisher: Docker Mastery, Docker Swarm Mastery, Docker Mastery for Node.js Projects\nElton Stoneman: Docker for .NET Apps - on Linux and Windows. Includes the discount code 644ABCBC33F474541885.\nNick Janetakis Dive into Docker, Docker for DevOps\nNigel Poulton: Kubernetes 101, Getting Started with Kubernetes, Docker and Kubernetes: The Big Picture, Kubernetes Deep Dive, Docker Deep Dive\nArun Gupta: Docker for Java Developers\nAjeet Singh Raina: Docker and Kubernetes Labs\n[French] Luc Juggery: Introduction to Kubernetes, The Docker Platform\n\n* Many of the courses are fee-based\n\nCommunity-translated docs\nNote\n\nThe following section contains a subset of Docker docs that are translated by community members. This is not an officially translated version of Docker docs and it may not be up to date. You must use the community-translated docs at your own discretion.\n\nSubset of Docker docs in Japanese translated by Docker Captain Masahito Zembutsu.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDocker Training\nHosted labs\nSelf-guided tutorials\nBooks\nCLI cheat sheet\nSelf-Paced online learning\nCommunity-translated docs\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995060,
    "timestamp": "2026-02-07T06:29:19.742Z",
    "title": "Part 9: What next | Docker Docs",
    "url": "https://docs.docker.com/get-started/workshop/10_what_next/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nGet Docker\nWhat is Docker?\nIntroduction\nDocker concepts\nDocker workshop\nPart 1: Containerize an application\nPart 2: Update the application\nPart 3: Share the application\nPart 4: Persist the DB\nPart 5: Use bind mounts\nPart 6: Multi-container apps\nPart 7: Use Docker Compose\nPart 8: Image-building best practices\nPart 9: What next\nEducational resources\nHome\n/\nGet started\n/\nDocker workshop\n/\nPart 9: What next\nWhat next after the Docker workshop\nCopy as Markdown\n\nCongratulations on completing the Docker workshop. You've learned how to containerize applications, work with multi-container setups, use Docker Compose, and apply image-building best practices.\n\nHere's what to explore next.\n\nSecure your images\n\nTake your image-building skills to the next level with Docker Hardened Images‚Äîsecure, minimal, and production-ready base images that are now free for everyone.\n\nWhat are Docker Hardened Images?\n\nUnderstand secure, minimal, production-ready base images with near-zero CVEs.\n\nGet started with DHI\n\nPull and run your first Docker Hardened Image in minutes.\n\nUse hardened images\n\nLearn how to use DHI in your Dockerfiles and CI/CD pipelines.\n\nExplore the DHI catalog\n\nBrowse available hardened images, variants, and security attestations.\n\nBuild with AI\n\nDocker makes it easy to run AI models locally and build agentic AI applications. Explore Docker's AI tools and start building AI-powered apps.\n\nDocker Model Runner\n\nRun and manage AI models locally using familiar Docker commands with OpenAI-compatible APIs.\n\nMCP Toolkit\n\nSet up, manage, and run containerized MCP servers to power your AI agents.\n\nBuild AI agents with cagent\n\nCreate teams of specialized AI agents that collaborate to solve complex problems.\n\nUse AI models in Compose\n\nDefine AI model dependencies in your Docker Compose applications.\n\nLanguage-specific guides\n\nApply what you've learned to your preferred programming language with hands-on tutorials.\n\nNode.js\n\nLearn how to containerize and develop Node.js applications.\n\nPython\n\nBuild and run Python applications in containers.\n\nJava\n\nContainerize Java applications with best practices.\n\nGo\n\nDevelop and deploy Go applications using Docker.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995156,
    "timestamp": "2026-02-07T06:32:16.236Z",
    "title": "Storage | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\nStorage\nCopy as Markdown\n\nDocker storage covers two different concepts:\n\nContainer data persistence (this page): How to store application data outside containers using volumes, bind mounts, and tmpfs mounts. This data persists independently of container lifecycle.\n\nDaemon storage backends (containerd image store and storage drivers): How the daemon stores image layers and container writable layers on disk.\n\nThis page focuses on container data persistence. For information about how Docker stores images and container layers, see containerd image store or Storage drivers.\n\nContainer layer basics\n\nBy default all files created inside a container are stored on a writable container layer that sits on top of the read-only, immutable image layers.\n\nData written to the container layer doesn't persist when the container is destroyed. This means that it can be difficult to get the data out of the container if another process needs it.\n\nThe writable layer is unique per container. You can't easily extract the data from the writeable layer to the host, or to another container.\n\nStorage mount options\n\nDocker supports the following types of storage mounts for storing data outside of the writable layer of the container:\n\nVolume mounts\nBind mounts\ntmpfs mounts\nNamed pipes\n\nNo matter which type of mount you choose to use, the data looks the same from within the container. It is exposed as either a directory or an individual file in the container's filesystem.\n\nVolume mounts\n\nVolumes are persistent storage mechanisms managed by the Docker daemon. They retain data even after the containers using them are removed. Volume data is stored on the filesystem on the host, but in order to interact with the data in the volume, you must mount the volume to a container. Directly accessing or interacting with the volume data is unsupported, undefined behavior, and may result in the volume or its data breaking in unexpected ways.\n\nVolumes are ideal for performance-critical data processing and long-term storage needs. Since the storage location is managed on the daemon host, volumes provide the same raw file performance as accessing the host filesystem directly.\n\nBind mounts\n\nBind mounts create a direct link between a host system path and a container, allowing access to files or directories stored anywhere on the host. Since they aren't isolated by Docker, both non-Docker processes on the host and container processes can modify the mounted files simultaneously.\n\nUse bind mounts when you need to be able to access files from both the container and the host.\n\ntmpfs mounts\n\nA tmpfs mount stores files directly in the host machine's memory, ensuring the data is not written to disk. This storage is ephemeral: the data is lost when the container is stopped or restarted, or when the host is rebooted. tmpfs mounts do not persist data either on the Docker host or within the container's filesystem.\n\nThese mounts are suitable for scenarios requiring temporary, in-memory storage, such as caching intermediate data, handling sensitive information like credentials, or reducing disk I/O. Use tmpfs mounts only when the data does not need to persist beyond the current container session.\n\nNamed pipes\n\nNamed pipes can be used for communication between the Docker host and a container. Common use case is to run a third-party tool inside of a container and connect to the Docker Engine API using a named pipe.\n\nNext steps\n\nLearn more about container data persistence:\n\nVolumes\nBind mounts\ntmpfs mounts\n\nLearn more about daemon storage backends:\n\ncontainerd image store\nStorage drivers\n\nEdit this page\n\nRequest changes\n\nTable of contents\nContainer layer basics\nStorage mount options\nVolume mounts\nBind mounts\ntmpfs mounts\nNamed pipes\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995162,
    "timestamp": "2026-02-07T06:32:16.238Z",
    "title": "Bind mounts | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/bind-mounts/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nBind mounts\nBind mounts\nCopy as Markdown\n\nWhen you use a bind mount, a file or directory on the host machine is mounted from the host into a container. By contrast, when you use a volume, a new directory is created within Docker's storage directory on the host machine, and Docker manages that directory's contents.\n\nWhen to use bind mounts\n\nBind mounts are appropriate for the following types of use case:\n\nSharing source code or build artifacts between a development environment on the Docker host and a container.\n\nWhen you want to create or generate files in a container and persist the files onto the host's filesystem.\n\nSharing configuration files from the host machine to containers. This is how Docker provides DNS resolution to containers by default, by mounting /etc/resolv.conf from the host machine into each container.\n\nBind mounts are also available for builds: you can bind mount source code from the host into the build container to test, lint, or compile a project.\n\nBind-mounting over existing data\n\nIf you bind mount file or directory into a directory in the container in which files or directories exist, the pre-existing files are obscured by the mount. This is similar to if you were to save files into /mnt on a Linux host, and then mounted a USB drive into /mnt. The contents of /mnt would be obscured by the contents of the USB drive until the USB drive was unmounted.\n\nWith containers, there's no straightforward way of removing a mount to reveal the obscured files again. Your best option is to recreate the container without the mount.\n\nConsiderations and constraints\n\nBind mounts have write access to files on the host by default.\n\nOne side effect of using bind mounts is that you can change the host filesystem via processes running in a container, including creating, modifying, or deleting important system files or directories. This capability can have security implications. For example, it may affect non-Docker processes on the host system.\n\nYou can use the readonly or ro option to prevent the container from writing to the mount.\n\nBind mounts are created to the Docker daemon host, not the client.\n\nIf you're using a remote Docker daemon, you can't create a bind mount to access files on the client machine in a container.\n\nFor Docker Desktop, the daemon runs inside a Linux VM, not directly on the native host. Docker Desktop has built-in mechanisms that transparently handle bind mounts, allowing you to share native host filesystem paths with containers running in the virtual machine.\n\nContainers with bind mounts are strongly tied to the host.\n\nBind mounts rely on the host machine's filesystem having a specific directory structure available. This reliance means that containers with bind mounts may fail if run on a different host without the same directory structure.\n\nSyntax\n\nTo create a bind mount, you can use either the --mount or --volume flag.\n\n$ docker run --mount type=bind,src=<host-path>,dst=<container-path>\n\n$ docker run --volume <host-path>:<container-path>\n\n\nIn general, --mount is preferred. The main difference is that the --mount flag is more explicit and supports all the available options.\n\nIf you use --volume to bind-mount a file or directory that does not yet exist on the Docker host, Docker automatically creates the directory on the host for you. It's always created as a directory.\n\n--mount does not automatically create a directory if the specified mount path does not exist on the host. Instead, it produces an error:\n\n$ docker run --mount type=bind,src=/dev/noexist,dst=/mnt/foo alpine\n\ndocker: Error response from daemon: invalid mount config for type \"bind\": bind source path does not exist: /dev/noexist.\n\nOptions for --mount\n\nThe --mount flag consists of multiple key-value pairs, separated by commas and each consisting of a <key>=<value> tuple. The order of the keys isn't significant.\n\n$ docker run --mount type=bind,src=<host-path>,dst=<container-path>[,<key>=<value>...]\n\n\nValid options for --mount type=bind include:\n\nOption\tDescription\nsource, src\tThe location of the file or directory on the host. This can be an absolute or relative path.\ndestination, dst, target\tThe path where the file or directory is mounted in the container. Must be an absolute path.\nreadonly, ro\tIf present, causes the bind mount to be mounted into the container as read-only.\nbind-propagation\tIf present, changes the bind propagation.\nExample\n$ docker run --mount type=bind,src=.,dst=/project,ro,bind-propagation=rshared\n\nOptions for --volume\n\nThe --volume or -v flag consists of three fields, separated by colon characters (:). The fields must be in the correct order.\n\n$ docker run -v <host-path>:<container-path>[:opts]\n\n\nThe first field is the path on the host to bind mount into the container. The second field is the path where the file or directory is mounted in the container.\n\nThe third field is optional, and is a comma-separated list of options. Valid options for --volume with a bind mount include:\n\nOption\tDescription\nreadonly, ro\tIf present, causes the bind mount to be mounted into the container as read-only.\nz, Z\tConfigures SELinux labeling. See Configure the SELinux label\nrprivate (default)\tSets bind propagation to rprivate for this mount. See Configure bind propagation.\nprivate\tSets bind propagation to private for this mount. See Configure bind propagation.\nrshared\tSets bind propagation to rshared for this mount. See Configure bind propagation.\nshared\tSets bind propagation to shared for this mount. See Configure bind propagation.\nrslave\tSets bind propagation to rslave for this mount. See Configure bind propagation.\nslave\tSets bind propagation to slave for this mount. See Configure bind propagation.\nExample\n$ docker run -v .:/project:ro,rshared\n\nStart a container with a bind mount\n\nConsider a case where you have a directory source and that when you build the source code, the artifacts are saved into another directory, source/target/. You want the artifacts to be available to the container at /app/, and you want the container to get access to a new build each time you build the source on your development host. Use the following command to bind-mount the target/ directory into your container at /app/. Run the command from within the source directory. The $(pwd) sub-command expands to the current working directory on Linux or macOS hosts. If you're on Windows, see also Path conversions on Windows.\n\nThe following --mount and -v examples produce the same result. You can't run them both unless you remove the devtest container after running the first one.\n\n--mount -v\n$ docker run -d \\\n\n  -it \\\n\n  --name devtest \\\n\n  --mount type=bind,source=\"$(pwd)\"/target,target=/app \\\n\n  nginx:latest\n\n\nUse docker inspect devtest to verify that the bind mount was created correctly. Look for the Mounts section:\n\n\"Mounts\": [\n\n    {\n\n        \"Type\": \"bind\",\n\n        \"Source\": \"/tmp/source/target\",\n\n        \"Destination\": \"/app\",\n\n        \"Mode\": \"\",\n\n        \"RW\": true,\n\n        \"Propagation\": \"rprivate\"\n\n    }\n\n],\n\nThis shows that the mount is a bind mount, it shows the correct source and destination, it shows that the mount is read-write, and that the propagation is set to rprivate.\n\nStop and remove the container:\n\n$ docker container rm -fv devtest\n\nMount into a non-empty directory on the container\n\nIf you bind-mount a directory into a non-empty directory on the container, the directory's existing contents are obscured by the bind mount. This can be beneficial, such as when you want to test a new version of your application without building a new image. However, it can also be surprising and this behavior differs from that of volumes.\n\nThis example is contrived to be extreme, but replaces the contents of the container's /usr/ directory with the /tmp/ directory on the host machine. In most cases, this would result in a non-functioning container.\n\nThe --mount and -v examples have the same end result.\n\n--mount -v\n$ docker run -d \\\n\n  -it \\\n\n  --name broken-container \\\n\n  --mount type=bind,source=/tmp,target=/usr \\\n\n  nginx:latest\n\n\n\ndocker: Error response from daemon: oci runtime error: container_linux.go:262:\n\nstarting container process caused \"exec: \\\"nginx\\\": executable file not found in $PATH\".\n\n\nThe container is created but does not start. Remove it:\n\n$ docker container rm broken-container\n\nUse a read-only bind mount\n\nFor some development applications, the container needs to write into the bind mount, so changes are propagated back to the Docker host. At other times, the container only needs read access.\n\nThis example modifies the previous one, but mounts the directory as a read-only bind mount, by adding ro to the (empty by default) list of options, after the mount point within the container. Where multiple options are present, separate them by commas.\n\nThe --mount and -v examples have the same result.\n\n--mount -v\n$ docker run -d \\\n\n  -it \\\n\n  --name devtest \\\n\n  --mount type=bind,source=\"$(pwd)\"/target,target=/app,readonly \\\n\n  nginx:latest\n\n\nUse docker inspect devtest to verify that the bind mount was created correctly. Look for the Mounts section:\n\n\"Mounts\": [\n\n    {\n\n        \"Type\": \"bind\",\n\n        \"Source\": \"/tmp/source/target\",\n\n        \"Destination\": \"/app\",\n\n        \"Mode\": \"ro\",\n\n        \"RW\": false,\n\n        \"Propagation\": \"rprivate\"\n\n    }\n\n],\n\nStop and remove the container:\n\n$ docker container rm -fv devtest\n\nRecursive mounts\n\nWhen you bind mount a path that itself contains mounts, those submounts are also included in the bind mount by default. This behavior is configurable, using the bind-recursive option for --mount. This option is only supported with the --mount flag, not with -v or --volume.\n\nIf the bind mount is read-only, the Docker Engine makes a best-effort attempt at making the submounts read-only as well. This is referred to as recursive read-only mounts. Recursive read-only mounts require Linux kernel version 5.12 or later. If you're running an older kernel version, submounts are automatically mounted as read-write by default. Attempting to set submounts to be read-only on a kernel version earlier than 5.12, using the bind-recursive=readonly option, results in an error.\n\nSupported values for the bind-recursive option are:\n\nValue\tDescription\nenabled (default)\tRead-only mounts are made recursively read-only if kernel is v5.12 or later. Otherwise, submounts are read-write.\ndisabled\tSubmounts are ignored (not included in the bind mount).\nwritable\tSubmounts are read-write.\nreadonly\tSubmounts are read-only. Requires kernel v5.12 or later.\nConfigure bind propagation\n\nBind propagation defaults to rprivate for both bind mounts and volumes. It is only configurable for bind mounts, and only on Linux host machines. Bind propagation is an advanced topic and many users never need to configure it.\n\nBind propagation refers to whether or not mounts created within a given bind-mount can be propagated to replicas of that mount. Consider a mount point /mnt, which is also mounted on /tmp. The propagation settings control whether a mount on /tmp/a would also be available on /mnt/a. Each propagation setting has a recursive counterpoint. In the case of recursion, consider that /tmp/a is also mounted as /foo. The propagation settings control whether /mnt/a and/or /tmp/a would exist.\n\nNote\n\nMount propagation doesn't work with Docker Desktop.\n\nPropagation setting\tDescription\nshared\tSub-mounts of the original mount are exposed to replica mounts, and sub-mounts of replica mounts are also propagated to the original mount.\nslave\tsimilar to a shared mount, but only in one direction. If the original mount exposes a sub-mount, the replica mount can see it. However, if the replica mount exposes a sub-mount, the original mount cannot see it.\nprivate\tThe mount is private. Sub-mounts within it are not exposed to replica mounts, and sub-mounts of replica mounts are not exposed to the original mount.\nrshared\tThe same as shared, but the propagation also extends to and from mount points nested within any of the original or replica mount points.\nrslave\tThe same as slave, but the propagation also extends to and from mount points nested within any of the original or replica mount points.\nrprivate\tThe default. The same as private, meaning that no mount points anywhere within the original or replica mount points propagate in either direction.\n\nBefore you can set bind propagation on a mount point, the host filesystem needs to already support bind propagation.\n\nFor more information about bind propagation, see the Linux kernel documentation for shared subtree.\n\nThe following example mounts the target/ directory into the container twice, and the second mount sets both the ro option and the rslave bind propagation option.\n\nThe --mount and -v examples have the same result.\n\n--mount -v\n$ docker run -d \\\n\n  -it \\\n\n  --name devtest \\\n\n  --mount type=bind,source=\"$(pwd)\"/target,target=/app \\\n\n  --mount type=bind,source=\"$(pwd)\"/target,target=/app2,readonly,bind-propagation=rslave \\\n\n  nginx:latest\n\n\nNow if you create /app/foo/, /app2/foo/ also exists.\n\nConfigure the SELinux label\n\nIf you use SELinux, you can add the z or Z options to modify the SELinux label of the host file or directory being mounted into the container. This affects the file or directory on the host machine itself and can have consequences outside of the scope of Docker.\n\nThe z option indicates that the bind mount content is shared among multiple containers.\nThe Z option indicates that the bind mount content is private and unshared.\n\nUse extreme caution with these options. Bind-mounting a system directory such as /home or /usr with the Z option renders your host machine inoperable and you may need to relabel the host machine files by hand.\n\nImportant\n\nWhen using bind mounts with services, SELinux labels (:Z and :z), as well as :ro are ignored. See moby/moby #32579 for details.\n\nThis example sets the z option to specify that multiple containers can share the bind mount's contents:\n\nIt is not possible to modify the SELinux label using the --mount flag.\n\n$ docker run -d \\\n\n  -it \\\n\n  --name devtest \\\n\n  -v \"$(pwd)\"/target:/app:z \\\n\n  nginx:latest\n\nUse a bind mount with Docker Compose\n\nA single Docker Compose service with a bind mount looks like this:\n\nservices:\n\n  frontend:\n\n    image: node:lts\n\n    volumes:\n\n      - type: bind\n\n        source: ./static\n\n        target: /opt/app/static\n\nvolumes:\n\n  myapp:\n\nFor more information about using volumes of the bind type with Compose, see Compose reference on the volumes top-level element. and Compose reference on the volume attribute.\n\nNext steps\nLearn about volumes.\nLearn about tmpfs mounts.\nLearn about storage drivers.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhen to use bind mounts\nBind-mounting over existing data\nConsiderations and constraints\nSyntax\nOptions for --mount\nOptions for --volume\nStart a container with a bind mount\nMount into a non-empty directory on the container\nUse a read-only bind mount\nRecursive mounts\nConfigure bind propagation\nConfigure the SELinux label\nUse a bind mount with Docker Compose\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995159,
    "timestamp": "2026-02-07T06:32:16.250Z",
    "title": "Volumes | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/volumes/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nVolumes\nVolumes\nCopy as Markdown\n\nVolumes are persistent data stores for containers, created and managed by Docker. You can create a volume explicitly using the docker volume create command, or Docker can create a volume during container or service creation.\n\nWhen you create a volume, it's stored within a directory on the Docker host. When you mount the volume into a container, this directory is what's mounted into the container. This is similar to the way that bind mounts work, except that volumes are managed by Docker and are isolated from the core functionality of the host machine.\n\nWhen to use volumes\n\nVolumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure and OS of the host machine, volumes are completely managed by Docker. Volumes are a good choice for the following use cases:\n\nVolumes are easier to back up or migrate than bind mounts.\nYou can manage volumes using Docker CLI commands or the Docker API.\nVolumes work on both Linux and Windows containers.\nVolumes can be more safely shared among multiple containers.\nNew volumes can have their content pre-populated by a container or build.\nWhen your application requires high-performance I/O.\n\nVolumes are not a good choice if you need to access the files from the host, as the volume is completely managed by Docker. Use bind mounts if you need to access files or directories from both containers and the host.\n\nVolumes are often a better choice than writing data directly to a container, because a volume doesn't increase the size of the containers using it. Using a volume is also faster; writing into a container's writable layer requires a storage driver to manage the filesystem. The storage driver provides a union filesystem, using the Linux kernel. This extra abstraction reduces performance as compared to using volumes, which write directly to the host filesystem.\n\nIf your container generates non-persistent state data, consider using a tmpfs mount to avoid storing the data anywhere permanently, and to increase the container's performance by avoiding writing into the container's writable layer.\n\nVolumes use rprivate (recursive private) bind propagation, and bind propagation isn't configurable for volumes.\n\nA volume's lifecycle\n\nA volume's contents exist outside the lifecycle of a given container. When a container is destroyed, the writable layer is destroyed with it. Using a volume ensures that the data is persisted even if the container using it is removed.\n\nA given volume can be mounted into multiple containers simultaneously. When no running container is using a volume, the volume is still available to Docker and isn't removed automatically. You can remove unused volumes using docker volume prune.\n\nMounting a volume over existing data\n\nIf you mount a non-empty volume into a directory in the container in which files or directories exist, the pre-existing files are obscured by the mount. This is similar to if you were to save files into /mnt on a Linux host, and then mounted a USB drive into /mnt. The contents of /mnt would be obscured by the contents of the USB drive until the USB drive was unmounted.\n\nWith containers, there's no straightforward way of removing a mount to reveal the obscured files again. Your best option is to recreate the container without the mount.\n\nIf you mount an empty volume into a directory in the container in which files or directories exist, these files or directories are propagated (copied) into the volume by default. Similarly, if you start a container and specify a volume which does not already exist, an empty volume is created for you. This is a good way to pre-populate data that another container needs.\n\nTo prevent Docker from copying a container's pre-existing files into an empty volume, use the volume-nocopy option, see Options for --mount.\n\nNamed and anonymous volumes\n\nA volume may be named or anonymous. Anonymous volumes are given a random name that's guaranteed to be unique within a given Docker host. Just like named volumes, anonymous volumes persist even if you remove the container that uses them, except if you use the --rm flag when creating the container, in which case the anonymous volume associated with the container is destroyed. See Remove anonymous volumes.\n\nIf you create multiple containers consecutively that each use anonymous volumes, each container creates its own volume. Anonymous volumes aren't reused or shared between containers automatically. To share an anonymous volume between two or more containers, you must mount the anonymous volume using the random volume ID.\n\nSyntax\n\nTo mount a volume with the docker run command, you can use either the --mount or --volume flag.\n\n$ docker run --mount type=volume,src=<volume-name>,dst=<mount-path>\n\n$ docker run --volume <volume-name>:<mount-path>\n\n\nIn general, --mount is preferred. The main difference is that the --mount flag is more explicit and supports all the available options.\n\nYou must use --mount if you want to:\n\nSpecify volume driver options\nMount a volume subdirectory\nMount a volume into a Swarm service\nOptions for --mount\n\nThe --mount flag consists of multiple key-value pairs, separated by commas and each consisting of a <key>=<value> tuple. The order of the keys isn't significant.\n\n$ docker run --mount type=volume[,src=<volume-name>],dst=<mount-path>[,<key>=<value>...]\n\n\nValid options for --mount type=volume include:\n\nOption\tDescription\nsource, src\tThe source of the mount. For named volumes, this is the name of the volume. For anonymous volumes, this field is omitted.\ndestination, dst, target\tThe path where the file or directory is mounted in the container.\nvolume-subpath\tA path to a subdirectory within the volume to mount into the container. The subdirectory must exist in the volume before the volume is mounted to a container. See Mount a volume subdirectory.\nreadonly, ro\tIf present, causes the volume to be mounted into the container as read-only.\nvolume-nocopy\tIf present, data at the destination isn't copied into the volume if the volume is empty. By default, content at the target destination gets copied into a mounted volume if empty.\nvolume-opt\tCan be specified more than once, takes a key-value pair consisting of the option name and its value.\nExample\n$ docker run --mount type=volume,src=myvolume,dst=/data,ro,volume-subpath=/foo\n\nOptions for --volume\n\nThe --volume or -v flag consists of three fields, separated by colon characters (:). The fields must be in the correct order.\n\n$ docker run -v [<volume-name>:]<mount-path>[:opts]\n\n\nIn the case of named volumes, the first field is the name of the volume, and is unique on a given host machine. For anonymous volumes, the first field is omitted. The second field is the path where the file or directory is mounted in the container.\n\nThe third field is optional, and is a comma-separated list of options. Valid options for --volume with a data volume include:\n\nOption\tDescription\nreadonly, ro\tIf present, causes the volume to be mounted into the container as read-only.\nvolume-nocopy\tIf present, data at the destination isn't copied into the volume if the volume is empty. By default, content at the target destination gets copied into a mounted volume if empty.\nExample\n$ docker run -v myvolume:/data:ro\n\nCreate and manage volumes\n\nUnlike a bind mount, you can create and manage volumes outside the scope of any container.\n\nCreate a volume:\n\n$ docker volume create my-vol\n\n\nList volumes:\n\n$ docker volume ls\n\n\n\nlocal               my-vol\n\n\nInspect a volume:\n\n$ docker volume inspect my-vol\n\n[\n\n    {\n\n        \"Driver\": \"local\",\n\n        \"Labels\": {},\n\n        \"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\",\n\n        \"Name\": \"my-vol\",\n\n        \"Options\": {},\n\n        \"Scope\": \"local\"\n\n    }\n\n]\n\n\nRemove a volume:\n\n$ docker volume rm my-vol\n\nStart a container with a volume\n\nIf you start a container with a volume that doesn't yet exist, Docker creates the volume for you. The following example mounts the volume myvol2 into /app/ in the container.\n\nThe following -v and --mount examples produce the same result. You can't run them both unless you remove the devtest container and the myvol2 volume after running the first one.\n\n--mount -v\n$ docker run -d \\\n\n  --name devtest \\\n\n  --mount source=myvol2,target=/app \\\n\n  nginx:latest\n\n\nUse docker inspect devtest to verify that Docker created the volume and it mounted correctly. Look for the Mounts section:\n\n\"Mounts\": [\n\n    {\n\n        \"Type\": \"volume\",\n\n        \"Name\": \"myvol2\",\n\n        \"Source\": \"/var/lib/docker/volumes/myvol2/_data\",\n\n        \"Destination\": \"/app\",\n\n        \"Driver\": \"local\",\n\n        \"Mode\": \"\",\n\n        \"RW\": true,\n\n        \"Propagation\": \"\"\n\n    }\n\n],\n\nThis shows that the mount is a volume, it shows the correct source and destination, and that the mount is read-write.\n\nStop the container and remove the volume. Note volume removal is a separate step.\n\n$ docker container stop devtest\n\n\n\n$ docker container rm devtest\n\n\n\n$ docker volume rm myvol2\n\nUse a volume with Docker Compose\n\nThe following example shows a single Docker Compose service with a volume:\n\nservices:\n\n  frontend:\n\n    image: node:lts\n\n    volumes:\n\n      - myapp:/home/node/app\n\nvolumes:\n\n  myapp:\n\nRunning docker compose up for the first time creates a volume. Docker reuses the same volume when you run the command subsequently.\n\nYou can create a volume directly outside of Compose using docker volume create and then reference it inside compose.yaml as follows:\n\nservices:\n\n  frontend:\n\n    image: node:lts\n\n    volumes:\n\n      - myapp:/home/node/app\n\nvolumes:\n\n  myapp:\n\n    external: true\n\nFor more information about using volumes with Compose, refer to the Volumes section in the Compose specification.\n\nStart a service with volumes\n\nWhen you start a service and define a volume, each service container uses its own local volume. None of the containers can share this data if you use the local volume driver. However, some volume drivers do support shared storage.\n\nThe following example starts an nginx service with four replicas, each of which uses a local volume called myvol2.\n\n$ docker service create -d \\\n\n  --replicas=4 \\\n\n  --name devtest-service \\\n\n  --mount source=myvol2,target=/app \\\n\n  nginx:latest\n\n\nUse docker service ps devtest-service to verify that the service is running:\n\n$ docker service ps devtest-service\n\n\n\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\n\n4d7oz1j85wwn        devtest-service.1   nginx:latest        moby                Running             Running 14 seconds ago\n\n\nYou can remove the service to stop the running tasks:\n\n$ docker service rm devtest-service\n\n\nRemoving the service doesn't remove any volumes created by the service. Volume removal is a separate step.\n\nPopulate a volume using a container\n\nIf you start a container which creates a new volume, and the container has files or directories in the directory to be mounted such as /app/, Docker copies the directory's contents into the volume. The container then mounts and uses the volume, and other containers which use the volume also have access to the pre-populated content.\n\nTo show this, the following example starts an nginx container and populates the new volume nginx-vol with the contents of the container's /usr/share/nginx/html directory. This is where Nginx stores its default HTML content.\n\nThe --mount and -v examples have the same end result.\n\n--mount -v\n$ docker run -d \\\n\n  --name=nginxtest \\\n\n  --mount source=nginx-vol,destination=/usr/share/nginx/html \\\n\n  nginx:latest\n\n\nAfter running either of these examples, run the following commands to clean up the containers and volumes. Note volume removal is a separate step.\n\n$ docker container stop nginxtest\n\n\n\n$ docker container rm nginxtest\n\n\n\n$ docker volume rm nginx-vol\n\nUse a read-only volume\n\nFor some development applications, the container needs to write into the bind mount so that changes are propagated back to the Docker host. At other times, the container only needs read access to the data. Multiple containers can mount the same volume. You can simultaneously mount a single volume as read-write for some containers and as read-only for others.\n\nThe following example changes the previous one. It mounts the directory as a read-only volume, by adding ro to the (empty by default) list of options, after the mount point within the container. Where multiple options are present, you can separate them using commas.\n\nThe --mount and -v examples have the same result.\n\n--mount -v\n$ docker run -d \\\n\n  --name=nginxtest \\\n\n  --mount source=nginx-vol,destination=/usr/share/nginx/html,readonly \\\n\n  nginx:latest\n\n\nUse docker inspect nginxtest to verify that Docker created the read-only mount correctly. Look for the Mounts section:\n\n\"Mounts\": [\n\n    {\n\n        \"Type\": \"volume\",\n\n        \"Name\": \"nginx-vol\",\n\n        \"Source\": \"/var/lib/docker/volumes/nginx-vol/_data\",\n\n        \"Destination\": \"/usr/share/nginx/html\",\n\n        \"Driver\": \"local\",\n\n        \"Mode\": \"\",\n\n        \"RW\": false,\n\n        \"Propagation\": \"\"\n\n    }\n\n],\n\nStop and remove the container, and remove the volume. Volume removal is a separate step.\n\n$ docker container stop nginxtest\n\n\n\n$ docker container rm nginxtest\n\n\n\n$ docker volume rm nginx-vol\n\nMount a volume subdirectory\n\nWhen you mount a volume to a container, you can specify a subdirectory of the volume to use, with the volume-subpath parameter for the --mount flag. The subdirectory that you specify must exist in the volume before you attempt to mount it into a container; if it doesn't exist, the mount fails.\n\nSpecifying volume-subpath is useful if you only want to share a specific portion of a volume with a container. Say for example that you have multiple containers running and you want to store logs from each container in a shared volume. You can create a subdirectory for each container in the shared volume, and mount the subdirectory to the container.\n\nThe following example creates a logs volume and initiates the subdirectories app1 and app2 in the volume. It then starts two containers and mounts one of the subdirectories of the logs volume to each container. This example assumes that the processes in the containers write their logs to /var/log/app1 and /var/log/app2.\n\n$ docker volume create logs\n\n$ docker run --rm \\\n\n  --mount src=logs,dst=/logs \\\n\n  alpine mkdir -p /logs/app1 /logs/app2\n\n$ docker run -d \\\n\n  --name=app1 \\\n\n  --mount src=logs,dst=/var/log/app1,volume-subpath=app1 \\\n\n  app1:latest\n\n$ docker run -d \\\n\n  --name=app2 \\\n\n  --mount src=logs,dst=/var/log/app2,volume-subpath=app2 \\\n\n  app2:latest\n\n\nWith this setup, the containers write their logs to separate subdirectories of the logs volume. The containers can't access the other container's logs.\n\nShare data between machines\n\nWhen building fault-tolerant applications, you may need to configure multiple replicas of the same service to have access to the same files.\n\nThere are several ways to achieve this when developing your applications. One is to add logic to your application to store files on a cloud object storage system like Amazon S3. Another is to create volumes with a driver that supports writing files to an external storage system like NFS or Amazon S3.\n\nVolume drivers let you abstract the underlying storage system from the application logic. For example, if your services use a volume with an NFS driver, you can update the services to use a different driver. For example, to store data in the cloud, without changing the application logic.\n\nUse a volume driver\n\nWhen you create a volume using docker volume create, or when you start a container which uses a not-yet-created volume, you can specify a volume driver. The following examples use the rclone/docker-volume-rclone volume driver, first when creating a standalone volume, and then when starting a container which creates a new volume.\n\nNote\n\nIf your volume driver accepts a comma-separated list as an option, you must escape the value from the outer CSV parser. To escape a volume-opt, surround it with double quotes (\") and surround the entire mount parameter with single quotes (').\n\nFor example, the local driver accepts mount options as a comma-separated list in the o parameter. This example shows the correct way to escape the list.\n\n$ docker service create \\\n\n --mount 'type=volume,src=<VOLUME-NAME>,dst=<CONTAINER-PATH>,volume-driver=local,volume-opt=type=nfs,volume-opt=device=<nfs-server>:<nfs-path>,\"volume-opt=o=addr=<nfs-address>,vers=4,soft,timeo=180,bg,tcp,rw\"'\n\n --name myservice \\\n\n IMAGE\n\nInitial setup\n\nThe following example assumes that you have two nodes, the first of which is a Docker host and can connect to the second node using SSH.\n\nOn the Docker host, install the rclone/docker-volume-rclone plugin:\n\n$ docker plugin install --grant-all-permissions rclone/docker-volume-rclone --aliases rclone\n\nCreate a volume using a volume driver\n\nThis example mounts the /remote directory on host 1.2.3.4 into a volume named rclonevolume. Each volume driver may have zero or more configurable options, you specify each of them using an -o flag.\n\n$ docker volume create \\\n\n  -d rclone \\\n\n  --name rclonevolume \\\n\n  -o type=sftp \\\n\n  -o path=remote \\\n\n  -o sftp-host=1.2.3.4 \\\n\n  -o sftp-user=user \\\n\n  -o \"sftp-password=$(cat file_containing_password_for_remote_host)\"\n\n\nThis volume can now be mounted into containers.\n\nStart a container which creates a volume using a volume driver\nNote\n\nIf the volume driver requires you to pass any options, you must use the --mount flag to mount the volume, and not -v.\n\n$ docker run -d \\\n\n  --name rclone-container \\\n\n  --mount type=volume,volume-driver=rclone,src=rclonevolume,target=/app,volume-opt=type=sftp,volume-opt=path=remote, volume-opt=sftp-host=1.2.3.4,volume-opt=sftp-user=user,volume-opt=-o \"sftp-password=$(cat file_containing_password_for_remote_host)\" \\\n\n  nginx:latest\n\nCreate a service which creates an NFS volume\n\nThe following example shows how you can create an NFS volume when creating a service. It uses 10.0.0.10 as the NFS server and /var/docker-nfs as the exported directory on the NFS server. Note that the volume driver specified is local.\n\nNFSv3\n$ docker service create -d \\\n\n  --name nfs-service \\\n\n  --mount 'type=volume,source=nfsvolume,target=/app,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/var/docker-nfs,volume-opt=o=addr=10.0.0.10' \\\n\n  nginx:latest\n\nNFSv4\n$ docker service create -d \\\n\n    --name nfs-service \\\n\n    --mount 'type=volume,source=nfsvolume,target=/app,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/var/docker-nfs,\"volume-opt=o=addr=10.0.0.10,rw,nfsvers=4,async\"' \\\n\n    nginx:latest\n\nCreate CIFS/Samba volumes\n\nYou can mount a Samba share directly in Docker without configuring a mount point on your host.\n\n$ docker volume create \\\n\n\t--driver local \\\n\n\t--opt type=cifs \\\n\n\t--opt device=//uxxxxx.your-server.de/backup \\\n\n\t--opt o=addr=uxxxxx.your-server.de,username=uxxxxxxx,password=*****,file_mode=0777,dir_mode=0777 \\\n\n\t--name cifs-volume\n\n\nThe addr option is required if you specify a hostname instead of an IP. This lets Docker perform the hostname lookup.\n\nBlock storage devices\n\nYou can mount a block storage device, such as an external drive or a drive partition, to a container. The following example shows how to create and use a file as a block storage device, and how to mount the block device as a container volume.\n\nImportant\n\nThe following procedure is only an example. The solution illustrated here isn't recommended as a general practice. Don't attempt this approach unless you're confident about what you're doing.\n\nHow mounting block devices works\n\nUnder the hood, the --mount flag using the local storage driver invokes the Linux mount syscall and forwards the options you pass to it unaltered. Docker doesn't implement any additional functionality on top of the native mount features supported by the Linux kernel.\n\nIf you're familiar with the Linux mount command, you can think of the --mount options as forwarded to the mount command in the following manner:\n\n$ mount -t <mount.volume-opt.type> <mount.volume-opt.device> <mount.dst> -o <mount.volume-opts.o>\n\n\nTo explain this further, consider the following mount command example. This command mounts the /dev/loop5 device to the path /external-drive on the system.\n\n$ mount -t ext4 /dev/loop5 /external-drive\n\n\nThe following docker run command achieves a similar result, from the point of view of the container being run. Running a container with this --mount option sets up the mount in the same way as if you had executed the mount command from the previous example.\n\n$ docker run \\\n\n  --mount='type=volume,dst=/external-drive,volume-driver=local,volume-opt=device=/dev/loop5,volume-opt=type=ext4'\n\n\nYou can't run the mount command inside the container directly, because the container is unable to access the /dev/loop5 device. That's why the docker run command uses the --mount option.\n\nExample: Mounting a block device in a container\n\nThe following steps create an ext4 filesystem and mounts it into a container. The filesystem support of your system depends on the version of the Linux kernel you are using.\n\nCreate a file and allocate some space to it:\n\n$ fallocate -l 1G disk.raw\n\n\nBuild a filesystem onto the disk.raw file:\n\n$ mkfs.ext4 disk.raw\n\n\nCreate a loop device:\n\n$ losetup -f --show disk.raw\n\n/dev/loop5\n\nNote\n\nlosetup creates an ephemeral loop device that's removed after system reboot, or manually removed with losetup -d.\n\nRun a container that mounts the loop device as a volume:\n\n$ docker run -it --rm \\\n\n  --mount='type=volume,dst=/external-drive,volume-driver=local,volume-opt=device=/dev/loop5,volume-opt=type=ext4' \\\n\n  ubuntu bash\n\n\nWhen the container starts, the path /external-drive mounts the disk.raw file from the host filesystem as a block device.\n\nWhen you're done, and the device is unmounted from the container, detach the loop device to remove the device from the host system:\n\n$ losetup -d /dev/loop5\n\nBack up, restore, or migrate data volumes\n\nVolumes are useful for backups, restores, and migrations. Use the --volumes-from flag to create a new container that mounts that volume.\n\nBack up a volume\n\nFor example, create a new container named dbstore:\n\n$ docker run -v /dbdata --name dbstore ubuntu /bin/bash\n\n\nIn the next command:\n\nLaunch a new container and mount the volume from the dbstore container\nMount a local host directory as /backup\nPass a command that tars the contents of the dbdata volume to a backup.tar file inside the /backup directory.\n$ docker run --rm --volumes-from dbstore -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata\n\n\nWhen the command completes and the container stops, it creates a backup of the dbdata volume.\n\nRestore volume from a backup\n\nWith the backup just created, you can restore it to the same container, or to another container that you created elsewhere.\n\nFor example, create a new container named dbstore2:\n\n$ docker run -v /dbdata --name dbstore2 ubuntu /bin/bash\n\n\nThen, un-tar the backup file in the new container‚Äôs data volume:\n\n$ docker run --rm --volumes-from dbstore2 -v $(pwd):/backup ubuntu bash -c \"cd /dbdata && tar xvf /backup/backup.tar --strip 1\"\n\n\nYou can use these techniques to automate backup, migration, and restore testing using your preferred tools.\n\nRemove volumes\n\nA Docker data volume persists after you delete a container. There are two types of volumes to consider:\n\nNamed volumes have a specific source from outside the container, for example, awesome:/bar.\nAnonymous volumes have no specific source. Therefore, when the container is deleted, you can instruct the Docker Engine daemon to remove them.\nRemove anonymous volumes\n\nTo automatically remove anonymous volumes, use the --rm option. For example, this command creates an anonymous /foo volume. When you remove the container, the Docker Engine removes the /foo volume but not the awesome volume.\n\n$ docker run --rm -v /foo -v awesome:/bar busybox top\n\nNote\n\nIf another container binds the volumes with --volumes-from, the volume definitions are copied and the anonymous volume also stays after the first container is removed.\n\nRemove all volumes\n\nTo remove all unused volumes and free up space:\n\n$ docker volume prune\n\nNext steps\nLearn about bind mounts.\nLearn about tmpfs mounts.\nLearn about storage drivers.\nLearn about third-party volume driver plugins.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhen to use volumes\nA volume's lifecycle\nMounting a volume over existing data\nNamed and anonymous volumes\nSyntax\nOptions for --mount\nOptions for --volume\nCreate and manage volumes\nStart a container with a volume\nUse a volume with Docker Compose\nStart a service with volumes\nPopulate a volume using a container\nUse a read-only volume\nMount a volume subdirectory\nShare data between machines\nUse a volume driver\nInitial setup\nCreate a volume using a volume driver\nStart a container which creates a volume using a volume driver\nCreate a service which creates an NFS volume\nCreate CIFS/Samba volumes\nBlock storage devices\nBack up, restore, or migrate data volumes\nBack up a volume\nRestore volume from a backup\nRemove volumes\nRemove anonymous volumes\nRemove all volumes\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995168,
    "timestamp": "2026-02-07T06:32:16.250Z",
    "title": "Storage drivers | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/drivers/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\nSelect a storage driver\nBTRFS storage driver\nDevice Mapper storage driver (deprecated)\nOverlayFS storage driver\nVFS storage driver\nwindowsfilter storage driver\nZFS storage driver\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nStorage drivers\nStorage drivers\nCopy as Markdown\nNote\n\nDocker Engine 29.0 and later uses the containerd image store by default for fresh installations. The containerd image store uses snapshotters instead of the classic storage drivers described on this page. If you're running a fresh installation of Docker Engine 29.0 or later, or if you've migrated to the containerd image store, this page provides background on how image layers work but the implementation details differ. For information about the containerd image store, see containerd image store.\n\nTo use storage drivers effectively, it's important to know how Docker builds and stores images, and how these images are used by containers. You can use this information to make informed choices about the best way to persist data from your applications and avoid performance problems along the way.\n\nStorage drivers versus Docker volumes\n\nDocker uses storage drivers to store image layers, and to store data in the writable layer of a container. The container's writable layer doesn't persist after the container is deleted, but is suitable for storing ephemeral data that is generated at runtime. Storage drivers are optimized for space efficiency, but (depending on the storage driver) write speeds are lower than native file system performance, especially for storage drivers that use a copy-on-write filesystem. Write-intensive applications, such as database storage, are impacted by a performance overhead, particularly if pre-existing data exists in the read-only layer.\n\nUse Docker volumes for write-intensive data, data that must persist beyond the container's lifespan, and data that must be shared between containers. Refer to the volumes section to learn how to use volumes to persist data and improve performance.\n\nImages and layers\n\nA Docker image is built up from a series of layers. Each layer represents an instruction in the image's Dockerfile. Each layer except the very last one is read-only. Consider the following Dockerfile:\n\n# syntax=docker/dockerfile:1\n\n\n\nFROM ubuntu:22.04\n\nLABEL org.opencontainers.image.authors=\"org@example.com\"\n\nCOPY . /app\n\nRUN make /app\n\nRUN rm -r $HOME/.cache\n\nCMD python /app/app.py\n\nThis Dockerfile contains four commands. Commands that modify the filesystem create a new layer. The FROM statement starts out by creating a layer from the ubuntu:22.04 image. The LABEL command only modifies the image's metadata, and doesn't produce a new layer. The COPY command adds some files from your Docker client's current directory. The first RUN command builds your application using the make command, and writes the result to a new layer. The second RUN command removes a cache directory, and writes the result to a new layer. Finally, the CMD instruction specifies what command to run within the container, which only modifies the image's metadata, which doesn't produce an image layer.\n\nEach layer is only a set of differences from the layer before it. Note that both adding, and removing files will result in a new layer. In the example above, the $HOME/.cache directory is removed, but will still be available in the previous layer and add up to the image's total size. Refer to the Best practices for writing Dockerfiles and use multi-stage builds sections to learn how to optimize your Dockerfiles for efficient images.\n\nThe layers are stacked on top of each other. When you create a new container, you add a new writable layer on top of the underlying layers. This layer is often called the \"container layer\". All changes made to the running container, such as writing new files, modifying existing files, and deleting files, are written to this thin writable container layer. The diagram below shows a container based on an ubuntu:15.04 image.\n\nA storage driver handles the details about the way these layers interact with each other. Different storage drivers are available, which have advantages and disadvantages in different situations.\n\nContainer and layers\n\nThe major difference between a container and an image is the top writable layer. All writes to the container that add new or modify existing data are stored in this writable layer. When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged.\n\nBecause each container has its own writable container layer, and all changes are stored in this container layer, multiple containers can share access to the same underlying image and yet have their own data state. The diagram below shows multiple containers sharing the same Ubuntu 15.04 image.\n\nDocker uses storage drivers to manage the contents of the image layers and the writable container layer. Each storage driver handles the implementation differently, but all drivers use stackable image layers and the copy-on-write (CoW) strategy.\n\nNote\n\nUse Docker volumes if you need multiple containers to have shared access to the exact same data. Refer to the volumes section to learn about volumes.\n\nContainer size on disk\n\nTo view the approximate size of a running container, you can use the docker ps -s command. Two different columns relate to size.\n\nsize: the amount of data (on disk) that's used for the writable layer of each container.\nvirtual size: the amount of data used for the read-only image data used by the container plus the container's writable layer size. Multiple containers may share some or all read-only image data. Two containers started from the same image share 100% of the read-only data, while two containers with different images which have layers in common share those common layers. Therefore, you can't just total the virtual sizes. This over-estimates the total disk usage by a potentially non-trivial amount.\n\nThe total disk space used by all of the running containers on disk is some combination of each container's size and the virtual size values. If multiple containers started from the same exact image, the total size on disk for these containers would be SUM (size of containers) plus one image size (virtual size - size).\n\nThis also doesn't count the following additional ways a container can take up disk space:\n\nDisk space used for log files stored by the logging-driver. This can be non-trivial if your container generates a large amount of logging data and log rotation isn't configured.\nVolumes and bind mounts used by the container.\nDisk space used for the container's configuration files, which are typically small.\nMemory written to disk (if swapping is enabled).\nCheckpoints, if you're using the experimental checkpoint/restore feature.\nThe copy-on-write (CoW) strategy\n\nCopy-on-write is a strategy of sharing and copying files for maximum efficiency. If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it, it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container), the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers. These advantages are explained in more depth below.\n\nSharing promotes smaller images\n\nWhen you use docker pull to pull down an image from a repository, or when you create a container from an image that doesn't yet exist locally, each layer is pulled down separately, and stored in Docker's local storage area, which is usually /var/lib/docker/ on Linux hosts. You can see these layers being pulled in this example:\n\n$ docker pull ubuntu:22.04\n\n22.04: Pulling from library/ubuntu\n\nf476d66f5408: Pull complete\n\n8882c27f669e: Pull complete\n\nd9af21273955: Pull complete\n\nf5029279ec12: Pull complete\n\nDigest: sha256:6120be6a2b7ce665d0cbddc3ce6eae60fe94637c6a66985312d1f02f63cc0bcd\n\nStatus: Downloaded newer image for ubuntu:22.04\n\ndocker.io/library/ubuntu:22.04\n\n\nEach of these layers is stored in its own directory inside the Docker host's local storage area. To examine the layers on the filesystem, list the contents of /var/lib/docker/<storage-driver>. This example uses the overlay2 storage driver:\n\n$ ls /var/lib/docker/overlay2\n\n16802227a96c24dcbeab5b37821e2b67a9f921749cd9a2e386d5a6d5bc6fc6d3\n\n377d73dbb466e0bc7c9ee23166771b35ebdbe02ef17753d79fd3571d4ce659d7\n\n3f02d96212b03e3383160d31d7c6aeca750d2d8a1879965b89fe8146594c453d\n\nec1ec45792908e90484f7e629330666e7eee599f08729c93890a7205a6ba35f5\n\nl\n\n\nThe directory names don't correspond to the layer IDs.\n\nNow imagine that you have two different Dockerfiles. You use the first one to create an image called acme/my-base-image:1.0.\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nRUN apk add --no-cache bash\n\nThe second one is based on acme/my-base-image:1.0, but has some additional layers:\n\n# syntax=docker/dockerfile:1\n\nFROM acme/my-base-image:1.0\n\nCOPY . /app\n\nRUN chmod +x /app/hello.sh\n\nCMD /app/hello.sh\n\nThe second image contains all the layers from the first image, plus new layers created by the COPY and RUN instructions, and a read-write container layer. Docker already has all the layers from the first image, so it doesn't need to pull them again. The two images share any layers they have in common.\n\nIf you build images from the two Dockerfiles, you can use docker image ls and docker image history commands to verify that the cryptographic IDs of the shared layers are the same.\n\nMake a new directory cow-test/ and change into it.\n\nWithin cow-test/, create a new file called hello.sh with the following contents.\n\n#!/usr/bin/env bash\n\necho \"Hello world\"\n\nCopy the contents of the first Dockerfile above into a new file called Dockerfile.base.\n\nCopy the contents of the second Dockerfile above into a new file called Dockerfile.\n\nWithin the cow-test/ directory, build the first image. Don't forget to include the final . in the command. That sets the PATH, which tells Docker where to look for any files that need to be added to the image.\n\n$ docker build -t acme/my-base-image:1.0 -f Dockerfile.base .\n\n[+] Building 6.0s (11/11) FINISHED\n\n=> [internal] load build definition from Dockerfile.base                                      0.4s\n\n=> => transferring dockerfile: 116B                                                           0.0s\n\n=> [internal] load .dockerignore                                                              0.3s\n\n=> => transferring context: 2B                                                                0.0s\n\n=> resolve image config for docker.io/docker/dockerfile:1                                     1.5s\n\n=> [auth] docker/dockerfile:pull token for registry-1.docker.io                               0.0s\n\n=> CACHED docker-image://docker.io/docker/dockerfile:1@sha256:9e2c9eca7367393aecc68795c671... 0.0s\n\n=> [internal] load .dockerignore                                                              0.0s\n\n=> [internal] load build definition from Dockerfile.base                                      0.0s\n\n=> [internal] load metadata for docker.io/library/alpine:latest                               0.0s\n\n=> CACHED [1/2] FROM docker.io/library/alpine                                                 0.0s\n\n=> [2/2] RUN apk add --no-cache bash                                                          3.1s\n\n=> exporting to image                                                                         0.2s\n\n=> => exporting layers                                                                        0.2s\n\n=> => writing image sha256:da3cf8df55ee9777ddcd5afc40fffc3ead816bda99430bad2257de4459625eaa   0.0s\n\n=> => naming to docker.io/acme/my-base-image:1.0                                              0.0s\n\n\nBuild the second image.\n\n$ docker build -t acme/my-final-image:1.0 -f Dockerfile .\n\n\n\n[+] Building 3.6s (12/12) FINISHED\n\n=> [internal] load build definition from Dockerfile                                            0.1s\n\n=> => transferring dockerfile: 156B                                                            0.0s\n\n=> [internal] load .dockerignore                                                               0.1s\n\n=> => transferring context: 2B                                                                 0.0s\n\n=> resolve image config for docker.io/docker/dockerfile:1                                      0.5s\n\n=> CACHED docker-image://docker.io/docker/dockerfile:1@sha256:9e2c9eca7367393aecc68795c671...  0.0s\n\n=> [internal] load .dockerignore                                                               0.0s\n\n=> [internal] load build definition from Dockerfile                                            0.0s\n\n=> [internal] load metadata for docker.io/acme/my-base-image:1.0                               0.0s\n\n=> [internal] load build context                                                               0.2s\n\n=> => transferring context: 340B                                                               0.0s\n\n=> [1/3] FROM docker.io/acme/my-base-image:1.0                                                 0.2s\n\n=> [2/3] COPY . /app                                                                           0.1s\n\n=> [3/3] RUN chmod +x /app/hello.sh                                                            0.4s\n\n=> exporting to image                                                                          0.1s\n\n=> => exporting layers                                                                         0.1s\n\n=> => writing image sha256:8bd85c42fa7ff6b33902ada7dcefaaae112bf5673873a089d73583b0074313dd    0.0s\n\n=> => naming to docker.io/acme/my-final-image:1.0                                              0.0s\n\n\nCheck out the sizes of the images.\n\n$ docker image ls\n\n\n\nREPOSITORY             TAG     IMAGE ID         CREATED               SIZE\n\nacme/my-final-image    1.0     8bd85c42fa7f     About a minute ago    7.75MB\n\nacme/my-base-image     1.0     da3cf8df55ee     2 minutes ago         7.75MB\n\n\nCheck out the history of each image.\n\n$ docker image history acme/my-base-image:1.0\n\n\n\nIMAGE          CREATED         CREATED BY                                      SIZE      COMMENT\n\nda3cf8df55ee   5 minutes ago   RUN /bin/sh -c apk add --no-cache bash # bui‚Ä¶   2.15MB    buildkit.dockerfile.v0\n\n<missing>      7 weeks ago     /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B\n\n<missing>      7 weeks ago     /bin/sh -c #(nop) ADD file:f278386b0cef68136‚Ä¶   5.6MB\n\n\nSome steps don't have a size (0B), and are metadata-only changes, which do not produce an image layer and don't take up any size, other than the metadata itself. The output above shows that this image consists of 2 image layers.\n\n$ docker image history  acme/my-final-image:1.0\n\n\n\nIMAGE          CREATED         CREATED BY                                      SIZE      COMMENT\n\n8bd85c42fa7f   3 minutes ago   CMD [\"/bin/sh\" \"-c\" \"/app/hello.sh\"]            0B        buildkit.dockerfile.v0\n\n<missing>      3 minutes ago   RUN /bin/sh -c chmod +x /app/hello.sh # buil‚Ä¶   39B       buildkit.dockerfile.v0\n\n<missing>      3 minutes ago   COPY . /app # buildkit                          222B      buildkit.dockerfile.v0\n\n<missing>      4 minutes ago   RUN /bin/sh -c apk add --no-cache bash # bui‚Ä¶   2.15MB    buildkit.dockerfile.v0\n\n<missing>      7 weeks ago     /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B\n\n<missing>      7 weeks ago     /bin/sh -c #(nop) ADD file:f278386b0cef68136‚Ä¶   5.6MB\n\n\nNotice that all steps of the first image are also included in the final image. The final image includes the two layers from the first image, and two layers that were added in the second image.\n\nThe <missing> lines in the docker history output indicate that those steps were either built on another system and part of the alpine image that was pulled from Docker Hub, or were built with BuildKit as builder. Before BuildKit, the \"classic\" builder would produce a new \"intermediate\" image for each step for caching purposes, and the IMAGE column would show the ID of that image.\n\nBuildKit uses its own caching mechanism, and no longer requires intermediate images for caching. Refer to BuildKit to learn more about other enhancements made in BuildKit.\n\nCheck out the layers for each image\n\nUse the docker image inspect command to view the cryptographic IDs of the layers in each image:\n\n$ docker image inspect --format \"{{json .RootFS.Layers}}\" acme/my-base-image:1.0\n\n[\n\n  \"sha256:72e830a4dff5f0d5225cdc0a320e85ab1ce06ea5673acfe8d83a7645cbd0e9cf\",\n\n  \"sha256:07b4a9068b6af337e8b8f1f1dae3dd14185b2c0003a9a1f0a6fd2587495b204a\"\n\n]\n\n$ docker image inspect --format \"{{json .RootFS.Layers}}\" acme/my-final-image:1.0\n\n[\n\n  \"sha256:72e830a4dff5f0d5225cdc0a320e85ab1ce06ea5673acfe8d83a7645cbd0e9cf\",\n\n  \"sha256:07b4a9068b6af337e8b8f1f1dae3dd14185b2c0003a9a1f0a6fd2587495b204a\",\n\n  \"sha256:cc644054967e516db4689b5282ee98e4bc4b11ea2255c9630309f559ab96562e\",\n\n  \"sha256:e84fb818852626e89a09f5143dbc31fe7f0e0a6a24cd8d2eb68062b904337af4\"\n\n]\n\n\nNotice that the first two layers are identical in both images. The second image adds two additional layers. Shared image layers are only stored once in /var/lib/docker/ and are also shared when pushing and pulling an image to an image registry. Shared image layers can therefore reduce network bandwidth and storage.\n\nTip\n\nFormat output of Docker commands with the --format option.\n\nThe examples above use the docker image inspect command with the --format option to view the layer IDs, formatted as a JSON array. The --format option on Docker commands can be a powerful feature that allows you to extract and format specific information from the output, without requiring additional tools such as awk or sed. To learn more about formatting the output of docker commands using the --format flag, refer to the format command and log output section. We also pretty-printed the JSON output using the jq utility for readability.\n\nCopying makes containers efficient\n\nWhen you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. Any files the container doesn't change don't get copied to this writable layer. This means that the writable layer is as small as possible.\n\nWhen an existing file in a container is modified, the storage driver performs a copy-on-write operation. The specific steps involved depend on the specific storage driver. For the overlay2 driver, the copy-on-write operation follows this rough sequence:\n\nSearch through the image layers for the file to update. The process starts at the newest layer and works down to the base layer one layer at a time. When results are found, they're added to a cache to speed future operations.\nPerform a copy_up operation on the first copy of the file that's found, to copy the file to the container's writable layer.\nAny modifications are made to this copy of the file, and the container can't see the read-only copy of the file that exists in the lower layer.\n\nBtrfs, ZFS, and other drivers handle the copy-on-write differently. You can read more about the methods of these drivers later in their detailed descriptions.\n\nContainers that write a lot of data consume more space than containers that don't. This is because most write operations consume new space in the container's thin writable top layer. Note that changing the metadata of files, for example, changing file permissions or ownership of a file, can also result in a copy_up operation, therefore duplicating the file to the writable layer.\n\nTip\n\nUse volumes for write-heavy applications.\n\nDon't store the data in the container for write-heavy applications. Such applications, for example write-intensive databases, are known to be problematic particularly when pre-existing data exists in the read-only layer.\n\nInstead, use Docker volumes, which are independent of the running container, and designed to be efficient for I/O. In addition, volumes can be shared among containers and don't increase the size of your container's writable layer. Refer to the use volumes section to learn about volumes.\n\nA copy_up operation can incur a noticeable performance overhead. This overhead is different depending on which storage driver is in use. Large files, lots of layers, and deep directory trees can make the impact more noticeable. This is mitigated by the fact that each copy_up operation only occurs the first time a given file is modified.\n\nTo verify the way that copy-on-write works, the following procedure spins up 5 containers based on the acme/my-final-image:1.0 image we built earlier and examines how much room they take up.\n\nFrom a terminal on your Docker host, run the following docker run commands. The strings at the end are the IDs of each container.\n\n$ docker run -dit --name my_container_1 acme/my-final-image:1.0 bash \\\n\n  && docker run -dit --name my_container_2 acme/my-final-image:1.0 bash \\\n\n  && docker run -dit --name my_container_3 acme/my-final-image:1.0 bash \\\n\n  && docker run -dit --name my_container_4 acme/my-final-image:1.0 bash \\\n\n  && docker run -dit --name my_container_5 acme/my-final-image:1.0 bash\n\n\n\n40ebdd7634162eb42bdb1ba76a395095527e9c0aa40348e6c325bd0aa289423c\n\na5ff32e2b551168b9498870faf16c9cd0af820edf8a5c157f7b80da59d01a107\n\n3ed3c1a10430e09f253704116965b01ca920202d52f3bf381fbb833b8ae356bc\n\n939b3bf9e7ece24bcffec57d974c939da2bdcc6a5077b5459c897c1e2fa37a39\n\ncddae31c314fbab3f7eabeb9b26733838187abc9a2ed53f97bd5b04cd7984a5a\n\n\nRun the docker ps command with the --size option to verify the 5 containers are running, and to see each container's size.\n\n$ docker ps --size --format \"table {{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.Size}}\"\n\n\n\nCONTAINER ID   IMAGE                     NAMES            SIZE\n\ncddae31c314f   acme/my-final-image:1.0   my_container_5   0B (virtual 7.75MB)\n\n939b3bf9e7ec   acme/my-final-image:1.0   my_container_4   0B (virtual 7.75MB)\n\n3ed3c1a10430   acme/my-final-image:1.0   my_container_3   0B (virtual 7.75MB)\n\na5ff32e2b551   acme/my-final-image:1.0   my_container_2   0B (virtual 7.75MB)\n\n40ebdd763416   acme/my-final-image:1.0   my_container_1   0B (virtual 7.75MB)\n\n\nThe output above shows that all containers share the image's read-only layers (7.75MB), but no data was written to the container's filesystem, so no additional storage is used for the containers.\n\nAdvanced: metadata and logs storage used for containers\n\nPer-container storage\n\nTo demonstrate this, run the following command to write the word 'hello' to a file on the container's writable layer in containers my_container_1, my_container_2, and my_container_3:\n\n$ for i in {1..3}; do docker exec my_container_$i sh -c 'printf hello > /out.txt'; done\n\n\nRunning the docker ps command again afterward shows that those containers now consume 5 bytes each. This data is unique to each container, and not shared. The read-only layers of the containers aren't affected, and are still shared by all containers.\n\n$ docker ps --size --format \"table {{.ID}}\\t{{.Image}}\\t{{.Names}}\\t{{.Size}}\"\n\n\n\nCONTAINER ID   IMAGE                     NAMES            SIZE\n\ncddae31c314f   acme/my-final-image:1.0   my_container_5   0B (virtual 7.75MB)\n\n939b3bf9e7ec   acme/my-final-image:1.0   my_container_4   0B (virtual 7.75MB)\n\n3ed3c1a10430   acme/my-final-image:1.0   my_container_3   5B (virtual 7.75MB)\n\na5ff32e2b551   acme/my-final-image:1.0   my_container_2   5B (virtual 7.75MB)\n\n40ebdd763416   acme/my-final-image:1.0   my_container_1   5B (virtual 7.75MB)\n\n\nThe previous examples illustrate how copy-on-write filesystems help make containers efficient. Not only does copy-on-write save space, but it also reduces container start-up time. When you create a container (or multiple containers from the same image), Docker only needs to create the thin writable container layer.\n\nIf Docker had to make an entire copy of the underlying image stack each time it created a new container, container creation times and disk space used would be significantly increased. This would be similar to the way that virtual machines work, with one or more virtual disks per virtual machine. The vfs storage doesn't provide a CoW filesystem or other optimizations. When using this storage driver, a full copy of the image's data is created for each container.\n\nRelated information\nVolumes\nSelect a storage driver\n\nEdit this page\n\nRequest changes\n\nTable of contents\nStorage drivers versus Docker volumes\nImages and layers\nContainer and layers\nContainer size on disk\nThe copy-on-write (CoW) strategy\nSharing promotes smaller images\nCopying makes containers efficient\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995165,
    "timestamp": "2026-02-07T06:32:16.252Z",
    "title": "tmpfs mounts | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/tmpfs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\ntmpfs mounts\ntmpfs mounts\nCopy as Markdown\n\nVolumes and bind mounts let you share files between the host machine and container so that you can persist data even after the container is stopped.\n\nIf you're running Docker on Linux, you have a third option: tmpfs mounts. When you create a container with a tmpfs mount, the container can create files outside the container's writable layer.\n\nAs opposed to volumes and bind mounts, a tmpfs mount is temporary, and only persisted in the host memory. When the container stops, the tmpfs mount is removed, and files written there won't be persisted.\n\ntmpfs mounts are best used for cases when you do not want the data to persist either on the host machine or within the container. This may be for security reasons or to protect the performance of the container when your application needs to write a large volume of non-persistent state data.\n\nImportant\n\ntmpfs mounts in Docker map directly to tmpfs in the Linux kernel. As such, the temporary data may be written to a swap file, and thereby persisted to the filesystem.\n\nMounting over existing data\n\nIf you create a tmpfs mount into a directory in the container in which files or directories exist, the pre-existing files are obscured by the mount. This is similar to if you were to save files into /mnt on a Linux host, and then mounted a USB drive into /mnt. The contents of /mnt would be obscured by the contents of the USB drive until the USB drive was unmounted.\n\nWith containers, there's no straightforward way of removing a mount to reveal the obscured files again. Your best option is to recreate the container without the mount.\n\nLimitations of tmpfs mounts\nUnlike volumes and bind mounts, you can't share tmpfs mounts between containers.\nThis functionality is only available if you're running Docker on Linux.\nSetting permissions on tmpfs may cause them to reset after container restart. In some cases setting the uid/gid can serve as a workaround.\nSyntax\n\nTo mount a tmpfs with the docker run command, you can use either the --mount or --tmpfs flag.\n\n$ docker run --mount type=tmpfs,dst=<mount-path>\n\n$ docker run --tmpfs <mount-path>\n\n\nIn general, --mount is preferred. The main difference is that the --mount flag is more explicit. On the other hand, --tmpfs is less verbose and gives you more flexibility as it lets you set more mount options.\n\nThe --tmpfs flag cannot be used with swarm services. You must use --mount.\n\nOptions for --tmpfs\n\nThe --tmpfs flag consists of two fields, separated by a colon character (:).\n\n$ docker run --tmpfs <mount-path>[:opts]\n\n\nThe first field is the container path to mount into a tmpfs. The second field is optional and lets you set mount options. Valid mount options for --tmpfs include:\n\nOption\tDescription\nro\tCreates a read-only tmpfs mount.\nrw\tCreates a read-write tmpfs mount (default behavior).\nnosuid\tPrevents setuid and setgid bits from being honored during execution.\nsuid\tAllows setuid and setgid bits to be honored during execution (default behavior).\nnodev\tDevice files can be created but are not functional (access results in an error).\ndev\tDevice files can be created and are fully functional.\nexec\tAllows the execution of executable binaries in the mounted file system.\nnoexec\tDoes not allow the execution of executable binaries in the mounted file system.\nsync\tAll I/O to the file system is done synchronously.\nasync\tAll I/O to the file system is done asynchronously (default behavior).\ndirsync\tDirectory updates within the file system are done synchronously.\natime\tUpdates file access time each time the file is accessed.\nnoatime\tDoes not update file access times when the file is accessed.\ndiratime\tUpdates directory access times each time the directory is accessed.\nnodiratime\tDoes not update directory access times when the directory is accessed.\nsize\tSpecifies the size of the tmpfs mount, for example, size=64m.\nmode\tSpecifies the file mode (permissions) for the tmpfs mount (for example, mode=1777).\nuid\tSpecifies the user ID for the owner of the tmpfs mount (for example, uid=1000).\ngid\tSpecifies the group ID for the owner of the tmpfs mount (for example, gid=1000).\nnr_inodes\tSpecifies the maximum number of inodes for the tmpfs mount (for example, nr_inodes=400k).\nnr_blocks\tSpecifies the maximum number of blocks for the tmpfs mount (for example, nr_blocks=1024).\nExample\n$ docker run --tmpfs /data:noexec,size=1024,mode=1777\n\n\nNot all tmpfs mount features available in the Linux mount command are supported with the --tmpfs flag. If you require advanced tmpfs options or features, you may need to use a privileged container or configure the mount outside of Docker.\n\nCaution\n\nRunning containers with --privileged grants elevated permissions and can expose the host system to security risks. Use this option only when absolutely necessary and in trusted environments.\n\n$ docker run --privileged -it debian sh\n\n/# mount -t tmpfs -o <options> tmpfs /data\n\nOptions for --mount\n\nThe --mount flag consists of multiple key-value pairs, separated by commas and each consisting of a <key>=<value> tuple. The order of the keys isn't significant.\n\n$ docker run --mount type=tmpfs,dst=<mount-path>[,<key>=<value>...]\n\n\nValid options for --mount type=tmpfs include:\n\nOption\tDescription\ndestination, dst, target\tContainer path to mount into a tmpfs.\ntmpfs-size\tSize of the tmpfs mount in bytes. If unset, the default maximum size of a tmpfs volume is 50% of the host's total RAM.\ntmpfs-mode\tFile mode of the tmpfs in octal. For instance, 700 or 0770. Defaults to 1777 or world-writable.\nExample\n$ docker run --mount type=tmpfs,dst=/app,tmpfs-size=21474836480,tmpfs-mode=1770\n\nUse a tmpfs mount in a container\n\nTo use a tmpfs mount in a container, use the --tmpfs flag, or use the --mount flag with type=tmpfs and destination options. There is no source for tmpfs mounts. The following example creates a tmpfs mount at /app in a Nginx container. The first example uses the --mount flag and the second uses the --tmpfs flag.\n\n--mount --tmpfs\n$ docker run -d \\\n\n  -it \\\n\n  --name tmptest \\\n\n  --mount type=tmpfs,destination=/app \\\n\n  nginx:latest\n\n\nVerify that the mount is a tmpfs mount by looking in the Mounts section of the docker inspect output:\n\n$ docker inspect tmptest --format '{{ json .Mounts }}'\n\n[{\"Type\":\"tmpfs\",\"Source\":\"\",\"Destination\":\"/app\",\"Mode\":\"\",\"RW\":true,\"Propagation\":\"\"}]\n\n\nStop and remove the container:\n\n$ docker stop tmptest\n\n$ docker rm tmptest\n\nNext steps\nLearn about volumes\nLearn about bind mounts\nLearn about storage drivers\n\nEdit this page\n\nRequest changes\n\nTable of contents\nMounting over existing data\nLimitations of tmpfs mounts\nSyntax\nOptions for --tmpfs\nOptions for --mount\nUse a tmpfs mount in a container\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995171,
    "timestamp": "2026-02-07T06:32:16.262Z",
    "title": "Select a storage driver | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/drivers/select-storage-driver/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\nSelect a storage driver\nBTRFS storage driver\nDevice Mapper storage driver (deprecated)\nOverlayFS storage driver\nVFS storage driver\nwindowsfilter storage driver\nZFS storage driver\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nStorage drivers\n/\nSelect a storage driver\nSelect a storage driver\nCopy as Markdown\n\nIdeally, very little data is written to a container's writable layer, and you use Docker volumes to write data. However, some workloads require you to be able to write to the container's writable layer. This is where storage drivers come in.\n\nNote\n\nDocker Engine 29.0 and later uses the containerd image store by default for fresh installations. If you upgraded from an earlier version, your daemon continues using the classic storage drivers described on this page. You can migrate to the containerd image store following the instructions in the containerd image store documentation.\n\nDocker supports several storage drivers, using a pluggable architecture. The storage driver controls how images and containers are stored and managed on your Docker host. After you have read the storage driver overview, the next step is to choose the best storage driver for your workloads. Use the storage driver with the best overall performance and stability in the most usual scenarios.\n\nNote\n\nThis page discusses storage drivers for Docker Engine on Linux. If you're running the Docker daemon with Windows as the host OS, the only supported storage driver is windowsfilter. For more information, see windowsfilter.\n\nThe Docker Engine provides the following storage backends on Linux:\n\nBackend\tDescription\ncontainerd (snapshotters)\tThe default for Docker Engine 29.0 and later. Uses containerd snapshotters for image storage. Supports multi-platform images and attestations. See containerd image store for details.\noverlay2\tClassic storage driver. Most widely compatible across all currently supported Linux distributions, and requires no extra configuration.\nfuse-overlayfs\tPreferred only for running Rootless Docker on hosts that don't support rootless overlay2. Not needed since Linux kernel 5.11, as overlay2 works in rootless mode. See rootless mode documentation for details.\nbtrfs and zfs\tAllow for advanced options, such as creating snapshots, but require more maintenance and setup. Each relies on the backing filesystem being configured correctly.\nvfs\tIntended for testing purposes, and for situations where no copy-on-write filesystem can be used. Performance is poor, and not generally recommended for production use.\n\nThe Docker Engine has a prioritized list of which storage driver to use if no storage driver is explicitly configured, assuming that the storage driver meets the prerequisites, and automatically selects a compatible storage driver. You can see the order in the source code for Docker Engine 29.2.1.\n\nSome storage drivers require you to use a specific format for the backing filesystem. If you have external requirements to use a specific backing filesystem, this may limit your choices. See Supported backing filesystems.\n\nAfter you have narrowed down which storage drivers you can choose from, your choice is determined by the characteristics of your workload and the level of stability you need. See Other considerations for help in making the final decision.\n\nSupported storage drivers per Linux distribution\nNote\n\nModifying the storage driver by editing the daemon configuration file isn't supported on Docker Desktop. Docker Desktop uses the containerd image store by default (version 4.34 and later for clean installs). The following table is also not applicable for the Docker Engine in rootless mode. For the drivers available in rootless mode, see the Rootless mode documentation.\n\nThis section applies to classic storage drivers only. If you're using the containerd image store (the default for Docker Engine 29.0+), see the containerd image store documentation instead.\n\nYour operating system and kernel may not support every classic storage driver. For example, btrfs is only supported if your system uses btrfs as storage. In general, the following configurations work on recent versions of the Linux distribution:\n\nLinux distribution\tDefault classic driver\tAlternative drivers\nUbuntu\toverlay2\tzfs, vfs\nDebian\toverlay2\tvfs\nCentOS\toverlay2\tzfs, vfs\nFedora\toverlay2\tzfs, vfs\nSLES 15\toverlay2\tvfs\nRHEL\toverlay2\tvfs\n\nFor systems using classic storage drivers, overlay2 provides broad compatibility across Linux distributions. Use Docker volumes for write-heavy workloads instead of relying on writing data to the container's writable layer.\n\nThe vfs storage driver is usually not the best choice, and primarily intended for debugging purposes in situations where no other storage-driver is supported. Before using the vfs storage driver, be sure to read about its performance and storage characteristics and limitations.\n\nThe recommendations in the table above are known to work for a large number of users. If you use a recommended configuration and find a reproducible issue, it's likely to be fixed very quickly. If the driver that you want to use is not recommended according to this table, you can run it at your own risk. You can and should still report any issues you run into. However, such issues have a lower priority than issues encountered when using a recommended configuration.\n\nDepending on your Linux distribution, other storage-drivers, such as btrfs may be available. These storage drivers can have advantages for specific use-cases, but may require additional set-up or maintenance, which make them not recommended for common scenarios. Refer to the documentation for those storage drivers for details.\n\nSupported backing filesystems\n\nWith regard to Docker, the backing filesystem is the filesystem where /var/lib/docker/ is located. Some storage drivers only work with specific backing filesystems.\n\nStorage driver\tSupported backing filesystems\noverlay2\txfs with ftype=1, ext4, btrfs, (and more)\nfuse-overlayfs\tany filesystem\nbtrfs\tbtrfs\nzfs\tzfs\nvfs\tany filesystem\nNote\n\nMost filesystems should work if they have the required features. Consult OverlayFS for more information.\n\nOther considerations\nSuitability for your workload\n\nAmong other things, each storage driver has its own performance characteristics that make it more or less suitable for different workloads. Consider the following generalizations:\n\noverlay2 operates at the file level rather than the block level. This uses memory more efficiently, but the container's writable layer may grow quite large in write-heavy workloads.\nBlock-level storage drivers such as btrfs, and zfs perform better for write-heavy workloads (though not as well as Docker volumes).\nbtrfs and zfs require a lot of memory.\nzfs is a good choice for high-density workloads such as PaaS.\n\nMore information about performance, suitability, and best practices is available in the documentation for each storage driver.\n\nShared storage systems and the storage driver\n\nIf you use SAN, NAS, hardware RAID, or other shared storage systems, those systems may provide high availability, increased performance, thin provisioning, deduplication, and compression. In many cases, Docker can work on top of these storage systems, but Docker doesn't closely integrate with them.\n\nEach Docker storage driver is based on a Linux filesystem or volume manager. Be sure to follow existing best practices for operating your storage driver (filesystem or volume manager) on top of your shared storage system. For example, if using the ZFS storage driver on top of a shared storage system, be sure to follow best practices for operating ZFS filesystems on top of that specific shared storage system.\n\nStability\n\nFor some users, stability is more important than performance. Though Docker considers all of the storage drivers mentioned here to be stable, some are newer and are still under active development. In general, overlay2 provides the highest stability.\n\nTest with your own workloads\n\nYou can test Docker's performance when running your own workloads on different storage drivers. Make sure to use equivalent hardware and workloads to match production conditions, so you can see which storage driver offers the best overall performance.\n\nCheck your current storage driver\n\nThe detailed documentation for each individual storage driver details all of the set-up steps to use a given storage driver.\n\nTo see what storage driver Docker is currently using, use docker info and look for the Storage Driver line:\n\n$ docker info\n\n\n\nContainers: 0\n\nImages: 0\n\nStorage Driver: overlay2\n\n Backing Filesystem: xfs\n\n<...>\n\n\nTo change the storage driver, see the specific instructions for the new storage driver. Some drivers require additional configuration, including configuration to physical or logical disks on the Docker host.\n\nImportant\n\nWhen you change the storage driver, any existing images and containers become inaccessible. This is because their layers can't be used by the new storage driver. If you revert your changes, you can access the old images and containers again, but any that you pulled or created using the new driver are then inaccessible.\n\nRelated information\nStorage drivers\noverlay2 storage driver\nbtrfs storage driver\nzfs storage driver\nwindowsfilter storage driver\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSupported storage drivers per Linux distribution\nSupported backing filesystems\nOther considerations\nSuitability for your workload\nShared storage systems and the storage driver\nStability\nTest with your own workloads\nCheck your current storage driver\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995174,
    "timestamp": "2026-02-07T06:32:16.263Z",
    "title": "BTRFS storage driver | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/drivers/btrfs-driver/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\nSelect a storage driver\nBTRFS storage driver\nDevice Mapper storage driver (deprecated)\nOverlayFS storage driver\nVFS storage driver\nwindowsfilter storage driver\nZFS storage driver\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nStorage drivers\n/\nBTRFS storage driver\nBTRFS storage driver\nCopy as Markdown\nImportant\n\nIn most cases you should use the overlay2 storage driver - it's not required to use the btrfs storage driver simply because your system uses Btrfs as its root filesystem.\n\nBtrfs driver has known issues. See Moby issue #27653 for more information.\n\nBtrfs is a copy-on-write filesystem that supports many advanced storage technologies, making it a good fit for Docker. Btrfs is included in the mainline Linux kernel.\n\nDocker's btrfs storage driver leverages many Btrfs features for image and container management. Among these features are block-level operations, thin provisioning, copy-on-write snapshots, and ease of administration. You can combine multiple physical block devices into a single Btrfs filesystem.\n\nThis page refers to Docker's Btrfs storage driver as btrfs and the overall Btrfs Filesystem as Btrfs.\n\nNote\n\nThe btrfs storage driver is only supported with Docker Engine CE on SLES, Ubuntu, and Debian systems.\n\nPrerequisites\n\nbtrfs is supported if you meet the following prerequisites:\n\nbtrfs is only recommended with Docker CE on Ubuntu or Debian systems.\n\nChanging the storage driver makes any containers you have already created inaccessible on the local system. Use docker save to save containers, and push existing images to Docker Hub or a private repository, so that you do not need to re-create them later.\n\nbtrfs requires a dedicated block storage device such as a physical disk. This block device must be formatted for Btrfs and mounted into /var/lib/docker/. The configuration instructions below walk you through this procedure. By default, the SLES / filesystem is formatted with Btrfs, so for SLES, you do not need to use a separate block device, but you can choose to do so for performance reasons.\n\nbtrfs support must exist in your kernel. To check this, run the following command:\n\n$ grep btrfs /proc/filesystems\n\n\n\nbtrfs\n\n\nTo manage Btrfs filesystems at the level of the operating system, you need the btrfs command. If you don't have this command, install the btrfsprogs package (SLES) or btrfs-tools package (Ubuntu).\n\nConfigure Docker to use the btrfs storage driver\n\nThis procedure is essentially identical on SLES and Ubuntu.\n\nStop Docker.\n\nCopy the contents of /var/lib/docker/ to a backup location, then empty the contents of /var/lib/docker/:\n\n$ sudo cp -au /var/lib/docker /var/lib/docker.bk\n\n$ sudo rm -rf /var/lib/docker/*\n\n\nFormat your dedicated block device or devices as a Btrfs filesystem. This example assumes that you are using two block devices called /dev/xvdf and /dev/xvdg. Double-check the block device names because this is a destructive operation.\n\n$ sudo mkfs.btrfs -f /dev/xvdf /dev/xvdg\n\n\nThere are many more options for Btrfs, including striping and RAID. See the Btrfs documentation.\n\nMount the new Btrfs filesystem on the /var/lib/docker/ mount point. You can specify any of the block devices used to create the Btrfs filesystem.\n\n$ sudo mount -t btrfs /dev/xvdf /var/lib/docker\n\nNote\n\nMake the change permanent across reboots by adding an entry to /etc/fstab.\n\nCopy the contents of /var/lib/docker.bk to /var/lib/docker/.\n\n$ sudo cp -au /var/lib/docker.bk/* /var/lib/docker/\n\n\nConfigure Docker to use the btrfs storage driver. This is required even though /var/lib/docker/ is now using a Btrfs filesystem. Edit or create the file /etc/docker/daemon.json. If it is a new file, add the following contents. If it is an existing file, add the key and value only, being careful to end the line with a comma if it isn't the final line before an ending curly bracket (}).\n\n{\n\n  \"storage-driver\": \"btrfs\"\n\n}\n\nSee all storage options for each storage driver in the daemon reference documentation\n\nStart Docker. When it's running, verify that btrfs is being used as the storage driver.\n\n$ docker info\n\n\n\nContainers: 0\n\n Running: 0\n\n Paused: 0\n\n Stopped: 0\n\nImages: 0\n\nServer Version: 17.03.1-ce\n\nStorage Driver: btrfs\n\n Build Version: Btrfs v4.4\n\n Library Version: 101\n\n<...>\n\n\nWhen you are ready, remove the /var/lib/docker.bk directory.\n\nManage a Btrfs volume\n\nOne of the benefits of Btrfs is the ease of managing Btrfs filesystems without the need to unmount the filesystem or restart Docker.\n\nWhen space gets low, Btrfs automatically expands the volume in chunks of roughly 1 GB.\n\nTo add a block device to a Btrfs volume, use the btrfs device add and btrfs filesystem balance commands.\n\n$ sudo btrfs device add /dev/svdh /var/lib/docker\n\n\n\n$ sudo btrfs filesystem balance /var/lib/docker\n\nNote\n\nWhile you can do these operations with Docker running, performance suffers. It might be best to plan an outage window to balance the Btrfs filesystem.\n\nHow the btrfs storage driver works\n\nThe btrfs storage driver works differently from other storage drivers in that your entire /var/lib/docker/ directory is stored on a Btrfs volume.\n\nImage and container layers on-disk\n\nInformation about image layers and writable container layers is stored in /var/lib/docker/btrfs/subvolumes/. This subdirectory contains one directory per image or container layer, with the unified filesystem built from a layer plus all its parent layers. Subvolumes are natively copy-on-write and have space allocated to them on-demand from an underlying storage pool. They can also be nested and snapshotted. The diagram below shows 4 subvolumes. 'Subvolume 2' and 'Subvolume 3' are nested, whereas 'Subvolume 4' shows its own internal directory tree.\n\nOnly the base layer of an image is stored as a true subvolume. All the other layers are stored as snapshots, which only contain the differences introduced in that layer. You can create snapshots of snapshots as shown in the diagram below.\n\nOn disk, snapshots look and feel just like subvolumes, but in reality they are much smaller and more space-efficient. Copy-on-write is used to maximize storage efficiency and minimize layer size, and writes in the container's writable layer are managed at the block level. The following image shows a subvolume and its snapshot sharing data.\n\nFor maximum efficiency, when a container needs more space, it is allocated in chunks of roughly 1 GB in size.\n\nDocker's btrfs storage driver stores every image layer and container in its own Btrfs subvolume or snapshot. The base layer of an image is stored as a subvolume whereas child image layers and containers are stored as snapshots. This is shown in the diagram below.\n\nThe high level process for creating images and containers on Docker hosts running the btrfs driver is as follows:\n\nThe image's base layer is stored in a Btrfs subvolume under /var/lib/docker/btrfs/subvolumes.\n\nSubsequent image layers are stored as a Btrfs snapshot of the parent layer's subvolume or snapshot, but with the changes introduced by this layer. These differences are stored at the block level.\n\nThe container's writable layer is a Btrfs snapshot of the final image layer, with the differences introduced by the running container. These differences are stored at the block level.\n\nHow container reads and writes work with btrfs\nReading files\n\nA container is a space-efficient snapshot of an image. Metadata in the snapshot points to the actual data blocks in the storage pool. This is the same as with a subvolume. Therefore, reads performed against a snapshot are essentially the same as reads performed against a subvolume.\n\nWriting files\n\nAs a general caution, writing and updating a large number of small files with Btrfs can result in slow performance.\n\nConsider three scenarios where a container opens a file for write access with Btrfs.\n\nWriting new files\n\nWriting a new file to a container invokes an allocate-on-demand operation to allocate new data block to the container's snapshot. The file is then written to this new space. The allocate-on-demand operation is native to all writes with Btrfs and is the same as writing new data to a subvolume. As a result, writing new files to a container's snapshot operates at native Btrfs speeds.\n\nModifying existing files\n\nUpdating an existing file in a container is a copy-on-write operation (redirect-on-write is the Btrfs terminology). The original data is read from the layer where the file currently exists, and only the modified blocks are written into the container's writable layer. Next, the Btrfs driver updates the filesystem metadata in the snapshot to point to this new data. This behavior incurs minor overhead.\n\nDeleting files or directories\n\nIf a container deletes a file or directory that exists in a lower layer, Btrfs masks the existence of the file or directory in the lower layer. If a container creates a file and then deletes it, this operation is performed in the Btrfs filesystem itself and the space is reclaimed.\n\nBtrfs and Docker performance\n\nThere are several factors that influence Docker's performance under the btrfs storage driver.\n\nNote\n\nMany of these factors are mitigated by using Docker volumes for write-heavy workloads, rather than relying on storing data in the container's writable layer. However, in the case of Btrfs, Docker volumes still suffer from these draw-backs unless /var/lib/docker/volumes/ isn't backed by Btrfs.\n\nPage caching\n\nBtrfs doesn't support page cache sharing. This means that each process accessing the same file copies the file into the Docker host's memory. As a result, the btrfs driver may not be the best choice for high-density use cases such as PaaS.\n\nSmall writes\n\nContainers performing lots of small writes (this usage pattern matches what happens when you start and stop many containers in a short period of time, as well) can lead to poor use of Btrfs chunks. This can prematurely fill the Btrfs filesystem and lead to out-of-space conditions on your Docker host. Use btrfs filesys show to closely monitor the amount of free space on your Btrfs device.\n\nSequential writes\n\nBtrfs uses a journaling technique when writing to disk. This can impact the performance of sequential writes, reducing performance by up to 50%.\n\nFragmentation\n\nFragmentation is a natural byproduct of copy-on-write filesystems like Btrfs. Many small random writes can compound this issue. Fragmentation can manifest as CPU spikes when using SSDs or head thrashing when using spinning disks. Either of these issues can harm performance.\n\nIf your Linux kernel version is 3.9 or higher, you can enable the autodefrag feature when mounting a Btrfs volume. Test this feature on your own workloads before deploying it into production, as some tests have shown a negative impact on performance.\n\nSSD performance\n\nBtrfs includes native optimizations for SSD media. To enable these features, mount the Btrfs filesystem with the -o ssd mount option. These optimizations include enhanced SSD write performance by avoiding optimization such as seek optimizations that don't apply to solid-state media.\n\nBalance Btrfs filesystems often\n\nUse operating system utilities such as a cron job to balance the Btrfs filesystem regularly, during non-peak hours. This reclaims unallocated blocks and helps to prevent the filesystem from filling up unnecessarily. You can't rebalance a totally full Btrfs filesystem unless you add additional physical block devices to the filesystem.\n\nSee the Btrfs Wiki.\n\nUse fast storage\n\nSolid-state drives (SSDs) provide faster reads and writes than spinning disks.\n\nUse volumes for write-heavy workloads\n\nVolumes provide the best and most predictable performance for write-heavy workloads. This is because they bypass the storage driver and don't incur any of the potential overheads introduced by thin provisioning and copy-on-write. Volumes have other benefits, such as allowing you to share data among containers and persisting even when no running container is using them.\n\nRelated Information\nVolumes\nUnderstand images, containers, and storage drivers\nSelect a storage driver\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nConfigure Docker to use the btrfs storage driver\nManage a Btrfs volume\nHow the btrfs storage driver works\nImage and container layers on-disk\nHow container reads and writes work with btrfs\nReading files\nWriting files\nBtrfs and Docker performance\nPage caching\nSmall writes\nSequential writes\nFragmentation\nSSD performance\nBalance Btrfs filesystems often\nUse fast storage\nUse volumes for write-heavy workloads\nRelated Information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995177,
    "timestamp": "2026-02-07T06:32:16.265Z",
    "title": "Device Mapper storage driver (deprecated) | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/drivers/device-mapper-driver/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\nSelect a storage driver\nBTRFS storage driver\nDevice Mapper storage driver (deprecated)\nOverlayFS storage driver\nVFS storage driver\nwindowsfilter storage driver\nZFS storage driver\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nStorage drivers\n/\nDevice Mapper storage driver (deprecated)\nDevice Mapper storage driver (deprecated)\nCopy as Markdown\n\nDeprecated\n\nThe Device Mapper driver has been deprecated, and is removed in Docker Engine v25.0. If you are using Device Mapper, you must migrate to a supported storage driver before upgrading to Docker Engine v25.0. Read the Docker storage drivers page for supported storage drivers.\n\nDevice Mapper is a kernel-based framework that underpins many advanced volume management technologies on Linux. Docker's devicemapper storage driver leverages the thin provisioning and snapshotting capabilities of this framework for image and container management. This article refers to the Device Mapper storage driver as devicemapper, and the kernel framework as Device Mapper.\n\nFor the systems where it is supported, devicemapper support is included in the Linux kernel. However, specific configuration is required to use it with Docker.\n\nThe devicemapper driver uses block devices dedicated to Docker and operates at the block level, rather than the file level. These devices can be extended by adding physical storage to your Docker host, and they perform better than using a filesystem at the operating system (OS) level.\n\nPrerequisites\ndevicemapper is supported on Docker Engine - Community running on CentOS, Fedora, SLES 15, Ubuntu, Debian, or RHEL.\ndevicemapper requires the lvm2 and device-mapper-persistent-data packages to be installed.\nChanging the storage driver makes any containers you have already created inaccessible on the local system. Use docker save to save containers, and push existing images to Docker Hub or a private repository, so you do not need to recreate them later.\nConfigure Docker with the devicemapper storage driver\n\nBefore following these procedures, you must first meet all the prerequisites.\n\nConfigure loop-lvm mode for testing\n\nThis configuration is only appropriate for testing. The loop-lvm mode makes use of a 'loopback' mechanism that allows files on the local disk to be read from and written to as if they were an actual physical disk or block device. However, the addition of the loopback mechanism, and interaction with the OS filesystem layer, means that IO operations can be slow and resource-intensive. Use of loopback devices can also introduce race conditions. However, setting up loop-lvm mode can help identify basic issues (such as missing user space packages, kernel drivers, etc.) ahead of attempting the more complex set up required to enable direct-lvm mode. loop-lvm mode should therefore only be used to perform rudimentary testing prior to configuring direct-lvm.\n\nFor production systems, see Configure direct-lvm mode for production.\n\nStop Docker.\n\n$ sudo systemctl stop docker\n\n\nEdit /etc/docker/daemon.json. If it does not yet exist, create it. Assuming that the file was empty, add the following contents.\n\n{\n\n  \"storage-driver\": \"devicemapper\"\n\n}\n\nSee all storage options for each storage driver in the daemon reference documentation\n\nDocker does not start if the daemon.json file contains badly-formed JSON.\n\nStart Docker.\n\n$ sudo systemctl start docker\n\n\nVerify that the daemon is using the devicemapper storage driver. Use the docker info command and look for Storage Driver.\n\n$ docker info\n\n\n\n  Containers: 0\n\n    Running: 0\n\n    Paused: 0\n\n    Stopped: 0\n\n  Images: 0\n\n  Server Version: 17.03.1-ce\n\n  Storage Driver: devicemapper\n\n  Pool Name: docker-202:1-8413957-pool\n\n  Pool Blocksize: 65.54 kB\n\n  Base Device Size: 10.74 GB\n\n  Backing Filesystem: xfs\n\n  Data file: /dev/loop0\n\n  Metadata file: /dev/loop1\n\n  Data Space Used: 11.8 MB\n\n  Data Space Total: 107.4 GB\n\n  Data Space Available: 7.44 GB\n\n  Metadata Space Used: 581.6 KB\n\n  Metadata Space Total: 2.147 GB\n\n  Metadata Space Available: 2.147 GB\n\n  Thin Pool Minimum Free Space: 10.74 GB\n\n  Udev Sync Supported: true\n\n  Deferred Removal Enabled: false\n\n  Deferred Deletion Enabled: false\n\n  Deferred Deleted Device Count: 0\n\n  Data loop file: /var/lib/docker/devicemapper/data\n\n  Metadata loop file: /var/lib/docker/devicemapper/metadata\n\n  Library Version: 1.02.135-RHEL7 (2016-11-16)\n\n<...>\n\n\nThis host is running in loop-lvm mode, which is not supported on production systems. This is indicated by the fact that the Data loop file and a Metadata loop file are on files under /var/lib/docker/devicemapper. These are loopback-mounted sparse files. For production systems, see Configure direct-lvm mode for production.\n\nConfigure direct-lvm mode for production\n\nProduction hosts using the devicemapper storage driver must use direct-lvm mode. This mode uses block devices to create the thin pool. This is faster than using loopback devices, uses system resources more efficiently, and block devices can grow as needed. However, more setup is required than in loop-lvm mode.\n\nAfter you have satisfied the prerequisites, follow the steps below to configure Docker to use the devicemapper storage driver in direct-lvm mode.\n\nWarning\n\nChanging the storage driver makes any containers you have already created inaccessible on the local system. Use docker save to save containers, and push existing images to Docker Hub or a private repository, so you do not need to recreate them later.\n\nAllow Docker to configure direct-lvm mode\n\nDocker can manage the block device for you, simplifying configuration of direct-lvm mode. This is appropriate for fresh Docker setups only. You can only use a single block device. If you need to use multiple block devices, configure direct-lvm mode manually instead. The following new configuration options are available:\n\nOption\tDescription\tRequired?\tDefault\tExample\ndm.directlvm_device\tThe path to the block device to configure for direct-lvm.\tYes\t\tdm.directlvm_device=\"/dev/xvdf\"\ndm.thinp_percent\tThe percentage of space to use for storage from the passed in block device.\tNo\t95\tdm.thinp_percent=95\ndm.thinp_metapercent\tThe percentage of space to use for metadata storage from the passed-in block device.\tNo\t1\tdm.thinp_metapercent=1\ndm.thinp_autoextend_threshold\tThe threshold for when lvm should automatically extend the thin pool as a percentage of the total storage space.\tNo\t80\tdm.thinp_autoextend_threshold=80\ndm.thinp_autoextend_percent\tThe percentage to increase the thin pool by when an autoextend is triggered.\tNo\t20\tdm.thinp_autoextend_percent=20\ndm.directlvm_device_force\tWhether to format the block device even if a filesystem already exists on it. If set to false and a filesystem is present, an error is logged and the filesystem is left intact.\tNo\tfalse\tdm.directlvm_device_force=true\n\nEdit the daemon.json file and set the appropriate options, then restart Docker for the changes to take effect. The following daemon.json configuration sets all of the options in the table above.\n\n{\n\n  \"storage-driver\": \"devicemapper\",\n\n  \"storage-opts\": [\n\n    \"dm.directlvm_device=/dev/xdf\",\n\n    \"dm.thinp_percent=95\",\n\n    \"dm.thinp_metapercent=1\",\n\n    \"dm.thinp_autoextend_threshold=80\",\n\n    \"dm.thinp_autoextend_percent=20\",\n\n    \"dm.directlvm_device_force=false\"\n\n  ]\n\n}\n\nSee all storage options for each storage driver in the daemon reference documentation\n\nRestart Docker for the changes to take effect. Docker invokes the commands to configure the block device for you.\n\nWarning\n\nChanging these values after Docker has prepared the block device for you is not supported and causes an error.\n\nYou still need to perform periodic maintenance tasks.\n\nConfigure direct-lvm mode manually\n\nThe procedure below creates a logical volume configured as a thin pool to use as backing for the storage pool. It assumes that you have a spare block device at /dev/xvdf with enough free space to complete the task. The device identifier and volume sizes may be different in your environment and you should substitute your own values throughout the procedure. The procedure also assumes that the Docker daemon is in the stopped state.\n\nIdentify the block device you want to use. The device is located under /dev/ (such as /dev/xvdf) and needs enough free space to store the images and container layers for the workloads that host runs. A solid state drive is ideal.\n\nStop Docker.\n\n$ sudo systemctl stop docker\n\n\nInstall the following packages:\n\nRHEL / CentOS: device-mapper-persistent-data, lvm2, and all dependencies\n\nUbuntu / Debian / SLES 15: thin-provisioning-tools, lvm2, and all dependencies\n\nCreate a physical volume on your block device from step 1, using the pvcreate command. Substitute your device name for /dev/xvdf.\n\nWarning\n\nThe next few steps are destructive, so be sure that you have specified the correct device.\n\n$ sudo pvcreate /dev/xvdf\n\n\n\nPhysical volume \"/dev/xvdf\" successfully created.\n\n\nCreate a docker volume group on the same device, using the vgcreate command.\n\n$ sudo vgcreate docker /dev/xvdf\n\n\n\nVolume group \"docker\" successfully created\n\n\nCreate two logical volumes named thinpool and thinpoolmeta using the lvcreate command. The last parameter specifies the amount of free space to allow for automatic expanding of the data or metadata if space runs low, as a temporary stop-gap. These are the recommended values.\n\n$ sudo lvcreate --wipesignatures y -n thinpool docker -l 95%VG\n\n\n\nLogical volume \"thinpool\" created.\n\n\n\n$ sudo lvcreate --wipesignatures y -n thinpoolmeta docker -l 1%VG\n\n\n\nLogical volume \"thinpoolmeta\" created.\n\n\nConvert the volumes to a thin pool and a storage location for metadata for the thin pool, using the lvconvert command.\n\n$ sudo lvconvert -y \\\n\n--zero n \\\n\n-c 512K \\\n\n--thinpool docker/thinpool \\\n\n--poolmetadata docker/thinpoolmeta\n\n\n\nWARNING: Converting logical volume docker/thinpool and docker/thinpoolmeta to\n\nthin pool's data and metadata volumes with metadata wiping.\n\nTHIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)\n\nConverted docker/thinpool to thin pool.\n\n\nConfigure autoextension of thin pools via an lvm profile.\n\n$ sudo vi /etc/lvm/profile/docker-thinpool.profile\n\n\nSpecify thin_pool_autoextend_threshold and thin_pool_autoextend_percent values.\n\nthin_pool_autoextend_threshold is the percentage of space used before lvm attempts to autoextend the available space (100 = disabled, not recommended).\n\nthin_pool_autoextend_percent is the amount of space to add to the device when automatically extending (0 = disabled).\n\nThe example below adds 20% more capacity when the disk usage reaches 80%.\n\nactivation {\n\n  thin_pool_autoextend_threshold=80\n\n  thin_pool_autoextend_percent=20\n\n}\n\nSave the file.\n\nApply the LVM profile, using the lvchange command.\n\n$ sudo lvchange --metadataprofile docker-thinpool docker/thinpool\n\n\n\nLogical volume docker/thinpool changed.\n\n\nEnsure monitoring of the logical volume is enabled.\n\n$ sudo lvs -o+seg_monitor\n\n\n\nLV       VG     Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Monitor\n\nthinpool docker twi-a-t--- 95.00g             0.00   0.01                             not monitored\n\n\nIf the output in the Monitor column reports, as above, that the volume is not monitored, then monitoring needs to be explicitly enabled. Without this step, automatic extension of the logical volume will not occur, regardless of any settings in the applied profile.\n\n$ sudo lvchange --monitor y docker/thinpool\n\n\nDouble check that monitoring is now enabled by running the sudo lvs -o+seg_monitor command a second time. The Monitor column should now report the logical volume is being monitored.\n\nIf you have ever run Docker on this host before, or if /var/lib/docker/ exists, move it out of the way so that Docker can use the new LVM pool to store the contents of image and containers.\n\n$ sudo su -\n\n# mkdir /var/lib/docker.bk\n\n# mv /var/lib/docker/* /var/lib/docker.bk\n\n# exit\n\n\nIf any of the following steps fail and you need to restore, you can remove /var/lib/docker and replace it with /var/lib/docker.bk.\n\nEdit /etc/docker/daemon.json and configure the options needed for the devicemapper storage driver. If the file was previously empty, it should now contain the following contents:\n\n{\n\n    \"storage-driver\": \"devicemapper\",\n\n    \"storage-opts\": [\n\n    \"dm.thinpooldev=/dev/mapper/docker-thinpool\",\n\n    \"dm.use_deferred_removal=true\",\n\n    \"dm.use_deferred_deletion=true\"\n\n    ]\n\n}\n\nStart Docker.\n\nsystemd:\n\n$ sudo systemctl start docker\n\n\nservice:\n\n$ sudo service docker start\n\n\nVerify that Docker is using the new configuration using docker info.\n\n$ docker info\n\n\n\nContainers: 0\n\n Running: 0\n\n Paused: 0\n\n Stopped: 0\n\nImages: 0\n\nServer Version: 17.03.1-ce\n\nStorage Driver: devicemapper\n\n Pool Name: docker-thinpool\n\n Pool Blocksize: 524.3 kB\n\n Base Device Size: 10.74 GB\n\n Backing Filesystem: xfs\n\n Data file:\n\n Metadata file:\n\n Data Space Used: 19.92 MB\n\n Data Space Total: 102 GB\n\n Data Space Available: 102 GB\n\n Metadata Space Used: 147.5 kB\n\n Metadata Space Total: 1.07 GB\n\n Metadata Space Available: 1.069 GB\n\n Thin Pool Minimum Free Space: 10.2 GB\n\n Udev Sync Supported: true\n\n Deferred Removal Enabled: true\n\n Deferred Deletion Enabled: true\n\n Deferred Deleted Device Count: 0\n\n Library Version: 1.02.135-RHEL7 (2016-11-16)\n\n<...>\n\n\nIf Docker is configured correctly, the Data file and Metadata file is blank, and the pool name is docker-thinpool.\n\nAfter you have verified that the configuration is correct, you can remove the /var/lib/docker.bk directory which contains the previous configuration.\n\n$ sudo rm -rf /var/lib/docker.bk\n\nManage devicemapper\nMonitor the thin pool\n\nDo not rely on LVM auto-extension alone. The volume group automatically extends, but the volume can still fill up. You can monitor free space on the volume using lvs or lvs -a. Consider using a monitoring tool at the OS level, such as Nagios.\n\nTo view the LVM logs, you can use journalctl:\n\n$ sudo journalctl -fu dm-event.service\n\n\nIf you run into repeated problems with thin pool, you can set the storage option dm.min_free_space to a value (representing a percentage) in /etc/docker/daemon.json. For instance, setting it to 10 ensures that operations fail with a warning when the free space is at or near 10%. See the storage driver options in the Engine daemon reference.\n\nIncrease capacity on a running device\n\nYou can increase the capacity of the pool on a running thin-pool device. This is useful if the data's logical volume is full and the volume group is at full capacity. The specific procedure depends on whether you are using a loop-lvm thin pool or a direct-lvm thin pool.\n\nResize a loop-lvm thin pool\n\nThe easiest way to resize a loop-lvm thin pool is to use the device_tool utility, but you can use operating system utilities instead.\n\nUse the device_tool utility\n\nA community-contributed script called device_tool.go is available in the moby/moby Github repository. You can use this tool to resize a loop-lvm thin pool, avoiding the long process above. This tool is not guaranteed to work, but you should only be using loop-lvm on non-production systems.\n\nIf you do not want to use device_tool, you can resize the thin pool manually instead.\n\nTo use the tool, clone the Github repository, change to the contrib/docker-device-tool, and follow the instructions in the README.md to compile the tool.\n\nUse the tool. The following example resizes the thin pool to 200GB.\n\n$ ./device_tool resize 200GB\n\nUse operating system utilities\n\nIf you do not want to use the device-tool utility, you can resize a loop-lvm thin pool manually using the following procedure.\n\nIn loop-lvm mode, a loopback device is used to store the data, and another to store the metadata. loop-lvm mode is only supported for testing, because it has significant performance and stability drawbacks.\n\nIf you are using loop-lvm mode, the output of docker info shows file paths for Data loop file and Metadata loop file:\n\n$ docker info |grep 'loop file'\n\n\n\n Data loop file: /var/lib/docker/devicemapper/data\n\n Metadata loop file: /var/lib/docker/devicemapper/metadata\n\n\nFollow these steps to increase the size of the thin pool. In this example, the thin pool is 100 GB, and is increased to 200 GB.\n\nList the sizes of the devices.\n\n$ sudo ls -lh /var/lib/docker/devicemapper/\n\n\n\ntotal 1175492\n\n-rw------- 1 root root 100G Mar 30 05:22 data\n\n-rw------- 1 root root 2.0G Mar 31 11:17 metadata\n\n\nIncrease the size of the data file to 200 G using the truncate command, which is used to increase or decrease the size of a file. Note that decreasing the size is a destructive operation.\n\n$ sudo truncate -s 200G /var/lib/docker/devicemapper/data\n\n\nVerify the file size changed.\n\n$ sudo ls -lh /var/lib/docker/devicemapper/\n\n\n\ntotal 1.2G\n\n-rw------- 1 root root 200G Apr 14 08:47 data\n\n-rw------- 1 root root 2.0G Apr 19 13:27 metadata\n\n\nThe loopback file has changed on disk but not in memory. List the size of the loopback device in memory, in GB. Reload it, then list the size again. After the reload, the size is 200 GB.\n\n$ echo $[ $(sudo blockdev --getsize64 /dev/loop0) / 1024 / 1024 / 1024 ]\n\n\n\n100\n\n\n\n$ sudo losetup -c /dev/loop0\n\n\n\n$ echo $[ $(sudo blockdev --getsize64 /dev/loop0) / 1024 / 1024 / 1024 ]\n\n\n\n200\n\n\nReload the devicemapper thin pool.\n\na. Get the pool name first. The pool name is the first field, delimited by :. This command extracts it.\n\n$ sudo dmsetup status | grep ' thin-pool ' | awk -F ': ' {'print $1'}\n\ndocker-8:1-123141-pool\n\n\nb. Dump the device mapper table for the thin pool.\n\n$ sudo dmsetup table docker-8:1-123141-pool\n\n0 209715200 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing\n\n\nc. Calculate the total sectors of the thin pool using the second field of the output. The number is expressed in 512-k sectors. A 100G file has 209715200 512-k sectors. If you double this number to 200G, you get 419430400 512-k sectors.\n\nd. Reload the thin pool with the new sector number, using the following three dmsetup commands.\n\n$ sudo dmsetup suspend docker-8:1-123141-pool\n\n$ sudo dmsetup reload docker-8:1-123141-pool --table '0 419430400 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing'\n\n$ sudo dmsetup resume docker-8:1-123141-pool\n\nResize a direct-lvm thin pool\n\nTo extend a direct-lvm thin pool, you need to first attach a new block device to the Docker host, and make note of the name assigned to it by the kernel. In this example, the new block device is /dev/xvdg.\n\nFollow this procedure to extend a direct-lvm thin pool, substituting your block device and other parameters to suit your situation.\n\nGather information about your volume group.\n\nUse the pvdisplay command to find the physical block devices currently in use by your thin pool, and the volume group's name.\n\n$ sudo pvdisplay |grep 'VG Name'\n\n\n\nPV Name               /dev/xvdf\n\nVG Name               docker\n\n\nIn the following steps, substitute your block device or volume group name as appropriate.\n\nExtend the volume group, using the vgextend command with the VG Name from the previous step, and the name of your new block device.\n\n$ sudo vgextend docker /dev/xvdg\n\n\n\nPhysical volume \"/dev/xvdg\" successfully created.\n\nVolume group \"docker\" successfully extended\n\n\nExtend the docker/thinpool logical volume. This command uses 100% of the volume right away, without auto-extend. To extend the metadata thinpool instead, use docker/thinpool_tmeta.\n\n$ sudo lvextend -l+100%FREE -n docker/thinpool\n\n\n\nSize of logical volume docker/thinpool_tdata changed from 95.00 GiB (24319 extents) to 198.00 GiB (50688 extents).\n\nLogical volume docker/thinpool_tdata successfully resized.\n\n\nVerify the new thin pool size using the Data Space Available field in the output of docker info. If you extended the docker/thinpool_tmeta logical volume instead, look for Metadata Space Available.\n\nStorage Driver: devicemapper\n\n Pool Name: docker-thinpool\n\n Pool Blocksize: 524.3 kB\n\n Base Device Size: 10.74 GB\n\n Backing Filesystem: xfs\n\n Data file:\n\n Metadata file:\n\n Data Space Used: 212.3 MB\n\n Data Space Total: 212.6 GB\n\n Data Space Available: 212.4 GB\n\n Metadata Space Used: 286.7 kB\n\n Metadata Space Total: 1.07 GB\n\n Metadata Space Available: 1.069 GB\n\n<...>\nActivate the devicemapper after reboot\n\nIf you reboot the host and find that the docker service failed to start, look for the error, \"Non existing device\". You need to re-activate the logical volumes with this command:\n\n$ sudo lvchange -ay docker/thinpool\n\nHow the devicemapper storage driver works\nWarning\n\nDo not directly manipulate any files or directories within /var/lib/docker/. These files and directories are managed by Docker.\n\nUse the lsblk command to see the devices and their pools, from the operating system's point of view:\n\n$ sudo lsblk\n\n\n\nNAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n\nxvda                    202:0    0    8G  0 disk\n\n‚îî‚îÄxvda1                 202:1    0    8G  0 part /\n\nxvdf                    202:80   0  100G  0 disk\n\n‚îú‚îÄdocker-thinpool_tmeta 253:0    0 1020M  0 lvm\n\n‚îÇ ‚îî‚îÄdocker-thinpool     253:2    0   95G  0 lvm\n\n‚îî‚îÄdocker-thinpool_tdata 253:1    0   95G  0 lvm\n\n  ‚îî‚îÄdocker-thinpool     253:2    0   95G  0 lvm\n\n\nUse the mount command to see the mount-point Docker is using:\n\n$ mount |grep devicemapper\n\n/dev/xvda1 on /var/lib/docker/devicemapper type xfs (rw,relatime,seclabel,attr2,inode64,noquota)\n\n\nWhen you use devicemapper, Docker stores image and layer contents in the thinpool, and exposes them to containers by mounting them under subdirectories of /var/lib/docker/devicemapper/.\n\nImage and container layers on-disk\n\nThe /var/lib/docker/devicemapper/metadata/ directory contains metadata about the Devicemapper configuration itself and about each image and container layer that exist. The devicemapper storage driver uses snapshots, and this metadata include information about those snapshots. These files are in JSON format.\n\nThe /var/lib/docker/devicemapper/mnt/ directory contains a mount point for each image and container layer that exists. Image layer mount points are empty, but a container's mount point shows the container's filesystem as it appears from within the container.\n\nImage layering and sharing\n\nThe devicemapper storage driver uses dedicated block devices rather than formatted filesystems, and operates on files at the block level for maximum performance during copy-on-write (CoW) operations.\n\nSnapshots\n\nAnother feature of devicemapper is its use of snapshots (also sometimes called thin devices or virtual devices), which store the differences introduced in each layer as very small, lightweight thin pools. Snapshots provide many benefits:\n\nLayers which are shared in common between containers are only stored on disk once, unless they are writable. For instance, if you have 10 different images which are all based on alpine, the alpine image and all its parent images are only stored once each on disk.\n\nSnapshots are an implementation of a copy-on-write (CoW) strategy. This means that a given file or directory is only copied to the container's writable layer when it is modified or deleted by that container.\n\nBecause devicemapper operates at the block level, multiple blocks in a writable layer can be modified simultaneously.\n\nSnapshots can be backed up using standard OS-level backup utilities. Just make a copy of /var/lib/docker/devicemapper/.\n\nDevicemapper workflow\n\nWhen you start Docker with the devicemapper storage driver, all objects related to image and container layers are stored in /var/lib/docker/devicemapper/, which is backed by one or more block-level devices, either loopback devices (testing only) or physical disks.\n\nThe base device is the lowest-level object. This is the thin pool itself. You can examine it using docker info. It contains a filesystem. This base device is the starting point for every image and container layer. The base device is a Device Mapper implementation detail, rather than a Docker layer.\n\nMetadata about the base device and each image or container layer is stored in /var/lib/docker/devicemapper/metadata/ in JSON format. These layers are copy-on-write snapshots, which means that they are empty until they diverge from their parent layers.\n\nEach container's writable layer is mounted on a mountpoint in /var/lib/docker/devicemapper/mnt/. An empty directory exists for each read-only image layer and each stopped container.\n\nEach image layer is a snapshot of the layer below it. The lowest layer of each image is a snapshot of the base device that exists in the pool. When you run a container, it is a snapshot of the image the container is based on. The following example shows a Docker host with two running containers. The first is a ubuntu container and the second is a busybox container.\n\nHow container reads and writes work with devicemapper\nReading files\n\nWith devicemapper, reads happen at the block level. The diagram below shows the high level process for reading a single block (0x44f) in an example container.\n\nAn application makes a read request for block 0x44f in the container. Because the container is a thin snapshot of an image, it doesn't have the block, but it has a pointer to the block on the nearest parent image where it does exist, and it reads the block from there. The block now exists in the container's memory.\n\nWriting files\n\nWriting a new file: With the devicemapper driver, writing new data to a container is accomplished by an allocate-on-demand operation. Each block of the new file is allocated in the container's writable layer and the block is written there.\n\nUpdating an existing file: The relevant block of the file is read from the nearest layer where it exists. When the container writes the file, only the modified blocks are written to the container's writable layer.\n\nDeleting a file or directory: When you delete a file or directory in a container's writable layer, or when an image layer deletes a file that exists in its parent layer, the devicemapper storage driver intercepts further read attempts on that file or directory and responds that the file or directory does not exist.\n\nWriting and then deleting a file: If a container writes to a file and later deletes the file, all of those operations happen in the container's writable layer. In that case, if you are using direct-lvm, the blocks are freed. If you use loop-lvm, the blocks may not be freed. This is another reason not to use loop-lvm in production.\n\nDevice Mapper and Docker performance\n\nallocate-on demand performance impact:\n\nThe devicemapper storage driver uses an allocate-on-demand operation to allocate new blocks from the thin pool into a container's writable layer. Each block is 64KB, so this is the minimum amount of space that is used for a write.\n\nCopy-on-write performance impact: The first time a container modifies a specific block, that block is written to the container's writable layer. Because these writes happen at the level of the block rather than the file, performance impact is minimized. However, writing a large number of blocks can still negatively impact performance, and the devicemapper storage driver may actually perform worse than other storage drivers in this scenario. For write-heavy workloads, you should use data volumes, which bypass the storage driver completely.\n\nPerformance best practices\n\nKeep these things in mind to maximize performance when using the devicemapper storage driver.\n\nUse direct-lvm: The loop-lvm mode is not performant and should never be used in production.\n\nUse fast storage: Solid-state drives (SSDs) provide faster reads and writes than spinning disks.\n\nMemory usage: the devicemapper uses more memory than some other storage drivers. Each launched container loads one or more copies of its files into memory, depending on how many blocks of the same file are being modified at the same time. Due to the memory pressure, the devicemapper storage driver may not be the right choice for certain workloads in high-density use cases.\n\nUse volumes for write-heavy workloads: Volumes provide the best and most predictable performance for write-heavy workloads. This is because they bypass the storage driver and do not incur any of the potential overheads introduced by thin provisioning and copy-on-write. Volumes have other benefits, such as allowing you to share data among containers and persisting even when no running container is using them.\n\nNote\n\nWhen using devicemapper and the json-file log driver, the log files generated by a container are still stored in Docker's dataroot directory, by default /var/lib/docker. If your containers generate lots of log messages, this may lead to increased disk usage or the inability to manage your system due to a full disk. You can configure a log driver to store your container logs externally.\n\nRelated Information\nVolumes\nUnderstand images, containers, and storage drivers\nSelect a storage driver\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nConfigure Docker with the devicemapper storage driver\nConfigure loop-lvm mode for testing\nConfigure direct-lvm mode for production\nManage devicemapper\nMonitor the thin pool\nIncrease capacity on a running device\nActivate the devicemapper after reboot\nHow the devicemapper storage driver works\nImage and container layers on-disk\nImage layering and sharing\nHow container reads and writes work with devicemapper\nReading files\nWriting files\nDevice Mapper and Docker performance\nPerformance best practices\nRelated Information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995183,
    "timestamp": "2026-02-07T06:32:16.273Z",
    "title": "VFS storage driver | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/drivers/vfs-driver/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\nSelect a storage driver\nBTRFS storage driver\nDevice Mapper storage driver (deprecated)\nOverlayFS storage driver\nVFS storage driver\nwindowsfilter storage driver\nZFS storage driver\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nStorage drivers\n/\nVFS storage driver\nVFS storage driver\nCopy as Markdown\n\nThe VFS storage driver isn't a union filesystem. Each layer is a directory on disk, and there is no copy-on-write support. To create a new layer, a \"deep copy\" is done of the previous layer. This leads to lower performance and more space used on disk than other storage drivers. However, it is robust, stable, and works in every environment. It can also be used as a mechanism to verify other storage back-ends against, in a testing environment.\n\nConfigure Docker with the vfs storage driver\n\nStop Docker.\n\n$ sudo systemctl stop docker\n\n\nEdit /etc/docker/daemon.json. If it doesn't yet exist, create it. Assuming that the file was empty, add the following contents.\n\n{\n\n  \"storage-driver\": \"vfs\"\n\n}\n\nIf you want to set a quota to control the maximum size the VFS storage driver can use, set the size option on the storage-opts key.\n\n{\n\n  \"storage-driver\": \"vfs\",\n\n  \"storage-opts\": [\"size=256M\"]\n\n}\n\nDocker doesn't start if the daemon.json file contains invalid JSON.\n\nStart Docker.\n\n$ sudo systemctl start docker\n\n\nVerify that the daemon is using the vfs storage driver. Use the docker info command and look for Storage Driver.\n\n$ docker info\n\n\n\nStorage Driver: vfs\n\n...\n\n\nDocker is now using the vfs storage driver. Docker has automatically created the /var/lib/docker/vfs/ directory, which contains all the layers used by running containers.\n\nHow the vfs storage driver works\n\nEach image layer and the writable container layer are represented on the Docker host as subdirectories within /var/lib/docker/. The union mount provides the unified view of all layers. The directory names don't directly correspond to the IDs of the layers themselves.\n\nVFS doesn't support copy-on-write (COW). Each time a new layer is created, it's a deep copy of its parent layer. These layers are all located under /var/lib/docker/vfs/dir/.\n\nExample: Image and container on-disk constructs\n\nThe following docker pull command shows a Docker host downloading a Docker image comprising five layers.\n\n$ docker pull ubuntu\n\n\n\nUsing default tag: latest\n\nlatest: Pulling from library/ubuntu\n\ne0a742c2abfd: Pull complete\n\n486cb8339a27: Pull complete\n\ndc6f0d824617: Pull complete\n\n4f7a5649a30e: Pull complete\n\n672363445ad2: Pull complete\n\nDigest: sha256:84c334414e2bfdcae99509a6add166bbb4fa4041dc3fa6af08046a66fed3005f\n\nStatus: Downloaded newer image for ubuntu:latest\n\n\nAfter pulling, each of these layers is represented as a subdirectory of /var/lib/docker/vfs/dir/. The directory names do not correlate with the image layer IDs shown in the docker pull command. To see the size taken up on disk by each layer, you can use the du -sh command, which gives the size as a human-readable value.\n\n$ ls -l /var/lib/docker/vfs/dir/\n\n\n\ntotal 0\n\ndrwxr-xr-x.  2 root root  19 Aug  2 18:19 3262dfbe53dac3e1ab7dcc8ad5d8c4d586a11d2ac3c4234892e34bff7f6b821e\n\ndrwxr-xr-x. 21 root root 224 Aug  2 18:23 6af21814449345f55d88c403e66564faad965d6afa84b294ae6e740c9ded2561\n\ndrwxr-xr-x. 21 root root 224 Aug  2 18:23 6d3be4585ba32f9f5cbff0110e8d07aea5f5b9fbb1439677c27e7dfee263171c\n\ndrwxr-xr-x. 21 root root 224 Aug  2 18:23 9ecd2d88ca177413ab89f987e1507325285a7418fc76d0dcb4bc021447ba2bab\n\ndrwxr-xr-x. 21 root root 224 Aug  2 18:23 a292ac6341a65bf3a5da7b7c251e19de1294bd2ec32828de621d41c7ad31f895\n\ndrwxr-xr-x. 21 root root 224 Aug  2 18:23 e92be7a4a4e3ccbb7dd87695bca1a0ea373d4f673f455491b1342b33ed91446b\n\n$ du -sh /var/lib/docker/vfs/dir/*\n\n\n\n4.0K\t/var/lib/docker/vfs/dir/3262dfbe53dac3e1ab7dcc8ad5d8c4d586a11d2ac3c4234892e34bff7f6b821e\n\n125M\t/var/lib/docker/vfs/dir/6af21814449345f55d88c403e66564faad965d6afa84b294ae6e740c9ded2561\n\n104M\t/var/lib/docker/vfs/dir/6d3be4585ba32f9f5cbff0110e8d07aea5f5b9fbb1439677c27e7dfee263171c\n\n125M\t/var/lib/docker/vfs/dir/9ecd2d88ca177413ab89f987e1507325285a7418fc76d0dcb4bc021447ba2bab\n\n104M\t/var/lib/docker/vfs/dir/a292ac6341a65bf3a5da7b7c251e19de1294bd2ec32828de621d41c7ad31f895\n\n104M\t/var/lib/docker/vfs/dir/e92be7a4a4e3ccbb7dd87695bca1a0ea373d4f673f455491b1342b33ed91446b\n\n\nThe above output shows that three layers each take 104M and two take 125M. These directories have only small differences from each other, but they all consume the same amount of disk space. This is one of the disadvantages of using the vfs storage driver.\n\nRelated information\nUnderstand images, containers, and storage drivers\nSelect a storage driver\n\nEdit this page\n\nRequest changes\n\nTable of contents\nConfigure Docker with the vfs storage driver\nHow the vfs storage driver works\nExample: Image and container on-disk constructs\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995180,
    "timestamp": "2026-02-07T06:32:16.273Z",
    "title": "OverlayFS storage driver | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/drivers/overlayfs-driver/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\nSelect a storage driver\nBTRFS storage driver\nDevice Mapper storage driver (deprecated)\nOverlayFS storage driver\nVFS storage driver\nwindowsfilter storage driver\nZFS storage driver\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nStorage drivers\n/\nOverlayFS storage driver\nOverlayFS storage driver\nCopy as Markdown\n\nOverlayFS is a union filesystem.\n\nThis page refers to the Linux kernel driver as OverlayFS and to the Docker storage driver as overlay2.\n\nNote\n\nDocker Engine 29.0 and later uses the containerd image store by default. The overlay2 driver is a legacy storage driver that is superseded by the overlayfs containerd snapshotter. For more information, see Select a storage driver.\n\nNote\n\nFor fuse-overlayfs driver, check Rootless mode documentation.\n\nPrerequisites\n\nThe overlay2 driver is supported if you meet the following prerequisites:\n\nVersion 4.0 or higher of the Linux kernel, or RHEL or CentOS using version 3.10.0-514 of the kernel or higher.\n\nThe overlay2 driver is supported on xfs backing filesystems, but only with d_type=true enabled.\n\nUse xfs_info to verify that the ftype option is set to 1. To format an xfs filesystem correctly, use the flag -n ftype=1.\n\nChanging the storage driver makes existing containers and images inaccessible on the local system. Use docker save to save any images you have built or push them to Docker Hub or a private registry before changing the storage driver, so that you don't need to re-create them later.\n\nConfigure Docker with the overlay2 storage driver\n\nBefore following this procedure, you must first meet all the prerequisites.\n\nThe following steps outline how to configure the overlay2 storage driver.\n\nStop Docker.\n\n$ sudo systemctl stop docker\n\n\nCopy the contents of /var/lib/docker to a temporary location.\n\n$ cp -au /var/lib/docker /var/lib/docker.bk\n\n\nIf you want to use a separate backing filesystem from the one used by /var/lib/, format the filesystem and mount it into /var/lib/docker. Make sure to add this mount to /etc/fstab to make it permanent.\n\nEdit /etc/docker/daemon.json. If it doesn't yet exist, create it. Assuming that the file was empty, add the following contents.\n\n{\n\n  \"storage-driver\": \"overlay2\"\n\n}\n\nDocker doesn't start if the daemon.json file contains invalid JSON.\n\nStart Docker.\n\n$ sudo systemctl start docker\n\n\nVerify that the daemon is using the overlay2 storage driver. Use the docker info command and look for Storage Driver and Backing filesystem.\n\n$ docker info\n\n\n\nContainers: 0\n\nImages: 0\n\nStorage Driver: overlay2\n\n Backing Filesystem: xfs\n\n Supports d_type: true\n\n Native Overlay Diff: true\n\n<...>\n\n\nDocker is now using the overlay2 storage driver and has automatically created the overlay mount with the required lowerdir, upperdir, merged, and workdir constructs.\n\nContinue reading for details about how OverlayFS works within your Docker containers, as well as performance advice and information about limitations of its compatibility with different backing filesystems.\n\nHow the overlay2 driver works\n\nOverlayFS layers two directories on a single Linux host and presents them as a single directory. These directories are called layers, and the unification process is referred to as a union mount. OverlayFS refers to the lower directory as lowerdir and the upper directory as upperdir. The unified view is exposed through its own directory called merged.\n\nThe overlay2 driver natively supports up to 128 lower OverlayFS layers. This capability provides better performance for layer-related Docker commands such as docker build and docker commit, and consumes fewer inodes on the backing filesystem.\n\nImage and container layers on-disk\n\nAfter downloading a five-layer image using docker pull ubuntu, you can see six directories under /var/lib/docker/overlay2.\n\nWarning\n\nDon't directly manipulate any files or directories within /var/lib/docker/. These files and directories are managed by Docker.\n\n$ ls -l /var/lib/docker/overlay2\n\n\n\ntotal 24\n\ndrwx------ 5 root root 4096 Jun 20 07:36 223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7\n\ndrwx------ 3 root root 4096 Jun 20 07:36 3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b\n\ndrwx------ 5 root root 4096 Jun 20 07:36 4e9fa83caff3e8f4cc83693fa407a4a9fac9573deaf481506c102d484dd1e6a1\n\ndrwx------ 5 root root 4096 Jun 20 07:36 e8876a226237217ec61c4baf238a32992291d059fdac95ed6303bdff3f59cff5\n\ndrwx------ 5 root root 4096 Jun 20 07:36 eca1e4e1694283e001f200a667bb3cb40853cf2d1b12c29feda7422fed78afed\n\ndrwx------ 2 root root 4096 Jun 20 07:36 l\n\n\nThe new l (lowercase L) directory contains shortened layer identifiers as symbolic links. These identifiers are used to avoid hitting the page size limitation on arguments to the mount command.\n\n$ ls -l /var/lib/docker/overlay2/l\n\n\n\ntotal 20\n\nlrwxrwxrwx 1 root root 72 Jun 20 07:36 6Y5IM2XC7TSNIJZZFLJCS6I4I4 -> ../3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b/diff\n\nlrwxrwxrwx 1 root root 72 Jun 20 07:36 B3WWEFKBG3PLLV737KZFIASSW7 -> ../4e9fa83caff3e8f4cc83693fa407a4a9fac9573deaf481506c102d484dd1e6a1/diff\n\nlrwxrwxrwx 1 root root 72 Jun 20 07:36 JEYMODZYFCZFYSDABYXD5MF6YO -> ../eca1e4e1694283e001f200a667bb3cb40853cf2d1b12c29feda7422fed78afed/diff\n\nlrwxrwxrwx 1 root root 72 Jun 20 07:36 NFYKDW6APBCCUCTOUSYDH4DXAT -> ../223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7/diff\n\nlrwxrwxrwx 1 root root 72 Jun 20 07:36 UL2MW33MSE3Q5VYIKBRN4ZAGQP -> ../e8876a226237217ec61c4baf238a32992291d059fdac95ed6303bdff3f59cff5/diff\n\n\nThe lowest layer contains a file called link, which contains the name of the shortened identifier, and a directory called diff which contains the layer's contents.\n\n$ ls /var/lib/docker/overlay2/3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b/\n\n\n\ndiff  link\n\n\n\n$ cat /var/lib/docker/overlay2/3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b/link\n\n\n\n6Y5IM2XC7TSNIJZZFLJCS6I4I4\n\n\n\n$ ls  /var/lib/docker/overlay2/3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b/diff\n\n\n\nbin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n\n\nThe second-lowest layer, and each higher layer, contain a file called lower, which denotes its parent, and a directory called diff which contains its contents. It also contains a merged directory, which contains the unified contents of its parent layer and itself, and a work directory which is used internally by OverlayFS.\n\n$ ls /var/lib/docker/overlay2/223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7\n\n\n\ndiff  link  lower  merged  work\n\n\n\n$ cat /var/lib/docker/overlay2/223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7/lower\n\n\n\nl/6Y5IM2XC7TSNIJZZFLJCS6I4I4\n\n\n\n$ ls /var/lib/docker/overlay2/223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7/diff/\n\n\n\netc  sbin  usr  var\n\n\nTo view the mounts which exist when you use the overlay storage driver with Docker, use the mount command. The output below is truncated for readability.\n\n$ mount | grep overlay\n\n\n\noverlay on /var/lib/docker/overlay2/9186877cdf386d0a3b016149cf30c208f326dca307529e646afce5b3f83f5304/merged\n\ntype overlay (rw,relatime,\n\nlowerdir=l/DJA75GUWHWG7EWICFYX54FIOVT:l/B3WWEFKBG3PLLV737KZFIASSW7:l/JEYMODZYFCZFYSDABYXD5MF6YO:l/UL2MW33MSE3Q5VYIKBRN4ZAGQP:l/NFYKDW6APBCCUCTOUSYDH4DXAT:l/6Y5IM2XC7TSNIJZZFLJCS6I4I4,\n\nupperdir=9186877cdf386d0a3b016149cf30c208f326dca307529e646afce5b3f83f5304/diff,\n\nworkdir=9186877cdf386d0a3b016149cf30c208f326dca307529e646afce5b3f83f5304/work)\n\n\nThe rw on the second line shows that the overlay mount is read-write.\n\nThe following diagram shows how a Docker image and a Docker container are layered. The image layer is the lowerdir and the container layer is the upperdir. If the image has multiple layers, multiple lowerdir directories are used. The unified view is exposed through a directory called merged which is effectively the containers mount point.\n\nWhere the image layer and the container layer contain the same files, the container layer (upperdir) takes precedence and obscures the existence of the same files in the image layer.\n\nTo create a container, the overlay2 driver combines the directory representing the image's top layer plus a new directory for the container. The image's layers are the lowerdirs in the overlay and are read-only. The new directory for the container is the upperdir and is writable.\n\nImage and container layers on-disk\n\nThe following docker pull command shows a Docker host downloading a Docker image comprising five layers.\n\n$ docker pull ubuntu\n\n\n\nUsing default tag: latest\n\nlatest: Pulling from library/ubuntu\n\n\n\n5ba4f30e5bea: Pull complete\n\n9d7d19c9dc56: Pull complete\n\nac6ad7efd0f9: Pull complete\n\ne7491a747824: Pull complete\n\na3ed95caeb02: Pull complete\n\nDigest: sha256:46fb5d001b88ad904c5c732b086b596b92cfb4a4840a3abd0e35dbb6870585e4\n\nStatus: Downloaded newer image for ubuntu:latest\n\nThe image layers\n\nEach image layer has its own directory within /var/lib/docker/overlay/, which contains its contents, as shown in the following example. The image layer IDs don't correspond to the directory IDs.\n\nWarning\n\nDon't directly manipulate any files or directories within /var/lib/docker/. These files and directories are managed by Docker.\n\n$ ls -l /var/lib/docker/overlay/\n\n\n\ntotal 20\n\ndrwx------ 3 root root 4096 Jun 20 16:11 38f3ed2eac129654acef11c32670b534670c3a06e483fce313d72e3e0a15baa8\n\ndrwx------ 3 root root 4096 Jun 20 16:11 55f1e14c361b90570df46371b20ce6d480c434981cbda5fd68c6ff61aa0a5358\n\ndrwx------ 3 root root 4096 Jun 20 16:11 824c8a961a4f5e8fe4f4243dab57c5be798e7fd195f6d88ab06aea92ba931654\n\ndrwx------ 3 root root 4096 Jun 20 16:11 ad0fe55125ebf599da124da175174a4b8c1878afe6907bf7c78570341f308461\n\ndrwx------ 3 root root 4096 Jun 20 16:11 edab9b5e5bf73f2997524eebeac1de4cf9c8b904fa8ad3ec43b3504196aa3801\n\n\nThe image layer directories contain the files unique to that layer as well as hard links to the data shared with lower layers. This allows for efficient use of disk space.\n\n$ ls -i /var/lib/docker/overlay2/38f3ed2eac129654acef11c32670b534670c3a06e483fce313d72e3e0a15baa8/root/bin/ls\n\n\n\n19793696 /var/lib/docker/overlay2/38f3ed2eac129654acef11c32670b534670c3a06e483fce313d72e3e0a15baa8/root/bin/ls\n\n\n\n$ ls -i /var/lib/docker/overlay2/55f1e14c361b90570df46371b20ce6d480c434981cbda5fd68c6ff61aa0a5358/root/bin/ls\n\n\n\n19793696 /var/lib/docker/overlay2/55f1e14c361b90570df46371b20ce6d480c434981cbda5fd68c6ff61aa0a5358/root/bin/ls\n\nThe container layer\n\nContainers also exist on-disk in the Docker host's filesystem under /var/lib/docker/overlay/. If you list a running container's subdirectory using the ls -l command, three directories and one file exist:\n\n$ ls -l /var/lib/docker/overlay2/<directory-of-running-container>\n\n\n\ntotal 16\n\n-rw-r--r-- 1 root root   64 Jun 20 16:39 lower-id\n\ndrwxr-xr-x 1 root root 4096 Jun 20 16:39 merged\n\ndrwxr-xr-x 4 root root 4096 Jun 20 16:39 upper\n\ndrwx------ 3 root root 4096 Jun 20 16:39 work\n\n\nThe lower-id file contains the ID of the top layer of the image the container is based on, which is the OverlayFS lowerdir.\n\n$ cat /var/lib/docker/overlay2/ec444863a55a9f1ca2df72223d459c5d940a721b2288ff86a3f27be28b53be6c/lower-id\n\n\n\n55f1e14c361b90570df46371b20ce6d480c434981cbda5fd68c6ff61aa0a5358\n\n\nThe upper directory contains the contents of the container's read-write layer, which corresponds to the OverlayFS upperdir.\n\nThe merged directory is the union mount of the lowerdir and upperdirs, which comprises the view of the filesystem from within the running container.\n\nThe work directory is internal to OverlayFS.\n\nTo view the mounts which exist when you use the overlay2 storage driver with Docker, use the mount command. The following output is truncated for readability.\n\n$ mount | grep overlay\n\n\n\noverlay on /var/lib/docker/overlay2/l/ec444863a55a.../merged\n\ntype overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/55f1e14c361b.../root,\n\nupperdir=/var/lib/docker/overlay2/l/ec444863a55a.../upper,\n\nworkdir=/var/lib/docker/overlay2/l/ec444863a55a.../work)\n\n\nThe rw on the second line shows that the overlay mount is read-write.\n\nHow container reads and writes work with overlay2\n\nReading files\n\nConsider three scenarios where a container opens a file for read access with overlay.\n\nThe file does not exist in the container layer\n\nIf a container opens a file for read access and the file does not already exist in the container (upperdir) it is read from the image (lowerdir). This incurs very little performance overhead.\n\nThe file only exists in the container layer\n\nIf a container opens a file for read access and the file exists in the container (upperdir) and not in the image (lowerdir), it's read directly from the container.\n\nThe file exists in both the container layer and the image layer\n\nIf a container opens a file for read access and the file exists in the image layer and the container layer, the file's version in the container layer is read. Files in the container layer (upperdir) obscure files with the same name in the image layer (lowerdir).\n\nModifying files or directories\n\nConsider some scenarios where files in a container are modified.\n\nWriting to a file for the first time\n\nThe first time a container writes to an existing file, that file does not exist in the container (upperdir). The overlay2 driver performs a copy_up operation to copy the file from the image (lowerdir) to the container (upperdir). The container then writes the changes to the new copy of the file in the container layer.\n\nHowever, OverlayFS works at the file level rather than the block level. This means that all OverlayFS copy_up operations copy the entire file, even if the file is large and only a small part of it's being modified. This can have a noticeable impact on container write performance. However, two things are worth noting:\n\nThe copy_up operation only occurs the first time a given file is written to. Subsequent writes to the same file operate against the copy of the file already copied up to the container.\n\nOverlayFS works with multiple layers. This means that performance can be impacted when searching for files in images with many layers.\n\nDeleting files and directories\n\nWhen a file is deleted within a container, a whiteout file is created in the container (upperdir). The version of the file in the image layer (lowerdir) is not deleted (because the lowerdir is read-only). However, the whiteout file prevents it from being available to the container.\n\nWhen a directory is deleted within a container, an opaque directory is created within the container (upperdir). This works in the same way as a whiteout file and effectively prevents the directory from being accessed, even though it still exists in the image (lowerdir).\n\nRenaming directories\n\nCalling rename(2) for a directory is allowed only when both the source and the destination path are on the top layer. Otherwise, it returns EXDEV error (\"cross-device link not permitted\"). Your application needs to be designed to handle EXDEV and fall back to a \"copy and unlink\" strategy.\n\nOverlayFS and Docker Performance\n\noverlay2 may perform better than btrfs. However, be aware of the following details:\n\nPage caching\n\nOverlayFS supports page cache sharing. Multiple containers accessing the same file share a single page cache entry for that file. This makes the overlay2 drivers efficient with memory and a good option for high-density use cases such as PaaS.\n\nCopyup\n\nAs with other copy-on-write filesystems, OverlayFS performs copy-up operations whenever a container writes to a file for the first time. This can add latency into the write operation, especially for large files. However, once the file has been copied up, all subsequent writes to that file occur in the upper layer, without the need for further copy-up operations.\n\nPerformance best practices\n\nThe following generic performance best practices apply to OverlayFS.\n\nUse fast storage\n\nSolid-state drives (SSDs) provide faster reads and writes than spinning disks.\n\nUse volumes for write-heavy workloads\n\nVolumes provide the best and most predictable performance for write-heavy workloads. This is because they bypass the storage driver and don't incur any of the potential overheads introduced by thin provisioning and copy-on-write. Volumes have other benefits, such as allowing you to share data among containers and persisting your data even if no running container is using them.\n\nLimitations on OverlayFS compatibility\n\nTo summarize the OverlayFS's aspect which is incompatible with other filesystems:\n\nopen(2)\nOverlayFS only implements a subset of the POSIX standards. This can result in certain OverlayFS operations breaking POSIX standards. One such operation is the copy-up operation. Suppose that your application calls fd1=open(\"foo\", O_RDONLY) and then fd2=open(\"foo\", O_RDWR). In this case, your application expects fd1 and fd2 to refer to the same file. However, due to a copy-up operation that occurs after the second calling to open(2), the descriptors refer to different files. The fd1 continues to reference the file in the image (lowerdir) and the fd2 references the file in the container (upperdir). A workaround for this is to touch the files which causes the copy-up operation to happen. All subsequent open(2) operations regardless of read-only or read-write access mode reference the file in the container (upperdir).\n\nyum is known to be affected unless the yum-plugin-ovl package is installed. If the yum-plugin-ovl package is not available in your distribution such as RHEL/CentOS prior to 6.8 or 7.2, you may need to run touch /var/lib/rpm/* before running yum install. This package implements the touch workaround referenced above for yum.\n\nrename(2)\nOverlayFS does not fully support the rename(2) system call. Your application needs to detect its failure and fall back to a \"copy and unlink\" strategy.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nConfigure Docker with the overlay2 storage driver\nHow the overlay2 driver works\nImage and container layers on-disk\nImage and container layers on-disk\nHow container reads and writes work with overlay2\nReading files\nModifying files or directories\nOverlayFS and Docker Performance\nPage caching\nCopyup\nPerformance best practices\nLimitations on OverlayFS compatibility\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995184,
    "timestamp": "2026-02-07T06:32:16.277Z",
    "title": "windowsfilter storage driver | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/drivers/windowsfilter-driver/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\nSelect a storage driver\nBTRFS storage driver\nDevice Mapper storage driver (deprecated)\nOverlayFS storage driver\nVFS storage driver\nwindowsfilter storage driver\nZFS storage driver\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nStorage drivers\n/\nwindowsfilter storage driver\nwindowsfilter storage driver\nCopy as Markdown\n\nThe windowsfilter storage driver is the default storage driver for Docker Engine on Windows. The windowsfilter driver uses Windows-native file system layers to for storing Docker layers and volume data on disk. The windowsfilter storage driver only works on file systems formatted with NTFS.\n\nConfigure the windowsfilter storage driver\n\nFor most use case, no configuring the windowsfilter storage driver is not necessary.\n\nThe default storage limit for Docker Engine on Windows is 127GB. To use a different storage size, set the size option for the windowsfilter storage driver. See windowsfilter options.\n\nData is stored on the Docker host in image and windowsfilter subdirectories within C:\\ProgramData\\docker by default. You can change the storage location by configuring the data-root option in the Daemon configuration file:\n\n{\n\n  \"data-root\": \"d:\\\\docker\"\n\n}\n\nYou must restart the daemon for the configuration change to take effect.\n\nAdditional information\n\nFor more information about how container storage works on Windows, refer to Microsoft's Containers on Windows documentation.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nConfigure the windowsfilter storage driver\nAdditional information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995189,
    "timestamp": "2026-02-07T06:32:16.288Z",
    "title": "ZFS storage driver | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/drivers/zfs-driver/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\nSelect a storage driver\nBTRFS storage driver\nDevice Mapper storage driver (deprecated)\nOverlayFS storage driver\nVFS storage driver\nwindowsfilter storage driver\nZFS storage driver\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\nStorage drivers\n/\nZFS storage driver\nZFS storage driver\nCopy as Markdown\n\nZFS is a next generation filesystem that supports many advanced storage technologies such as volume management, snapshots, checksumming, compression and deduplication, replication and more.\n\nIt was created by Sun Microsystems (now Oracle Corporation) and is open sourced under the CDDL license. Due to licensing incompatibilities between the CDDL and GPL, ZFS cannot be shipped as part of the mainline Linux kernel. However, the ZFS On Linux (ZoL) project provides an out-of-tree kernel module and userspace tools which can be installed separately.\n\nThe ZFS on Linux (ZoL) port is healthy and maturing. However, at this point in time it is not recommended to use the zfs Docker storage driver for production use unless you have substantial experience with ZFS on Linux.\n\nNote\n\nThere is also a FUSE implementation of ZFS on the Linux platform. This is not recommended. The native ZFS driver (ZoL) is more tested, has better performance, and is more widely used. The remainder of this document refers to the native ZoL port.\n\nPrerequisites\nZFS requires one or more dedicated block devices, preferably solid-state drives (SSDs).\nThe /var/lib/docker/ directory must be mounted on a ZFS-formatted filesystem.\nChanging the storage driver makes any containers you have already created inaccessible on the local system. Use docker save to save containers, and push existing images to Docker Hub or a private repository, so that you do not need to re-create them later.\nNote\n\nThere is no need to use MountFlags=slave because dockerd and containerd are in different mount namespaces.\n\nConfigure Docker with the zfs storage driver\n\nStop Docker.\n\nCopy the contents of /var/lib/docker/ to /var/lib/docker.bk and remove the contents of /var/lib/docker/.\n\n$ sudo cp -au /var/lib/docker /var/lib/docker.bk\n\n\n\n$ sudo rm -rf /var/lib/docker/*\n\n\nCreate a new zpool on your dedicated block device or devices, and mount it into /var/lib/docker/. Be sure you have specified the correct devices, because this is a destructive operation. This example adds two devices to the pool.\n\n$ sudo zpool create -f zpool-docker -m /var/lib/docker /dev/xvdf /dev/xvdg\n\n\nThe command creates the zpool and names it zpool-docker. The name is for display purposes only, and you can use a different name. Check that the pool was created and mounted correctly using zfs list.\n\n$ sudo zfs list\n\n\n\nNAME           USED  AVAIL  REFER  MOUNTPOINT\n\nzpool-docker    55K  96.4G    19K  /var/lib/docker\n\n\nConfigure Docker to use zfs. Edit /etc/docker/daemon.json and set the storage-driver to zfs. If the file was empty before, it should now look like this:\n\n{\n\n  \"storage-driver\": \"zfs\"\n\n}\n\nSave and close the file.\n\nStart Docker. Use docker info to verify that the storage driver is zfs.\n\n$ sudo docker info\n\n  Containers: 0\n\n   Running: 0\n\n   Paused: 0\n\n   Stopped: 0\n\n  Images: 0\n\n  Server Version: 17.03.1-ce\n\n  Storage Driver: zfs\n\n   Zpool: zpool-docker\n\n   Zpool Health: ONLINE\n\n   Parent Dataset: zpool-docker\n\n   Space Used By Parent: 249856\n\n   Space Available: 103498395648\n\n   Parent Quota: no\n\n   Compression: off\n\n<...>\n\nManage zfs\nIncrease capacity on a running device\n\nTo increase the size of the zpool, you need to add a dedicated block device to the Docker host, and then add it to the zpool using the zpool add command:\n\n$ sudo zpool add zpool-docker /dev/xvdh\n\nLimit a container's writable storage quota\n\nIf you want to implement a quota on a per-image/dataset basis, you can set the size storage option to limit the amount of space a single container can use for its writable layer.\n\nEdit /etc/docker/daemon.json and add the following:\n\n{\n\n  \"storage-driver\": \"zfs\",\n\n  \"storage-opts\": [\"size=256M\"]\n\n}\n\nSee all storage options for each storage driver in the daemon reference documentation\n\nSave and close the file, and restart Docker.\n\nHow the zfs storage driver works\n\nZFS uses the following objects:\n\nfilesystems: thinly provisioned, with space allocated from the zpool on demand.\nsnapshots: read-only space-efficient point-in-time copies of filesystems.\nclones: Read-write copies of snapshots. Used for storing the differences from the previous layer.\n\nThe process of creating a clone:\n\nA read-only snapshot is created from the filesystem.\nA writable clone is created from the snapshot. This contains any differences from the parent layer.\n\nFilesystems, snapshots, and clones all allocate space from the underlying zpool.\n\nImage and container layers on-disk\n\nEach running container's unified filesystem is mounted on a mount point in /var/lib/docker/zfs/graph/. Continue reading for an explanation of how the unified filesystem is composed.\n\nImage layering and sharing\n\nThe base layer of an image is a ZFS filesystem. Each child layer is a ZFS clone based on a ZFS snapshot of the layer below it. A container is a ZFS clone based on a ZFS Snapshot of the top layer of the image it's created from.\n\nThe diagram below shows how this is put together with a running container based on a two-layer image.\n\nWhen you start a container, the following steps happen in order:\n\nThe base layer of the image exists on the Docker host as a ZFS filesystem.\n\nAdditional image layers are clones of the dataset hosting the image layer directly below it.\n\nIn the diagram, \"Layer 1\" is added by taking a ZFS snapshot of the base layer and then creating a clone from that snapshot. The clone is writable and consumes space on-demand from the zpool. The snapshot is read-only, maintaining the base layer as an immutable object.\n\nWhen the container is launched, a writable layer is added above the image.\n\nIn the diagram, the container's read-write layer is created by making a snapshot of the top layer of the image (Layer 1) and creating a clone from that snapshot.\n\nAs the container modifies the contents of its writable layer, space is allocated for the blocks that are changed. By default, these blocks are 128k.\n\nHow container reads and writes work with zfs\nReading files\n\nEach container's writable layer is a ZFS clone which shares all its data with the dataset it was created from (the snapshots of its parent layers). Read operations are fast, even if the data being read is from a deep layer. This diagram illustrates how block sharing works:\n\nWriting files\n\nWriting a new file: space is allocated on demand from the underlying zpool and the blocks are written directly into the container's writable layer.\n\nModifying an existing file: space is allocated only for the changed blocks, and those blocks are written into the container's writable layer using a copy-on-write (CoW) strategy. This minimizes the size of the layer and increases write performance.\n\nDeleting a file or directory:\n\nWhen you delete a file or directory that exists in a lower layer, the ZFS driver masks the existence of the file or directory in the container's writable layer, even though the file or directory still exists in the lower read-only layers.\nIf you create and then delete a file or directory within the container's writable layer, the blocks are reclaimed by the zpool.\nZFS and Docker performance\n\nThere are several factors that influence the performance of Docker using the zfs storage driver.\n\nMemory: Memory has a major impact on ZFS performance. ZFS was originally designed for large enterprise-grade servers with a large amount of memory.\n\nZFS Features: ZFS includes a de-duplication feature. Using this feature may save disk space, but uses a large amount of memory. It is recommended that you disable this feature for the zpool you are using with Docker, unless you are using SAN, NAS, or other hardware RAID technologies.\n\nZFS Caching: ZFS caches disk blocks in a memory structure called the adaptive replacement cache (ARC). The Single Copy ARC feature of ZFS allows a single cached copy of a block to be shared by multiple clones of a With this feature, multiple running containers can share a single copy of a cached block. This feature makes ZFS a good option for PaaS and other high-density use cases.\n\nFragmentation: Fragmentation is a natural byproduct of copy-on-write filesystems like ZFS. ZFS mitigates this by using a small block size of 128k. The ZFS intent log (ZIL) and the coalescing of writes (delayed writes) also help to reduce fragmentation. You can monitor fragmentation using zpool status. However, there is no way to defragment ZFS without reformatting and restoring the filesystem.\n\nUse the native ZFS driver for Linux: The ZFS FUSE implementation is not recommended, due to poor performance.\n\nPerformance best practices\n\nUse fast storage: Solid-state drives (SSDs) provide faster reads and writes than spinning disks.\n\nUse volumes for write-heavy workloads: Volumes provide the best and most predictable performance for write-heavy workloads. This is because they bypass the storage driver and do not incur any of the potential overheads introduced by thin provisioning and copy-on-write. Volumes have other benefits, such as allowing you to share data among containers and persisting even when no running container is using them.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nConfigure Docker with the zfs storage driver\nManage zfs\nIncrease capacity on a running device\nLimit a container's writable storage quota\nHow the zfs storage driver works\nImage and container layers on-disk\nImage layering and sharing\nHow container reads and writes work with zfs\nReading files\nWriting files\nZFS and Docker performance\nPerformance best practices\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995192,
    "timestamp": "2026-02-07T06:32:16.288Z",
    "title": "containerd image store | Docker Docs",
    "url": "https://docs.docker.com/engine/storage/containerd/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nVolumes\nBind mounts\ntmpfs mounts\nStorage drivers\ncontainerd image store\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nStorage\n/\ncontainerd image store\ncontainerd image store with Docker Engine\nCopy as Markdown\n\nThe containerd image store is the default storage backend for Docker Engine 29.0 and later on fresh installations. If you upgraded from an earlier version, your daemon continues using the legacy graph drivers (overlay2) until you enable the containerd image store.\n\ncontainerd, the industry-standard container runtime, uses snapshotters instead of classic storage drivers for storing image and container data.\n\nNote\n\nThe containerd image store is not available when using user namespace remapping (userns-remap). See moby#47377 for details.\n\nWhy use the containerd image store\n\nThe containerd image store uses snapshotters to manage how image layers are stored and accessed on the filesystem. This differs from the classic graph drivers like overlay2.\n\nThe containerd image store enables:\n\nBuilding and storing multi-platform images locally. With classic storage drivers, you need external builders for multi-platform images.\nWorking with images that include attestations (provenance, SBOM). These use image indices that the classic store doesn't support.\nRunning Wasm containers. The containerd image store supports WebAssembly workloads.\nUsing advanced snapshotters. containerd supports pluggable snapshotters that provide features like lazy-pulling of images (stargz) or peer-to-peer image distribution (nydus, dragonfly).\n\nFor most users, switching to the containerd image store is transparent. The storage backend changes, but your workflows remain the same.\n\nDisk space usage\n\nThe containerd image store uses more disk space than the legacy storage drivers for the same images. This is because containerd stores images in both compressed and uncompressed formats, while the legacy drivers stored only the uncompressed layers.\n\nWhen you pull an image, containerd keeps the compressed layers (as received from the registry) and also extracts them to disk. This dual storage means each layer occupies more space. The compressed format enables faster pulls and pushes, but requires additional disk capacity.\n\nThis difference is particularly noticeable with multiple images sharing the same base layers. With legacy storage drivers, shared base layers were stored once locally, and reused images that depended on them. With containerd, each image stores its own compressed version of shared layers, even though the uncompressed layers are still de-duplicated through snapshotters. The compressed storage adds overhead proportional to the number of images using those layers.\n\nIf disk space is constrained, consider the following:\n\nRegularly prune unused images with docker image prune\nUse docker system df to monitor disk usage\nConfigure the data directory to use a partition with adequate space\nEnable containerd image store on Docker Engine\n\nIf you're upgrading from an earlier Docker Engine version, you need to manually enable the containerd image store.\n\nImportant\n\nSwitching storage backends temporarily hides images and containers created with the other backend. Your data remains on disk. To access the old images again, switch back to your previous storage configuration.\n\nAdd the following configuration to your /etc/docker/daemon.json file:\n\n{\n\n  \"features\": {\n\n    \"containerd-snapshotter\": true\n\n  }\n\n}\n\nSave the file and restart the daemon:\n\n$ sudo systemctl restart docker\n\n\nAfter restarting the daemon, verify you're using the containerd image store:\n\n$ docker info -f '{{ .DriverStatus }}'\n\n[[driver-type io.containerd.snapshotter.v1]]\n\n\nDocker Engine uses the overlayfs containerd snapshotter by default.\n\nNote\n\nWhen you enable the containerd image store, existing images and containers from the overlay2 driver remain on disk but become hidden. They reappear if you switch back to overlay2. To use your existing images with the containerd image store, push them to a registry first, or use docker save to export them.\n\nExperimental automatic migration\n\nDocker Engine includes an experimental feature that can automatically switch to the containerd image store under certain conditions. This feature is experimental. It's provided for those who want to test it, but starting fresh is the recommended approach.\n\nCaution\n\nThe automatic migration feature is experimental and may not work reliably in all scenarios. Create backups before attempting to use it.\n\nTo enable automatic migration, add the containerd-migration feature to your /etc/docker/daemon.json:\n\n{\n\n  \"features\": {\n\n    \"containerd-migration\": true\n\n  }\n\n}\n\nYou can also set the DOCKER_MIGRATE_SNAPSHOTTER_THRESHOLD environment variable to make the daemon switch automatically if you have no containers and your image count is at or below the threshold. For systemd:\n\n$ sudo systemctl edit docker.service\n\n\nAdd:\n\n[Service]\n\nEnvironment=\"DOCKER_MIGRATE_SNAPSHOTTER_THRESHOLD=5\"\n\nIf you have no running or stopped containers and 5 or fewer images, the daemon switches to the containerd image store on restart. Your overlay2 data remains on disk but becomes hidden.\n\nAdditional resources\n\nTo learn more about the containerd image store and its capabilities in Docker Desktop, see containerd image store on Docker Desktop.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhy use the containerd image store\nDisk space usage\nEnable containerd image store on Docker Engine\nExperimental automatic migration\nAdditional resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995195,
    "timestamp": "2026-02-07T06:32:16.288Z",
    "title": "Networking | Docker Docs",
    "url": "https://docs.docker.com/engine/network/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\nNetworking overview\nCopy as Markdown\n\nContainer networking refers to the ability for containers to connect to and communicate with each other, and with non-Docker network services.\n\nContainers have networking enabled by default, and they can make outgoing connections. A container has no information about what kind of network it's attached to, or whether its network peers are also Docker containers. A container only sees a network interface with an IP address, a gateway, a routing table, DNS services, and other networking details.\n\nThis page describes networking from the point of view of the container, and the concepts around container networking.\n\nWhen Docker Engine on Linux starts for the first time, it has a single built-in network called the \"default bridge\" network. When you run a container without the --network option, it is connected to the default bridge.\n\nContainers attached to the default bridge have access to network services outside the Docker host. They use \"masquerading\" which means, if the Docker host has Internet access, no additional configuration is needed for the container to have Internet access.\n\nFor example, to run a container on the default bridge network, and have it ping an Internet host:\n\n$ docker run --rm -ti busybox ping -c1 docker.com\n\nPING docker.com (23.185.0.4): 56 data bytes\n\n64 bytes from 23.185.0.4: seq=0 ttl=62 time=6.564 ms\n\n\n\n--- docker.com ping statistics ---\n\n1 packets transmitted, 1 packets received, 0% packet loss\n\nround-trip min/avg/max = 6.564/6.564/6.564 ms\n\nUser-defined networks\n\nWith the default configuration, containers attached to the default bridge network have unrestricted network access to each other using container IP addresses. They cannot refer to each other by name.\n\nIt can be useful to separate groups of containers that should have full access to each other, but restricted access to containers in other groups.\n\nYou can create custom, user-defined networks, and connect groups of containers to the same network. Once connected to a user-defined network, containers can communicate with each other using container IP addresses or container names.\n\nThe following example creates a network using the bridge network driver and runs a container in that network:\n\n$ docker network create -d bridge my-net\n\n$ docker run --network=my-net -it busybox\n\nDrivers\n\nDocker Engine has a number of network drivers, as well as the default \"bridge\". On Linux, the following built-in network drivers are available:\n\nDriver\tDescription\nbridge\tThe default network driver.\nhost\tRemove network isolation between the container and the Docker host.\nnone\tCompletely isolate a container from the host and other containers.\noverlay\tSwarm Overlay networks connect multiple Docker daemons together.\nipvlan\tConnect containers to external VLANs.\nmacvlan\tContainers appear as devices on the host's network.\n\nMore information can be found in the network driver specific pages, including their configuration options and details about their functionality.\n\nNative Windows containers have a different set of drivers, see Windows container network drivers.\n\nConnecting to multiple networks\n\nConnecting a container to a network can be compared to connecting an Ethernet cable to a physical host. Just as a host can be connected to multiple Ethernet networks, a container can be connected to multiple Docker networks.\n\nFor example, a frontend container may be connected to a bridge network with external access, and a --internal network to communicate with containers running backend services that do not need external network access.\n\nA container may also be connected to different types of network. For example, an ipvlan network to provide internet access, and a bridge network for access to local services.\n\nContainers can also share networking stacks, see Container networks.\n\nWhen sending packets, if the destination is an address in a directly connected network, packets are sent to that network. Otherwise, packets are sent to a default gateway for routing to their destination. In the example above, the ipvlan network's gateway must be the default gateway.\n\nThe default gateway is selected by Docker, and may change whenever a container's network connections change. To make Docker choose a specific default gateway when creating the container or connecting a new network, set a gateway priority. See option gw-priority for the docker run and docker network connect commands.\n\nThe default gw-priority is 0 and the gateway in the network with the highest priority is the default gateway. So, when a network should always be the default gateway, it is enough to set its gw-priority to 1.\n\n$ docker run --network name=gwnet,gw-priority=1 --network anet1 --name myctr myimage\n\n$ docker network connect anet2 myctr\n\nPublished ports\n\nWhen you create or run a container using docker create or docker run, all ports of containers on bridge networks are accessible from the Docker host and other containers connected to the same network. Ports are not accessible from outside the host or, with the default configuration, from containers in other networks.\n\nUse the --publish or -p flag to make a port available outside the host, and to containers in other bridge networks.\n\nFor more information about port mapping, including how to disable it and use direct routing to containers, see port publishing.\n\nIP address and hostname\n\nWhen creating a network, IPv4 address allocation is enabled by default, it can be disabled using --ipv4=false. IPv6 address allocation can be enabled using --ipv6.\n\n$ docker network create --ipv6 --ipv4=false v6net\n\n\nBy default, the container gets an IP address for every Docker network it attaches to. A container receives an IP address out of the IP subnet of the network. The Docker daemon performs dynamic subnetting and IP address allocation for containers. Each network also has a default subnet mask and gateway.\n\nYou can connect a running container to multiple networks, either by passing the --network flag multiple times when creating the container, or using the docker network connect command for already running containers. In both cases, you can use the --ip or --ip6 flags to specify the container's IP address on that particular network.\n\nIn the same way, a container's hostname defaults to be the container's ID in Docker. You can override the hostname using --hostname. When connecting to an existing network using docker network connect, you can use the --alias flag to specify an additional network alias for the container on that network.\n\nSubnet allocation\n\nDocker networks can use either explicitly configured subnets or automatically allocated ones from default pools.\n\nExplicit subnet configuration\n\nYou can specify exact subnets when creating a network:\n\n$ docker network create --ipv6 --subnet 192.0.2.0/24 --subnet 2001:db8::/64 mynet\n\nAutomatic subnet allocation\n\nWhen no --subnet option is provided, Docker automatically selects a subnet from predefined \"default address pools\". These pools can be configured in /etc/docker/daemon.json. Docker's built-in default is equivalent to:\n\n{\n\n  \"default-address-pools\": [\n\n    {\"base\":\"172.17.0.0/16\",\"size\":16},\n\n    {\"base\":\"172.18.0.0/16\",\"size\":16},\n\n    {\"base\":\"172.19.0.0/16\",\"size\":16},\n\n    {\"base\":\"172.20.0.0/14\",\"size\":16},\n\n    {\"base\":\"172.24.0.0/14\",\"size\":16},\n\n    {\"base\":\"172.28.0.0/14\",\"size\":16},\n\n    {\"base\":\"192.168.0.0/16\",\"size\":20}\n\n  ]\n\n}\nbase: The subnet that can be allocated from.\nsize: The prefix length used for each allocated subnet.\n\nWhen an IPv6 subnet is required and there are no IPv6 addresses in default-address-pools, Docker allocates subnets from a Unique Local Address (ULA) prefix. To use specific IPv6 subnets instead, add them to your default-address-pools. See Dynamic IPv6 subnet allocation for more information.\n\nDocker attempts to avoid address prefixes already in use on the host. However, you may need to customize default-address-pools to prevent routing conflicts in some network environments.\n\nThe default pools use large subnets, which limits the number of networks you can create. You can divide base subnets into smaller pools to support more networks.\n\nFor example, this configuration allows Docker to create 256 networks from 172.17.0.0/16. Docker will allocate subnets 172.17.0.0/24, 172.17.1.0/24, and so on, up to 172.17.255.0/24:\n\n{\n\n  \"default-address-pools\": [\n\n    {\"base\": \"172.17.0.0/16\", \"size\": 24}\n\n  ]\n\n}\n\nYou can also request a subnet with a specific prefix length from the default pools by using unspecified addresses in the --subnet option:\n\n$ docker network create --ipv6 --subnet ::/56 --subnet 0.0.0.0/24 mynet\n\n6686a6746b17228f5052528113ddad0e6d68e2e3905d648e336b33409f2d3b64\n\n$ docker network inspect mynet -f '{{json .IPAM.Config}}' | jq .\n\n[\n\n  {\n\n    \"Subnet\": \"172.19.0.0/24\",\n\n    \"Gateway\": \"172.19.0.1\"\n\n  },\n\n  {\n\n    \"Subnet\": \"fdd3:6f80:972c::/56\",\n\n    \"Gateway\": \"fdd3:6f80:972c::1\"\n\n  }\n\n]\n\nNote\n\nSupport for unspecified addresses in --subnet was introduced in Docker 29.0.0. If Docker is downgraded to an older version, networks created in this way will become unusable. They can be removed and re-created, or will function again if the daemon is restored to 29.0.0 or later.\n\nDNS services\n\nContainers use the same DNS servers as the host by default, but you can override this with --dns.\n\nBy default, containers inherit the DNS settings as defined in the /etc/resolv.conf configuration file. Containers that attach to the default bridge network receive a copy of this file. Containers that attach to a custom network use Docker's embedded DNS server. The embedded DNS server forwards external DNS lookups to the DNS servers configured on the host.\n\nYou can configure DNS resolution on a per-container basis, using flags for the docker run or docker create command used to start the container. The following table describes the available docker run flags related to DNS configuration.\n\nFlag\tDescription\n--dns\tThe IP address of a DNS server. To specify multiple DNS servers, use multiple --dns flags. DNS requests will be forwarded from the container's network namespace so, for example, --dns=127.0.0.1 refers to the container's own loopback address.\n--dns-search\tA DNS search domain to search non-fully qualified hostnames. To specify multiple DNS search prefixes, use multiple --dns-search flags.\n--dns-opt\tA key-value pair representing a DNS option and its value. See your operating system's documentation for resolv.conf for valid options.\n--hostname\tThe hostname a container uses for itself. Defaults to the container's ID if not specified.\nCustom hosts\n\nYour container will have lines in /etc/hosts which define the hostname of the container itself, as well as localhost and a few other common things. Custom hosts, defined in /etc/hosts on the host machine, aren't inherited by containers. To pass additional hosts into a container, refer to add entries to container hosts file in the docker run reference documentation.\n\nContainer networks\n\nIn addition to user-defined networks, you can attach a container to another container's networking stack directly, using the --network container:<name|id> flag format.\n\nThe following flags aren't supported for containers using the container: networking mode:\n\n--add-host\n--hostname\n--dns\n--dns-search\n--dns-option\n--mac-address\n--publish\n--publish-all\n--expose\n\nThe following example runs a Redis container, with Redis binding to 127.0.0.1, then running the redis-cli command and connecting to the Redis server over 127.0.0.1.\n\n$ docker run -d --name redis redis --bind 127.0.0.1\n\n$ docker run --rm -it --network container:redis redis redis-cli -h 127.0.0.1\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUser-defined networks\nDrivers\nConnecting to multiple networks\nPublished ports\nIP address and hostname\nSubnet allocation\nDNS services\nCustom hosts\nContainer networks\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995198,
    "timestamp": "2026-02-07T06:32:16.296Z",
    "title": "Docker with iptables | Docker Docs",
    "url": "https://docs.docker.com/engine/network/firewall-iptables/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nDocker with iptables\nDocker with iptables\nCopy as Markdown\n\nDocker creates iptables rules in the host's network namespace for bridge networks. For bridge and other network types, iptables rules for DNS are also created in the container's network namespace.\n\nCreation of iptables rules can be disabled using daemon options iptables and ip6tables, see Prevent Docker from manipulating firewall rules. However, this is not recommended for most users as it will likely break container networking.\n\nDocker and iptables chains\n\nTo support bridge and overlay networks, Docker creates the following custom iptables chains in the filter table:\n\nDOCKER-USER\nA placeholder for user-defined rules that will be processed before rules in the DOCKER-FORWARD and DOCKER chains.\nDOCKER-FORWARD\nThe first stage of processing for Docker's networks. Rules that pass packets that are not related to established connections to the other Docker chains, as well as rules to accept packets that are part of established connections.\nDOCKER, DOCKER-BRIDGE, DOCKER-INTERNAL\nRules that determine whether a packet that is not part of an established connection should be accepted, based on the port forwarding configuration of running containers.\nDOCKER-CT\nPer-bridge connection tracking rules.\nDOCKER-INGRESS\nRules related to Swarm networking.\n\nIn the FORWARD chain, Docker adds rules that unconditionally jump to the DOCKER-USER, DOCKER-FORWARD and DOCKER-INGRESS chains.\n\nIn the nat table, Docker creates chain DOCKER and adds rules to implement masquerading and port-mapping.\n\nDocker requires IP Forwarding to be enabled on the host for its default bridge network configuration. If it enables IP Forwarding, it also sets the default policy of the iptables FORWARD chain in the filter table to DROP.\n\nAdd iptables policies before Docker's rules\n\nPackets that get accepted or rejected by rules in these custom chains will not be seen by user-defined rules appended to the FORWARD chain. So, to add additional rules to filter these packets, use the DOCKER-USER chain.\n\nRules appended to the FORWARD chain will be processed after Docker's rules.\n\nMatch the original IP and ports for requests\n\nWhen packets arrive to the DOCKER-USER chain, they have already passed through a Destination Network Address Translation (DNAT) filter. That means that the iptables flags you use can only match internal IP addresses and ports of containers.\n\nIf you want to match traffic based on the original IP and port in the network request, you must use the conntrack iptables extension. For example:\n\n$ sudo iptables -I DOCKER-USER -p tcp -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\n\n$ sudo iptables -I DOCKER-USER -p tcp -m conntrack --ctorigdst 198.51.100.2 --ctorigdstport 80 -j ACCEPT\n\nImportant\n\nUsing the conntrack extension may result in degraded performance.\n\nAllow forwarding between host interfaces\n\nIf Docker has set the default policy of the FORWARD chain in the filter table to DROP, a rule in DOCKER-USER can be used to allow forwarding between host interfaces. For example:\n\n$ iptables -I DOCKER-USER -i src_if -o dst_if -j ACCEPT\n\nRestrict external connections to containers\n\nBy default, all external source IPs are allowed to connect to ports that have been published to the Docker host's addresses.\n\nTo allow only a specific IP or network to access the containers, insert a negated rule at the top of the DOCKER-USER filter chain. For example, the following rule drops packets from all IP addresses except 192.0.2.2:\n\n$ iptables -I DOCKER-USER -i ext_if ! -s 192.0.2.2 -j DROP\n\n\nYou will need to change ext_if to correspond with your host's actual external interface. You could instead allow connections from a source subnet. The following rule only allows access from the subnet 192.0.2.0/24:\n\n$ iptables -I DOCKER-USER -i ext_if ! -s 192.0.2.0/24 -j DROP\n\n\nFinally, you can specify a range of IP addresses to accept using --src-range (Remember to also add -m iprange when using --src-range or --dst-range):\n\n$ iptables -I DOCKER-USER -m iprange -i ext_if ! --src-range 192.0.2.1-192.0.2.3 -j DROP\n\n\nYou can combine -s or --src-range with -d or --dst-range to control both the source and destination. For example, if the Docker host has addresses 2001:db8:1111::2 and 2001:db8:2222::2, you can make rules specific to 2001:db8:1111::2 and leave 2001:db8:2222::2 open.\n\nYou may need to allow responses from servers outside the permitted external address ranges. For example, containers may send DNS or HTTP requests to hosts that are not allowed to access the container's services. The following rule accepts any incoming or outgoing packet belonging to a flow that has already been accepted by other rules. It must be placed before DROP rules that restrict access from external address ranges.\n\n$ iptables -I DOCKER-USER -m state --state RELATED,ESTABLISHED -j ACCEPT\n\n\nFor more information about iptables configuration and advanced usage, refer to the Netfilter.org HOWTO.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDocker and iptables chains\nAdd iptables policies before Docker's rules\nMatch the original IP and ports for requests\nAllow forwarding between host interfaces\nRestrict external connections to containers\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995201,
    "timestamp": "2026-02-07T06:32:16.296Z",
    "title": "Docker with nftables | Docker Docs",
    "url": "https://docs.docker.com/engine/network/firewall-nftables/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nDocker with nftables\nDocker with nftables\nCopy as Markdown\nWarning\n\nSupport for nftables introduced in Docker 29.0.0 is experimental, configuration options, behavior and implementation may all change in future releases. The rules for overlay networks have not yet been migrated from iptables. Therefore, nftables cannot be enabled when the Docker daemon is running in Swarm mode.\n\nTo use nftables instead of iptables, use Docker Engine option --firewall-backend=nftables on its command line, or \"firewall-backend\": \"nftables\" in its configuration file. You may also need to modify IP forwarding configuration on the host, and migrate rules from the iptables DOCKER-USER chain, see migrating from iptables to nftables.\n\nFor bridge networks, Docker creates nftables rules in the host's network namespace. For bridge and other network types, nftables rules for DNS are also created in the container's network namespace.\n\nCreation of nftables rules can be disabled using daemon options iptables and ip6tables. These options apply to both iptables and nftables. See Prevent Docker from manipulating firewall rules. However, this is not recommended for most users as it will likely break container networking.\n\nDocker's nftables tables\n\nFor bridge networks, Docker creates two tables, ip docker-bridges and ip6 docker-bridges.\n\nEach table contains a number of base chains, and further chains are added for each bridge network. The moby project has some internal documentation describing its nftables, and how they depend on network and container configuration. However, the tables and their rules are likely to change between Docker Engine releases.\n\nNote\n\nDo not modify Docker's tables directly as the modifications are likely to be lost, Docker expects to have full ownership of its tables.\n\nNote\n\nBecause iptables has a fixed set of chains, equivalent to nftables base chains, all rules are included in those chains. The DOCKER-USER chain is supplied as a way to insert rules into the filter table's FORWARD chain, to run before Docker's rules. In Docker's nftables implementation, there is no DOCKER-USER chain. Instead, rules can be added in separate tables, with base chains that have the same types and hook points as Docker's base chains. If necessary, base chain priority can be used to tell nftables which order to call the chains in. Docker uses well known priority values for each of its base chains.\n\nMigrating from iptables to nftables\n\nIf the Docker daemon has been running with the iptables firewall backend, restarting it with the nftables backend will delete most of Docker's iptables chains and rules, and create nftables rules instead.\n\nIf IP forwarding is not enabled, Docker will report an error when creating a bridge network that needs it. Because of the default bridge, if IPv4 forwarding is disabled, the error will be reported during daemon startup. See IP forwarding.\n\nIf you have rules in the DOCKER-USER chain, see Migrating DOCKER-USER.\n\nYou may need to manually update the iptables FORWARD policy if it has been set to DROP by Docker with iptables, or as part of your host's firewall configuration. See FORWARD policy in iptables.\n\nIP forwarding\n\nIP forwarding on the Docker host enables Docker functionality including port publishing, communication between bridge networks, and direct routing from outside the host to containers in bridge networks.\n\nWhen running with iptables, depending on network and daemon configuration, Docker may enable IPv4 and IPv6 forwarding on the host.\n\nWith its nftables firewall backend enabled, Docker will not enable IP forwarding itself. It will report an error if forwarding is needed, but not already enabled. To disable Docker's check for IP forwarding, letting it start and create networks when it determines that forwarding is disabled, use Daemon option --ip-forward=false, or \"ip-forward\": false in its configuration file.\n\nWarning\n\nWhen enabling IP forwarding, make sure you have firewall rules to block unwanted forwarding between non-Docker interfaces.\n\nNote\n\nIf you stop Docker to migrate to nftables, Docker may have already enabled IP forwarding on your system. After a reboot, if no other service re-enables forwarding, Docker will fail to start.\n\nIf Docker is in a VM that has a single network interface and no other software running, there is probably no unwanted forwarding to block. But, on a physical host with multiple network interfaces, forwarding between those interfaces should probably be blocked with nftables rules unless the host is acting as a router.\n\nTo enable IP forwarding on the host, set the following sysctls:\n\nnet.ipv4.ip_forward=1\nnet.ipv6.conf.all.forwarding=1\n\nIf your host uses systemd, you may be able to use systemd-sysctl. For example, by editing /etc/sysctl.d/99-sysctl.conf.\n\nIf the host is running firewalld, you may be able to use it to block unwanted forwarding. Docker's bridges are in a firewalld zone called docker, it creates a forwarding policy called docker-forwarding that accepts forwarding from ANY zone to the docker zone.\n\nFor example, to use nftables to block forwarding between interfaces eth0 and eth1, you could use:\n\ntable inet no-ext-forwarding {\n\n\tchain no-ext-forwarding {\n\n\t\ttype filter hook forward priority filter; policy accept;\n\n\t\tiifname \"eth0\" oifname \"eth1\" drop\n\n\t\tiifname \"eth1\" oifname \"eth0\" drop\n\n\t}\n\n}\n\nFORWARD policy in iptables\n\nAn iptables chain with FORWARD policy DROP will drop packets that have been accepted by Docker's nftables rules, because the packet will be processed by the iptables chains as well as Docker's nftables chains.\n\nSome features, including port publishing, will not work unless the DROP policy is removed, or additional iptables rules are added to the iptables FORWARD chain to accept Docker-related traffic.\n\nWhen Docker is using iptables, and it enables IP forwarding on the host, it sets the default policy of the iptables FORWARD chain to DROP. So, if you stop Docker to migrate to nftables, it may have set a DROP that you need to remove. It will be removed anyway on reboot.\n\nTo keep using rules in DOCKER-USER that rely on the chain having policy DROP, you must add explicit ACCEPT rules for Docker related traffic.\n\nTo check the current iptables FORWARD policy, use:\n\n$ iptables -L FORWARD\n\nChain FORWARD (policy DROP)\n\ntarget     prot opt source               destination\n\n$ ip6tables -L FORWARD\n\nChain FORWARD (policy ACCEPT)\n\ntarget     prot opt source               destination\n\n\nTo set the iptables policies to ACCEPT for IPv4 and IPv6:\n\n$ iptables -P FORWARD ACCEPT\n\n$ ip6tables -P FORWARD ACCEPT\n\nMigrating DOCKER-USER\n\nWith firewall backend \"iptables\", rules added to the iptables DOCKER-USER are processed before Docker's rules in the filter table's FORWARD chain.\n\nWhen starting the daemon with nftables after running with iptables, Docker will not remove the jump from the FORWARD chain to DOCKER-USER. So, rules created in DOCKER-USER will continue to run until the jump is removed or the host is rebooted.\n\nWhen starting with nftables, the daemon will not add the jump. So, unless there is an existing jump, rules in DOCKER-USER will be ignored.\n\nMigrating ACCEPT rules\n\nSome rules in the DOCKER-USER chain will continue to work. For example, if a packet is dropped, it will be dropped before or after the nftables rules in Docker's filter-FORWARD chain. But other rules, particularly ACCEPT rules to override Docker's DROP rules, will not work.\n\nIn nftables, an \"accept\" rule is not final. It terminates processing for its base chain, but the accepted packet will still be processed by other base chains, which may drop it.\n\nTo override Docker's drop rule, you must use a firewall mark. Select a mark not already in use on your host, and use Docker Engine option --bridge-accept-fwmark.\n\nFor example, --bridge-accept-fwmark=1 tells the daemon to accept any packet with an fwmark value of 1. Optionally, you can supply a mask to match specific bits in the mark, --bridge-accept-fwmark=0x1/0x3.\n\nThen, instead of accepting the packet in DOCKER-USER, add the firewall mark you have chosen and Docker will not drop it.\n\nThe firewall mark must be added before Docker's rules run. So if the mark is added in a chain with type filter and hook forward, it must have priority filter - 1 or lower.\n\nReplacing DOCKER-USER with an nftables table\n\nBecause nftables doesn't have pre-defined chains, to replace the DOCKER-USER chain you can create your own table and add chains and rules to it.\n\nThe DOCKER-USER chain has type filter and hook forward, so it can only have rules in the filter forward chain. The base chains in your table can have any type or hook. If your rules need to run before Docker's rules, give the base chains a lower priority number than Docker's chain. Or, a higher priority to make sure they run after Docker's rules.\n\nDocker's base chains use the priority values defined at priority values\n\nExample: restricting external connections to containers\n\nBy default, any remote host can connect to ports published to the Docker host's external addresses.\n\nTo allow only a specific IP or network to access the containers, create a table with a base chain that has a drop rule. For example, the following table drops packets from all IP addresses except 192.0.2.2:\n\ntable ip my-table {\n\n\tchain my-filter-forward {\n\n\t\ttype filter hook forward priority filter; policy accept;\n\n\t\tiifname \"ext_if\" ip saddr != 192.0.2.2 counter drop\n\n\t}\n\n}\n\n\nYou will need to change ext_if to your host's external interface name.\n\nYou could instead accept connections from a source subnet. The following table only accepts access from the subnet 192.0.2.0/24:\n\ntable ip my-table {\n\n\tchain my-filter-forward {\n\n\t\ttype filter hook forward priority filter; policy accept;\n\n\t\tiifname \"ext_if\" ip saddr != 192.0.2.0/24 counter drop\n\n\t}\n\n}\n\n\nIf you are running other services on the host that use IP forwarding and need to be accessed by different external hosts, you will need more specific filters. For example, to match the default prefix br- of bridge devices belonging to Docker's user-defined bridge networks:\n\ntable ip my-table {\n\n\tchain my-filter-forward {\n\n\t\ttype filter hook forward priority filter; policy accept;\n\n\t\tiifname \"ext_if\" oifname \"br-*\" ip saddr != 192.0.2.0/24 counter drop\n\n\t}\n\n}\n\n\nFor more information about nftables configuration and advanced usage, refer to the nftables wiki.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDocker's nftables tables\nMigrating from iptables to nftables\nIP forwarding\nFORWARD policy in iptables\nMigrating DOCKER-USER\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995204,
    "timestamp": "2026-02-07T06:32:16.298Z",
    "title": "Packet filtering and firewalls | Docker Docs",
    "url": "https://docs.docker.com/engine/network/packet-filtering-firewalls/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nPacket filtering and firewalls\nPacket filtering and firewalls\nCopy as Markdown\n\nOn Linux, Docker creates firewall rules to implement network isolation, port publishing and filtering.\n\nBecause these rules are required for the correct functioning of Docker bridge networks, you should not modify the rules created by Docker.\n\nThis page describes options that control Docker's firewall rules to implement functionality including port publishing, and NAT/masquerading.\n\nNote\n\nDocker creates firewall rules for bridge networks.\n\nNo rules are created for ipvlan, macvlan or host networking.\n\nFirewall backend\n\nBy default, Docker Engine creates its firewall rules using iptables, see Docker with iptables. It also has support for nftables, see Docker with nftables.\n\nFor bridge networks, iptables and nftables have the same functionality.\n\nDocker Engine option firewall-backend can be used to select whether iptables or nftables is used. See daemon configuration.\n\nDocker on a router\n\nOn Linux, Docker needs \"IP Forwarding\" enabled on the host. So, it enables the sysctl settings net.ipv4.ip_forward and net.ipv6.conf.all.forwarding if they are not already enabled when it starts. When it does that, it also configures the firewall to drop forwarded packets unless they are explicitly accepted.\n\nWhen Docker sets the default forwarding policy to \"drop\", it will prevent your Docker host from acting as a router. This is the recommended setting when IP Forwarding is enabled, unless router functionality is required.\n\nTo stop Docker from setting the forwarding policy to \"drop\", include \"ip-forward-no-drop\": true in /etc/docker/daemon.json, or add option --ip-forward-no-drop to the dockerd command line.\n\nNote\n\nWith the experimental nftables backend, Docker does not enable IP forwarding itself, and it will not create a default \"drop\" nftables policy. See Migrating from iptables to nftables.\n\nPrevent Docker from manipulating firewall rules\n\nSetting the iptables or ip6tables keys to false in daemon configuration, will prevent Docker from creating most of its iptables or nftables rules. But, this option is not appropriate for most users, it is likely to break container networking for the Docker Engine.\n\nFor example, with Docker's firewalling disabled and no replacement rules, containers in bridge networks will not be able to access internet hosts by masquerading, but all of their ports will be accessible to hosts on the local network.\n\nIt is not possible to completely prevent Docker from creating firewall rules, and creating rules after-the-fact is extremely involved and beyond the scope of these instructions.\n\nIntegration with firewalld\n\nIf you are running Docker with the iptables or ip6tables options set to true, and firewalld is enabled on your system, in addition to its usual iptables or nftables rules, Docker creates a firewalld zone called docker, with target ACCEPT.\n\nAll bridge network interfaces created by Docker (for example, docker0) are inserted into the docker zone.\n\nDocker also creates a forwarding policy called docker-forwarding that allows forwarding from ANY zone to the docker zone.\n\nDocker and ufw\n\nUncomplicated Firewall (ufw) is a frontend that ships with Debian and Ubuntu, and it lets you manage firewall rules. Docker and ufw use firewall rules in ways that make them incompatible with each other.\n\nWhen you publish a container's ports using Docker, traffic to and from that container gets diverted before it goes through the ufw firewall settings. Docker routes container traffic in the nat table, which means that packets are diverted before it reaches the INPUT and OUTPUT chains that ufw uses. Packets are routed before the firewall rules can be applied, effectively ignoring your firewall configuration.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nFirewall backend\nDocker on a router\nPrevent Docker from manipulating firewall rules\nIntegration with firewalld\nDocker and ufw\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995210,
    "timestamp": "2026-02-07T06:32:16.309Z",
    "title": "Network drivers | Docker Docs",
    "url": "https://docs.docker.com/engine/network/drivers/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nBridge network driver\nHost network driver\nIPvlan network driver\nMacvlan network driver\nNone network driver\nOverlay network driver\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nNetwork drivers\nNetwork drivers\nCopy as Markdown\n\nDocker's networking subsystem is pluggable, using drivers. Several drivers exist by default, and provide core networking functionality:\n\nbridge: The default network driver. If you don't specify a driver, this is the type of network you are creating. Bridge networks are commonly used when your application runs in a container that needs to communicate with other containers on the same host. See Bridge network driver.\n\nhost: Remove network isolation between the container and the Docker host, and use the host's networking directly. See Host network driver.\n\noverlay: Overlay networks connect multiple Docker daemons together and enable Swarm services and containers to communicate across nodes. This strategy removes the need to do OS-level routing. See Overlay network driver.\n\nipvlan: IPvlan networks give users total control over both IPv4 and IPv6 addressing. The VLAN driver builds on top of that in giving operators complete control of layer 2 VLAN tagging and even IPvlan L3 routing for users interested in underlay network integration. See IPvlan network driver.\n\nmacvlan: Macvlan networks allow you to assign a MAC address to a container, making it appear as a physical device on your network. The Docker daemon routes traffic to containers by their MAC addresses. Using the macvlan driver is sometimes the best choice when dealing with legacy applications that expect to be directly connected to the physical network, rather than routed through the Docker host's network stack. See Macvlan network driver.\n\nnone: Completely isolate a container from the host and other containers. none is not available for Swarm services. See None network driver.\n\nNetwork plugins: You can install and use third-party network plugins with Docker.\n\nNetwork driver summary\nThe default bridge network is good for running containers that don't require special networking capabilities.\nUser-defined bridge networks enable containers on the same Docker host to communicate with each other. A user-defined network typically defines an isolated network for multiple containers belonging to a common project or component.\nHost network shares the host's network with the container. When you use this driver, the container's network isn't isolated from the host.\nOverlay networks are best when you need containers running on different Docker hosts to communicate, or when multiple applications work together using Swarm services.\nMacvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address.\nIPvlan is similar to Macvlan, but doesn't assign unique MAC addresses to containers. Consider using IPvlan when there's a restriction on the number of MAC addresses that can be assigned to a network interface or port.\nThird-party network plugins allow you to integrate Docker with specialized network stacks.\nNext steps\n\nEach driver page includes detailed explanations, configuration options, and hands-on usage examples to help you work with that driver effectively.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nNetwork driver summary\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995207,
    "timestamp": "2026-02-07T06:32:16.309Z",
    "title": "Port publishing and mapping | Docker Docs",
    "url": "https://docs.docker.com/engine/network/port-publishing/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nPort publishing and mapping\nPort publishing and mapping\nCopy as Markdown\n\nBy default, for both IPv4 and IPv6, the Docker daemon blocks access to ports that have not been published. Published container ports are mapped to host IP addresses. To do this, it uses firewall rules to perform Network Address Translation (NAT), Port Address Translation (PAT), and masquerading.\n\nFor example, docker run -p 8080:80 [...] creates a mapping between port 8080 on any address on the Docker host, and the container's port 80. Outgoing connections from the container will masquerade, using the Docker host's IP address.\n\nPublishing ports\n\nWhen you create or run a container using docker create or docker run, all ports of containers on bridge networks are accessible from the Docker host and other containers connected to the same network. Ports are not accessible from outside the host or, with the default configuration, from containers in other networks.\n\nUse the --publish or -p flag to make a port available outside the host, and to containers in other bridge networks.\n\nThis creates a firewall rule in the host, mapping a container port to a port on the Docker host to the outside world. Here are some examples:\n\nFlag value\tDescription\n-p 8080:80\tMap port 8080 on the Docker host to TCP port 80 in the container.\n-p 192.168.1.100:8080:80\tMap port 8080 on the Docker host IP 192.168.1.100 to TCP port 80 in the container.\n-p 8080:80/udp\tMap port 8080 on the Docker host to UDP port 80 in the container.\n-p 8080:80/tcp -p 8080:80/udp\tMap TCP port 8080 on the Docker host to TCP port 80 in the container, and map UDP port 8080 on the Docker host to UDP port 80 in the container.\nImportant\n\nPublishing container ports is insecure by default. Meaning, when you publish a container's ports it becomes available not only to the Docker host, but to the outside world as well.\n\nIf you include the localhost IP address (127.0.0.1, or ::1) with the publish flag, only the Docker host can access the published container port.\n\n$ docker run -p 127.0.0.1:8080:80 -p '[::1]:8080:80' nginx\n\nWarning\n\nIn releases older than 28.0.0, hosts within the same L2 segment (for example, hosts connected to the same network switch) can reach ports published to localhost. For more information, see moby/moby#45610\n\nPorts on the host's IPv6 addresses will map to the container's IPv4 address if no host IP is given in a port mapping, the bridge network is IPv4-only, and --userland-proxy=true (default).\n\nDirect routing\n\nPort mapping ensures that published ports are accessible on the host's network addresses, which are likely to be routable for any external clients. No routes are normally set up in the host's network for container addresses that exist within a host.\n\nBut, particularly with IPv6 you may prefer to avoid using NAT and instead arrange for external routing to container addresses (\"direct routing\").\n\nTo access containers on a bridge network from outside the Docker host, you must first set up routing to the bridge network via an address on the Docker host. This can be achieved using static routes, Border Gateway Protocol (BGP), or any other means appropriate for your network. For example, within a local layer 2 network, remote hosts can set up static routes to a container network via the Docker daemon host's address on the local network.\n\nDirect routing to containers in bridge networks\n\nBy default, remote hosts are not allowed direct access to container IP addresses in Docker's Linux bridge networks. They can only access ports published to host IP addresses.\n\nTo allow direct access to any published port, on any container, in any Linux bridge network, use daemon option \"allow-direct-routing\": true in /etc/docker/daemon.json or the equivalent --allow-direct-routing.\n\nTo allow direct routing from anywhere to containers in a specific bridge network, see Gateway modes.\n\nOr, to allow direct routing via specific host interfaces, to a specific bridge network, use the following option when creating the network:\n\ncom.docker.network.bridge.trusted_host_interfaces\nExample\n\nCreate a network where published ports on container IP addresses can be accessed directly from interfaces vxlan.1 and eth3:\n\n$ docker network create --subnet 192.0.2.0/24 --ip-range 192.0.2.0/29 -o com.docker.network.bridge.trusted_host_interfaces=\"vxlan.1:eth3\" mynet\n\n\nRun a container in that network, publishing its port 80 to port 8080 on the host's loopback interface:\n\n$ docker run -d --ip 192.0.2.100 -p 127.0.0.1:8080:80 nginx\n\n\nThe web server running on the container's port 80 can now be accessed from the Docker host at http://127.0.0.1:8080, or directly at http://192.0.2.100:80. If remote hosts on networks connected to interfaces vxlan.1 and eth3 have a route to the 192.0.2.0/24 network inside the Docker host, they can also access the web server via http://192.0.2.100:80.\n\nGateway modes\n\nThe bridge network driver has the following options:\n\ncom.docker.network.bridge.gateway_mode_ipv6\ncom.docker.network.bridge.gateway_mode_ipv4\n\nEach of these can be set to one of the gateway modes:\n\nnat\nnat-unprotected\nrouted\nisolated\n\nThe default is nat, NAT and masquerading rules are set up for each published container port. Packets leaving the host will use a host address.\n\nWith mode routed, no NAT or masquerading rules are set up, but firewall rules are still set up so that only published container ports are accessible. Outgoing packets from the container will use the container's address, not a host address.\n\nTo access a published port in a routed network, remote hosts must have a route to the container network via an external address on the Docker host (\"direct routing\"). Hosts on the local layer-2 network can set up direct routing without needing any additional network configuration. Hosts outside the local network can only use direct routing to the container if the network's routers are configured to enable it.\n\nIn a nat mode network, publishing a port to an address on the loopback interface means remote hosts cannot access it. Other published container ports in routed and nat networks are always accessible from remote hosts using direct routing, unless the Docker host's firewall has additional restrictions.\n\nNote\n\nWhen a port is published to a specific host address in nat mode, if IP forwarding is enabled on the Docker host, the published port can be accessed via other host interfaces using direct routing to the host address.\n\nFor example, a Docker host with IP forwarding enabled has two NICs with addresses 192.168.100.10/24 and 10.0.0.10/24. When a port is published to 192.168.100.10, a host in the 10.0.0.0/24 subnet can access that port by routing to 192.168.100.10 via 10.0.0.10.\n\nIn nat-unprotected mode, unpublished container ports are also accessible using direct routing, no port filtering rules are set up. This mode is included for compatibility with legacy default behaviour.\n\nThe gateway mode also affects communication between containers that are connected to different Docker networks on the same host.\n\nIn nat and nat-unprotected modes, containers in other bridge networks can only access published ports via the host addresses they are published to. Direct routing from other networks is not allowed.\nIn routed mode containers in other networks can use direct routing to access ports, without going via a host address.\n\nIn routed mode, a host port in a -p or --publish port mapping is not used, and the host address is only used to decide whether to apply the mapping to IPv4 or IPv6. So, when a mapping only applies to routed mode, only addresses 0.0.0.0 or :: should be used, and a host port should not be given. If a specific address or port is given, it will have no effect on the published port and a warning message will be logged.\n\nMode isolated can only be used when the network is also created with CLI flag --internal, or equivalent. An address is normally assigned to the bridge device in an internal network. So, processes on the Docker host can access the network, and containers in the network can access host services listening on that bridge address (including services listening on \"any\" host address, 0.0.0.0 or ::). No address is assigned to the bridge when the network is created with gateway mode isolated.\n\nExample\n\nCreate a network suitable for direct routing for IPv6, with NAT enabled for IPv4:\n\n$ docker network create --ipv6 --subnet 2001:db8::/64 -o com.docker.network.bridge.gateway_mode_ipv6=routed mynet\n\n\nCreate a container with a published port:\n\n$ docker run --network=mynet -p 8080:80 myimage\n\n\nThen:\n\nOnly container port 80 will be open, for IPv4 and IPv6.\nFor IPv6, using routed mode, port 80 will be open on the container's IP address. Port 8080 will not be opened on the host's IP addresses, and outgoing packets will use the container's IP address.\nFor IPv4, using the default nat mode, the container's port 80 will be accessible via port 8080 on the host's IP addresses, as well as directly from within the Docker host. But, container port 80 cannot be accessed directly from outside the host. Connections originating from the container will masquerade, using the host's IP address.\n\nIn docker inspect, this port mapping will be shown as follows. Note that there is no HostPort for IPv6, because it is using routed mode:\n\n$ docker container inspect <id> --format \"{{json .NetworkSettings.Ports}}\"\n\n{\"80/tcp\":[{\"HostIp\":\"0.0.0.0\",\"HostPort\":\"8080\"},{\"HostIp\":\"::\",\"HostPort\":\"\"}]}\n\n\nAlternatively, to make the mapping IPv6-only, disabling IPv4 access to the container's port 80, use the unspecified IPv6 address [::] and do not include a host port number:\n\n$ docker run --network mynet -p '[::]::80'\n\nSetting the default bind address for containers\n\nBy default, when a container's ports are mapped without any specific host address, the Docker daemon publishes ports to all host addresses (0.0.0.0 and [::]).\n\nFor example, the following command publishes port 8080 to all network interfaces on the host, on both IPv4 and IPv6 addresses, potentially making them available to the outside world.\n\ndocker run -p 8080:80 nginx\n\n\nYou can change the default binding address for published container ports so that they're only accessible to the Docker host by default. To do that, you can configure the daemon to use the loopback address (127.0.0.1) instead.\n\nWarning\n\nIn releases older than 28.0.0, hosts within the same L2 segment (for example, hosts connected to the same network switch) can reach ports published to localhost. For more information, see moby/moby#45610\n\nTo configure this setting for user-defined bridge networks, use the com.docker.network.bridge.host_binding_ipv4 driver option when you create the network. Despite the option name, it is possible to specify an IPv6 address.\n\n$ docker network create mybridge \\\n\n  -o \"com.docker.network.bridge.host_binding_ipv4=127.0.0.1\"\n\n\nOr, to set the default binding address for containers in all user-defined bridge networks, use daemon configuration option default-network-opts. For example:\n\n{\n\n  \"default-network-opts\": {\n\n    \"bridge\": {\n\n      \"com.docker.network.bridge.host_binding_ipv4\": \"127.0.0.1\"\n\n    }\n\n  }\n\n}\nNote\n\nSetting the default binding address to :: means port bindings with no host address specified will work for any IPv6 address on the host. But, 0.0.0.0 means any IPv4 or IPv6 address.\n\nChanging the default bind address doesn't have any effect on Swarm services. Swarm services are always exposed on the 0.0.0.0 network interface.\n\nMasquerade or SNAT for outgoing packets\n\nNAT is enabled by default for bridge networks, meaning outgoing packets from containers are masqueraded. The source address of packets leaving the Docker host is changed to an address on the host interface the packet is sent on.\n\nMasquerading can be disabled for a user-defined bridge network by using the com.docker.network.bridge.enable_ip_masquerade driver option when creating the network. For example:\n\n$ docker network create mybridge \\\n\n  -o com.docker.network.bridge.enable_ip_masquerade=false ...\n\n\nTo use a specific source address for outgoing packets for a user-defined network, instead of letting masquerading select an address, use options com.docker.network.host_ipv4 and com.docker.network.host_ipv6 to specify the Source NAT (SNAT) address to use. The com.docker.network.bridge.enable_ip_masquerade option must be true, the default, for these options to have any effect.\n\nDefault bridge\n\nTo set the default binding for the default bridge network, configure the \"ip\" key in the daemon.json configuration file:\n\n{\n\n  \"ip\": \"127.0.0.1\"\n\n}\n\nThis changes the default binding address to 127.0.0.1 for published container ports on the default bridge network. Restart the daemon for this change to take effect. Alternatively, you can use the dockerd --ip flag when starting the daemon.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPublishing ports\nDirect routing\nDirect routing to containers in bridge networks\nGateway modes\nExample\nSetting the default bind address for containers\nMasquerade or SNAT for outgoing packets\nDefault bridge\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995213,
    "timestamp": "2026-02-07T06:32:16.311Z",
    "title": "Bridge network driver | Docker Docs",
    "url": "https://docs.docker.com/engine/network/drivers/bridge/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nBridge network driver\nHost network driver\nIPvlan network driver\nMacvlan network driver\nNone network driver\nOverlay network driver\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nNetwork drivers\n/\nBridge network driver\nBridge network driver\nCopy as Markdown\n\nA Docker bridge network has an IPv4 subnet and, optionally, an IPv6 subnet. Each container connected to the bridge network has a network interface with addresses in the network's subnets. By default, it:\n\nAllows unrestricted network access to containers in the network from the host, and from other containers connected to the same bridge network.\nBlocks access from containers in other networks and from outside the Docker host.\nUses masquerading to give containers external network access. Devices on the host's external networks only see the IP address of the Docker host.\nSupports port publishing, where network traffic is forwarded between container ports and ports on host IP addresses. The published ports can be accessed from outside the Docker host, on its IP addresses.\n\nIn terms of Docker, a bridge network uses a software bridge which lets containers connected to the same bridge network communicate, while providing isolation from containers that aren't connected to that bridge network. By default, the Docker bridge driver automatically installs rules in the host machine so that containers connected to different bridge networks can only communicate with each other using published ports.\n\nBridge networks apply to containers running on the same Docker daemon host. For communication among containers running on different Docker daemon hosts, you can either manage routing at the OS level, or you can use an overlay network.\n\nWhen you start Docker, a default bridge network (also called bridge) is created automatically, and newly-started containers connect to it unless otherwise specified. You can also create user-defined custom bridge networks. User-defined bridge networks are superior to the default bridge network.\n\nDifferences between user-defined bridges and the default bridge\n\nUser-defined bridges provide automatic DNS resolution between containers.\n\nContainers on the default bridge network can only access each other by IP addresses, unless you use the --link option, which is considered legacy. On a user-defined bridge network, containers can resolve each other by name or alias.\n\nImagine an application with a web front-end and a database back-end. If you call your containers web and db, the web container can connect to the db container at db, no matter which Docker host the application stack is running on.\n\nIf you run the same application stack on the default bridge network, you need to manually create links between the containers (using the legacy --link flag). These links need to be created in both directions, so you can see this gets complex with more than two containers which need to communicate. Alternatively, you can manipulate the /etc/hosts files within the containers, but this creates problems that are difficult to debug.\n\nUser-defined bridges provide better isolation.\n\nAll containers without a --network specified, are attached to the default bridge network. This can be a risk, as unrelated stacks/services/containers are then able to communicate.\n\nUsing a user-defined network provides a scoped network in which only containers attached to that network are able to communicate.\n\nContainers can be attached and detached from user-defined networks on the fly.\n\nDuring a container's lifetime, you can connect or disconnect it from user-defined networks on the fly. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options.\n\nEach user-defined network creates a configurable bridge.\n\nIf your containers use the default bridge network, you can configure it, but all the containers use the same settings, such as MTU and iptables rules. In addition, configuring the default bridge network happens outside of Docker itself, and requires a restart of Docker.\n\nUser-defined bridge networks are created and configured using docker network create. If different groups of applications have different network requirements, you can configure each user-defined bridge separately, as you create it.\n\nLinked containers on the default bridge network share environment variables.\n\nOriginally, the only way to share environment variables between two containers was to link them using the --link flag. This type of variable sharing isn't possible with user-defined networks. However, there are superior ways to share environment variables. A few ideas:\n\nMultiple containers can mount a file or directory containing the shared information, using a Docker volume.\n\nMultiple containers can be started together using docker-compose and the compose file can define the shared variables.\n\nYou can use swarm services instead of standalone containers, and take advantage of shared secrets and configs.\n\nContainers connected to the same user-defined bridge network effectively expose all ports to each other. For a port to be accessible to containers or non-Docker hosts on different networks, that port must be published using the -p or --publish flag.\n\nOptions\n\nThe following table describes the driver-specific options that you can pass to --opt when creating a custom network using the bridge driver.\n\nOption\tDefault\tDescription\ncom.docker.network.bridge.name\t\tInterface name to use when creating the Linux bridge.\ncom.docker.network.bridge.enable_ip_masquerade\ttrue\tEnable IP masquerading.\ncom.docker.network.host_ipv4\ncom.docker.network.host_ipv6\t\tAddress to use for source NAT. See Packet filtering and firewalls.\ncom.docker.network.bridge.gateway_mode_ipv4\ncom.docker.network.bridge.gateway_mode_ipv6\tnat\tControl external connectivity. See Packet filtering and firewalls.\ncom.docker.network.bridge.enable_icc\ttrue\tEnable or Disable inter-container connectivity.\ncom.docker.network.bridge.host_binding_ipv4\tall IPv4 and IPv6 addresses\tDefault IP when binding container ports.\ncom.docker.network.driver.mtu\t0 (no limit)\tSet the containers network Maximum Transmission Unit (MTU).\ncom.docker.network.container_iface_prefix\teth\tSet a custom prefix for container interfaces.\ncom.docker.network.bridge.inhibit_ipv4\tfalse\tPrevent Docker from assigning an IP address to the bridge.\n\nSome of these options are also available as flags to the dockerd CLI, and you can use them to configure the default docker0 bridge when starting the Docker daemon. The following table shows which options have equivalent flags in the dockerd CLI.\n\nOption\tFlag\ncom.docker.network.bridge.name\t-\ncom.docker.network.bridge.enable_ip_masquerade\t--ip-masq\ncom.docker.network.bridge.enable_icc\t--icc\ncom.docker.network.bridge.host_binding_ipv4\t--ip\ncom.docker.network.driver.mtu\t--mtu\ncom.docker.network.container_iface_prefix\t-\n\nThe Docker daemon supports a --bridge flag, which you can use to define your own docker0 bridge. Use this option if you want to run multiple daemon instances on the same host. For details, see Run multiple daemons.\n\nDefault host binding address\n\nWhen no host address is given in port publishing options like -p 80 or -p 8080:80, the default is to make the container's port 80 available on all host addresses, IPv4 and IPv6.\n\nThe bridge network driver option com.docker.network.bridge.host_binding_ipv4 can be used to modify the default address for published ports.\n\nDespite the option's name, it is possible to specify an IPv6 address.\n\nWhen the default binding address is an address assigned to a specific interface, the container's port will only be accessible via that address.\n\nSetting the default binding address to :: means published ports will only be available on the host's IPv6 addresses. However, setting it to 0.0.0.0 means it will be available on the host's IPv4 and IPv6 addresses.\n\nTo restrict a published port to IPv4 only, the address must be included in the container's publishing options. For example, -p 0.0.0.0:8080:80.\n\nManage a user-defined bridge\n\nUse the docker network create command to create a user-defined bridge network.\n\n$ docker network create my-net\n\n\nYou can specify the subnet, the IP address range, the gateway, and other options. See the docker network create reference or the output of docker network create --help for details.\n\nUse the docker network rm command to remove a user-defined bridge network. If containers are currently connected to the network, disconnect them first.\n\n$ docker network rm my-net\n\n\nWhat's really happening?\n\nWhen you create or remove a user-defined bridge or connect or disconnect a container from a user-defined bridge, Docker uses tools specific to the operating system to manage the underlying network infrastructure (such as adding or removing bridge devices or configuring iptables rules on Linux). These details should be considered implementation details. Let Docker manage your user-defined networks for you.\n\nConnect a container to a user-defined bridge\n\nWhen you create a new container, you can specify one or more --network flags. This example connects an Nginx container to the my-net network. It also publishes port 80 in the container to port 8080 on the Docker host, so external clients can access that port. Any other container connected to the my-net network has access to all ports on the my-nginx container, and vice versa.\n\n$ docker create --name my-nginx \\\n\n  --network my-net \\\n\n  --publish 8080:80 \\\n\n  nginx:latest\n\n\nTo connect a running container to an existing user-defined bridge, use the docker network connect command. The following command connects an already-running my-nginx container to an already-existing my-net network:\n\n$ docker network connect my-net my-nginx\n\nDisconnect a container from a user-defined bridge\n\nTo disconnect a running container from a user-defined bridge, use the docker network disconnect command. The following command disconnects the my-nginx container from the my-net network.\n\n$ docker network disconnect my-net my-nginx\n\nUse IPv6 in a user-defined bridge network\n\nWhen you create your network, you can specify the --ipv6 flag to enable IPv6.\n\n$ docker network create --ipv6 --subnet 2001:db8:1234::/64 my-net\n\n\nIf you do not provide a --subnet option, a Unique Local Address (ULA) prefix will be chosen automatically.\n\nIPv6-only bridge networks\n\nTo skip IPv4 address configuration on the bridge and in its containers, create the network with option --ipv4=false, and enable IPv6 using --ipv6.\n\n$ docker network create --ipv6 --ipv4=false v6net\n\n\nIPv4 address configuration cannot be disabled in the default bridge network.\n\nUse the default bridge network\n\nThe default bridge network is considered a legacy detail of Docker and is not recommended for production use. Configuring it is a manual operation, and it has technical shortcomings.\n\nConnect a container to the default bridge network\n\nIf you do not specify a network using the --network flag, and you do specify a network driver, your container is connected to the default bridge network by default. Containers connected to the default bridge network can communicate, but only by IP address, unless they're linked using the legacy --link flag.\n\nConfigure the default bridge network\n\nTo configure the default bridge network, you specify options in daemon.json. Here is an example daemon.json with several options specified. Only specify the settings you need to customize.\n\n{\n\n  \"bip\": \"192.168.1.1/24\",\n\n  \"fixed-cidr\": \"192.168.1.0/25\",\n\n  \"mtu\": 1500,\n\n  \"default-gateway\": \"192.168.1.254\",\n\n  \"dns\": [\"10.20.1.2\", \"10.20.1.3\"]\n\n}\n\nIn this example:\n\nThe bridge's address is \"192.168.1.1/24\" (from bip).\nThe bridge network's subnet is \"192.168.1.0/24\" (from bip).\nContainer addresses will be allocated from \"192.168.1.0/25\" (from fixed-cidr).\nUse IPv6 with the default bridge network\n\nIPv6 can be enabled for the default bridge using the following options in daemon.json, or their command line equivalents.\n\nThese three options only affect the default bridge, they are not used by user-defined networks. The addresses in below are examples from the IPv6 documentation range.\n\nOption ipv6 is required.\nOption bip6 is optional, it specifies the address of the default bridge, which will be used as the default gateway by containers. It also specifies the subnet for the bridge network.\nOption fixed-cidr-v6 is optional, it specifies the address range Docker may automatically allocate to containers.\nThe prefix should normally be /64 or shorter.\nFor experimentation on a local network, it is better to use a Unique Local Address (ULA) prefix (matching fd00::/8) than a Link Local prefix (matching fe80::/10).\nOption default-gateway-v6 is optional. If unspecified, the default is the first address in the fixed-cidr-v6 subnet.\n{\n\n  \"ipv6\": true,\n\n  \"bip6\": \"2001:db8::1111/64\",\n\n  \"fixed-cidr-v6\": \"2001:db8::/64\",\n\n  \"default-gateway-v6\": \"2001:db8:abcd::89\"\n\n}\n\nIf no bip6 is specified, fixed-cidr-v6 defines the subnet for the bridge network. If no bip6 or fixed-cidr-v6 is specified, a ULA prefix will be chosen.\n\nRestart Docker for changes to take effect.\n\nConnection limit for bridge networks\n\nDue to limitations set by the Linux kernel, bridge networks become unstable and inter-container communications may break when 1000 containers or more connect to a single network.\n\nFor more information about this limitation, see moby/moby#44973.\n\nSkip Bridge IP address configuration\n\nThe bridge is normally assigned the network's --gateway address, which is used as the default route from the bridge network to other networks.\n\nThe com.docker.network.bridge.inhibit_ipv4 option lets you create a network without the IPv4 gateway address being assigned to the bridge. This is useful if you want to configure the gateway IP address for the bridge manually. For instance if you add a physical interface to your bridge, and need it to have the gateway address.\n\nWith this configuration, north-south traffic (to and from the bridge network) won't work unless you've manually configured the gateway address on the bridge, or a device attached to it.\n\nThis option can only be used with user-defined bridge networks.\n\nUsage examples\n\nThis section provides hands-on examples for working with bridge networks.\n\nUse the default bridge network\n\nThis example shows how the default bridge network works. You start two alpine containers on the default bridge and test how they communicate.\n\nNote\n\nThe default bridge network is not recommended for production. Use user-defined bridge networks instead.\n\nList current networks:\n\n$ docker network ls\n\n\n\nNETWORK ID          NAME                DRIVER              SCOPE\n\n17e324f45964        bridge              bridge              local\n\n6ed54d316334        host                host                local\n\n7092879f2cc8        none                null                local\n\n\nThe default bridge network is listed, along with host and none.\n\nStart two alpine containers running ash. The -dit flags mean detached, interactive, and with a TTY. Since you haven't specified a --network flag, the containers connect to the default bridge network.\n\n$ docker run -dit --name alpine1 alpine ash\n\n$ docker run -dit --name alpine2 alpine ash\n\n\nVerify both containers are running:\n\n$ docker container ls\n\n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\n602dbf1edc81        alpine              \"ash\"               4 seconds ago       Up 3 seconds                            alpine2\n\nda33b7aa74b0        alpine              \"ash\"               17 seconds ago      Up 16 seconds                           alpine1\n\n\nInspect the bridge network to see connected containers:\n\n$ docker network inspect bridge\n\n\nThe output shows both containers connected, with their assigned IP addresses (172.17.0.2 for alpine1 and 172.17.0.3 for alpine2).\n\nConnect to alpine1:\n\n$ docker attach alpine1\n\n\n\n/ #\n\n\nShow the network interfaces for alpine1 from within the container:\n\n# ip addr show\n\n\n\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1\n\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n\n    inet 127.0.0.1/8 scope host lo\n\n       valid_lft forever preferred_lft forever\n\n    inet6 ::1/128 scope host\n\n       valid_lft forever preferred_lft forever\n\n27: eth0@if28: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP\n\n    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff\n\n    inet 172.17.0.2/16 scope global eth0\n\n       valid_lft forever preferred_lft forever\n\n\nIn this example, the eth0 interface has the IP address 172.17.0.2.\n\nFrom within alpine1, verify you can connect to the internet:\n\n# ping -c 2 google.com\n\n\n\nPING google.com (172.217.3.174): 56 data bytes\n\n64 bytes from 172.217.3.174: seq=0 ttl=41 time=9.841 ms\n\n64 bytes from 172.217.3.174: seq=1 ttl=41 time=9.897 ms\n\n\n\n--- google.com ping statistics ---\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max = 9.841/9.869/9.897 ms\n\n\nPing the second container by its IP address:\n\n# ping -c 2 172.17.0.3\n\n\n\nPING 172.17.0.3 (172.17.0.3): 56 data bytes\n\n64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.086 ms\n\n64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.094 ms\n\n\n\n--- 172.17.0.3 ping statistics ---\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max = 0.086/0.090/0.094 ms\n\n\nThis succeeds. Now try pinging by container name:\n\n# ping -c 2 alpine2\n\n\n\nping: bad address 'alpine2'\n\n\nOn the default bridge network, containers can't resolve each other by name.\n\nDetach from alpine1 without stopping it using CTRL+p CTRL+q.\n\nClean up: stop the containers and remove them.\n\n$ docker container stop alpine1 alpine2\n\n$ docker container rm alpine1 alpine2\n\n\nStopped containers lose their IP addresses.\n\nUse user-defined bridge networks\n\nThis example shows how user-defined bridge networks provide better isolation and automatic DNS resolution between containers.\n\nCreate the alpine-net network:\n\n$ docker network create --driver bridge alpine-net\n\n\nList Docker's networks:\n\n$ docker network ls\n\n\n\nNETWORK ID          NAME                DRIVER              SCOPE\n\ne9261a8c9a19        alpine-net          bridge              local\n\n17e324f45964        bridge              bridge              local\n\n6ed54d316334        host                host                local\n\n7092879f2cc8        none                null                local\n\n\nInspect the alpine-net network:\n\n$ docker network inspect alpine-net\n\n\nThis shows the network's gateway (for example, 172.18.0.1) and that no containers are connected yet.\n\nCreate four containers. Three connect to alpine-net, and one connects to the default bridge. Then connect one container to both networks:\n\n$ docker run -dit --name alpine1 --network alpine-net alpine ash\n\n$ docker run -dit --name alpine2 --network alpine-net alpine ash\n\n$ docker run -dit --name alpine3 alpine ash\n\n$ docker run -dit --name alpine4 --network alpine-net alpine ash\n\n$ docker network connect bridge alpine4\n\n\nVerify all containers are running:\n\n$ docker container ls\n\n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES\n\n156849ccd902        alpine              \"ash\"               41 seconds ago       Up 41 seconds                           alpine4\n\nfa1340b8d83e        alpine              \"ash\"               51 seconds ago       Up 51 seconds                           alpine3\n\na535d969081e        alpine              \"ash\"               About a minute ago   Up About a minute                       alpine2\n\n0a02c449a6e9        alpine              \"ash\"               About a minute ago   Up About a minute                       alpine1\n\n\nInspect both networks again to see which containers are connected:\n\n$ docker network inspect bridge\n\n\nContainers alpine3 and alpine4 are connected to the bridge network.\n\n$ docker network inspect alpine-net\n\n\nContainers alpine1, alpine2, and alpine4 are connected to alpine-net.\n\nOn user-defined networks, containers can resolve each other by name. Connect to alpine1 and test:\n\nNote\n\nAutomatic service discovery only resolves custom container names, not default automatically generated names.\n\n$ docker container attach alpine1\n\n\n\n# ping -c 2 alpine2\n\n\n\nPING alpine2 (172.18.0.3): 56 data bytes\n\n64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.085 ms\n\n64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.090 ms\n\n\n\n--- alpine2 ping statistics ---\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max = 0.085/0.087/0.090 ms\n\n\n\n# ping -c 2 alpine4\n\n\n\nPING alpine4 (172.18.0.4): 56 data bytes\n\n64 bytes from 172.18.0.4: seq=0 ttl=64 time=0.076 ms\n\n64 bytes from 172.18.0.4: seq=1 ttl=64 time=0.091 ms\n\n\n\n--- alpine4 ping statistics ---\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max = 0.076/0.083/0.091 ms\n\n\nFrom alpine1, you can't connect to alpine3 because it's on a different network:\n\n# ping -c 2 alpine3\n\n\n\nping: bad address 'alpine3'\n\n\nYou also can't connect by IP address. If alpine3's IP is 172.17.0.2:\n\n# ping -c 2 172.17.0.2\n\n\n\nPING 172.17.0.2 (172.17.0.2): 56 data bytes\n\n\n\n--- 172.17.0.2 ping statistics ---\n\n2 packets transmitted, 0 packets received, 100% packet loss\n\n\nDetach from alpine1 using CTRL+p CTRL+q.\n\nSince alpine4 is connected to both networks, it can reach all containers. However, you need to use alpine3's IP address:\n\n$ docker container attach alpine4\n\n\n\n# ping -c 2 alpine1\n\n\n\nPING alpine1 (172.18.0.2): 56 data bytes\n\n64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.074 ms\n\n64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.082 ms\n\n\n\n--- alpine1 ping statistics ---\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max = 0.074/0.078/0.082 ms\n\n\n\n# ping -c 2 alpine2\n\n\n\nPING alpine2 (172.18.0.3): 56 data bytes\n\n64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.075 ms\n\n64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.080 ms\n\n\n\n--- alpine2 ping statistics ---\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max = 0.075/0.077/0.080 ms\n\n\n\n# ping -c 2 alpine3\n\nping: bad address 'alpine3'\n\n\n\n# ping -c 2 172.17.0.2\n\n\n\nPING 172.17.0.2 (172.17.0.2): 56 data bytes\n\n64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.089 ms\n\n64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.075 ms\n\n\n\n--- 172.17.0.2 ping statistics ---\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max = 0.075/0.082/0.089 ms\n\n\nVerify all containers can connect to the internet:\n\n# ping -c 2 google.com\n\n\n\nPING google.com (172.217.3.174): 56 data bytes\n\n64 bytes from 172.217.3.174: seq=0 ttl=41 time=9.778 ms\n\n64 bytes from 172.217.3.174: seq=1 ttl=41 time=9.634 ms\n\n\n\n--- google.com ping statistics ---\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max = 9.634/9.706/9.778 ms\n\n\nDetach with CTRL+p CTRL+q and repeat for alpine3 and alpine1 if desired.\n\nClean up:\n\n$ docker container stop alpine1 alpine2 alpine3 alpine4\n\n$ docker container rm alpine1 alpine2 alpine3 alpine4\n\n$ docker network rm alpine-net\n\nNext steps\nLearn about networking from the container's point of view\nLearn about overlay networks\nLearn about Macvlan networks\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDifferences between user-defined bridges and the default bridge\nOptions\nDefault host binding address\nManage a user-defined bridge\nConnect a container to a user-defined bridge\nDisconnect a container from a user-defined bridge\nUse IPv6 in a user-defined bridge network\nIPv6-only bridge networks\nUse the default bridge network\nConnect a container to the default bridge network\nConfigure the default bridge network\nUse IPv6 with the default bridge network\nConnection limit for bridge networks\nSkip Bridge IP address configuration\nUsage examples\nUse the default bridge network\nUse user-defined bridge networks\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995216,
    "timestamp": "2026-02-07T06:32:16.317Z",
    "title": "Host network driver | Docker Docs",
    "url": "https://docs.docker.com/engine/network/drivers/host/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nBridge network driver\nHost network driver\nIPvlan network driver\nMacvlan network driver\nNone network driver\nOverlay network driver\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nNetwork drivers\n/\nHost network driver\nHost network driver\nCopy as Markdown\n\nIf you use the host network mode for a container, that container's network stack isn't isolated from the Docker host (the container shares the host's networking namespace), and the container doesn't get its own IP-address allocated. For instance, if you run a container which binds to port 80 and you use host networking, the container's application is available on port 80 on the host's IP address.\n\nNote\n\nGiven that the container does not have its own IP-address when using host mode networking, port-mapping doesn't take effect, and the -p, --publish, -P, and --publish-all option are ignored, producing a warning instead:\n\nWARNING: Published ports are discarded when using host network mode\n\n\nHost mode networking can be useful for the following use cases:\n\nTo optimize performance\nIn situations where a container needs to handle a large range of ports\n\nThis is because it doesn't require network address translation (NAT), and no \"userland-proxy\" is created for each port.\n\nPlatform support\n\nThe host networking driver is supported on:\n\nDocker Engine on Linux\nDocker Desktop version 4.34 and later (requires enabling the feature in Settings)\nNote\n\nFor Docker Desktop users, see the Docker Desktop section below for setup instructions.\n\nYou can also use a host network for a swarm service, by passing --network host to the docker service create command. In this case, control traffic (traffic related to managing the swarm and the service) is still sent across an overlay network, but the individual swarm service containers send data using the Docker daemon's host network and ports. This creates some extra limitations. For instance, if a service container binds to port 80, only one service container can run on a given swarm node.\n\nDocker Desktop\n\nHost networking is supported on Docker Desktop version 4.34 and later. To enable this feature:\n\nSign in to your Docker account in Docker Desktop.\nNavigate to Settings.\nUnder the Resources tab, select Network.\nCheck the Enable host networking option.\nSelect Apply and restart.\n\nThis feature works in both directions. This means you can access a server that is running in a container from your host and you can access servers running on your host from any container that is started with host networking enabled. TCP as well as UDP are supported as communication protocols.\n\nExamples\n\nThe following command starts netcat in a container that listens on port 8000:\n\n$ docker run --rm -it --net=host nicolaka/netshoot nc -lkv 0.0.0.0 8000\n\n\nPort 8000 will then be available on the host and you can connect to it with the following command from another terminal:\n\n$ nc localhost 8000\n\n\nWhat you type in here will then appear on the terminal where the container is running.\n\nTo access a service running on the host from the container, you can start a container with host networking enabled with this command:\n\n$ docker run --rm -it --net=host nicolaka/netshoot\n\n\nIf you then want to access a service on your host from the container (in this example a web server running on port 80), you can do it like this:\n\n$ nc localhost 80\n\nLimitations\nProcesses inside the container cannot bind to the IP addresses of the host because the container has no direct access to the interfaces of the host.\nThe host network feature of Docker Desktop works on layer 4. This means that unlike with Docker on Linux, network protocols that operate below TCP or UDP are not supported.\nThis feature doesn't work with Enhanced Container Isolation enabled, since isolating your containers from the host and allowing them access to the host network contradict each other.\nOnly Linux containers are supported. Host networking does not work with Windows containers.\nUsage example\n\nThis example shows how to start an Nginx container that binds directly to port 80 on the Docker host. From a networking perspective, this provides the same level of isolation as if Nginx were running directly on the host, but the container remains isolated in all other aspects (storage, process namespace, user namespace).\n\nPrerequisites\nPort 80 must be available on the Docker host. To make Nginx listen on a different port, see the Nginx image documentation.\nThe host networking driver only works on Linux hosts, and as an opt-in feature in Docker Desktop version 4.34 and later.\nSteps\n\nCreate and start the container as a detached process. The --rm option removes the container when it exits. The -d flag starts it in the background:\n\n$ docker run --rm -d --network host --name my_nginx nginx\n\n\nAccess Nginx by browsing to http://localhost:80/.\n\nExamine your network stack:\n\nCheck all network interfaces and verify that no new interface was created:\n\n$ ip addr show\n\n\nVerify which process is bound to port 80 using netstat. You need sudo because the process is owned by the Docker daemon user:\n\n$ sudo netstat -tulpn | grep :80\n\n\nStop the container. It's removed automatically because of the --rm option:\n\n$ docker container stop my_nginx\n\nNext steps\nLearn about networking from the container's point of view\nLearn about bridge networks\nLearn about overlay networks\nLearn about Macvlan networks\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPlatform support\nDocker Desktop\nExamples\nLimitations\nUsage example\nPrerequisites\nSteps\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995222,
    "timestamp": "2026-02-07T06:32:16.319Z",
    "title": "Macvlan network driver | Docker Docs",
    "url": "https://docs.docker.com/engine/network/drivers/macvlan/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nBridge network driver\nHost network driver\nIPvlan network driver\nMacvlan network driver\nNone network driver\nOverlay network driver\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nNetwork drivers\n/\nMacvlan network driver\nMacvlan network driver\nCopy as Markdown\n\nSome applications, especially legacy applications or applications which monitor network traffic, expect to be directly connected to the physical network. In this type of situation, you can use the macvlan network driver to assign a MAC address to each container's virtual network interface, making it appear to be a physical network interface directly connected to the physical network. In this case, you need to designate a physical interface on your Docker host to use for the Macvlan, as well as the subnet and gateway of the network. You can even isolate your Macvlan networks using different physical network interfaces.\n\nPlatform support and requirements\nThe macvlan driver only works on Linux hosts. It is not supported on Docker Desktop for Mac or Windows, or Docker Engine on Windows.\nMost cloud providers block macvlan networking. You may need physical access to your networking equipment.\nRequires at least Linux kernel version 3.9 (version 4.0 or later is recommended).\nThe macvlan driver is not supported in rootless mode.\nConsiderations\n\nYou may unintentionally degrade your network due to IP address exhaustion or to \"VLAN spread\", a situation that occurs when you have an inappropriately large number of unique MAC addresses in your network.\n\nYour networking equipment needs to be able to handle \"promiscuous mode\", where one physical interface can be assigned multiple MAC addresses.\n\nIf your application can work using a bridge (on a single Docker host) or overlay (to communicate across multiple Docker hosts), these solutions may be better in the long term.\n\nContainers attached to a macvlan network cannot communicate with the host directly, this is a restriction in the Linux kernel. If you need communication between the host and the containers, you can connect the containers to a bridge network as well as the macvlan. It is also possible to create a macvlan interface on the host with the same parent interface, and assign it an IP address in the Docker network's subnet.\n\nOptions\n\nThe following table describes the driver-specific options that you can pass to --opt when creating a network using the macvlan driver.\n\nOption\tDefault\tDescription\nmacvlan_mode\tbridge\tSets the Macvlan mode. Can be one of: bridge, vepa, passthru, private\nparent\t\tSpecifies the parent interface to use.\nCreate a Macvlan network\n\nWhen you create a Macvlan network, it can either be in bridge mode or 802.1Q trunk bridge mode.\n\nIn bridge mode, Macvlan traffic goes through a physical device on the host.\n\nIn 802.1Q trunk bridge mode, traffic goes through an 802.1Q sub-interface which Docker creates on the fly. This allows you to control routing and filtering at a more granular level.\n\nBridge mode\n\nTo create a macvlan network which bridges with a given physical network interface, use --driver macvlan with the docker network create command. You also need to specify the parent, which is the interface the traffic will physically go through on the Docker host.\n\n$ docker network create -d macvlan \\\n\n  --subnet=172.16.86.0/24 \\\n\n  --gateway=172.16.86.1 \\\n\n  -o parent=eth0 pub_net\n\n\nIf you need to exclude IP addresses from being used in the macvlan network, such as when a given IP address is already in use, use --aux-addresses:\n\n$ docker network create -d macvlan \\\n\n  --subnet=192.168.32.0/24 \\\n\n  --ip-range=192.168.32.128/25 \\\n\n  --gateway=192.168.32.254 \\\n\n  --aux-address=\"my-router=192.168.32.129\" \\\n\n  -o parent=eth0 macnet32\n\n802.1Q trunk bridge mode\n\nIf you specify a parent interface name with a dot included, such as eth0.50, Docker interprets that as a sub-interface of eth0 and creates the sub-interface automatically.\n\n$ docker network create -d macvlan \\\n\n    --subnet=192.168.50.0/24 \\\n\n    --gateway=192.168.50.1 \\\n\n    -o parent=eth0.50 macvlan50\n\nUse an IPvlan instead of Macvlan\n\nAn ipvlan network created with option -o ipvlan_mode=l2 is similar to a macvlan network. The main difference is that the ipvlan driver doesn't assign a MAC address to each container, the layer-2 network stack is shared by devices in the ipvlan network. So, containers use the parent interface's MAC address.\n\nThe network will see fewer MAC addresses, and the host's MAC address will be associated with the IP address of each container.\n\nThe choice of network type depends on your environment and requirements. There are some notes about the trade-offs in the Linux kernel documentation.\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.210.0/24 \\\n\n    --gateway=192.168.210.254 \\\n\n     -o ipvlan_mode=l2 -o parent=eth0 ipvlan210\n\nUse IPv6\n\nIf you have configured the Docker daemon to allow IPv6, you can use dual-stack IPv4/IPv6 macvlan networks.\n\n$ docker network create -d macvlan \\\n\n    --subnet=192.168.216.0/24 --subnet=192.168.218.0/24 \\\n\n    --gateway=192.168.216.1 --gateway=192.168.218.1 \\\n\n    --subnet=2001:db8:abc8::/64 --gateway=2001:db8:abc8::10 \\\n\n     -o parent=eth0.218 \\\n\n     -o macvlan_mode=bridge macvlan216\n\nUsage examples\n\nThis section provides hands-on examples for working with macvlan networks, including bridge mode and 802.1Q trunk bridge mode.\n\nNote\n\nThese examples assume your ethernet interface is eth0. If your device has a different name, use that instead.\n\nBridge mode example\n\nIn bridge mode, your traffic flows through eth0 and Docker routes traffic to your container using its MAC address. To network devices on your network, your container appears to be physically attached to the network.\n\nCreate a macvlan network called my-macvlan-net. Modify the subnet, gateway, and parent values to match your environment:\n\n$ docker network create -d macvlan \\\n\n  --subnet=172.16.86.0/24 \\\n\n  --gateway=172.16.86.1 \\\n\n  -o parent=eth0 \\\n\n  my-macvlan-net\n\n\nVerify the network was created:\n\n$ docker network ls\n\n$ docker network inspect my-macvlan-net\n\n\nStart an alpine container and attach it to the my-macvlan-net network. The -dit flags start the container in the background. The --rm flag removes the container when it stops:\n\n$ docker run --rm -dit \\\n\n  --network my-macvlan-net \\\n\n  --name my-macvlan-alpine \\\n\n  alpine:latest \\\n\n  ash\n\n\nInspect the container and notice the MacAddress key within the Networks section:\n\n$ docker container inspect my-macvlan-alpine\n\n\nLook for output similar to:\n\n\"Networks\": {\n\n  \"my-macvlan-net\": {\n\n    \"Gateway\": \"172.16.86.1\",\n\n    \"IPAddress\": \"172.16.86.2\",\n\n    \"IPPrefixLen\": 24,\n\n    \"MacAddress\": \"02:42:ac:10:56:02\",\n\n    ...\n\n  }\n\n}\n\nCheck how the container sees its own network interfaces:\n\n$ docker exec my-macvlan-alpine ip addr show eth0\n\n\n\n9: eth0@tunl0: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP\n\nlink/ether 02:42:ac:10:56:02 brd ff:ff:ff:ff:ff:ff\n\ninet 172.16.86.2/24 brd 172.16.86.255 scope global eth0\n\n   valid_lft forever preferred_lft forever\n\n\nCheck the routing table:\n\n$ docker exec my-macvlan-alpine ip route\n\n\n\ndefault via 172.16.86.1 dev eth0\n\n172.16.86.0/24 dev eth0 scope link  src 172.16.86.2\n\n\nStop the container (Docker removes it automatically) and remove the network:\n\n$ docker container stop my-macvlan-alpine\n\n$ docker network rm my-macvlan-net\n\n802.1Q trunked bridge mode example\n\nIn 802.1Q trunk bridge mode, your traffic flows through a sub-interface of eth0 (called eth0.10) and Docker routes traffic to your container using its MAC address. To network devices on your network, your container appears to be physically attached to the network.\n\nCreate a macvlan network called my-8021q-macvlan-net. Modify the subnet, gateway, and parent values to match your environment:\n\n$ docker network create -d macvlan \\\n\n  --subnet=172.16.86.0/24 \\\n\n  --gateway=172.16.86.1 \\\n\n  -o parent=eth0.10 \\\n\n  my-8021q-macvlan-net\n\n\nVerify the network was created and has parent eth0.10. You can use ip addr show on the Docker host to verify that the interface eth0.10 exists:\n\n$ docker network ls\n\n$ docker network inspect my-8021q-macvlan-net\n\n\nStart an alpine container and attach it to the my-8021q-macvlan-net network:\n\n$ docker run --rm -itd \\\n\n  --network my-8021q-macvlan-net \\\n\n  --name my-second-macvlan-alpine \\\n\n  alpine:latest \\\n\n  ash\n\n\nInspect the container and notice the MacAddress key:\n\n$ docker container inspect my-second-macvlan-alpine\n\n\nLook for the Networks section with the MAC address.\n\nCheck how the container sees its own network interfaces:\n\n$ docker exec my-second-macvlan-alpine ip addr show eth0\n\n\n\n11: eth0@if10: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP\n\nlink/ether 02:42:ac:10:56:02 brd ff:ff:ff:ff:ff:ff\n\ninet 172.16.86.2/24 brd 172.16.86.255 scope global eth0\n\n   valid_lft forever preferred_lft forever\n\n\nCheck the routing table:\n\n$ docker exec my-second-macvlan-alpine ip route\n\n\n\ndefault via 172.16.86.1 dev eth0\n\n172.16.86.0/24 dev eth0 scope link  src 172.16.86.2\n\n\nStop the container and remove the network:\n\n$ docker container stop my-second-macvlan-alpine\n\n$ docker network rm my-8021q-macvlan-net\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPlatform support and requirements\nConsiderations\nOptions\nCreate a Macvlan network\nBridge mode\n802.1Q trunk bridge mode\nUse an IPvlan instead of Macvlan\nUse IPv6\nUsage examples\nBridge mode example\n802.1Q trunked bridge mode example\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995219,
    "timestamp": "2026-02-07T06:32:16.319Z",
    "title": "IPvlan network driver | Docker Docs",
    "url": "https://docs.docker.com/engine/network/drivers/ipvlan/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nBridge network driver\nHost network driver\nIPvlan network driver\nMacvlan network driver\nNone network driver\nOverlay network driver\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nNetwork drivers\n/\nIPvlan network driver\nIPvlan network driver\nCopy as Markdown\n\nThe IPvlan driver gives users total control over both IPv4 and IPv6 addressing. The VLAN driver builds on top of that in giving operators complete control of layer 2 VLAN tagging and even IPvlan L3 routing for users interested in underlay network integration. For overlay deployments that abstract away physical constraints see the multi-host overlay driver.\n\nIPvlan is a new twist on the tried and true network virtualization technique. The Linux implementations are extremely lightweight because rather than using the traditional Linux bridge for isolation, they are associated to a Linux Ethernet interface or sub-interface to enforce separation between networks and connectivity to the physical network.\n\nIPvlan offers a number of unique features and plenty of room for further innovations with the various modes. Two high level advantages of these approaches are, the positive performance implications of bypassing the Linux bridge and the simplicity of having fewer moving parts. Removing the bridge that traditionally resides in between the Docker host NIC and container interface leaves a simple setup consisting of container interfaces, attached directly to the Docker host interface. This result is easy to access for external facing services as there is no need for port mappings in these scenarios.\n\nOptions\n\nThe following table describes the driver-specific options that you can pass to --opt when creating a network using the ipvlan driver.\n\nOption\tDefault\tDescription\nipvlan_mode\tl2\tSets the IPvlan operating mode. Can be one of: l2, l3, l3s\nipvlan_flag\tbridge\tSets the IPvlan mode flag. Can be one of: bridge, private, vepa\nparent\t\tSpecifies the parent interface to use.\nExamples\nPrerequisites\nThe examples on this page are all single host.\nAll examples can be performed on a single host running Docker. Any example using a sub-interface like eth0.10 can be replaced with eth0 or any other valid parent interface on the Docker host. Sub-interfaces with a . are created on the fly. -o parent interfaces can also be left out of the docker network create all together and the driver will create a dummy interface that will enable local host connectivity to perform the examples.\nKernel requirements:\nIPvlan Linux kernel v4.2+ (support for earlier kernels exists but is buggy). To check your current kernel version, use uname -r\nIPvlan L2 mode example usage\n\nAn example of the IPvlan L2 mode topology is shown in the following image. The driver is specified with -d driver_name option. In this case -d ipvlan.\n\nThe parent interface in the next example -o parent=eth0 is configured as follows:\n\n$ ip addr show eth0\n\n3: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n\n    inet 192.168.1.250/24 brd 192.168.1.255 scope global eth0\n\n\nUse the network from the host's interface as the --subnet in the docker network create. The container will be attached to the same network as the host interface as set via the -o parent= option.\n\nCreate the IPvlan network and run a container attaching to it:\n\n# IPvlan  (-o ipvlan_mode= Defaults to L2 mode if not specified)\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.1.0/24 \\\n\n    --gateway=192.168.1.1 \\\n\n    -o ipvlan_mode=l2 \\\n\n    -o parent=eth0 db_net\n\n\n\n# Start a container on the db_net network\n\n$ docker run --net=db_net -it --rm alpine /bin/sh\n\n\n\n# NOTE: the containers can NOT ping the underlying host interfaces as\n\n# they are intentionally filtered by Linux for additional isolation.\n\n\nThe default mode for IPvlan is l2. If -o ipvlan_mode= is left unspecified, the default mode will be used. Similarly, if the --gateway is left empty, the first usable address on the network will be set as the gateway. For example, if the subnet provided in the network create is --subnet=192.168.1.0/24 then the gateway the container receives is 192.168.1.1.\n\nTo help understand how this mode interacts with other hosts, the following figure shows the same layer 2 segment between two Docker hosts that applies to and IPvlan L2 mode.\n\nThe following will create the exact same network as the network db_net created earlier, with the driver defaults for --gateway=192.168.1.1 and -o ipvlan_mode=l2.\n\n# IPvlan  (-o ipvlan_mode= Defaults to L2 mode if not specified)\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.1.0/24 \\\n\n    -o parent=eth0 db_net_ipv\n\n\n\n# Start a container with an explicit name in daemon mode\n\n$ docker run --net=db_net_ipv --name=ipv1 -itd alpine /bin/sh\n\n\n\n# Start a second container and ping using the container name\n\n# to see the docker included name resolution functionality\n\n$ docker run --net=db_net_ipv --name=ipv2 -it --rm alpine /bin/sh\n\n$ ping -c 4 ipv1\n\n\n\n# NOTE: the containers can NOT ping the underlying host interfaces as\n\n# they are intentionally filtered by Linux for additional isolation.\n\n\nThe drivers also support the --internal flag that will completely isolate containers on a network from any communications external to that network. Since network isolation is tightly coupled to the network's parent interface the result of leaving the -o parent= option off of a docker network create is the exact same as the --internal option. If the parent interface is not specified or the --internal flag is used, a netlink type dummy parent interface is created for the user and used as the parent interface effectively isolating the network completely.\n\nThe following two docker network create examples result in identical networks that you can attach container to:\n\n# Empty '-o parent=' creates an isolated network\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.10.0/24 isolated1\n\n\n\n# Explicit '--internal' flag is the same:\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.11.0/24 --internal isolated2\n\n\n\n# Even the '--subnet=' can be left empty and the default\n\n# IPAM subnet of 172.18.0.0/16 will be assigned\n\n$ docker network create -d ipvlan isolated3\n\n\n\n$ docker run --net=isolated1 --name=cid1 -it --rm alpine /bin/sh\n\n$ docker run --net=isolated2 --name=cid2 -it --rm alpine /bin/sh\n\n$ docker run --net=isolated3 --name=cid3 -it --rm alpine /bin/sh\n\n\n\n# To attach to any use `docker exec` and start a shell\n\n$ docker exec -it cid1 /bin/sh\n\n$ docker exec -it cid2 /bin/sh\n\n$ docker exec -it cid3 /bin/sh\n\nIPvlan 802.1Q trunk L2 mode example usage\n\nArchitecturally, IPvlan L2 mode trunking is the same as Macvlan with regard to gateways and L2 path isolation. There are nuances that can be advantageous for CAM table pressure in ToR switches, one MAC per port and MAC exhaustion on a host's parent NIC to name a few. The 802.1Q trunk scenario looks the same. Both modes adhere to tagging standards and have seamless integration with the physical network for underlay integration and hardware vendor plugin integrations.\n\nHosts on the same VLAN are typically on the same subnet and almost always are grouped together based on their security policy. In most scenarios, a multi-tier application is tiered into different subnets because the security profile of each process requires some form of isolation. For example, hosting your credit card processing on the same virtual network as the frontend webserver would be a regulatory compliance issue, along with circumventing the long standing best practice of layered defense in depth architectures. VLANs or the equivocal VNI (Virtual Network Identifier) when using the Overlay driver, are the first step in isolating tenant traffic.\n\nThe Linux sub-interface tagged with a VLAN can either already exist or will be created when you call a docker network create. docker network rm will delete the sub-interface. Parent interfaces such as eth0 are not deleted, only sub-interfaces with a netlink parent index > 0.\n\nFor the driver to add/delete the VLAN sub-interfaces the format needs to be interface_name.vlan_tag. Other sub-interface naming can be used as the specified parent, but the link will not be deleted automatically when docker network rm is invoked.\n\nThe option to use either existing parent VLAN sub-interfaces or let Docker manage them enables the user to either completely manage the Linux interfaces and networking or let Docker create and delete the VLAN parent sub-interfaces (netlink ip link) with no effort from the user.\n\nFor example: use eth0.10 to denote a sub-interface of eth0 tagged with the VLAN id of 10. The equivalent ip link command would be ip link add link eth0 name eth0.10 type vlan id 10.\n\nThe example creates the VLAN tagged networks and then starts two containers to test connectivity between containers. Different VLANs cannot ping one another without a router routing between the two networks. The default namespace is not reachable per IPvlan design in order to isolate container namespaces from the underlying host.\n\nVLAN ID 20\n\nIn the first network tagged and isolated by the Docker host, eth0.20 is the parent interface tagged with VLAN id 20 specified with -o parent=eth0.20. Other naming formats can be used, but the links need to be added and deleted manually using ip link or Linux configuration files. As long as the -o parent exists, anything can be used if it is compliant with Linux netlink.\n\n# now add networks and hosts as you would normally by attaching to the master (sub)interface that is tagged\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.20.0/24 \\\n\n    --gateway=192.168.20.1 \\\n\n    -o parent=eth0.20 ipvlan20\n\n\n\n# in two separate terminals, start a Docker container and the containers can now ping one another.\n\n$ docker run --net=ipvlan20 -it --name ivlan_test1 --rm alpine /bin/sh\n\n$ docker run --net=ipvlan20 -it --name ivlan_test2 --rm alpine /bin/sh\n\nVLAN ID 30\n\nIn the second network, tagged and isolated by the Docker host, eth0.30 is the parent interface tagged with VLAN id 30 specified with -o parent=eth0.30. The ipvlan_mode= defaults to l2 mode ipvlan_mode=l2. It can also be explicitly set with the same result as shown in the next example.\n\n# now add networks and hosts as you would normally by attaching to the master (sub)interface that is tagged.\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.30.0/24 \\\n\n    --gateway=192.168.30.1 \\\n\n    -o parent=eth0.30 \\\n\n    -o ipvlan_mode=l2 ipvlan30\n\n\n\n# in two separate terminals, start a Docker container and the containers can now ping one another.\n\n$ docker run --net=ipvlan30 -it --name ivlan_test3 --rm alpine /bin/sh\n\n$ docker run --net=ipvlan30 -it --name ivlan_test4 --rm alpine /bin/sh\n\n\nThe gateway is set inside of the container as the default gateway. That gateway would typically be an external router on the network.\n\n$$ ip route\n\n  default via 192.168.30.1 dev eth0\n\n  192.168.30.0/24 dev eth0  src 192.168.30.2\n\n\nExample: Multi-Subnet IPvlan L2 Mode starting two containers on the same subnet and pinging one another. In order for the 192.168.114.0/24 to reach 192.168.116.0/24 it requires an external router in L2 mode. L3 mode can route between subnets that share a common -o parent=.\n\nSecondary addresses on network routers are common as an address space becomes exhausted to add another secondary to an L3 VLAN interface or commonly referred to as a \"switched virtual interface\" (SVI).\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.114.0/24 --subnet=192.168.116.0/24 \\\n\n    --gateway=192.168.114.254 --gateway=192.168.116.254 \\\n\n    -o parent=eth0.114 \\\n\n    -o ipvlan_mode=l2 ipvlan114\n\n\n\n$ docker run --net=ipvlan114 --ip=192.168.114.10 -it --rm alpine /bin/sh\n\n$ docker run --net=ipvlan114 --ip=192.168.114.11 -it --rm alpine /bin/sh\n\n\nA key takeaway is, operators have the ability to map their physical network into their virtual network for integrating containers into their environment with no operational overhauls required. NetOps drops an 802.1Q trunk into the Docker host. That virtual link would be the -o parent= passed in the network creation. For untagged (non-VLAN) links, it is as simple as -o parent=eth0 or for 802.1Q trunks with VLAN IDs each network gets mapped to the corresponding VLAN/Subnet from the network.\n\nAn example being, NetOps provides VLAN ID and the associated subnets for VLANs being passed on the Ethernet link to the Docker host server. Those values are plugged into the docker network create commands when provisioning the Docker networks. These are persistent configurations that are applied every time the Docker engine starts which alleviates having to manage often complex configuration files. The network interfaces can also be managed manually by being pre-created and Docker networking will never modify them, and use them as parent interfaces. Example mappings from NetOps to Docker network commands are as follows:\n\nVLAN: 10, Subnet: 172.16.80.0/24, Gateway: 172.16.80.1\n--subnet=172.16.80.0/24 --gateway=172.16.80.1 -o parent=eth0.10\nVLAN: 20, IP subnet: 172.16.50.0/22, Gateway: 172.16.50.1\n--subnet=172.16.50.0/22 --gateway=172.16.50.1 -o parent=eth0.20\nVLAN: 30, Subnet: 10.1.100.0/16, Gateway: 10.1.100.1\n--subnet=10.1.100.0/16 --gateway=10.1.100.1 -o parent=eth0.30\nIPvlan L3 mode example\n\nIPvlan will require routes to be distributed to each endpoint. The driver only builds the IPvlan L3 mode port and attaches the container to the interface. Route distribution throughout a cluster is beyond the initial implementation of this single host scoped driver. In L3 mode, the Docker host is very similar to a router starting new networks in the container. They are on networks that the upstream network will not know about without route distribution. For those curious how IPvlan L3 will fit into container networking, see the following examples.\n\nIPvlan L3 mode drops all broadcast and multicast traffic. This reason alone makes IPvlan L3 mode a prime candidate for those looking for massive scale and predictable network integrations. It is predictable and in turn will lead to greater uptimes because there is no bridging involved. Bridging loops have been responsible for high profile outages that can be hard to pinpoint depending on the size of the failure domain. This is due to the cascading nature of BPDUs (Bridge Port Data Units) that are flooded throughout a broadcast domain (VLAN) to find and block topology loops. Eliminating bridging domains, or at the least, keeping them isolated to a pair of ToRs (top of rack switches) will reduce hard to troubleshoot bridging instabilities. IPvlan L2 modes is well suited for isolated VLANs only trunked into a pair of ToRs that can provide a loop-free non-blocking fabric. The next step further is to route at the edge via IPvlan L3 mode that reduces a failure domain to a local host only.\n\nL3 mode needs to be on a separate subnet as the default namespace since it requires a netlink route in the default namespace pointing to the IPvlan parent interface.\nThe parent interface used in this example is eth0 and it is on the subnet 192.168.1.0/24. Notice the docker network is not on the same subnet as eth0.\nUnlike IPvlan l2 modes, different subnets/networks can ping one another as long as they share the same parent interface -o parent=.\n$$ ip a show eth0\n\n3: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n\n    link/ether 00:50:56:39:45:2e brd ff:ff:ff:ff:ff:ff\n\n    inet 192.168.1.250/24 brd 192.168.1.255 scope global eth0\n\nA traditional gateway doesn't mean much to an L3 mode IPvlan interface since there is no broadcast traffic allowed. Because of that, the container default gateway points to the containers eth0 device. See below for CLI output of ip route or ip -6 route from inside an L3 container for details.\n\nThe mode -o ipvlan_mode=l3 must be explicitly specified since the default IPvlan mode is l2.\n\nThe following example does not specify a parent interface. The network drivers will create a dummy type link for the user rather than rejecting the network creation and isolating containers from only communicating with one another.\n\n# Create the IPvlan L3 network\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.214.0/24 \\\n\n    --subnet=10.1.214.0/24 \\\n\n    -o ipvlan_mode=l3 ipnet210\n\n\n\n# Test 192.168.214.0/24 connectivity\n\n$ docker run --net=ipnet210 --ip=192.168.214.10 -itd alpine /bin/sh\n\n$ docker run --net=ipnet210 --ip=10.1.214.10 -itd alpine /bin/sh\n\n\n\n# Test L3 connectivity from 10.1.214.0/24 to 192.168.214.0/24\n\n$ docker run --net=ipnet210 --ip=192.168.214.9 -it --rm alpine ping -c 2 10.1.214.10\n\n\n\n# Test L3 connectivity from 192.168.214.0/24 to 10.1.214.0/24\n\n$ docker run --net=ipnet210 --ip=10.1.214.9 -it --rm alpine ping -c 2 192.168.214.10\n\nNote\n\nNotice that there is no --gateway= option in the network create. The field is ignored if one is specified l3 mode. Take a look at the container routing table from inside of the container:\n\n# Inside an L3 mode container\n\n$$ ip route\n\n default dev eth0\n\n  192.168.214.0/24 dev eth0  src 192.168.214.10\n\n\nIn order to ping the containers from a remote Docker host or the container be able to ping a remote host, the remote host or the physical network in between need to have a route pointing to the host IP address of the container's Docker host eth interface.\n\nDual stack IPv4 IPv6 IPvlan L2 mode\n\nNot only does Libnetwork give you complete control over IPv4 addressing, but it also gives you total control over IPv6 addressing as well as feature parity between the two address families.\n\nThe next example will start with IPv6 only. Start two containers on the same VLAN 139 and ping one another. Since the IPv4 subnet is not specified, the default IPAM will provision a default IPv4 subnet. That subnet is isolated unless the upstream network is explicitly routing it on VLAN 139.\n\n# Create a v6 network\n\n$ docker network create -d ipvlan \\\n\n    --ipv6 --subnet=2001:db8:abc2::/64 --gateway=2001:db8:abc2::22 \\\n\n    -o parent=eth0.139 v6ipvlan139\n\n\n\n# Start a container on the network\n\n$ docker run --net=v6ipvlan139 -it --rm alpine /bin/sh\n\n\nView the container eth0 interface and v6 routing table:\n\n# Inside the IPv6 container\n\n$$ ip a show eth0\n\n75: eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default\n\n    link/ether 00:50:56:2b:29:40 brd ff:ff:ff:ff:ff:ff\n\n    inet 172.18.0.2/16 scope global eth0\n\n       valid_lft forever preferred_lft forever\n\n    inet6 2001:db8:abc4::250:56ff:fe2b:2940/64 scope link\n\n       valid_lft forever preferred_lft forever\n\n    inet6 2001:db8:abc2::1/64 scope link nodad\n\n       valid_lft forever preferred_lft forever\n\n\n\n$$ ip -6 route\n\n2001:db8:abc4::/64 dev eth0  proto kernel  metric 256\n\n2001:db8:abc2::/64 dev eth0  proto kernel  metric 256\n\ndefault via 2001:db8:abc2::22 dev eth0  metric 1024\n\n\nStart a second container and ping the first container's v6 address.\n\n# Test L2 connectivity over IPv6\n\n$ docker run --net=v6ipvlan139 -it --rm alpine /bin/sh\n\n\n\n# Inside the second IPv6 container\n\n$$ ip a show eth0\n\n75: eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default\n\n    link/ether 00:50:56:2b:29:40 brd ff:ff:ff:ff:ff:ff\n\n    inet 172.18.0.3/16 scope global eth0\n\n       valid_lft forever preferred_lft forever\n\n    inet6 2001:db8:abc4::250:56ff:fe2b:2940/64 scope link tentative dadfailed\n\n       valid_lft forever preferred_lft forever\n\n    inet6 2001:db8:abc2::2/64 scope link nodad\n\n       valid_lft forever preferred_lft forever\n\n\n\n$$ ping6 2001:db8:abc2::1\n\nPING 2001:db8:abc2::1 (2001:db8:abc2::1): 56 data bytes\n\n64 bytes from 2001:db8:abc2::1%eth0: icmp_seq=0 ttl=64 time=0.044 ms\n\n64 bytes from 2001:db8:abc2::1%eth0: icmp_seq=1 ttl=64 time=0.058 ms\n\n\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max/stddev = 0.044/0.051/0.058/0.000 ms\n\n\nThe next example with setup a dual stack IPv4/IPv6 network with an example VLAN ID of 140.\n\nNext create a network with two IPv4 subnets and one IPv6 subnets, all of which have explicit gateways:\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.140.0/24 --subnet=192.168.142.0/24 \\\n\n    --gateway=192.168.140.1 --gateway=192.168.142.1 \\\n\n    --subnet=2001:db8:abc9::/64 --gateway=2001:db8:abc9::22 \\\n\n    -o parent=eth0.140 \\\n\n    -o ipvlan_mode=l2 ipvlan140\n\n\nStart a container and view eth0 and both v4 & v6 routing tables:\n\n$ docker run --net=ipvlan140 --ip6=2001:db8:abc2::51 -it --rm alpine /bin/sh\n\n\n\n$ ip a show eth0\n\n78: eth0@if77: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default\n\n    link/ether 00:50:56:2b:29:40 brd ff:ff:ff:ff:ff:ff\n\n    inet 192.168.140.2/24 scope global eth0\n\n       valid_lft forever preferred_lft forever\n\n    inet6 2001:db8:abc4::250:56ff:fe2b:2940/64 scope link\n\n       valid_lft forever preferred_lft forever\n\n    inet6 2001:db8:abc9::1/64 scope link nodad\n\n       valid_lft forever preferred_lft forever\n\n\n\n$$ ip route\n\ndefault via 192.168.140.1 dev eth0\n\n192.168.140.0/24 dev eth0  proto kernel  scope link  src 192.168.140.2\n\n\n\n$$ ip -6 route\n\n2001:db8:abc4::/64 dev eth0  proto kernel  metric 256\n\n2001:db8:abc9::/64 dev eth0  proto kernel  metric 256\n\ndefault via 2001:db8:abc9::22 dev eth0  metric 1024\n\n\nStart a second container with a specific --ip4 address and ping the first host using IPv4 packets:\n\n$ docker run --net=ipvlan140 --ip=192.168.140.10 -it --rm alpine /bin/sh\n\nNote\n\nDifferent subnets on the same parent interface in IPvlan L2 mode cannot ping one another. That requires a router to proxy-arp the requests with a secondary subnet. However, IPvlan L3 will route the unicast traffic between disparate subnets as long as they share the same -o parent parent link.\n\nDual stack IPv4 IPv6 IPvlan L3 mode\n\nExample: IPvlan L3 Mode Dual Stack IPv4/IPv6, Multi-Subnet w/ 802.1Q VLAN Tag:118\n\nAs in all of the examples, a tagged VLAN interface does not have to be used. The sub-interfaces can be swapped with eth0, eth1, bond0 or any other valid interface on the host other then the lo loopback.\n\nThe primary difference you will see is that L3 mode does not create a default route with a next-hop but rather sets a default route pointing to dev eth only since ARP/Broadcasts/Multicast are all filtered by Linux as per the design. Since the parent interface is essentially acting as a router, the parent interface IP and subnet needs to be different from the container networks. That is the opposite of bridge and L2 modes, which need to be on the same subnet (broadcast domain) in order to forward broadcast and multicast packets.\n\n# Create an IPv6+IPv4 Dual Stack IPvlan L3 network\n\n# Gateways for both v4 and v6 are set to a dev e.g. 'default dev eth0'\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.110.0/24 \\\n\n    --subnet=192.168.112.0/24 \\\n\n    --subnet=2001:db8:abc6::/64 \\\n\n    -o parent=eth0 \\\n\n    -o ipvlan_mode=l3 ipnet110\n\n\n\n\n\n# Start a few of containers on the network (ipnet110)\n\n# in separate terminals and check connectivity\n\n$ docker run --net=ipnet110 -it --rm alpine /bin/sh\n\n# Start a second container specifying the v6 address\n\n$ docker run --net=ipnet110 --ip6=2001:db8:abc6::10 -it --rm alpine /bin/sh\n\n# Start a third specifying the IPv4 address\n\n$ docker run --net=ipnet110 --ip=192.168.112.30 -it --rm alpine /bin/sh\n\n# Start a 4th specifying both the IPv4 and IPv6 addresses\n\n$ docker run --net=ipnet110 --ip6=2001:db8:abc6::50 --ip=192.168.112.50 -it --rm alpine /bin/sh\n\n\nInterface and routing table outputs are as follows:\n\n$$ ip a show eth0\n\n63: eth0@if59: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default\n\n    link/ether 00:50:56:2b:29:40 brd ff:ff:ff:ff:ff:ff\n\n    inet 192.168.112.2/24 scope global eth0\n\n       valid_lft forever preferred_lft forever\n\n    inet6 2001:db8:abc4::250:56ff:fe2b:2940/64 scope link\n\n       valid_lft forever preferred_lft forever\n\n    inet6 2001:db8:abc6::10/64 scope link nodad\n\n       valid_lft forever preferred_lft forever\n\n\n\n# Note the default route is the eth device because ARPs are filtered.\n\n$$ ip route\n\n  default dev eth0  scope link\n\n  192.168.112.0/24 dev eth0  proto kernel  scope link  src 192.168.112.2\n\n\n\n$$ ip -6 route\n\n2001:db8:abc4::/64 dev eth0  proto kernel  metric 256\n\n2001:db8:abc6::/64 dev eth0  proto kernel  metric 256\n\ndefault dev eth0  metric 1024\n\nNote\n\nThere may be a bug when specifying --ip6= addresses when you delete a container with a specified v6 address and then start a new container with the same v6 address it throws the following like the address isn't properly being released to the v6 pool. It will fail to unmount the container and be left dead.\n\ndocker: Error response from daemon: Address already in use.\n\nManually create 802.1Q links\nVLAN ID 40\n\nIf a user does not want the driver to create the VLAN sub-interface, it needs to exist before running docker network create. If you have sub-interface naming that is not interface.vlan_id it is honored in the -o parent= option again as long as the interface exists and is up.\n\nLinks, when manually created, can be named anything as long as they exist when the network is created. Manually created links do not get deleted regardless of the name when the network is deleted with docker network rm.\n\n# create a new sub-interface tied to dot1q vlan 40\n\n$ ip link add link eth0 name eth0.40 type vlan id 40\n\n\n\n# enable the new sub-interface\n\n$ ip link set eth0.40 up\n\n\n\n# now add networks and hosts as you would normally by attaching to the master (sub)interface that is tagged\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.40.0/24 \\\n\n    --gateway=192.168.40.1 \\\n\n    -o parent=eth0.40 ipvlan40\n\n\n\n# in two separate terminals, start a Docker container and the containers can now ping one another.\n\n$ docker run --net=ipvlan40 -it --name ivlan_test5 --rm alpine /bin/sh\n\n$ docker run --net=ipvlan40 -it --name ivlan_test6 --rm alpine /bin/sh\n\n\nExample: VLAN sub-interface manually created with any name:\n\n# create a new sub interface tied to dot1q vlan 40\n\n$ ip link add link eth0 name foo type vlan id 40\n\n\n\n# enable the new sub-interface\n\n$ ip link set foo up\n\n\n\n# now add networks and hosts as you would normally by attaching to the master (sub)interface that is tagged\n\n$ docker network create -d ipvlan \\\n\n    --subnet=192.168.40.0/24 --gateway=192.168.40.1 \\\n\n    -o parent=foo ipvlan40\n\n\n\n# in two separate terminals, start a Docker container and the containers can now ping one another.\n\n$ docker run --net=ipvlan40 -it --name ivlan_test5 --rm alpine /bin/sh\n\n$ docker run --net=ipvlan40 -it --name ivlan_test6 --rm alpine /bin/sh\n\n\nManually created links can be cleaned up with:\n\n$ ip link del foo\n\n\nAs with all of the Libnetwork drivers, they can be mixed and matched, even as far as running 3rd party ecosystem drivers in parallel for maximum flexibility to the Docker user.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nOptions\nExamples\nPrerequisites\nIPvlan L2 mode example usage\nIPvlan 802.1Q trunk L2 mode example usage\nIPvlan L3 mode example\nDual stack IPv4 IPv6 IPvlan L2 mode\nDual stack IPv4 IPv6 IPvlan L3 mode\nManually create 802.1Q links\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995225,
    "timestamp": "2026-02-07T06:32:16.326Z",
    "title": "None network driver | Docker Docs",
    "url": "https://docs.docker.com/engine/network/drivers/none/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nBridge network driver\nHost network driver\nIPvlan network driver\nMacvlan network driver\nNone network driver\nOverlay network driver\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nNetwork drivers\n/\nNone network driver\nNone network driver\nCopy as Markdown\n\nIf you want to completely isolate the networking stack of a container, you can use the --network none flag when starting the container. Within the container, only the loopback device is created.\n\nThe following example shows the output of ip link show in an alpine container using the none network driver.\n\n$ docker run --rm --network none alpine:latest ip link show\n\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000\n\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n\n\nNo IPv6 loopback address is configured for containers using the none driver.\n\n$ docker run --rm --network none --name no-net-alpine alpine:latest ip addr show\n\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000\n\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n\n    inet 127.0.0.1/8 scope host lo\n\n       valid_lft forever preferred_lft forever\n\nNext steps\nLearn about networking from the container's point of view\nLearn about host networking\nLearn about bridge networks\nLearn about overlay networks\nLearn about Macvlan networks\n\nEdit this page\n\nRequest changes\n\nTable of contents\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995231,
    "timestamp": "2026-02-07T06:32:16.328Z",
    "title": "CA certificates | Docker Docs",
    "url": "https://docs.docker.com/engine/network/ca-certs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nCA certificates\nUse CA certificates with Docker\nCopy as Markdown\nCaution\n\nBest practices should be followed when using Man-in-the-Middle (MITM) CA certificates in production containers. If compromised, attackers could intercept sensitive data, spoof a trusted service, or perform man-in-the-middle attacks. Consult your security team before you proceed.\n\nIf your company uses a proxy that inspects HTTPS traffic, you might need to add the required root certificates to your host machine and your Docker containers or images. This is because Docker and its containers, when pulling images or making network requests, need to trust the proxy‚Äôs certificates.\n\nOn the host, adding the root certificate ensures that any Docker commands (like docker pull) work without issues. For containers, you'll need to add the root certificate to the container's trust store either during the build process or at runtime. This ensures that applications running inside the containers can communicate through the proxy without encountering security warnings or connection failures.\n\nAdd CA certificate to the host\n\nThe following sections describe how to install CA certificates on your macOS or Windows host. For Linux, refer to the documentation for your distribution.\n\nmacOS\nDownload the CA certificate for your MITM proxy software.\nOpen the Keychain Access app.\nIn Keychain Access, select System, then switch to the Certificates tab.\nDrag-and-drop the downloaded certificate into the list of certificates. Enter your password if prompted.\nFind the newly added certificate, double-click it, and expand the Trust section.\nSet Always Trust for the certificate. Enter your password if prompted.\nStart Docker Desktop and verify that docker pull works, assuming Docker Desktop is configured to use the MITM proxy.\nWindows\n\nChoose whether you want to install the certificate using the Microsoft Management Console (MMC) or your web browser.\n\nMMC Web browser\nDownload CA certificate for the MITM proxy software.\nOpen the Microsoft Management Console (mmc.exe).\nAdd the Certificates Snap-In in the MMC.\nSelect File ‚Üí Add/Remove Snap-in, and then select Certificates ‚Üí Add >.\nSelect Computer Account and then Next.\nSelect Local computer and then select Finish.\nImport the CA certificate:\nFrom the MMC, expand Certificates (Local Computer).\nExpand the Trusted Root Certification Authorities section.\nRight-click Certificates and select All Tasks and Import‚Ä¶.\nFollow the prompts to import your CA certificate.\nSelect Finish and then Close.\nStart Docker Desktop and verify that docker pull succeeds (assuming Docker Desktop is already configured to use the MITM proxy server).\nNote\n\nDepending on the SDK and/or runtime/framework in use, further steps may be required beyond adding the CA certificate to the operating system's trust store.\n\nAdd CA certificates to Linux images and containers\n\nIf you need to run containerized workloads that rely on internal or custom certificates, such as in environments with corporate proxies or secure services, you must ensure that the containers trust these certificates. Without adding the necessary CA certificates, applications inside your containers may encounter failed requests or security warnings when attempting to connect to HTTPS endpoints.\n\nBy adding CA certificates to images at build time, you ensure that any containers started from the image will trust the specified certificates. This is particularly important for applications that require seamless access to internal APIs, databases, or other services during production.\n\nIn cases where rebuilding the image isn't feasible, you can instead add certificates to containers directly. However, certificates added at runtime won‚Äôt persist if the container is destroyed or recreated, so this method is typically used for temporary fixes or testing scenarios.\n\nAdd certificates to images\nNote\n\nThe following commands are for an Ubuntu base image. If your build uses a different Linux distribution, use equivalent commands for package management (apt-get, update-ca-certificates, and so on).\n\nTo add ca certificate to a container image when you're building it, add the following instructions to your Dockerfile.\n\n# Install the ca-certificate package\n\nRUN apt-get update && apt-get install -y ca-certificates\n\n# Copy the CA certificate from the context to the build container\n\nCOPY your_certificate.crt /usr/local/share/ca-certificates/\n\n# Update the CA certificates in the container\n\nRUN update-ca-certificates\nAdd certificates to containers\nNote\n\nThe following commands are for an Ubuntu-based container. If your container uses a different Linux distribution, use equivalent commands for package management (apt-get, update-ca-certificates, and so on).\n\nTo add a CA certificate to a running Linux container:\n\nDownload the CA certificate for your MITM proxy software.\n\nIf the certificate is in a format other than .crt, convert it to .crt format:\n\nExample command\n$ openssl x509 -in cacert.der -inform DER -out myca.crt\n\n\nCopy the certificate into the running container:\n\n$ docker cp myca.crt <containerid>:/tmp\n\n\nAttach to the container:\n\n$ docker exec -it <containerid> sh\n\n\nEnsure the ca-certificates package is installed (required for updating certificates):\n\n# apt-get update && apt-get install -y ca-certificates\n\n\nCopy the certificate to the correct location for CA certificates:\n\n# cp /tmp/myca.crt /usr/local/share/ca-certificates/root_cert.crt\n\n\nUpdate the CA certificates:\n\n# update-ca-certificates\n\nExample output\nUpdating certificates in /etc/ssl/certs...\n\nrehash: warning: skipping ca-certificates.crt, it does not contain exactly one certificate or CRL\n\n1 added, 0 removed; done.\n\nVerify that the container can communicate via the MITM proxy:\n\n# curl https://example.com\n\nExample output\n<!doctype html>\n\n<html>\n\n<head>\n\n    <title>Example Domain</title>\n\n...\n\nEdit this page\n\nRequest changes\n\nTable of contents\nAdd CA certificate to the host\nmacOS\nWindows\nAdd CA certificates to Linux images and containers\nAdd certificates to images\nAdd certificates to containers\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995228,
    "timestamp": "2026-02-07T06:32:16.329Z",
    "title": "Overlay network driver | Docker Docs",
    "url": "https://docs.docker.com/engine/network/drivers/overlay/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nBridge network driver\nHost network driver\nIPvlan network driver\nMacvlan network driver\nNone network driver\nOverlay network driver\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nNetwork drivers\n/\nOverlay network driver\nOverlay network driver\nCopy as Markdown\n\nThe overlay network driver creates a distributed network among multiple Docker daemon hosts. This network sits on top of (overlays) the host-specific networks, allowing containers connected to it to communicate securely when encryption is enabled. Docker transparently handles routing of each packet to and from the correct Docker daemon host and the correct destination container.\n\nYou can create user-defined overlay networks using docker network create, in the same way that you can create user-defined bridge networks. Services or containers can be connected to more than one network at a time. Services or containers can only communicate across networks they're each connected to.\n\nOverlay networks are often used to create a connection between Swarm services, but you can also use it to connect standalone containers running on different hosts. When using standalone containers, it's still required that you use Swarm mode to establish a connection between the hosts.\n\nThis page describes overlay networks in general, and when used with standalone containers. For information about overlay for Swarm services, see Manage Swarm service networks.\n\nRequirements\n\nDocker hosts must be part of a swarm to use overlay networks, even when connecting standalone containers. The following ports must be open between participating hosts:\n\n2377/tcp: Swarm control plane (configurable)\n4789/udp: Overlay traffic (configurable)\n7946/tcp and 7946/udp: Node communication (not configurable)\nCreate an overlay network\n\nThe following table lists the ports that need to be open to each host participating in an overlay network:\n\nPorts\tDescription\n2377/tcp\tThe default Swarm control plane port, is configurable with docker swarm join --listen-addr\n4789/udp\tThe default overlay traffic port, configurable with docker swarm init --data-path-addr\n7946/tcp, 7946/udp\tUsed for communication among nodes, not configurable\n\nTo create an overlay network that containers on other Docker hosts can connect to, run the following command:\n\n$ docker network create -d overlay --attachable my-attachable-overlay\n\n\nThe --attachable option enables both standalone containers and Swarm services to connect to the overlay network. Without --attachable, only Swarm services can connect to the network.\n\nYou can specify the IP address range, subnet, gateway, and other options. See docker network create --help for details.\n\nEncrypt traffic on an overlay network\n\nUse the --opt encrypted flag to encrypt the application data transmitted over the overlay network:\n\n$ docker network create \\\n\n  --opt encrypted \\\n\n  --driver overlay \\\n\n  --attachable \\\n\n  my-attachable-multi-host-network\n\n\nThis enables IPsec encryption at the level of the Virtual Extensible LAN (VXLAN). This encryption imposes a non-negligible performance penalty, so you should test this option before using it in production.\n\nWarning\n\nDon't attach Windows containers to encrypted overlay networks.\n\nOverlay network encryption isn't supported on Windows. Swarm doesn't report an error when a Windows host attempts to connect to an encrypted overlay network, but networking for the Windows containers is affected as follows:\n\nWindows containers can't communicate with Linux containers on the network\nData traffic between Windows containers on the network isn't encrypted\nAttach a container to an overlay network\n\nAdding containers to an overlay network gives them the ability to communicate with other containers without having to set up routing on the individual Docker daemon hosts. A prerequisite for doing this is that the hosts have joined the same Swarm.\n\nTo join an overlay network named multi-host-network with a busybox container:\n\n$ docker run --network multi-host-network busybox sh\n\nNote\n\nThis only works if the overlay network is attachable (created with the --attachable flag).\n\nContainer discovery\n\nPublishing ports of a container on an overlay network opens the ports to other containers on the same network. Containers are discoverable by doing a DNS lookup using the container name.\n\nFlag value\tDescription\n-p 8080:80\tMap TCP port 80 in the container to port 8080 on the overlay network.\n-p 8080:80/udp\tMap UDP port 80 in the container to port 8080 on the overlay network.\n-p 8080:80/sctp\tMap SCTP port 80 in the container to port 8080 on the overlay network.\n-p 8080:80/tcp -p 8080:80/udp\tMap TCP port 80 in the container to TCP port 8080 on the overlay network, and map UDP port 80 in the container to UDP port 8080 on the overlay network.\nConnection limit for overlay networks\n\nDue to limitations set by the Linux kernel, overlay networks become unstable and inter-container communications may break when 1000 containers are co-located on the same host.\n\nFor more information about this limitation, see moby/moby#44973.\n\nUsage examples\n\nThis section provides hands-on examples for working with overlay networks. These examples cover swarm services and standalone containers on multiple Docker hosts.\n\nPrerequisites\n\nAll examples require at least a single-node swarm. Initialize one by running docker swarm init on the host. You can run these examples on multi-node swarms as well.\n\nUse the default overlay network\n\nThis example shows how the default overlay network works with swarm services. You'll create an nginx service and examine the network from the service containers' perspective.\n\nPrerequisites for multi-node setup\n\nThis walkthrough requires three Docker hosts that can communicate with each other on the same network with no firewall blocking traffic between them:\n\nmanager: Functions as both manager and worker\nworker-1: Functions as worker only\nworker-2: Functions as worker only\n\nIf you don't have three hosts available, you can set up three virtual machines on a cloud provider with Docker installed.\n\nCreate the swarm\n\nOn manager, initialize the swarm. If the host has one network interface, the --advertise-addr flag is optional:\n\n$ docker swarm init --advertise-addr=<IP-ADDRESS-OF-MANAGER>\n\n\nSave the join token displayed for use with workers.\n\nOn worker-1, join the swarm:\n\n$ docker swarm join --token TOKEN \\\n\n  --advertise-addr <IP-ADDRESS-OF-WORKER-1> \\\n\n  <IP-ADDRESS-OF-MANAGER>:2377\n\n\nOn worker-2, join the swarm:\n\n$ docker swarm join --token TOKEN \\\n\n  --advertise-addr <IP-ADDRESS-OF-WORKER-2> \\\n\n  <IP-ADDRESS-OF-MANAGER>:2377\n\n\nOn manager, list all nodes:\n\n$ docker node ls\n\n\n\nID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS\n\nd68ace5iraw6whp7llvgjpu48 *   ip-172-31-34-146    Ready               Active              Leader\n\nnvp5rwavvb8lhdggo8fcf7plg     ip-172-31-35-151    Ready               Active\n\nouvx2l7qfcxisoyms8mtkgahw     ip-172-31-36-89     Ready               Active\n\n\nFilter by role if needed:\n\n$ docker node ls --filter role=manager\n\n$ docker node ls --filter role=worker\n\n\nList Docker networks on all hosts. Each now has an overlay network called ingress and a bridge network called docker_gwbridge:\n\n$ docker network ls\n\n\n\nNETWORK ID          NAME                DRIVER              SCOPE\n\n495c570066be        bridge              bridge              local\n\n961c6cae9945        docker_gwbridge     bridge              local\n\nff35ceda3643        host                host                local\n\ntrtnl4tqnc3n        ingress             overlay             swarm\n\nc8357deec9cb        none                null                local\n\n\nThe docker_gwbridge connects the ingress network to the Docker host's network interface. If you create services without specifying a network, they connect to ingress. It's recommended to use separate overlay networks for each application or group of related applications.\n\nCreate the services\n\nOn manager, create a new overlay network:\n\n$ docker network create -d overlay nginx-net\n\n\nThe overlay network is automatically created on worker nodes when they run service tasks that need it.\n\nOn manager, create a 5-replica Nginx service connected to nginx-net:\n\nNote\n\nServices can only be created on a manager.\n\n$ docker service create \\\n\n  --name my-nginx \\\n\n  --publish target=80,published=80 \\\n\n  --replicas=5 \\\n\n  --network nginx-net \\\n\n  nginx\n\n\nThe default ingress publish mode means you can browse to port 80 on any node and connect to one of the 5 service tasks, even if no tasks run on that node.\n\nMonitor service creation progress:\n\n$ docker service ls\n\n\nInspect the nginx-net network on all hosts. The Containers section lists all service tasks connected to the overlay network from that host.\n\nFrom manager, inspect the service:\n\n$ docker service inspect my-nginx\n\n\nNotice the information about ports and endpoints.\n\nCreate a second network and update the service to use it:\n\n$ docker network create -d overlay nginx-net-2\n\n$ docker service update \\\n\n  --network-add nginx-net-2 \\\n\n  --network-rm nginx-net \\\n\n  my-nginx\n\n\nVerify the update completed:\n\n$ docker service ls\n\n\nInspect both networks to verify containers moved from nginx-net to nginx-net-2.\n\nNote\n\nOverlay networks are automatically created on swarm worker nodes as needed, but aren't automatically removed.\n\nClean up:\n\n$ docker service rm my-nginx\n\n$ docker network rm nginx-net nginx-net-2\n\nUse a user-defined overlay network\n\nThis example shows the recommended approach for production services using custom overlay networks.\n\nPrerequisites\n\nThis assumes the swarm is already set up and you're on a manager node.\n\nSteps\n\nCreate a user-defined overlay network:\n\n$ docker network create -d overlay my-overlay\n\n\nStart a service using the overlay network, publishing port 80 to port 8080:\n\n$ docker service create \\\n\n  --name my-nginx \\\n\n  --network my-overlay \\\n\n  --replicas 1 \\\n\n  --publish published=8080,target=80 \\\n\n  nginx:latest\n\n\nVerify the service task is connected to the network:\n\n$ docker network inspect my-overlay\n\n\nCheck the Containers section for the my-nginx service task.\n\nClean up:\n\n$ docker service rm my-nginx\n\n$ docker network rm my-overlay\n\nUse an overlay network for standalone containers\n\nThis example demonstrates DNS container discovery between standalone containers on different Docker hosts using an overlay network.\n\nPrerequisites\n\nYou need two Docker hosts that can communicate with each other with the following ports open between them:\n\nTCP port 2377\nTCP and UDP port 7946\nUDP port 4789\n\nThis example refers to the hosts as host1 and host2.\n\nSteps\n\nSet up the swarm:\n\nOn host1, initialize a swarm:\n\n$ docker swarm init\n\nSwarm initialized: current node (vz1mm9am11qcmo979tlrlox42) is now a manager.\n\n\n\nTo add a worker to this swarm, run the following command:\n\n\n\n    docker swarm join --token SWMTKN-1-5g90q48weqrtqryq4kj6ow0e8xm9wmv9o6vgqc5j320ymybd5c-8ex8j0bc40s6hgvy5ui5gl4gy 172.31.47.252:2377\n\n\nOn host2, join the swarm using the token from the previous output:\n\n$ docker swarm join --token <your_token> <your_ip_address>:2377\n\nThis node joined a swarm as a worker.\n\n\nIf the join fails, run docker swarm leave --force on host2, verify network and firewall settings, and try again.\n\nOn host1, create an attachable overlay network:\n\n$ docker network create --driver=overlay --attachable test-net\n\nuqsof8phj3ak0rq9k86zta6ht\n\n\nNote the returned network ID.\n\nOn host1, start an interactive container that connects to test-net:\n\n$ docker run -it --name alpine1 --network test-net alpine\n\n/ #\n\n\nOn host2, list available networks. Notice that test-net doesn't exist yet:\n\n$ docker network ls\n\nNETWORK ID          NAME                DRIVER              SCOPE\n\nec299350b504        bridge              bridge              local\n\n66e77d0d0e9a        docker_gwbridge     bridge              local\n\n9f6ae26ccb82        host                host                local\n\nomvdxqrda80z        ingress             overlay             swarm\n\nb65c952a4b2b        none                null                local\n\n\nOn host2, start a detached, interactive container that connects to test-net:\n\n$ docker run -dit --name alpine2 --network test-net alpine\n\nfb635f5ece59563e7b8b99556f816d24e6949a5f6a5b1fbd92ca244db17a4342\n\nNote\n\nAutomatic DNS container discovery only works with unique container names.\n\nOn host2, verify that test-net was created with the same network ID as on host1:\n\n$ docker network ls\n\nNETWORK ID          NAME                DRIVER              SCOPE\n\n...\n\nuqsof8phj3ak        test-net            overlay             swarm\n\n\nOn host1, ping alpine2 from within alpine1:\n\n/ # ping -c 2 alpine2\n\nPING alpine2 (10.0.0.5): 56 data bytes\n\n64 bytes from 10.0.0.5: seq=0 ttl=64 time=0.600 ms\n\n64 bytes from 10.0.0.5: seq=1 ttl=64 time=0.555 ms\n\n\n\n--- alpine2 ping statistics ---\n\n2 packets transmitted, 2 packets received, 0% packet loss\n\nround-trip min/avg/max = 0.555/0.577/0.600 ms\n\n\nThe two containers communicate over the overlay network connecting the two hosts. You can also run another container on host2 and ping alpine1:\n\n$ docker run -it --rm --name alpine3 --network test-net alpine\n\n/ # ping -c 2 alpine1\n\n/ # exit\n\n\nOn host1, close the alpine1 session (which stops the container):\n\n/ # exit\n\n\nClean up. You must stop and remove containers on each host independently:\n\nOn host2:\n\n$ docker container stop alpine2\n\n$ docker network ls\n\n$ docker container rm alpine2\n\n\nWhen you stop alpine2, test-net disappears from host2.\n\nOn host1:\n\n$ docker container rm alpine1\n\n$ docker network rm test-net\n\nNext steps\nLearn about networking from the container's point of view\nLearn about standalone bridge networks\nLearn about Macvlan networks\n\nEdit this page\n\nRequest changes\n\nTable of contents\nRequirements\nCreate an overlay network\nEncrypt traffic on an overlay network\nAttach a container to an overlay network\nContainer discovery\nConnection limit for overlay networks\nUsage examples\nPrerequisites\nUse the default overlay network\nUse a user-defined overlay network\nUse an overlay network for standalone containers\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995234,
    "timestamp": "2026-02-07T06:32:16.343Z",
    "title": "Legacy container links | Docker Docs",
    "url": "https://docs.docker.com/engine/network/links/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nDocker with iptables\nDocker with nftables\nPacket filtering and firewalls\nPort publishing and mapping\nNetwork drivers\nCA certificates\nLegacy container links\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nNetworking\n/\nLegacy container links\nLegacy container links\nCopy as Markdown\nWarning\n\nThe --link flag is a legacy feature of Docker. It may eventually be removed. Unless you absolutely need to continue using it, we recommend that you use user-defined networks to facilitate communication between two containers instead of using --link. One feature that user-defined networks do not support that you can do with --link is sharing environment variables between containers. However, you can use other mechanisms such as volumes to share environment variables between containers in a more controlled way.\n\nSee Differences between user-defined bridges and the default bridge for some alternatives to using --link.\n\nThe information in this section explains legacy container links within the Docker default bridge network which is created automatically when you install Docker.\n\nBefore the Docker networks feature, you could use the Docker link feature to allow containers to discover each other and securely transfer information about one container to another container. With the introduction of the Docker networks feature, you can still create links but they behave differently between default bridge network and user defined networks.\n\nThis section briefly discusses connecting via a network port and then goes into detail on container linking in default bridge network.\n\nConnect using network port mapping\n\nLet's say you used this command to run a simple Python Flask application:\n\n$ docker run -d -P training/webapp python app.py\n\n\nWhen that container was created, the -P flag was used to automatically map any network port inside it to a random high port within an ephemeral port range on your Docker host. Next, when docker ps was run, you saw that port 5000 in the container was bound to port 49155 on the host.\n\n$ docker ps nostalgic_morse\n\n\n\nCONTAINER ID  IMAGE                   COMMAND       CREATED        STATUS        PORTS                    NAMES\n\nbc533791f3f5  training/webapp:latest  python app.py 5 seconds ago  Up 2 seconds  0.0.0.0:49155->5000/tcp  nostalgic_morse\n\n\nYou also saw how you can bind a container's ports to a specific port using the -p flag. Here port 80 of the host is mapped to port 5000 of the container:\n\n$ docker run -d -p 80:5000 training/webapp python app.py\n\n\nAnd you saw why this isn't such a great idea because it constrains you to only one container on that specific port.\n\nInstead, you may specify a range of host ports to bind a container port to that is different than the default ephemeral port range:\n\n$ docker run -d -p 8000-9000:5000 training/webapp python app.py\n\n\nThis would bind port 5000 in the container to a randomly available port between 8000 and 9000 on the host.\n\nThere are also a few other ways you can configure the -p flag. By default the -p flag binds the specified port to all interfaces on the host machine. But you can also specify a binding to a specific interface, for example only to the localhost.\n\n$ docker run -d -p 127.0.0.1:80:5000 training/webapp python app.py\n\n\nThis would bind port 5000 inside the container to port 80 on the localhost or 127.0.0.1 interface on the host machine.\n\nOr, to bind port 5000 of the container to a dynamic port but only on the localhost, you could use:\n\n$ docker run -d -p 127.0.0.1::5000 training/webapp python app.py\n\n\nYou can also bind UDP and SCTP (typically used by telecom protocols such as SIGTRAN, Diameter, and S1AP/X2AP) ports by adding a trailing /udp or /sctp. For example:\n\n$ docker run -d -p 127.0.0.1:80:5000/udp training/webapp python app.py\n\n\nYou also learned about the useful docker port shortcut which showed us the current port bindings. This is also useful for showing you specific port configurations. For example, if you've bound the container port to the localhost on the host machine, then the docker port output reflects that.\n\n$ docker port nostalgic_morse 5000\n\n\n\n127.0.0.1:49155\n\nNote\n\nThe -p flag can be used multiple times to configure multiple ports.\n\nConnect with the linking system\nNote\n\nThis section covers the legacy link feature in the default bridge network. Refer to differences between user-defined bridges and the default bridge for more information on links in user-defined networks.\n\nNetwork port mappings are not the only way Docker containers can connect to one another. Docker also has a linking system that allows you to link multiple containers together and send connection information from one to another. When containers are linked, information about a source container can be sent to a recipient container. This allows the recipient to see selected data describing aspects of the source container.\n\nThe importance of naming\n\nTo establish links, Docker relies on the names of your containers. You've already seen that each container you create has an automatically created name; indeed you've become familiar with our old friend nostalgic_morse during this guide. You can also name containers yourself. This naming provides two useful functions:\n\nIt can be useful to name containers that do specific functions in a way that makes it easier for you to remember them, for example naming a container containing a web application web.\n\nIt provides Docker with a reference point that allows it to refer to other containers, for example, you can specify to link the container web to container db.\n\nYou can name your container by using the --name flag, for example:\n\n$ docker run -d -P --name web training/webapp python app.py\n\n\nThis launches a new container and uses the --name flag to name the container web. You can see the container's name using the docker ps command.\n\n$ docker ps -l\n\n\n\nCONTAINER ID  IMAGE                  COMMAND        CREATED       STATUS       PORTS                    NAMES\n\naed84ee21bde  training/webapp:latest python app.py  12 hours ago  Up 2 seconds 0.0.0.0:49154->5000/tcp  web\n\n\nYou can also use docker inspect to return the container's name.\n\nNote\n\nContainer names must be unique. That means you can only call one container web. If you want to re-use a container name you must delete the old container (with docker container rm) before you can create a new container with the same name. As an alternative you can use the --rm flag with the docker run command. This deletes the container immediately after it is stopped.\n\nCommunication across links\n\nLinks allow containers to discover each other and securely transfer information about one container to another container. When you set up a link, you create a conduit between a source container and a recipient container. The recipient can then access select data about the source. To create a link, you use the --link flag. First, create a new container, this time one containing a database.\n\n$ docker run -d --name db training/postgres\n\n\nThis creates a new container called db from the training/postgres image, which contains a PostgreSQL database.\n\nNow, you need to delete the web container you created previously so you can replace it with a linked one:\n\n$ docker container rm -f web\n\n\nNow, create a new web container and link it with your db container.\n\n$ docker run -d -P --name web --link db:db training/webapp python app.py\n\n\nThis links the new web container with the db container you created earlier. The --link flag takes the form:\n\n--link <name or id>:alias\n\n\nWhere name is the name of the container we're linking to and alias is an alias for the link name. That alias is used shortly. The --link flag also takes the form:\n\n--link <name or id>\n\n\nIn this case the alias matches the name. You could write the previous example as:\n\n$ docker run -d -P --name web --link db training/webapp python app.py\n\n\nNext, inspect your linked containers with docker inspect:\n\n$ docker inspect -f \"{{ .HostConfig.Links }}\" web\n\n\n\n[/db:/web/db]\n\n\nYou can see that the web container is now linked to the db container web/db. Which allows it to access information about the db container.\n\nSo what does linking the containers actually do? You've learned that a link allows a source container to provide information about itself to a recipient container. In our example, the recipient, web, can access information about the source db. To do this, Docker creates a secure tunnel between the containers that doesn't need to expose any ports externally on the container; when we started the db container we did not use either the -P or -p flags. That's a big benefit of linking: we don't need to expose the source container, here the PostgreSQL database, to the network.\n\nDocker exposes connectivity information for the source container to the recipient container in two ways:\n\nEnvironment variables,\nUpdating the /etc/hosts file.\nEnvironment variables\n\nDocker creates several environment variables when you link containers. Docker automatically creates environment variables in the target container based on the --link parameters. It also exposes all environment variables originating from Docker from the source container. These include variables from:\n\nthe ENV commands in the source container's Dockerfile\nthe -e, --env, and --env-file options on the docker run command when the source container is started\n\nThese environment variables enable programmatic discovery from within the target container of information related to the source container.\n\nWarning\n\nIt is important to understand that all environment variables originating from Docker within a container are made available to any container that links to it. This could have serious security implications if sensitive data is stored in them.\n\nDocker sets an <alias>_NAME environment variable for each target container listed in the --link parameter. For example, if a new container called web is linked to a database container called db via --link db:webdb, then Docker creates a WEBDB_NAME=/web/webdb variable in the web container.\n\nDocker also defines a set of environment variables for each port exposed by the source container. Each variable has a unique prefix in the form <name>_PORT_<port>_<protocol>\n\nThe components in this prefix are:\n\nthe alias <name> specified in the --link parameter (for example, webdb)\nthe <port> number exposed\na <protocol> which is either TCP or UDP\n\nDocker uses this prefix format to define three distinct environment variables:\n\nThe prefix_ADDR variable contains the IP Address from the URL, for example WEBDB_PORT_5432_TCP_ADDR=172.17.0.82.\nThe prefix_PORT variable contains just the port number from the URL of example WEBDB_PORT_5432_TCP_PORT=5432.\nThe prefix_PROTO variable contains just the protocol from the URL of example WEBDB_PORT_5432_TCP_PROTO=tcp.\n\nIf the container exposes multiple ports, an environment variable set is defined for each one. This means, for example, if a container exposes 4 ports that Docker creates 12 environment variables, 3 for each port.\n\nAdditionally, Docker creates an environment variable called <alias>_PORT. This variable contains the URL of the source container's first exposed port. The 'first' port is defined as the exposed port with the lowest number. For example, consider the WEBDB_PORT=tcp://172.17.0.82:5432 variable. If that port is used for both tcp and udp, then the tcp one is specified.\n\nFinally, Docker also exposes each Docker originated environment variable from the source container as an environment variable in the target. For each variable Docker creates an <alias>_ENV_<name> variable in the target container. The variable's value is set to the value Docker used when it started the source container.\n\nReturning back to our database example, you can run the env command to list the specified container's environment variables.\n\n$ docker run --rm --name web2 --link db:db training/webapp env\n\n\n\n<...>\n\nDB_NAME=/web2/db\n\nDB_PORT=tcp://172.17.0.5:5432\n\nDB_PORT_5432_TCP=tcp://172.17.0.5:5432\n\nDB_PORT_5432_TCP_PROTO=tcp\n\nDB_PORT_5432_TCP_PORT=5432\n\nDB_PORT_5432_TCP_ADDR=172.17.0.5\n\n<...>\n\n\nYou can see that Docker has created a series of environment variables with useful information about the source db container. Each variable is prefixed with DB_, which is populated from the alias you specified above. If the alias were db1, the variables would be prefixed with DB1_. You can use these environment variables to configure your applications to connect to the database on the db container. The connection is secure and private; only the linked web container can communicate with the db container.\n\nImportant notes on Docker environment variables\n\nUnlike host entries in the /etc/hosts file, IP addresses stored in the environment variables are not automatically updated if the source container is restarted. We recommend using the host entries in /etc/hosts to resolve the IP address of linked containers.\n\nThese environment variables are only set for the first process in the container. Some daemons, such as sshd, scrub them when spawning shells for connection.\n\nUpdating the /etc/hosts file\n\nIn addition to the environment variables, Docker adds a host entry for the source container to the /etc/hosts file. Here's an entry for the web container:\n\n$ docker run -t -i --rm --link db:webdb training/webapp /bin/bash\n\n\n\nroot@aed84ee21bde:/opt/webapp# cat /etc/hosts\n\n172.17.0.7  aed84ee21bde\n\n<...>\n\n172.17.0.5  webdb 6e5cdeb2d300 db\n\n\nYou can see two relevant host entries. The first is an entry for the web container that uses the Container ID as a host name. The second entry uses the link alias to reference the IP address of the db container. In addition to the alias you provide, the linked container's name, if unique from the alias provided to the --link parameter, and the linked container's hostname are also added to /etc/hosts for the linked container's IP address. You can ping that host via any of these entries:\n\nroot@aed84ee21bde:/opt/webapp# apt-get install -yqq inetutils-ping\n\nroot@aed84ee21bde:/opt/webapp# ping webdb\n\n\n\nPING webdb (172.17.0.5): 48 data bytes\n\n56 bytes from 172.17.0.5: icmp_seq=0 ttl=64 time=0.267 ms\n\n56 bytes from 172.17.0.5: icmp_seq=1 ttl=64 time=0.250 ms\n\n56 bytes from 172.17.0.5: icmp_seq=2 ttl=64 time=0.256 ms\n\nNote\n\nIn the example, you had to install ping because it was not included in the container initially.\n\nHere, you used the ping command to ping the db container using its host entry, which resolves to 172.17.0.5. You can use this host entry to configure an application to make use of your db container.\n\nNote\n\nYou can link multiple recipient containers to a single source. For example, you could have multiple (differently named) web containers attached to your db container.\n\nIf you restart the source container, the /etc/hosts files on the linked containers are automatically updated with the source container's new IP address, allowing linked communication to continue.\n\n$ docker restart db\n\ndb\n\n\n\n$ docker run -t -i --rm --link db:db training/webapp /bin/bash\n\n\n\nroot@aed84ee21bde:/opt/webapp# cat /etc/hosts\n\n172.17.0.7  aed84ee21bde\n\n<...>\n\n172.17.0.9  db\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nConnect using network port mapping\nConnect with the linking system\nThe importance of naming\nCommunication across links\nEnvironment variables\nImportant notes on Docker environment variables\nUpdating the /etc/hosts file\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995237,
    "timestamp": "2026-02-07T06:32:16.344Z",
    "title": "Start containers automatically | Docker Docs",
    "url": "https://docs.docker.com/engine/containers/start-containers-automatically/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nStart containers automatically\nRun multiple processes in a container\nResource constraints\nRuntime metrics\nRunning containers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nContainers\n/\nStart containers automatically\nStart containers automatically\nCopy as Markdown\n\nDocker provides restart policies to control whether your containers start automatically when they exit, or when Docker restarts. Restart policies start linked containers in the correct order. Docker recommends that you use restart policies, and avoid using process managers to start containers.\n\nRestart policies are different from the --live-restore flag of the dockerd command. Using --live-restore lets you to keep your containers running during a Docker upgrade, though networking and user input are interrupted.\n\nUse a restart policy\n\nTo configure the restart policy for a container, use the --restart flag when using the docker run command. The value of the --restart flag can be any of the following:\n\nFlag\tDescription\nno\tDon't automatically restart the container. (Default)\non-failure[:max-retries]\tRestart the container if it exits due to an error, which manifests as a non-zero exit code. Optionally, limit the number of times the Docker daemon attempts to restart the container using the :max-retries option. The on-failure policy only prompts a restart if the container exits with a failure. It doesn't restart the container if the daemon restarts.\nalways\tAlways restart the container if it stops. If it's manually stopped, it's restarted only when Docker daemon restarts or the container itself is manually restarted. (See the second bullet listed in restart policy details)\nunless-stopped\tSimilar to always, except that when the container is stopped (manually or otherwise), it isn't restarted even after Docker daemon restarts.\n\nThe following command starts a Redis container and configures it to always restart, unless the container is explicitly stopped, or the daemon restarts.\n\n$ docker run -d --restart unless-stopped redis\n\n\nThe following command changes the restart policy for an already running container named redis.\n\n$ docker update --restart unless-stopped redis\n\n\nThe following command ensures all running containers restart.\n\n$ docker update --restart unless-stopped $(docker ps -q)\n\nRestart policy details\n\nKeep the following in mind when using restart policies:\n\nA restart policy only takes effect after a container starts successfully. In this case, starting successfully means that the container is up for at least 10 seconds and Docker has started monitoring it. This prevents a container which doesn't start at all from going into a restart loop.\n\nIf you manually stop a container, the restart policy is ignored until the Docker daemon restarts or the container is manually restarted. This prevents a restart loop.\n\nRestart policies only apply to containers. To configure restart policies for Swarm services, see flags related to service restart.\n\nRestarting foreground containers\n\nWhen you run a container in the foreground, stopping a container causes the attached CLI to exit as well, regardless of the restart policy of the container. This behavior is illustrated in the following example.\n\nCreate a Dockerfile that prints the numbers 1 to 5 and then exits.\n\nFROM busybox:latest\n\nCOPY --chmod=755 <<\"EOF\" /start.sh\n\necho \"Starting...\"\n\nfor i in $(seq 1 5); do\n\n  echo \"$i\"\n\n  sleep 1\n\ndone\n\necho \"Exiting...\"\n\nexit 1\n\nEOF\n\nENTRYPOINT /start.sh\n\nBuild an image from the Dockerfile.\n\n$ docker build -t startstop .\n\n\nRun a container from the image, specifying always for its restart policy.\n\nThe container prints the numbers 1..5 to stdout, and then exits. This causes the attached CLI to exit as well.\n\n$ docker run --restart always startstop\n\nStarting...\n\n1\n\n2\n\n3\n\n4\n\n5\n\nExiting...\n\n$\n\n\nRunning docker ps shows that is still running or restarting, thanks to the restart policy. The CLI session has already exited, however. It doesn't survive the initial container exit.\n\n$ docker ps\n\nCONTAINER ID   IMAGE       COMMAND                  CREATED         STATUS         PORTS     NAMES\n\n081991b35afe   startstop   \"/bin/sh -c /start.sh\"   9 seconds ago   Up 4 seconds             gallant_easley\n\n\nYou can re-attach your terminal to the container between restarts, using the docker container attach command. It's detached again the next time the container exits.\n\n$ docker container attach 081991b35afe\n\n4\n\n5\n\nExiting...\n\n$\n\nUse a process manager\n\nIf restart policies don't suit your needs, such as when processes outside Docker depend on Docker containers, you can use a process manager such as systemd or supervisor instead.\n\nWarning\n\nDon't combine Docker restart policies with host-level process managers, as this creates conflicts.\n\nTo use a process manager, configure it to start your container or service using the same docker start or docker service command you would normally use to start the container manually. Consult the documentation for the specific process manager for more details.\n\nUsing a process manager inside containers\n\nProcess managers can also run within the container to check whether a process is running and starts/restart it if not.\n\nWarning\n\nThese aren't Docker-aware, and only monitor operating system processes within the container. Docker doesn't recommend this approach, because it's platform-dependent and may differ between versions of a given Linux distribution.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUse a restart policy\nRestart policy details\nRestarting foreground containers\nUse a process manager\nUsing a process manager inside containers\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995240,
    "timestamp": "2026-02-07T06:32:16.344Z",
    "title": "Run multiple processes in a container | Docker Docs",
    "url": "https://docs.docker.com/engine/containers/multi-service_container/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nStart containers automatically\nRun multiple processes in a container\nResource constraints\nRuntime metrics\nRunning containers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nContainers\n/\nRun multiple processes in a container\nRun multiple processes in a container\nCopy as Markdown\n\nA container's main running process is the ENTRYPOINT and/or CMD at the end of the Dockerfile. It's best practice to separate areas of concern by using one service per container. That service may fork into multiple processes (for example, Apache web server starts multiple worker processes). It's ok to have multiple processes, but to get the most benefit out of Docker, avoid one container being responsible for multiple aspects of your overall application. You can connect multiple containers using user-defined networks and shared volumes.\n\nThe container's main process is responsible for managing all processes that it starts. In some cases, the main process isn't well-designed, and doesn't handle \"reaping\" (stopping) child processes gracefully when the container exits. If your process falls into this category, you can use the --init option when you run the container. The --init flag inserts a tiny init-process into the container as the main process, and handles reaping of all processes when the container exits. Handling such processes this way is superior to using a full-fledged init process such as sysvinit or systemd to handle process lifecycle within your container.\n\nIf you need to run more than one service within a container, you can achieve this in a few different ways.\n\nUse a wrapper script\n\nPut all of your commands in a wrapper script, complete with testing and debugging information. Run the wrapper script as your CMD. The following is a naive example. First, the wrapper script:\n\n#!/bin/bash\n\n\n\n# Start the first process\n\n./my_first_process &\n\n\n\n# Start the second process\n\n./my_second_process &\n\n\n\n# Wait for any process to exit\n\nwait -n\n\n\n\n# Exit with status of process that exited first\n\nexit $?\n\nNext, the Dockerfile:\n\n# syntax=docker/dockerfile:1\n\nFROM ubuntu:latest\n\nCOPY my_first_process my_first_process\n\nCOPY my_second_process my_second_process\n\nCOPY my_wrapper_script.sh my_wrapper_script.sh\n\nCMD ./my_wrapper_script.sh\nUse Bash job controls\n\nIf you have one main process that needs to start first and stay running but you temporarily need to run some other processes (perhaps to interact with the main process) then you can use bash's job control. First, the wrapper script:\n\n#!/bin/bash\n\n\n\n# turn on bash's job control\n\nset -m\n\n\n\n# Start the primary process and put it in the background\n\n./my_main_process &\n\n\n\n# Start the helper process\n\n./my_helper_process\n\n\n\n# the my_helper_process might need to know how to wait on the\n\n# primary process to start before it does its work and returns\n\n\n\n\n\n# now we bring the primary process back into the foreground\n\n# and leave it there\n\nfg %1\n# syntax=docker/dockerfile:1\n\nFROM ubuntu:latest\n\nCOPY my_main_process my_main_process\n\nCOPY my_helper_process my_helper_process\n\nCOPY my_wrapper_script.sh my_wrapper_script.sh\n\nCMD ./my_wrapper_script.sh\nUse a process manager\n\nUse a process manager like supervisord. This is more involved than the other options, as it requires you to bundle supervisord and its configuration into your image (or base your image on one that includes supervisord), along with the different applications it manages. Then you start supervisord, which manages your processes for you.\n\nThe following Dockerfile example shows this approach. The example assumes that these files exist at the root of the build context:\n\nsupervisord.conf\nmy_first_process\nmy_second_process\n# syntax=docker/dockerfile:1\n\nFROM ubuntu:latest\n\nRUN apt-get update && apt-get install -y supervisor\n\nRUN mkdir -p /var/log/supervisor\n\nCOPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf\n\nCOPY my_first_process my_first_process\n\nCOPY my_second_process my_second_process\n\nCMD [\"/usr/bin/supervisord\"]\n\nIf you want to make sure both processes output their stdout and stderr to the container logs, you can add the following to the supervisord.conf file:\n\n[supervisord]\n\nnodaemon=true\n\nlogfile=/dev/null\n\nlogfile_maxbytes=0\n\n\n\n[program:app]\n\nstdout_logfile=/dev/fd/1\n\nstdout_logfile_maxbytes=0\n\nredirect_stderr=true\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUse a wrapper script\nUse Bash job controls\nUse a process manager\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995243,
    "timestamp": "2026-02-07T06:32:16.352Z",
    "title": "Resource constraints | Docker Docs",
    "url": "https://docs.docker.com/engine/containers/resource_constraints/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nStart containers automatically\nRun multiple processes in a container\nResource constraints\nRuntime metrics\nRunning containers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nContainers\n/\nResource constraints\nResource constraints\nCopy as Markdown\n\nBy default, a container has no resource constraints and can use as much of a given resource as the host's kernel scheduler allows. Docker provides ways to control how much memory, or CPU a container can use, setting runtime configuration flags of the docker run command. This section provides details on when you should set such limits and the possible implications of setting them.\n\nMany of these features require your kernel to support Linux capabilities. To check for support, you can use the docker info command. If a capability is disabled in your kernel, you may see a warning at the end of the output like the following:\n\nWARNING: No swap limit support\n\n\nConsult your operating system's documentation for enabling them. See also the Docker Engine troubleshooting guide for more information.\n\nMemory\nUnderstand the risks of running out of memory\n\nIt's important not to allow a running container to consume too much of the host machine's memory. On Linux hosts, if the kernel detects that there isn't enough memory to perform important system functions, it throws an OOME, or Out Of Memory Exception, and starts killing processes to free up memory. Any process is subject to killing, including Docker and other important applications. This can effectively bring the entire system down if the wrong process is killed.\n\nDocker attempts to mitigate these risks by adjusting the OOM priority on the Docker daemon so that it's less likely to be killed than other processes on the system. The OOM priority on containers isn't adjusted. This makes it more likely for an individual container to be killed than for the Docker daemon or other system processes to be killed. You shouldn't try to circumvent these safeguards by manually setting --oom-score-adj to an extreme negative number on the daemon or a container, or by setting --oom-kill-disable on a container.\n\nFor more information about the Linux kernel's OOM management, see Out of Memory Management.\n\nYou can mitigate the risk of system instability due to OOME by:\n\nPerform tests to understand the memory requirements of your application before placing it into production.\nEnsure that your application runs only on hosts with adequate resources.\nLimit the amount of memory your container can use, as described below.\nBe mindful when configuring swap on your Docker hosts. Swap is slower than memory but can provide a buffer against running out of system memory.\nConsider converting your container to a service, and using service-level constraints and node labels to ensure that the application runs only on hosts with enough memory\nLimit a container's access to memory\n\nDocker can enforce hard or soft memory limits.\n\nHard limits let the container use no more than a fixed amount of memory.\nSoft limits let the container use as much memory as it needs unless certain conditions are met, such as when the kernel detects low memory or contention on the host machine.\n\nSome of these options have different effects when used alone or when more than one option is set.\n\nMost of these options take a positive integer, followed by a suffix of b, k, m, g, to indicate bytes, kilobytes, megabytes, or gigabytes.\n\nOption\tDescription\n-m or --memory=\tThe maximum amount of memory the container can use. If you set this option, the minimum allowed value is 6m (6 megabytes). That is, you must set the value to at least 6 megabytes.\n--memory-swap*\tThe amount of memory this container is allowed to swap to disk. See --memory-swap details.\n--memory-swappiness\tBy default, the host kernel can swap out a percentage of anonymous pages used by a container. You can set --memory-swappiness to a value between 0 and 100, to tune this percentage. See --memory-swappiness details.\n--memory-reservation\tAllows you to specify a soft limit smaller than --memory which is activated when Docker detects contention or low memory on the host machine. If you use --memory-reservation, it must be set lower than --memory for it to take precedence. Because it is a soft limit, it doesn't guarantee that the container doesn't exceed the limit.\n--kernel-memory\tThe maximum amount of kernel memory the container can use. The minimum allowed value is 6m. Because kernel memory can't be swapped out, a container which is starved of kernel memory may block host machine resources, which can have side effects on the host machine and on other containers. See --kernel-memory details.\n--oom-kill-disable\tBy default, if an out-of-memory (OOM) error occurs, the kernel kills processes in a container. To change this behavior, use the --oom-kill-disable option. Only disable the OOM killer on containers where you have also set the -m/--memory option. If the -m flag isn't set, the host can run out of memory and the kernel may need to kill the host system's processes to free memory.\n\nFor more information about cgroups and memory in general, see the documentation for Memory Resource Controller.\n\n--memory-swap details\n\n--memory-swap is a modifier flag that only has meaning if --memory is also set. Using swap allows the container to write excess memory requirements to disk when the container has exhausted all the RAM that's available to it. There is a performance penalty for applications that swap memory to disk often.\n\nIts setting can have complicated effects:\n\nIf --memory-swap is set to a positive integer, then both --memory and --memory-swap must be set. --memory-swap represents the total amount of memory and swap that can be used, and --memory controls the amount used by non-swap memory. So if --memory=\"300m\" and --memory-swap=\"1g\", the container can use 300m of memory and 700m (1g - 300m) swap.\n\nIf --memory-swap is set to 0, the setting is ignored, and the value is treated as unset.\n\nIf --memory-swap is set to the same value as --memory, and --memory is set to a positive integer, the container doesn't have access to swap. See Prevent a container from using swap.\n\nIf --memory-swap is unset, and --memory is set, the container can use as much swap as the --memory setting, if the host container has swap memory configured. For instance, if --memory=\"300m\" and --memory-swap is not set, the container can use 600m in total of memory and swap.\n\nIf --memory-swap is explicitly set to -1, the container is allowed to use unlimited swap, up to the amount available on the host system.\n\nInside the container, tools like free report the host's available swap, not what's available inside the container. Don't rely on the output of free or similar tools to determine whether swap is present.\n\nPrevent a container from using swap\n\nIf --memory and --memory-swap are set to the same value, this prevents containers from using any swap. This is because --memory-swap is the amount of combined memory and swap that can be used, while --memory is only the amount of physical memory that can be used.\n\n--memory-swappiness details\nA value of 0 turns off anonymous page swapping.\nA value of 100 sets all anonymous pages as swappable.\nBy default, if you don't set --memory-swappiness, the value is inherited from the host machine.\n--kernel-memory details\n\nKernel memory limits are expressed in terms of the overall memory allocated to a container. Consider the following scenarios:\n\nUnlimited memory, unlimited kernel memory: This is the default behavior.\nUnlimited memory, limited kernel memory: This is appropriate when the amount of memory needed by all cgroups is greater than the amount of memory that actually exists on the host machine. You can configure the kernel memory to never go over what's available on the host machine, and containers which need more memory need to wait for it.\nLimited memory, unlimited kernel memory: The overall memory is limited, but the kernel memory isn't.\nLimited memory, limited kernel memory: Limiting both user and kernel memory can be useful for debugging memory-related problems. If a container is using an unexpected amount of either type of memory, it runs out of memory without affecting other containers or the host machine. Within this setting, if the kernel memory limit is lower than the user memory limit, running out of kernel memory causes the container to experience an OOM error. If the kernel memory limit is higher than the user memory limit, the kernel limit doesn't cause the container to experience an OOM.\n\nWhen you enable kernel memory limits, the host machine tracks the \"high water mark\" statistics on a per-process basis, so you can track which processes (in this case, containers) are using excess memory. This can be seen per process by viewing /proc/<PID>/status on the host machine.\n\nCPU\n\nBy default, each container's access to the host machine's CPU cycles is unlimited. You can set various constraints to limit a given container's access to the host machine's CPU cycles. Most users use and configure the default CFS scheduler. You can also configure the real-time scheduler.\n\nConfigure the default CFS scheduler\n\nThe CFS is the Linux kernel CPU scheduler for normal Linux processes. Several runtime flags let you configure the amount of access to CPU resources your container has. When you use these settings, Docker modifies the settings for the container's cgroup on the host machine.\n\nOption\tDescription\n--cpus=<value>\tSpecify how much of the available CPU resources a container can use. For instance, if the host machine has two CPUs and you set --cpus=\"1.5\", the container is guaranteed at most one and a half of the CPUs. This is the equivalent of setting --cpu-period=\"100000\" and --cpu-quota=\"150000\".\n--cpu-period=<value>\tSpecify the CPU CFS scheduler period, which is used alongside --cpu-quota. Defaults to 100000 microseconds (100 milliseconds). Most users don't change this from the default. For most use-cases, --cpus is a more convenient alternative.\n--cpu-quota=<value>\tImpose a CPU CFS quota on the container. The number of microseconds per --cpu-period that the container is limited to before being throttled. As such acting as the effective ceiling. For most use-cases, --cpus is a more convenient alternative.\n--cpuset-cpus\tLimit the specific CPUs or cores a container can use. A comma-separated list or hyphen-separated range of CPUs a container can use, if you have more than one CPU. The first CPU is numbered 0. A valid value might be 0-3 (to use the first, second, third, and fourth CPU) or 1,3 (to use the second and fourth CPU).\n--cpu-shares\tSet this flag to a value greater or less than the default of 1024 to increase or reduce the container's weight, and give it access to a greater or lesser proportion of the host machine's CPU cycles. This is only enforced when CPU cycles are constrained. When plenty of CPU cycles are available, all containers use as much CPU as they need. In that way, this is a soft limit. --cpu-shares doesn't prevent containers from being scheduled in Swarm mode. It prioritizes container CPU resources for the available CPU cycles. It doesn't guarantee or reserve any specific CPU access.\n\nIf you have 1 CPU, each of the following commands guarantees the container at most 50% of the CPU every second.\n\n$ docker run -it --cpus=\".5\" ubuntu /bin/bash\n\n\nWhich is the equivalent to manually specifying --cpu-period and --cpu-quota;\n\n$ docker run -it --cpu-period=100000 --cpu-quota=50000 ubuntu /bin/bash\n\nConfigure the real-time scheduler\n\nYou can configure your container to use the real-time scheduler, for tasks which can't use the CFS scheduler. You need to make sure the host machine's kernel is configured correctly before you can configure the Docker daemon or configure individual containers.\n\nWarning\n\nCPU scheduling and prioritization are advanced kernel-level features. Most users don't need to change these values from their defaults. Setting these values incorrectly can cause your host system to become unstable or unusable.\n\nConfigure the host machine's kernel\n\nVerify that CONFIG_RT_GROUP_SCHED is enabled in the Linux kernel by running zcat /proc/config.gz | grep CONFIG_RT_GROUP_SCHED or by checking for the existence of the file /sys/fs/cgroup/cpu.rt_runtime_us. For guidance on configuring the kernel real-time scheduler, consult the documentation for your operating system.\n\nConfigure the Docker daemon\n\nTo run containers using the real-time scheduler, run the Docker daemon with the --cpu-rt-runtime flag set to the maximum number of microseconds reserved for real-time tasks per runtime period. For instance, with the default period of 1000000 microseconds (1 second), setting --cpu-rt-runtime=950000 ensures that containers using the real-time scheduler can run for 950000 microseconds for every 1000000-microsecond period, leaving at least 50000 microseconds available for non-real-time tasks. To make this configuration permanent on systems which use systemd, create a systemd unit file for the docker service. For example, see the instruction on how to configure the daemon to use a proxy with a systemd unit file.\n\nConfigure individual containers\n\nYou can pass several flags to control a container's CPU priority when you start the container using docker run. Consult your operating system's documentation or the ulimit command for information on appropriate values.\n\nOption\tDescription\n--cap-add=sys_nice\tGrants the container the CAP_SYS_NICE capability, which allows the container to raise process nice values, set real-time scheduling policies, set CPU affinity, and other operations.\n--cpu-rt-runtime=<value>\tThe maximum number of microseconds the container can run at real-time priority within the Docker daemon's real-time scheduler period. You also need the --cap-add=sys_nice flag.\n--ulimit rtprio=<value>\tThe maximum real-time priority allowed for the container. You also need the --cap-add=sys_nice flag.\n\nThe following example command sets each of these three flags on a debian:jessie container.\n\n$ docker run -it \\\n\n    --cpu-rt-runtime=950000 \\\n\n    --ulimit rtprio=99 \\\n\n    --cap-add=sys_nice \\\n\n    debian:jessie\n\n\nIf the kernel or Docker daemon isn't configured correctly, an error occurs.\n\nGPU\nAccess an NVIDIA GPU\nPrerequisites\n\nVisit the official NVIDIA drivers page to download and install the proper drivers. Reboot your system once you have done so.\n\nVerify that your GPU is running and accessible.\n\nInstall nvidia-container-toolkit\n\nFollow the official NVIDIA Container Toolkit installation instructions.\n\nExpose GPUs for use\n\nInclude the --gpus flag when you start a container to access GPU resources. Specify how many GPUs to use. For example:\n\n$ docker run -it --rm --gpus all ubuntu nvidia-smi\n\n\nExposes all available GPUs and returns a result akin to the following:\n\n+-------------------------------------------------------------------------------+\n\n| NVIDIA-SMI 384.130            \tDriver Version: 384.130               \t|\n\n|-------------------------------+----------------------+------------------------+\n\n| GPU  Name \t   Persistence-M| Bus-Id    \tDisp.A | Volatile Uncorr. ECC   |\n\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M.   |\n\n|===============================+======================+========================|\n\n|   0  GRID K520       \tOff  | 00000000:00:03.0 Off |                  N/A      |\n\n| N/A   36C\tP0    39W / 125W |  \t0MiB /  4036MiB |      0%  \tDefault |\n\n+-------------------------------+----------------------+------------------------+\n\n+-------------------------------------------------------------------------------+\n\n| Processes:                                                       GPU Memory   |\n\n|  GPU   \tPID   Type   Process name                         \tUsage  \t|\n\n|===============================================================================|\n\n|  No running processes found                                                   |\n\n+-------------------------------------------------------------------------------+\n\nUse the device option to specify GPUs. For example:\n\n$ docker run -it --rm --gpus device=GPU-3a23c669-1f69-c64e-cf85-44e9b07e7a2a ubuntu nvidia-smi\n\n\nExposes that specific GPU.\n\n$ docker run -it --rm --gpus '\"device=0,2\"' ubuntu nvidia-smi\n\n\nExposes the first and third GPUs.\n\nNote\n\nNVIDIA GPUs can only be accessed by systems running a single engine.\n\nSet NVIDIA capabilities\n\nYou can set capabilities manually. For example, on Ubuntu you can run the following:\n\n$ docker run --gpus 'all,capabilities=utility' --rm ubuntu nvidia-smi\n\n\nThis enables the utility driver capability which adds the nvidia-smi tool to the container.\n\nCapabilities as well as other configurations can be set in images via environment variables. More information on valid variables can be found in the nvidia-container-toolkit documentation. These variables can be set in a Dockerfile.\n\nYou can also use CUDA images, which set these variables automatically. See the official CUDA images NGC catalog page.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nMemory\nUnderstand the risks of running out of memory\nLimit a container's access to memory\n--memory-swap details\n--memory-swappiness details\n--kernel-memory details\nCPU\nConfigure the default CFS scheduler\nConfigure the real-time scheduler\nGPU\nAccess an NVIDIA GPU\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995246,
    "timestamp": "2026-02-07T06:32:16.354Z",
    "title": "Runtime metrics | Docker Docs",
    "url": "https://docs.docker.com/engine/containers/runmetrics/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nStart containers automatically\nRun multiple processes in a container\nResource constraints\nRuntime metrics\nRunning containers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nContainers\n/\nRuntime metrics\nRuntime metrics\nCopy as Markdown\nDocker stats\n\nYou can use the docker stats command to live stream a container's runtime metrics. The command supports CPU, memory usage, memory limit, and network IO metrics.\n\nThe following is a sample output from the docker stats command\n\n$ docker stats redis1 redis2\n\n\n\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O\n\nredis1              0.07%               796 KB / 64 MB        1.21%               788 B / 648 B       3.568 MB / 512 KB\n\nredis2              0.07%               2.746 MB / 64 MB      4.29%               1.266 KB / 648 B    12.4 MB / 0 B\n\n\nThe docker stats reference page has more details about the docker stats command.\n\nControl groups\n\nLinux Containers rely on control groups which not only track groups of processes, but also expose metrics about CPU, memory, and block I/O usage. You can access those metrics and obtain network usage metrics as well. This is relevant for \"pure\" LXC containers, as well as for Docker containers.\n\nControl groups are exposed through a pseudo-filesystem. In modern distributions, you should find this filesystem under /sys/fs/cgroup. Under that directory, you see multiple sub-directories, called devices, freezer, blkio, and so on. Each sub-directory actually corresponds to a different cgroup hierarchy.\n\nOn older systems, the control groups might be mounted on /cgroup, without distinct hierarchies. In that case, instead of seeing the sub-directories, you see a bunch of files in that directory, and possibly some directories corresponding to existing containers.\n\nTo figure out where your control groups are mounted, you can run:\n\n$ grep cgroup /proc/mounts\n\nEnumerate cgroups\n\nThe file layout of cgroups is significantly different between v1 and v2.\n\nIf /sys/fs/cgroup/cgroup.controllers is present on your system, you are using v2, otherwise you are using v1. Refer to the subsection that corresponds to your cgroup version.\n\ncgroup v2 is used by default on the following distributions:\n\nFedora (since 31)\nDebian GNU/Linux (since 11)\nUbuntu (since 21.10)\ncgroup v1\n\nYou can look into /proc/cgroups to see the different control group subsystems known to the system, the hierarchy they belong to, and how many groups they contain.\n\nYou can also look at /proc/<pid>/cgroup to see which control groups a process belongs to. The control group is shown as a path relative to the root of the hierarchy mountpoint. / means the process hasn't been assigned to a group, while /lxc/pumpkin indicates that the process is a member of a container named pumpkin.\n\ncgroup v2\n\nOn cgroup v2 hosts, the content of /proc/cgroups isn't meaningful. See /sys/fs/cgroup/cgroup.controllers to the available controllers.\n\nChanging cgroup version\n\nChanging cgroup version requires rebooting the entire system.\n\nOn systemd-based systems, cgroup v2 can be enabled by adding systemd.unified_cgroup_hierarchy=1 to the kernel command line. To revert the cgroup version to v1, you need to set systemd.unified_cgroup_hierarchy=0 instead.\n\nIf grubby command is available on your system (e.g. on Fedora), the command line can be modified as follows:\n\n$ sudo grubby --update-kernel=ALL --args=\"systemd.unified_cgroup_hierarchy=1\"\n\n\nIf grubby command isn't available, edit the GRUB_CMDLINE_LINUX line in /etc/default/grub and run sudo update-grub.\n\nRunning Docker on cgroup v2\n\nDocker supports cgroup v2 since Docker 20.10. Running Docker on cgroup v2 also requires the following conditions to be satisfied:\n\ncontainerd: v1.4 or later\nrunc: v1.0.0-rc91 or later\nKernel: v4.15 or later (v5.2 or later is recommended)\n\nNote that the cgroup v2 mode behaves slightly different from the cgroup v1 mode:\n\nThe default cgroup driver (dockerd --exec-opt native.cgroupdriver) is systemd on v2, cgroupfs on v1.\nThe default cgroup namespace mode (docker run --cgroupns) is private on v2, host on v1.\nThe docker run flags --oom-kill-disable and --kernel-memory are discarded on v2.\nFind the cgroup for a given container\n\nFor each container, one cgroup is created in each hierarchy. On older systems with older versions of the LXC userland tools, the name of the cgroup is the name of the container. With more recent versions of the LXC tools, the cgroup is lxc/<container_name>.\n\nFor Docker containers using cgroups, the cgroup name is the full ID or long ID of the container. If a container shows up as ae836c95b4c3 in docker ps, its long ID might be something like ae836c95b4c3c9e9179e0e91015512da89fdec91612f63cebae57df9a5444c79. You can look it up with docker inspect or docker ps --no-trunc.\n\nPutting everything together to look at the memory metrics for a Docker container, take a look at the following paths:\n\n/sys/fs/cgroup/memory/docker/<longid>/ on cgroup v1, cgroupfs driver\n/sys/fs/cgroup/memory/system.slice/docker-<longid>.scope/ on cgroup v1, systemd driver\n/sys/fs/cgroup/docker/<longid>/ on cgroup v2, cgroupfs driver\n/sys/fs/cgroup/system.slice/docker-<longid>.scope/ on cgroup v2, systemd driver\nMetrics from cgroups: memory, CPU, block I/O\nNote\n\nThis section isn't yet updated for cgroup v2. For further information about cgroup v2, refer to the kernel documentation.\n\nFor each subsystem (memory, CPU, and block I/O), one or more pseudo-files exist and contain statistics.\n\nMemory metrics: memory.stat\n\nMemory metrics are found in the memory cgroup. The memory control group adds a little overhead, because it does very fine-grained accounting of the memory usage on your host. Therefore, many distributions chose to not enable it by default. Generally, to enable it, all you have to do is to add some kernel command-line parameters: cgroup_enable=memory swapaccount=1.\n\nThe metrics are in the pseudo-file memory.stat. Here is what it looks like:\n\ncache 11492564992\nrss 1930993664\nmapped_file 306728960\npgpgin 406632648\npgpgout 403355412\nswap 0\npgfault 728281223\npgmajfault 1724\ninactive_anon 46608384\nactive_anon 1884520448\ninactive_file 7003344896\nactive_file 4489052160\nunevictable 32768\nhierarchical_memory_limit 9223372036854775807\nhierarchical_memsw_limit 9223372036854775807\ntotal_cache 11492564992\ntotal_rss 1930993664\ntotal_mapped_file 306728960\ntotal_pgpgin 406632648\ntotal_pgpgout 403355412\ntotal_swap 0\ntotal_pgfault 728281223\ntotal_pgmajfault 1724\ntotal_inactive_anon 46608384\ntotal_active_anon 1884520448\ntotal_inactive_file 7003344896\ntotal_active_file 4489052160\ntotal_unevictable 32768\n\n\nThe first half (without the total_ prefix) contains statistics relevant to the processes within the cgroup, excluding sub-cgroups. The second half (with the total_ prefix) includes sub-cgroups as well.\n\nSome metrics are \"gauges\", or values that can increase or decrease. For instance, swap is the amount of swap space used by the members of the cgroup. Some others are \"counters\", or values that can only go up, because they represent occurrences of a specific event. For instance, pgfault indicates the number of page faults since the creation of the cgroup.\n\ncache\nThe amount of memory used by the processes of this control group that can be associated precisely with a block on a block device. When you read from and write to files on disk, this amount increases. This is the case if you use \"conventional\" I/O (open, read, write syscalls) as well as mapped files (with mmap). It also accounts for the memory used by tmpfs mounts, though the reasons are unclear.\nrss\nThe amount of memory that doesn't correspond to anything on disk: stacks, heaps, and anonymous memory maps.\nmapped_file\nIndicates the amount of memory mapped by the processes in the control group. It doesn't give you information about how much memory is used; it rather tells you how it's used.\npgfault, pgmajfault\nIndicate the number of times that a process of the cgroup triggered a \"page fault\" and a \"major fault\", respectively. A page fault happens when a process accesses a part of its virtual memory space which is nonexistent or protected. The former can happen if the process is buggy and tries to access an invalid address (it is sent a SIGSEGV signal, typically killing it with the famous Segmentation fault message). The latter can happen when the process reads from a memory zone which has been swapped out, or which corresponds to a mapped file: in that case, the kernel loads the page from disk, and let the CPU complete the memory access. It can also happen when the process writes to a copy-on-write memory zone: likewise, the kernel preempts the process, duplicate the memory page, and resume the write operation on the process's own copy of the page. \"Major\" faults happen when the kernel actually needs to read the data from disk. When it just duplicates an existing page, or allocate an empty page, it's a regular (or \"minor\") fault.\nswap\nThe amount of swap currently used by the processes in this cgroup.\nactive_anon, inactive_anon\nThe amount of anonymous memory that has been identified has respectively active and inactive by the kernel. \"Anonymous\" memory is the memory that is not linked to disk pages. In other words, that's the equivalent of the rss counter described above. In fact, the very definition of the rss counter is active_anon + inactive_anon - tmpfs (where tmpfs is the amount of memory used up by tmpfs filesystems mounted by this control group). Now, what's the difference between \"active\" and \"inactive\"? Pages are initially \"active\"; and at regular intervals, the kernel sweeps over the memory, and tags some pages as \"inactive\". Whenever they're accessed again, they're immediately re-tagged \"active\". When the kernel is almost out of memory, and time comes to swap out to disk, the kernel swaps \"inactive\" pages.\nactive_file, inactive_file\nCache memory, with active and inactive similar to the anon memory above. The exact formula is cache = active_file + inactive_file + tmpfs. The exact rules used by the kernel to move memory pages between active and inactive sets are different from the ones used for anonymous memory, but the general principle is the same. When the kernel needs to reclaim memory, it's cheaper to reclaim a clean (=non modified) page from this pool, since it can be reclaimed immediately (while anonymous pages and dirty/modified pages need to be written to disk first).\nunevictable\nThe amount of memory that cannot be reclaimed; generally, it accounts for memory that has been \"locked\" with mlock. It's often used by crypto frameworks to make sure that secret keys and other sensitive material never gets swapped out to disk.\nmemory_limit, memsw_limit\nThese aren't really metrics, but a reminder of the limits applied to this cgroup. The first one indicates the maximum amount of physical memory that can be used by the processes of this control group; the second one indicates the maximum amount of RAM+swap.\n\nAccounting for memory in the page cache is very complex. If two processes in different control groups both read the same file (ultimately relying on the same blocks on disk), the corresponding memory charge is split between the control groups. It's nice, but it also means that when a cgroup is terminated, it could increase the memory usage of another cgroup, because they're not splitting the cost anymore for those memory pages.\n\nCPU metrics: cpuacct.stat\n\nNow that we've covered memory metrics, everything else is simple in comparison. CPU metrics are in the cpuacct controller.\n\nFor each container, a pseudo-file cpuacct.stat contains the CPU usage accumulated by the processes of the container, broken down into user and system time. The distinction is:\n\nuser time is the amount of time a process has direct control of the CPU, executing process code.\nsystem time is the time the kernel is executing system calls on behalf of the process.\n\nThose times are expressed in ticks of 1/100th of a second, also called \"user jiffies\". There are USER_HZ \"jiffies\" per second, and on x86 systems, USER_HZ is 100. Historically, this mapped exactly to the number of scheduler \"ticks\" per second, but higher frequency scheduling and tickless kernels have made the number of ticks irrelevant.\n\nBlock I/O metrics\n\nBlock I/O is accounted in the blkio controller. Different metrics are scattered across different files. While you can find in-depth details in the blkio-controller file in the kernel documentation, here is a short list of the most relevant ones:\n\nblkio.sectors\nContains the number of 512-bytes sectors read and written by the processes member of the cgroup, device by device. Reads and writes are merged in a single counter.\nblkio.io_service_bytes\nIndicates the number of bytes read and written by the cgroup. It has 4 counters per device, because for each device, it differentiates between synchronous vs. asynchronous I/O, and reads vs. writes.\nblkio.io_serviced\nThe number of I/O operations performed, regardless of their size. It also has 4 counters per device.\nblkio.io_queued\nIndicates the number of I/O operations currently queued for this cgroup. In other words, if the cgroup isn't doing any I/O, this is zero. The opposite is not true. In other words, if there is no I/O queued, it doesn't mean that the cgroup is idle (I/O-wise). It could be doing purely synchronous reads on an otherwise quiescent device, which can therefore handle them immediately, without queuing. Also, while it's helpful to figure out which cgroup is putting stress on the I/O subsystem, keep in mind that it's a relative quantity. Even if a process group doesn't perform more I/O, its queue size can increase just because the device load increases because of other devices.\nNetwork metrics\n\nNetwork metrics aren't exposed directly by control groups. There is a good explanation for that: network interfaces exist within the context of network namespaces. The kernel could probably accumulate metrics about packets and bytes sent and received by a group of processes, but those metrics wouldn't be very useful. You want per-interface metrics (because traffic happening on the local lo interface doesn't really count). But since processes in a single cgroup can belong to multiple network namespaces, those metrics would be harder to interpret: multiple network namespaces means multiple lo interfaces, potentially multiple eth0 interfaces, etc.; so this is why there is no easy way to gather network metrics with control groups.\n\nInstead you can gather network metrics from other sources.\n\niptables\n\niptables (or rather, the netfilter framework for which iptables is just an interface) can do some serious accounting.\n\nFor instance, you can setup a rule to account for the outbound HTTP traffic on a web server:\n\n$ iptables -I OUTPUT -p tcp --sport 80\n\n\nThere is no -j or -g flag, so the rule just counts matched packets and goes to the following rule.\n\nLater, you can check the values of the counters, with:\n\n$ iptables -nxvL OUTPUT\n\n\nTechnically, -n isn't required, but it prevents iptables from doing DNS reverse lookups, which are probably useless in this scenario.\n\nCounters include packets and bytes. If you want to setup metrics for container traffic like this, you could execute a for loop to add two iptables rules per container IP address (one in each direction), in the FORWARD chain. This only meters traffic going through the NAT layer; you also need to add traffic going through the userland proxy.\n\nThen, you need to check those counters on a regular basis. If you happen to use collectd, there is a nice plugin to automate iptables counters collection.\n\nInterface-level counters\n\nSince each container has a virtual Ethernet interface, you might want to check directly the TX and RX counters of this interface. Each container is associated to a virtual Ethernet interface in your host, with a name like vethKk8Zqi. Figuring out which interface corresponds to which container is, unfortunately, difficult.\n\nBut for now, the best way is to check the metrics from within the containers. To accomplish this, you can run an executable from the host environment within the network namespace of a container using ip-netns magic.\n\nThe ip-netns exec command allows you to execute any program (present in the host system) within any network namespace visible to the current process. This means that your host can enter the network namespace of your containers, but your containers can't access the host or other peer containers. Containers can interact with their sub-containers, though.\n\nThe exact format of the command is:\n\n$ ip netns exec <nsname> <command...>\n\n\nFor example:\n\n$ ip netns exec mycontainer netstat -i\n\n\nip netns finds the mycontainer container by using namespaces pseudo-files. Each process belongs to one network namespace, one PID namespace, one mnt namespace, etc., and those namespaces are materialized under /proc/<pid>/ns/. For example, the network namespace of PID 42 is materialized by the pseudo-file /proc/42/ns/net.\n\nWhen you run ip netns exec mycontainer ..., it expects /var/run/netns/mycontainer to be one of those pseudo-files. (Symlinks are accepted.)\n\nIn other words, to execute a command within the network namespace of a container, we need to:\n\nFind out the PID of any process within the container that we want to investigate;\nCreate a symlink from /var/run/netns/<somename> to /proc/<thepid>/ns/net\nExecute ip netns exec <somename> ....\n\nReview Enumerate Cgroups for how to find the cgroup of an in-container process whose network usage you want to measure. From there, you can examine the pseudo-file named tasks, which contains all the PIDs in the cgroup (and thus, in the container). Pick any one of the PIDs.\n\nPutting everything together, if the \"short ID\" of a container is held in the environment variable $CID, then you can do this:\n\n$ TASKS=/sys/fs/cgroup/devices/docker/$CID*/tasks\n\n$ PID=$(head -n 1 $TASKS)\n\n$ mkdir -p /var/run/netns\n\n$ ln -sf /proc/$PID/ns/net /var/run/netns/$CID\n\n$ ip netns exec $CID netstat -i\n\nTips for high-performance metric collection\n\nRunning a new process each time you want to update metrics is (relatively) expensive. If you want to collect metrics at high resolutions, and/or over a large number of containers (think 1000 containers on a single host), you don't want to fork a new process each time.\n\nHere is how to collect metrics from a single process. You need to write your metric collector in C (or any language that lets you do low-level system calls). You need to use a special system call, setns(), which lets the current process enter any arbitrary namespace. It requires, however, an open file descriptor to the namespace pseudo-file (remember: that's the pseudo-file in /proc/<pid>/ns/net).\n\nHowever, there is a catch: you must not keep this file descriptor open. If you do, when the last process of the control group exits, the namespace isn't destroyed, and its network resources (like the virtual interface of the container) stays around forever (or until you close that file descriptor).\n\nThe right approach would be to keep track of the first PID of each container, and re-open the namespace pseudo-file each time.\n\nCollect metrics when a container exits\n\nSometimes, you don't care about real time metric collection, but when a container exits, you want to know how much CPU, memory, etc. it has used.\n\nDocker makes this difficult because it relies on lxc-start, which carefully cleans up after itself. It is usually easier to collect metrics at regular intervals, and this is the way the collectd LXC plugin works.\n\nBut, if you'd still like to gather the stats when a container stops, here is how:\n\nFor each container, start a collection process, and move it to the control groups that you want to monitor by writing its PID to the tasks file of the cgroup. The collection process should periodically re-read the tasks file to check if it's the last process of the control group. (If you also want to collect network statistics as explained in the previous section, you should also move the process to the appropriate network namespace.)\n\nWhen the container exits, lxc-start attempts to delete the control groups. It fails, since the control group is still in use; but that's fine. Your process should now detect that it is the only one remaining in the group. Now is the right time to collect all the metrics you need!\n\nFinally, your process should move itself back to the root control group, and remove the container control group. To remove a control group, just rmdir its directory. It's counter-intuitive to rmdir a directory as it still contains files; but remember that this is a pseudo-filesystem, so usual rules don't apply. After the cleanup is done, the collection process can exit safely.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDocker stats\nControl groups\nEnumerate cgroups\nChanging cgroup version\nRunning Docker on cgroup v2\nFind the cgroup for a given container\nMetrics from cgroups: memory, CPU, block I/O\nCPU metrics: cpuacct.stat\nNetwork metrics\nTips for high-performance metric collection\nCollect metrics when a container exits\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995249,
    "timestamp": "2026-02-07T06:32:16.357Z",
    "title": "Running containers | Docker Docs",
    "url": "https://docs.docker.com/engine/containers/run/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nStart containers automatically\nRun multiple processes in a container\nResource constraints\nRuntime metrics\nRunning containers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nContainers\n/\nRunning containers\nRunning containers\nCopy as Markdown\n\nDocker runs processes in isolated containers. A container is a process which runs on a host. The host may be local or remote. When you execute docker run, the container process that runs is isolated in that it has its own file system, its own networking, and its own isolated process tree separate from the host.\n\nThis page details how to use the docker run command to run containers.\n\nGeneral form\n\nA docker run command takes the following form:\n\n$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\n\n\nThe docker run command must specify an image reference to create the container from.\n\nImage references\n\nThe image reference is the name and version of the image. You can use the image reference to create or run a container based on an image.\n\ndocker run IMAGE[:TAG][@DIGEST]\ndocker create IMAGE[:TAG][@DIGEST]\n\nAn image tag is the image version, which defaults to latest when omitted. Use the tag to run a container from specific version of an image. For example, to run version 24.04 of the ubuntu image: docker run ubuntu:24.04.\n\nImage digests\n\nImages using the v2 or later image format have a content-addressable identifier called a digest. As long as the input used to generate the image is unchanged, the digest value is predictable.\n\nThe following example runs a container from the alpine image with the sha256:9cacb71397b640eca97488cf08582ae4e4068513101088e9f96c9814bfda95e0 digest:\n\n$ docker run alpine@sha256:9cacb71397b640eca97488cf08582ae4e4068513101088e9f96c9814bfda95e0 date\n\nOptions\n\n[OPTIONS] let you configure options for the container. For example, you can give the container a name (--name), or run it as a background process (-d). You can also set options to control things like resource constraints and networking.\n\nCommands and arguments\n\nYou can use the [COMMAND] and [ARG...] positional arguments to specify commands and arguments for the container to run when it starts up. For example, you can specify sh as the [COMMAND], combined with the -i and -t flags, to start an interactive shell in the container (if the image you select has an sh executable on PATH).\n\n$ docker run -it IMAGE sh\n\nNote\n\nDepending on your Docker system configuration, you may be required to preface the docker run command with sudo. To avoid having to use sudo with the docker command, your system administrator can create a Unix group called docker and add users to it. For more information about this configuration, refer to the Docker installation documentation for your operating system.\n\nForeground and background\n\nWhen you start a container, the container runs in the foreground by default. If you want to run the container in the background instead, you can use the --detach (or -d) flag. This starts the container without occupying your terminal window.\n\n$ docker run -d IMAGE\n\n\nWhile the container runs in the background, you can interact with the container using other CLI commands. For example, docker logs lets you view the logs for the container, and docker attach brings it to the foreground.\n\n$ docker run -d nginx\n\n0246aa4d1448a401cabd2ce8f242192b6e7af721527e48a810463366c7ff54f1\n\n$ docker ps\n\nCONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS        PORTS     NAMES\n\n0246aa4d1448   nginx     \"/docker-entrypoint.‚Ä¶\"   2 seconds ago   Up 1 second   80/tcp    pedantic_liskov\n\n$ docker logs -n 5 0246aa4d1448\n\n2023/11/06 15:58:23 [notice] 1#1: start worker process 33\n\n2023/11/06 15:58:23 [notice] 1#1: start worker process 34\n\n2023/11/06 15:58:23 [notice] 1#1: start worker process 35\n\n2023/11/06 15:58:23 [notice] 1#1: start worker process 36\n\n2023/11/06 15:58:23 [notice] 1#1: start worker process 37\n\n$ docker attach 0246aa4d1448\n\n^C\n\n2023/11/06 15:58:40 [notice] 1#1: signal 2 (SIGINT) received, exiting\n\n...\n\n\nFor more information about docker run flags related to foreground and background modes, see:\n\ndocker run --detach: run container in background\ndocker run --attach: attach to stdin, stdout, and stderr\ndocker run --tty: allocate a pseudo-tty\ndocker run --interactive: keep stdin open even if not attached\n\nFor more information about re-attaching to a background container, see docker attach.\n\nContainer identification\n\nYou can identify a container in three ways:\n\nIdentifier type\tExample value\nUUID long identifier\tf78375b1c487e03c9438c729345e54db9d20cfa2ac1fc3494b6eb60872e74778\nUUID short identifier\tf78375b1c487\nName\tevil_ptolemy\n\nThe UUID identifier is a random ID assigned to the container by the daemon.\n\nThe daemon generates a random string name for containers automatically. You can also define a custom name using the --name flag. Defining a name can be a handy way to add meaning to a container. If you specify a name, you can use it when referring to the container in a user-defined network. This works for both background and foreground Docker containers.\n\nA container identifier is not the same thing as an image reference. The image reference specifies which image to use when you run a container. You can't run docker exec nginx:alpine sh to open a shell in a container based on the nginx:alpine image, because docker exec expects a container identifier (name or ID), not an image.\n\nWhile the image used by a container is not an identifier for the container, you find out the IDs of containers using an image by using the --filter flag. For example, the following docker ps command gets the IDs of all running containers based on the nginx:alpine image:\n\n$ docker ps -q --filter ancestor=nginx:alpine\n\n\nFor more information about using filters, see Filtering.\n\nContainer networking\n\nContainers have networking enabled by default, and they can make outgoing connections. If you're running multiple containers that need to communicate with each other, you can create a custom network and attach the containers to the network.\n\nWhen multiple containers are attached to the same custom network, they can communicate with each other using the container names as a DNS hostname. The following example creates a custom network named my-net, and runs two containers that attach to the network.\n\n$ docker network create my-net\n\n$ docker run -d --name web --network my-net nginx:alpine\n\n$ docker run --rm -it --network my-net busybox\n\n/ # ping web\n\nPING web (172.18.0.2): 56 data bytes\n\n64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.326 ms\n\n64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.257 ms\n\n64 bytes from 172.18.0.2: seq=2 ttl=64 time=0.281 ms\n\n^C\n\n--- web ping statistics ---\n\n3 packets transmitted, 3 packets received, 0% packet loss\n\nround-trip min/avg/max = 0.257/0.288/0.326 ms\n\n\nFor more information about container networking, see Networking overview\n\nFilesystem mounts\n\nBy default, the data in a container is stored in an ephemeral, writable container layer. Removing the container also removes its data. If you want to use persistent data with containers, you can use filesystem mounts to store the data persistently on the host system. Filesystem mounts can also let you share data between containers and the host.\n\nDocker supports two main categories of mounts:\n\nVolume mounts\nBind mounts\n\nVolume mounts are great for persistently storing data for containers, and for sharing data between containers. Bind mounts, on the other hand, are for sharing data between a container and the host.\n\nYou can add a filesystem mount to a container using the --mount flag for the docker run command.\n\nThe following sections show basic examples of how to create volumes and bind mounts. For more in-depth examples and descriptions, refer to the section of the storage section in the documentation.\n\nVolume mounts\n\nTo create a volume mount:\n\n$ docker run --mount source=VOLUME_NAME,target=[PATH] [IMAGE] [COMMAND...]\n\n\nThe --mount flag takes two parameters in this case: source and target. The value for the source parameter is the name of the volume. The value of target is the mount location of the volume inside the container. Once you've created the volume, any data you write to the volume is persisted, even if you stop or remove the container:\n\n$ docker run --rm --mount source=my_volume,target=/foo busybox \\\n\n  echo \"hello, volume!\" > /foo/hello.txt\n\n$ docker run --mount source=my_volume,target=/bar busybox\n\n  cat /bar/hello.txt\n\nhello, volume!\n\n\nThe target must always be an absolute path, such as /src/docs. An absolute path starts with a / (forward slash). Volume names must start with an alphanumeric character, followed by a-z0-9, _ (underscore), . (period) or - (hyphen).\n\nBind mounts\n\nTo create a bind mount:\n\n$ docker run -it --mount type=bind,source=[PATH],target=[PATH] busybox\n\n\nIn this case, the --mount flag takes three parameters. A type (bind), and two paths. The source path is the location on the host that you want to bind mount into the container. The target path is the mount destination inside the container.\n\nBind mounts are read-write by default, meaning that you can both read and write files to and from the mounted location from the container. Changes that you make, such as adding or editing files, are reflected on the host filesystem:\n\n$ docker run -it --mount type=bind,source=.,target=/foo busybox\n\n/ # echo \"hello from container\" > /foo/hello.txt\n\n/ # exit\n\n$ cat hello.txt\n\nhello from container\n\nExit status\n\nThe exit code from docker run gives information about why the container failed to run or why it exited. The following sections describe the meanings of different container exit codes values.\n\n125\n\nExit code 125 indicates that the error is with Docker daemon itself.\n\n$ docker run --foo busybox; echo $?\n\n\n\nflag provided but not defined: --foo\n\nSee 'docker run --help'.\n\n125\n\n126\n\nExit code 126 indicates that the specified contained command can't be invoked. The container command in the following example is: /etc.\n\n$ docker run busybox /etc; echo $?\n\n\n\ndocker: Error response from daemon: Container command '/etc' could not be invoked.\n\n126\n\n127\n\nExit code 127 indicates that the contained command can't be found.\n\n$ docker run busybox foo; echo $?\n\n\n\ndocker: Error response from daemon: Container command 'foo' not found or does not exist.\n\n127\n\nOther exit codes\n\nAny exit code other than 125, 126, and 127 represent the exit code of the provided container command.\n\n$ docker run busybox /bin/sh -c 'exit 3'\n\n$ echo $?\n\n3\n\nRuntime constraints on resources\n\nThe operator can also adjust the performance parameters of the container:\n\nOption\tDescription\n-m, --memory=\"\"\tMemory limit (format: <number>[<unit>]). Number is a positive integer. Unit can be one of b, k, m, or g. Minimum is 6M.\n--memory-swap=\"\"\tTotal memory limit (memory + swap, format: <number>[<unit>]). Number is a positive integer. Unit can be one of b, k, m, or g.\n--memory-reservation=\"\"\tMemory soft limit (format: <number>[<unit>]). Number is a positive integer. Unit can be one of b, k, m, or g.\n--kernel-memory=\"\"\tKernel memory limit (format: <number>[<unit>]). Number is a positive integer. Unit can be one of b, k, m, or g. Minimum is 4M.\n-c, --cpu-shares=0\tCPU shares (relative weight)\n--cpus=0.000\tNumber of CPUs. Number is a fractional number. 0.000 means no limit.\n--cpu-period=0\tLimit the CPU CFS (Completely Fair Scheduler) period\n--cpuset-cpus=\"\"\tCPUs in which to allow execution (0-3, 0,1)\n--cpuset-mems=\"\"\tMemory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems.\n--cpu-quota=0\tLimit the CPU CFS (Completely Fair Scheduler) quota\n--cpu-rt-period=0\tLimit the CPU real-time period. In microseconds. Requires parent cgroups be set and cannot be higher than parent. Also check rtprio ulimits.\n--cpu-rt-runtime=0\tLimit the CPU real-time runtime. In microseconds. Requires parent cgroups be set and cannot be higher than parent. Also check rtprio ulimits.\n--blkio-weight=0\tBlock IO weight (relative weight) accepts a weight value between 10 and 1000.\n--blkio-weight-device=\"\"\tBlock IO weight (relative device weight, format: DEVICE_NAME:WEIGHT)\n--device-read-bps=\"\"\tLimit read rate from a device (format: <device-path>:<number>[<unit>]). Number is a positive integer. Unit can be one of kb, mb, or gb.\n--device-write-bps=\"\"\tLimit write rate to a device (format: <device-path>:<number>[<unit>]). Number is a positive integer. Unit can be one of kb, mb, or gb.\n--device-read-iops=\"\"\tLimit read rate (IO per second) from a device (format: <device-path>:<number>). Number is a positive integer.\n--device-write-iops=\"\"\tLimit write rate (IO per second) to a device (format: <device-path>:<number>). Number is a positive integer.\n--oom-kill-disable=false\tWhether to disable OOM Killer for the container or not.\n--oom-score-adj=0\tTune container's OOM preferences (-1000 to 1000)\n--memory-swappiness=\"\"\tTune a container's memory swappiness behavior. Accepts an integer between 0 and 100.\n--shm-size=\"\"\tSize of /dev/shm. The format is <number><unit>. number must be greater than 0. Unit is optional and can be b (bytes), k (kilobytes), m (megabytes), or g (gigabytes). If you omit the unit, the system uses bytes. If you omit the size entirely, the system uses 64m.\nUser memory constraints\n\nWe have four ways to set user memory usage:\n\nOption\tResult\nmemory=inf, memory-swap=inf (default)\tThere is no memory limit for the container. The container can use as much memory as needed.\nmemory=L<inf, memory-swap=inf\t(specify memory and set memory-swap as -1) The container is not allowed to use more than L bytes of memory, but can use as much swap as is needed (if the host supports swap memory).\nmemory=L<inf, memory-swap=2*L\t(specify memory without memory-swap) The container is not allowed to use more than L bytes of memory, swap plus memory usage is double of that.\nmemory=L<inf, memory-swap=S<inf, L<=S\t(specify both memory and memory-swap) The container is not allowed to use more than L bytes of memory, swap plus memory usage is limited by S.\n\nExamples:\n\n$ docker run -it ubuntu:24.04 /bin/bash\n\n\nWe set nothing about memory, this means the processes in the container can use as much memory and swap memory as they need.\n\n$ docker run -it -m 300M --memory-swap -1 ubuntu:24.04 /bin/bash\n\n\nWe set memory limit and disabled swap memory limit, this means the processes in the container can use 300M memory and as much swap memory as they need (if the host supports swap memory).\n\n$ docker run -it -m 300M ubuntu:24.04 /bin/bash\n\n\nWe set memory limit only, this means the processes in the container can use 300M memory and 300M swap memory, by default, the total virtual memory size (--memory-swap) will be set as double of memory, in this case, memory + swap would be 2*300M, so processes can use 300M swap memory as well.\n\n$ docker run -it -m 300M --memory-swap 1G ubuntu:24.04 /bin/bash\n\n\nWe set both memory and swap memory, so the processes in the container can use 300M memory and 700M swap memory.\n\nMemory reservation is a kind of memory soft limit that allows for greater sharing of memory. Under normal circumstances, containers can use as much of the memory as needed and are constrained only by the hard limits set with the -m/--memory option. When memory reservation is set, Docker detects memory contention or low memory and forces containers to restrict their consumption to a reservation limit.\n\nAlways set the memory reservation value below the hard limit, otherwise the hard limit takes precedence. A reservation of 0 is the same as setting no reservation. By default (without reservation set), memory reservation is the same as the hard memory limit.\n\nMemory reservation is a soft-limit feature and does not guarantee the limit won't be exceeded. Instead, the feature attempts to ensure that, when memory is heavily contended for, memory is allocated based on the reservation hints/setup.\n\nThe following example limits the memory (-m) to 500M and sets the memory reservation to 200M.\n\n$ docker run -it -m 500M --memory-reservation 200M ubuntu:24.04 /bin/bash\n\n\nUnder this configuration, when the container consumes memory more than 200M and less than 500M, the next system memory reclaim attempts to shrink container memory below 200M.\n\nThe following example set memory reservation to 1G without a hard memory limit.\n\n$ docker run -it --memory-reservation 1G ubuntu:24.04 /bin/bash\n\n\nThe container can use as much memory as it needs. The memory reservation setting ensures the container doesn't consume too much memory for long time, because every memory reclaim shrinks the container's consumption to the reservation.\n\nBy default, kernel kills processes in a container if an out-of-memory (OOM) error occurs. To change this behaviour, use the --oom-kill-disable option. Only disable the OOM killer on containers where you have also set the -m/--memory option. If the -m flag is not set, this can result in the host running out of memory and require killing the host's system processes to free memory.\n\nThe following example limits the memory to 100M and disables the OOM killer for this container:\n\n$ docker run -it -m 100M --oom-kill-disable ubuntu:24.04 /bin/bash\n\n\nThe following example, illustrates a dangerous way to use the flag:\n\n$ docker run -it --oom-kill-disable ubuntu:24.04 /bin/bash\n\n\nThe container has unlimited memory which can cause the host to run out memory and require killing system processes to free memory. The --oom-score-adj parameter can be changed to select the priority of which containers will be killed when the system is out of memory, with negative scores making them less likely to be killed, and positive scores more likely.\n\nKernel memory constraints\n\nKernel memory is fundamentally different than user memory as kernel memory can't be swapped out. The inability to swap makes it possible for the container to block system services by consuming too much kernel memory. Kernel memory includesÔºö\n\nstack pages\nslab pages\nsockets memory pressure\ntcp memory pressure\n\nYou can setup kernel memory limit to constrain these kinds of memory. For example, every process consumes some stack pages. By limiting kernel memory, you can prevent new processes from being created when the kernel memory usage is too high.\n\nKernel memory is never completely independent of user memory. Instead, you limit kernel memory in the context of the user memory limit. Assume \"U\" is the user memory limit and \"K\" the kernel limit. There are three possible ways to set limits:\n\nOption\tResult\nU != 0, K = inf (default)\tThis is the standard memory limitation mechanism already present before using kernel memory. Kernel memory is completely ignored.\nU != 0, K < U\tKernel memory is a subset of the user memory. This setup is useful in deployments where the total amount of memory per-cgroup is overcommitted. Overcommitting kernel memory limits is definitely not recommended, since the box can still run out of non-reclaimable memory. In this case, you can configure K so that the sum of all groups is never greater than the total memory. Then, freely set U at the expense of the system's service quality.\nU != 0, K > U\tSince kernel memory charges are also fed to the user counter and reclamation is triggered for the container for both kinds of memory. This configuration gives the admin a unified view of memory. It is also useful for people who just want to track kernel memory usage.\n\nExamples:\n\n$ docker run -it -m 500M --kernel-memory 50M ubuntu:24.04 /bin/bash\n\n\nWe set memory and kernel memory, so the processes in the container can use 500M memory in total, in this 500M memory, it can be 50M kernel memory tops.\n\n$ docker run -it --kernel-memory 50M ubuntu:24.04 /bin/bash\n\n\nWe set kernel memory without -m, so the processes in the container can use as much memory as they want, but they can only use 50M kernel memory.\n\nSwappiness constraint\n\nBy default, a container's kernel can swap out a percentage of anonymous pages. To set this percentage for a container, specify a --memory-swappiness value between 0 and 100. A value of 0 turns off anonymous page swapping. A value of 100 sets all anonymous pages as swappable. By default, if you are not using --memory-swappiness, memory swappiness value will be inherited from the parent.\n\nFor example, you can set:\n\n$ docker run -it --memory-swappiness=0 ubuntu:24.04 /bin/bash\n\n\nSetting the --memory-swappiness option is helpful when you want to retain the container's working set and to avoid swapping performance penalties.\n\nCPU share constraint\n\nBy default, all containers get the same proportion of CPU cycles. This proportion can be modified by changing the container's CPU share weighting relative to the weighting of all other running containers.\n\nTo modify the proportion from the default of 1024, use the -c or --cpu-shares flag to set the weighting to 2 or higher. If 0 is set, the system will ignore the value and use the default of 1024.\n\nThe proportion will only apply when CPU-intensive processes are running. When tasks in one container are idle, other containers can use the left-over CPU time. The actual amount of CPU time will vary depending on the number of containers running on the system.\n\nFor example, consider three containers, one has a cpu-share of 1024 and two others have a cpu-share setting of 512. When processes in all three containers attempt to use 100% of CPU, the first container would receive 50% of the total CPU time. If you add a fourth container with a cpu-share of 1024, the first container only gets 33% of the CPU. The remaining containers receive 16.5%, 16.5% and 33% of the CPU.\n\nOn a multi-core system, the shares of CPU time are distributed over all CPU cores. Even if a container is limited to less than 100% of CPU time, it can use 100% of each individual CPU core.\n\nFor example, consider a system with more than three cores. If you start one container {C0} with -c=512 running one process, and another container {C1} with -c=1024 running two processes, this can result in the following division of CPU shares:\n\nPID    container\tCPU\tCPU share\n100    {C0}\t\t0\t100% of CPU0\n101    {C1}\t\t1\t100% of CPU1\n102    {C1}\t\t2\t100% of CPU2\n\nCPU period constraint\n\nThe default CPU CFS (Completely Fair Scheduler) period is 100ms. We can use --cpu-period to set the period of CPUs to limit the container's CPU usage. And usually --cpu-period should work with --cpu-quota.\n\nExamples:\n\n$ docker run -it --cpu-period=50000 --cpu-quota=25000 ubuntu:24.04 /bin/bash\n\n\nIf there is 1 CPU, this means the container can get 50% CPU worth of run-time every 50ms.\n\nIn addition to use --cpu-period and --cpu-quota for setting CPU period constraints, it is possible to specify --cpus with a float number to achieve the same purpose. For example, if there is 1 CPU, then --cpus=0.5 will achieve the same result as setting --cpu-period=50000 and --cpu-quota=25000 (50% CPU).\n\nThe default value for --cpus is 0.000, which means there is no limit.\n\nFor more information, see the CFS documentation on bandwidth limiting.\n\nCpuset constraint\n\nWe can set cpus in which to allow execution for containers.\n\nExamples:\n\n$ docker run -it --cpuset-cpus=\"1,3\" ubuntu:24.04 /bin/bash\n\n\nThis means processes in container can be executed on cpu 1 and cpu 3.\n\n$ docker run -it --cpuset-cpus=\"0-2\" ubuntu:24.04 /bin/bash\n\n\nThis means processes in container can be executed on cpu 0, cpu 1 and cpu 2.\n\nWe can set mems in which to allow execution for containers. Only effective on NUMA systems.\n\nExamples:\n\n$ docker run -it --cpuset-mems=\"1,3\" ubuntu:24.04 /bin/bash\n\n\nThis example restricts the processes in the container to only use memory from memory nodes 1 and 3.\n\n$ docker run -it --cpuset-mems=\"0-2\" ubuntu:24.04 /bin/bash\n\n\nThis example restricts the processes in the container to only use memory from memory nodes 0, 1 and 2.\n\nCPU quota constraint\n\nThe --cpu-quota flag limits the container's CPU usage. The default 0 value allows the container to take 100% of a CPU resource (1 CPU). The CFS (Completely Fair Scheduler) handles resource allocation for executing processes and is default Linux Scheduler used by the kernel. Set this value to 50000 to limit the container to 50% of a CPU resource. For multiple CPUs, adjust the --cpu-quota as necessary. For more information, see the CFS documentation on bandwidth limiting.\n\nBlock IO bandwidth (Blkio) constraint\n\nBy default, all containers get the same proportion of block IO bandwidth (blkio). This proportion is 500. To modify this proportion, change the container's blkio weight relative to the weighting of all other running containers using the --blkio-weight flag.\n\nNote\n\nThe blkio weight setting is only available for direct IO. Buffered IO is not currently supported.\n\nThe --blkio-weight flag can set the weighting to a value between 10 to 1000. For example, the commands below create two containers with different blkio weight:\n\n$ docker run -it --name c1 --blkio-weight 300 ubuntu:24.04 /bin/bash\n\n$ docker run -it --name c2 --blkio-weight 600 ubuntu:24.04 /bin/bash\n\n\nIf you do block IO in the two containers at the same time, by, for example:\n\n$ time dd if=/mnt/zerofile of=test.out bs=1M count=1024 oflag=direct\n\n\nYou'll find that the proportion of time is the same as the proportion of blkio weights of the two containers.\n\nThe --blkio-weight-device=\"DEVICE_NAME:WEIGHT\" flag sets a specific device weight. The DEVICE_NAME:WEIGHT is a string containing a colon-separated device name and weight. For example, to set /dev/sda device weight to 200:\n\n$ docker run -it \\\n\n    --blkio-weight-device \"/dev/sda:200\" \\\n\n    ubuntu\n\n\nIf you specify both the --blkio-weight and --blkio-weight-device, Docker uses the --blkio-weight as the default weight and uses --blkio-weight-device to override this default with a new value on a specific device. The following example uses a default weight of 300 and overrides this default on /dev/sda setting that weight to 200:\n\n$ docker run -it \\\n\n    --blkio-weight 300 \\\n\n    --blkio-weight-device \"/dev/sda:200\" \\\n\n    ubuntu\n\n\nThe --device-read-bps flag limits the read rate (bytes per second) from a device. For example, this command creates a container and limits the read rate to 1mb per second from /dev/sda:\n\n$ docker run -it --device-read-bps /dev/sda:1mb ubuntu\n\n\nThe --device-write-bps flag limits the write rate (bytes per second) to a device. For example, this command creates a container and limits the write rate to 1mb per second for /dev/sda:\n\n$ docker run -it --device-write-bps /dev/sda:1mb ubuntu\n\n\nBoth flags take limits in the <device-path>:<limit>[unit] format. Both read and write rates must be a positive integer. You can specify the rate in kb (kilobytes), mb (megabytes), or gb (gigabytes).\n\nThe --device-read-iops flag limits read rate (IO per second) from a device. For example, this command creates a container and limits the read rate to 1000 IO per second from /dev/sda:\n\n$ docker run -it --device-read-iops /dev/sda:1000 ubuntu\n\n\nThe --device-write-iops flag limits write rate (IO per second) to a device. For example, this command creates a container and limits the write rate to 1000 IO per second to /dev/sda:\n\n$ docker run -it --device-write-iops /dev/sda:1000 ubuntu\n\n\nBoth flags take limits in the <device-path>:<limit> format. Both read and write rates must be a positive integer.\n\nAdditional groups\n--group-add: Add additional groups to run as\n\n\nBy default, the docker container process runs with the supplementary groups looked up for the specified user. If one wants to add more to that list of groups, then one can use this flag:\n\n$ docker run --rm --group-add audio --group-add nogroup --group-add 777 busybox id\n\n\n\nuid=0(root) gid=0(root) groups=10(wheel),29(audio),99(nogroup),777\n\nRuntime privilege and Linux capabilities\nOption\tDescription\n--cap-add\tAdd Linux capabilities\n--cap-drop\tDrop Linux capabilities\n--privileged\tGive extended privileges to this container\n--device=[]\tAllows you to run devices inside the container without the --privileged flag.\n\nBy default, Docker containers are \"unprivileged\" and cannot, for example, run a Docker daemon inside a Docker container. This is because by default a container is not allowed to access any devices, but a \"privileged\" container is given access to all devices (see the documentation on cgroups devices).\n\nThe --privileged flag gives all capabilities to the container. When the operator executes docker run --privileged, Docker enables access to all devices on the host, and reconfigures AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host. Use this flag with caution. For more information about the --privileged flag, see the docker run reference.\n\nIf you want to limit access to a specific device or devices you can use the --device flag. It allows you to specify one or more devices that will be accessible within the container.\n\n$ docker run --device=/dev/snd:/dev/snd ...\n\n\nBy default, the container will be able to read, write, and mknod these devices. This can be overridden using a third :rwm set of options to each --device flag:\n\n$ docker run --device=/dev/sda:/dev/xvdc --rm -it ubuntu fdisk  /dev/xvdc\n\n\n\nCommand (m for help): q\n\n$ docker run --device=/dev/sda:/dev/xvdc:r --rm -it ubuntu fdisk  /dev/xvdc\n\nYou will not be able to write the partition table.\n\n\n\nCommand (m for help): q\n\n\n\n$ docker run --device=/dev/sda:/dev/xvdc:w --rm -it ubuntu fdisk  /dev/xvdc\n\n    crash....\n\n\n\n$ docker run --device=/dev/sda:/dev/xvdc:m --rm -it ubuntu fdisk  /dev/xvdc\n\nfdisk: unable to open /dev/xvdc: Operation not permitted\n\n\nIn addition to --privileged, the operator can have fine grain control over the capabilities using --cap-add and --cap-drop. By default, Docker has a default list of capabilities that are kept. The following table lists the Linux capability options which are allowed by default and can be dropped.\n\nCapability Key\tCapability Description\nAUDIT_WRITE\tWrite records to kernel auditing log.\nCHOWN\tMake arbitrary changes to file UIDs and GIDs (see chown(2)).\nDAC_OVERRIDE\tBypass file read, write, and execute permission checks.\nFOWNER\tBypass permission checks on operations that normally require the file system UID of the process to match the UID of the file.\nFSETID\tDon't clear set-user-ID and set-group-ID permission bits when a file is modified.\nKILL\tBypass permission checks for sending signals.\nMKNOD\tCreate special files using mknod(2).\nNET_BIND_SERVICE\tBind a socket to internet domain privileged ports (port numbers less than 1024).\nNET_RAW\tUse RAW and PACKET sockets.\nSETFCAP\tSet file capabilities.\nSETGID\tMake arbitrary manipulations of process GIDs and supplementary GID list.\nSETPCAP\tModify process capabilities.\nSETUID\tMake arbitrary manipulations of process UIDs.\nSYS_CHROOT\tUse chroot(2), change root directory.\n\nThe next table shows the capabilities which are not granted by default and may be added.\n\nCapability Key\tCapability Description\nAUDIT_CONTROL\tEnable and disable kernel auditing; change auditing filter rules; retrieve auditing status and filtering rules.\nAUDIT_READ\tAllow reading the audit log via multicast netlink socket.\nBLOCK_SUSPEND\tAllow preventing system suspends.\nBPF\tAllow creating BPF maps, loading BPF Type Format (BTF) data, retrieve JITed code of BPF programs, and more.\nCHECKPOINT_RESTORE\tAllow checkpoint/restore related operations. Introduced in kernel 5.9.\nDAC_READ_SEARCH\tBypass file read permission checks and directory read and execute permission checks.\nIPC_LOCK\tLock memory (mlock(2), mlockall(2), mmap(2), shmctl(2)).\nIPC_OWNER\tBypass permission checks for operations on System V IPC objects.\nLEASE\tEstablish leases on arbitrary files (see fcntl(2)).\nLINUX_IMMUTABLE\tSet the FS_APPEND_FL and FS_IMMUTABLE_FL i-node flags.\nMAC_ADMIN\tAllow MAC configuration or state changes. Implemented for the Smack LSM.\nMAC_OVERRIDE\tOverride Mandatory Access Control (MAC). Implemented for the Smack Linux Security Module (LSM).\nNET_ADMIN\tPerform various network-related operations.\nNET_BROADCAST\tMake socket broadcasts, and listen to multicasts.\nPERFMON\tAllow system performance and observability privileged operations using perf_events, i915_perf and other kernel subsystems\nSYS_ADMIN\tPerform a range of system administration operations.\nSYS_BOOT\tUse reboot(2) and kexec_load(2), reboot and load a new kernel for later execution.\nSYS_MODULE\tLoad and unload kernel modules.\nSYS_NICE\tRaise process nice value (nice(2), setpriority(2)) and change the nice value for arbitrary processes.\nSYS_PACCT\tUse acct(2), switch process accounting on or off.\nSYS_PTRACE\tTrace arbitrary processes using ptrace(2).\nSYS_RAWIO\tPerform I/O port operations (iopl(2) and ioperm(2)).\nSYS_RESOURCE\tOverride resource Limits.\nSYS_TIME\tSet system clock (settimeofday(2), stime(2), adjtimex(2)); set real-time (hardware) clock.\nSYS_TTY_CONFIG\tUse vhangup(2); employ various privileged ioctl(2) operations on virtual terminals.\nSYSLOG\tPerform privileged syslog(2) operations.\nWAKE_ALARM\tTrigger something that will wake up the system.\n\nFurther reference information is available on the capabilities(7) - Linux man page, and in the Linux kernel source code.\n\nBoth flags support the value ALL, so to allow a container to use all capabilities except for MKNOD:\n\n$ docker run --cap-add=ALL --cap-drop=MKNOD ...\n\n\nThe --cap-add and --cap-drop flags accept capabilities to be specified with a CAP_ prefix. The following examples are therefore equivalent:\n\n$ docker run --cap-add=SYS_ADMIN ...\n\n$ docker run --cap-add=CAP_SYS_ADMIN ...\n\n\nFor interacting with the network stack, instead of using --privileged they should use --cap-add=NET_ADMIN to modify the network interfaces.\n\n$ docker run -it --rm  ubuntu:24.04 ip link add dummy0 type dummy\n\n\n\nRTNETLINK answers: Operation not permitted\n\n\n\n$ docker run -it --rm --cap-add=NET_ADMIN ubuntu:24.04 ip link add dummy0 type dummy\n\n\nTo mount a FUSE based filesystem, you need to combine both --cap-add and --device:\n\n$ docker run --rm -it --cap-add SYS_ADMIN sshfs sshfs sven@10.10.10.20:/home/sven /mnt\n\n\n\nfuse: failed to open /dev/fuse: Operation not permitted\n\n\n\n$ docker run --rm -it --device /dev/fuse sshfs sshfs sven@10.10.10.20:/home/sven /mnt\n\n\n\nfusermount: mount failed: Operation not permitted\n\n\n\n$ docker run --rm -it --cap-add SYS_ADMIN --device /dev/fuse sshfs\n\n\n\n# sshfs sven@10.10.10.20:/home/sven /mnt\n\nThe authenticity of host '10.10.10.20 (10.10.10.20)' can't be established.\n\nECDSA key fingerprint is 25:34:85:75:25:b0:17:46:05:19:04:93:b5:dd:5f:c6.\n\nAre you sure you want to continue connecting (yes/no)? yes\n\nsven@10.10.10.20's password:\n\n\n\nroot@30aa0cfaf1b5:/# ls -la /mnt/src/docker\n\n\n\ntotal 1516\n\ndrwxrwxr-x 1 1000 1000   4096 Dec  4 06:08 .\n\ndrwxrwxr-x 1 1000 1000   4096 Dec  4 11:46 ..\n\n-rw-rw-r-- 1 1000 1000     16 Oct  8 00:09 .dockerignore\n\n-rwxrwxr-x 1 1000 1000    464 Oct  8 00:09 .drone.yml\n\ndrwxrwxr-x 1 1000 1000   4096 Dec  4 06:11 .git\n\n-rw-rw-r-- 1 1000 1000    461 Dec  4 06:08 .gitignore\n\n....\n\n\nThe default seccomp profile will adjust to the selected capabilities, in order to allow use of facilities allowed by the capabilities, so you should not have to adjust this.\n\nOverriding image defaults\n\nWhen you build an image from a Dockerfile, or when committing it, you can set a number of default parameters that take effect when the image starts up as a container. When you run an image, you can override those defaults using flags for the docker run command.\n\nDefault entrypoint\nDefault command and options\nExpose ports\nEnvironment variables\nHealthcheck\nUser\nWorking directory\nDefault command and options\n\nThe command syntax for docker run supports optionally specifying commands and arguments to the container's entrypoint, represented as [COMMAND] and [ARG...] in the following synopsis example:\n\n$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\n\n\nThis command is optional because whoever created the IMAGE may have already provided a default COMMAND, using the Dockerfile CMD instruction. When you run a container, you can override that CMD instruction just by specifying a new COMMAND.\n\nIf the image also specifies an ENTRYPOINT then the CMD or COMMAND get appended as arguments to the ENTRYPOINT.\n\nDefault entrypoint\n--entrypoint=\"\": Overwrite the default entrypoint set by the image\n\nThe entrypoint refers to the default executable that's invoked when you run a container. A container's entrypoint is defined using the Dockerfile ENTRYPOINT instruction. It's similar to specifying a default command because it specifies, but the difference is that you need to pass an explicit flag to override the entrypoint, whereas you can override default commands with positional arguments. The defines a container's default behavior, with the idea that when you set an entrypoint you can run the container as if it were that binary, complete with default options, and you can pass in more options as commands. But there are cases where you may want to run something else inside the container. This is when overriding the default entrypoint at runtime comes in handy, using the --entrypoint flag for the docker run command.\n\nThe --entrypoint flag expects a string value, representing the name or path of the binary that you want to invoke when the container starts. The following example shows you how to run a Bash shell in a container that has been set up to automatically run some other binary (like /usr/bin/redis-server):\n\n$ docker run -it --entrypoint /bin/bash example/redis\n\n\nThe following examples show how to pass additional parameters to the custom entrypoint, using the positional command arguments:\n\n$ docker run -it --entrypoint /bin/bash example/redis -c ls -l\n\n$ docker run -it --entrypoint /usr/bin/redis-cli example/redis --help\n\n\nYou can reset a containers entrypoint by passing an empty string, for example:\n\n$ docker run -it --entrypoint=\"\" mysql bash\n\nNote\n\nPassing --entrypoint clears out any default command set on the image. That is, any CMD instruction in the Dockerfile used to build it.\n\nExposed ports\n\nBy default, when you run a container, none of the container's ports are exposed to the host. This means you won't be able to access any ports that the container might be listening on. To make a container's ports accessible from the host, you need to publish the ports.\n\nYou can start the container with the -P or -p flags to expose its ports:\n\nThe -P (or --publish-all) flag publishes all the exposed ports to the host. Docker binds each exposed port to a random port on the host.\n\nThe -P flag only publishes port numbers that are explicitly flagged as exposed, either using the Dockerfile EXPOSE instruction or the --expose flag for the docker run command.\n\nThe -p (or --publish) flag lets you explicitly map a single port or range of ports in the container to the host.\n\nThe port number inside the container (where the service listens) doesn't need to match the port number published on the outside of the container (where clients connect). For example, inside the container an HTTP service might be listening on port 80. At runtime, the port might be bound to 42800 on the host. To find the mapping between the host ports and the exposed ports, use the docker port command.\n\nEnvironment variables\n\nDocker automatically sets some environment variables when creating a Linux container. Docker doesn't set any environment variables when creating a Windows container.\n\nThe following environment variables are set for Linux containers:\n\nVariable\tValue\nHOME\tSet based on the value of USER\nHOSTNAME\tThe hostname associated with the container\nPATH\tIncludes popular directories, such as /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nTERM\txterm if the container is allocated a pseudo-TTY\n\nAdditionally, you can set any environment variable in the container by using one or more -e flags. You can even override the variables mentioned above, or variables defined using a Dockerfile ENV instruction when building the image.\n\nIf you name an environment variable without specifying a value, the current value of the named variable on the host is propagated into the container's environment:\n\n$ export today=Wednesday\n\n$ docker run -e \"deep=purple\" -e today --rm alpine env\n\n\n\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\nHOSTNAME=d2219b854598\n\ndeep=purple\n\ntoday=Wednesday\n\nHOME=/root\n\nPS C:\\> docker run --rm -e \"foo=bar\" microsoft/nanoserver cmd /s /c set\n\nALLUSERSPROFILE=C:\\ProgramData\n\nAPPDATA=C:\\Users\\ContainerAdministrator\\AppData\\Roaming\n\nCommonProgramFiles=C:\\Program Files\\Common Files\n\nCommonProgramFiles(x86)=C:\\Program Files (x86)\\Common Files\n\nCommonProgramW6432=C:\\Program Files\\Common Files\n\nCOMPUTERNAME=C2FAEFCC8253\n\nComSpec=C:\\Windows\\system32\\cmd.exe\n\nfoo=bar\n\nLOCALAPPDATA=C:\\Users\\ContainerAdministrator\\AppData\\Local\n\nNUMBER_OF_PROCESSORS=8\n\nOS=Windows_NT\n\nPath=C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Users\\ContainerAdministrator\\AppData\\Local\\Microsoft\\WindowsApps\n\nPATHEXT=.COM;.EXE;.BAT;.CMD\n\nPROCESSOR_ARCHITECTURE=AMD64\n\nPROCESSOR_IDENTIFIER=Intel64 Family 6 Model 62 Stepping 4, GenuineIntel\n\nPROCESSOR_LEVEL=6\n\nPROCESSOR_REVISION=3e04\n\nProgramData=C:\\ProgramData\n\nProgramFiles=C:\\Program Files\n\nProgramFiles(x86)=C:\\Program Files (x86)\n\nProgramW6432=C:\\Program Files\n\nPROMPT=$P$G\n\nPUBLIC=C:\\Users\\Public\n\nSystemDrive=C:\n\nSystemRoot=C:\\Windows\n\nTEMP=C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\n\nTMP=C:\\Users\\ContainerAdministrator\\AppData\\Local\\Temp\n\nUSERDOMAIN=User Manager\n\nUSERNAME=ContainerAdministrator\n\nUSERPROFILE=C:\\Users\\ContainerAdministrator\n\nwindir=C:\\Windows\nHealthchecks\n\nThe following flags for the docker run command let you control the parameters for container healthchecks:\n\nOption\tDescription\n--health-cmd\tCommand to run to check health\n--health-interval\tTime between running the check\n--health-retries\tConsecutive failures needed to report unhealthy\n--health-timeout\tMaximum time to allow one check to run\n--health-start-period\tStart period for the container to initialize before starting health-retries countdown\n--health-start-interval\tTime between running the check during the start period\n--no-healthcheck\tDisable any container-specified HEALTHCHECK\n\nExample:\n\n$ docker run --name=test -d \\\n\n    --health-cmd='stat /etc/passwd || exit 1' \\\n\n    --health-interval=2s \\\n\n    busybox sleep 1d\n\n$ sleep 2; docker inspect --format='{{.State.Health.Status}}' test\n\nhealthy\n\n$ docker exec test rm /etc/passwd\n\n$ sleep 2; docker inspect --format='{{json .State.Health}}' test\n\n{\n\n  \"Status\": \"unhealthy\",\n\n  \"FailingStreak\": 3,\n\n  \"Log\": [\n\n    {\n\n      \"Start\": \"2016-05-25T17:22:04.635478668Z\",\n\n      \"End\": \"2016-05-25T17:22:04.7272552Z\",\n\n      \"ExitCode\": 0,\n\n      \"Output\": \"  File: /etc/passwd\\n  Size: 334       \\tBlocks: 8          IO Block: 4096   regular file\\nDevice: 32h/50d\\tInode: 12          Links: 1\\nAccess: (0664/-rw-rw-r--)  Uid: (    0/    root)   Gid: (    0/    root)\\nAccess: 2015-12-05 22:05:32.000000000\\nModify: 2015...\"\n\n    },\n\n    {\n\n      \"Start\": \"2016-05-25T17:22:06.732900633Z\",\n\n      \"End\": \"2016-05-25T17:22:06.822168935Z\",\n\n      \"ExitCode\": 0,\n\n      \"Output\": \"  File: /etc/passwd\\n  Size: 334       \\tBlocks: 8          IO Block: 4096   regular file\\nDevice: 32h/50d\\tInode: 12          Links: 1\\nAccess: (0664/-rw-rw-r--)  Uid: (    0/    root)   Gid: (    0/    root)\\nAccess: 2015-12-05 22:05:32.000000000\\nModify: 2015...\"\n\n    },\n\n    {\n\n      \"Start\": \"2016-05-25T17:22:08.823956535Z\",\n\n      \"End\": \"2016-05-25T17:22:08.897359124Z\",\n\n      \"ExitCode\": 1,\n\n      \"Output\": \"stat: can't stat '/etc/passwd': No such file or directory\\n\"\n\n    },\n\n    {\n\n      \"Start\": \"2016-05-25T17:22:10.898802931Z\",\n\n      \"End\": \"2016-05-25T17:22:10.969631866Z\",\n\n      \"ExitCode\": 1,\n\n      \"Output\": \"stat: can't stat '/etc/passwd': No such file or directory\\n\"\n\n    },\n\n    {\n\n      \"Start\": \"2016-05-25T17:22:12.971033523Z\",\n\n      \"End\": \"2016-05-25T17:22:13.082015516Z\",\n\n      \"ExitCode\": 1,\n\n      \"Output\": \"stat: can't stat '/etc/passwd': No such file or directory\\n\"\n\n    }\n\n  ]\n\n}\n\n\nThe health status is also displayed in the docker ps output.\n\nUser\n\nThe default user within a container is root (uid = 0). You can set a default user to run the first process with the Dockerfile USER instruction. When starting a container, you can override the USER instruction by passing the -u option.\n\n-u=\"\", --user=\"\": Sets the username or UID used and optionally the groupname or GID for the specified command.\n\nThe followings examples are all valid:\n\n--user=[ user | user:group | uid | uid:gid | user:gid | uid:group ]\nNote\n\nIf you pass a numeric user ID, it must be in the range of 0-2147483647. If you pass a username, the user must exist in the container.\n\nWorking directory\n\nThe default working directory for running binaries within a container is the root directory (/). The default working directory of an image is set using the Dockerfile WORKDIR command. You can override the default working directory for an image using the -w (or --workdir) flag for the docker run command:\n\n$ docker run --rm -w /my/workdir alpine pwd\n\n/my/workdir\n\nIf the directory doesn't already exist in the container, it's created.\n\nRequest changes\n\nTable of contents\nGeneral form\nImage references\nOptions\nCommands and arguments\nForeground and background\nContainer identification\nContainer networking\nFilesystem mounts\nVolume mounts\nBind mounts\nExit status\n125\n126\n127\nOther exit codes\nRuntime constraints on resources\nUser memory constraints\nKernel memory constraints\nSwappiness constraint\nCPU share constraint\nCPU period constraint\nCpuset constraint\nCPU quota constraint\nBlock IO bandwidth (Blkio) constraint\nAdditional groups\nRuntime privilege and Linux capabilities\nOverriding image defaults\nDefault command and options\nDefault entrypoint\nExposed ports\nEnvironment variables\nHealthchecks\nUser\nWorking directory\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995252,
    "timestamp": "2026-02-07T06:32:16.362Z",
    "title": "Completion | Docker Docs",
    "url": "https://docs.docker.com/engine/cli/completion/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nCompletion\nProxy configuration\nFilter commands\nFormat command and log output\nOpenTelemetry for the Docker CLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nCLI\n/\nCompletion\nCompletion\nCopy as Markdown\n\nYou can generate a shell completion script for the Docker CLI using the docker completion command. The completion script gives you word completion for commands, flags, and Docker objects (such as container and volume names) when you hit <Tab> as you type into your terminal.\n\nYou can generate completion scripts for the following shells:\n\nBash\nZsh\nfish\nBash\n\nTo get Docker CLI completion with Bash, you first need to install the bash-completion package which contains a number of Bash functions for shell completion.\n\n# Install using APT:\n\nsudo apt install bash-completion\n\n\n\n# Install using Homebrew (Bash version 4 or later):\n\nbrew install bash-completion@2\n\n# Homebrew install for older versions of Bash:\n\nbrew install bash-completion\n\n\n\n# With pacman:\n\nsudo pacman -S bash-completion\n\nAfter installing bash-completion, source the script in your shell configuration file (in this example, .bashrc):\n\n# On Linux:\n\ncat <<EOT >> ~/.bashrc\n\nif [ -f /etc/bash_completion ]; then\n\n    . /etc/bash_completion\n\nfi\n\nEOT\n\n\n\n# On macOS / with Homebrew:\n\ncat <<EOT >> ~/.bash_profile\n\n[[ -r \"$(brew --prefix)/etc/profile.d/bash_completion.sh\" ]] && . \"$(brew --prefix)/etc/profile.d/bash_completion.sh\"\n\nEOT\n\nAnd reload your shell configuration:\n\n$ source ~/.bashrc\n\n\nNow you can generate the Bash completion script using the docker completion command:\n\n$ mkdir -p ~/.local/share/bash-completion/completions\n\n$ docker completion bash > ~/.local/share/bash-completion/completions/docker\n\nZsh\n\nThe Zsh completion system takes care of things as long as the completion can be sourced using FPATH.\n\nIf you use Oh My Zsh, you can install completions without modifying ~/.zshrc by storing the completion script in the ~/.oh-my-zsh/completions directory.\n\n$ mkdir -p ~/.oh-my-zsh/completions\n\n$ docker completion zsh > ~/.oh-my-zsh/completions/_docker\n\n\nIf you're not using Oh My Zsh, store the completion script in a directory of your choice and add the directory to FPATH in your .zshrc.\n\n$ mkdir -p ~/.docker/completions\n\n$ docker completion zsh > ~/.docker/completions/_docker\n\n$ cat <<\"EOT\" >> ~/.zshrc\n\nFPATH=\"$HOME/.docker/completions:$FPATH\"\n\nautoload -Uz compinit\n\ncompinit\n\nEOT\n\nFish\n\nfish shell supports a completion system natively. To activate completion for Docker commands, copy or symlink the completion script to your fish shell completions/ directory:\n\n$ mkdir -p ~/.config/fish/completions\n\n$ docker completion fish > ~/.config/fish/completions/docker.fish\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nBash\nZsh\nFish\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995255,
    "timestamp": "2026-02-07T06:32:16.364Z",
    "title": "Proxy configuration | Docker Docs",
    "url": "https://docs.docker.com/engine/cli/proxy/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nCompletion\nProxy configuration\nFilter commands\nFormat command and log output\nOpenTelemetry for the Docker CLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nCLI\n/\nProxy configuration\nUse a proxy server with the Docker CLI\nCopy as Markdown\n\nThis page describes how to configure the Docker CLI to use proxies via environment variables in containers.\n\nThis page doesn't describe how to configure proxies for Docker Desktop. For instructions, see configuring Docker Desktop to use HTTP/HTTPS proxies.\n\nIf you're running Docker Engine without Docker Desktop, refer to Configure the Docker daemon to use a proxy to learn how to configure a proxy server for the Docker daemon (dockerd) itself.\n\nIf your container needs to use an HTTP, HTTPS, or FTP proxy server, you can configure it in different ways:\n\nConfigure the Docker client\nSet proxy using the CLI\nNote\n\nUnfortunately, there's no standard that defines how web clients should handle proxy environment variables, or the format for defining them.\n\nIf you're interested in the history of these variables, check out this blog post on the subject, by the GitLab team: We need to talk: Can we standardize NO_PROXY?.\n\nConfigure the Docker client\n\nYou can add proxy configurations for the Docker client using a JSON configuration file, located in ~/.docker/config.json. Builds and containers use the configuration specified in this file.\n\n{\n\n \"proxies\": {\n\n   \"default\": {\n\n     \"httpProxy\": \"http://proxy.example.com:3128\",\n\n     \"httpsProxy\": \"https://proxy.example.com:3129\",\n\n     \"noProxy\": \"*.test.example.com,.example.org,127.0.0.0/8\"\n\n   }\n\n }\n\n}\nWarning\n\nProxy settings may contain sensitive information. For example, some proxy servers require authentication information to be included in their URL, or their address may expose IP-addresses or hostnames of your company's environment.\n\nEnvironment variables are stored as plain text in the container's configuration, and as such can be inspected through the remote API or committed to an image when using docker commit.\n\nThe configuration becomes active after saving the file, you don't need to restart Docker. However, the configuration only applies to new containers and builds, and doesn't affect existing containers.\n\nThe following table describes the available configuration parameters.\n\nProperty\tDescription\nhttpProxy\tSets the HTTP_PROXY and http_proxy environment variables and build arguments.\nhttpsProxy\tSets the HTTPS_PROXY and https_proxy environment variables and build arguments.\nftpProxy\tSets the FTP_PROXY and ftp_proxy environment variables and build arguments.\nnoProxy\tSets the NO_PROXY and no_proxy environment variables and build arguments.\nallProxy\tSets the ALL_PROXY and all_proxy environment variables and build arguments.\n\nThese settings are used to configure proxy environment variables for containers only, and not used as proxy settings for the Docker CLI or the Docker Engine itself. Refer to the environment variables and configure the Docker daemon to use a proxy server sections for configuring proxy settings for the CLI and daemon.\n\nRun containers with a proxy configuration\n\nWhen you start a container, its proxy-related environment variables are set to reflect your proxy configuration in ~/.docker/config.json.\n\nFor example, assuming a proxy configuration like the example shown in the earlier section, environment variables for containers that you run are set as follows:\n\n$ docker run --rm alpine sh -c 'env | grep -i  _PROXY'\n\nhttps_proxy=http://proxy.example.com:3129\n\nHTTPS_PROXY=http://proxy.example.com:3129\n\nhttp_proxy=http://proxy.example.com:3128\n\nHTTP_PROXY=http://proxy.example.com:3128\n\nno_proxy=*.test.example.com,.example.org,127.0.0.0/8\n\nNO_PROXY=*.test.example.com,.example.org,127.0.0.0/8\n\nBuild with a proxy configuration\n\nWhen you invoke a build, proxy-related build arguments are pre-populated automatically, based on the proxy settings in your Docker client configuration file.\n\nAssuming a proxy configuration like the example shown in the earlier section, environment are set as follows during builds:\n\n$ docker build \\\n\n  --no-cache \\\n\n  --progress=plain \\\n\n  - <<EOF\n\nFROM alpine\n\nRUN env | grep -i _PROXY\n\nEOF\n\n#5 [2/2] RUN env | grep -i _PROXY\n\n#5 0.100 HTTPS_PROXY=https://proxy.example.com:3129\n\n#5 0.100 no_proxy=*.test.example.com,.example.org,127.0.0.0/8\n\n#5 0.100 NO_PROXY=*.test.example.com,.example.org,127.0.0.0/8\n\n#5 0.100 https_proxy=https://proxy.example.com:3129\n\n#5 0.100 http_proxy=http://proxy.example.com:3128\n\n#5 0.100 HTTP_PROXY=http://proxy.example.com:3128\n\n#5 DONE 0.1s\n\nConfigure proxy settings per daemon\n\nThe default key under proxies in ~/.docker/config.json configures the proxy settings for all daemons that the client connects to. To configure the proxies for individual daemons, use the address of the daemon instead of the default key.\n\nThe following example configures both a default proxy config, and a no-proxy override for the Docker daemon on address tcp://docker-daemon1.example.com:\n\n{\n\n \"proxies\": {\n\n   \"default\": {\n\n     \"httpProxy\": \"http://proxy.example.com:3128\",\n\n     \"httpsProxy\": \"https://proxy.example.com:3129\",\n\n     \"noProxy\": \"*.test.example.com,.example.org,127.0.0.0/8\"\n\n   },\n\n   \"tcp://docker-daemon1.example.com\": {\n\n     \"noProxy\": \"*.internal.example.net\"\n\n   }\n\n }\n\n}\nSet proxy using the CLI\n\nInstead of configuring the Docker client, you can specify proxy configurations on the command-line when you invoke the docker build and docker run commands.\n\nProxy configuration on the command-line uses the --build-arg flag for builds, and the --env flag for when you want to run containers with a proxy.\n\n$ docker build --build-arg HTTP_PROXY=\"http://proxy.example.com:3128\" .\n\n$ docker run --env HTTP_PROXY=\"http://proxy.example.com:3128\" redis\n\n\nFor a list of all the proxy-related build arguments that you can use with the docker build command, see Predefined ARGs. These proxy values are only available in the build container. They're not included in the build output.\n\nProxy as environment variable for builds\n\nDon't use the ENV Dockerfile instruction to specify proxy settings for builds. Use build arguments instead.\n\nUsing environment variables for proxies embeds the configuration into the image. If the proxy is an internal proxy, it might not be accessible for containers created from that image.\n\nEmbedding proxy settings in images also poses a security risk, as the values may include sensitive information.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nConfigure the Docker client\nRun containers with a proxy configuration\nBuild with a proxy configuration\nConfigure proxy settings per daemon\nSet proxy using the CLI\nProxy as environment variable for builds\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995258,
    "timestamp": "2026-02-07T06:32:16.370Z",
    "title": "Filter commands | Docker Docs",
    "url": "https://docs.docker.com/engine/cli/filter/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nCompletion\nProxy configuration\nFilter commands\nFormat command and log output\nOpenTelemetry for the Docker CLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nCLI\n/\nFilter commands\nFilter commands\nCopy as Markdown\n\nYou can use the --filter flag to scope your commands. When filtering, the commands only include entries that match the pattern you specify.\n\nUsing filters\n\nThe --filter flag expects a key-value pair separated by an operator.\n\n$ docker COMMAND --filter \"KEY=VALUE\"\n\n\nThe key represents the field that you want to filter on. The value is the pattern that the specified field must match. The operator can be either equals (=) or not equals (!=).\n\nFor example, the command docker images --filter reference=alpine filters the output of the docker images command to only print alpine images.\n\n$ docker images\n\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\n\nubuntu       24.04     33a5cc25d22c   36 minutes ago   101MB\n\nubuntu       22.04     152dc042452c   36 minutes ago   88.1MB\n\nalpine       3.21      a8cbb8c69ee7   40 minutes ago   8.67MB\n\nalpine       latest    7144f7bab3d4   40 minutes ago   11.7MB\n\nbusybox      uclibc    3e516f71d880   48 minutes ago   2.4MB\n\nbusybox      glibc     7338d0c72c65   48 minutes ago   6.09MB\n\n$ docker images --filter reference=alpine\n\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\n\nalpine       3.21      a8cbb8c69ee7   40 minutes ago   8.67MB\n\nalpine       latest    7144f7bab3d4   40 minutes ago   11.7MB\n\n\nThe available fields (reference in this case) depend on the command you run. Some filters expect an exact match. Others handle partial matches. Some filters let you use regular expressions.\n\nRefer to the CLI reference description for each command to learn about the supported filtering capabilities for each command.\n\nCombining filters\n\nYou can combine multiple filters by passing multiple --filter flags. The following example shows how to print all images that match alpine:latest or busybox - a logical OR.\n\n$ docker images\n\nREPOSITORY   TAG       IMAGE ID       CREATED       SIZE\n\nubuntu       24.04     33a5cc25d22c   2 hours ago   101MB\n\nubuntu       22.04     152dc042452c   2 hours ago   88.1MB\n\nalpine       3.21      a8cbb8c69ee7   2 hours ago   8.67MB\n\nalpine       latest    7144f7bab3d4   2 hours ago   11.7MB\n\nbusybox      uclibc    3e516f71d880   2 hours ago   2.4MB\n\nbusybox      glibc     7338d0c72c65   2 hours ago   6.09MB\n\n$ docker images --filter reference=alpine:latest --filter=reference=busybox\n\nREPOSITORY   TAG       IMAGE ID       CREATED       SIZE\n\nalpine       latest    7144f7bab3d4   2 hours ago   11.7MB\n\nbusybox      uclibc    3e516f71d880   2 hours ago   2.4MB\n\nbusybox      glibc     7338d0c72c65   2 hours ago   6.09MB\n\nMultiple negated filters\n\nSome commands support negated filters on labels. Negated filters only consider results that don't match the specified patterns. The following command prunes all containers that aren't labeled foo.\n\n$ docker container prune --filter \"label!=foo\"\n\n\nThere's a catch in combining multiple negated label filters. Multiple negated filters create a single negative constraint - a logical AND. The following command prunes all containers except those labeled both foo and bar. Containers labeled either foo or bar, but not both, will be pruned.\n\n$ docker container prune --filter \"label!=foo\" --filter \"label!=bar\"\n\nReference\n\nFor more information about filtering commands, refer to the CLI reference description for commands that support the --filter flag:\n\ndocker config ls\ndocker container prune\ndocker image prune\ndocker image ls\ndocker network ls\ndocker network prune\ndocker node ls\ndocker node ps\ndocker plugin ls\ndocker container ls\ndocker search\ndocker secret ls\ndocker service ls\ndocker service ps\ndocker stack ps\ndocker system prune\ndocker volume ls\ndocker volume prune\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsing filters\nCombining filters\nMultiple negated filters\nReference\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995261,
    "timestamp": "2026-02-07T06:32:16.371Z",
    "title": "Format command and log output | Docker Docs",
    "url": "https://docs.docker.com/engine/cli/formatting/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nCompletion\nProxy configuration\nFilter commands\nFormat command and log output\nOpenTelemetry for the Docker CLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nCLI\n/\nFormat command and log output\nFormat command and log output\nCopy as Markdown\n\nDocker supports Go templates which you can use to manipulate the output format of certain commands and log drivers.\n\nDocker provides a set of basic functions to manipulate template elements. All of these examples use the docker inspect command, but many other CLI commands have a --format flag, and many of the CLI command references include examples of customizing the output format.\n\nNote\n\nWhen using the --format flag, you need to observe your shell environment. In a POSIX shell, you can run the following with a single quote:\n\n$ docker inspect --format '{{join .Args \" , \"}}'\n\n\nOtherwise, in a Windows shell (for example, PowerShell), you need to use single quotes, but escape the double quotes inside the parameters as follows:\n\n$ docker inspect --format '{{join .Args \\\" , \\\"}}'\n\njoin\n\njoin concatenates a list of strings to create a single string. It puts a separator between each element in the list.\n\n$ docker inspect --format '{{join .Args \" , \"}}' container\n\ntable\n\ntable specifies which fields you want to see its output.\n\n$ docker image list --format \"table {{.ID}}\\t{{.Repository}}\\t{{.Tag}}\\t{{.Size}}\"\n\njson\n\njson encodes an element as a json string.\n\n$ docker inspect --format '{{json .Mounts}}' container\n\nlower\n\nlower transforms a string into its lowercase representation.\n\n$ docker inspect --format \"{{lower .Name}}\" container\n\nsplit\n\nsplit slices a string into a list of strings separated by a separator.\n\n$ docker inspect --format '{{split .Image \":\"}}' container\n\ntitle\n\ntitle capitalizes the first character of a string.\n\n$ docker inspect --format \"{{title .Name}}\" container\n\nupper\n\nupper transforms a string into its uppercase representation.\n\n$ docker inspect --format \"{{upper .Name}}\" container\n\npad\n\npad adds whitespace padding to a string. You can specify the number of spaces to add before and after the string.\n\n$ docker image list --format '{{pad .Repository 5 10}}'\n\n\nThis example adds 5 spaces before the image repository name and 10 spaces after.\n\ntruncate\n\ntruncate shortens a string to a specified length. If the string is shorter than the specified length, it remains unchanged.\n\n$ docker image list --format '{{truncate .Repository 15}}'\n\n\nThis example displays the image repository name, truncating it to the first 15 characters if it's longer.\n\nprintln\n\nprintln prints each value on a new line.\n\n$ docker inspect --format='{{range .NetworkSettings.Networks}}{{println .IPAddress}}{{end}}' container\n\nHint\n\nTo find out what data can be printed, show all content as json:\n\n$ docker container ls --format='{{json .}}'\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\njoin\ntable\njson\nlower\nsplit\ntitle\nupper\npad\ntruncate\nprintln\nHint\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995264,
    "timestamp": "2026-02-07T06:32:16.373Z",
    "title": "OpenTelemetry for the Docker CLI | Docker Docs",
    "url": "https://docs.docker.com/engine/cli/otel/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nCompletion\nProxy configuration\nFilter commands\nFormat command and log output\nOpenTelemetry for the Docker CLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nCLI\n/\nOpenTelemetry for the Docker CLI\nOpenTelemetry for the Docker CLI\nCopy as Markdown\nRequires:\nDocker Engine 26.1.0 and later\n\nThe Docker CLI supports OpenTelemetry instrumentation for emitting metrics about command invocations. This is disabled by default. You can configure the CLI to start emitting metrics to the endpoint that you specify. This allows you to capture information about your docker command invocations for more insight into your Docker usage.\n\nExporting metrics is opt-in, and you control where data is being sent by specifying the destination address of the metrics collector.\n\nWhat is OpenTelemetry?\n\nOpenTelemetry, or OTel for short, is an open observability framework for creating and managing telemetry data, such as traces, metrics, and logs. OpenTelemetry is vendor- and tool-agnostic, meaning that it can be used with a broad variety of Observability backends.\n\nSupport for OpenTelemetry instrumentation in the Docker CLI means that the CLI can emit information about events that take place, using the protocols and conventions defined in the Open Telemetry specification.\n\nHow it works\n\nThe Docker CLI doesn't emit telemetry data by default. Only if you've set an environment variable on your system will Docker CLI attempt to emit OpenTelemetry metrics, to the endpoint that you specify.\n\nDOCKER_CLI_OTEL_EXPORTER_OTLP_ENDPOINT=<endpoint>\n\nThe variable specifies the endpoint of an OpenTelemetry collector, where telemetry data about docker CLI invocation should be sent. To capture the data, you'll need an OpenTelemetry collector listening on that endpoint.\n\nThe purpose of a collector is to receive the telemetry data, process it, and exports it to a backend. The backend is where the telemetry data gets stored. You can choose from a number of different backends, such as Prometheus or InfluxDB.\n\nSome backends provide tools for visualizing the metrics directly. Alternatively, you can also run a dedicated frontend with support for generating more useful graphs, such as Grafana.\n\nSetup\n\nTo get started capturing telemetry data for the Docker CLI, you'll need to:\n\nSet the DOCKER_CLI_OTEL_EXPORTER_OTLP_ENDPOINT environment variable to point to an OpenTelemetry collector endpoint\nRun an OpenTelemetry collector that receives the signals from CLI command invocations\nRun a backend for storing the data received from the collector\n\nThe following Docker Compose file bootstraps a set of services to get started with OpenTelemetry. It includes an OpenTelemetry collector that the CLI can send metrics to, and a Prometheus backend that scrapes the metrics off the collector.\n\ncompose.yaml\nShow more\nname: cli-otel\n\nservices:\n\n  prometheus:\n\n    image: prom/prometheus\n\n    command:\n\n      - \"--config.file=/etc/prometheus/prom.yml\"\n\n    ports:\n\n      # Publish the Prometheus frontend on localhost:9091\n\n      - 9091:9090\n\n    restart: always\n\n    volumes:\n\n      # Store Prometheus data in a volume:\n\n      - prom_data:/prometheus\n\n      # Mount the prom.yml config file\n\n      - ./prom.yml:/etc/prometheus/prom.yml\n\n  otelcol:\n\n    image: otel/opentelemetry-collector\n\n    restart: always\n\n    depends_on:\n\n      - prometheus\n\n    ports:\n\n      - 4317:4317\n\n    volumes:\n\n      # Mount the otelcol.yml config file\n\n      - ./otelcol.yml:/etc/otelcol/config.yaml\n\n\n\nvolumes:\n\n  prom_data:\n\nThis service assumes that the following two configuration files exist alongside compose.yaml:\n\notelcol.yml\nShow more\n# Receive signals over gRPC and HTTP\n\nreceivers:\n\n  otlp:\n\n    protocols:\n\n      grpc:\n\n      http:\n\n\n\n# Establish an endpoint for Prometheus to scrape from\n\nexporters:\n\n  prometheus:\n\n    endpoint: \"0.0.0.0:8889\"\n\n\n\nservice:\n\n  pipelines:\n\n    metrics:\n\n      receivers: [otlp]\n\n      exporters: [prometheus]\nprom.yml\nShow more\n# Configure Prometheus to scrape the OpenTelemetry collector endpoint\n\nscrape_configs:\n\n  - job_name: \"otel-collector\"\n\n    scrape_interval: 1s\n\n    static_configs:\n\n      - targets: [\"otelcol:8889\"]\n\nWith these files in place:\n\nStart the Docker Compose services:\n\n$ docker compose up\n\n\nConfigure Docker CLI to export telemetry to the OpenTelemetry collector.\n\n$ export DOCKER_CLI_OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317\n\n\nRun a docker command to trigger the CLI into sending a metric signal to the OpenTelemetry collector.\n\n$ docker version\n\n\nTo view telemetry metrics created by the CLI, open the Prometheus expression browser by going to http://localhost:9091/graph.\n\nIn the Query field, enter command_time_milliseconds_total, and execute the query to see the telemetry data.\n\nAvailable metrics\n\nDocker CLI currently exports a single metric, command.time, which measures the execution duration of a command in milliseconds. This metric has the following attributes:\n\ncommand.name: the name of the command\ncommand.status.code: the exit code of the command\ncommand.stderr.isatty: true if stderr is attached to a TTY\ncommand.stdin.isatty: true if stdin is attached to a TTY\ncommand.stdout.isatty: true if stdout is attached to a TTY\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat is OpenTelemetry?\nHow it works\nSetup\nAvailable metrics\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995270,
    "timestamp": "2026-02-07T06:32:16.378Z",
    "title": "Start the daemon | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/start/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\n/\nStart the daemon\nStart the daemon\nCopy as Markdown\n\nThis page shows how to start the daemon, either manually or using OS utilities.\n\nStart the daemon using operating system utilities\n\nOn a typical installation the Docker daemon is started by a system utility, not manually by a user. This makes it easier to automatically start Docker when the machine reboots.\n\nThe command to start Docker depends on your operating system. Check the correct page under Install Docker.\n\nStart with systemd\n\nOn some operating systems, like Ubuntu and Debian, the Docker daemon service starts automatically. Use the following command to start it manually:\n\n$ sudo systemctl start docker\n\n\nIf you want Docker to start at boot, see Configure Docker to start on boot.\n\nStart the daemon manually\n\nIf you don't want to use a system utility to manage the Docker daemon, or just want to test things out, you can manually run it using the dockerd command. You may need to use sudo, depending on your operating system configuration.\n\nWhen you start Docker this way, it runs in the foreground and sends its logs directly to your terminal.\n\n$ dockerd\n\n\n\nINFO[0000] +job init_networkdriver()\n\nINFO[0000] +job serveapi(unix:///var/run/docker.sock)\n\nINFO[0000] Listening for HTTP on unix (/var/run/docker.sock)\n\n\nTo stop Docker when you have started it manually, issue a Ctrl+C in your terminal.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nStart the daemon using operating system utilities\nStart with systemd\nStart the daemon manually\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995267,
    "timestamp": "2026-02-07T06:32:16.378Z",
    "title": "Daemon | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\nDocker daemon configuration overview\nCopy as Markdown\n\nThis page shows you how to customize the Docker daemon, dockerd.\n\nNote\n\nThis page is for users who've installed Docker Engine manually. If you're using Docker Desktop, refer to the settings page.\n\nConfigure the Docker daemon\n\nThere are two ways to configure the Docker daemon:\n\nUse a JSON configuration file. This is the preferred option, since it keeps all configurations in a single place.\nUse flags when starting dockerd.\n\nYou can use both of these options together as long as you don't specify the same option both as a flag and in the JSON file. If that happens, the Docker daemon won't start and prints an error message.\n\nConfiguration file\n\nThe following table shows the location where the Docker daemon expects to find the configuration file by default, depending on your system and how you're running the daemon.\n\nOS and configuration\tFile location\nLinux, regular setup\t/etc/docker/daemon.json\nLinux, rootless mode\t~/.config/docker/daemon.json\nWindows\tC:\\ProgramData\\docker\\config\\daemon.json\n\nFor rootless mode, the daemon respects the XDG_CONFIG_HOME variable. If set, the expected file location is $XDG_CONFIG_HOME/docker/daemon.json.\n\nYou can also explicitly specify the location of the configuration file on startup, using the dockerd --config-file flag.\n\nLearn about the available configuration options in the dockerd reference docs\n\nConfiguration using flags\n\nYou can also start the Docker daemon manually and configure it using flags. This can be useful for troubleshooting problems.\n\nHere's an example of how to manually start the Docker daemon, using the same configurations as shown in the previous JSON configuration:\n\n$ dockerd --debug \\\n\n  --tls=true \\\n\n  --tlscert=/var/docker/server.pem \\\n\n  --tlskey=/var/docker/serverkey.pem \\\n\n  --host tcp://192.168.59.3:2376\n\n\nLearn about the available configuration options in the dockerd reference docs, or by running:\n\n$ dockerd --help\n\nDaemon data directory\n\nThe Docker daemon persists all data in a single directory. This tracks everything related to Docker, including containers, images, volumes, service definition, and secrets.\n\nBy default the daemon stores data in:\n\n/var/lib/docker on Linux\nC:\\ProgramData\\docker on Windows\n\nWhen using the containerd image store (the default for Docker Engine 29.0 and later on fresh installations), image contents and container snapshots are stored in /var/lib/containerd. Other daemon data (volumes, configs) remains in /var/lib/docker.\n\nWhen using classic storage drivers like overlay2 (the default for upgraded installations), all data is stored in /var/lib/docker.\n\nConfigure the data directory location\n\nYou can configure the Docker daemon to use a different storage directory using the data-root configuration option.\n\n{\n\n  \"data-root\": \"/mnt/docker-data\"\n\n}\n\nThe data-root option does not affect image and container data stored in /var/lib/containerd when using the containerd image store. To change the storage location of containerd snapshotters, use the system containerd configuration file:\n\n/etc/containerd/config.toml\nversion = 2\n\nroot = \"/mnt/containerd-data\"\n\nMake sure you use a dedicated directory for each daemon. If two daemons share the same directory, for example an NFS share, you will experience errors that are difficult to troubleshoot.\n\nNext steps\n\nMany specific configuration options are discussed throughout the Docker documentation. Some places to go next include:\n\nAutomatically start containers\nLimit a container's resources\nConfigure storage drivers\nContainer security\nConfigure the Docker daemon to use a proxy\n\nEdit this page\n\nRequest changes\n\nTable of contents\nConfigure the Docker daemon\nConfiguration file\nConfiguration using flags\nDaemon data directory\nConfigure the data directory location\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995273,
    "timestamp": "2026-02-07T06:32:16.378Z",
    "title": "Use IPv6 networking | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/ipv6/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\n/\nUse IPv6 networking\nUse IPv6 networking\nCopy as Markdown\n\nIPv6 is only supported on Docker daemons running on Linux hosts.\n\nCreate an IPv6 network\n\nUsing docker network create:\n\n$ docker network create --ipv6 ip6net\n\n\nUsing docker network create, specifying an IPv6 subnet:\n\n$ docker network create --ipv6 --subnet 2001:db8::/64 ip6net\n\n\nUsing a Docker Compose file:\n\n networks:\n\n   ip6net:\n\n     enable_ipv6: true\n\n     ipam:\n\n       config:\n\n         - subnet: 2001:db8::/64\n\nYou can now run containers that attach to the ip6net network.\n\n$ docker run --rm --network ip6net -p 80:80 traefik/whoami\n\n\nThis publishes port 80 on both IPv6 and IPv4. You can verify the IPv6 connection by running curl, connecting to port 80 on the IPv6 loopback address:\n\n$ curl http://[::1]:80\n\nHostname: ea1cfde18196\n\nIP: 127.0.0.1\n\nIP: ::1\n\nIP: 172.17.0.2\n\nIP: 2001:db8::2\n\nIP: fe80::42:acff:fe11:2\n\nRemoteAddr: [2001:db8::1]:37574\n\nGET / HTTP/1.1\n\nHost: [::1]\n\nUser-Agent: curl/8.1.2\n\nAccept: */*\n\nUse IPv6 for the default bridge network\n\nThe following steps show you how to use IPv6 on the default bridge network.\n\nEdit the Docker daemon configuration file, located at /etc/docker/daemon.json. Configure the following parameters:\n\n{\n\n  \"ipv6\": true,\n\n  \"fixed-cidr-v6\": \"2001:db8:1::/64\"\n\n}\nipv6 enables IPv6 networking on the default network.\nfixed-cidr-v6 assigns a subnet to the default bridge network, enabling dynamic IPv6 address allocation.\nip6tables enables additional IPv6 packet filter rules, providing network isolation and port mapping. It is enabled by-default, but can be disabled.\n\nSave the configuration file.\n\nRestart the Docker daemon for your changes to take effect.\n\n$ sudo systemctl restart docker\n\n\nYou can now run containers on the default bridge network.\n\n$ docker run --rm -p 80:80 traefik/whoami\n\n\nThis publishes port 80 on both IPv6 and IPv4. You can verify the IPv6 connection by making a request to port 80 on the IPv6 loopback address:\n\n$ curl http://[::1]:80\n\nHostname: ea1cfde18196\n\nIP: 127.0.0.1\n\nIP: ::1\n\nIP: 172.17.0.2\n\nIP: 2001:db8:1::242:ac12:2\n\nIP: fe80::42:acff:fe12:2\n\nRemoteAddr: [2001:db8:1::1]:35558\n\nGET / HTTP/1.1\n\nHost: [::1]\n\nUser-Agent: curl/8.1.2\n\nAccept: */*\n\nDynamic IPv6 subnet allocation\n\nIf you don't explicitly configure subnets for user-defined networks, using docker network create --subnet=<your-subnet>, those networks use the default address pools of the daemon as a fallback. This also applies to networks created from a Docker Compose file, with enable_ipv6 set to true.\n\nIf no IPv6 pools are included in Docker Engine's default-address-pools, and no --subnet option is given, Unique Local Addresses (ULAs) will be used when IPv6 is enabled. These /64 subnets include a 40-bit Global ID based on the Docker Engine's randomly generated ID, to give a high probability of uniqueness.\n\nThe built-in default address pool configuration is shown in Subnet allocation. It does not include any IPv6 pools.\n\nTo use different pools of IPv6 subnets for dynamic address allocation, you must manually configure address pools of the daemon to include:\n\nThe default IPv4 address pools\nOne or more IPv6 pools of your own\n\nThe following example shows a valid configuration with IPv4 and IPv6 pools, both pools provide 256 subnets. IPv4 subnets with prefix length /24 will be allocated from a /16 pool. IPv6 subnets with prefix length /64 will be allocated from a /56 pool.\n\n{\n\n  \"default-address-pools\": [\n\n    { \"base\": \"172.17.0.0/16\", \"size\": 24 },\n\n    { \"base\": \"2001:db8::/56\", \"size\": 64 }\n\n  ]\n\n}\nNote\n\nThe address 2001:db8:: in this example is reserved for use in documentation. Replace it with a valid IPv6 network.\n\nThe default IPv4 pools are from the private address range, similar to the default IPv6 ULA networks.\n\nSee Subnet allocation for more information about default-address-pools.\n\nDocker in Docker\n\nOn a host using xtables (legacy iptables) instead of nftables, kernel module ip6_tables must be loaded before an IPv6 Docker network can be created, It is normally loaded automatically when Docker starts.\n\nHowever, if you running Docker in Docker that is not based on a recent version of the official docker image, you may need to run modprobe ip6_tables on your host. Alternatively, use daemon option --ip6tables=false to disable ip6tables for the containerized Docker Engine.\n\nNext steps\nNetworking overview\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCreate an IPv6 network\nUse IPv6 for the default bridge network\nDynamic IPv6 subnet allocation\nDocker in Docker\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995276,
    "timestamp": "2026-02-07T06:32:16.384Z",
    "title": "Daemon proxy configuration | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/proxy/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\n/\nDaemon proxy configuration\nDaemon proxy configuration\nCopy as Markdown\n\nIf your organization uses a proxy server to connect to the internet, you may need to configure the Docker daemon to use the proxy server. The daemon uses a proxy server to access images stored on Docker Hub and other registries, and to reach other nodes in a Docker swarm.\n\nThis page describes how to configure a proxy for the Docker daemon. For instructions on configuring proxy settings for the Docker CLI, see Configure Docker CLI to use a proxy server.\n\nImportant\n\nProxy configurations specified in the daemon.json are ignored by Docker Desktop. If you use Docker Desktop, you can configure proxies using the Docker Desktop settings.\n\nThere are two ways you can configure these settings:\n\nConfiguring the daemon through a configuration file or CLI flags\nSetting environment variables on the system\n\nConfiguring the daemon directly takes precedence over environment variables.\n\nDaemon configuration\n\nYou may configure proxy behavior for the daemon in the daemon.json file, or using CLI flags for the --http-proxy or --https-proxy flags for the dockerd command. Configuration using daemon.json is recommended.\n\n{\n\n  \"proxies\": {\n\n    \"http-proxy\": \"http://proxy.example.com:3128\",\n\n    \"https-proxy\": \"https://proxy.example.com:3129\",\n\n    \"no-proxy\": \"*.test.example.com,.example.org,127.0.0.0/8\"\n\n  }\n\n}\n\nAfter changing the configuration file, restart the daemon for the proxy configuration to take effect:\n\n$ sudo systemctl restart docker\n\nEnvironment variables\n\nThe Docker daemon checks the following environment variables in its start-up environment to configure HTTP or HTTPS proxy behavior:\n\nHTTP_PROXY\nhttp_proxy\nHTTPS_PROXY\nhttps_proxy\nNO_PROXY\nno_proxy\nsystemd unit file\n\nIf you're running the Docker daemon as a systemd service, you can create a systemd drop-in file that sets the variables for the docker service.\n\nNote for rootless mode\n\nThe location of systemd configuration files are different when running Docker in rootless mode. When running in rootless mode, Docker is started as a user-mode systemd service, and uses files stored in each users' home directory in ~/.config/systemd/<user>/docker.service.d/. In addition, systemctl must be executed without sudo and with the --user flag. Select the \"Rootless mode\" tab if you are running Docker in rootless mode.\n\nRegular install Rootless mode\n\nCreate a systemd drop-in directory for the docker service:\n\n$ sudo mkdir -p /etc/systemd/system/docker.service.d\n\n\nCreate a file named /etc/systemd/system/docker.service.d/http-proxy.conf that adds the HTTP_PROXY environment variable:\n\n[Service]\n\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:3128\"\n\nIf you are behind an HTTPS proxy server, set the HTTPS_PROXY environment variable:\n\n[Service]\n\nEnvironment=\"HTTPS_PROXY=https://proxy.example.com:3129\"\n\nMultiple environment variables can be set; to set both a non-HTTPS and a HTTPs proxy;\n\n[Service]\n\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:3128\"\n\nEnvironment=\"HTTPS_PROXY=https://proxy.example.com:3129\"\nNote\n\nSpecial characters in the proxy value, such as #?!()[]{}, must be double escaped using %%. For example:\n\n[Service]\n\nEnvironment=\"HTTP_PROXY=http://domain%%5Cuser:complex%%23pass@proxy.example.com:3128/\"\n\nIf you have internal Docker registries that you need to contact without proxying, you can specify them via the NO_PROXY environment variable.\n\nThe NO_PROXY variable specifies a string that contains comma-separated values for hosts that should be excluded from proxying. These are the options you can specify to exclude hosts:\n\nIP address prefix (1.2.3.4)\nDomain name, or a special DNS label (*)\nA domain name matches that name and all subdomains. A domain name with a leading \".\" matches subdomains only. For example, given the domains foo.example.com and example.com:\nexample.com matches example.com and foo.example.com, and\n.example.com matches only foo.example.com\nA single asterisk (*) indicates that no proxying should be done\nLiteral port numbers are accepted by IP address prefixes (1.2.3.4:80) and domain names (foo.example.com:80)\n\nExample:\n\n[Service]\n\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:3128\"\n\nEnvironment=\"HTTPS_PROXY=https://proxy.example.com:3129\"\n\nEnvironment=\"NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp\"\n\nFlush changes and restart Docker\n\n$ sudo systemctl daemon-reload\n\n$ sudo systemctl restart docker\n\n\nVerify that the configuration has been loaded and matches the changes you made, for example:\n\n$ sudo systemctl show --property=Environment docker\n\n\n\nEnvironment=HTTP_PROXY=http://proxy.example.com:3128 HTTPS_PROXY=https://proxy.example.com:3129 NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDaemon configuration\nEnvironment variables\nsystemd unit file\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995279,
    "timestamp": "2026-02-07T06:32:16.385Z",
    "title": "Live restore | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/live-restore/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\n/\nLive restore\nLive restore\nCopy as Markdown\n\nBy default, when the Docker daemon terminates, it shuts down running containers. You can configure the daemon so that containers remain running if the daemon becomes unavailable. This functionality is called live restore. The live restore option helps reduce container downtime due to daemon crashes, planned outages, or upgrades.\n\nNote\n\nLive restore isn't supported for Windows containers, but it does work for Linux containers running on Docker Desktop for Windows.\n\nEnable live restore\n\nThere are two ways to enable the live restore setting to keep containers alive when the daemon becomes unavailable. Only do one of the following.\n\nAdd the configuration to the daemon configuration file. On Linux, this defaults to /etc/docker/daemon.json. On Docker Desktop for Mac or Docker Desktop for Windows, select the Docker icon from the task bar, then click Settings -> Docker Engine.\n\nUse the following JSON to enable live-restore.\n\n{\n\n  \"live-restore\": true\n\n}\n\nRestart the Docker daemon. On Linux, you can avoid a restart (and avoid any downtime for your containers) by reloading the Docker daemon. If you use systemd, then use the command systemctl reload docker. Otherwise, send a SIGHUP signal to the dockerd process.\n\nIf you prefer, you can start the dockerd process manually with the --live-restore flag. This approach isn't recommended because it doesn't set up the environment that systemd or another process manager would use when starting the Docker process. This can cause unexpected behavior.\n\nLive restore during upgrades\n\nLive restore allows you to keep containers running across Docker daemon updates, but is only supported when installing patch releases (YY.MM.x), not for major (YY.MM) daemon upgrades.\n\nIf you skip releases during an upgrade, the daemon may not restore its connection to the containers. If the daemon can't restore the connection, it can't manage the running containers and you must stop them manually.\n\nLive restore upon restart\n\nThe live restore option only works to restore containers if the daemon options, such as bridge IP addresses and graph driver, didn't change. If any of these daemon-level configuration options have changed, the live restore may not work and you may need to manually stop the containers.\n\nImpact of live restore on running containers\n\nIf the daemon is down for a long time, running containers may fill up the FIFO log the daemon normally reads. A full log blocks containers from logging more data. The default buffer size is 64K. If the buffers fill, you must restart the Docker daemon to flush them.\n\nOn Linux, you can modify the kernel's buffer size by changing /proc/sys/fs/pipe-max-size. You can't modify the buffer size on Docker Desktop for Mac or Docker Desktop for Windows.\n\nLive restore and Swarm mode\n\nThe live restore option only pertains to standalone containers, and not to Swarm services. Swarm services are managed by Swarm managers. If Swarm managers are not available, Swarm services continue to run on worker nodes but can't be managed until enough Swarm managers are available to maintain a quorum.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nEnable live restore\nLive restore during upgrades\nLive restore upon restart\nImpact of live restore on running containers\nLive restore and Swarm mode\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995282,
    "timestamp": "2026-02-07T06:32:16.385Z",
    "title": "Alternative container runtimes | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/alternative-runtimes/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\n/\nAlternative container runtimes\nAlternative container runtimes\nCopy as Markdown\n\nDocker Engine uses containerd for managing the container lifecycle, which includes creating, starting, and stopping containers. By default, containerd uses runc as its container runtime.\n\nWhat runtimes can I use?\n\nYou can use any runtime that implements the containerd shim API. Such runtimes ship with a containerd shim, and you can use them without any additional configuration. See Use containerd shims.\n\nExamples of runtimes that implement their own containerd shims include:\n\nWasmtime\ngVisor\nKata Containers\n\nYou can also use runtimes designed as drop-in replacements for runc. Such runtimes depend on the runc containerd shim for invoking the runtime binary. You must manually register such runtimes in the daemon configuration.\n\nyouki is one example of a runtime that can function as a runc drop-in replacement. Refer to the youki example explaining the setup.\n\nUse containerd shims\n\ncontainerd shims let you use alternative runtimes without having to change the configuration of the Docker daemon. To use a containerd shim, install the shim binary on PATH on the system where the Docker daemon is running.\n\nTo use a shim with docker run, specify the fully qualified name of the runtime as the value to the --runtime flag:\n\n$ docker run --runtime io.containerd.kata.v2 hello-world\n\nUse a containerd shim without installing on PATH\n\nYou can use a shim without installing it on PATH, in which case you need to register the shim in the daemon configuration as follows:\n\n{\n\n  \"runtimes\": {\n\n    \"foo\": {\n\n      \"runtimeType\": \"/path/to/containerd-shim-foobar-v1\"\n\n    }\n\n  }\n\n}\n\nTo use the shim, specify the name that you assigned to it:\n\n$ docker run --runtime foo hello-world\n\nConfigure shims\n\nIf you need to pass additional configuration for a containerd shim, you can use the runtimes option in the daemon configuration file.\n\nEdit the daemon configuration file by adding a runtimes entry for the shim you want to configure.\n\nSpecify the fully qualified name for the runtime in runtimeType key\nAdd your runtime configuration under the options key\n{\n\n  \"runtimes\": {\n\n    \"gvisor\": {\n\n      \"runtimeType\": \"io.containerd.runsc.v1\",\n\n      \"options\": {\n\n        \"TypeUrl\": \"io.containerd.runsc.v1.options\",\n\n        \"ConfigPath\": \"/etc/containerd/runsc.toml\"\n\n      }\n\n    }\n\n  }\n\n}\n\nReload the daemon's configuration.\n\n# systemctl reload docker\n\n\nUse the customized runtime using the --runtime flag for docker run.\n\n$ docker run --runtime gvisor hello-world\n\n\nFor more information about the configuration options for containerd shims, see Configure containerd shims.\n\nExamples\n\nThe following examples show you how to set up and use alternative container runtimes with Docker Engine.\n\nyouki\nWasmtime\nyouki\n\nyouki is a container runtime written in Rust. youki claims to be faster and use less memory than runc, making it a good choice for resource-constrained environments.\n\nyouki functions as a drop-in replacement for runc, meaning it relies on the runc shim to invoke the runtime binary. When you register runtimes acting as runc replacements, you configure the path to the runtime executable, and optionally a set of runtime arguments. For more information, see Configure runc drop-in replacements.\n\nTo add youki as a container runtime:\n\nInstall youki and its dependencies.\n\nFor instructions, refer to the official setup guide.\n\nRegister youki as a runtime for Docker by editing the Docker daemon configuration file, located at /etc/docker/daemon.json by default.\n\nThe path key should specify the path to wherever you installed youki.\n\n# cat > /etc/docker/daemon.json <<EOF\n\n{\n\n  \"runtimes\": {\n\n    \"youki\": {\n\n      \"path\": \"/usr/local/bin/youki\"\n\n    }\n\n  }\n\n}\n\nEOF\n\n\nReload the daemon's configuration.\n\n# systemctl reload docker\n\n\nNow you can run containers that use youki as a runtime.\n\n$ docker run --rm --runtime youki hello-world\n\nWasmtime\nAvailability:\nExperimental \n\nWasmtime is a Bytecode Alliance project, and a Wasm runtime that lets you run Wasm containers. Running Wasm containers with Docker provides two layers of security. You get all the benefits from container isolation, plus the added sandboxing provided by the Wasm runtime environment.\n\nTo add Wasmtime as a container runtime, follow these steps:\n\nTurn on the containerd image store feature in the daemon configuration file.\n\n{\n\n  \"features\": {\n\n    \"containerd-snapshotter\": true\n\n  }\n\n}\n\nRestart the Docker daemon.\n\n# systemctl restart docker\n\n\nInstall the Wasmtime containerd shim on PATH.\n\nThe following command Dockerfile builds the Wasmtime binary from source and exports it to ./containerd-shim-wasmtime-v1.\n\n$ docker build --output . - <<EOF\n\nFROM rust:latest as build\n\nRUN cargo install \\\n\n    --git https://github.com/containerd/runwasi.git \\\n\n    --bin containerd-shim-wasmtime-v1 \\\n\n    --root /out \\\n\n    containerd-shim-wasmtime\n\nFROM scratch\n\nCOPY --from=build /out/bin /\n\nEOF\n\n\nPut the binary in a directory on PATH.\n\n$ mv ./containerd-shim-wasmtime-v1 /usr/local/bin\n\n\nNow you can run containers that use Wasmtime as a runtime.\n\n$ docker run --rm \\\n\n --runtime io.containerd.wasmtime.v1 \\\n\n --platform wasi/wasm32 \\\n\n michaelirwin244/wasm-example\n\nRelated information\nTo learn more about the configuration options for container runtimes, see Configure container runtimes.\nYou can configure which runtime that the daemon should use as its default. Refer to Configure the default container runtime.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat runtimes can I use?\nUse containerd shims\nUse a containerd shim without installing on PATH\nConfigure shims\nExamples\nyouki\nWasmtime\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995283,
    "timestamp": "2026-02-07T06:32:16.399Z",
    "title": "Collect Docker metrics with Prometheus | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/prometheus/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\n/\nCollect Docker metrics with Prometheus\nCollect Docker metrics with Prometheus\nCopy as Markdown\n\nPrometheus is an open-source systems monitoring and alerting toolkit. You can configure Docker as a Prometheus target.\n\nWarning\n\nThe available metrics and the names of those metrics are in active development and may change at any time.\n\nCurrently, you can only monitor Docker itself. You can't currently monitor your application using the Docker target.\n\nExample\n\nThe following example shows you how to configure your Docker daemon, set up Prometheus to run as a container on your local machine, and monitor your Docker instance using Prometheus.\n\nConfigure the daemon\n\nTo configure the Docker daemon as a Prometheus target, you need to specify the metrics-address in the daemon.json configuration file. This daemon expects the file to be located at one of the following locations by default. If the file doesn't exist, create it.\n\nLinux: /etc/docker/daemon.json\nWindows Server: C:\\ProgramData\\docker\\config\\daemon.json\nDocker Desktop: Open the Docker Desktop settings and select Docker Engine to edit the file.\n\nAdd the following configuration:\n\n{\n\n  \"metrics-addr\": \"127.0.0.1:9323\"\n\n}\n\nSave the file, or in the case of Docker Desktop for Mac or Docker Desktop for Windows, save the configuration. Restart Docker.\n\nDocker now exposes Prometheus-compatible metrics on port 9323 via the loopback interface. You can configure it to use the wildcard address 0.0.0.0 instead, but this will expose the Prometheus port to the wider network. Consider your threat model carefully when deciding which option best suits your environment.\n\nCreate a Prometheus configuration\n\nCopy the following configuration file and save it to a location of your choice, for example /tmp/prometheus.yml. This is a stock Prometheus configuration file, except for the addition of the Docker job definition at the bottom of the file.\n\n# my global config\n\nglobal:\n\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\n\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n\n  # scrape_timeout is set to the global default (10s).\n\n\n\n  # Attach these labels to any time series or alerts when communicating with\n\n  # external systems (federation, remote storage, Alertmanager).\n\n  external_labels:\n\n    monitor: \"codelab-monitor\"\n\n\n\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\n\nrule_files:\n\n  # - \"first.rules\"\n\n  # - \"second.rules\"\n\n\n\n# A scrape configuration containing exactly one endpoint to scrape:\n\n# Here it's Prometheus itself.\n\nscrape_configs:\n\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n\n  - job_name: prometheus\n\n\n\n    # metrics_path defaults to '/metrics'\n\n    # scheme defaults to 'http'.\n\n\n\n    static_configs:\n\n      - targets: [\"localhost:9090\"]\n\n\n\n  - job_name: docker\n\n      # metrics_path defaults to '/metrics'\n\n      # scheme defaults to 'http'.\n\n\n\n    static_configs:\n\n      - targets: [\"host.docker.internal:9323\"]\nRun Prometheus in a container\n\nNext, start a Prometheus container using this configuration.\n\n$ docker run --name my-prometheus \\\n\n    --mount type=bind,source=/tmp/prometheus.yml,destination=/etc/prometheus/prometheus.yml \\\n\n    -p 9090:9090 \\\n\n    --add-host host.docker.internal=host-gateway \\\n\n    prom/prometheus\n\n\nIf you're using Docker Desktop, the --add-host flag is optional. This flag makes sure that the host's internal IP gets exposed to the Prometheus container. Docker Desktop does this by default. The host IP is exposed as the host.docker.internal hostname. This matches the configuration defined in prometheus.yml in the previous step.\n\nOpen the Prometheus Dashboard\n\nVerify that the Docker target is listed at http://localhost:9090/targets/.\n\nNote\n\nYou can't access the endpoint URLs on this page directly if you use Docker Desktop.\n\nUse Prometheus\n\nCreate a graph. Select the Graphs link in the Prometheus UI. Choose a metric from the combo box to the right of the Execute button, and click Execute. The screenshot below shows the graph for engine_daemon_network_actions_seconds_count.\n\nThe graph shows a pretty idle Docker instance, unless you're already running active workloads on your system.\n\nTo make the graph more interesting, run a container that uses some network actions by starting downloading some packages using a package manager:\n\n$ docker run --rm alpine apk add git make musl-dev go\n\n\nWait a few seconds (the default scrape interval is 15 seconds) and reload your graph. You should see an uptick in the graph, showing the increased network traffic caused by the container you just ran.\n\nNext steps\n\nThe example provided here shows how to run Prometheus as a container on your local system. In practice, you'll probably be running Prometheus on another system or as a cloud service somewhere. You can set up the Docker daemon as a Prometheus target in such contexts too. Configure the metrics-addr of the daemon and add the address of the daemon as a scrape endpoint in your Prometheus configuration.\n\n- job_name: docker\n\n  static_configs:\n\n    - targets: [\"docker.daemon.example:PORT\"]\n\nFor more information about Prometheus, refer to the Prometheus documentation\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExample\nConfigure the daemon\nCreate a Prometheus configuration\nRun Prometheus in a container\nOpen the Prometheus Dashboard\nUse Prometheus\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995288,
    "timestamp": "2026-02-07T06:32:16.399Z",
    "title": "Configure remote access for Docker daemon | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/remote-access/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\n/\nConfigure remote access for Docker daemon\nConfigure remote access for Docker daemon\nCopy as Markdown\n\nBy default, the Docker daemon listens for connections on a Unix socket to accept requests from local clients. You can configure Docker to accept requests from remote clients by configuring it to listen on an IP address and port as well as the Unix socket.\n\nWarning\n\nConfiguring Docker to accept connections from remote clients can leave you vulnerable to unauthorized access to the host and other attacks.\n\nIt's critically important that you understand the security implications of opening Docker to the network. If steps aren't taken to secure the connection, it's possible for remote non-root users to gain root access on the host.\n\nRemote access without TLS is not recommended, and will require explicit opt-in in a future release. For more information on how to use TLS certificates to secure this connection, see Protect the Docker daemon socket.\n\nEnable remote access\n\nYou can enable remote access to the daemon either using a docker.service systemd unit file for Linux distributions using systemd. Or you can use the daemon.json file, if your distribution doesn't use systemd.\n\nConfiguring Docker to listen for connections using both the systemd unit file and the daemon.json file causes a conflict that prevents Docker from starting.\n\nConfiguring remote access with systemd unit file\n\nUse the command sudo systemctl edit docker.service to open an override file for docker.service in a text editor.\n\nAdd or modify the following lines, substituting your own values.\n\n[Service]\n\nExecStart=\n\nExecStart=/usr/bin/dockerd -H fd:// -H tcp://127.0.0.1:2375\n\nSave the file.\n\nReload the systemctl configuration.\n\n$ sudo systemctl daemon-reload\n\n\nRestart Docker.\n\n$ sudo systemctl restart docker.service\n\n\nVerify that the change has gone through.\n\n$ sudo netstat -lntp | grep dockerd\n\ntcp        0      0 127.0.0.1:2375          0.0.0.0:*               LISTEN      3758/dockerd\n\nConfiguring remote access with daemon.json\n\nSet the hosts array in the /etc/docker/daemon.json to connect to the Unix socket and an IP address, as follows:\n\n{\n\n  \"hosts\": [\"unix:///var/run/docker.sock\", \"tcp://127.0.0.1:2375\"]\n\n}\n\nRestart Docker.\n\nVerify that the change has gone through.\n\n$ sudo netstat -lntp | grep dockerd\n\ntcp        0      0 127.0.0.1:2375          0.0.0.0:*               LISTEN      3758/dockerd\n\nAllow access to the remote API through a firewall\n\nIf you run a firewall on the same host as you run Docker, and you want to access the Docker Remote API from another remote host, you must configure your firewall to allow incoming connections on the Docker port. The default port is 2376 if you're using TLS encrypted transport, or 2375 otherwise.\n\nTwo common firewall daemons are:\n\nUncomplicated Firewall (ufw), often used for Ubuntu systems.\nfirewalld, often used for RPM-based systems.\n\nConsult the documentation for your OS and firewall. The following information might help you get started. The settings used in this instruction are permissive, and you may want to use a different configuration that locks your system down more.\n\nFor ufw, set DEFAULT_FORWARD_POLICY=\"ACCEPT\" in your configuration.\n\nFor firewalld, add rules similar to the following to your policy. One for incoming requests, and one for outgoing requests.\n\n<direct>\n\n  [ <rule ipv=\"ipv6\" table=\"filter\" chain=\"FORWARD_direct\" priority=\"0\"> -i zt0 -j ACCEPT </rule> ]\n\n  [ <rule ipv=\"ipv6\" table=\"filter\" chain=\"FORWARD_direct\" priority=\"0\"> -o zt0 -j ACCEPT </rule> ]\n\n</direct>\n\nMake sure that the interface names and chain names are correct.\n\nAdditional information\n\nFor more detailed information on configuration options for remote access to the daemon, refer to the dockerd CLI reference.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nEnable remote access\nConfiguring remote access with systemd unit file\nConfiguring remote access with daemon.json\nAllow access to the remote API through a firewall\nAdditional information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995291,
    "timestamp": "2026-02-07T06:32:16.402Z",
    "title": "Read the daemon logs | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/logs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\n/\nRead the daemon logs\nRead the daemon logs\nCopy as Markdown\n\nThe daemon logs may help you diagnose problems. The logs may be saved in one of a few locations, depending on the operating system configuration and the logging subsystem used:\n\nOperating system\tLocation\nLinux\tUse the command journalctl -xu docker.service (or read /var/log/syslog or /var/log/messages, depending on your Linux Distribution)\nmacOS (dockerd logs)\t~/Library/Containers/com.docker.docker/Data/log/vm/dockerd.log\nmacOS (containerd logs)\t~/Library/Containers/com.docker.docker/Data/log/vm/containerd.log\nWindows (WSL2) (dockerd logs)\t%LOCALAPPDATA%\\Docker\\log\\vm\\dockerd.log\nWindows (WSL2) (containerd logs)\t%LOCALAPPDATA%\\Docker\\log\\vm\\containerd.log\nWindows (Windows containers)\tLogs are in the Windows Event Log\n\nTo view the dockerd logs on macOS, open a terminal Window, and use the tail command with the -f flag to \"follow\" the logs. Logs will be printed until you terminate the command using CTRL+c:\n\n$ tail -f ~/Library/Containers/com.docker.docker/Data/log/vm/dockerd.log\n\n2021-07-28T10:21:21Z dockerd time=\"2021-07-28T10:21:21.497642089Z\" level=debug msg=\"attach: stdout: begin\"\n\n2021-07-28T10:21:21Z dockerd time=\"2021-07-28T10:21:21.497714291Z\" level=debug msg=\"attach: stderr: begin\"\n\n2021-07-28T10:21:21Z dockerd time=\"2021-07-28T10:21:21.499798390Z\" level=debug msg=\"Calling POST /v1.41/containers/35fc5ec0ffe1ad492d0a4fbf51fd6286a087b89d4dd66367fa3b7aec70b46a40/wait?condition=removed\"\n\n2021-07-28T10:21:21Z dockerd time=\"2021-07-28T10:21:21.518403686Z\" level=debug msg=\"Calling GET /v1.41/containers/35fc5ec0ffe1ad492d0a4fbf51fd6286a087b89d4dd66367fa3b7aec70b46a40/json\"\n\n2021-07-28T10:21:21Z dockerd time=\"2021-07-28T10:21:21.527074928Z\" level=debug msg=\"Calling POST /v1.41/containers/35fc5ec0ffe1ad492d0a4fbf51fd6286a087b89d4dd66367fa3b7aec70b46a40/start\"\n\n2021-07-28T10:21:21Z dockerd time=\"2021-07-28T10:21:21.528203579Z\" level=debug msg=\"container mounted via layerStore: &{/var/lib/docker/overlay2/6e76ffecede030507fcaa576404e141e5f87fc4d7e1760e9ce5b52acb24\n\n...\n\n^C\n\nEnable debugging\n\nThere are two ways to enable debugging. The recommended approach is to set the debug key to true in the daemon.json file. This method works for every Docker platform.\n\nEdit the daemon.json file, which is usually located in /etc/docker/. You may need to create this file, if it doesn't yet exist. On macOS or Windows, don't edit the file directly. Instead, edit the file through the Docker Desktop settings.\n\nIf the file is empty, add the following:\n\n{\n\n  \"debug\": true\n\n}\n\nIf the file already contains JSON, just add the key \"debug\": true, being careful to add a comma to the end of the line if it's not the last line before the closing bracket. Also verify that if the log-level key is set, it's set to either info or debug. info is the default, and possible values are debug, info, warn, error, fatal.\n\nSend a HUP signal to the daemon to cause it to reload its configuration. On Linux hosts, use the following command.\n\n$ sudo kill -SIGHUP $(pidof dockerd)\n\n\nOn Windows hosts, restart Docker.\n\nInstead of following this procedure, you can also stop the Docker daemon and restart it manually with the debug flag -D. However, this may result in Docker restarting with a different environment than the one the hosts' startup scripts create, and this may make debugging more difficult.\n\nForce a stack trace to be logged\n\nIf the daemon is unresponsive, you can force a full stack trace to be logged by sending a SIGUSR1 signal to the daemon.\n\nLinux:\n\n$ sudo kill -SIGUSR1 $(pidof dockerd)\n\n\nWindows Server:\n\nDownload docker-signal.\n\nGet the process ID of dockerd Get-Process dockerd.\n\nRun the executable with the flag --pid=<PID of daemon>.\n\nThis forces a stack trace to be logged but doesn't stop the daemon. Daemon logs show the stack trace or the path to a file containing the stack trace if it was logged to a file.\n\nThe daemon continues operating after handling the SIGUSR1 signal and dumping the stack traces to the log. The stack traces can be used to determine the state of all goroutines and threads within the daemon.\n\nView stack traces\n\nThe Docker daemon log can be viewed by using one of the following methods:\n\nBy running journalctl -u docker.service on Linux systems using systemctl\n/var/log/messages, /var/log/daemon.log, or /var/log/docker.log on older Linux systems\nNote\n\nIt isn't possible to manually generate a stack trace on Docker Desktop for Mac or Docker Desktop for Windows. However, you can click the Docker taskbar icon and choose Troubleshoot to send information to Docker if you run into issues.\n\nLook in the Docker logs for a message like the following:\n\n...goroutine stacks written to /var/run/docker/goroutine-stacks-2017-06-02T193336z.log\n\nThe locations where Docker saves these stack traces and dumps depends on your operating system and configuration. You can sometimes get useful diagnostic information straight from the stack traces and dumps. Otherwise, you can provide this information to Docker for help diagnosing the problem.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nEnable debugging\nForce a stack trace to be logged\nView stack traces\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995297,
    "timestamp": "2026-02-07T06:32:16.416Z",
    "title": "Docker contexts | Docker Docs",
    "url": "https://docs.docker.com/engine/manage-resources/contexts/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nDocker contexts\nDocker object labels\nPrune unused Docker objects\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nManage resources\n/\nDocker contexts\nDocker contexts\nCopy as Markdown\nIntroduction\n\nThis guide shows how you can use contexts to manage Docker daemons from a single client.\n\nEach context contains all information required to manage resources on the daemon. The docker context command makes it easy to configure these contexts and switch between them.\n\nAs an example, a single Docker client might be configured with two contexts:\n\nA default context running locally\nA remote, shared context\n\nOnce these contexts are configured, you can use the docker context use <context-name> command to switch between them.\n\nPrerequisites\n\nTo follow the examples in this guide, you'll need:\n\nA Docker client that supports the top-level context command\n\nRun docker context to verify that your Docker client supports contexts.\n\nThe anatomy of a context\n\nA context is a combination of several properties. These include:\n\nName and description\nEndpoint configuration\nTLS info\n\nTo list available contexts, use the docker context ls command.\n\n$ docker context ls\n\nNAME        DESCRIPTION                               DOCKER ENDPOINT               ERROR\n\ndefault *                                             unix:///var/run/docker.sock\n\n\nThis shows a single context called \"default\". It's configured to talk to a daemon through the local /var/run/docker.sock Unix socket.\n\nThe asterisk in the NAME column indicates that this is the active context. This means all docker commands run against this context, unless overridden with environment variables such as DOCKER_HOST and DOCKER_CONTEXT, or on the command-line with the --context and --host flags.\n\nDig a bit deeper with docker context inspect. The following example shows how to inspect the context called default.\n\n$ docker context inspect default\n\n[\n\n    {\n\n        \"Name\": \"default\",\n\n        \"Metadata\": {},\n\n        \"Endpoints\": {\n\n            \"docker\": {\n\n                \"Host\": \"unix:///var/run/docker.sock\",\n\n                \"SkipTLSVerify\": false\n\n            }\n\n        },\n\n        \"TLSMaterial\": {},\n\n        \"Storage\": {\n\n            \"MetadataPath\": \"\\u003cIN MEMORY\\u003e\",\n\n            \"TLSPath\": \"\\u003cIN MEMORY\\u003e\"\n\n        }\n\n    }\n\n]\n\nCreate a new context\n\nYou can create new contexts with the docker context create command.\n\nThe following example creates a new context called docker-test and specifies the host endpoint of the context to TCP socket tcp://docker:2375.\n\n$ docker context create docker-test --docker host=tcp://docker:2375\n\ndocker-test\n\nSuccessfully created context \"docker-test\"\n\n\nThe new context is stored in a meta.json file below ~/.docker/contexts/. Each new context you create gets its own meta.json stored in a dedicated sub-directory of ~/.docker/contexts/.\n\nYou can view the new context with docker context ls and docker context inspect <context-name>.\n\n$ docker context ls\n\nNAME          DESCRIPTION                             DOCKER ENDPOINT               ERROR\n\ndefault *                                             unix:///var/run/docker.sock\n\ndocker-test                                           tcp://docker:2375\n\n\nThe current context is indicated with an asterisk (\"*\").\n\nUse a different context\n\nYou can use docker context use to switch between contexts.\n\nThe following command will switch the docker CLI to use the docker-test context.\n\n$ docker context use docker-test\n\ndocker-test\n\nCurrent context is now \"docker-test\"\n\n\nVerify the operation by listing all contexts and ensuring the asterisk (\"*\") is against the docker-test context.\n\n$ docker context ls\n\nNAME            DESCRIPTION                           DOCKER ENDPOINT               ERROR\n\ndefault                                               unix:///var/run/docker.sock\n\ndocker-test *                                         tcp://docker:2375\n\n\ndocker commands will now target endpoints defined in the docker-test context.\n\nYou can also set the current context using the DOCKER_CONTEXT environment variable. The environment variable overrides the context set with docker context use.\n\nUse the appropriate command below to set the context to docker-test using an environment variable.\n\nPowerShell Bash\n> $env:DOCKER_CONTEXT='docker-test'\n\nRun docker context ls to verify that the docker-test context is now the active context.\n\nYou can also use the global --context flag to override the context. The following command uses a context called production.\n\n$ docker --context production container ls\n\nExporting and importing Docker contexts\n\nYou can use the docker context export and docker context import commands to export and import contexts on different hosts.\n\nThe docker context export command exports an existing context to a file. The file can be imported on any host that has the docker client installed.\n\nExporting and importing a context\n\nThe following example exports an existing context called docker-test. It will be written to a file called docker-test.dockercontext.\n\n$ docker context export docker-test\n\nWritten file \"docker-test.dockercontext\"\n\n\nCheck the contents of the export file.\n\n$ cat docker-test.dockercontext\n\n\nImport this file on another host using docker context import to create context with the same configuration.\n\n$ docker context import docker-test docker-test.dockercontext\n\ndocker-test\n\nSuccessfully imported context \"docker-test\"\n\n\nYou can verify that the context was imported with docker context ls.\n\nThe format of the import command is docker context import <context-name> <context-file>.\n\nUpdating a context\n\nYou can use docker context update to update fields in an existing context.\n\nThe following example updates the description field in the existing docker-test context.\n\n$ docker context update docker-test --description \"Test context\"\n\ndocker-test\n\nSuccessfully updated context \"docker-test\"\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nIntroduction\nPrerequisites\nThe anatomy of a context\nCreate a new context\nUse a different context\nExporting and importing Docker contexts\nExporting and importing a context\nUpdating a context\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995294,
    "timestamp": "2026-02-07T06:32:16.418Z",
    "title": "Troubleshooting the Docker daemon | Docker Docs",
    "url": "https://docs.docker.com/engine/daemon/troubleshoot/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nStart the daemon\nUse IPv6 networking\nDaemon proxy configuration\nLive restore\nAlternative container runtimes\nCollect Docker metrics with Prometheus\nConfigure remote access for Docker daemon\nRead the daemon logs\nTroubleshooting the Docker daemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDaemon\n/\nTroubleshooting the Docker daemon\nTroubleshooting the Docker daemon\nCopy as Markdown\n\nThis page describes how to troubleshoot and debug the daemon if you run into issues.\n\nYou can turn on debugging on the daemon to learn about the runtime activity of the daemon and to aid in troubleshooting. If the daemon is unresponsive, you can also force a full stack trace of all threads to be added to the daemon log by sending the SIGUSR signal to the Docker daemon.\n\nDaemon\nUnable to connect to the Docker daemon\nCannot connect to the Docker daemon. Is 'docker daemon' running on this host?\n\nThis error may indicate:\n\nThe Docker daemon isn't running on your system. Start the daemon and try running the command again.\nYour Docker client is attempting to connect to a Docker daemon on a different host, and that host is unreachable.\nCheck whether Docker is running\n\nThe operating-system independent way to check whether Docker is running is to ask Docker, using the docker info command.\n\nYou can also use operating system utilities, such as sudo systemctl is-active docker or sudo status docker or sudo service docker status, or checking the service status using Windows utilities.\n\nFinally, you can check in the process list for the dockerd process, using commands like ps or top.\n\nCheck which host your client is connecting to\n\nTo see which host your client is connecting to, check the value of the DOCKER_HOST variable in your environment.\n\n$ env | grep DOCKER_HOST\n\n\nIf this command returns a value, the Docker client is set to connect to a Docker daemon running on that host. If it's unset, the Docker client is set to connect to the Docker daemon running on the local host. If it's set in error, use the following command to unset it:\n\n$ unset DOCKER_HOST\n\n\nYou may need to edit your environment in files such as ~/.bashrc or ~/.profile to prevent the DOCKER_HOST variable from being set erroneously.\n\nIf DOCKER_HOST is set as intended, verify that the Docker daemon is running on the remote host and that a firewall or network outage isn't preventing you from connecting.\n\nTroubleshoot conflicts between the daemon.json and startup scripts\n\nIf you use a daemon.json file and also pass options to the dockerd command manually or using start-up scripts, and these options conflict, Docker fails to start with an error such as:\n\nunable to configure the Docker daemon with file /etc/docker/daemon.json:\n\nthe following directives are specified both as a flag and in the configuration\n\nfile: hosts: (from flag: [unix:///var/run/docker.sock], from file: [tcp://127.0.0.1:2376])\n\nIf you see an error similar to this one and you are starting the daemon manually with flags, you may need to adjust your flags or the daemon.json to remove the conflict.\n\nNote\n\nIf you see this specific error message about hosts, continue to the next section for a workaround.\n\nIf you are starting Docker using your operating system's init scripts, you may need to override the defaults in these scripts in ways that are specific to the operating system.\n\nConfigure the daemon host with systemd\n\nOne notable example of a configuration conflict that's difficult to troubleshoot is when you want to specify a different daemon address from the default. Docker listens on a socket by default. On Debian and Ubuntu systems using systemd, this means that a host flag -H is always used when starting dockerd. If you specify a hosts entry in the daemon.json, this causes a configuration conflict and results in the Docker daemon failing to start.\n\nTo work around this problem, create a new file /etc/systemd/system/docker.service.d/docker.conf with the following contents, to remove the -H argument that's used when starting the daemon by default.\n\n[Service]\n\nExecStart=\n\nExecStart=/usr/bin/dockerd\n\nThere are other times when you might need to configure systemd with Docker, such as configuring a HTTP or HTTPS proxy.\n\nNote\n\nIf you override this option without specifying a hosts entry in the daemon.json or a -H flag when starting Docker manually, Docker fails to start.\n\nRun sudo systemctl daemon-reload before attempting to start Docker. If Docker starts successfully, it's now listening on the IP address specified in the hosts key of the daemon.json instead of a socket.\n\nImportant\n\nSetting hosts in the daemon.json isn't supported on Docker Desktop for Windows or Docker Desktop for Mac.\n\nOut of memory issues\n\nIf your containers attempt to use more memory than the system has available, you may experience an Out of Memory (OOM) exception, and a container, or the Docker daemon, might be stopped by the kernel OOM killer. To prevent this from happening, ensure that your application runs on hosts with adequate memory and see Understand the risks of running out of memory.\n\nKernel compatibility\n\nDocker can't run correctly if your kernel is older than version 3.10, or if it's missing kernel modules. To check kernel compatibility, you can download and run the check-config.sh script.\n\n$ curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh > check-config.sh\n\n\n\n$ bash ./check-config.sh\n\n\nThe script only works on Linux.\n\nKernel cgroup swap limit capabilities\n\nOn Ubuntu or Debian hosts, you may see messages similar to the following when working with an image.\n\nWARNING: Your kernel does not support swap limit capabilities. Limitation discarded.\n\nIf you don't need these capabilities, you can ignore the warning.\n\nYou can turn on these capabilities on Ubuntu or Debian by following these instructions. Memory and swap accounting incur an overhead of about 1% of the total available memory and a 10% overall performance degradation, even when Docker isn't running.\n\nLog into the Ubuntu or Debian host as a user with sudo privileges.\n\nEdit the /etc/default/grub file. Add or edit the GRUB_CMDLINE_LINUX line to add the following two key-value pairs:\n\nGRUB_CMDLINE_LINUX=\"cgroup_enable=memory swapaccount=1\"\n\nSave and close the file.\n\nUpdate the GRUB boot loader.\n\n$ sudo update-grub\n\n\nAn error occurs if your GRUB configuration file has incorrect syntax. In this case, repeat steps 2 and 3.\n\nThe changes take effect when you reboot the system.\n\nNetworking\nIP forwarding problems\n\nIf you manually configure your network using systemd-network with systemd version 219 or later, Docker containers may not be able to access your network. Beginning with systemd version 220, the forwarding setting for a given network (net.ipv4.conf.<interface>.forwarding) defaults to off. This setting prevents IP forwarding. It also conflicts with Docker's behavior of enabling the net.ipv4.conf.all.forwarding setting within containers.\n\nTo work around this on RHEL, CentOS, or Fedora, edit the <interface>.network file in /usr/lib/systemd/network/ on your Docker host, for example, /usr/lib/systemd/network/80-container-host0.network.\n\nAdd the following block within the [Network] section.\n\n[Network]\n\n...\n\nIPForward=kernel\n\n# OR\n\nIPForward=true\n\nThis configuration allows IP forwarding from the container as expected.\n\nDNS resolver issues\nDNS resolver found in resolv.conf and containers can't use it\n\n\nLinux desktop environments often have a network manager program running, that uses dnsmasq to cache DNS requests by adding them to /etc/resolv.conf. The dnsmasq instance runs on a loopback address such as 127.0.0.1 or 127.0.1.1. It speeds up DNS look-ups and provides DHCP services. Such a configuration doesn't work within a Docker container. The Docker container uses its own network namespace, and resolves loopback addresses such as 127.0.0.1 to itself, and it's unlikely to be running a DNS server on its own loopback address.\n\nIf Docker detects that no DNS server referenced in /etc/resolv.conf is a fully functional DNS server, the following warning occurs:\n\nWARNING: Local (127.0.0.1) DNS resolver found in resolv.conf and containers\n\ncan't use it. Using default external servers : [8.8.8.8 8.8.4.4]\n\nIf you see this warning, first check to see if you use dnsmasq:\n\n$ ps aux | grep dnsmasq\n\n\nIf your container needs to resolve hosts which are internal to your network, the public nameservers aren't adequate. You have two choices:\n\nSpecify DNS servers for Docker to use.\n\nTurn off dnsmasq.\n\nTurning off dnsmasq adds the IP addresses of actual DNS nameservers to /etc/resolv.conf, and you lose the benefits of dnsmasq.\n\nYou only need to use one of these methods.\n\nSpecify DNS servers for Docker\n\nThe default location of the configuration file is /etc/docker/daemon.json. You can change the location of the configuration file using the --config-file daemon flag. The following instruction assumes that the location of the configuration file is /etc/docker/daemon.json.\n\nCreate or edit the Docker daemon configuration file, which defaults to /etc/docker/daemon.json file, which controls the Docker daemon configuration.\n\n$ sudo nano /etc/docker/daemon.json\n\n\nAdd a dns key with one or more DNS server IP addresses as values.\n\n{\n\n  \"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\n\n}\n\nIf the file has existing contents, you only need to add or edit the dns line. If your internal DNS server can't resolve public IP addresses, include at least one DNS server that can. Doing so allows you to connect to Docker Hub, and your containers to resolve internet domain names.\n\nSave and close the file.\n\nRestart the Docker daemon.\n\n$ sudo service docker restart\n\n\nVerify that Docker can resolve external IP addresses by trying to pull an image:\n\n$ docker pull hello-world\n\n\nIf necessary, verify that Docker containers can resolve an internal hostname by pinging it.\n\n$ docker run --rm -it alpine ping -c4 <my_internal_host>\n\n\n\nPING google.com (192.168.1.2): 56 data bytes\n\n64 bytes from 192.168.1.2: seq=0 ttl=41 time=7.597 ms\n\n64 bytes from 192.168.1.2: seq=1 ttl=41 time=7.635 ms\n\n64 bytes from 192.168.1.2: seq=2 ttl=41 time=7.660 ms\n\n64 bytes from 192.168.1.2: seq=3 ttl=41 time=7.677 ms\n\nTurn off dnsmasq\nUbuntu RHEL, CentOS, or Fedora\n\nIf you prefer not to change the Docker daemon's configuration to use a specific IP address, follow these instructions to turn off dnsmasq in NetworkManager.\n\nEdit the /etc/NetworkManager/NetworkManager.conf file.\n\nComment out the dns=dnsmasq line by adding a # character to the beginning of the line.\n\n# dns=dnsmasq\n\nSave and close the file.\n\nRestart both NetworkManager and Docker. As an alternative, you can reboot your system.\n\n$ sudo systemctl restart network-manager\n\n$ sudo systemctl restart docker\n\nDocker networks disappearing\n\nIf a Docker network, such as the docker0 bridge or a custom network, randomly disappears or otherwise appears to be working incorrectly, it could be because another service is interfering with or modifying Docker interfaces. Tools that manage networking interfaces on the host are known to sometimes also inappropriately modify Docker interfaces.\n\nRefer to the following sections for instructions on how to configure your network manager to set Docker interfaces as un-managed, depending on the network management tools that exist on the host:\n\nIf netscript is installed, consider uninstalling it\nConfigure the network manager to treat Docker interfaces as un-managed\nIf you're using Netplan, you may need to apply a custom Netplan configuration\nUninstall netscript\n\nIf netscript is installed on your system, you can likely fix this issue by uninstalling it. For example, on a Debian-based system:\n\n$ sudo apt-get remove netscript-2.4\n\nUn-manage Docker interfaces\n\nIn some cases, the network manager will attempt to manage Docker interfaces by default. You can try to explicitly flag Docker networks as un-managed by editing your system's network configuration settings.\n\nNetworkManager systemd-networkd\n\nIf you're using NetworkManager, edit your system network configuration under /etc/network/interfaces\n\nCreate a file at /etc/network/interfaces.d/20-docker0 with the following contents:\n\niface docker0 inet manual\n\nNote that this example configuration only \"un-manages\" the default docker0 bridge, not custom networks.\n\nRestart NetworkManager for the configuration change to take effect.\n\n$ systemctl restart NetworkManager\n\n\nVerify that the docker0 interface has the unmanaged state.\n\n$ nmcli device\n\nPrevent Netplan from overriding network configuration\n\nOn systems that use Netplan through cloud-init, you may need to apply a custom configuration to prevent netplan from overriding the network manager configuration:\n\nFollow the steps in Un-manage Docker interfaces for creating the network manager configuration.\n\nCreate a netplan configuration file under /etc/netplan/50-cloud-init.yml.\n\nThe following example configuration file is a starting point. Adjust it to match the interfaces you want to un-manage. Incorrect configuration can lead to network connectivity issues.\n\n/etc/netplan/50-cloud-init.yml\nnetwork:\n\n  ethernets:\n\n    all:\n\n      dhcp4: true\n\n      dhcp6: true\n\n      match:\n\n        # edit this filter to match whatever makes sense for your system\n\n        name: en*\n\n  renderer: networkd\n\n  version: 2\n\nApply the new Netplan configuration.\n\n$ sudo netplan apply\n\n\nRestart the Docker daemon:\n\n$ sudo systemctl restart docker\n\n\nVerify that the Docker interfaces have the unmanaged state.\n\n$ networkctl\n\nVolumes\nUnable to remove filesystem\nError: Unable to remove filesystem\n\nSome container-based utilities, such as Google cAdvisor, mount Docker system directories, such as /var/lib/docker/, into a container. For instance, the documentation for cadvisor instructs you to run the cadvisor container as follows:\n\n$ sudo docker run \\\n\n  --volume=/:/rootfs:ro \\\n\n  --volume=/var/run:/var/run:rw \\\n\n  --volume=/sys:/sys:ro \\\n\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n\n  --publish=8080:8080 \\\n\n  --detach=true \\\n\n  --name=cadvisor \\\n\n  google/cadvisor:latest\n\n\nWhen you bind-mount /var/lib/docker/, this effectively mounts all resources of all other running containers as filesystems within the container which mounts /var/lib/docker/. When you attempt to remove any of these containers, the removal attempt may fail with an error like the following:\n\nError: Unable to remove filesystem for\n\n74bef250361c7817bee19349c93139621b272bc8f654ae112dd4eb9652af9515:\n\nremove /var/lib/docker/containers/74bef250361c7817bee19349c93139621b272bc8f654ae112dd4eb9652af9515/shm:\n\nDevice or resource busy\n\nThe problem occurs if the container which bind-mounts /var/lib/docker/ uses statfs or fstatfs on filesystem handles within /var/lib/docker/ and does not close them.\n\nTypically, we would advise against bind-mounting /var/lib/docker in this way. However, cAdvisor requires this bind-mount for core functionality.\n\nIf you are unsure which process is causing the path mentioned in the error to be busy and preventing it from being removed, you can use the lsof command to find its process. For instance, for the error above:\n\n$ sudo lsof /var/lib/docker/containers/74bef250361c7817bee19349c93139621b272bc8f654ae112dd4eb9652af9515/shm\n\n\nTo work around this problem, stop the container which bind-mounts /var/lib/docker and try again to remove the other container.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDaemon\nUnable to connect to the Docker daemon\nCheck whether Docker is running\nTroubleshoot conflicts between the daemon.json and startup scripts\nOut of memory issues\nKernel compatibility\nKernel cgroup swap limit capabilities\nNetworking\nIP forwarding problems\nDNS resolver issues\nSpecify DNS servers for Docker\nTurn off dnsmasq\nDocker networks disappearing\nPrevent Netplan from overriding network configuration\nVolumes\nUnable to remove filesystem\nTroubleshooting\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995300,
    "timestamp": "2026-02-07T06:32:16.418Z",
    "title": "Docker object labels | Docker Docs",
    "url": "https://docs.docker.com/engine/manage-resources/labels/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nDocker contexts\nDocker object labels\nPrune unused Docker objects\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nManage resources\n/\nDocker object labels\nDocker object labels\nCopy as Markdown\n\nLabels are a mechanism for applying metadata to Docker objects, including:\n\nImages\nContainers\nLocal daemons\nVolumes\nNetworks\nSwarm nodes\nSwarm services\n\nYou can use labels to organize your images, record licensing information, annotate relationships between containers, volumes, and networks, or in any way that makes sense for your business or application.\n\nLabel keys and values\n\nA label is a key-value pair, stored as a string. You can specify multiple labels for an object, but each key must be unique within an object. If the same key is given multiple values, the most-recently-written value overwrites all previous values.\n\nKey format recommendations\n\nA label key is the left-hand side of the key-value pair. Keys are alphanumeric strings which may contain periods (.), underscores (_), slashes (/), and hyphens (-). Most Docker users use images created by other organizations, and the following guidelines help to prevent inadvertent duplication of labels across objects, especially if you plan to use labels as a mechanism for automation.\n\nAuthors of third-party tools should prefix each label key with the reverse DNS notation of a domain they own, such as com.example.some-label.\n\nDon't use a domain in your label key without the domain owner's permission.\n\nThe com.docker.*, io.docker.*, and org.dockerproject.* namespaces are reserved by Docker for internal use.\n\nLabel keys should begin and end with a lower-case letter and should only contain lower-case alphanumeric characters, the period character (.), and the hyphen character (-). Consecutive periods or hyphens aren't allowed.\n\nThe period character (.) separates namespace \"fields\". Label keys without namespaces are reserved for CLI use, allowing users of the CLI to interactively label Docker objects using shorter typing-friendly strings.\n\nThese guidelines aren't currently enforced and additional guidelines may apply to specific use cases.\n\nValue guidelines\n\nLabel values can contain any data type that can be represented as a string, including (but not limited to) JSON, XML, CSV, or YAML. The only requirement is that the value be serialized to a string first, using a mechanism specific to the type of structure. For instance, to serialize JSON into a string, you might use the JSON.stringify() JavaScript method.\n\nSince Docker doesn't deserialize the value, you can't treat a JSON or XML document as a nested structure when querying or filtering by label value unless you build this functionality into third-party tooling.\n\nManage labels on objects\n\nEach type of object with support for labels has mechanisms for adding and managing them and using them as they relate to that type of object. These links provide a good place to start learning about how you can use labels in your Docker deployments.\n\nLabels on images, containers, local daemons, volumes, and networks are static for the lifetime of the object. To change these labels you must recreate the object. Labels on Swarm nodes and services can be updated dynamically.\n\nImages and containers\n\nAdding labels to images\nOverriding a container's labels at runtime\nInspecting labels on images or containers\nFiltering images by label\nFiltering containers by label\n\nLocal Docker daemons\n\nAdding labels to a Docker daemon at runtime\nInspecting a Docker daemon's labels\n\nVolumes\n\nAdding labels to volumes\nInspecting a volume's labels\nFiltering volumes by label\n\nNetworks\n\nAdding labels to a network\nInspecting a network's labels\nFiltering networks by label\n\nSwarm nodes\n\nAdding or updating a Swarm node's labels\nInspecting a Swarm node's labels\nFiltering Swarm nodes by label\n\nSwarm services\n\nAdding labels when creating a Swarm service\nUpdating a Swarm service's labels\nInspecting a Swarm service's labels\nFiltering Swarm services by label\n\nEdit this page\n\nRequest changes\n\nTable of contents\nLabel keys and values\nKey format recommendations\nValue guidelines\nManage labels on objects\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995303,
    "timestamp": "2026-02-07T06:32:16.426Z",
    "title": "Prune unused Docker objects | Docker Docs",
    "url": "https://docs.docker.com/engine/manage-resources/pruning/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nDocker contexts\nDocker object labels\nPrune unused Docker objects\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nManage resources\n/\nPrune unused Docker objects\nPrune unused Docker objects\nCopy as Markdown\n\nDocker takes a conservative approach to cleaning up unused objects (often referred to as \"garbage collection\"), such as images, containers, volumes, and networks. These objects are generally not removed unless you explicitly ask Docker to do so. This can cause Docker to use extra disk space. For each type of object, Docker provides a prune command. In addition, you can use docker system prune to clean up multiple types of objects at once. This topic shows how to use these prune commands.\n\nPrune images\n\nThe docker image prune command allows you to clean up unused images. By default, docker image prune only cleans up dangling images. A dangling image is one that isn't tagged, and isn't referenced by any container. To remove dangling images:\n\n$ docker image prune\n\n\n\nWARNING! This will remove all dangling images.\n\nAre you sure you want to continue? [y/N] y\n\n\nTo remove all images which aren't used by existing containers, use the -a flag:\n\n$ docker image prune -a\n\n\n\nWARNING! This will remove all images without at least one container associated to them.\n\nAre you sure you want to continue? [y/N] y\n\n\nBy default, you are prompted to continue. To bypass the prompt, use the -f or --force flag.\n\nYou can limit which images are pruned using filtering expressions with the --filter flag. For example, to only consider images created more than 24 hours ago:\n\n$ docker image prune -a --filter \"until=24h\"\n\n\nOther filtering expressions are available. See the docker image prune reference for more examples.\n\nPrune containers\n\nWhen you stop a container, it isn't automatically removed unless you started it with the --rm flag. To see all containers on the Docker host, including stopped containers, use docker ps -a. You may be surprised how many containers exist, especially on a development system! A stopped container's writable layers still take up disk space. To clean this up, you can use the docker container prune command.\n\n$ docker container prune\n\n\n\nWARNING! This will remove all stopped containers.\n\nAre you sure you want to continue? [y/N] y\n\n\nBy default, you're prompted to continue. To bypass the prompt, use the -f or --force flag.\n\nBy default, all stopped containers are removed. You can limit the scope using the --filter flag. For instance, the following command only removes stopped containers older than 24 hours:\n\n$ docker container prune --filter \"until=24h\"\n\n\nOther filtering expressions are available. See the docker container prune reference for more examples.\n\nPrune volumes\n\nVolumes can be used by one or more containers, and take up space on the Docker host. Volumes are never removed automatically, because to do so could destroy data.\n\n$ docker volume prune\n\n\n\nWARNING! This will remove all volumes not used by at least one container.\n\nAre you sure you want to continue? [y/N] y\n\n\nBy default, you are prompted to continue. To bypass the prompt, use the -f or --force flag.\n\nBy default, all unused volumes are removed. You can limit the scope using the --filter flag. For instance, the following command only removes volumes which aren't labelled with the keep label:\n\n$ docker volume prune --filter \"label!=keep\"\n\n\nOther filtering expressions are available. See the docker volume prune reference for more examples.\n\nPrune networks\n\nDocker networks don't take up much disk space, but they do create iptables rules, bridge network devices, and routing table entries. To clean these things up, you can use docker network prune to clean up networks which aren't used by any containers.\n\n$ docker network prune\n\n\n\nWARNING! This will remove all networks not used by at least one container.\n\nAre you sure you want to continue? [y/N] y\n\n\nBy default, you're prompted to continue. To bypass the prompt, use the -f or --force flag.\n\nBy default, all unused networks are removed. You can limit the scope using the --filter flag. For instance, the following command only removes networks older than 24 hours:\n\n$ docker network prune --filter \"until=24h\"\n\n\nOther filtering expressions are available. See the docker network prune reference for more examples.\n\nPrune everything\n\nThe docker system prune command is a shortcut that prunes images, containers, and networks. Volumes aren't pruned by default, and you must specify the --volumes flag for docker system prune to prune volumes.\n\n$ docker system prune\n\n\n\nWARNING! This will remove:\n\n        - all stopped containers\n\n        - all networks not used by at least one container\n\n        - all dangling images\n\n        - unused build cache\n\n\n\nAre you sure you want to continue? [y/N] y\n\n\nTo also prune volumes, add the --volumes flag:\n\n$ docker system prune --volumes\n\n\n\nWARNING! This will remove:\n\n        - all stopped containers\n\n        - all networks not used by at least one container\n\n        - all volumes not used by at least one container\n\n        - all dangling images\n\n        - all build cache\n\n\n\nAre you sure you want to continue? [y/N] y\n\n\nBy default, you're prompted to continue. To bypass the prompt, use the -f or --force flag.\n\nBy default, all unused containers, networks, and images are removed. You can limit the scope using the --filter flag. For instance, the following command removes items older than 24 hours:\n\n$ docker system prune --filter \"until=24h\"\n\n\nOther filtering expressions are available. See the docker system prune reference for more examples.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrune images\nPrune containers\nPrune volumes\nPrune networks\nPrune everything\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995306,
    "timestamp": "2026-02-07T06:32:16.428Z",
    "title": "Logs and metrics | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\nView container logs\nCopy as Markdown\n\nThe docker logs command shows information logged by a running container. The docker service logs command shows information logged by all containers participating in a service. The information that's logged and the format of the log depends almost entirely on the container's endpoint command.\n\nBy default, docker logs or docker service logs shows the command's output just as it would appear if you ran the command interactively in a terminal. Unix and Linux commands typically open three I/O streams when they run, called STDIN, STDOUT, and STDERR. STDIN is the command's input stream, which may include input from the keyboard or input from another command. STDOUT is usually a command's normal output, and STDERR is typically used to output error messages. By default, docker logs shows the command's STDOUT and STDERR. To read more about I/O and Linux, see the Linux Documentation Project article on I/O redirection.\n\nIn some cases, docker logs may not show useful information unless you take additional steps.\n\nIf you use a logging driver which sends logs to a file, an external host, a database, or another logging back-end, and have \"dual logging\" disabled, docker logs may not show useful information.\nIf your image runs a non-interactive process such as a web server or a database, that application may send its output to log files instead of STDOUT and STDERR.\n\nIn the first case, your logs are processed in other ways and you may choose not to use docker logs. In the second case, the official nginx image shows one workaround, and the official Apache httpd image shows another.\n\nThe official nginx image creates a symbolic link from /var/log/nginx/access.log to /dev/stdout, and creates another symbolic link from /var/log/nginx/error.log to /dev/stderr, overwriting the log files and causing logs to be sent to the relevant special device instead. See the Dockerfile.\n\nThe official httpd driver changes the httpd application's configuration to write its normal output directly to /proc/self/fd/1 (which is STDOUT) and its errors to /proc/self/fd/2 (which is STDERR). See the Dockerfile.\n\nNext steps\nConfigure logging drivers.\nWrite a Dockerfile.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995309,
    "timestamp": "2026-02-07T06:32:16.428Z",
    "title": "Configure logging drivers | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/configure/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nConfigure logging drivers\nConfigure logging drivers\nCopy as Markdown\n\nDocker includes multiple logging mechanisms to help you get information from running containers and services. These mechanisms are called logging drivers. Each Docker daemon has a default logging driver, which each container uses unless you configure it to use a different logging driver, or log driver for short.\n\nAs a default, Docker uses the json-file logging driver, which caches container logs as JSON internally. In addition to using the logging drivers included with Docker, you can also implement and use logging driver plugins.\n\nTip\n\nUse the local logging driver to prevent disk-exhaustion. By default, no log-rotation is performed. As a result, log-files stored by the default json-file logging driver logging driver can cause a significant amount of disk space to be used for containers that generate much output, which can lead to disk space exhaustion.\n\nDocker keeps the json-file logging driver (without log-rotation) as a default to remain backwards compatible with older versions of Docker, and for situations where Docker is used as runtime for Kubernetes.\n\nFor other situations, the local logging driver is recommended as it performs log-rotation by default, and uses a more efficient file format. Refer to the Configure the default logging driver section below to learn how to configure the local logging driver as a default, and the local file logging driver page for more details about the local logging driver.\n\nConfigure the default logging driver\n\nTo configure the Docker daemon to default to a specific logging driver, set the value of log-driver to the name of the logging driver in the daemon.json configuration file. Refer to the \"daemon configuration file\" section in the dockerd reference manual for details.\n\nThe default logging driver is json-file. The following example sets the default logging driver to the local log driver:\n\n{\n\n  \"log-driver\": \"local\"\n\n}\n\nIf the logging driver has configurable options, you can set them in the daemon.json file as a JSON object with the key log-opts. The following example sets four configurable options on the json-file logging driver:\n\n{\n\n  \"log-driver\": \"json-file\",\n\n  \"log-opts\": {\n\n    \"max-size\": \"10m\",\n\n    \"max-file\": \"3\",\n\n    \"labels\": \"production_status\",\n\n    \"env\": \"os,customer\"\n\n  }\n\n}\n\nRestart Docker for the changes to take effect for newly created containers. Existing containers don't use the new logging configuration automatically.\n\nNote\n\nlog-opts configuration options in the daemon.json configuration file must be provided as strings. Boolean and numeric values (such as the value for max-file in the example above) must therefore be enclosed in quotes (\").\n\nIf you don't specify a logging driver, the default is json-file. To find the current default logging driver for the Docker daemon, run docker info and search for Logging Driver. You can use the following command on Linux, macOS, or PowerShell on Windows:\n\n$ docker info --format '{{.LoggingDriver}}'\n\n\n\njson-file\n\nNote\n\nChanging the default logging driver or logging driver options in the daemon configuration only affects containers that are created after the configuration is changed. Existing containers retain the logging driver options that were used when they were created. To update the logging driver for a container, the container has to be re-created with the desired options. Refer to the configure the logging driver for a container section below to learn how to find the logging-driver configuration of a container.\n\nConfigure the logging driver for a container\n\nWhen you start a container, you can configure it to use a different logging driver than the Docker daemon's default, using the --log-driver flag. If the logging driver has configurable options, you can set them using one or more instances of the --log-opt <NAME>=<VALUE> flag. Even if the container uses the default logging driver, it can use different configurable options.\n\nThe following example starts an Alpine container with the none logging driver.\n\n$ docker run -it --log-driver none alpine ash\n\n\nTo find the current logging driver for a running container, if the daemon is using the json-file logging driver, run the following docker inspect command, substituting the container name or ID for <CONTAINER>:\n\n$ docker inspect -f '{{.HostConfig.LogConfig.Type}}' CONTAINER\n\n\n\njson-file\n\nConfigure the delivery mode of log messages from container to log driver\n\nDocker provides two modes for delivering messages from the container to the log driver:\n\n(default) direct, blocking delivery from container to driver\nnon-blocking delivery that stores log messages in an intermediate per-container buffer for consumption by driver\n\nThe non-blocking message delivery mode prevents applications from blocking due to logging back pressure. Applications are likely to fail in unexpected ways when STDERR or STDOUT streams block.\n\nWarning\n\nWhen the buffer is full, new messages will not be enqueued. Dropping messages is often preferred to blocking the log-writing process of an application.\n\nThe mode log option controls whether to use the blocking (default) or non-blocking message delivery.\n\nThe max-buffer-size controls the size of the buffer used for intermediate message storage when mode is set to non-blocking. The default is 1m meaning 1 MB (1 million bytes). See function FromHumanSize() in the go-units package for the allowed format strings, some examples are 1KiB for 1024 bytes, 2g for 2 billion bytes.\n\nThe following example starts an Alpine container with log output in non-blocking mode and a 4 megabyte buffer:\n\n$ docker run -it --log-opt mode=non-blocking --log-opt max-buffer-size=4m alpine ping 127.0.0.1\n\nUse environment variables or labels with logging drivers\n\nSome logging drivers add the value of a container's --env|-e or --label flags to the container's logs. This example starts a container using the Docker daemon's default logging driver (in the following example, json-file) but sets the environment variable os=ubuntu.\n\n$ docker run -dit --label production_status=testing -e os=ubuntu alpine sh\n\n\nIf the logging driver supports it, this adds additional fields to the logging output. The following output is generated by the json-file logging driver:\n\n\"attrs\":{\"production_status\":\"testing\",\"os\":\"ubuntu\"}\nSupported logging drivers\n\nThe following logging drivers are supported. See the link to each driver's documentation for its configurable options, if applicable. If you are using logging driver plugins, you may see more options.\n\nDriver\tDescription\nnone\tNo logs are available for the container and docker logs does not return any output.\nlocal\tLogs are stored in a custom format designed for minimal overhead.\njson-file\tThe logs are formatted as JSON. The default logging driver for Docker.\nsyslog\tWrites logging messages to the syslog facility. The syslog daemon must be running on the host machine.\njournald\tWrites log messages to journald. The journald daemon must be running on the host machine.\ngelf\tWrites log messages to a Graylog Extended Log Format (GELF) endpoint such as Graylog or Logstash.\nfluentd\tWrites log messages to fluentd (forward input). The fluentd daemon must be running on the host machine.\nawslogs\tWrites log messages to Amazon CloudWatch Logs.\nsplunk\tWrites log messages to splunk using the HTTP Event Collector.\netwlogs\tWrites log messages as Event Tracing for Windows (ETW) events. Only available on Windows platforms.\ngcplogs\tWrites log messages to Google Cloud Platform (GCP) Logging.\nLimitations of logging drivers\nReading log information requires decompressing rotated log files, which causes a temporary increase in disk usage (until the log entries from the rotated files are read) and an increased CPU usage while decompressing.\nThe capacity of the host storage where the Docker data directory resides determines the maximum size of the log file information.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nConfigure the default logging driver\nConfigure the logging driver for a container\nConfigure the delivery mode of log messages from container to log driver\nUse environment variables or labels with logging drivers\nSupported logging drivers\nLimitations of logging drivers\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995312,
    "timestamp": "2026-02-07T06:32:16.435Z",
    "title": "Customize log driver output | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/log_tags/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nCustomize log driver output\nCustomize log driver output\nCopy as Markdown\n\nThe tag log option specifies how to format a tag that identifies the container's log messages. By default, the system uses the first 12 characters of the container ID. To override this behavior, specify a tag option:\n\n$ docker run --log-driver=fluentd --log-opt fluentd-address=myhost.local:24224 --log-opt tag=\"mailer\"\n\n\nDocker supports some special template markup you can use when specifying a tag's value:\n\nMarkup\tDescription\n{{.ID}}\tThe first 12 characters of the container ID.\n{{.FullID}}\tThe full container ID.\n{{.Name}}\tThe container name.\n{{.ImageID}}\tThe first 12 characters of the container's image ID.\n{{.ImageFullID}}\tThe container's full image ID.\n{{.ImageName}}\tThe name of the image used by the container.\n{{.DaemonName}}\tThe name of the Docker program (docker).\n\nFor example, specifying a --log-opt tag=\"{{.ImageName}}/{{.Name}}/{{.ID}}\" value yields syslog log lines like:\n\nAug  7 18:33:19 HOSTNAME hello-world/foobar/5790672ab6a0[9103]: Hello from Docker.\n\nAt startup time, the system sets the container_name field and {{.Name}} in the tags. If you use docker rename to rename a container, the new name isn't reflected in the log messages. Instead, these messages continue to use the original container name.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995318,
    "timestamp": "2026-02-07T06:32:16.437Z",
    "title": "ETW logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/etwlogs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nETW logging driver\nETW logging driver\nCopy as Markdown\n\nThe Event Tracing for Windows (ETW) logging driver forwards container logs as ETW events. ETW stands for Event Tracing in Windows, and is the common framework for tracing applications in Windows. Each ETW event contains a message with both the log and its context information. A client can then create an ETW listener to listen to these events.\n\nThe ETW provider that this logging driver registers with Windows, has the GUID identifier of: {a3693192-9ed6-46d2-a981-f8226c8363bd}. A client creates an ETW listener and registers to listen to events from the logging driver's provider. It doesn't matter the order in which the provider and listener are created. A client can create their ETW listener and start listening for events from the provider, before the provider has been registered with the system.\n\nUsage\n\nHere is an example of how to listen to these events using the logman utility program included in most installations of Windows:\n\nlogman start -ets DockerContainerLogs -p \"{a3693192-9ed6-46d2-a981-f8226c8363bd}\" 0x0 -o trace.etl\nRun your container(s) with the etwlogs driver, by adding --log-driver=etwlogs to the Docker run command, and generate log messages.\nlogman stop -ets DockerContainerLogs\nThis generates an etl file that contains the events. One way to convert this file into human-readable form is to run: tracerpt -y trace.etl.\n\nEach ETW event contains a structured message string in this format:\n\ncontainer_name: %s, image_name: %s, container_id: %s, image_id: %s, source: [stdout | stderr], log: %s\n\nDetails on each item in the message can be found below:\n\nField\tDescription\ncontainer_name\tThe container name at the time it was started.\nimage_name\tThe name of the container's image.\ncontainer_id\tThe full 64-character container ID.\nimage_id\tThe full ID of the container's image.\nsource\tstdout or stderr.\nlog\tThe container log message.\n\nHere is an example event message (output formatted for readability):\n\ncontainer_name: backstabbing_spence,\n\nimage_name: windowsservercore,\n\ncontainer_id: f14bb55aa862d7596b03a33251c1be7dbbec8056bbdead1da8ec5ecebbe29731,\n\nimage_id: sha256:2f9e19bd998d3565b4f345ac9aaf6e3fc555406239a4fb1b1ba879673713824b,\n\nsource: stdout,\n\nlog: Hello world!\n\nA client can parse this message string to get both the log message, as well as its context information. The timestamp is also available within the ETW event.\n\nNote\n\nThis ETW provider only emits a message string, and not a specially structured ETW event. Therefore, you don't have to register a manifest file with the system to read and interpret its ETW events.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995315,
    "timestamp": "2026-02-07T06:32:16.438Z",
    "title": "Amazon CloudWatch Logs logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/awslogs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nAmazon CloudWatch Logs logging driver\nAmazon CloudWatch Logs logging driver\nCopy as Markdown\n\nThe awslogs logging driver sends container logs to Amazon CloudWatch Logs. Log entries can be retrieved through the AWS Management Console or the AWS SDKs and Command Line Tools.\n\nUsage\n\nTo use the awslogs driver as the default logging driver, set the log-driver and log-opt keys to appropriate values in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config\\daemon.json on Windows Server. For more about configuring Docker using daemon.json, see daemon.json. The following example sets the log driver to awslogs and sets the awslogs-region option.\n\n{\n\n  \"log-driver\": \"awslogs\",\n\n  \"log-opts\": {\n\n    \"awslogs-region\": \"us-east-1\"\n\n  }\n\n}\n\nRestart Docker for the changes to take effect.\n\nYou can set the logging driver for a specific container by using the --log-driver option to docker run:\n\n$ docker run --log-driver=awslogs ...\n\n\nIf you are using Docker Compose, set awslogs using the following declaration example:\n\nmyservice:\n\n  logging:\n\n    driver: awslogs\n\n    options:\n\n      awslogs-region: us-east-1\nAmazon CloudWatch Logs options\n\nYou can add logging options to the daemon.json to set Docker-wide defaults, or use the --log-opt NAME=VALUE flag to specify Amazon CloudWatch Logs logging driver options when starting a container.\n\nawslogs-region\n\nThe awslogs logging driver sends your Docker logs to a specific region. Use the awslogs-region log option or the AWS_REGION environment variable to set the region. By default, if your Docker daemon is running on an EC2 instance and no region is set, the driver uses the instance's region.\n\n$ docker run --log-driver=awslogs --log-opt awslogs-region=us-east-1 ...\n\nawslogs-endpoint\n\nBy default, Docker uses either the awslogs-region log option or the detected region to construct the remote CloudWatch Logs API endpoint. Use the awslogs-endpoint log option to override the default endpoint with the provided endpoint.\n\nNote\n\nThe awslogs-region log option or detected region controls the region used for signing. You may experience signature errors if the endpoint you've specified with awslogs-endpoint uses a different region.\n\nawslogs-group\n\nYou must specify a log group for the awslogs logging driver. You can specify the log group with the awslogs-group log option:\n\n$ docker run --log-driver=awslogs --log-opt awslogs-region=us-east-1 --log-opt awslogs-group=myLogGroup ...\n\nawslogs-stream\n\nTo configure which log stream should be used, you can specify the awslogs-stream log option. If not specified, the container ID is used as the log stream.\n\nNote\n\nLog streams within a given log group should only be used by one container at a time. Using the same log stream for multiple containers concurrently can cause reduced logging performance.\n\nawslogs-create-group\n\nLog driver returns an error by default if the log group doesn't exist. However, you can set the awslogs-create-group to true to automatically create the log group as needed. The awslogs-create-group option defaults to false.\n\n$ docker run \\\n\n    --log-driver=awslogs \\\n\n    --log-opt awslogs-region=us-east-1 \\\n\n    --log-opt awslogs-group=myLogGroup \\\n\n    --log-opt awslogs-create-group=true \\\n\n    ...\n\nNote\n\nYour AWS IAM policy must include the logs:CreateLogGroup permission before you attempt to use awslogs-create-group.\n\nawslogs-create-stream\n\nBy default, the log driver creates the AWS CloudWatch Logs stream used for container log persistence.\n\nSet awslogs-create-stream to false to disable log stream creation. When disabled, the Docker daemon assumes the log stream already exists. A use case where this is beneficial is when log stream creation is handled by another process avoiding redundant AWS CloudWatch Logs API calls.\n\nIf awslogs-create-stream is set to false and the log stream does not exist, log persistence to CloudWatch fails during container runtime, resulting in Failed to put log events error messages in daemon logs.\n\n$ docker run \\\n\n    --log-driver=awslogs \\\n\n    --log-opt awslogs-region=us-east-1 \\\n\n    --log-opt awslogs-group=myLogGroup \\\n\n    --log-opt awslogs-stream=myLogStream \\\n\n    --log-opt awslogs-create-stream=false \\\n\n    ...\n\nawslogs-datetime-format\n\nThe awslogs-datetime-format option defines a multi-line start pattern in Python strftime format. A log message consists of a line that matches the pattern and any following lines that don't match the pattern. Thus the matched line is the delimiter between log messages.\n\nOne example of a use case for using this format is for parsing output such as a stack dump, which might otherwise be logged in multiple entries. The correct pattern allows it to be captured in a single entry.\n\nThis option always takes precedence if both awslogs-datetime-format and awslogs-multiline-pattern are configured.\n\nNote\n\nMulti-line logging performs regular expression parsing and matching of all log messages, which may have a negative impact on logging performance.\n\nConsider the following log stream, where new log messages start with a timestamp:\n\n[May 01, 2017 19:00:01] A message was logged\n\n[May 01, 2017 19:00:04] Another multi-line message was logged\n\nSome random message\n\nwith some random words\n\n[May 01, 2017 19:01:32] Another message was logged\n\n\nThe format can be expressed as a strftime expression of [%b %d, %Y %H:%M:%S], and the awslogs-datetime-format value can be set to that expression:\n\n$ docker run \\\n\n    --log-driver=awslogs \\\n\n    --log-opt awslogs-region=us-east-1 \\\n\n    --log-opt awslogs-group=myLogGroup \\\n\n    --log-opt awslogs-datetime-format='\\[%b %d, %Y %H:%M:%S\\]' \\\n\n    ...\n\n\nThis parses the logs into the following CloudWatch log events:\n\n# First event\n\n[May 01, 2017 19:00:01] A message was logged\n\n\n\n# Second event\n\n[May 01, 2017 19:00:04] Another multi-line message was logged\n\nSome random message\n\nwith some random words\n\n\n\n# Third event\n\n[May 01, 2017 19:01:32] Another message was logged\n\n\nThe following strftime codes are supported:\n\nCode\tMeaning\tExample\n%a\tWeekday abbreviated name.\tMon\n%A\tWeekday full name.\tMonday\n%w\tWeekday as a decimal number where 0 is Sunday and 6 is Saturday.\t0\n%d\tDay of the month as a zero-padded decimal number.\t08\n%b\tMonth abbreviated name.\tFeb\n%B\tMonth full name.\tFebruary\n%m\tMonth as a zero-padded decimal number.\t02\n%Y\tYear with century as a decimal number.\t2008\n%y\tYear without century as a zero-padded decimal number.\t08\n%H\tHour (24-hour clock) as a zero-padded decimal number.\t19\n%I\tHour (12-hour clock) as a zero-padded decimal number.\t07\n%p\tAM or PM.\tAM\n%M\tMinute as a zero-padded decimal number.\t57\n%S\tSecond as a zero-padded decimal number.\t04\n%f\tMicroseconds as a zero-padded decimal number.\t000345\n%z\tUTC offset in the form +HHMM or -HHMM.\t+1300\n%Z\tTime zone name.\tPST\n%j\tDay of the year as a zero-padded decimal number.\t363\n\nIn addition, the following non-strftime codes are supported:\n\nCode\tMeaning\tExample\n%L\tMilliseconds as a zero-padded decimal number preceded with a period.\t.123\nawslogs-multiline-pattern\n\nThe awslogs-multiline-pattern option defines a multi-line start pattern using a regular expression. A log message consists of a line that matches the pattern and any following lines that don't match the pattern. Thus the matched line is the delimiter between log messages.\n\nThis option is ignored if awslogs-datetime-format is also configured.\n\nNote\n\nMulti-line logging performs regular expression parsing and matching of all log messages. This may have a negative impact on logging performance.\n\nConsider the following log stream, where each log message should start with the pattern INFO:\n\nINFO A message was logged\n\nINFO Another multi-line message was logged\n\n     Some random message\n\nINFO Another message was logged\n\n\nYou can use the regular expression of ^INFO:\n\n$ docker run \\\n\n    --log-driver=awslogs \\\n\n    --log-opt awslogs-region=us-east-1 \\\n\n    --log-opt awslogs-group=myLogGroup \\\n\n    --log-opt awslogs-multiline-pattern='^INFO' \\\n\n    ...\n\n\nThis parses the logs into the following CloudWatch log events:\n\n# First event\n\nINFO A message was logged\n\n\n\n# Second event\n\nINFO Another multi-line message was logged\n\n     Some random message\n\n\n\n# Third event\n\nINFO Another message was logged\n\ntag\n\nSpecify tag as an alternative to the awslogs-stream option. tag interprets Go template markup, such as {{.ID}}, {{.FullID}} or {{.Name}} docker.{{.ID}}. See the tag option documentation for details on supported template substitutions.\n\nWhen both awslogs-stream and tag are specified, the value supplied for awslogs-stream overrides the template specified with tag.\n\nIf not specified, the container ID is used as the log stream.\n\nNote\n\nThe CloudWatch log API doesn't support : in the log name. This can cause some issues when using the {{ .ImageName }} as a tag, since a Docker image has a format of IMAGE:TAG, such as alpine:latest. Template markup can be used to get the proper format. To get the image name and the first 12 characters of the container ID, you can use:\n\n--log-opt tag='{{ with split .ImageName \":\" }}{{join . \"_\"}}{{end}}-{{.ID}}'\n\nthe output is something like: alpine_latest-bf0072049c76\n\nawslogs-force-flush-interval-seconds\n\nThe awslogs driver periodically flushes logs to CloudWatch.\n\nThe awslogs-force-flush-interval-seconds option changes log flush interval seconds.\n\nDefault is 5 seconds.\n\nawslogs-max-buffered-events\n\nThe awslogs driver buffers logs.\n\nThe awslogs-max-buffered-events option changes log buffer size.\n\nDefault is 4K.\n\nCredentials\n\nYou must provide AWS credentials to the Docker daemon to use the awslogs logging driver. You can provide these credentials with the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_SESSION_TOKEN environment variables, the default AWS shared credentials file (~/.aws/credentials of the root user), or if you are running the Docker daemon on an Amazon EC2 instance, the Amazon EC2 instance profile.\n\nCredentials must have a policy applied that allows the logs:CreateLogStream and logs:PutLogEvents actions, as shown in the following example.\n\n{\n\n  \"Version\": \"2012-10-17\",\n\n  \"Statement\": [\n\n    {\n\n      \"Action\": [\"logs:CreateLogStream\", \"logs:PutLogEvents\"],\n\n      \"Effect\": \"Allow\",\n\n      \"Resource\": \"*\"\n\n    }\n\n  ]\n\n}\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\nAmazon CloudWatch Logs options\nawslogs-region\nawslogs-endpoint\nawslogs-group\nawslogs-stream\nawslogs-create-group\nawslogs-create-stream\nawslogs-datetime-format\nawslogs-multiline-pattern\ntag\nawslogs-force-flush-interval-seconds\nawslogs-max-buffered-events\nCredentials\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995321,
    "timestamp": "2026-02-07T06:32:16.444Z",
    "title": "Fluentd logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/fluentd/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nFluentd logging driver\nFluentd logging driver\nCopy as Markdown\n\nThe fluentd logging driver sends container logs to the Fluentd collector as structured log data. Then, users can use any of the various output plugins of Fluentd to write these logs to various destinations.\n\nIn addition to the log message itself, the fluentd log driver sends the following metadata in the structured log message:\n\nField\tDescription\ncontainer_id\tThe full 64-character container ID.\ncontainer_name\tThe container name at the time it was started. If you use docker rename to rename a container, the new name isn't reflected in the journal entries.\nsource\tstdout or stderr\nlog\tThe container log\nUsage\n\nSome options are supported by specifying --log-opt as many times as needed:\n\nfluentd-address: specify a socket address to connect to the Fluentd daemon, ex fluentdhost:24224 or unix:///path/to/fluentd.sock.\ntag: specify a tag for Fluentd messages. Supports some Go template markup, ex {{.ID}}, {{.FullID}} or {{.Name}} docker.{{.ID}}.\n\nTo use the fluentd driver as the default logging driver, set the log-driver and log-opt keys to appropriate values in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config\\daemon.json on Windows Server. For more about configuring Docker using daemon.json, see daemon.json.\n\nThe following example sets the log driver to fluentd and sets the fluentd-address option.\n\n{\n\n  \"log-driver\": \"fluentd\",\n\n  \"log-opts\": {\n\n    \"fluentd-address\": \"fluentdhost:24224\"\n\n  }\n\n}\n\nRestart Docker for the changes to take effect.\n\nNote\n\nlog-opts configuration options in the daemon.json configuration file must be provided as strings. Boolean and numeric values (such as the value for fluentd-async or fluentd-max-retries) must therefore be enclosed in quotes (\").\n\nTo set the logging driver for a specific container, pass the --log-driver option to docker run:\n\n$ docker run --log-driver=fluentd ...\n\n\nBefore using this logging driver, launch a Fluentd daemon. The logging driver connects to this daemon through localhost:24224 by default. Use the fluentd-address option to connect to a different address.\n\n$ docker run --log-driver=fluentd --log-opt fluentd-address=fluentdhost:24224\n\n\nIf container cannot connect to the Fluentd daemon, the container stops immediately unless the fluentd-async option is used.\n\nOptions\n\nUsers can use the --log-opt NAME=VALUE flag to specify additional Fluentd logging driver options.\n\nfluentd-address\n\nBy default, the logging driver connects to localhost:24224. Supply the fluentd-address option to connect to a different address. tcp(default) and unix sockets are supported.\n\n$ docker run --log-driver=fluentd --log-opt fluentd-address=fluentdhost:24224\n\n$ docker run --log-driver=fluentd --log-opt fluentd-address=tcp://fluentdhost:24224\n\n$ docker run --log-driver=fluentd --log-opt fluentd-address=unix:///path/to/fluentd.sock\n\n\nTwo of the above specify the same address, because tcp is default.\n\ntag\n\nBy default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format.\n\nlabels, labels-regex, env, and env-regex\n\nThe labels and env options each take a comma-separated list of keys. If there is collision between label and env keys, the value of the env takes precedence. Both options add additional fields to the extra attributes of a logging message.\n\nThe env-regex and labels-regex options are similar to and compatible with respectively env and labels. Their values are regular expressions to match logging-related environment variables and labels. It is used for advanced log tag options.\n\nfluentd-async\n\nDocker connects to Fluentd in the background. Messages are buffered until the connection is established. Defaults to false.\n\nfluentd-async-reconnect-interval\n\nWhen fluentd-async is enabled, the fluentd-async-reconnect-interval option defines the interval, in milliseconds, at which the connection to fluentd-address is re-established. This option is useful if the address resolves to one or more IP addresses, for example a Consul service address.\n\nfluentd-buffer-limit\n\nSets the number of events buffered on the memory. Records will be stored in memory up to this number. If the buffer is full, the call to record logs will fail. The default is 1048576. (https://github.com/fluent/fluent-logger-golang/tree/master#bufferlimit)\n\nfluentd-retry-wait\n\nHow long to wait between retries. Defaults to 1 second.\n\nfluentd-max-retries\n\nThe maximum number of retries. Defaults to 4294967295 (2**32 - 1).\n\nfluentd-sub-second-precision\n\nGenerates event logs in nanosecond resolution. Defaults to false.\n\nfluentd-write-timeout\n\nSets the timeout for the write call to the fluentd daemon. By default, writes have no timeout and will block indefinitely.\n\nFluentd daemon management with Docker\n\nAbout Fluentd itself, see the project webpage and its documents.\n\nTo use this logging driver, start the fluentd daemon on a host. We recommend that you use the Fluentd docker image. This image is especially useful if you want to aggregate multiple container logs on each host then, later, transfer the logs to another Fluentd node to create an aggregate store.\n\nTest container loggers\n\nWrite a configuration file (test.conf) to dump input logs:\n\n<source>\n\n  @type forward\n\n</source>\n\n\n\n<match *>\n\n  @type stdout\n\n</match>\n\nLaunch Fluentd container with this configuration file:\n\n$ docker run -it -p 24224:24224 -v /path/to/conf/test.conf:/fluentd/etc/test.conf -e FLUENTD_CONF=test.conf fluent/fluentd:latest\n\n\nStart one or more containers with the fluentd logging driver:\n\n$ docker run --log-driver=fluentd your/application\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\nOptions\nfluentd-address\ntag\nlabels, labels-regex, env, and env-regex\nfluentd-async\nfluentd-async-reconnect-interval\nfluentd-buffer-limit\nfluentd-retry-wait\nfluentd-max-retries\nfluentd-sub-second-precision\nfluentd-write-timeout\nFluentd daemon management with Docker\nTest container loggers\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995324,
    "timestamp": "2026-02-07T06:32:16.447Z",
    "title": "Google Cloud Logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/gcplogs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nGoogle Cloud Logging driver\nGoogle Cloud Logging driver\nCopy as Markdown\n\nThe Google Cloud Logging driver sends container logs to Google Cloud Logging Logging.\n\nUsage\n\nTo use the gcplogs driver as the default logging driver, set the log-driver and log-opt keys to appropriate values in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config\\daemon.json on Windows Server. For more about configuring Docker using daemon.json, see daemon.json.\n\nThe following example sets the log driver to gcplogs and sets the gcp-meta-name option.\n\n{\n\n  \"log-driver\": \"gcplogs\",\n\n  \"log-opts\": {\n\n    \"gcp-meta-name\": \"example-instance-12345\"\n\n  }\n\n}\n\nRestart Docker for the changes to take effect.\n\nYou can set the logging driver for a specific container by using the --log-driver option to docker run:\n\n$ docker run --log-driver=gcplogs ...\n\n\nIf Docker detects that it's running in a Google Cloud Project, it discovers configuration from the instance metadata service. Otherwise, the user must specify which project to log to using the --gcp-project log option and Docker attempts to obtain credentials from the Google Application Default Credential. The --gcp-project flag takes precedence over information discovered from the metadata server, so a Docker daemon running in a Google Cloud project can be overridden to log to a different project using --gcp-project.\n\nDocker fetches the values for zone, instance name and instance ID from Google Cloud metadata server. Those values can be provided via options if metadata server isn't available. They don't override the values from metadata server.\n\ngcplogs options\n\nYou can use the --log-opt NAME=VALUE flag to specify these additional Google Cloud Logging driver options:\n\nOption\tRequired\tDescription\ngcp-project\toptional\tWhich Google Cloud project to log to. Defaults to discovering this value from the Google Cloud metadata server.\ngcp-log-cmd\toptional\tWhether to log the command that the container was started with. Defaults to false.\nlabels\toptional\tComma-separated list of keys of labels, which should be included in message, if these labels are specified for the container.\nlabels-regex\toptional\tSimilar to and compatible with labels. A regular expression to match logging-related labels. Used for advanced log tag options.\nenv\toptional\tComma-separated list of keys of environment variables, which should be included in message, if these variables are specified for the container.\nenv-regex\toptional\tSimilar to and compatible with env. A regular expression to match logging-related environment variables. Used for advanced log tag options.\ngcp-meta-zone\toptional\tZone name for the instance.\ngcp-meta-name\toptional\tInstance name.\ngcp-meta-id\toptional\tInstance ID.\n\nIf there is collision between label and env keys, the value of the env takes precedence. Both options add additional fields to the attributes of a logging message.\n\nThe following is an example of the logging options required to log to the default logging destination which is discovered by querying the Google Cloud metadata server.\n\n$ docker run \\\n\n    --log-driver=gcplogs \\\n\n    --log-opt labels=location \\\n\n    --log-opt env=TEST \\\n\n    --log-opt gcp-log-cmd=true \\\n\n    --env \"TEST=false\" \\\n\n    --label location=west \\\n\n    your/application\n\n\nThis configuration also directs the driver to include in the payload the label location, the environment variable ENV, and the command used to start the container.\n\nThe following example shows logging options for running outside of Google Cloud. The GOOGLE_APPLICATION_CREDENTIALS environment variable must be set for the daemon, for example via systemd:\n\n[Service]\n\nEnvironment=\"GOOGLE_APPLICATION_CREDENTIALS=uQWVCPkMTI34bpssr1HI\"\n$ docker run \\\n\n    --log-driver=gcplogs \\\n\n    --log-opt gcp-project=test-project \\\n\n    --log-opt gcp-meta-zone=west1 \\\n\n    --log-opt gcp-meta-name=`hostname` \\\n\n    your/application\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\ngcplogs options\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995325,
    "timestamp": "2026-02-07T06:32:16.447Z",
    "title": "Graylog Extended Format logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/gelf/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nGraylog Extended Format logging driver\nGraylog Extended Format logging driver\nCopy as Markdown\n\nThe gelf logging driver is a convenient format that's understood by a number of tools such as Graylog, Logstash, and Fluentd. Many tools use this format.\n\nIn GELF, every log message is a dict with the following fields:\n\nVersion\nHost (who sent the message in the first place)\nTimestamp\nShort and long version of the message\nAny custom fields you configure yourself\nUsage\n\nTo use the gelf driver as the default logging driver, set the log-driver and log-opt keys to appropriate values in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config\\daemon.json on Windows Server. For more about configuring Docker using daemon.json, see daemon.json.\n\nThe following example sets the log driver to gelf and sets the gelf-address option.\n\n{\n\n  \"log-driver\": \"gelf\",\n\n  \"log-opts\": {\n\n    \"gelf-address\": \"udp://1.2.3.4:12201\"\n\n  }\n\n}\n\nRestart Docker for the changes to take effect.\n\nNote\n\nlog-opts configuration options in the daemon.json configuration file must be provided as strings. Boolean and numeric values (such as the value for gelf-tcp-max-reconnect) must therefore be enclosed in quotes (\").\n\nYou can set the logging driver for a specific container by setting the --log-driver flag when using docker container create or docker run:\n\n$ docker run \\\n\n      --log-driver gelf --log-opt gelf-address=udp://1.2.3.4:12201 \\\n\n      alpine echo hello world\n\nGELF options\n\nThe gelf logging driver supports the following options:\n\nOption\tRequired\tDescription\tExample value\ngelf-address\trequired\tThe address of the GELF server. tcp and udp are the only supported URI specifier and you must specify the port.\t--log-opt gelf-address=udp://192.168.0.42:12201\ngelf-compression-type\toptional\tUDP Only The type of compression the GELF driver uses to compress each log message. Allowed values are gzip, zlib and none. The default is gzip. Note that enabled compression leads to excessive CPU usage, so it's highly recommended to set this to none.\t--log-opt gelf-compression-type=gzip\ngelf-compression-level\toptional\tUDP Only The level of compression when gzip or zlib is the gelf-compression-type. An integer in the range of -1 to 9 (BestCompression). Default value is 1 (BestSpeed). Higher levels provide more compression at lower speed. Either -1 or 0 disables compression.\t--log-opt gelf-compression-level=2\ngelf-tcp-max-reconnect\toptional\tTCP Only The maximum number of reconnection attempts when the connection drop. A positive integer. Default value is 3.\t--log-opt gelf-tcp-max-reconnect=3\ngelf-tcp-reconnect-delay\toptional\tTCP Only The number of seconds to wait between reconnection attempts. A positive integer. Default value is 1.\t--log-opt gelf-tcp-reconnect-delay=1\ntag\toptional\tA string that's appended to the APP-NAME in the gelf message. By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format.\t--log-opt tag=mailer\nlabels\toptional\tApplies when starting the Docker daemon. A comma-separated list of logging-related labels this daemon accepts. Adds additional key on the extra fields, prefixed by an underscore (_). Used for advanced log tag options.\t--log-opt labels=production_status,geo\nlabels-regex\toptional\tSimilar to and compatible with labels. A regular expression to match logging-related labels. Used for advanced log tag options.\t--log-opt labels-regex=^(production_status|geo)\nenv\toptional\tApplies when starting the Docker daemon. A comma-separated list of logging-related environment variables this daemon accepts. Adds additional key on the extra fields, prefixed by an underscore (_). Used for advanced log tag options.\t--log-opt env=os,customer\nenv-regex\toptional\tSimilar to and compatible with env. A regular expression to match logging-related environment variables. Used for advanced log tag options.\t--log-opt env-regex=^(os|customer)\nNote\n\nThe gelf driver doesn't support TLS for TCP connections. Messages sent to TLS-protected inputs can silently fail.\n\nExamples\n\nThis example configures the container to use the GELF server running at 192.168.0.42 on port 12201.\n\n$ docker run -dit \\\n\n    --log-driver=gelf \\\n\n    --log-opt gelf-address=udp://192.168.0.42:12201 \\\n\n    alpine sh\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\nGELF options\nExamples\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995328,
    "timestamp": "2026-02-07T06:32:16.453Z",
    "title": "Journald logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/journald/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nJournald logging driver\nJournald logging driver\nCopy as Markdown\n\nThe journald logging driver sends container logs to the systemd journal. Log entries can be retrieved using the journalctl command, through use of the journal API, or using the docker logs command.\n\nIn addition to the text of the log message itself, the journald log driver stores the following metadata in the journal with each message:\n\nField\tDescription\nCONTAINER_ID\tThe container ID truncated to 12 characters.\nCONTAINER_ID_FULL\tThe full 64-character container ID.\nCONTAINER_NAME\tThe container name at the time it was started. If you use docker rename to rename a container, the new name isn't reflected in the journal entries.\nCONTAINER_TAG, SYSLOG_IDENTIFIER\tThe container tag (log tag option documentation).\nCONTAINER_PARTIAL_MESSAGE\tA field that flags log integrity. Improve logging of long log lines.\nIMAGE_NAME\tThe name of the container image.\nUsage\n\nTo use the journald driver as the default logging driver, set the log-driver and log-opts keys to appropriate values in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config\\daemon.json on Windows Server. For more about configuring Docker using daemon.json, see daemon.json.\n\nThe following example sets the log driver to journald:\n\n{\n\n  \"log-driver\": \"journald\"\n\n}\n\nRestart Docker for the changes to take effect.\n\nTo configure the logging driver for a specific container, use the --log-driver flag on the docker run command.\n\n$ docker run --log-driver=journald ...\n\nOptions\n\nUse the --log-opt NAME=VALUE flag to specify additional journald logging driver options.\n\nOption\tRequired\tDescription\ntag\toptional\tSpecify template to set CONTAINER_TAG and SYSLOG_IDENTIFIER value in journald logs. Refer to log tag option documentation to customize the log tag format.\nlabels\toptional\tComma-separated list of keys of labels, which should be included in message, if these labels are specified for the container.\nlabels-regex\toptional\tSimilar to and compatible with labels. A regular expression to match logging-related labels. Used for advanced¬†log tag options.\nenv\toptional\tComma-separated list of keys of environment variables, which should be included in message, if these variables are specified for the container.\nenv-regex\toptional\tSimilar to and compatible with env. A regular expression to match logging-related environment variables. Used for advanced¬†log tag options.\n\nIf a collision occurs between label and env options, the value of the env takes precedence. Each option adds additional fields to the attributes of a logging message.\n\nThe following is an example of the logging options required to log to journald.\n\n$ docker run \\\n\n    --log-driver=journald \\\n\n    --log-opt labels=location \\\n\n    --log-opt env=TEST \\\n\n    --env \"TEST=false\" \\\n\n    --label location=west \\\n\n    your/application\n\n\nThis configuration also directs the driver to include in the payload the label location, and the environment variable TEST. If the --env \"TEST=false\" or --label location=west arguments were omitted, the corresponding key would not be set in the journald log.\n\nNote regarding container names\n\nThe value logged in the CONTAINER_NAME field is the name of the container that was set at startup. If you use docker rename to rename a container, the new name isn't reflected in the journal entries. Journal entries continue to use the original name.\n\nRetrieve log messages with journalctl\n\nUse the journalctl command to retrieve log messages. You can apply filter expressions to limit the retrieved messages to those associated with a specific container:\n\n$ sudo journalctl CONTAINER_NAME=webserver\n\n\nYou can use additional filters to further limit the messages retrieved. The -b flag only retrieves messages generated since the last system boot:\n\n$ sudo journalctl -b CONTAINER_NAME=webserver\n\n\nThe -o flag specifies the format for the retrieved log messages. Use -o json to return the log messages in JSON format.\n\n$ sudo journalctl -o json CONTAINER_NAME=webserver\n\nView logs for a container with a TTY enabled\n\nIf TTY is enabled on a container you may see [10B blob data] in the output when retrieving log messages. The reason for that is that \\r is appended to the end of the line and journalctl doesn't strip it automatically unless --all is set:\n\n$ sudo journalctl -b CONTAINER_NAME=webserver --all\n\nRetrieve log messages with the journal API\n\nThis example uses the systemd Python module to retrieve container logs:\n\nimport systemd.journal\n\n\n\nreader = systemd.journal.Reader()\n\nreader.add_match('CONTAINER_NAME=web')\n\n\n\nfor msg in reader:\n\n    print '{CONTAINER_ID_FULL}: {MESSAGE}'.format(**msg)\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\nOptions\nNote regarding container names\nRetrieve log messages with journalctl\nView logs for a container with a TTY enabled\nRetrieve log messages with the journal API\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995333,
    "timestamp": "2026-02-07T06:32:16.458Z",
    "title": "JSON File logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/json-file/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nJSON File logging driver\nJSON File logging driver\nCopy as Markdown\n\nBy default, Docker captures the standard output (and standard error) of all your containers, and writes them in files using the JSON format. The JSON format annotates each line with its origin (stdout or stderr) and its timestamp. Each log file contains information about only one container.\n\n{\n\n  \"log\": \"Log line is here\\n\",\n\n  \"stream\": \"stdout\",\n\n  \"time\": \"2019-01-01T11:11:11.111111111Z\"\n\n}\nWarning\n\nThe json-file logging driver uses file-based storage. These files are designed to be exclusively accessed by the Docker daemon. Interacting with these files with external tools may interfere with Docker's logging system and result in unexpected behavior, and should be avoided.\n\nUsage\n\nTo use the json-file driver as the default logging driver, set the log-driver and log-opts keys to appropriate values in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config\\ on Windows Server. If the file does not exist, create it first. For more information about configuring Docker using daemon.json, see daemon.json.\n\nThe following example sets the log driver to json-file and sets the max-size and max-file options to enable automatic log-rotation.\n\n{\n\n  \"log-driver\": \"json-file\",\n\n  \"log-opts\": {\n\n    \"max-size\": \"10m\",\n\n    \"max-file\": \"3\"\n\n  }\n\n}\nNote\n\nlog-opts configuration options in the daemon.json configuration file must be provided as strings. Boolean and numeric values (such as the value for max-file in the example above) must therefore be enclosed in quotes (\").\n\nRestart Docker for the changes to take effect for newly created containers. Existing containers don't use the new logging configuration automatically.\n\nYou can set the logging driver for a specific container by using the --log-driver flag to docker container create or docker run:\n\n$ docker run \\\n\n      --log-driver json-file --log-opt max-size=10m \\\n\n      alpine echo hello world\n\nOptions\n\nThe json-file logging driver supports the following logging options:\n\nOption\tDescription\tExample value\nmax-size\tThe maximum size of the log before it is rolled. A positive integer plus a modifier representing the unit of measure (k, m, or g). Defaults to -1 (unlimited).\t--log-opt max-size=10m\nmax-file\tThe maximum number of log files that can be present. If rolling the logs creates excess files, the oldest file is removed. Only effective when max-size is also set. A positive integer. Defaults to 1.\t--log-opt max-file=3\nlabels\tApplies when starting the Docker daemon. A comma-separated list of logging-related labels this daemon accepts. Used for advanced log tag options.\t--log-opt labels=production_status,geo\nlabels-regex\tSimilar to and compatible with labels. A regular expression to match logging-related labels. Used for advanced log tag options.\t--log-opt labels-regex=^(production_status|geo)\nenv\tApplies when starting the Docker daemon. A comma-separated list of logging-related environment variables this daemon accepts. Used for advanced log tag options.\t--log-opt env=os,customer\nenv-regex\tSimilar to and compatible with env. A regular expression to match logging-related environment variables. Used for advanced log tag options.\t--log-opt env-regex=^(os|customer)\ncompress\tToggles compression for rotated logs. Default is disabled.\t--log-opt compress=true\nExamples\n\nThis example starts an alpine container which can have a maximum of 3 log files no larger than 10 megabytes each.\n\n$ docker run -it --log-opt max-size=10m --log-opt max-file=3 alpine ash\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\nOptions\nExamples\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995336,
    "timestamp": "2026-02-07T06:32:16.459Z",
    "title": "Local file logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/local/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nLocal file logging driver\nLocal file logging driver\nCopy as Markdown\n\nThe local logging driver captures output from container's stdout/stderr and writes them to an internal storage that's optimized for performance and disk use.\n\nBy default, the local driver preserves 100MB of log messages per container and uses automatic compression to reduce the size on disk. The 100MB default value is based on a 20M default size for each file and a default count of 5 for the number of such files (to account for log rotation).\n\nWarning\n\nThe local logging driver uses file-based storage. These files are designed to be exclusively accessed by the Docker daemon. Interacting with these files with external tools may interfere with Docker's logging system and result in unexpected behavior, and should be avoided.\n\nUsage\n\nTo use the local driver as the default logging driver, set the log-driver and log-opt keys to appropriate values in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config\\daemon.json on Windows Server. For more about configuring Docker using daemon.json, see daemon.json.\n\nThe following example sets the log driver to local and sets the max-size option.\n\n{\n\n  \"log-driver\": \"local\",\n\n  \"log-opts\": {\n\n    \"max-size\": \"10m\"\n\n  }\n\n}\n\nRestart Docker for the changes to take effect for newly created containers. Existing containers don't use the new logging configuration automatically.\n\nYou can set the logging driver for a specific container by using the --log-driver flag to docker container create or docker run:\n\n$ docker run \\\n\n      --log-driver local --log-opt max-size=10m \\\n\n      alpine echo hello world\n\n\nNote that local is a bash reserved keyword, so you may need to quote it in scripts.\n\nOptions\n\nThe local logging driver supports the following logging options:\n\nOption\tDescription\tExample value\nmax-size\tThe maximum size of the log before it's rolled. A positive integer plus a modifier representing the unit of measure (k, m, or g). Defaults to 20m.\t--log-opt max-size=10m\nmax-file\tThe maximum number of log files that can be present. If rolling the logs creates excess files, the oldest file is removed. A positive integer. Defaults to 5.\t--log-opt max-file=3\ncompress\tToggle compression of rotated log files. Enabled by default.\t--log-opt compress=false\nExamples\n\nThis example starts an alpine container which can have a maximum of 3 log files no larger than 10 megabytes each.\n\n$ docker run -it --log-driver local --log-opt max-size=10m --log-opt max-file=3 alpine ash\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\nOptions\nExamples\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995339,
    "timestamp": "2026-02-07T06:32:16.462Z",
    "title": "Splunk logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/splunk/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nSplunk logging driver\nSplunk logging driver\nCopy as Markdown\n\nThe splunk logging driver sends container logs to HTTP Event Collector in Splunk Enterprise and Splunk Cloud.\n\nUsage\n\nYou can configure Docker logging to use the splunk driver by default or on a per-container basis.\n\nTo use the splunk driver as the default logging driver, set the keys log-driver and log-opts to appropriate values in the daemon.json configuration file and restart Docker. For example:\n\n{\n\n  \"log-driver\": \"splunk\",\n\n  \"log-opts\": {\n\n    \"splunk-token\": \"\",\n\n    \"splunk-url\": \"\",\n\n    ...\n\n  }\n\n}\n\nThe daemon.json file is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config\\daemon.json on Windows Server. For more about configuring Docker using daemon.json, see daemon.json.\n\nNote\n\nlog-opts configuration options in the daemon.json configuration file must be provided as strings. Boolean and numeric values (such as the value for splunk-gzip or splunk-gzip-level) must therefore be enclosed in quotes (\").\n\nTo use the splunk driver for a specific container, use the commandline flags --log-driver and log-opt with docker run:\n\n$ docker run --log-driver=splunk --log-opt splunk-token=VALUE --log-opt splunk-url=VALUE ...\n\nSplunk options\n\nThe following properties let you configure the Splunk logging driver.\n\nTo configure the splunk driver across the Docker environment, edit daemon.json with the key, \"log-opts\": {\"NAME\": \"VALUE\", ...}.\nTo configure the splunk driver for an individual container, use docker run with the flag, --log-opt NAME=VALUE ....\nOption\tRequired\tDescription\nsplunk-token\trequired\tSplunk HTTP Event Collector token.\nsplunk-url\trequired\tPath to your Splunk Enterprise, self-service Splunk Cloud instance, or Splunk Cloud managed cluster (including port and scheme used by HTTP Event Collector) in one of the following formats: https://your_splunk_instance:8088, https://input-prd-p-XXXXXXX.cloud.splunk.com:8088, or https://http-inputs-XXXXXXXX.splunkcloud.com.\nsplunk-source\toptional\tEvent source.\nsplunk-sourcetype\toptional\tEvent source type.\nsplunk-index\toptional\tEvent index.\nsplunk-capath\toptional\tPath to root certificate.\nsplunk-caname\toptional\tName to use for validating server certificate; by default the hostname of the splunk-url is used.\nsplunk-insecureskipverify\toptional\tIgnore server certificate validation.\nsplunk-format\toptional\tMessage format. Can be inline, json or raw. Defaults to inline.\nsplunk-verify-connection\toptional\tVerify on start, that Docker can connect to Splunk server. Defaults to true.\nsplunk-gzip\toptional\tEnable/disable gzip compression to send events to Splunk Enterprise or Splunk Cloud instance. Defaults to false.\nsplunk-gzip-level\toptional\tSet compression level for gzip. Valid values are -1 (default), 0 (no compression), 1 (best speed) ... 9 (best compression). Defaults to DefaultCompression.\ntag\toptional\tSpecify tag for message, which interpret some markup. Default value is {{.ID}} (12 characters of the container ID). Refer to the log tag option documentation for customizing the log tag format.\nlabels\toptional\tComma-separated list of keys of labels, which should be included in message, if these labels are specified for container.\nlabels-regex\toptional\tSimilar to and compatible with labels. A regular expression to match logging-related labels. Used for advanced log tag options.\nenv\toptional\tComma-separated list of keys of environment variables, which should be included in message, if these variables are specified for container.\nenv-regex\toptional\tSimilar to and compatible with env. A regular expression to match logging-related environment variables. Used for advanced log tag options.\n\nIf there is collision between the label and env keys, the value of the env takes precedence. Both options add additional fields to the attributes of a logging message.\n\nBelow is an example of the logging options specified for the Splunk Enterprise instance. The instance is installed locally on the same machine on which the Docker daemon is running.\n\nThe path to the root certificate and Common Name is specified using an HTTPS scheme. This is used for verification. The SplunkServerDefaultCert is automatically generated by Splunk certificates.\n\n$ docker run \\\n\n    --log-driver=splunk \\\n\n    --log-opt splunk-token=176FCEBF-4CF5-4EDF-91BC-703796522D20 \\\n\n    --log-opt splunk-url=https://splunkhost:8088 \\\n\n    --log-opt splunk-capath=/path/to/cert/cacert.pem \\\n\n    --log-opt splunk-caname=SplunkServerDefaultCert \\\n\n    --log-opt tag=\"{{.Name}}/{{.FullID}}\" \\\n\n    --log-opt labels=location \\\n\n    --log-opt env=TEST \\\n\n    --env \"TEST=false\" \\\n\n    --label location=west \\\n\n    your/application\n\n\nThe splunk-url for Splunk instances hosted on Splunk Cloud is in a format like https://http-inputs-XXXXXXXX.splunkcloud.com and does not include a port specifier.\n\nMessage formats\n\nThere are three logging driver messaging formats: inline (default), json, and raw.\n\nInline JSON Raw\n\nThe default format is inline where each log message is embedded as a string. For example:\n\n{\n\n  \"attrs\": {\n\n    \"env1\": \"val1\",\n\n    \"label1\": \"label1\"\n\n  },\n\n  \"tag\": \"MyImage/MyContainer\",\n\n  \"source\": \"stdout\",\n\n  \"line\": \"my message\"\n\n}\n{\n\n  \"attrs\": {\n\n    \"env1\": \"val1\",\n\n    \"label1\": \"label1\"\n\n  },\n\n  \"tag\": \"MyImage/MyContainer\",\n\n  \"source\": \"stdout\",\n\n  \"line\": \"{\\\"foo\\\": \\\"bar\\\"}\"\n\n}\nAdvanced options\n\nThe Splunk logging driver lets you configure a few advanced options by setting environment variables for the Docker daemon.\n\nEnvironment variable name\tDefault value\tDescription\nSPLUNK_LOGGING_DRIVER_POST_MESSAGES_FREQUENCY\t5s\tThe time to wait for more messages to batch.\nSPLUNK_LOGGING_DRIVER_POST_MESSAGES_BATCH_SIZE\t1000\tThe number of messages that should accumulate before sending them in one batch.\nSPLUNK_LOGGING_DRIVER_BUFFER_MAX\t10 * 1000\tThe maximum number of messages held in buffer for retries.\nSPLUNK_LOGGING_DRIVER_CHANNEL_SIZE\t4 * 1000\tThe maximum number of pending messages that can be in the channel used to send messages to background logger worker, which batches them.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\nSplunk options\nMessage formats\nAdvanced options\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995345,
    "timestamp": "2026-02-07T06:32:16.464Z",
    "title": "Use a logging driver plugin | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/plugins/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nUse a logging driver plugin\nUse a logging driver plugin\nCopy as Markdown\n\nDocker logging plugins allow you to extend and customize Docker's logging capabilities beyond those of the built-in logging drivers. A logging service provider can implement their own plugins and make them available on Docker Hub, or a private registry. This topic shows how a user of that logging service can configure Docker to use the plugin.\n\nInstall the logging driver plugin\n\nTo install a logging driver plugin, use docker plugin install <org/image>, using the information provided by the plugin developer.\n\nYou can list all installed plugins using docker plugin ls, and you can inspect a specific plugin using docker inspect.\n\nConfigure the plugin as the default logging driver\n\nWhen the plugin is installed, you can configure the Docker daemon to use it as the default by setting the plugin's name as the value of the log-driver key in the daemon.json, as detailed in the logging overview. If the logging driver supports additional options, you can set those as the values of the log-opts array in the same file.\n\nConfigure a container to use the plugin as the logging driver\n\nAfter the plugin is installed, you can configure a container to use the plugin as its logging driver by specifying the --log-driver flag to docker run, as detailed in the logging overview. If the logging driver supports additional options, you can specify them using one or more --log-opt flags with the option name as the key and the option value as the value.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nInstall the logging driver plugin\nConfigure the plugin as the default logging driver\nConfigure a container to use the plugin as the logging driver\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995342,
    "timestamp": "2026-02-07T06:32:16.465Z",
    "title": "Syslog logging driver | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/drivers/syslog/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nAmazon CloudWatch Logs logging driver\nETW logging driver\nFluentd logging driver\nGoogle Cloud Logging driver\nGraylog Extended Format logging driver\nJournald logging driver\nJSON File logging driver\nLocal file logging driver\nSplunk logging driver\nSyslog logging driver\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nLogging drivers\n/\nSyslog logging driver\nSyslog logging driver\nCopy as Markdown\n\nThe syslog logging driver routes logs to a syslog server. The syslog protocol uses a raw string as the log message and supports a limited set of metadata. The syslog message must be formatted in a specific way to be valid. From a valid message, the receiver can extract the following information:\n\nPriority: the logging level, such as debug, warning, error, info.\nTimestamp: when the event occurred.\nHostname: where the event happened.\nFacility: which subsystem logged the message, such as mail or kernel.\nProcess name and process ID (PID): The name and ID of the process that generated the log.\n\nThe format is defined in RFC 5424 and Docker's syslog driver implements the ABNF reference in the following way:\n\n                TIMESTAMP SP HOSTNAME SP APP-NAME SP PROCID SP MSGID\n\n                    +          +             +           |        +\n\n                    |          |             |           |        |\n\n                    |          |             |           |        |\n\n       +------------+          +----+        |           +----+   +---------+\n\n       v                            v        v                v             v\n\n2017-04-01T17:41:05.616647+08:00 a.vm {taskid:aa,version:} 1787791 {taskid:aa,version:}\nUsage\n\nTo use the syslog driver as the default logging driver, set the log-driver and log-opt keys to appropriate values in the daemon.json file, which is located in /etc/docker/ on Linux hosts or C:\\ProgramData\\docker\\config\\daemon.json on Windows Server. For more about configuring Docker using daemon.json, see daemon.json.\n\nThe following example sets the log driver to syslog and sets the syslog-address option. The syslog-address options supports both UDP and TCP; this example uses UDP.\n\n{\n\n  \"log-driver\": \"syslog\",\n\n  \"log-opts\": {\n\n    \"syslog-address\": \"udp://1.2.3.4:1111\"\n\n  }\n\n}\n\nRestart Docker for the changes to take effect.\n\nNote\n\nlog-opts configuration options in the daemon.json configuration file must be provided as strings. Numeric and Boolean values (such as the value for syslog-tls-skip-verify) must therefore be enclosed in quotes (\").\n\nYou can set the logging driver for a specific container by using the --log-driver flag to docker container create or docker run:\n\n$ docker run \\\n\n      --log-driver syslog --log-opt syslog-address=udp://1.2.3.4:1111 \\\n\n      alpine echo hello world\n\nOptions\n\nThe following logging options are supported as options for the syslog logging driver. They can be set as defaults in the daemon.json, by adding them as key-value pairs to the log-opts JSON array. They can also be set on a given container by adding a --log-opt <key>=<value> flag for each option when starting the container.\n\nOption\tDescription\tExample value\nsyslog-address\tThe address of an external syslog server. The URI specifier may be [tcp|udp|tcp+tls]://host:port, unix://path, or unixgram://path. If the transport is tcp, udp, or tcp+tls, the default port is 514.\t--log-opt syslog-address=tcp+tls://192.168.1.3:514, --log-opt syslog-address=unix:///tmp/syslog.sock\nsyslog-facility\tThe syslog facility to use. Can be the number or name for any valid syslog facility. See the syslog documentation.\t--log-opt syslog-facility=daemon\nsyslog-tls-ca-cert\tThe absolute path to the trust certificates signed by the CA. Ignored if the address protocol isn't tcp+tls.\t--log-opt syslog-tls-ca-cert=/etc/ca-certificates/custom/ca.pem\nsyslog-tls-cert\tThe absolute path to the TLS certificate file. Ignored if the address protocol isn't tcp+tls.\t--log-opt syslog-tls-cert=/etc/ca-certificates/custom/cert.pem\nsyslog-tls-key\tThe absolute path to the TLS key file. Ignored if the address protocol isn't tcp+tls.\t--log-opt syslog-tls-key=/etc/ca-certificates/custom/key.pem\nsyslog-tls-skip-verify\tIf set to true, TLS verification is skipped when connecting to the syslog daemon. Defaults to false. Ignored if the address protocol isn't tcp+tls.\t--log-opt syslog-tls-skip-verify=true\ntag\tA string that's appended to the APP-NAME in the syslog message. By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format.\t--log-opt tag=mailer\nsyslog-format\tThe syslog message format to use. If not specified the local Unix syslog format is used, without a specified hostname. Specify rfc3164 for the RFC-3164 compatible format, rfc5424 for RFC-5424 compatible format, or rfc5424micro for RFC-5424 compatible format with microsecond timestamp resolution.\t--log-opt syslog-format=rfc5424micro\nlabels\tApplies when starting the Docker daemon. A comma-separated list of logging-related labels this daemon accepts. Used for advanced log tag options.\t--log-opt labels=production_status,geo\nlabels-regex\tApplies when starting the Docker daemon. Similar to and compatible with labels. A regular expression to match logging-related labels. Used for advanced log tag options.\t--log-opt labels-regex=^(production_status|geo)\nenv\tApplies when starting the Docker daemon. A comma-separated list of logging-related environment variables this daemon accepts. Used for advanced log tag options.\t--log-opt env=os,customer\nenv-regex\tApplies when starting the Docker daemon. Similar to and compatible with env. A regular expression to match logging-related environment variables. Used for advanced log tag options.\t--log-opt env-regex=^(os|customer)\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsage\nOptions\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995348,
    "timestamp": "2026-02-07T06:32:16.467Z",
    "title": "Use docker logs with remote logging drivers | Docker Docs",
    "url": "https://docs.docker.com/engine/logging/dual-logging/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nConfigure logging drivers\nCustomize log driver output\nLogging drivers\nUse a logging driver plugin\nUse docker logs with remote logging drivers\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nLogs and metrics\n/\nUse docker logs with remote logging drivers\nUse docker logs with remote logging drivers\nCopy as Markdown\nOverview\n\nYou can use the docker logs command to read container logs regardless of the configured logging driver or plugin. Docker Engine uses the local logging driver to act as cache for reading the latest logs of your containers. This is called dual logging. By default, the cache has log-file rotation enabled, and is limited to a maximum of 5 files of 20 MB each (before compression) per container.\n\nRefer to the configuration options section to customize these defaults, or to the disable dual logging section to disable this feature.\n\nPrerequisites\n\nDocker Engine automatically enables dual logging if the configured logging driver doesn't support reading logs.\n\nThe following examples show the result of running a docker logs command with and without dual logging availability:\n\nWithout dual logging capability\n\nWhen a container is configured with a remote logging driver such as splunk, and dual logging is disabled, an error is displayed when attempting to read container logs locally:\n\nStep 1: Configure Docker daemon\n\n$ cat /etc/docker/daemon.json\n\n{\n\n  \"log-driver\": \"splunk\",\n\n  \"log-opts\": {\n\n    \"cache-disabled\": \"true\",\n\n    ... (options for \"splunk\" logging driver)\n\n  }\n\n}\n\n\nStep 2: Start the container\n\n$ docker run -d busybox --name testlog top\n\n\nStep 3: Read the container logs\n\n$ docker logs 7d6ac83a89a0\n\nError response from daemon: configured logging driver does not support reading\n\nWith dual logging capability\n\nWith the dual logging cache enabled, the docker logs command can be used to read logs, even if the logging driver doesn't support reading logs. The following example shows a daemon configuration that uses the splunk remote logging driver as a default, with dual logging caching enabled:\n\nStep 1: Configure Docker daemon\n\n$ cat /etc/docker/daemon.json\n\n{\n\n  \"log-driver\": \"splunk\",\n\n  \"log-opts\": {\n\n    ... (options for \"splunk\" logging driver)\n\n  }\n\n}\n\n\nStep 2: Start the container\n\n$ docker run -d busybox --name testlog top\n\n\nStep 3: Read the container logs\n\n$ docker logs 7d6ac83a89a0\n\n2019-02-04T19:48:15.423Z [INFO]  core: marked as sealed\n\n2019-02-04T19:48:15.423Z [INFO]  core: pre-seal teardown starting\n\n2019-02-04T19:48:15.423Z [INFO]  core: stopping cluster listeners\n\n2019-02-04T19:48:15.423Z [INFO]  core: shutting down forwarding rpc listeners\n\n2019-02-04T19:48:15.423Z [INFO]  core: forwarding rpc listeners stopped\n\n2019-02-04T19:48:15.599Z [INFO]  core: rpc listeners successfully shut down\n\n2019-02-04T19:48:15.599Z [INFO]  core: cluster listeners successfully shut down\n\nNote\n\nFor logging drivers that support reading logs, such as the local, json-file and journald drivers, there is no difference in functionality before or after the dual logging capability became available. For these drivers, Logs can be read using docker logs in both scenarios.\n\nConfiguration options\n\nThe dual logging cache accepts the same configuration options as the local logging driver, but with a cache- prefix. These options can be specified per container, and defaults for new containers can be set using the daemon configuration file.\n\nBy default, the cache has log-file rotation enabled, and is limited to a maximum of 5 files of 20MB each (before compression) per container. Use the configuration options described below to customize these defaults.\n\nOption\tDefault\tDescription\ncache-disabled\t\"false\"\tDisable local caching. Boolean value passed as a string (true, 1, 0, or false).\ncache-max-size\t\"20m\"\tThe maximum size of the cache before it is rotated. A positive integer plus a modifier representing the unit of measure (k, m, or g).\ncache-max-file\t\"5\"\tThe maximum number of cache files that can be present. If rotating the logs creates excess files, the oldest file is removed. A positive integer.\ncache-compress\t\"true\"\tEnable or disable compression of rotated log files. Boolean value passed as a string (true, 1, 0, or false).\nDisable the dual logging cache\n\nUse the cache-disabled option to disable the dual logging cache. Disabling the cache can be useful to save storage space in situations where logs are only read through a remote logging system, and if there is no need to read logs through docker logs for debugging purposes.\n\nCaching can be disabled for individual containers or by default for new containers, when using the daemon configuration file.\n\nThe following example uses the daemon configuration file to use the splunk logging driver as a default, with caching disabled:\n\n$ cat /etc/docker/daemon.json\n\n{\n\n  \"log-driver\": \"splunk\",\n\n  \"log-opts\": {\n\n    \"cache-disabled\": \"true\",\n\n    ... (options for \"splunk\" logging driver)\n\n  }\n\n}\n\nNote\n\nFor logging drivers that support reading logs, such as the local, json-file and journald drivers, dual logging isn't used, and disabling the option has no effect.\n\nLimitations\nIf a container using a logging driver or plugin that sends logs remotely has a network issue, no write to the local cache occurs.\nIf a write to logdriver fails for any reason (file system full, write permissions removed), the cache write fails and is logged in the daemon log. The log entry to the cache isn't retried.\nSome logs might be lost from the cache in the default configuration because a ring buffer is used to prevent blocking the stdio of the container in case of slow file writes. An admin must repair these while the daemon is shut down.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nOverview\nPrerequisites\nWithout dual logging capability\nWith dual logging capability\nConfiguration options\nDisable the dual logging cache\nLimitations\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995416,
    "timestamp": "2026-02-07T06:33:19.726Z",
    "title": "Rootless mode | Docker Docs",
    "url": "https://docs.docker.com/engine/security/rootless/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nTips\nTroubleshooting\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nRootless mode\nRootless mode\nCopy as Markdown\n\nRootless mode lets you run the Docker daemon and containers as a non-root user to mitigate potential vulnerabilities in the daemon and the container runtime.\n\nRootless mode does not require root privileges even during the installation of the Docker daemon, as long as the prerequisites are met.\n\nHow it works\n\nRootless mode executes the Docker daemon and containers inside a user namespace. This is similar to userns-remap mode, except that with userns-remap mode, the daemon itself is running with root privileges, whereas in rootless mode, both the daemon and the container are running without root privileges.\n\nRootless mode does not use binaries with SETUID bits or file capabilities, except newuidmap and newgidmap, which are needed to allow multiple UIDs/GIDs to be used in the user namespace.\n\nPrerequisites\n\nYou must install newuidmap and newgidmap on the host. These commands are provided by the uidmap package on most distributions.\n\n/etc/subuid and /etc/subgid should contain at least 65,536 subordinate UIDs/GIDs for the user. In the following example, the user testuser has 65,536 subordinate UIDs/GIDs (231072-296607).\n\n$ id -u\n\n1001\n\n$ whoami\n\ntestuser\n\n$ grep ^$(whoami): /etc/subuid\n\ntestuser:231072:65536\n\n$ grep ^$(whoami): /etc/subgid\n\ntestuser:231072:65536\n\n\nThe dockerd-rootless-setuptool.sh install script (see following) automatically shows help when the prerequisites are not satisfied.\n\nInstall\nNote\n\nIf the system-wide Docker daemon is already running, consider disabling it:\n\n$ sudo systemctl disable --now docker.service docker.socket\n\n$ sudo rm /var/run/docker.sock\n\n\nShould you choose not to shut down the docker service and socket, you will need to use the --force parameter in the next section. There are no known issues, but until you shutdown and disable you're still running rootful Docker.\n\nWith packages (RPM/DEB) Without packages\n\nIf you installed Docker 20.10 or later with RPM/DEB packages, you should have dockerd-rootless-setuptool.sh in /usr/bin.\n\nRun dockerd-rootless-setuptool.sh install as a non-root user to set up the daemon:\n\n$ dockerd-rootless-setuptool.sh install\n\n[INFO] Creating /home/testuser/.config/systemd/user/docker.service\n\n...\n\n[INFO] Installed docker.service successfully.\n\n[INFO] To control docker.service, run: `systemctl --user (start|stop|restart) docker.service`\n\n[INFO] To run docker.service on system startup, run: `sudo loginctl enable-linger testuser`\n\n\n\n[INFO] Creating CLI context \"rootless\"\n\nSuccessfully created context \"rootless\"\n\n[INFO] Using CLI context \"rootless\"\n\nCurrent context is now \"rootless\"\n\n\n\n[INFO] Make sure the following environment variable(s) are set (or add them to ~/.bashrc):\n\nexport PATH=/usr/bin:$PATH\n\n\n\n[INFO] Some applications may require the following environment variable too:\n\nexport DOCKER_HOST=unix:///run/user/1000/docker.sock\n\n\nIf dockerd-rootless-setuptool.sh is not present, you may need to install the docker-ce-rootless-extras package manually, e.g.,\n\n$ sudo apt-get install -y docker-ce-rootless-extras\n\n\nRun docker info to confirm that the docker client is connecting to the Rootless daemon:\n\n$ docker info\n\nClient: Docker Engine - Community\n\n Version:    28.3.3\n\n Context:    rootless\n\n...\n\nServer:\n\n...\n\n Security Options:\n\n  seccomp\n\n   Profile: builtin\n\n  rootless\n\n  cgroupns\n\n...\n\n\nSee Troubleshooting if you faced an error.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow it works\nPrerequisites\nInstall\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995419,
    "timestamp": "2026-02-07T06:33:19.726Z",
    "title": "Tips | Docker Docs",
    "url": "https://docs.docker.com/engine/security/rootless/tips/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nTips\nTroubleshooting\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nRootless mode\n/\nTips\nTips\nCopy as Markdown\nAdvanced usage\nDaemon\nWith systemd (Highly recommended) Without systemd\n\nThe systemd unit file is installed as ~/.config/systemd/user/docker.service.\n\nUse systemctl --user to manage the lifecycle of the daemon:\n\n$ systemctl --user start docker\n\n\nTo launch the daemon on system startup, enable the systemd service and lingering:\n\n$ systemctl --user enable docker\n\n$ sudo loginctl enable-linger $(whoami)\n\n\nStarting Rootless Docker as a systemd-wide service (/etc/systemd/system/docker.service) is not supported, even with the User= directive.\n\nIt's important to note that with directory paths:\n\nThe socket path is set to $XDG_RUNTIME_DIR/docker.sock by default. $XDG_RUNTIME_DIR is typically set to /run/user/$UID.\nThe data dir is set to ~/.local/share/docker by default. The data dir should not be on NFS.\nThe daemon config dir is set to ~/.config/docker by default. This directory is different from ~/.docker that is used by the client.\nClient\n\nSince Docker Engine v23.0, dockerd-rootless-setuptool.sh install automatically configures the docker CLI to use the rootless context.\n\nPrior to Docker Engine v23.0, a user had to specify either the socket path or the CLI context explicitly.\n\nTo specify the socket path using $DOCKER_HOST:\n\n$ export DOCKER_HOST=unix://$XDG_RUNTIME_DIR/docker.sock\n\n$ docker run -d -p 8080:80 nginx\n\n\nTo specify the CLI context using docker context:\n\n$ docker context use rootless\n\nrootless\n\nCurrent context is now \"rootless\"\n\n$ docker run -d -p 8080:80 nginx\n\nBest practices\nRootless Docker in Docker\n\nTo run Rootless Docker inside \"rootful\" Docker, use the docker:<version>-dind-rootless image instead of docker:<version>-dind.\n\n$ docker run -d --name dind-rootless --privileged docker:25.0-dind-rootless\n\n\nThe docker:<version>-dind-rootless image runs as a non-root user (UID 1000). However, --privileged is required for disabling seccomp, AppArmor, and mount masks.\n\nExpose Docker API socket through TCP\n\nTo expose the Docker API socket through TCP, you need to launch dockerd-rootless.sh with DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS=\"-p 0.0.0.0:2376:2376/tcp\".\n\n$ DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS=\"-p 0.0.0.0:2376:2376/tcp\" \\\n\n  dockerd-rootless.sh \\\n\n  -H tcp://0.0.0.0:2376 \\\n\n  --tlsverify --tlscacert=ca.pem --tlscert=cert.pem --tlskey=key.pem\n\nExpose Docker API socket through SSH\n\nTo expose the Docker API socket through SSH, you need to make sure $DOCKER_HOST is set on the remote host.\n\n$ ssh -l REMOTEUSER REMOTEHOST 'echo $DOCKER_HOST'\n\nunix:///run/user/1001/docker.sock\n\n$ docker -H ssh://REMOTEUSER@REMOTEHOST run ...\n\nRouting ping packets\n\nOn some distributions, ping does not work by default.\n\nAdd net.ipv4.ping_group_range = 0 2147483647 to /etc/sysctl.conf (or /etc/sysctl.d) and run sudo sysctl --system to allow using ping.\n\nExposing privileged ports\n\nTo expose privileged ports (< 1024), set CAP_NET_BIND_SERVICE on rootlesskit binary and restart the daemon.\n\n$ sudo setcap cap_net_bind_service=ep $(which rootlesskit)\n\n$ systemctl --user restart docker\n\n\nOr add net.ipv4.ip_unprivileged_port_start=0 to /etc/sysctl.conf (or /etc/sysctl.d) and run sudo sysctl --system.\n\nLimiting resources\n\nLimiting resources with cgroup-related docker run flags such as --cpus, --memory, --pids-limit is supported only when running with cgroup v2 and systemd. See Changing cgroup version to enable cgroup v2.\n\nIf docker info shows none as Cgroup Driver, the conditions are not satisfied. When these conditions are not satisfied, rootless mode ignores the cgroup-related docker run flags. See Limiting resources without cgroup for workarounds.\n\nIf docker info shows systemd as Cgroup Driver, the conditions are satisfied. However, typically, only memory and pids controllers are delegated to non-root users by default.\n\n$ cat /sys/fs/cgroup/user.slice/user-$(id -u).slice/user@$(id -u).service/cgroup.controllers\n\nmemory pids\n\n\nTo allow delegation of all controllers, you need to change the systemd configuration as follows:\n\n# mkdir -p /etc/systemd/system/user@.service.d\n\n# cat > /etc/systemd/system/user@.service.d/delegate.conf << EOF\n\n[Service]\n\nDelegate=cpu cpuset io memory pids\n\nEOF\n\n# systemctl daemon-reload\n\nNote\n\nDelegating cpuset requires systemd 244 or later.\n\nLimiting resources without cgroup\n\nEven when cgroup is not available, you can still use the traditional ulimit and cpulimit, though they work in process-granularity rather than in container-granularity, and can be arbitrarily disabled by the container process.\n\nFor example:\n\nTo limit CPU usage to 0.5 cores (similar to docker run --cpus 0.5): docker run <IMAGE> cpulimit --limit=50 --include-children <COMMAND>\n\nTo limit max VSZ to 64MiB (similar to docker run --memory 64m): docker run <IMAGE> sh -c \"ulimit -v 65536; <COMMAND>\"\n\nTo limit max number of processes to 100 per namespaced UID 2000 (similar to docker run --pids-limit=100): docker run --user 2000 --ulimit nproc=100 <IMAGE> <COMMAND>\n\nEdit this page\n\nRequest changes\n\nTable of contents\nAdvanced usage\nDaemon\nClient\nBest practices\nRootless Docker in Docker\nExpose Docker API socket through TCP\nExpose Docker API socket through SSH\nRouting ping packets\nExposing privileged ports\nLimiting resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995422,
    "timestamp": "2026-02-07T06:33:19.729Z",
    "title": "Troubleshooting | Docker Docs",
    "url": "https://docs.docker.com/engine/security/rootless/troubleshoot/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nTips\nTroubleshooting\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nRootless mode\n/\nTroubleshooting\nTroubleshooting\nCopy as Markdown\nDistribution-specific hint\nUbuntu Arch Linux openSUSE and SLES CentOS, RHEL, and Fedora\n\nUbuntu 24.04 and later enables restricted unprivileged user namespaces by default, which prevents unprivileged processes in creating user namespaces unless an AppArmor profile is configured to allow programs to use unprivileged user namespaces.\n\nIf you install docker-ce-rootless-extras using the deb package (apt-get install docker-ce-rootless-extras), then the AppArmor profile for rootlesskit is already bundled with the apparmor deb package. With this installation method, you don't need to add any manual the AppArmor configuration. If you install the rootless extras using the installation script, however, you must add an AppArmor profile for rootlesskit manually:\n\nCreate and install the currently logged-in user's AppArmor profile:\n\n$ filename=$(echo $HOME/bin/rootlesskit | sed -e 's@^/@@' -e 's@/@.@g')\n\n$ [ ! -z \"${filename}\" ] && sudo cat <<EOF > /etc/apparmor.d/${filename}\n\nabi <abi/4.0>,\n\ninclude <tunables/global>\n\n\n\n\"$HOME/bin/rootlesskit\" flags=(unconfined) {\n\n  userns,\n\n\n\n  include if exists <local/${filename}>\n\n}\n\nEOF\n\n\nRestart AppArmor.\n\n$ systemctl restart apparmor.service\n\nKnown limitations\nOnly the following storage drivers are supported:\noverlay2 (only if running with kernel 5.11 or later)\nfuse-overlayfs (only if running with kernel 4.18 or later, and fuse-overlayfs is installed)\nbtrfs (only if running with kernel 4.18 or later, or ~/.local/share/docker is mounted with user_subvol_rm_allowed mount option)\nvfs\ncgroup is supported only when running with cgroup v2 and systemd. See Limiting resources.\nFollowing features are not supported:\nAppArmor\nCheckpoint\nOverlay network\nExposing SCTP ports\nTo use the ping command, see Routing ping packets.\nTo expose privileged TCP/UDP ports (< 1024), see Exposing privileged ports.\nIPAddress shown in docker inspect is namespaced inside RootlessKit's network namespace. This means the IP address is not reachable from the host without nsenter-ing into the network namespace.\nHost network (docker run --net=host) is also namespaced inside RootlessKit.\nNFS mounts as the docker \"data-root\" is not supported. This limitation is not specific to rootless mode.\nTroubleshooting\nUnable to install with systemd when systemd is present on the system\n$ dockerd-rootless-setuptool.sh install\n\n[INFO] systemd not detected, dockerd-rootless.sh needs to be started manually:\n\n...\n\n\nrootlesskit cannot detect systemd properly if you switch to your user via sudo su. For users which cannot be logged-in, you must use the machinectl command which is part of the systemd-container package. After installing systemd-container switch to myuser with the following command:\n\n$ sudo machinectl shell myuser@\n\n\nWhere myuser@ is your desired username and @ signifies this machine.\n\nErrors when starting the Docker daemon\n\n[rootlesskit:parent] error: failed to start the child: fork/exec /proc/self/exe: operation not permitted\n\nThis error occurs mostly when the value of /proc/sys/kernel/unprivileged_userns_clone is set to 0:\n\n$ cat /proc/sys/kernel/unprivileged_userns_clone\n\n0\n\n\nTo fix this issue, add kernel.unprivileged_userns_clone=1 to /etc/sysctl.conf (or /etc/sysctl.d) and run sudo sysctl --system.\n\n[rootlesskit:parent] error: failed to start the child: fork/exec /proc/self/exe: no space left on device\n\nThis error occurs mostly when the value of /proc/sys/user/max_user_namespaces is too small:\n\n$ cat /proc/sys/user/max_user_namespaces\n\n0\n\n\nTo fix this issue, add user.max_user_namespaces=28633 to /etc/sysctl.conf (or /etc/sysctl.d) and run sudo sysctl --system.\n\n[rootlesskit:parent] error: failed to setup UID/GID map: failed to compute uid/gid map: No subuid ranges found for user 1001 (\"testuser\")\n\nThis error occurs when /etc/subuid and /etc/subgid are not configured. See Prerequisites.\n\ncould not get XDG_RUNTIME_DIR\n\nThis error occurs when $XDG_RUNTIME_DIR is not set.\n\nOn a non-systemd host, you need to create a directory and then set the path:\n\n$ export XDG_RUNTIME_DIR=$HOME/.docker/xrd\n\n$ rm -rf $XDG_RUNTIME_DIR\n\n$ mkdir -p $XDG_RUNTIME_DIR\n\n$ dockerd-rootless.sh\n\nNote\n\nYou must remove the directory every time you log out.\n\nOn a systemd host, log into the host using pam_systemd (see below). The value is automatically set to /run/user/$UID and cleaned up on every logout.\n\nsystemctl --user fails with \"Failed to connect to bus: No such file or directory\"\n\nThis error occurs mostly when you switch from the root user to a non-root user with sudo:\n\n# sudo -iu testuser\n\n$ systemctl --user start docker\n\nFailed to connect to bus: No such file or directory\n\n\nInstead of sudo -iu <USERNAME>, you need to log in using pam_systemd. For example:\n\nLog in through the graphic console\nssh <USERNAME>@localhost\nmachinectl shell <USERNAME>@\n\nThe daemon does not start up automatically\n\nYou need sudo loginctl enable-linger $(whoami) to enable the daemon to start up automatically. See Advanced Usage.\n\ndocker pull errors\n\ndocker: failed to register layer: Error processing tar file(exit status 1): lchown <FILE>: invalid argument\n\nThis error occurs when the number of available entries in /etc/subuid or /etc/subgid is not sufficient. The number of entries required vary across images. However, 65,536 entries are sufficient for most images. See Prerequisites.\n\ndocker: failed to register layer: ApplyLayer exit status 1 stdout: stderr: lchown <FILE>: operation not permitted\n\nThis error occurs mostly when ~/.local/share/docker is located on NFS.\n\nA workaround is to specify non-NFS data-root directory in ~/.config/docker/daemon.json as follows:\n\n{\"data-root\":\"/somewhere-out-of-nfs\"}\ndocker run errors\n\ndocker: Error response from daemon: OCI runtime create failed: ...: read unix @->/run/systemd/private: read: connection reset by peer: unknown.\n\nThis error occurs on cgroup v2 hosts mostly when the dbus daemon is not running for the user.\n\n$ systemctl --user is-active dbus\n\ninactive\n\n\n\n$ docker run hello-world\n\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:385: applying cgroup configuration for process caused: error while starting unit \"docker\n\n-931c15729b5a968ce803784d04c7421f791d87e5ca1891f34387bb9f694c488e.scope\" with properties [{Name:Description Value:\"libcontainer container 931c15729b5a968ce803784d04c7421f791d87e5ca1891f34387bb9f694c488e\"} {Name:Slice Value:\"use\n\nr.slice\"} {Name:PIDs Value:@au [4529]} {Name:Delegate Value:true} {Name:MemoryAccounting Value:true} {Name:CPUAccounting Value:true} {Name:IOAccounting Value:true} {Name:TasksAccounting Value:true} {Name:DefaultDependencies Val\n\nue:false}]: read unix @->/run/systemd/private: read: connection reset by peer: unknown.\n\n\nTo fix the issue, run sudo apt-get install -y dbus-user-session or sudo dnf install -y dbus-daemon, and then relogin.\n\nIf the error still occurs, try running systemctl --user enable --now dbus (without sudo).\n\n--cpus, --memory, and --pids-limit are ignored\n\nThis is an expected behavior on cgroup v1 mode. To use these flags, the host needs to be configured for enabling cgroup v2. For more information, see Limiting resources.\n\nNetworking errors\n\nThis section provides troubleshooting tips for networking in rootless mode.\n\nNetworking in rootless mode is supported via network and port drivers in RootlessKit. Network performance and characteristics depend on the combination of network and port driver you use. If you're experiencing unexpected behavior or performance related to networking, review the following table which shows the configurations supported by RootlessKit, and how they compare:\n\nNetwork driver\tPort driver\tNet throughput\tPort throughput\tSource IP propagation\tNo SUID\tNote\nslirp4netns\tbuiltin\tSlow\tFast ‚úÖ\t‚ùå\t‚úÖ\tDefault in a typical setup\nvpnkit\tbuiltin\tSlow\tFast ‚úÖ\t‚ùå\t‚úÖ\tDefault when slirp4netns isn't installed\nslirp4netns\tslirp4netns\tSlow\tSlow\t‚úÖ\t‚úÖ\t\npasta\timplicit\tSlow\tFast ‚úÖ\t‚úÖ\t‚úÖ\tExperimental; Needs pasta version 2023_12_04 or later\nlxc-user-nic\tbuiltin\tFast ‚úÖ\tFast ‚úÖ\t‚ùå\t‚ùå\tExperimental\nbypass4netns\tbypass4netns\tFast ‚úÖ\tFast ‚úÖ\t‚úÖ\t‚úÖ\tNote: Not integrated to RootlessKit as it needs a custom seccomp profile\n\nFor information about troubleshooting specific networking issues, see:\n\ndocker run -p fails with cannot expose privileged port\nPing doesn't work\nIPAddress shown in docker inspect is unreachable\n--net=host doesn't listen ports on the host network namespace\nNetwork is slow\ndocker run -p does not propagate source IP addresses\ndocker run -p fails with cannot expose privileged port\n\ndocker run -p fails with this error when a privileged port (< 1024) is specified as the host port.\n\n$ docker run -p 80:80 nginx:alpine\n\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint focused_swanson (9e2e139a9d8fc92b37c36edfa6214a6e986fa2028c0cc359812f685173fa6df7): Error starting userland proxy: error while calling PortManager.AddPort(): cannot expose privileged port 80, you might need to add \"net.ipv4.ip_unprivileged_port_start=0\" (currently 1024) to /etc/sysctl.conf, or set CAP_NET_BIND_SERVICE on rootlesskit binary, or choose a larger port number (>= 1024): listen tcp 0.0.0.0:80: bind: permission denied.\n\n\nWhen you experience this error, consider using an unprivileged port instead. For example, 8080 instead of 80.\n\n$ docker run -p 8080:80 nginx:alpine\n\n\nTo allow exposing privileged ports, see Exposing privileged ports.\n\nPing doesn't work\n\nPing does not work when /proc/sys/net/ipv4/ping_group_range is set to 1 0:\n\n$ cat /proc/sys/net/ipv4/ping_group_range\n\n1       0\n\n\nFor details, see Routing ping packets.\n\nIPAddress shown in docker inspect is unreachable\n\nThis is an expected behavior, as the daemon is namespaced inside RootlessKit's network namespace. Use docker run -p instead.\n\n--net=host doesn't listen ports on the host network namespace\n\nThis is an expected behavior, as the daemon is namespaced inside RootlessKit's network namespace. Use docker run -p instead.\n\nNetwork is slow\n\nDocker with rootless mode uses slirp4netns as the default network stack if slirp4netns v0.4.0 or later is installed. If slirp4netns is not installed, Docker falls back to VPNKit. Installing slirp4netns may improve the network throughput.\n\nFor more information about network drivers for RootlessKit, see RootlessKit documentation.\n\nAlso, changing MTU value may improve the throughput. The MTU value can be specified by creating ~/.config/systemd/user/docker.service.d/override.conf with the following content:\n\n[Service]\n\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_MTU=INTEGER\"\n\nAnd then restart the daemon:\n\n$ systemctl --user daemon-reload\n\n$ systemctl --user restart docker\n\ndocker run -p does not propagate source IP addresses\n\nThis is because Docker in rootless mode uses RootlessKit's builtin port driver by default, which doesn't support source IP propagation. To enable source IP propagation, you can:\n\nUse the slirp4netns RootlessKit port driver\nUse the pasta RootlessKit network driver, with the implicit port driver\n\nThe pasta network driver is experimental, but provides improved throughput performance compared to the slirp4netns port driver. The pasta driver requires Docker Engine version 25.0 or later.\n\nTo change the RootlessKit networking configuration:\n\nCreate a file at ~/.config/systemd/user/docker.service.d/override.conf.\n\nAdd the following contents, depending on which configuration you would like to use:\n\nslirp4netns\n\n[Service]\n\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=slirp4netns\"\n\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=slirp4netns\"\n\npasta network driver with implicit port driver\n\n[Service]\n\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_NET=pasta\"\n\nEnvironment=\"DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=implicit\"\n\nRestart the daemon:\n\n$ systemctl --user daemon-reload\n\n$ systemctl --user restart docker\n\n\nFor more information about networking options for RootlessKit, see:\n\nNetwork drivers\nPort drivers\nTips for debugging\n\nEntering into dockerd namespaces\n\nThe dockerd-rootless.sh script executes dockerd in its own user, mount, and network namespaces.\n\nFor debugging, you can enter the namespaces by running nsenter -U --preserve-credentials -n -m -t $(cat $XDG_RUNTIME_DIR/docker.pid).\n\nUninstall\n\nTo remove the systemd service of the Docker daemon, run dockerd-rootless-setuptool.sh uninstall:\n\n$ dockerd-rootless-setuptool.sh uninstall\n\n+ systemctl --user stop docker.service\n\n+ systemctl --user disable docker.service\n\nRemoved /home/testuser/.config/systemd/user/default.target.wants/docker.service.\n\n[INFO] Uninstalled docker.service\n\n[INFO] This uninstallation tool does NOT remove Docker binaries and data.\n\n[INFO] To remove data, run: `/usr/bin/rootlesskit rm -rf /home/testuser/.local/share/docker`\n\n\nUnset environment variables PATH and DOCKER_HOST if you have added them to ~/.bashrc.\n\nTo remove the data directory, run rootlesskit rm -rf ~/.local/share/docker.\n\nTo remove the binaries, remove docker-ce-rootless-extras package if you installed Docker with package managers. If you installed Docker with https://get.docker.com/rootless (Install without packages), remove the binary files under ~/bin:\n\n$ cd ~/bin\n\n$ rm -f containerd containerd-shim containerd-shim-runc-v2 ctr docker docker-init docker-proxy dockerd dockerd-rootless-setuptool.sh dockerd-rootless.sh rootlesskit rootlesskit-docker-proxy runc vpnkit\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDistribution-specific hint\nKnown limitations\nTroubleshooting\nUnable to install with systemd when systemd is present on the system\nErrors when starting the Docker daemon\ndocker pull errors\ndocker run errors\nNetworking errors\nTips for debugging\nUninstall\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995425,
    "timestamp": "2026-02-07T06:33:19.739Z",
    "title": "Antivirus software and Docker | Docker Docs",
    "url": "https://docs.docker.com/engine/security/antivirus/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nAntivirus software and Docker\nAntivirus software and Docker\nCopy as Markdown\n\nWhen antivirus software scans files used by Docker, these files may be locked in a way that causes Docker commands to hang.\n\nOne way to reduce these problems is to add the Docker data directory (/var/lib/docker on Linux, %ProgramData%\\docker on Windows Server, or $HOME/Library/Containers/com.docker.docker/ on Mac) to the antivirus's exclusion list. However, this comes with the trade-off that viruses or malware in Docker images, writable layers of containers, or volumes are not detected. If you do choose to exclude Docker's data directory from background virus scanning, you may want to schedule a recurring task that stops Docker, scans the data directory, and restarts Docker.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995428,
    "timestamp": "2026-02-07T06:33:19.743Z",
    "title": "AppArmor security profiles for Docker | Docker Docs",
    "url": "https://docs.docker.com/engine/security/apparmor/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nAppArmor security profiles for Docker\nAppArmor security profiles for Docker\nCopy as Markdown\n\nAppArmor (Application Armor) is a Linux security module that protects an operating system and its applications from security threats. To use it, a system administrator associates an AppArmor security profile with each program. Docker expects to find an AppArmor policy loaded and enforced.\n\nDocker automatically generates and loads a default profile for containers named docker-default. The Docker binary generates this profile in tmpfs and then loads it into the kernel.\n\nNote\n\nThis profile is used on containers, not on the Docker daemon.\n\nA profile for the Docker Engine daemon exists but it is not currently installed with the deb packages. If you are interested in the source for the daemon profile, it is located in contrib/apparmor in the Docker Engine source repository.\n\nUnderstand the policies\n\nThe docker-default profile is the default for running containers. It is moderately protective while providing wide application compatibility. The profile is generated from the following template.\n\nWhen you run a container, it uses the docker-default policy unless you override it with the security-opt option. For example, the following explicitly specifies the default policy:\n\n$ docker run --rm -it --security-opt apparmor=docker-default hello-world\n\nLoad and unload profiles\n\nTo load a new profile into AppArmor for use with containers:\n\n$ apparmor_parser -r -W /path/to/your_profile\n\n\nThen, run the custom profile with --security-opt:\n\n$ docker run --rm -it --security-opt apparmor=your_profile hello-world\n\n\nTo unload a profile from AppArmor:\n\n# unload the profile\n\n$ apparmor_parser -R /path/to/profile\n\nResources for writing profiles\n\nThe syntax for file globbing in AppArmor is a bit different than some other globbing implementations. It is highly suggested you take a look at some of the below resources with regard to AppArmor profile syntax.\n\nQuick Profile Language\nGlobbing Syntax\nNginx example profile\n\nIn this example, you create a custom AppArmor profile for Nginx. Below is the custom profile.\n\n#include <tunables/global>\n\n\n\n\n\nprofile docker-nginx flags=(attach_disconnected,mediate_deleted) {\n\n  #include <abstractions/base>\n\n\n\n  network inet tcp,\n\n  network inet udp,\n\n  network inet icmp,\n\n\n\n  deny network raw,\n\n\n\n  deny network packet,\n\n\n\n  file,\n\n  umount,\n\n\n\n  deny /bin/** wl,\n\n  deny /boot/** wl,\n\n  deny /dev/** wl,\n\n  deny /etc/** wl,\n\n  deny /home/** wl,\n\n  deny /lib/** wl,\n\n  deny /lib64/** wl,\n\n  deny /media/** wl,\n\n  deny /mnt/** wl,\n\n  deny /opt/** wl,\n\n  deny /proc/** wl,\n\n  deny /root/** wl,\n\n  deny /sbin/** wl,\n\n  deny /srv/** wl,\n\n  deny /tmp/** wl,\n\n  deny /sys/** wl,\n\n  deny /usr/** wl,\n\n\n\n  audit /** w,\n\n\n\n  /var/run/nginx.pid w,\n\n\n\n  /usr/sbin/nginx ix,\n\n\n\n  deny /bin/dash mrwklx,\n\n  deny /bin/sh mrwklx,\n\n  deny /usr/bin/top mrwklx,\n\n\n\n\n\n  capability chown,\n\n  capability dac_override,\n\n  capability setuid,\n\n  capability setgid,\n\n  capability net_bind_service,\n\n\n\n  deny @{PROC}/* w,   # deny write for all files directly in /proc (not in a subdir)\n\n  # deny write to files not in /proc/<number>/** or /proc/sys/**\n\n  deny @{PROC}/{[^1-9],[^1-9][^0-9],[^1-9s][^0-9y][^0-9s],[^1-9][^0-9][^0-9][^0-9]*}/** w,\n\n  deny @{PROC}/sys/[^k]** w,  # deny /proc/sys except /proc/sys/k* (effectively /proc/sys/kernel)\n\n  deny @{PROC}/sys/kernel/{?,??,[^s][^h][^m]**} w,  # deny everything except shm* in /proc/sys/kernel/\n\n  deny @{PROC}/sysrq-trigger rwklx,\n\n  deny @{PROC}/mem rwklx,\n\n  deny @{PROC}/kmem rwklx,\n\n  deny @{PROC}/kcore rwklx,\n\n\n\n  deny mount,\n\n\n\n  deny /sys/[^f]*/** wklx,\n\n  deny /sys/f[^s]*/** wklx,\n\n  deny /sys/fs/[^c]*/** wklx,\n\n  deny /sys/fs/c[^g]*/** wklx,\n\n  deny /sys/fs/cg[^r]*/** wklx,\n\n  deny /sys/firmware/** rwklx,\n\n  deny /sys/kernel/security/** rwklx,\n\n}\n\n\nSave the custom profile to disk in the /etc/apparmor.d/containers/docker-nginx file.\n\nThe file path in this example is not a requirement. In production, you could use another.\n\nLoad the profile.\n\n$ sudo apparmor_parser -r -W /etc/apparmor.d/containers/docker-nginx\n\n\nRun a container with the profile.\n\nTo run nginx in detached mode:\n\n$ docker run --security-opt \"apparmor=docker-nginx\" \\\n\n     -p 80:80 -d --name apparmor-nginx nginx\n\n\nExec into the running container.\n\n$ docker container exec -it apparmor-nginx bash\n\n\nTry some operations to test the profile.\n\nroot@6da5a2a930b9:~# ping 8.8.8.8\n\nping: Lacking privilege for raw socket.\n\n\n\nroot@6da5a2a930b9:/# top\n\nbash: /usr/bin/top: Permission denied\n\n\n\nroot@6da5a2a930b9:~# touch ~/thing\n\ntouch: cannot touch 'thing': Permission denied\n\n\n\nroot@6da5a2a930b9:/# sh\n\nbash: /bin/sh: Permission denied\n\n\n\nroot@6da5a2a930b9:/# dash\n\nbash: /bin/dash: Permission denied\n\n\nYou just deployed a container secured with a custom apparmor profile.\n\nDebug AppArmor\n\nYou can use dmesg to debug problems and aa-status check the loaded profiles.\n\nUse dmesg\n\nHere are some helpful tips for debugging any problems you might be facing with regard to AppArmor.\n\nAppArmor sends quite verbose messaging to dmesg. Usually an AppArmor line looks like the following:\n\n[ 5442.864673] audit: type=1400 audit(1453830992.845:37): apparmor=\"ALLOWED\" operation=\"open\" profile=\"/usr/bin/docker\" name=\"/home/jessie/docker/man/man1/docker-attach.1\" pid=10923 comm=\"docker\" requested_mask=\"r\" denied_mask=\"r\" fsuid=1000 ouid=0\n\nIn the above example, you can see profile=/usr/bin/docker. This means the user has the docker-engine (Docker Engine daemon) profile loaded.\n\nLook at another log line:\n\n[ 3256.689120] type=1400 audit(1405454041.341:73): apparmor=\"DENIED\" operation=\"ptrace\" profile=\"docker-default\" pid=17651 comm=\"docker\" requested_mask=\"receive\" denied_mask=\"receive\"\n\nThis time the profile is docker-default, which is run on containers by default unless in privileged mode. This line shows that apparmor has denied ptrace in the container. This is exactly as expected.\n\nUse aa-status\n\nIf you need to check which profiles are loaded, you can use aa-status. The output looks like:\n\n$ sudo aa-status\n\napparmor module is loaded.\n\n14 profiles are loaded.\n\n1 profiles are in enforce mode.\n\n   docker-default\n\n13 profiles are in complain mode.\n\n   /usr/bin/docker\n\n   /usr/bin/docker///bin/cat\n\n   /usr/bin/docker///bin/ps\n\n   /usr/bin/docker///sbin/apparmor_parser\n\n   /usr/bin/docker///sbin/auplink\n\n   /usr/bin/docker///sbin/blkid\n\n   /usr/bin/docker///sbin/iptables\n\n   /usr/bin/docker///sbin/mke2fs\n\n   /usr/bin/docker///sbin/modprobe\n\n   /usr/bin/docker///sbin/tune2fs\n\n   /usr/bin/docker///sbin/xtables-multi\n\n   /usr/bin/docker///sbin/zfs\n\n   /usr/bin/docker///usr/bin/xz\n\n38 processes have profiles defined.\n\n37 processes are in enforce mode.\n\n   docker-default (6044)\n\n   ...\n\n   docker-default (31899)\n\n1 processes are in complain mode.\n\n   /usr/bin/docker (29756)\n\n0 processes are unconfined but have a profile defined.\n\n\nThe above output shows that the docker-default profile running on various container PIDs is in enforce mode. This means AppArmor is actively blocking and auditing in dmesg anything outside the bounds of the docker-default profile.\n\nThe output above also shows the /usr/bin/docker (Docker Engine daemon) profile is running in complain mode. This means AppArmor only logs to dmesg activity outside the bounds of the profile. (Except in the case of Ubuntu Trusty, where some interesting behaviors are enforced.)\n\nContribute to Docker's AppArmor code\n\nAdvanced users and package managers can find a profile for /usr/bin/docker (Docker Engine daemon) underneath contrib/apparmor in the Docker Engine source repository.\n\nThe docker-default profile for containers lives in profiles/apparmor.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUnderstand the policies\nLoad and unload profiles\nResources for writing profiles\nNginx example profile\nDebug AppArmor\nUse dmesg\nUse aa-status\nContribute to Docker's AppArmor code\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995431,
    "timestamp": "2026-02-07T06:33:19.745Z",
    "title": "Content trust in Docker | Docker Docs",
    "url": "https://docs.docker.com/engine/security/trust/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nAutomation with content trust\nDelegations for content trust\nDeploy Notary Server with Compose\nManage keys for content trust\nPlay in a content trust sandbox\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nContent trust in Docker\nContent trust in Docker\nCopy as Markdown\n\nWhen transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and the publisher of all the data a system operates on. You use Docker Engine to push and pull images (data) to a public or private registry. Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.\n\nAbout Docker Content Trust (DCT)\n\nDocker Content Trust (DCT) provides the ability to use digital signatures for data sent to and received from remote Docker registries. These signatures allow client-side or runtime verification of the integrity and publisher of specific image tags.\n\nThrough DCT, image publishers can sign their images and image consumers can ensure that the images they pull are signed. Publishers could be individuals or organizations manually signing their content or automated software supply chains signing content as part of their release process.\n\nNote\n\nDocker is retiring DCT for Docker Official Images (DOI). You should start planning to transition to a different image signing and verification solution (like Sigstore or Notation). Timelines for the complete deprecation of DCT are being finalized and will be published soon.\n\nFor more information, see Retiring Docker Content Trust.\n\nImage tags and DCT\n\nAn individual image record has the following identifier:\n\n[REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG]\n\nA particular image REPOSITORY can have multiple tags. For example, latest and 3.1.2 are both tags on the mongo image. An image publisher can build an image and tag combination many times changing the image with each build.\n\nDCT is associated with the TAG portion of an image. Each image repository has a set of keys that image publishers use to sign an image tag. Image publishers have discretion on which tags they sign.\n\nAn image repository can contain an image with one tag that is signed and another tag that is not. For example, consider the Mongo image repository. The latest tag could be unsigned while the 3.1.6 tag could be signed. It is the responsibility of the image publisher to decide if an image tag is signed or not. In this representation, some image tags are signed, others are not:\n\nPublishers can choose to sign a specific tag or not. As a result, the content of an unsigned tag and that of a signed tag with the same name may not match. For example, a publisher can push a tagged image someimage:latest and sign it. Later, the same publisher can push an unsigned someimage:latest image. This second push replaces the last unsigned tag latest but does not affect the signed latest version. The ability to choose which tags they can sign, allows publishers to iterate over the unsigned version of an image before officially signing it.\n\nImage consumers can enable DCT to ensure that images they use were signed. If a consumer enables DCT, they can only pull, run, or build with trusted images. Enabling DCT is a bit like applying a \"filter\" to your registry. Consumers \"see\" only signed image tags and the less desirable, unsigned image tags are \"invisible\" to them.\n\nTo the consumer who has not enabled DCT, nothing about how they work with Docker images changes. Every image is visible regardless of whether it is signed or not.\n\nDocker Content Trust Keys\n\nTrust for an image tag is managed through the use of signing keys. A key set is created when an operation using DCT is first invoked. A key set consists of the following classes of keys:\n\nAn offline key that is the root of DCT for an image tag\nRepository or tagging keys that sign tags\nServer-managed keys such as the timestamp key, which provides freshness security guarantees for your repository\n\nThe following image depicts the various signing keys and their relationships:\n\nWarning\n\nThe root key once lost is not recoverable. If you lose any other key, send an email to Docker Hub Support. This loss also requires manual intervention from every consumer that used a signed tag from this repository prior to the loss.\n\nYou should back up the root key somewhere safe. Given that it is only required to create new repositories, it is a good idea to store it offline in hardware. For details on securing, and backing up your keys, make sure you read how to manage keys for DCT.\n\nSigning images with Docker Content Trust\n\nWithin the Docker CLI we can sign and push a container image with the $ docker trust command syntax. This is built on top of the Notary feature set. For more information, see the Notary GitHub repository.\n\nA prerequisite for signing an image is a Docker Registry with a Notary server (such as Docker Hub) attached. Refer to Deploying Notary for instructions.\n\nNote\n\nDocker is retiring DCT for Docker Official Images (DOI). You should start planning to transition to a different image signing and verification solution (like Sigstore or Notation). Timelines for the complete deprecation of DCT are being finalized and will be published soon.\n\nFor more information, see Retiring Docker Content Trust.\n\nTo sign a Docker Image you will need a delegation key pair. These keys can be generated locally using $ docker trust key generate or generated by a certificate authority.\n\nFirst we will add the delegation private key to the local Docker trust repository. (By default this is stored in ~/.docker/trust/). If you are generating delegation keys with $ docker trust key generate, the private key is automatically added to the local trust store. If you are importing a separate key, you will need to use the $ docker trust key load command.\n\n$ docker trust key generate jeff\n\nGenerating key for jeff...\n\nEnter passphrase for new jeff key with ID 9deed25:\n\nRepeat passphrase for new jeff key with ID 9deed25:\n\nSuccessfully generated and loaded private key. Corresponding public key available: /home/ubuntu/Documents/mytrustdir/jeff.pub\n\n\nOr if you have an existing key:\n\n$ docker trust key load key.pem --name jeff\n\nLoading key from \"key.pem\"...\n\nEnter passphrase for new jeff key with ID 8ae710e:\n\nRepeat passphrase for new jeff key with ID 8ae710e:\n\nSuccessfully imported key from key.pem\n\n\nNext we will need to add the delegation public key to the Notary server; this is specific to a particular image repository in Notary known as a Global Unique Name (GUN). If this is the first time you are adding a delegation to that repository, this command will also initiate the repository, using a local Notary canonical root key. To understand more about initiating a repository, and the role of delegations, head to delegations for content trust.\n\n$ docker trust signer add --key cert.pem jeff registry.example.com/admin/demo\n\nAdding signer \"jeff\" to registry.example.com/admin/demo...\n\nEnter passphrase for new repository key with ID 10b5e94:\n\n\nFinally, we will use the delegation private key to sign a particular tag and push it up to the registry.\n\n$ docker trust sign registry.example.com/admin/demo:1\n\nSigning and pushing trust data for local image registry.example.com/admin/demo:1, may overwrite remote trust data\n\nThe push refers to repository [registry.example.com/admin/demo]\n\n7bff100f35cb: Pushed\n\n1: digest: sha256:3d2e482b82608d153a374df3357c0291589a61cc194ec4a9ca2381073a17f58e size: 528\n\nSigning and pushing trust metadata\n\nEnter passphrase for signer key with ID 8ae710e:\n\nSuccessfully signed registry.example.com/admin/demo:1\n\n\nAlternatively, once the keys have been imported an image can be pushed with the $ docker push command, by exporting the DCT environmental variable.\n\n$ export DOCKER_CONTENT_TRUST=1\n\n\n\n$ docker push registry.example.com/admin/demo:1\n\nThe push refers to repository [registry.example.com/admin/demo:1]\n\n7bff100f35cb: Pushed\n\n1: digest: sha256:3d2e482b82608d153a374df3357c0291589a61cc194ec4a9ca2381073a17f58e size: 528\n\nSigning and pushing trust metadata\n\nEnter passphrase for signer key with ID 8ae710e:\n\nSuccessfully signed registry.example.com/admin/demo:1\n\n\nRemote trust data for a tag or a repository can be viewed by the $ docker trust inspect command:\n\n$ docker trust inspect --pretty registry.example.com/admin/demo:1\n\n\n\nSignatures for registry.example.com/admin/demo:1\n\n\n\nSIGNED TAG          DIGEST                                                             SIGNERS\n\n1                   3d2e482b82608d153a374df3357c0291589a61cc194ec4a9ca2381073a17f58e   jeff\n\n\n\nList of signers and their keys for registry.example.com/admin/demo:1\n\n\n\nSIGNER              KEYS\n\njeff                8ae710e3ba82\n\n\n\nAdministrative keys for registry.example.com/admin/demo:1\n\n\n\n  Repository Key:\t10b5e94c916a0977471cc08fa56c1a5679819b2005ba6a257aa78ce76d3a1e27\n\n  Root Key:\t84ca6e4416416d78c4597e754f38517bea95ab427e5f95871f90d460573071fc\n\n\nRemote Trust data for a tag can be removed by the $ docker trust revoke command:\n\n$ docker trust revoke registry.example.com/admin/demo:1\n\nEnter passphrase for signer key with ID 8ae710e:\n\nSuccessfully deleted signature for registry.example.com/admin/demo:1\n\nClient enforcement with Docker Content Trust\n\nContent trust is disabled by default in the Docker Client. To enable it, set the DOCKER_CONTENT_TRUST environment variable to 1. This prevents users from working with tagged images unless they contain a signature.\n\nWhen DCT is enabled in the Docker client, docker CLI commands that operate on tagged images must either have content signatures or explicit content hashes. The commands that operate with DCT are:\n\npush\nbuild\ncreate\npull\nrun\n\nFor example, with DCT enabled a docker pull someimage:latest only succeeds if someimage:latest is signed. However, an operation with an explicit content hash always succeeds as long as the hash exists:\n\n$ docker pull registry.example.com/user/image:1\n\nError: remote trust data does not exist for registry.example.com/user/image: registry.example.com does not have trust data for registry.example.com/user/image\n\n\n\n$ docker pull registry.example.com/user/image@sha256:d149ab53f8718e987c3a3024bb8aa0e2caadf6c0328f1d9d850b2a2a67f2819a\n\nsha256:ee7491c9c31db1ffb7673d91e9fac5d6354a89d0e97408567e09df069a1687c1: Pulling from user/image\n\nff3a5c916c92: Pull complete\n\na59a168caba3: Pull complete\n\nDigest: sha256:ee7491c9c31db1ffb7673d91e9fac5d6354a89d0e97408567e09df069a1687c1\n\nStatus: Downloaded newer image for registry.example.com/user/image@sha256:ee7491c9c31db1ffb7673d91e9fac5d6354a89d0e97408567e09df069a1687c1\n\nRelated information\nDelegations for content trust\nAutomation with content trust\nManage keys for content trust\nPlay in a content trust sandbox\n\nEdit this page\n\nRequest changes\n\nTable of contents\nAbout Docker Content Trust (DCT)\nImage tags and DCT\nDocker Content Trust Keys\nSigning images with Docker Content Trust\nClient enforcement with Docker Content Trust\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995434,
    "timestamp": "2026-02-07T06:33:19.749Z",
    "title": "Automation with content trust | Docker Docs",
    "url": "https://docs.docker.com/engine/security/trust/trust_automation/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nAutomation with content trust\nDelegations for content trust\nDeploy Notary Server with Compose\nManage keys for content trust\nPlay in a content trust sandbox\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nContent trust in Docker\n/\nAutomation with content trust\nAutomation with content trust\nCopy as Markdown\n\nIt is very common for Docker Content Trust to be built into existing automation systems. To allow tools to wrap Docker and push trusted content, there are environment variables that can be passed through to the client.\n\nThis guide follows the steps as described in Signing images with Docker Content Trust. Make sure you understand and follow the prerequisites.\n\nWhen working directly with the Notary client, it uses its own set of environment variables.\n\nAdd a delegation private key\n\nTo automate importing a delegation private key to the local Docker trust store, we need to pass a passphrase for the new key. This passphrase will be required everytime that delegation signs a tag.\n\n$ export DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=\"mypassphrase123\"\n\n\n\n$ docker trust key load delegation.key --name jeff\n\nLoading key from \"delegation.key\"...\n\nSuccessfully imported key from delegation.key\n\nAdd a delegation public key\n\nIf you initialize a repository at the same time as adding a delegation public key, then you will need to use the local Notary Canonical Root Key's passphrase to create the repositories trust data. If the repository has already been initiated then you only need the repositories passphrase.\n\n# Export the Local Root Key Passphrase if required.\n\n$ export DOCKER_CONTENT_TRUST_ROOT_PASSPHRASE=\"rootpassphrase123\"\n\n\n\n# Export the Repository Passphrase\n\n$ export DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=\"repopassphrase123\"\n\n\n\n# Initialize Repo and Push Delegation\n\n$ docker trust signer add --key delegation.crt jeff registry.example.com/admin/demo\n\nAdding signer \"jeff\" to registry.example.com/admin/demo...\n\nInitializing signed repository for registry.example.com/admin/demo...\n\nSuccessfully initialized \"registry.example.com/admin/demo\"\n\nSuccessfully added signer: registry.example.com/admin/demo\n\nSign an image\n\nFinally when signing an image, we will need to export the passphrase of the signing key. This was created when the key was loaded into the local Docker trust store with $ docker trust key load.\n\n$ export DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=\"mypassphrase123\"\n\n\n\n$ docker trust sign registry.example.com/admin/demo:1\n\nSigning and pushing trust data for local image registry.example.com/admin/demo:1, may overwrite remote trust data\n\nThe push refers to repository [registry.example.com/admin/demo]\n\n428c97da766c: Layer already exists\n\n2: digest: sha256:1a6fd470b9ce10849be79e99529a88371dff60c60aab424c077007f6979b4812 size: 524\n\nSigning and pushing trust metadata\n\nSuccessfully signed registry.example.com/admin/demo:1\n\nBuild with content trust\n\nYou can also build with content trust. Before running the docker build command, you should set the environment variable DOCKER_CONTENT_TRUST either manually or in a scripted fashion. Consider the simple Dockerfile below.\n\n# syntax=docker/dockerfile:1\n\nFROM docker/trusttest:latest\n\nRUN echo\n\nThe FROM tag is pulling a signed image. You cannot build an image that has a FROM that is not either present locally or signed. Given that content trust data exists for the tag latest, the following build should succeed:\n\n$  docker build -t docker/trusttest:testing .\n\nUsing default tag: latest\n\nlatest: Pulling from docker/trusttest\n\n\n\nb3dbab3810fc: Pull complete\n\na9539b34a6ab: Pull complete\n\nDigest: sha256:d149ab53f871\n\n\nIf content trust is enabled, building from a Dockerfile that relies on tag without trust data, causes the build command to fail:\n\n$  docker build -t docker/trusttest:testing .\n\nunable to process Dockerfile: No trust data for notrust\n\nRelated information\nDelegations for content trust\nContent trust in Docker\nManage keys for content trust\nPlay in a content trust sandbox\n\nEdit this page\n\nRequest changes\n\nTable of contents\nAdd a delegation private key\nAdd a delegation public key\nSign an image\nBuild with content trust\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995435,
    "timestamp": "2026-02-07T06:33:19.754Z",
    "title": "Delegations for content trust | Docker Docs",
    "url": "https://docs.docker.com/engine/security/trust/trust_delegation/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nAutomation with content trust\nDelegations for content trust\nDeploy Notary Server with Compose\nManage keys for content trust\nPlay in a content trust sandbox\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nContent trust in Docker\n/\nDelegations for content trust\nDelegations for content trust\nCopy as Markdown\n\nDelegations in Docker Content Trust (DCT) allow you to control who can and cannot sign an image tag. A delegation will have a pair of private and public delegation keys. A delegation could contain multiple pairs of keys and contributors in order to a) allow multiple users to be part of a delegation, and b) to support key rotation.\n\nThe most important delegation within Docker Content Trust is targets/releases. This is seen as the canonical source of a trusted image tag, and without a contributor's key being under this delegation, they will be unable to sign a tag.\n\nFortunately when using the $ docker trust commands, we will automatically initialize a repository, manage the repository keys, and add a collaborator's key to the targets/releases delegation via docker trust signer add.\n\nConfiguring the Docker client\n\nBy default, the $ docker trust commands expect the notary server URL to be the same as the registry URL specified in the image tag (following a similar logic to $ docker push). When using Docker Hub or DTR, the notary server URL is the same as the registry URL. However, for self-hosted environments or 3rd party registries, you will need to specify an alternative URL of the notary server. This is done with:\n\n$ export DOCKER_CONTENT_TRUST_SERVER=https://URL:PORT\n\n\nIf you do not export this variable in self-hosted environments, you may see errors such as:\n\n$ docker trust signer add --key cert.pem jeff registry.example.com/admin/demo\n\nAdding signer \"jeff\" to registry.example.com/admin/demo...\n\n<...>\n\nError: trust data missing for remote repository registry.example.com/admin/demo or remote repository not found: timestamp key trust data unavailable.  Has a notary repository been initialized?\n\n\n\n$ docker trust inspect registry.example.com/admin/demo --pretty\n\nWARN[0000] Error while downloading remote metadata, using cached timestamp - this might not be the latest version available remotely\n\n<...>\n\n\nIf you have enabled authentication for your notary server, or are using DTR, you will need to log in before you can push data to the notary server.\n\n$ docker login registry.example.com/user/repo\n\nUsername: admin\n\nPassword:\n\n\n\nLogin Succeeded\n\n\n\n$ docker trust signer add --key cert.pem jeff registry.example.com/user/repo\n\nAdding signer \"jeff\" to registry.example.com/user/repo...\n\nInitializing signed repository for registry.example.com/user/repo...\n\nSuccessfully initialized \"registry.example.com/user/repo\"\n\nSuccessfully added signer: jeff to registry.example.com/user/repo\n\n\nIf you do not log in, you will see:\n\n$ docker trust signer add --key cert.pem jeff registry.example.com/user/repo\n\nAdding signer \"jeff\" to registry.example.com/user/repo...\n\nInitializing signed repository for registry.example.com/user/repo...\n\nyou are not authorized to perform this operation: server returned 401.\n\n\n\nFailed to add signer to: registry.example.com/user/repo\n\nConfiguring the Notary client\n\nSome of the more advanced features of DCT require the Notary CLI. To install and configure the Notary CLI:\n\nDownload the client and ensure that it is available on your path.\n\nCreate a configuration file at ~/.notary/config.json with the following content:\n\n{\n\n  \"trust_dir\" : \"~/.docker/trust\",\n\n  \"remote_server\": {\n\n    \"url\": \"https://registry.example.com\",\n\n    \"root_ca\": \"../.docker/ca.pem\"\n\n  }\n\n}\n\nThe newly created configuration file contains information about the location of your local Docker trust data and the notary server URL.\n\nFor more detailed information about how to use notary outside of the Docker Content Trust use cases, refer to the Notary CLI documentation\n\nCreating delegation keys\n\nA prerequisite to adding your first contributor is a pair of delegation keys. These keys can either be generated locally using $ docker trust, generated by a certificate authority.\n\nUsing Docker Trust to generate keys\n\nDocker trust has a built-in generator for a delegation key pair, $ docker trust generate <name>. Running this command will automatically load the delegation private key in to the local Docker trust store.\n\n$ docker trust key generate jeff\n\n\n\nGenerating key for jeff...\n\nEnter passphrase for new jeff key with ID 9deed25: \n\nRepeat passphrase for new jeff key with ID 9deed25: \n\nSuccessfully generated and loaded private key. Corresponding public key available: /home/ubuntu/Documents/mytrustdir/jeff.pub\n\nManually generating keys\n\nIf you need to manually generate a private key (either RSA or ECDSA) and an X.509 certificate containing the public key, you can use local tools like openssl or cfssl along with a local or company-wide Certificate Authority.\n\nHere is an example of how to generate a 2048-bit RSA portion key (all RSA keys must be at least 2048 bits):\n\n$ openssl genrsa -out delegation.key 2048\n\n\n\nGenerating RSA private key, 2048 bit long modulus\n\n....................................................+++\n\n............+++\n\ne is 65537 (0x10001)\n\n\nThey should keep delegation.key private because it is used to sign tags.\n\nThen they need to generate an x509 certificate containing the public key, which is what you need from them. Here is the command to generate a CSR (certificate signing request):\n\n$ openssl req -new -sha256 -key delegation.key -out delegation.csr\n\n\nThen they can send it to whichever CA you trust to sign certificates, or they can self-sign the certificate (in this example, creating a certificate that is valid for 1 year):\n\n$ openssl x509 -req -sha256 -days 365 -in delegation.csr -signkey delegation.key -out delegation.crt\n\n\nThen they need to give you delegation.crt, whether it is self-signed or signed by a CA.\n\nFinally you will need to add the private key into your local Docker trust store.\n\n$ docker trust key load delegation.key --name jeff\n\n\n\nLoading key from \"delegation.key\"...\n\nEnter passphrase for new jeff key with ID 8ae710e: \n\nRepeat passphrase for new jeff key with ID 8ae710e: \n\nSuccessfully imported key from delegation.key\n\nViewing local delegation keys\n\nTo list the keys that have been imported in to the local Docker trust store we can use the Notary CLI.\n\n$ notary key list\n\n\n\nROLE       GUN                          KEY ID                                                              LOCATION\n\n----       ---                          ------                                                              --------\n\nroot                                    f6c6a4b00fefd8751f86194c7d87a3bede444540eb3378c4a11ce10852ab1f96    /home/ubuntu/.docker/trust/private\n\njeff                                    9deed251daa1aa6f9d5f9b752847647cf8d705da0763aa5467650d0987ed5306    /home/ubuntu/.docker/trust/private\n\nManaging delegations in a Notary Server\n\nWhen the first delegation is added to the Notary Server using $ docker trust, we automatically initiate trust data for the repository. This includes creating the notary target and snapshots keys, and rotating the snapshot key to be managed by the notary server. More information on these keys can be found in Manage keys for content trust.\n\nWhen initiating a repository, you will need the key and the passphrase of a local Notary Canonical Root Key. If you have not initiated a repository before, and therefore don't have a Notary root key, $ docker trust will create one for you.\n\nImportant\n\nBe sure to protect and back up your Notary Canonical Root Key.\n\nInitiating the repository\n\nTo upload the first key to a delegation, at the same time initiating a repository, you can use the $ docker trust signer add command. This will add the contributor's public key to the targets/releases delegation, and create a second targets/<name> delegation.\n\nFor DCT the name of the second delegation, in the below example jeff, is there to help you keep track of the owner of the keys. In more advanced use cases of Notary additional delegations are used for hierarchy.\n\n$ docker trust signer add --key cert.pem jeff registry.example.com/admin/demo\n\n\n\nAdding signer \"jeff\" to registry.example.com/admin/demo...\n\nInitializing signed repository for registry.example.com/admin/demo...\n\nEnter passphrase for root key with ID f6c6a4b: \n\nEnter passphrase for new repository key with ID b0014f8: \n\nRepeat passphrase for new repository key with ID b0014f8: \n\nSuccessfully initialized \"registry.example.com/admin/demo\"\n\nSuccessfully added signer: jeff to registry.example.com/admin/demo\n\n\nYou can see which keys have been pushed to the Notary server for each repository with the $ docker trust inspect command.\n\n$ docker trust inspect --pretty registry.example.com/admin/demo\n\n\n\nNo signatures for registry.example.com/admin/demo\n\n\n\n\n\nList of signers and their keys for registry.example.com/admin/demo\n\n\n\nSIGNER              KEYS\n\njeff                1091060d7bfd\n\n\n\nAdministrative keys for registry.example.com/admin/demo\n\n\n\n  Repository Key:\tb0014f8e4863df2d028095b74efcb05d872c3591de0af06652944e310d96598d\n\n  Root Key:\t64d147e59e44870311dd2d80b9f7840039115ef3dfa5008127d769a5f657a5d7\n\n\nYou could also use the Notary CLI to list delegations and keys. Here you can clearly see the keys were attached to targets/releases and targets/jeff.\n\n$ notary delegation list registry.example.com/admin/demo\n\n\n\nROLE                PATHS             KEY IDS                                                             THRESHOLD\n\n----                -----             -------                                                             ---------\n\ntargets/jeff        \"\" <all paths>    1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1    1\n\n                                          \n\ntargets/releases    \"\" <all paths>    1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1    1 \n\nAdding additional signers\n\nDocker Trust allows you to configure multiple delegations per repository, allowing you to manage the lifecycle of delegations. When adding additional delegations with $ docker trust the collaborators key is once again added to the targets/release role.\n\nNote you will need the passphrase for the repository key; this would have been configured when you first initiated the repository.\n\n$ docker trust signer add --key ben.pub ben registry.example.com/admin/demo\n\n\n\nAdding signer \"ben\" to registry.example.com/admin/demo...\n\nEnter passphrase for repository key with ID b0014f8: \n\nSuccessfully added signer: ben to registry.example.com/admin/demo\n\n\nCheck to prove that there are now 2 delegations (Signer).\n\n$ docker trust inspect --pretty registry.example.com/admin/demo\n\n\n\nNo signatures for registry.example.com/admin/demo\n\n\n\nList of signers and their keys for registry.example.com/admin/demo\n\n\n\nSIGNER              KEYS\n\nben                 afa404703b25\n\njeff                1091060d7bfd\n\n\n\nAdministrative keys for registry.example.com/admin/demo\n\n\n\n  Repository Key:\tb0014f8e4863df2d028095b74efcb05d872c3591de0af06652944e310d96598d\n\n  Root Key:\t64d147e59e44870311dd2d80b9f7840039115ef3dfa5008127d769a5f657a5d7\n\nAdding keys to an existing delegation\n\nTo support things like key rotation and expiring / retiring keys you can publish multiple contributor keys per delegation. The only prerequisite here is to make sure you use the same the delegation name, in this case jeff. Docker trust will automatically handle adding this new key to targets/releases.\n\nNote\n\nYou will need the passphrase for the repository key; this would have been configured when you first initiated the repository.\n\n$ docker trust signer add --key cert2.pem jeff registry.example.com/admin/demo\n\n\n\nAdding signer \"jeff\" to registry.example.com/admin/demo...\n\nEnter passphrase for repository key with ID b0014f8: \n\nSuccessfully added signer: jeff to registry.example.com/admin/demo\n\n\nCheck to prove that the delegation (Signer) now contains multiple Key IDs.\n\n$ docker trust inspect --pretty registry.example.com/admin/demo\n\n\n\nNo signatures for registry.example.com/admin/demo\n\n\n\n\n\nList of signers and their keys for registry.example.com/admin/demo\n\n\n\nSIGNER              KEYS\n\njeff                1091060d7bfd, 5570b88df073\n\n\n\nAdministrative keys for registry.example.com/admin/demo\n\n\n\n  Repository Key:\tb0014f8e4863df2d028095b74efcb05d872c3591de0af06652944e310d96598d\n\n  Root Key:\t64d147e59e44870311dd2d80b9f7840039115ef3dfa5008127d769a5f657a5d7\n\nRemoving a delegation\n\nIf you need to remove a delegation, including the contributor keys that are attached to the targets/releases role, you can use the $ docker trust signer remove command.\n\nNote\n\nTags that were signed by the removed delegation will need to be resigned by an active delegation\n\n$ docker trust signer remove ben registry.example.com/admin/demo\n\nRemoving signer \"ben\" from registry.example.com/admin/demo...\n\nEnter passphrase for repository key with ID b0014f8: \n\nSuccessfully removed ben from registry.example.com/admin/demo\n\nTroubleshooting\n\nIf you see an error that there are no usable keys in targets/releases, you will need to add additional delegations using docker trust signer add before resigning images.\n\nWARN[0000] role targets/releases has fewer keys than its threshold of 1; it will not be usable until keys are added to it\n\nIf you have added additional delegations already and are seeing an error message that there are no valid signatures in targest/releases, you will need to resign the targets/releases delegation file with the Notary CLI.\n\nWARN[0000] Error getting targets/releases: valid signatures did not meet threshold for targets/releases \n\nResigning the delegation file is done with the $ notary witness command\n\n$ notary witness registry.example.com/admin/demo targets/releases --publish\n\n\nFor more information on the notary witness command, refer to the Notary client advanced usage guide\n\nRemoving a contributor's key from a delegation\n\nAs part of rotating keys for a delegation, you may want to remove an individual key but retain the delegation. This can be done with the Notary CLI.\n\nRemember you will have to remove the key from both the targets/releases role and the role specific to that signer targets/<name>.\n\nWe will need to grab the Key ID from the Notary Server\n\n$ notary delegation list registry.example.com/admin/demo\n\n\n\nROLE                PATHS             KEY IDS                                                             THRESHOLD\n\n----                -----             -------                                                             ---------\n\ntargets/jeff        \"\" <all paths>    8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9    1\n\n                                      1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1    \n\ntargets/releases    \"\" <all paths>    8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9    1\n\n                                      1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1    \n\n\nRemove from the targets/releases delegation\n\n$ notary delegation remove registry.example.com/admin/demo targets/releases 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 --publish\n\n\n\nAuto-publishing changes to registry.example.com/admin/demo\n\nEnter username: admin\n\nEnter password: \n\nEnter passphrase for targets key with ID b0014f8: \n\nSuccessfully published changes for repository registry.example.com/admin/demo\n\n\nRemove from the targets/<name> delegation\n\n$ notary delegation remove registry.example.com/admin/demo targets/jeff 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 --publish\n\n\n\nRemoval of delegation role targets/jeff with keys [5570b88df0736c468493247a07e235e35cf3641270c944d0e9e8899922fc6f99], to repository \"registry.example.com/admin/demo\" staged for next publish.\n\n\n\nAuto-publishing changes to registry.example.com/admin/demo\n\nEnter username: admin    \n\nEnter password: \n\nEnter passphrase for targets key with ID b0014f8: \n\nSuccessfully published changes for repository registry.example.com/admin/demo\n\n\nCheck the remaining delegation list\n\n$ notary delegation list registry.example.com/admin/demo\n\n\n\nROLE                PATHS             KEY IDS                                                             THRESHOLD\n\n----                -----             -------                                                             ---------\n\ntargets/jeff        \"\" <all paths>    8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9    1    \n\ntargets/releases    \"\" <all paths>    8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9    1    \n\nRemoving a local delegation private key\n\nAs part of rotating delegation keys, you may need to remove a local delegation key from the local Docker trust store. This is done with the Notary CLI, using the $ notary key remove command.\n\nWe will need to get the Key ID from the local Docker Trust store\n\n$ notary key list\n\n\n\nROLE       GUN                          KEY ID                                                              LOCATION\n\n----       ---                          ------                                                              --------\n\nroot                                    f6c6a4b00fefd8751f86194c7d87a3bede444540eb3378c4a11ce10852ab1f96    /home/ubuntu/.docker/trust/private\n\nadmin                                   8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9    /home/ubuntu/.docker/trust/private\n\njeff                                    1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1    /home/ubuntu/.docker/trust/private\n\ntargets    ...example.com/admin/demo    c819f2eda8fba2810ec6a7f95f051c90276c87fddfc3039058856fad061c009d    /home/ubuntu/.docker/trust/private\n\n\nRemove the key from the local Docker Trust store\n\n$ notary key remove 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1\n\n\n\nAre you sure you want to remove 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 (role jeff) from /home/ubuntu/.docker/trust/private?  (yes/no)  y\n\n\n\nDeleted 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 (role jeff) from /home/ubuntu/.docker/trust/private.\n\nRemoving all trust data from a repository\n\nYou can remove all trust data from a repository, including repository, target, snapshot and all delegations keys using the Notary CLI.\n\nThis is often required by a container registry before a particular repository can be deleted.\n\n$ notary delete registry.example.com/admin/demo --remote\n\n\n\nDeleting trust data for repository registry.example.com/admin/demo\n\nEnter username: admin\n\nEnter password: \n\nSuccessfully deleted local and remote trust data for repository registry.example.com/admin/demo\n\n\n\n$ docker trust inspect --pretty registry.example.com/admin/demo\n\n\n\nNo signatures or cannot access registry.example.com/admin/demo\n\nRelated information\nContent trust in Docker\nManage keys for content trust\nAutomation with content trust\nPlay in a content trust sandbox\n\nEdit this page\n\nRequest changes\n\nTable of contents\nConfiguring the Docker client\nConfiguring the Notary client\nCreating delegation keys\nUsing Docker Trust to generate keys\nManually generating keys\nViewing local delegation keys\nManaging delegations in a Notary Server\nInitiating the repository\nAdding additional signers\nAdding keys to an existing delegation\nRemoving a delegation\nRemoving a contributor's key from a delegation\nRemoving a local delegation private key\nRemoving all trust data from a repository\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995438,
    "timestamp": "2026-02-07T06:33:19.756Z",
    "title": "Deploy Notary Server with Compose | Docker Docs",
    "url": "https://docs.docker.com/engine/security/trust/deploying_notary/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nAutomation with content trust\nDelegations for content trust\nDeploy Notary Server with Compose\nManage keys for content trust\nPlay in a content trust sandbox\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nContent trust in Docker\n/\nDeploy Notary Server with Compose\nDeploy Notary Server with Compose\nCopy as Markdown\n\nThe easiest way to deploy Notary Server is by using Docker Compose. To follow the procedure on this page, you must have already installed Docker Compose.\n\nClone the Notary repository.\n\n$ git clone https://github.com/theupdateframework/notary.git\n\n\nBuild and start Notary Server with the sample certificates.\n\n$ docker compose up -d \n\n\nFor more detailed documentation about how to deploy Notary Server, see the instructions to run a Notary service as well as the Notary repository for more information.\n\nMake sure that your Docker or Notary client trusts Notary Server's certificate before you try to interact with the Notary server.\n\nSee the instructions for Docker or for Notary depending on which one you are using.\n\nIf you want to use Notary in production\n\nCheck back here for instructions after Notary Server has an official stable release. To get a head start on deploying Notary in production, see the Notary repository.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nIf you want to use Notary in production\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995441,
    "timestamp": "2026-02-07T06:33:19.763Z",
    "title": "Manage keys for content trust | Docker Docs",
    "url": "https://docs.docker.com/engine/security/trust/trust_key_mng/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nAutomation with content trust\nDelegations for content trust\nDeploy Notary Server with Compose\nManage keys for content trust\nPlay in a content trust sandbox\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nContent trust in Docker\n/\nManage keys for content trust\nManage keys for content trust\nCopy as Markdown\n\nTrust for an image tag is managed through the use of keys. Docker's content trust makes use of five different types of keys:\n\nKey\tDescription\nroot key\tRoot of content trust for an image tag. When content trust is enabled, you create the root key once. Also known as the offline key, because it should be kept offline.\ntargets\tThis key allows you to sign image tags, to manage delegations including delegated keys or permitted delegation paths. Also known as the repository key, since this key determines what tags can be signed into an image repository.\nsnapshot\tThis key signs the current collection of image tags, preventing mix and match attacks.\ntimestamp\tThis key allows Docker image repositories to have freshness security guarantees without requiring periodic content refreshes on the client's side.\ndelegation\tDelegation keys are optional tagging keys and allow you to delegate signing image tags to other publishers without having to share your targets key.\n\nWhen doing a docker push with Content Trust enabled for the first time, the root, targets, snapshot, and timestamp keys are generated automatically for the image repository:\n\nThe root and targets key are generated and stored locally client-side.\n\nThe timestamp and snapshot keys are safely generated and stored in a signing server that is deployed alongside the Docker registry. These keys are generated in a backend service that isn't directly exposed to the internet and are encrypted at rest. Use the Notary CLI to manage your snapshot key locally.\n\nDelegation keys are optional, and not generated as part of the normal docker workflow. They need to be manually generated and added to the repository.\n\nChoose a passphrase\n\nThe passphrases you chose for both the root key and your repository key should be randomly generated and stored in a password manager. Having the repository key allows users to sign image tags on a repository. Passphrases are used to encrypt your keys at rest and ensure that a lost laptop or an unintended backup doesn't put the private key material at risk.\n\nBack up your keys\n\nAll the Docker trust keys are stored encrypted using the passphrase you provide on creation. Even so, you should still take care of the location where you back them up. Good practice is to create two encrypted USB keys.\n\nWarning\n\nIt is very important that you back up your keys to a safe, secure location. The loss of the repository key is recoverable, but the loss of the root key is not.\n\nThe Docker client stores the keys in the ~/.docker/trust/private directory. Before backing them up, you should tar them into an archive:\n\n$ umask 077; tar -zcvf private_keys_backup.tar.gz ~/.docker/trust/private; umask 022\n\nHardware storage and signing\n\nDocker Content Trust can store and sign with root keys from a Yubikey 4. The Yubikey is prioritized over keys stored in the filesystem. When you initialize a new repository with content trust, Docker Engine looks for a root key locally. If a key is not found and the Yubikey 4 exists, Docker Engine creates a root key in the Yubikey 4. Consult the Notary documentation for more details.\n\nPrior to Docker Engine 1.11, this feature was only in the experimental branch.\n\nKey loss\nWarning\n\nIf a publisher loses keys it means losing the ability to sign images for the repositories in question. If you lose a key, send an email to Docker Hub Support. As a reminder, the loss of a root key is not recoverable.\n\nThis loss also requires manual intervention from every consumer that used a signed tag from this repository prior to the loss.\nImage consumers get the following error for content previously downloaded from the affected repo(s):\n\nWarning: potential malicious behavior - trust data has insufficient signatures for remote repository docker.io/my/image: valid signatures did not meet threshold\n\n\nTo correct this, they need to download a new image tag that is signed with the new key.\n\nRelated information\nContent trust in Docker\nAutomation with content trust\nDelegations for content trust\nPlay in a content trust sandbox\n\nEdit this page\n\nRequest changes\n\nTable of contents\nChoose a passphrase\nBack up your keys\nHardware storage and signing\nKey loss\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995446,
    "timestamp": "2026-02-07T06:33:19.768Z",
    "title": "Play in a content trust sandbox | Docker Docs",
    "url": "https://docs.docker.com/engine/security/trust/trust_sandbox/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nAutomation with content trust\nDelegations for content trust\nDeploy Notary Server with Compose\nManage keys for content trust\nPlay in a content trust sandbox\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nContent trust in Docker\n/\nPlay in a content trust sandbox\nPlay in a content trust sandbox\nCopy as Markdown\n\nThis page explains how to set up and use a sandbox for experimenting with trust. The sandbox allows you to configure and try trust operations locally without impacting your production images.\n\nBefore working through this sandbox, you should have read through the trust overview.\n\nPrerequisites\n\nThese instructions assume you are running in Linux or macOS. You can run this sandbox on a local machine or on a virtual machine. You need to have privileges to run docker commands on your local machine or in the VM.\n\nThis sandbox requires you to install two Docker tools: Docker Engine >= 1.10.0 and Docker Compose >= 1.6.0. To install the Docker Engine, choose from the list of supported platforms. To install Docker Compose, see the detailed instructions here.\n\nWhat is in the sandbox?\n\nIf you are just using trust out-of-the-box you only need your Docker Engine client and access to the Docker Hub. The sandbox mimics a production trust environment, and sets up these additional components.\n\nContainer\tDescription\ntrustsandbox\tA container with the latest version of Docker Engine and with some preconfigured certificates. This is your sandbox where you can use the docker client to test trust operations.\nRegistry server\tA local registry service.\nNotary server\tThe service that does all the heavy-lifting of managing trust\n\nThis means you run your own content trust (Notary) server and registry. If you work exclusively with the Docker Hub, you would not need these components. They are built into the Docker Hub for you. For the sandbox, however, you build your own entire, mock production environment.\n\nWithin the trustsandbox container, you interact with your local registry rather than the Docker Hub. This means your everyday image repositories are not used. They are protected while you play.\n\nWhen you play in the sandbox, you also create root and repository keys. The sandbox is configured to store all the keys and files inside the trustsandbox container. Since the keys you create in the sandbox are for play only, destroying the container destroys them as well.\n\nBy using a docker-in-docker image for the trustsandbox container, you also don't pollute your real Docker daemon cache with any images you push and pull. The images are stored in an anonymous volume attached to this container, and can be destroyed after you destroy the container.\n\nBuild the sandbox\n\nIn this section, you use Docker Compose to specify how to set up and link together the trustsandbox container, the Notary server, and the Registry server.\n\nCreate a new trustsandbox directory and change into it.\n\n$ mkdir trustsandbox\n\n$ cd trustsandbox\n\n\nCreate a file called compose.yaml with your favorite editor. For example, using vim:\n\n$ touch compose.yaml\n\n$ vim compose.yaml\n\n\nAdd the following to the new file.\n\nversion: \"2\"\n\nservices:\n\n  notaryserver:\n\n    image: dockersecurity/notary_autobuilds:server-v0.5.1\n\n    volumes:\n\n      - notarycerts:/var/lib/notary/fixtures\n\n    networks:\n\n      - sandbox\n\n    environment:\n\n      - NOTARY_SERVER_STORAGE_TYPE=memory\n\n      - NOTARY_SERVER_TRUST_SERVICE_TYPE=local\n\n  sandboxregistry:\n\n    image: registry:3\n\n    networks:\n\n      - sandbox\n\n    container_name: sandboxregistry\n\n  trustsandbox:\n\n    image: docker:dind\n\n    networks:\n\n      - sandbox\n\n    volumes:\n\n      - notarycerts:/notarycerts\n\n    privileged: true\n\n    container_name: trustsandbox\n\n    entrypoint: \"\"\n\n    command: |-\n\n        sh -c '\n\n            cp /notarycerts/root-ca.crt /usr/local/share/ca-certificates/root-ca.crt &&\n\n            update-ca-certificates &&\n\n            dockerd-entrypoint.sh --insecure-registry sandboxregistry:5000'\n\nvolumes:\n\n  notarycerts:\n\n    external: false\n\nnetworks:\n\n  sandbox:\n\n    external: false\n\nSave and close the file.\n\nRun the containers on your local system.\n\n$ docker compose up -d\n\n\nThe first time you run this, the docker-in-docker, Notary server, and registry images are downloaded from Docker Hub.\n\nPlay in the sandbox\n\nNow that everything is setup, you can go into your trustsandbox container and start testing Docker content trust. From your host machine, obtain a shell in the trustsandbox container.\n\n$ docker container exec -it trustsandbox sh\n\n/ #\n\nTest some trust operations\n\nNow, pull some images from within the trustsandbox container.\n\nDownload a docker image to test with.\n\n/ # docker pull docker/trusttest\n\ndocker pull docker/trusttest\n\nUsing default tag: latest\n\nlatest: Pulling from docker/trusttest   \n\nb3dbab3810fc: Pull complete\n\na9539b34a6ab: Pull complete\n\nDigest: sha256:d149ab53f8718e987c3a3024bb8aa0e2caadf6c0328f1d9d850b2a2a67f2819a\n\nStatus: Downloaded newer image for docker/trusttest:latest\n\n\nTag it to be pushed to your sandbox registry:\n\n/ # docker tag docker/trusttest sandboxregistry:5000/test/trusttest:latest\n\n\nEnable content trust.\n\n/ # export DOCKER_CONTENT_TRUST=1\n\n\nIdentify the trust server.\n\n/ # export DOCKER_CONTENT_TRUST_SERVER=https://notaryserver:4443\n\n\nThis step is only necessary because the sandbox is using its own server. Normally, if you are using the Docker Public Hub this step isn't necessary.\n\nPull the test image.\n\n/ # docker pull sandboxregistry:5000/test/trusttest\n\nUsing default tag: latest\n\nError: remote trust data does not exist for sandboxregistry:5000/test/trusttest: notaryserver:4443 does not have trust data for      sandboxregistry:5000/test/trusttest\n\n\nYou see an error, because this content doesn't exist on the notaryserver yet.\n\nPush and sign the trusted image.\n\n/ # docker push sandboxregistry:5000/test/trusttest:latest\n\nThe push refers to a repository [sandboxregistry:5000/test/trusttest]\n\n5f70bf18a086: Pushed\n\nc22f7bc058a9: Pushed\n\nlatest: digest: sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926 size: 734\n\nSigning and pushing trust metadata\n\nYou are about to create a new root signing key passphrase. This passphrase\n\nwill be used to protect the most sensitive key in your signing system. Please\n\nchoose a long, complex passphrase and be careful to keep the password and the\n\nkey file itself secure and backed up. It is highly recommended that you use a\n\npassword manager to generate the passphrase and keep it safe. There will be no\n\nway to recover this key. You can find the key in your config directory.\n\nEnter passphrase for new root key with ID 27ec255:\n\nRepeat passphrase for new root key with ID 27ec255:\n\nEnter passphrase for new repository key with ID 58233f9 (sandboxregistry:5000/test/trusttest):\n\nRepeat passphrase for new repository key with ID 58233f9 (sandboxregistry:5000/test/trusttest):\n\nFinished initializing \"sandboxregistry:5000/test/trusttest\"\n\nSuccessfully signed \"sandboxregistry:5000/test/trusttest\":latest\n\n\nBecause you are pushing this repository for the first time, Docker creates new root and repository keys and asks you for passphrases with which to encrypt them. If you push again after this, it only asks you for repository passphrase so it can decrypt the key and sign again.\n\nTry pulling the image you just pushed:\n\n/ # docker pull sandboxregistry:5000/test/trusttest\n\nUsing default tag: latest\n\nPull (1 of 1): sandboxregistry:5000/test/trusttest:latest@sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926\n\nsha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926: Pulling from test/trusttest\n\nDigest: sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926\n\nStatus: Downloaded newer image for sandboxregistry:5000/test/trusttest@sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926\n\nTagging sandboxregistry:5000/test/trusttest@sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926 as sandboxregistry:5000   test/trusttest:latest\n\nTest with malicious images\n\nWhat happens when data is corrupted and you try to pull it when trust is enabled? In this section, you go into the sandboxregistry and tamper with some data. Then, you try and pull it.\n\nLeave the trustsandbox shell and container running.\n\nOpen a new interactive terminal from your host, and obtain a shell into the sandboxregistry container.\n\n$ docker container exec -it sandboxregistry bash\n\nroot@65084fc6f047:/#\n\n\nList the layers for the test/trusttest image you pushed:\n\nroot@65084fc6f047:/# ls -l /var/lib/registry/docker/registry/v2/repositories/test/trusttest/_layers/sha256\n\ntotal 12\n\ndrwxr-xr-x 2 root root 4096 Jun 10 17:26 a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\n\ndrwxr-xr-x 2 root root 4096 Jun 10 17:26 aac0c133338db2b18ff054943cee3267fe50c75cdee969aed88b1992539ed042\n\ndrwxr-xr-x 2 root root 4096 Jun 10 17:26 cc7629d1331a7362b5e5126beb5bf15ca0bf67eb41eab994c719a45de53255cd\n\n\nChange into the registry storage for one of those layers (this is in a different directory):\n\nroot@65084fc6f047:/# cd /var/lib/registry/docker/registry/v2/blobs/sha256/aa/aac0c133338db2b18ff054943cee3267fe50c75cdee969aed88b1992539ed042\n\n\nAdd malicious data to one of the trusttest layers:\n\nroot@65084fc6f047:/# echo \"Malicious data\" > data\n\n\nGo back to your trustsandbox terminal.\n\nList the trusttest image.\n\n/ # docker image ls | grep trusttest\n\nREPOSITORY                            TAG                 IMAGE ID            CREATED             SIZE\n\ndocker/trusttest                      latest              cc7629d1331a        11 months ago       5.025 MB\n\nsandboxregistry:5000/test/trusttest   latest              cc7629d1331a        11 months ago       5.025 MB\n\nsandboxregistry:5000/test/trusttest   <none>              cc7629d1331a        11 months ago       5.025 MB\n\n\nRemove the trusttest:latest image from your local cache.\n\n/ # docker image rm -f cc7629d1331a\n\nUntagged: docker/trusttest:latest\n\nUntagged: sandboxregistry:5000/test/trusttest:latest\n\nUntagged: sandboxregistry:5000/test/trusttest@sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926\n\nDeleted: sha256:cc7629d1331a7362b5e5126beb5bf15ca0bf67eb41eab994c719a45de53255cd\n\nDeleted: sha256:2a1f6535dc6816ffadcdbe20590045e6cbf048d63fd4cc753a684c9bc01abeea\n\nDeleted: sha256:c22f7bc058a9a8ffeb32989b5d3338787e73855bf224af7aa162823da015d44c\n\n\nDocker does not re-download images that it already has cached, but you want Docker to attempt to download the tampered image from the registry and reject it because it is invalid.\n\nPull the image again. This downloads the image from the registry, because you don't have it cached.\n\n/ # docker pull sandboxregistry:5000/test/trusttest\n\nUsing default tag: latest\n\nPull (1 of 1): sandboxregistry:5000/test/trusttest:latest@sha256:35d5bc26fd358da8320c137784fe590d8fcf9417263ef261653e8e1c7f15672e\n\nsha256:35d5bc26fd358da8320c137784fe590d8fcf9417263ef261653e8e1c7f15672e: Pulling from test/trusttest\n\n\n\naac0c133338d: Retrying in 5 seconds\n\na3ed95caeb02: Download complete\n\nerror pulling image configuration: unexpected EOF\n\n\nThe pull did not complete because the trust system couldn't verify the image.\n\nMore play in the sandbox\n\nNow, you have a full Docker content trust sandbox on your local system, feel free to play with it and see how it behaves. If you find any security issues with Docker, feel free to send us an email at security@docker.com.\n\nClean up your sandbox\n\nWhen you are done, and want to clean up all the services you've started and any anonymous volumes that have been created, just run the following command in the directory where you've created your Docker Compose file:\n\n$ docker compose down -v\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nWhat is in the sandbox?\nBuild the sandbox\nPlay in the sandbox\nTest some trust operations\nTest with malicious images\nMore play in the sandbox\nClean up your sandbox\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995447,
    "timestamp": "2026-02-07T06:33:19.769Z",
    "title": "Docker security non-events | Docker Docs",
    "url": "https://docs.docker.com/engine/security/non-events/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nDocker security non-events\nDocker security non-events\nCopy as Markdown\n\nThis page lists security vulnerabilities which Docker mitigated, such that processes run in Docker containers were never vulnerable to the bug‚Äîeven before it was fixed. This assumes containers are run without adding extra capabilities or not run as --privileged.\n\nThe list below is not even remotely complete. Rather, it is a sample of the few bugs we've actually noticed to have attracted security review and publicly disclosed vulnerabilities. In all likelihood, the bugs that haven't been reported far outnumber those that have. Luckily, since Docker's approach to secure by default through apparmor, seccomp, and dropping capabilities, it likely mitigates unknown bugs just as well as it does known ones.\n\nBugs mitigated:\n\nCVE-2013-1956, 1957, 1958, 1959, 1979, CVE-2014-4014, 5206, 5207, 7970, 7975, CVE-2015-2925, 8543, CVE-2016-3134, 3135, etc.: The introduction of unprivileged user namespaces lead to a huge increase in the attack surface available to unprivileged users by giving such users legitimate access to previously root-only system calls like mount(). All of these CVEs are examples of security vulnerabilities due to introduction of user namespaces. Docker can use user namespaces to set up containers, but then disallows the process inside the container from creating its own nested namespaces through the default seccomp profile, rendering these vulnerabilities unexploitable.\nCVE-2014-0181, CVE-2015-3339: These are bugs that require the presence of a setuid binary. Docker disables setuid binaries inside containers via the NO_NEW_PRIVS process flag and other mechanisms.\nCVE-2014-4699: A bug in ptrace() could allow privilege escalation. Docker disables ptrace() inside the container using apparmor, seccomp and by dropping CAP_PTRACE. Three times the layers of protection there!\nCVE-2014-9529: A series of crafted keyctl() calls could cause kernel DoS / memory corruption. Docker disables keyctl() inside containers using seccomp.\nCVE-2015-3214, 4036: These are bugs in common virtualization drivers which could allow a guest OS user to execute code on the host OS. Exploiting them requires access to virtualization devices in the guest. Docker hides direct access to these devices when run without --privileged. Interestingly, these seem to be cases where containers are \"more secure\" than a VM, going against common wisdom that VMs are \"more secure\" than containers.\nCVE-2016-0728: Use-after-free caused by crafted keyctl() calls could lead to privilege escalation. Docker disables keyctl() inside containers using the default seccomp profile.\nCVE-2016-2383: A bug in eBPF -- the special in-kernel DSL used to express things like seccomp filters -- allowed arbitrary reads of kernel memory. The bpf() system call is blocked inside Docker containers using (ironically) seccomp.\nCVE-2016-3134, 4997, 4998: A bug in setsockopt with IPT_SO_SET_REPLACE, ARPT_SO_SET_REPLACE, and ARPT_SO_SET_REPLACE causing memory corruption / local privilege escalation. These arguments are blocked by CAP_NET_ADMIN, which Docker does not allow by default.\n\nBugs not mitigated:\n\nCVE-2015-3290, 5157: Bugs in the kernel's non-maskable interrupt handling allowed privilege escalation. Can be exploited in Docker containers because the modify_ldt() system call is not currently blocked using seccomp.\nCVE-2016-5195: A race condition was found in the way the Linux kernel's memory subsystem handled the copy-on-write (COW) breakage of private read-only memory mappings, which allowed unprivileged local users to gain write access to read-only memory. Also known as \"dirty COW.\" Partial mitigations: on some operating systems this vulnerability is mitigated by the combination of seccomp filtering of ptrace and the fact that /proc/self/mem is read-only.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995450,
    "timestamp": "2026-02-07T06:33:19.773Z",
    "title": "Isolate containers with a user namespace | Docker Docs",
    "url": "https://docs.docker.com/engine/security/userns-remap/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nIsolate containers with a user namespace\nIsolate containers with a user namespace\nCopy as Markdown\n\nLinux namespaces provide isolation for running processes, limiting their access to system resources without the running process being aware of the limitations. For more information on Linux namespaces, see Linux namespaces.\n\nThe best way to prevent privilege-escalation attacks from within a container is to configure your container's applications to run as unprivileged users. For containers whose processes must run as the root user within the container, you can re-map this user to a less-privileged user on the Docker host. The mapped user is assigned a range of UIDs which function within the namespace as normal UIDs from 0 to 65536, but have no privileges on the host machine itself.\n\nAbout remapping and subordinate user and group IDs\n\nThe remapping itself is handled by two files: /etc/subuid and /etc/subgid. Each file works the same, but one is concerned with the user ID range, and the other with the group ID range. Consider the following entry in /etc/subuid:\n\ntestuser:231072:65536\n\nThis means that testuser is assigned a subordinate user ID range of 231072 and the next 65536 integers in sequence. UID 231072 is mapped within the namespace (within the container, in this case) as UID 0 (root). UID 231073 is mapped as UID 1, and so forth. If a process attempts to escalate privilege outside of the namespace, the process is running as an unprivileged high-number UID on the host, which does not even map to a real user. This means the process has no privileges on the host system at all.\n\nNote\n\nIt is possible to assign multiple subordinate ranges for a given user or group by adding multiple non-overlapping mappings for the same user or group in the /etc/subuid or /etc/subgid file. In this case, Docker uses only the first five mappings, in accordance with the kernel's limitation of only five entries in /proc/self/uid_map and /proc/self/gid_map.\n\nWhen you configure Docker to use the userns-remap feature, you can optionally specify an existing user and/or group, or you can specify default. If you specify default, a user and group dockremap is created and used for this purpose.\n\nWarning\n\nSome distributions do not automatically add the new group to the /etc/subuid and /etc/subgid files. If that's the case, you may have to manually edit these files and assign non-overlapping ranges. This step is covered in Prerequisites.\n\nIt is very important that the ranges do not overlap, so that a process cannot gain access in a different namespace. On most Linux distributions, system utilities manage the ranges for you when you add or remove users.\n\nThis re-mapping is transparent to the container, but introduces some configuration complexity in situations where the container needs access to resources on the Docker host, such as bind mounts into areas of the filesystem that the system user cannot write to. From a security standpoint, it is best to avoid these situations.\n\nPrerequisites\n\nThe subordinate UID and GID ranges must be associated with an existing user, even though the association is an implementation detail. The user owns the namespaced storage directories under /var/lib/docker/. If you don't want to use an existing user, Docker can create one for you and use that. If you want to use an existing username or user ID, it must already exist. Typically, this means that the relevant entries need to be in /etc/passwd and /etc/group, but if you are using a different authentication back-end, this requirement may translate differently.\n\nTo verify this, use the id command:\n\n$ id testuser\n\n\n\nuid=1001(testuser) gid=1001(testuser) groups=1001(testuser)\n\n\nThe way the namespace remapping is handled on the host is using two files, /etc/subuid and /etc/subgid. These files are typically managed automatically when you add or remove users or groups, but on some distributions, you may need to manage these files manually.\n\nEach file contains three fields: the username or ID of the user, followed by a beginning UID or GID (which is treated as UID or GID 0 within the namespace) and a maximum number of UIDs or GIDs available to the user. For instance, given the following entry:\n\ntestuser:231072:65536\n\nThis means that user-namespaced processes started by testuser are owned by host UID 231072 (which looks like UID 0 inside the namespace) through 296607 (231072 + 65536 - 1). These ranges should not overlap, to ensure that namespaced processes cannot access each other's namespaces.\n\nAfter adding your user, check /etc/subuid and /etc/subgid to see if your user has an entry in each. If not, you need to add it, being careful to avoid overlap.\n\nIf you want to use the dockremap user automatically created by Docker, check for the dockremap entry in these files after configuring and restarting Docker.\n\nIf there are any locations on the Docker host where the unprivileged user needs to write, adjust the permissions of those locations accordingly. This is also true if you want to use the dockremap user automatically created by Docker, but you can't modify the permissions until after configuring and restarting Docker.\n\nEnabling userns-remap effectively masks existing image and container layers, as well as other Docker objects within /var/lib/docker/. This is because Docker needs to adjust the ownership of these resources and actually stores them in a subdirectory within /var/lib/docker/. It is best to enable this feature on a new Docker installation rather than an existing one.\n\nAlong the same lines, if you disable userns-remap you can't access any of the resources created while it was enabled.\n\nCheck the limitations on user namespaces to be sure your use case is possible.\n\nEnable userns-remap on the daemon\n\nYou can start dockerd with the --userns-remap flag or follow this procedure to configure the daemon using the daemon.json configuration file. The daemon.json method is recommended. If you use the flag, use the following command as a model:\n\n$ dockerd --userns-remap=\"testuser:testuser\"\n\n\nEdit /etc/docker/daemon.json. Assuming the file was previously empty, the following entry enables userns-remap using user and group called testuser. You can address the user and group by ID or name. You only need to specify the group name or ID if it is different from the user name or ID. If you provide both the user and group name or ID, separate them by a colon (:) character. The following formats all work for the value, assuming the UID and GID of testuser are 1001:\n\ntestuser\ntestuser:testuser\n1001\n1001:1001\ntestuser:1001\n1001:testuser\n{\n\n  \"userns-remap\": \"testuser\"\n\n}\nNote\n\nTo use the dockremap user and have Docker create it for you, set the value to default rather than testuser.\n\nSave the file and restart Docker.\n\nIf you are using the dockremap user, verify that Docker created it using the id command.\n\n$ id dockremap\n\n\n\nuid=112(dockremap) gid=116(dockremap) groups=116(dockremap)\n\n\nVerify that the entry has been added to /etc/subuid and /etc/subgid:\n\n$ grep dockremap /etc/subuid\n\n\n\ndockremap:231072:65536\n\n\n\n$ grep dockremap /etc/subgid\n\n\n\ndockremap:231072:65536\n\n\nIf these entries are not present, edit the files as the root user and assign a starting UID and GID that is the highest-assigned one plus the offset (in this case, 65536). Be careful not to allow any overlap in the ranges.\n\nVerify that previous images are not available using the docker image ls command. The output should be empty.\n\nStart a container from the hello-world image.\n\n$ docker run hello-world\n\n\nVerify that a namespaced directory exists within /var/lib/docker/ named with the UID and GID of the namespaced user, owned by that UID and GID, and not group-or-world-readable. Some of the subdirectories are still owned by root and have different permissions.\n\n$ sudo ls -ld /var/lib/docker/231072.231072/\n\n\n\ndrwx------ 11 231072 231072 11 Jun 21 21:19 /var/lib/docker/231072.231072/\n\n\n\n$ sudo ls -l /var/lib/docker/231072.231072/\n\n\n\ntotal 14\n\ndrwx------ 5 231072 231072 5 Jun 21 21:19 aufs\n\ndrwx------ 3 231072 231072 3 Jun 21 21:21 containers\n\ndrwx------ 3 root   root   3 Jun 21 21:19 image\n\ndrwxr-x--- 3 root   root   3 Jun 21 21:19 network\n\ndrwx------ 4 root   root   4 Jun 21 21:19 plugins\n\ndrwx------ 2 root   root   2 Jun 21 21:19 swarm\n\ndrwx------ 2 231072 231072 2 Jun 21 21:21 tmp\n\ndrwx------ 2 root   root   2 Jun 21 21:19 trust\n\ndrwx------ 2 231072 231072 3 Jun 21 21:19 volumes\n\n\nYour directory listing may have some differences, especially if you use a different container storage driver than aufs.\n\nThe directories which are owned by the remapped user are used instead of the same directories directly beneath /var/lib/docker/ and the unused versions (such as /var/lib/docker/tmp/ in the example here) can be removed. Docker does not use them while userns-remap is enabled.\n\nDisable namespace remapping for a container\n\nIf you enable user namespaces on the daemon, all containers are started with user namespaces enabled by default. In some situations, such as privileged containers, you may need to disable user namespaces for a specific container. See user namespace known limitations for some of these limitations.\n\nTo disable user namespaces for a specific container, add the --userns=host flag to the docker container create, docker container run, or docker container exec command.\n\nThere is a side effect when using this flag: user remapping will not be enabled for that container but, because the read-only (image) layers are shared between containers, ownership of the containers filesystem will still be remapped.\n\nWhat this means is that the whole container filesystem will belong to the user specified in the --userns-remap daemon config (231072 in the example above). This can lead to unexpected behavior of programs inside the container. For instance sudo (which checks that its binaries belong to user 0) or binaries with a setuid flag.\n\nUser namespace known limitations\n\nThe following standard Docker features are incompatible with running a Docker daemon with user namespaces enabled:\n\nSharing PID or NET namespaces with the host (--pid=host or --network=host).\nExternal (volume or storage) drivers which are unaware or incapable of using daemon user mappings.\nUsing the --privileged mode flag on docker run without also specifying --userns=host.\n\nUser namespaces are an advanced feature and require coordination with other capabilities. For example, if volumes are mounted from the host, file ownership must be pre-arranged if you need read or write access to the volume contents.\n\nWhile the root user inside a user-namespaced container process has many of the expected privileges of the superuser within the container, the Linux kernel imposes restrictions based on internal knowledge that this is a user-namespaced process. One notable restriction is the inability to use the mknod command. Permission is denied for device creation within the container when run by the root user.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nAbout remapping and subordinate user and group IDs\nPrerequisites\nEnable userns-remap on the daemon\nDisable namespace remapping for a container\nUser namespace known limitations\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995458,
    "timestamp": "2026-02-07T06:33:19.778Z",
    "title": "Seccomp security profiles for Docker | Docker Docs",
    "url": "https://docs.docker.com/engine/security/seccomp/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nSeccomp security profiles for Docker\nSeccomp security profiles for Docker\nCopy as Markdown\n\nSecure computing mode (seccomp) is a Linux kernel feature. You can use it to restrict the actions available within the container. The seccomp() system call operates on the seccomp state of the calling process. You can use this feature to restrict your application's access.\n\nThis feature is available only if Docker has been built with seccomp and the kernel is configured with CONFIG_SECCOMP enabled. To check if your kernel supports seccomp:\n\n$ grep CONFIG_SECCOMP= /boot/config-$(uname -r)\n\nCONFIG_SECCOMP=y\n\nPass a profile for a container\n\nThe default seccomp profile provides a sane default for running containers with seccomp and disables around 44 system calls out of 300+. It is moderately protective while providing wide application compatibility.\n\nIn effect, the profile is an allowlist that denies access to system calls by default and then allows specific system calls. The profile works by defining a defaultAction of SCMP_ACT_ERRNO and overriding that action only for specific system calls. The effect of SCMP_ACT_ERRNO is to cause a Permission Denied error. Next, the profile defines a specific list of system calls which are fully allowed, because their action is overridden to be SCMP_ACT_ALLOW. Finally, some specific rules are for individual system calls such as personality, and others, to allow variants of those system calls with specific arguments.\n\nseccomp is instrumental for running Docker containers with least privilege. It is not recommended to change the default seccomp profile.\n\nWhen you run a container, it uses the default profile unless you override it with the --security-opt option. For example, the following explicitly specifies a policy:\n\n$ docker run --rm \\\n\n             -it \\\n\n             --security-opt seccomp=/path/to/seccomp/profile.json \\\n\n             hello-world\n\nSignificant syscalls blocked by the default profile\n\nDocker's default seccomp profile is an allowlist which specifies the calls that are allowed. The table below lists the significant (but not all) syscalls that are effectively blocked because they are not on the allowlist. The table includes the reason each syscall is blocked rather than white-listed.\n\nSyscall\tDescription\nacct\tAccounting syscall which could let containers disable their own resource limits or process accounting. Also gated by CAP_SYS_PACCT.\nadd_key\tPrevent containers from using the kernel keyring, which is not namespaced.\nbpf\tDeny loading potentially persistent BPF programs into kernel, already gated by CAP_SYS_ADMIN.\nclock_adjtime\tTime/date is not namespaced. Also gated by CAP_SYS_TIME.\nclock_settime\tTime/date is not namespaced. Also gated by CAP_SYS_TIME.\nclone\tDeny cloning new namespaces. Also gated by CAP_SYS_ADMIN for CLONE_* flags, except CLONE_NEWUSER.\ncreate_module\tDeny manipulation and functions on kernel modules. Obsolete. Also gated by CAP_SYS_MODULE.\ndelete_module\tDeny manipulation and functions on kernel modules. Also gated by CAP_SYS_MODULE.\nfinit_module\tDeny manipulation and functions on kernel modules. Also gated by CAP_SYS_MODULE.\nget_kernel_syms\tDeny retrieval of exported kernel and module symbols. Obsolete.\nget_mempolicy\tSyscall that modifies kernel memory and NUMA settings. Already gated by CAP_SYS_NICE.\ninit_module\tDeny manipulation and functions on kernel modules. Also gated by CAP_SYS_MODULE.\nioperm\tPrevent containers from modifying kernel I/O privilege levels. Already gated by CAP_SYS_RAWIO.\niopl\tPrevent containers from modifying kernel I/O privilege levels. Already gated by CAP_SYS_RAWIO.\nkcmp\tRestrict process inspection capabilities, already blocked by dropping CAP_SYS_PTRACE.\nkexec_file_load\tSister syscall of kexec_load that does the same thing, slightly different arguments. Also gated by CAP_SYS_BOOT.\nkexec_load\tDeny loading a new kernel for later execution. Also gated by CAP_SYS_BOOT.\nkeyctl\tPrevent containers from using the kernel keyring, which is not namespaced.\nlookup_dcookie\tTracing/profiling syscall, which could leak a lot of information on the host. Also gated by CAP_SYS_ADMIN.\nmbind\tSyscall that modifies kernel memory and NUMA settings. Already gated by CAP_SYS_NICE.\nmount\tDeny mounting, already gated by CAP_SYS_ADMIN.\nmove_pages\tSyscall that modifies kernel memory and NUMA settings.\nnfsservctl\tDeny interaction with the kernel NFS daemon. Obsolete since Linux 3.1.\nopen_by_handle_at\tCause of an old container breakout. Also gated by CAP_DAC_READ_SEARCH.\nperf_event_open\tTracing/profiling syscall, which could leak a lot of information on the host.\npersonality\tPrevent container from enabling BSD emulation. Not inherently dangerous, but poorly tested, potential for a lot of kernel vulnerabilities.\npivot_root\tDeny pivot_root, should be privileged operation.\nprocess_vm_readv\tRestrict process inspection capabilities, already blocked by dropping CAP_SYS_PTRACE.\nprocess_vm_writev\tRestrict process inspection capabilities, already blocked by dropping CAP_SYS_PTRACE.\nptrace\tTracing/profiling syscall. Blocked in Linux kernel versions before 4.8 to avoid seccomp bypass. Tracing/profiling arbitrary processes is already blocked by dropping CAP_SYS_PTRACE, because it could leak a lot of information on the host.\nquery_module\tDeny manipulation and functions on kernel modules. Obsolete.\nquotactl\tQuota syscall which could let containers disable their own resource limits or process accounting. Also gated by CAP_SYS_ADMIN.\nreboot\tDon't let containers reboot the host. Also gated by CAP_SYS_BOOT.\nrequest_key\tPrevent containers from using the kernel keyring, which is not namespaced.\nset_mempolicy\tSyscall that modifies kernel memory and NUMA settings. Already gated by CAP_SYS_NICE.\nsetns\tDeny associating a thread with a namespace. Also gated by CAP_SYS_ADMIN.\nsettimeofday\tTime/date is not namespaced. Also gated by CAP_SYS_TIME.\nstime\tTime/date is not namespaced. Also gated by CAP_SYS_TIME.\nswapon\tDeny start/stop swapping to file/device. Also gated by CAP_SYS_ADMIN.\nswapoff\tDeny start/stop swapping to file/device. Also gated by CAP_SYS_ADMIN.\nsysfs\tObsolete syscall.\n_sysctl\tObsolete, replaced by /proc/sys.\numount\tShould be a privileged operation. Also gated by CAP_SYS_ADMIN.\numount2\tShould be a privileged operation. Also gated by CAP_SYS_ADMIN.\nunshare\tDeny cloning new namespaces for processes. Also gated by CAP_SYS_ADMIN, with the exception of unshare --user.\nuselib\tOlder syscall related to shared libraries, unused for a long time.\nuserfaultfd\tUserspace page fault handling, largely needed for process migration.\nustat\tObsolete syscall.\nvm86\tIn kernel x86 real mode virtual machine. Also gated by CAP_SYS_ADMIN.\nvm86old\tIn kernel x86 real mode virtual machine. Also gated by CAP_SYS_ADMIN.\nRun without the default seccomp profile\n\nYou can pass unconfined to run a container without the default seccomp profile.\n\n$ docker run --rm -it --security-opt seccomp=unconfined debian:latest \\\n\n    unshare --map-root-user --user sh -c whoami\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPass a profile for a container\nSignificant syscalls blocked by the default profile\nRun without the default seccomp profile\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995455,
    "timestamp": "2026-02-07T06:33:19.778Z",
    "title": "Protect the Docker daemon socket | Docker Docs",
    "url": "https://docs.docker.com/engine/security/protect-access/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nProtect the Docker daemon socket\nProtect the Docker daemon socket\nCopy as Markdown\n\nBy default, Docker runs through a non-networked UNIX socket. It can also optionally communicate using SSH or a TLS (HTTPS) socket.\n\nUse SSH to protect the Docker daemon socket\nNote\n\nThe given USERNAME must have permissions to access the docker socket on the remote machine. Refer to manage Docker as a non-root user to learn how to give a non-root user access to the docker socket.\n\nThe following example creates a docker context to connect with a remote dockerd daemon on host1.example.com using SSH, and as the docker-user user on the remote machine:\n\n$ docker context create \\\n\n    --docker host=ssh://docker-user@host1.example.com \\\n\n    --description=\"Remote engine\" \\\n\n    my-remote-engine\n\n\n\nmy-remote-engine\n\nSuccessfully created context \"my-remote-engine\"\n\n\nAfter creating the context, use docker context use to switch the docker CLI to use it, and to connect to the remote engine:\n\n$ docker context use my-remote-engine\n\nmy-remote-engine\n\nCurrent context is now \"my-remote-engine\"\n\n\n\n$ docker info\n\n<prints output of the remote engine>\n\n\nUse the default context to switch back to the default (local) daemon:\n\n$ docker context use default\n\ndefault\n\nCurrent context is now \"default\"\n\n\nAlternatively, use the DOCKER_HOST environment variable to temporarily switch the docker CLI to connect to the remote host using SSH. This does not require creating a context, and can be useful to create an ad-hoc connection with a different engine:\n\n$ export DOCKER_HOST=ssh://docker-user@host1.example.com\n\n$ docker info\n\n<prints output of the remote engine>\n\nSSH Tips\n\nFor the best user experience with SSH, configure ~/.ssh/config as follows to allow reusing a SSH connection for multiple invocations of the docker CLI:\n\nControlMaster     auto\n\nControlPath       ~/.ssh/control-%C\n\nControlPersist    yes\nUse TLS (HTTPS) to protect the Docker daemon socket\n\nIf you need Docker to be reachable through HTTP rather than SSH in a safe manner, you can enable TLS (HTTPS) by specifying the tlsverify flag and pointing Docker's tlscacert flag to a trusted CA certificate.\n\nIn the daemon mode, it only allows connections from clients authenticated by a certificate signed by that CA. In the client mode, it only connects to servers with a certificate signed by that CA.\n\nImportant\n\nUsing TLS and managing a CA is an advanced topic. Familiarize yourself with OpenSSL, x509, and TLS before using it in production.\n\nCreate a CA, server and client keys with OpenSSL\nNote\n\nReplace all instances of $HOST in the following example with the DNS name of your Docker daemon's host.\n\nFirst, on the Docker daemon's host machine, generate CA private and public keys:\n\n$ openssl genrsa -aes256 -out ca-key.pem 4096\n\nGenerating RSA private key, 4096 bit long modulus\n\n..............................................................................++\n\n........++\n\ne is 65537 (0x10001)\n\nEnter pass phrase for ca-key.pem:\n\nVerifying - Enter pass phrase for ca-key.pem:\n\n\n\n$ openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem\n\nEnter pass phrase for ca-key.pem:\n\nYou are about to be asked to enter information that will be incorporated\n\ninto your certificate request.\n\nWhat you are about to enter is what is called a Distinguished Name or a DN.\n\nThere are quite a few fields but you can leave some blank\n\nFor some fields there will be a default value,\n\nIf you enter '.', the field will be left blank.\n\n-----\n\nCountry Name (2 letter code) [AU]:\n\nState or Province Name (full name) [Some-State]:Queensland\n\nLocality Name (eg, city) []:Brisbane\n\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Docker Inc\n\nOrganizational Unit Name (eg, section) []:Sales\n\nCommon Name (e.g. server FQDN or YOUR name) []:$HOST\n\nEmail Address []:Sven@home.org.au\n\n\nNow that you have a CA, you can create a server key and certificate signing request (CSR). Make sure that \"Common Name\" matches the hostname you use to connect to Docker:\n\nNote\n\nReplace all instances of $HOST in the following example with the DNS name of your Docker daemon's host.\n\n$ openssl genrsa -out server-key.pem 4096\n\nGenerating RSA private key, 4096 bit long modulus\n\n.....................................................................++\n\n.................................................................................................++\n\ne is 65537 (0x10001)\n\n\n\n$ openssl req -subj \"/CN=$HOST\" -sha256 -new -key server-key.pem -out server.csr\n\n\nNext, we're going to sign the public key with our CA:\n\nSince TLS connections can be made through IP address as well as DNS name, the IP addresses need to be specified when creating the certificate. For example, to allow connections using 10.10.10.20 and 127.0.0.1:\n\n$ echo subjectAltName = DNS:$HOST,IP:10.10.10.20,IP:127.0.0.1 >> extfile.cnf\n\n\nSet the Docker daemon key's extended usage attributes to be used only for server authentication:\n\n$ echo extendedKeyUsage = serverAuth >> extfile.cnf\n\n\nNow, generate the signed certificate:\n\n$ openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \\\n\n  -CAcreateserial -out server-cert.pem -extfile extfile.cnf\n\nSignature ok\n\nsubject=/CN=your.host.com\n\nGetting CA Private Key\n\nEnter pass phrase for ca-key.pem:\n\n\nAuthorization plugins offer more fine-grained control to supplement authentication from mutual TLS. In addition to other information described in the above document, authorization plugins running on a Docker daemon receive the certificate information for connecting Docker clients.\n\nFor client authentication, create a client key and certificate signing request:\n\nNote\n\nFor simplicity of the next couple of steps, you may perform this step on the Docker daemon's host machine as well.\n\n$ openssl genrsa -out key.pem 4096\n\nGenerating RSA private key, 4096 bit long modulus\n\n.........................................................++\n\n................++\n\ne is 65537 (0x10001)\n\n\n\n$ openssl req -subj '/CN=client' -new -key key.pem -out client.csr\n\n\nTo make the key suitable for client authentication, create a new extensions config file:\n\n$ echo extendedKeyUsage = clientAuth > extfile-client.cnf\n\n\nNow, generate the signed certificate:\n\n$ openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \\\n\n  -CAcreateserial -out cert.pem -extfile extfile-client.cnf\n\nSignature ok\n\nsubject=/CN=client\n\nGetting CA Private Key\n\nEnter pass phrase for ca-key.pem:\n\n\nAfter generating cert.pem and server-cert.pem you can safely remove the two certificate signing requests and extensions config files:\n\n$ rm -v client.csr server.csr extfile.cnf extfile-client.cnf\n\n\nWith a default umask of 022, your secret keys are world-readable and writable for you and your group.\n\nTo protect your keys from accidental damage, remove their write permissions. To make them only readable by you, change file modes as follows:\n\n$ chmod -v 0400 ca-key.pem key.pem server-key.pem\n\n\nCertificates can be world-readable, but you might want to remove write access to prevent accidental damage:\n\n$ chmod -v 0444 ca.pem server-cert.pem cert.pem\n\n\nNow you can make the Docker daemon only accept connections from clients providing a certificate trusted by your CA:\n\n$ dockerd \\\n\n    --tlsverify \\\n\n    --tlscacert=ca.pem \\\n\n    --tlscert=server-cert.pem \\\n\n    --tlskey=server-key.pem \\\n\n    -H=0.0.0.0:2376\n\n\nTo connect to Docker and validate its certificate, provide your client keys, certificates and trusted CA:\n\nTip\n\nThis step should be run on your Docker client machine. As such, you need to copy your CA certificate, your server certificate, and your client certificate to that machine.\n\nNote\n\nReplace all instances of $HOST in the following example with the DNS name of your Docker daemon's host.\n\n$ docker --tlsverify \\\n\n    --tlscacert=ca.pem \\\n\n    --tlscert=cert.pem \\\n\n    --tlskey=key.pem \\\n\n    -H=$HOST:2376 version\n\nNote\n\nDocker over TLS should run on TCP port 2376.\n\nWarning\n\nAs shown in the example above, you don't need to run the docker client with sudo or the docker group when you use certificate authentication. That means anyone with the keys can give any instructions to your Docker daemon, giving them root access to the machine hosting the daemon. Guard these keys as you would a root password!\n\nSecure by default\n\nIf you want to secure your Docker client connections by default, you can move the files to the .docker directory in your home directory --- and set the DOCKER_HOST and DOCKER_TLS_VERIFY variables as well (instead of passing -H=tcp://$HOST:2376 and --tlsverify on every call).\n\n$ mkdir -pv ~/.docker\n\n$ cp -v {ca,cert,key}.pem ~/.docker\n\n\n\n$ export DOCKER_HOST=tcp://$HOST:2376 DOCKER_TLS_VERIFY=1\n\n\nDocker now connects securely by default:\n\n$ docker ps\n\nOther modes\n\nIf you don't want to have complete two-way authentication, you can run Docker in various other modes by mixing the flags.\n\nDaemon modes\ntlsverify, tlscacert, tlscert, tlskey set: Authenticate clients\ntls, tlscert, tlskey: Do not authenticate clients\nClient modes\ntls: Authenticate server based on public/default CA pool\ntlsverify, tlscacert: Authenticate server based on given CA\ntls, tlscert, tlskey: Authenticate with client certificate, do not authenticate server based on given CA\ntlsverify, tlscacert, tlscert, tlskey: Authenticate with client certificate and authenticate server based on given CA\n\nIf found, the client sends its client certificate, so you just need to drop your keys into ~/.docker/{ca,cert,key}.pem. Alternatively, if you want to store your keys in another location, you can specify that location using the environment variable DOCKER_CERT_PATH.\n\n$ export DOCKER_CERT_PATH=~/.docker/zone1/\n\n$ docker --tlsverify ps\n\nConnecting to the secure Docker port using curl\n\nTo use curl to make test API requests, you need to use three extra command line flags:\n\n$ curl https://$HOST:2376/images/json \\\n\n  --cert ~/.docker/cert.pem \\\n\n  --key ~/.docker/key.pem \\\n\n  --cacert ~/.docker/ca.pem\n\nRelated information\nUsing certificates for repository client verification\nUse trusted images\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUse SSH to protect the Docker daemon socket\nSSH Tips\nUse TLS (HTTPS) to protect the Docker daemon socket\nCreate a CA, server and client keys with OpenSSL\nSecure by default\nOther modes\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995461,
    "timestamp": "2026-02-07T06:33:19.781Z",
    "title": "Verify repository client with certificates | Docker Docs",
    "url": "https://docs.docker.com/engine/security/certificates/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nRootless mode\nAntivirus software and Docker\nAppArmor security profiles for Docker\nContent trust in Docker\nDocker security non-events\nIsolate containers with a user namespace\nProtect the Docker daemon socket\nSeccomp security profiles for Docker\nVerify repository client with certificates\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSecurity\n/\nVerify repository client with certificates\nVerify repository client with certificates\nCopy as Markdown\n\nIn Running Docker with HTTPS, you learned that, by default, Docker runs via a non-networked Unix socket and TLS must be enabled in order to have the Docker client and the daemon communicate securely over HTTPS. TLS ensures authenticity of the registry endpoint and that traffic to/from registry is encrypted.\n\nThis article demonstrates how to ensure the traffic between the Docker registry server and the Docker daemon (a client of the registry server) is encrypted and properly authenticated using certificate-based client-server authentication.\n\nWe show you how to install a Certificate Authority (CA) root certificate for the registry and how to set the client TLS certificate for verification.\n\nUnderstand the configuration\n\nA custom certificate is configured by creating a directory under /etc/docker/certs.d using the same name as the registry's hostname, such as localhost. All *.crt files are added to this directory as CA roots.\n\nNote\n\nOn Linux any root certificates authorities are merged with the system defaults, including the host's root CA set. If you are running Docker on Windows Server, or Docker Desktop for Windows with Windows containers, the system default certificates are only used when no custom root certificates are configured.\n\nThe presence of one or more <filename>.key/cert pairs indicates to Docker that there are custom certificates required for access to the desired repository.\n\nNote\n\nIf multiple certificates exist, each is tried in alphabetical order. If there is a 4xx-level or 5xx-level authentication error, Docker continues to try with the next certificate.\n\nThe following illustrates a configuration with custom certificates:\n\n    /etc/docker/certs.d/        <-- Certificate directory\n\n    ‚îî‚îÄ‚îÄ localhost:5000          <-- Hostname:port\n\n       ‚îú‚îÄ‚îÄ client.cert          <-- Client certificate\n\n       ‚îú‚îÄ‚îÄ client.key           <-- Client key\n\n       ‚îî‚îÄ‚îÄ ca.crt               <-- Root CA that signed\n\n                                    the registry certificate, in PEM\n\nThe preceding example is operating-system specific and is for illustrative purposes only. You should consult your operating system documentation for creating an os-provided bundled certificate chain.\n\nCreate the client certificates\n\nUse OpenSSL's genrsa and req commands to first generate an RSA key and then use the key to create the certificate.\n\n$ openssl genrsa -out client.key 4096\n\n$ openssl req -new -x509 -text -key client.key -out client.cert\n\nNote\n\nThese TLS commands only generate a working set of certificates on Linux. The version of OpenSSL in macOS is incompatible with the type of certificate Docker requires.\n\nTroubleshooting tips\n\nThe Docker daemon interprets .crt files as CA certificates and .cert files as client certificates. If a CA certificate is accidentally given the extension .cert instead of the correct .crt extension, the Docker daemon logs the following error message:\n\nMissing key KEY_NAME for client certificate CERT_NAME. CA certificates should use the extension .crt.\n\nIf the Docker registry is accessed without a port number, do not add the port to the directory name. The following shows the configuration for a registry on default port 443 which is accessed with docker login my-https.registry.example.com:\n\n    /etc/docker/certs.d/\n\n    ‚îî‚îÄ‚îÄ my-https.registry.example.com          <-- Hostname without port\n\n       ‚îú‚îÄ‚îÄ client.cert\n\n       ‚îú‚îÄ‚îÄ client.key\n\n       ‚îî‚îÄ‚îÄ ca.crt\nRelated information\nUse trusted images\nProtect the Docker daemon socket\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUnderstand the configuration\nCreate the client certificates\nTroubleshooting tips\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995464,
    "timestamp": "2026-02-07T06:33:19.785Z",
    "title": "Swarm mode | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\nSwarm mode\nCopy as Markdown\nNote\n\nSwarm mode is an advanced feature for managing a cluster of Docker daemons.\n\nUse Swarm mode if you intend to use Swarm as a production runtime environment.\n\nIf you're not planning on deploying with Swarm, use Docker Compose instead. If you're developing for a Kubernetes deployment, consider using the integrated Kubernetes feature in Docker Desktop.\n\nCurrent versions of Docker include Swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.\n\nDocker Swarm mode is built into the Docker Engine. Do not confuse Docker Swarm mode with Docker Classic Swarm which is no longer actively developed.\n\nFeature highlights\nCluster management integrated with Docker Engine\n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don't need additional orchestration software to create or manage a swarm.\n\nDecentralized design\n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\nDeclarative service model\n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\nScaling\n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\nDesired state reconciliation\n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state and your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager creates two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\nMulti-host networking\n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\nService discovery\n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balance running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\nLoad balancing\n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\nSecure by default\n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\nRolling updates\n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back to a previous version of the service.\n\nWhat's next?\nLearn Swarm mode key concepts.\nGet started with the Swarm mode tutorial.\nExplore Swarm mode CLI commands\nswarm init\nswarm join\nservice create\nservice inspect\nservice ls\nservice rm\nservice scale\nservice ps\nservice update\n\nEdit this page\n\nRequest changes\n\nTable of contents\nFeature highlights\nCluster management integrated with Docker Engine\nDecentralized design\nDeclarative service model\nScaling\nDesired state reconciliation\nMulti-host networking\nService discovery\nLoad balancing\nSecure by default\nRolling updates\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995467,
    "timestamp": "2026-02-07T06:33:19.785Z",
    "title": "Administer and maintain a swarm of Docker Engines | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/admin_guide/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nAdminister and maintain a swarm of Docker Engines\nAdminister and maintain a swarm of Docker Engines\nCopy as Markdown\n\nWhen you run a swarm of Docker Engines, manager nodes are the key components for managing the swarm and storing the swarm state. It is important to understand some key features of manager nodes to properly deploy and maintain the swarm.\n\nRefer to How nodes work for a brief overview of Docker Swarm mode and the difference between manager and worker nodes.\n\nOperate manager nodes in a swarm\n\nSwarm manager nodes use the Raft Consensus Algorithm to manage the swarm state. You only need to understand some general concepts of Raft in order to manage a swarm.\n\nThere is no limit on the number of manager nodes. The decision about how many manager nodes to implement is a trade-off between performance and fault-tolerance. Adding manager nodes to a swarm makes the swarm more fault-tolerant. However, additional manager nodes reduce write performance because more nodes must acknowledge proposals to update the swarm state. This means more network round-trip traffic.\n\nRaft requires a majority of managers, also called the quorum, to agree on proposed updates to the swarm, such as node additions or removals. Membership operations are subject to the same constraints as state replication.\n\nMaintain the quorum of managers\n\nIf the swarm loses the quorum of managers, the swarm cannot perform management tasks. If your swarm has multiple managers, always have more than two. To maintain quorum, a majority of managers must be available. An odd number of managers is recommended, because the next even number does not make the quorum easier to keep. For instance, whether you have 3 or 4 managers, you can still only lose 1 manager and maintain the quorum. If you have 5 or 6 managers, you can still only lose two.\n\nEven if a swarm loses the quorum of managers, swarm tasks on existing worker nodes continue to run. However, swarm nodes cannot be added, updated, or removed, and new or existing tasks cannot be started, stopped, moved, or updated.\n\nSee Recovering from losing the quorum for troubleshooting steps if you do lose the quorum of managers.\n\nConfigure the manager to advertise on a static IP address\n\nWhen initiating a swarm, you must specify the --advertise-addr flag to advertise your address to other manager nodes in the swarm. For more information, see Run Docker Engine in swarm mode. Because manager nodes are meant to be a stable component of the infrastructure, you should use a fixed IP address for the advertise address to prevent the swarm from becoming unstable on machine reboot.\n\nIf the whole swarm restarts and every manager node subsequently gets a new IP address, there is no way for any node to contact an existing manager. Therefore the swarm is hung while nodes try to contact one another at their old IP addresses.\n\nDynamic IP addresses are OK for worker nodes.\n\nAdd manager nodes for fault tolerance\n\nYou should maintain an odd number of managers in the swarm to support manager node failures. Having an odd number of managers ensures that during a network partition, there is a higher chance that the quorum remains available to process requests if the network is partitioned into two sets. Keeping the quorum is not guaranteed if you encounter more than two network partitions.\n\nSwarm Size\tMajority\tFault Tolerance\n1\t1\t0\n2\t2\t0\n3\t2\t1\n4\t3\t1\n5\t3\t2\n6\t4\t2\n7\t4\t3\n8\t5\t3\n9\t5\t4\n\nFor example, in a swarm with 5 nodes, if you lose 3 nodes, you don't have a quorum. Therefore you can't add or remove nodes until you recover one of the unavailable manager nodes or recover the swarm with disaster recovery commands. See Recover from disaster.\n\nWhile it is possible to scale a swarm down to a single manager node, it is impossible to demote the last manager node. This ensures you maintain access to the swarm and that the swarm can still process requests. Scaling down to a single manager is an unsafe operation and is not recommended. If the last node leaves the swarm unexpectedly during the demote operation, the swarm becomes unavailable until you reboot the node or restart with --force-new-cluster.\n\nYou manage swarm membership with the docker swarm and docker node subsystems. Refer to Add nodes to a swarm for more information on how to add worker nodes and promote a worker node to be a manager.\n\nDistribute manager nodes\n\nIn addition to maintaining an odd number of manager nodes, pay attention to datacenter topology when placing managers. For optimal fault-tolerance, distribute manager nodes across a minimum of 3 availability-zones to support failures of an entire set of machines or common maintenance scenarios. If you suffer a failure in any of those zones, the swarm should maintain the quorum of manager nodes available to process requests and rebalance workloads.\n\nSwarm manager nodes\tRepartition (on 3 Availability zones)\n3\t1-1-1\n5\t2-2-1\n7\t3-2-2\n9\t3-3-3\nRun manager-only nodes\n\nBy default manager nodes also act as a worker nodes. This means the scheduler can assign tasks to a manager node. For small and non-critical swarms assigning tasks to managers is relatively low-risk as long as you schedule services using resource constraints for cpu and memory.\n\nHowever, because manager nodes use the Raft consensus algorithm to replicate data in a consistent way, they are sensitive to resource starvation. You should isolate managers in your swarm from processes that might block swarm operations like swarm heartbeat or leader elections.\n\nTo avoid interference with manager node operation, you can drain manager nodes to make them unavailable as worker nodes:\n\n$ docker node update --availability drain NODE\n\n\nWhen you drain a node, the scheduler reassigns any tasks running on the node to other available worker nodes in the swarm. It also prevents the scheduler from assigning tasks to the node.\n\nAdd worker nodes for load balancing\n\nAdd nodes to the swarm to balance your swarm's load. Replicated service tasks are distributed across the swarm as evenly as possible over time, as long as the worker nodes are matched to the requirements of the services. When limiting a service to run on only specific types of nodes, such as nodes with a specific number of CPUs or amount of memory, remember that worker nodes that do not meet these requirements cannot run these tasks.\n\nMonitor swarm health\n\nYou can monitor the health of manager nodes by querying the docker nodes API in JSON format through the /nodes HTTP endpoint. Refer to the nodes API documentation for more information.\n\nFrom the command line, run docker node inspect <id-node> to query the nodes. For instance, to query the reachability of the node as a manager:\n\n$ docker node inspect manager1 --format \"{{ .ManagerStatus.Reachability }}\"\n\nreachable\n\n\nTo query the status of the node as a worker that accept tasks:\n\n$ docker node inspect manager1 --format \"{{ .Status.State }}\"\n\nready\n\n\nFrom those commands, we can see that manager1 is both at the status reachable as a manager and ready as a worker.\n\nAn unreachable health status means that this particular manager node is unreachable from other manager nodes. In this case you need to take action to restore the unreachable manager:\n\nRestart the daemon and see if the manager comes back as reachable.\nReboot the machine.\nIf neither restarting nor rebooting works, you should add another manager node or promote a worker to be a manager node. You also need to cleanly remove the failed node entry from the manager set with docker node demote <NODE> and docker node rm <id-node>.\n\nAlternatively you can also get an overview of the swarm health from a manager node with docker node ls:\n\n$ docker node ls\n\nID                           HOSTNAME  MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS\n\n1mhtdwhvsgr3c26xxbnzdc3yp    node05    Accepted    Ready   Active\n\n516pacagkqp2xc3fk9t1dhjor    node02    Accepted    Ready   Active        Reachable\n\n9ifojw8of78kkusuc4a6c23fx *  node01    Accepted    Ready   Active        Leader\n\nax11wdpwrrb6db3mfjydscgk7    node04    Accepted    Ready   Active\n\nbb1nrq2cswhtbg4mrsqnlx1ck    node03    Accepted    Ready   Active        Reachable\n\ndi9wxgz8dtuh9d2hn089ecqkf    node06    Accepted    Ready   Active\n\nTroubleshoot a manager node\n\nYou should never restart a manager node by copying the raft directory from another node. The data directory is unique to a node ID. A node can only use a node ID once to join the swarm. The node ID space should be globally unique.\n\nTo cleanly re-join a manager node to a cluster:\n\nDemote the node to a worker using docker node demote <NODE>.\nRemove the node from the swarm using docker node rm <NODE>.\nRe-join the node to the swarm with a fresh state using docker swarm join.\n\nFor more information on joining a manager node to a swarm, refer to Join nodes to a swarm.\n\nForcibly remove a node\n\nIn most cases, you should shut down a node before removing it from a swarm with the docker node rm command. If a node becomes unreachable, unresponsive, or compromised you can forcefully remove the node without shutting it down by passing the --force flag. For instance, if node9 becomes compromised:\n\n$ docker node rm node9\n\n\n\nError response from daemon: rpc error: code = 9 desc = node node9 is not down and can't be removed\n\n\n\n$ docker node rm --force node9\n\n\n\nNode node9 removed from swarm\n\n\nBefore you forcefully remove a manager node, you must first demote it to the worker role. Make sure that you always have an odd number of manager nodes if you demote or remove a manager.\n\nBack up the swarm\n\nDocker manager nodes store the swarm state and manager logs in the /var/lib/docker/swarm/ directory. This data includes the keys used to encrypt the Raft logs. Without these keys, you cannot restore the swarm.\n\nYou can back up the swarm using any manager. Use the following procedure.\n\nIf the swarm has auto-lock enabled, you need the unlock key to restore the swarm from backup. Retrieve the unlock key if necessary and store it in a safe location. If you are unsure, read Lock your swarm to protect its encryption key.\n\nStop Docker on the manager before backing up the data, so that no data is being changed during the backup. It is possible to take a backup while the manager is running (a \"hot\" backup), but this is not recommended and your results are less predictable when restoring. While the manager is down, other nodes continue generating swarm data that is not part of this backup.\n\nNote\n\nBe sure to maintain the quorum of swarm managers. During the time that a manager is shut down, your swarm is more vulnerable to losing the quorum if further nodes are lost. The number of managers you run is a trade-off. If you regularly take down managers to do backups, consider running a five manager swarm, so that you can lose an additional manager while the backup is running, without disrupting your services.\n\nBack up the entire /var/lib/docker/swarm directory.\n\nRestart the manager.\n\nTo restore, see Restore from a backup.\n\nRecover from disaster\nRestore from a backup\n\nAfter backing up the swarm as described in Back up the swarm, use the following procedure to restore the data to a new swarm.\n\nShut down Docker on the target host machine for the restored swarm.\n\nRemove the contents of the /var/lib/docker/swarm directory on the new swarm.\n\nRestore the /var/lib/docker/swarm directory with the contents of the backup.\n\nNote\n\nThe new node uses the same encryption key for on-disk storage as the old one. It is not possible to change the on-disk storage encryption keys at this time.\n\nIn the case of a swarm with auto-lock enabled, the unlock key is also the same as on the old swarm, and the unlock key is needed to restore the swarm.\n\nStart Docker on the new node. Unlock the swarm if necessary. Re-initialize the swarm using the following command, so that this node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist.\n\n$ docker swarm init --force-new-cluster\n\n\nVerify that the state of the swarm is as expected. This may include application-specific tests or simply checking the output of docker service ls to be sure that all expected services are present.\n\nIf you use auto-lock, rotate the unlock key.\n\nAdd manager and worker nodes to bring your new swarm up to operating capacity.\n\nReinstate your previous backup regimen on the new swarm.\n\nRecover from losing the quorum\n\nSwarm is resilient to failures and can recover from any number of temporary node failures (machine reboots or crash with restart) or other transient errors. However, a swarm cannot automatically recover if it loses a quorum. Tasks on existing worker nodes continue to run, but administrative tasks are not possible, including scaling or updating services and joining or removing nodes from the swarm. The best way to recover is to bring the missing manager nodes back online. If that is not possible, continue reading for some options for recovering your swarm.\n\nIn a swarm of N managers, a quorum (a majority) of manager nodes must always be available. For example, in a swarm with five managers, a minimum of three must be operational and in communication with each other. In other words, the swarm can tolerate up to (N-1)/2 permanent failures beyond which requests involving swarm management cannot be processed. These types of failures include data corruption or hardware failures.\n\nIf you lose the quorum of managers, you cannot administer the swarm. If you have lost the quorum and you attempt to perform any management operation on the swarm, an error occurs:\n\nError response from daemon: rpc error: code = 4 desc = context deadline exceeded\n\nThe best way to recover from losing the quorum is to bring the failed nodes back online. If you can't do that, the only way to recover from this state is to use the --force-new-cluster action from a manager node. This removes all managers except the manager the command was run from. The quorum is achieved because there is now only one manager. Promote nodes to be managers until you have the desired number of managers.\n\nFrom the node to recover, run:\n\n$ docker swarm init --force-new-cluster --advertise-addr node01:2377\n\n\nWhen you run the docker swarm init command with the --force-new-cluster flag, the Docker Engine where you run the command becomes the manager node of a single-node swarm which is capable of managing and running services. The manager has all the previous information about services and tasks, worker nodes are still part of the swarm, and services are still running. You need to add or re-add manager nodes to achieve your previous task distribution and ensure that you have enough managers to maintain high availability and prevent losing the quorum.\n\nForce the swarm to rebalance\n\nGenerally, you do not need to force the swarm to rebalance its tasks. When you add a new node to a swarm, or a node reconnects to the swarm after a period of unavailability, the swarm does not automatically give a workload to the idle node. This is a design decision. If the swarm periodically shifted tasks to different nodes for the sake of balance, the clients using those tasks would be disrupted. The goal is to avoid disrupting running services for the sake of balance across the swarm. When new tasks start, or when a node with running tasks becomes unavailable, those tasks are given to less busy nodes. The goal is eventual balance, with minimal disruption to the end user.\n\nYou can use the --force or -f flag with the docker service update command to force the service to redistribute its tasks across the available worker nodes. This causes the service tasks to restart. Client applications may be disrupted. If you have configured it, your service uses a rolling update.\n\nIf you use an earlier version and you want to achieve an even balance of load across workers and don't mind disrupting running tasks, you can force your swarm to re-balance by temporarily scaling the service upward. Use docker service inspect --pretty <servicename> to see the configured scale of a service. When you use docker service scale, the nodes with the lowest number of tasks are targeted to receive the new workloads. There may be multiple under-loaded nodes in your swarm. You may need to scale the service up by modest increments a few times to achieve the balance you want across all the nodes.\n\nWhen the load is balanced to your satisfaction, you can scale the service back down to the original scale. You can use docker service ps to assess the current balance of your service across nodes.\n\nSee also docker service scale and docker service ps.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nOperate manager nodes in a swarm\nMaintain the quorum of managers\nConfigure the manager to advertise on a static IP address\nAdd manager nodes for fault tolerance\nDistribute manager nodes\nRun manager-only nodes\nAdd worker nodes for load balancing\nMonitor swarm health\nTroubleshoot a manager node\nForcibly remove a node\nBack up the swarm\nRecover from disaster\nRestore from a backup\nRecover from losing the quorum\nForce the swarm to rebalance\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995470,
    "timestamp": "2026-02-07T06:33:19.790Z",
    "title": "Deploy a stack to a swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/stack-deploy/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nDeploy a stack to a swarm\nDeploy a stack to a swarm\nCopy as Markdown\n\nWhen running Docker Engine in swarm mode, you can use docker stack deploy to deploy a complete application stack to the swarm. The deploy command accepts a stack description in the form of a Compose file.\n\nNote\n\nThe docker stack deploy command uses the legacy Compose file version 3 format, used by Compose V1. The latest format, defined by the Compose specification isn't compatible with the docker stack deploy command.\n\nFor more information about the evolution of Compose, see History of Compose.\n\nTo run through this tutorial, you need:\n\nA Docker Engine running in Swarm mode. If you're not familiar with Swarm mode, you might want to read Swarm mode key concepts and How services work.\n\nNote\n\nIf you're trying things out on a local development environment, you can put your engine into Swarm mode with docker swarm init.\n\nIf you've already got a multi-node swarm running, keep in mind that all docker stack and docker service commands must be run from a manager node.\n\nA current version of Docker Compose.\n\nSet up a Docker registry\n\nBecause a swarm consists of multiple Docker Engines, a registry is required to distribute images to all of them. You can use the Docker Hub or maintain your own. Here's how to create a throwaway registry, which you can discard afterward.\n\nStart the registry as a service on your swarm:\n\n$ docker service create --name registry --publish published=5000,target=5000 registry:2\n\n\nCheck its status with docker service ls:\n\n$ docker service ls\n\n\n\nID            NAME      REPLICAS  IMAGE                                                                               COMMAND\n\nl7791tpuwkco  registry  1/1       registry:2@sha256:1152291c7f93a4ea2ddc95e46d142c31e743b6dd70e194af9e6ebe530f782c17\n\n\nOnce it reads 1/1 under REPLICAS, it's running. If it reads 0/1, it's probably still pulling the image.\n\nCheck that it's working with curl:\n\n$ curl http://127.0.0.1:5000/v2/\n\n\n\n{}\n\nCreate the example application\n\nThe app used in this guide is based on the hit counter app in the Get started with Docker Compose guide. It consists of a Python app which maintains a counter in a Redis instance and increments the counter whenever you visit it.\n\nCreate a directory for the project:\n\n$ mkdir stackdemo\n\n$ cd stackdemo\n\n\nCreate a file called app.py in the project directory and paste this in:\n\nfrom flask import Flask\n\nfrom redis import Redis\n\n\n\napp = Flask(__name__)\n\nredis = Redis(host='redis', port=6379)\n\n\n\n@app.route('/')\n\ndef hello():\n\n    count = redis.incr('hits')\n\n    return 'Hello World! I have been seen {} times.\\n'.format(count)\n\n\n\nif __name__ == \"__main__\":\n\n    app.run(host=\"0.0.0.0\", port=8000, debug=True)\n\nCreate a file called requirements.txt and paste these two lines in:\n\nflask\n\nredis\n\nCreate a file called Dockerfile and paste this in:\n\n# syntax=docker/dockerfile:1\n\nFROM python:3.4-alpine\n\nADD . /code\n\nWORKDIR /code\n\nRUN pip install -r requirements.txt\n\nCMD [\"python\", \"app.py\"]\n\nCreate a file called compose.yaml and paste this in:\n\n  services:\n\n    web:\n\n      image: 127.0.0.1:5000/stackdemo\n\n      build: .\n\n      ports:\n\n        - \"8000:8000\"\n\n    redis:\n\n      image: redis:alpine\n\nThe image for the web app is built using the Dockerfile defined above. It's also tagged with 127.0.0.1:5000 - the address of the registry created earlier. This is important when distributing the app to the swarm.\n\nTest the app with Compose\n\nStart the app with docker compose up. This builds the web app image, pulls the Redis image if you don't already have it, and creates two containers.\n\nYou see a warning about the Engine being in swarm mode. This is because Compose doesn't take advantage of swarm mode, and deploys everything to a single node. You can safely ignore this.\n\n$ docker compose up -d\n\n\n\nWARNING: The Docker Engine you're using is running in swarm mode.\n\n\n\nCompose does not use swarm mode to deploy services to multiple nodes in\n\na swarm. All containers are scheduled on the current node.\n\n\n\nTo deploy your application across the swarm, use `docker stack deploy`.\n\n\n\nCreating network \"stackdemo_default\" with the default driver\n\nBuilding web\n\n...(build output)...\n\nCreating stackdemo_redis_1\n\nCreating stackdemo_web_1\n\n\nCheck that the app is running with docker compose ps:\n\n$ docker compose ps\n\n\n\n      Name                     Command               State           Ports\n\n-----------------------------------------------------------------------------------\n\nstackdemo_redis_1   docker-entrypoint.sh redis ...   Up      6379/tcp\n\nstackdemo_web_1     python app.py                    Up      0.0.0.0:8000->8000/tcp\n\n\nYou can test the app with curl:\n\n$ curl http://localhost:8000\n\nHello World! I have been seen 1 times.\n\n\n\n$ curl http://localhost:8000\n\nHello World! I have been seen 2 times.\n\n\n\n$ curl http://localhost:8000\n\nHello World! I have been seen 3 times.\n\n\nBring the app down:\n\n$ docker compose down --volumes\n\n\n\nStopping stackdemo_web_1 ... done\n\nStopping stackdemo_redis_1 ... done\n\nRemoving stackdemo_web_1 ... done\n\nRemoving stackdemo_redis_1 ... done\n\nRemoving network stackdemo_default\n\nPush the generated image to the registry\n\nTo distribute the web app's image across the swarm, it needs to be pushed to the registry you set up earlier. With Compose, this is very simple:\n\n$ docker compose push\n\n\n\nPushing web (127.0.0.1:5000/stackdemo:latest)...\n\nThe push refers to a repository [127.0.0.1:5000/stackdemo]\n\n5b5a49501a76: Pushed\n\nbe44185ce609: Pushed\n\nbd7330a79bcf: Pushed\n\nc9fc143a069a: Pushed\n\n011b303988d2: Pushed\n\nlatest: digest: sha256:a81840ebf5ac24b42c1c676cbda3b2cb144580ee347c07e1bc80e35e5ca76507 size: 1372\n\n\nThe stack is now ready to be deployed.\n\nDeploy the stack to the swarm\n\nCreate the stack with docker stack deploy:\n\n$ docker stack deploy --compose-file compose.yaml stackdemo\n\n\n\nIgnoring unsupported options: build\n\n\n\nCreating network stackdemo_default\n\nCreating service stackdemo_web\n\nCreating service stackdemo_redis\n\n\nThe last argument is a name for the stack. Each network, volume and service name is prefixed with the stack name.\n\nCheck that it's running with docker stack services stackdemo:\n\n$ docker stack services stackdemo\n\n\n\nID            NAME             MODE        REPLICAS  IMAGE\n\norvjk2263y1p  stackdemo_redis  replicated  1/1       redis:3.2-alpine@sha256:f1ed3708f538b537eb9c2a7dd50dc90a706f7debd7e1196c9264edeea521a86d\n\ns1nf0xy8t1un  stackdemo_web    replicated  1/1       127.0.0.1:5000/stackdemo@sha256:adb070e0805d04ba2f92c724298370b7a4eb19860222120d43e0f6351ddbc26f\n\n\nOnce it's running, you should see 1/1 under REPLICAS for both services. This might take some time if you have a multi-node swarm, as images need to be pulled.\n\nAs before, you can test the app with curl:\n\n$ curl http://localhost:8000\n\nHello World! I have been seen 1 times.\n\n\n\n$ curl http://localhost:8000\n\nHello World! I have been seen 2 times.\n\n\n\n$ curl http://localhost:8000\n\nHello World! I have been seen 3 times.\n\n\nWith Docker's built-in routing mesh, you can access any node in the swarm on port 8000 and get routed to the app:\n\n$ curl http://address-of-other-node:8000\n\nHello World! I have been seen 4 times.\n\n\nBring the stack down with docker stack rm:\n\n$ docker stack rm stackdemo\n\n\n\nRemoving service stackdemo_web\n\nRemoving service stackdemo_redis\n\nRemoving network stackdemo_default\n\n\nBring the registry down with docker service rm:\n\n$ docker service rm registry\n\n\nIf you're just testing things out on a local machine and want to bring your Docker Engine out of Swarm mode, use docker swarm leave:\n\n$ docker swarm leave --force\n\n\n\nNode left the swarm.\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSet up a Docker registry\nCreate the example application\nTest the app with Compose\nPush the generated image to the registry\nDeploy the stack to the swarm\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995476,
    "timestamp": "2026-02-07T06:33:19.795Z",
    "title": "Getting started with Swarm mode | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-tutorial/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nCreate a swarm\nAdd nodes to the swarm\nDeploy a service to the swarm\nInspect a service on the swarm\nScale the service in the swarm\nDelete the service running on the swarm\nApply rolling updates to a service\nDrain a node on the swarm\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nGetting started with Swarm mode\nGetting started with Swarm mode\nCopy as Markdown\n\nThis tutorial introduces you to the features of Docker Engine Swarm mode. You may want to familiarize yourself with the key concepts before you begin.\n\nThe tutorial guides you through:\n\nInitializing a cluster of Docker Engines in swarm mode\nAdding nodes to the swarm\nDeploying application services to the swarm\nManaging the swarm once you have everything running\n\nThis tutorial uses Docker Engine CLI commands entered on the command line of a terminal window.\n\nIf you are brand new to Docker, see About Docker Engine.\n\nSet up\n\nTo run this tutorial, you need:\n\nThree Linux hosts which can communicate over a network, with Docker installed\nThe IP address of the manager machine\nOpen ports between the hosts\nThree networked host machines\n\nThis tutorial requires three Linux hosts which have Docker installed and can communicate over a network. These can be physical machines, virtual machines, Amazon EC2 instances, or hosted in some other way. Check out Deploy to Swarm for one possible set-up for the hosts.\n\nOne of these machines is a manager (called manager1) and two of them are workers (worker1 and worker2).\n\nNote\n\nYou can follow many of the tutorial steps to test single-node swarm as well, in which case you need only one host. Multi-node commands do not work, but you can initialize a swarm, create services, and scale them.\n\nInstall Docker Engine on Linux machines\n\nIf you are using Linux based physical computers or cloud-provided computers as hosts, simply follow the Linux install instructions for your platform. Spin up the three machines, and you are ready. You can test both single-node and multi-node swarm scenarios on Linux machines.\n\nThe IP address of the manager machine\n\nThe IP address must be assigned to a network interface available to the host operating system. All nodes in the swarm need to connect to the manager at the IP address.\n\nBecause other nodes contact the manager node on its IP address, you should use a fixed IP address.\n\nYou can run ifconfig on Linux or macOS to see a list of the available network interfaces.\n\nThe tutorial uses manager1 : 192.168.99.100.\n\nOpen protocols and ports between the hosts\n\nThe following ports must be available. On some systems, these ports are open by default.\n\nPort 2377 TCP for communication with and between manager nodes\nPort 7946 TCP/UDP for overlay network node discovery\nPort 4789 UDP (configurable) for overlay network traffic\n\nIf you plan on creating an overlay network with encryption (--opt encrypted), you also need to ensure IP protocol 50 (IPSec ESP) traffic is allowed.\n\nPort 4789 is the default value for the Swarm data path port, also known as the VXLAN port. It is important to prevent any untrusted traffic from reaching this port, as VXLAN does not provide authentication. This port should only be opened to a trusted network, and never at a perimeter firewall.\n\nIf the network which Swarm traffic traverses is not fully trusted, it is strongly suggested that encrypted overlay networks be used. If encrypted overlay networks are in exclusive use, some additional hardening is suggested:\n\nCustomize the default ingress network to use encryption\nOnly accept encrypted packets on the Data Path Port:\n# Example iptables rule (order and other tools may require customization)\n\niptables -I INPUT -m udp --dport 4789 -m policy --dir in --pol none -j DROP\nNext steps\n\nNext, you'll create a swarm.\n\nCreate a swarm\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSet up\nThree networked host machines\nInstall Docker Engine on Linux machines\nThe IP address of the manager machine\nOpen protocols and ports between the hosts\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995473,
    "timestamp": "2026-02-07T06:33:19.795Z",
    "title": "Deploy services to a swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/services/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nDeploy services to a swarm\nDeploy services to a swarm\nCopy as Markdown\n\nSwarm services use a declarative model, which means that you define the desired state of the service, and rely upon Docker to maintain this state. The state includes information such as (but not limited to):\n\nThe image name and tag the service containers should run\nHow many containers participate in the service\nWhether any ports are exposed to clients outside the swarm\nWhether the service should start automatically when Docker starts\nThe specific behavior that happens when the service is restarted (such as whether a rolling restart is used)\nCharacteristics of the nodes where the service can run (such as resource constraints and placement preferences)\n\nFor an overview of Swarm mode, see Swarm mode key concepts. For an overview of how services work, see How services work.\n\nCreate a service\n\nTo create a single-replica service with no extra configuration, you only need to supply the image name. This command starts an Nginx service with a randomly-generated name and no published ports. This is a naive example, since you can't interact with the Nginx service.\n\n$ docker service create nginx\n\n\nThe service is scheduled on an available node. To confirm that the service was created and started successfully, use the docker service ls command:\n\n$ docker service ls\n\n\n\nID                  NAME                MODE                REPLICAS            IMAGE                                                                                             PORTS\n\na3iixnklxuem        quizzical_lamarr    replicated          1/1                 docker.io/library/nginx@sha256:41ad9967ea448d7c2b203c699b429abe1ed5af331cd92533900c6d77490e0268\n\n\nCreated services do not always run right away. A service can be in a pending state if its image is unavailable, if no node meets the requirements you configure for the service, or for other reasons. See Pending services for more information.\n\nTo provide a name for your service, use the --name flag:\n\n$ docker service create --name my_web nginx\n\n\nJust like with standalone containers, you can specify a command that the service's containers should run, by adding it after the image name. This example starts a service called helloworld which uses an alpine image and runs the command ping docker.com:\n\n$ docker service create --name helloworld alpine ping docker.com\n\n\nYou can also specify an image tag for the service to use. This example modifies the previous one to use the alpine:3.6 tag:\n\n$ docker service create --name helloworld alpine:3.6 ping docker.com\n\n\nFor more details about image tag resolution, see Specify the image version the service should use.\n\ngMSA for Swarm\nNote\n\nThis example only works for a Windows container.\n\nSwarm now allows using a Docker config as a gMSA credential spec - a requirement for Active Directory-authenticated applications. This reduces the burden of distributing credential specs to the nodes they're used on.\n\nThe following example assumes a gMSA and its credential spec (called credspec.json) already exists, and that the nodes being deployed to are correctly configured for the gMSA.\n\nTo use a config as a credential spec, first create the Docker config containing the credential spec:\n\n$ docker config create credspec credspec.json\n\n\nNow, you should have a Docker config named credspec, and you can create a service using this credential spec. To do so, use the --credential-spec flag with the config name, like this:\n\n$ docker service create --credential-spec=\"config://credspec\" <your image>\n\n\nYour service uses the gMSA credential spec when it starts, but unlike a typical Docker config (used by passing the --config flag), the credential spec is not mounted into the container.\n\nCreate a service using an image on a private registry\n\nIf your image is available on a private registry which requires login, use the --with-registry-auth flag with docker service create, after logging in. If your image is stored on registry.example.com, which is a private registry, use a command like the following:\n\n$ docker login registry.example.com\n\n\n\n$ docker service  create \\\n\n  --with-registry-auth \\\n\n  --name my_service \\\n\n  registry.example.com/acme/my_image:latest\n\n\nThis passes the login token from your local client to the swarm nodes where the service is deployed, using the encrypted WAL logs. With this information, the nodes are able to log into the registry and pull the image.\n\nProvide credential specs for managed service accounts\n\nIn Enterprise Edition 3.0, security is improved through the centralized distribution and management of Group Managed Service Account(gMSA) credentials using Docker config functionality. Swarm now allows using a Docker config as a gMSA credential spec, which reduces the burden of distributing credential specs to the nodes on which they are used.\n\nNote\n\nThis option is only applicable to services using Windows containers.\n\nCredential spec files are applied at runtime, eliminating the need for host-based credential spec files or registry entries - no gMSA credentials are written to disk on worker nodes. You can make credential specs available to Docker Engine running swarm kit worker nodes before a container starts. When deploying a service using a gMSA-based config, the credential spec is passed directly to the runtime of containers in that service.\n\nThe --credential-spec must be in one of the following formats:\n\nfile://<filename>: The referenced file must be present in the CredentialSpecs subdirectory in the docker data directory, which defaults to C:\\ProgramData\\Docker\\ on Windows. For example, specifying file://spec.json loads C:\\ProgramData\\Docker\\CredentialSpecs\\spec.json.\nregistry://<value-name>: The credential spec is read from the Windows registry on the daemon‚Äôs host.\nconfig://<config-name>: The config name is automatically converted to the config ID in the CLI. The credential spec contained in the specified config is used.\n\nThe following simple example retrieves the gMSA name and JSON contents from your Active Directory (AD) instance:\n\n$ name=\"mygmsa\"\n\n$ contents=\"{...}\"\n\n$ echo $contents > contents.json\n\n\nMake sure that the nodes to which you are deploying are correctly configured for the gMSA.\n\nTo use a config as a credential spec, create a Docker config in a credential spec file named credpspec.json. You can specify any name for the name of the config.\n\n$ docker config create --label com.docker.gmsa.name=mygmsa credspec credspec.json\n\n\nNow you can create a service using this credential spec. Specify the --credential-spec flag with the config name:\n\n$ docker service create --credential-spec=\"config://credspec\" <your image>\n\n\nYour service uses the gMSA credential spec when it starts, but unlike a typical Docker config (used by passing the --config flag), the credential spec is not mounted into the container.\n\nUpdate a service\n\nYou can change almost everything about an existing service using the docker service update command. When you update a service, Docker stops its containers and restarts them with the new configuration.\n\nSince Nginx is a web service, it works much better if you publish port 80 to clients outside the swarm. You can specify this when you create the service, using the -p or --publish flag. When updating an existing service, the flag is --publish-add. There is also a --publish-rm flag to remove a port that was previously published.\n\nAssuming that the my_web service from the previous section still exists, use the following command to update it to publish port 80.\n\n$ docker service update --publish-add 80 my_web\n\n\nTo verify that it worked, use docker service ls:\n\n$ docker service ls\n\n\n\nID                  NAME                MODE                REPLICAS            IMAGE                                                                                             PORTS\n\n4nhxl7oxw5vz        my_web              replicated          1/1                 docker.io/library/nginx@sha256:41ad9967ea448d7c2b203c699b429abe1ed5af331cd92533900c6d77490e0268   *:0->80/tcp\n\n\nFor more information on how publishing ports works, see publish ports.\n\nYou can update almost every configuration detail about an existing service, including the image name and tag it runs. See Update a service's image after creation.\n\nRemove a service\n\nTo remove a service, use the docker service remove command. You can remove a service by its ID or name, as shown in the output of the docker service ls command. The following command removes the my_web service.\n\n$ docker service remove my_web\n\nService configuration details\n\nThe following sections provide details about service configuration. This topic does not cover every flag or scenario. In almost every instance where you can define a configuration at service creation, you can also update an existing service's configuration in a similar way.\n\nSee the command-line references for docker service create and docker service update, or run one of those commands with the --help flag.\n\nConfigure the runtime environment\n\nYou can configure the following options for the runtime environment in the container:\n\nEnvironment variables using the --env flag\nThe working directory inside the container using the --workdir flag\nThe username or UID using the --user flag\n\nThe following service's containers have an environment variable $MYVAR set to myvalue, run from the /tmp/ directory, and run as the my_user user.\n\n$ docker service create --name helloworld \\\n\n  --env MYVAR=myvalue \\\n\n  --workdir /tmp \\\n\n  --user my_user \\\n\n  alpine ping docker.com\n\nUpdate the command an existing service runs\n\nTo update the command an existing service runs, you can use the --args flag. The following example updates an existing service called helloworld so that it runs the command ping docker.com instead of whatever command it was running before:\n\n$ docker service update --args \"ping docker.com\" helloworld\n\nSpecify the image version a service should use\n\nWhen you create a service without specifying any details about the version of the image to use, the service uses the version tagged with the latest tag. You can force the service to use a specific version of the image in a few different ways, depending on your desired outcome.\n\nAn image version can be expressed in several different ways:\n\nIf you specify a tag, the manager (or the Docker client, if you use content trust) resolves that tag to a digest. When the request to create a container task is received on a worker node, the worker node only sees the digest, not the tag.\n\n$ docker service create --name=\"myservice\" ubuntu:16.04\n\n\nSome tags represent discrete releases, such as ubuntu:16.04. Tags like this almost always resolve to a stable digest over time. It is recommended that you use this kind of tag when possible.\n\nOther types of tags, such as latest or nightly, may resolve to a new digest often, depending on how often an image's author updates the tag. It is not recommended to run services using a tag which is updated frequently, to prevent different service replica tasks from using different image versions.\n\nIf you don't specify a version at all, by convention the image's latest tag is resolved to a digest. Workers use the image at this digest when creating the service task.\n\nThus, the following two commands are equivalent:\n\n$ docker service create --name=\"myservice\" ubuntu\n\n\n\n$ docker service create --name=\"myservice\" ubuntu:latest\n\n\nIf you specify a digest directly, that exact version of the image is always used when creating service tasks.\n\n$ docker service create \\\n\n    --name=\"myservice\" \\\n\n    ubuntu:16.04@sha256:35bc48a1ca97c3971611dc4662d08d131869daa692acb281c7e9e052924e38b1\n\n\nWhen you create a service, the image's tag is resolved to the specific digest the tag points to at the time of service creation. Worker nodes for that service use that specific digest forever unless the service is explicitly updated. This feature is particularly important if you do use often-changing tags such as latest, because it ensures that all service tasks use the same version of the image.\n\nNote\n\nIf content trust is enabled, the client actually resolves the image's tag to a digest before contacting the swarm manager, to verify that the image is signed. Thus, if you use content trust, the swarm manager receives the request pre-resolved. In this case, if the client cannot resolve the image to a digest, the request fails.\n\nIf the manager can't resolve the tag to a digest, each worker node is responsible for resolving the tag to a digest, and different nodes may use different versions of the image. If this happens, a warning like the following is logged, substituting the placeholders for real information.\n\nunable to pin image <IMAGE-NAME> to digest: REASON\n\nTo see an image's current digest, issue the command docker inspect <IMAGE>:<TAG> and look for the RepoDigests line. The following is the current digest for ubuntu:latest at the time this content was written. The output is truncated for clarity.\n\n$ docker inspect ubuntu:latest\n\n\"RepoDigests\": [\n\n    \"ubuntu@sha256:35bc48a1ca97c3971611dc4662d08d131869daa692acb281c7e9e052924e38b1\"\n\n],\n\nAfter you create a service, its image is never updated unless you explicitly run docker service update with the --image flag as described below. Other update operations such as scaling the service, adding or removing networks or volumes, renaming the service, or any other type of update operation do not update the service's image.\n\nUpdate a service's image after creation\n\nEach tag represents a digest, similar to a Git hash. Some tags, such as latest, are updated often to point to a new digest. Others, such as ubuntu:16.04, represent a released software version and are not expected to update to point to a new digest often if at all. When you create a service, it is constrained to create tasks using a specific digest of an image until you update the service using service update with the --image flag.\n\nWhen you run service update with the --image flag, the swarm manager queries Docker Hub or your private Docker registry for the digest the tag currently points to and updates the service tasks to use that digest.\n\nNote\n\nIf you use content trust, the Docker client resolves image and the swarm manager receives the image and digest, rather than a tag.\n\nUsually, the manager can resolve the tag to a new digest and the service updates, redeploying each task to use the new image. If the manager can't resolve the tag or some other problem occurs, the next two sections outline what to expect.\n\nIf the manager resolves the tag\n\nIf the swarm manager can resolve the image tag to a digest, it instructs the worker nodes to redeploy the tasks and use the image at that digest.\n\nIf a worker has cached the image at that digest, it uses it.\n\nIf not, it attempts to pull the image from Docker Hub or the private registry.\n\nIf it succeeds, the task is deployed using the new image.\n\nIf the worker fails to pull the image, the service fails to deploy on that worker node. Docker tries again to deploy the task, possibly on a different worker node.\n\nIf the manager cannot resolve the tag\n\nIf the swarm manager cannot resolve the image to a digest, all is not lost:\n\nThe manager instructs the worker nodes to redeploy the tasks using the image at that tag.\n\nIf the worker has a locally cached image that resolves to that tag, it uses that image.\n\nIf the worker does not have a locally cached image that resolves to the tag, the worker tries to connect to Docker Hub or the private registry to pull the image at that tag.\n\nIf this succeeds, the worker uses that image.\n\nIf this fails, the task fails to deploy and the manager tries again to deploy the task, possibly on a different worker node.\n\nPublish ports\n\nWhen you create a swarm service, you can publish that service's ports to hosts outside the swarm in two ways:\n\nYou can rely on the routing mesh. When you publish a service port, the swarm makes the service accessible at the target port on every node, regardless of whether there is a task for the service running on that node or not. This is less complex and is the right choice for many types of services.\n\nYou can publish a service task's port directly on the swarm node where that service is running. This bypasses the routing mesh and provides the maximum flexibility, including the ability for you to develop your own routing framework. However, you are responsible for keeping track of where each task is running and routing requests to the tasks, and load-balancing across the nodes.\n\nKeep reading for more information and use cases for each of these methods.\n\nPublish a service's ports using the routing mesh\n\nTo publish a service's ports externally to the swarm, use the --publish <PUBLISHED-PORT>:<SERVICE-PORT> flag. The swarm makes the service accessible at the published port on every swarm node. If an external host connects to that port on any swarm node, the routing mesh routes it to a task. The external host does not need to know the IP addresses or internally-used ports of the service tasks to interact with the service. When a user or process connects to a service, any worker node running a service task may respond. For more details about swarm service networking, see Manage swarm service networks.\n\nExample: Run a three-task Nginx service on 10-node swarm\n\nImagine that you have a 10-node swarm, and you deploy an Nginx service running three tasks on a 10-node swarm:\n\n$ docker service create --name my_web \\\n\n                        --replicas 3 \\\n\n                        --publish published=8080,target=80 \\\n\n                        nginx\n\n\nThree tasks run on up to three nodes. You don't need to know which nodes are running the tasks; connecting to port 8080 on any of the 10 nodes connects you to one of the three nginx tasks. You can test this using curl. The following example assumes that localhost is one of the swarm nodes. If this is not the case, or localhost does not resolve to an IP address on your host, substitute the host's IP address or resolvable host name.\n\nThe HTML output is truncated:\n\n$ curl localhost:8080\n\n\n\n<!DOCTYPE html>\n\n<html>\n\n<head>\n\n<title>Welcome to nginx!</title>\n\n...truncated...\n\n</html>\n\n\nSubsequent connections may be routed to the same swarm node or a different one.\n\nPublish a service's ports directly on the swarm node\n\nUsing the routing mesh may not be the right choice for your application if you need to make routing decisions based on application state or you need total control of the process for routing requests to your service's tasks. To publish a service's port directly on the node where it is running, use the mode=host option to the --publish flag.\n\nNote\n\nIf you publish a service's ports directly on the swarm node using mode=host and also set published=<PORT> this creates an implicit limitation that you can only run one task for that service on a given swarm node. You can work around this by specifying published without a port definition, which causes Docker to assign a random port for each task.\n\nIn addition, if you use mode=host and you do not use the --mode=global flag on docker service create, it is difficult to know which nodes are running the service to route work to them.\n\nExample: Run an nginx web server service on every swarm node\n\nnginx is an open source reverse proxy, load balancer, HTTP cache, and a web server. If you run nginx as a service using the routing mesh, connecting to the nginx port on any swarm node shows you the web page for (effectively) a random swarm node running the service.\n\nThe following example runs nginx as a service on each node in your swarm and exposes nginx port locally on each swarm node.\n\n$ docker service create \\\n\n  --mode global \\\n\n  --publish mode=host,target=80,published=8080 \\\n\n  --name=nginx \\\n\n  nginx:latest\n\n\nYou can reach the nginx server on port 8080 of every swarm node. If you add a node to the swarm, a nginx task is started on it. You cannot start another service or container on any swarm node which binds to port 8080.\n\nNote\n\nThis is a purely illustrative example. Creating an application-layer routing framework for a multi-tiered service is complex and out of scope for this topic.\n\nConnect the service to an overlay network\n\nYou can use overlay networks to connect one or more services within the swarm.\n\nFirst, create overlay network on a manager node using the docker network create command with the --driver overlay flag.\n\n$ docker network create --driver overlay my-network\n\n\nAfter you create an overlay network in swarm mode, all manager nodes have access to the network.\n\nYou can create a new service and pass the --network flag to attach the service to the overlay network:\n\n$ docker service create \\\n\n  --replicas 3 \\\n\n  --network my-network \\\n\n  --name my-web \\\n\n  nginx\n\n\nThe swarm extends my-network to each node running the service.\n\nYou can also connect an existing service to an overlay network using the --network-add flag.\n\n$ docker service update --network-add my-network my-web\n\n\nTo disconnect a running service from a network, use the --network-rm flag.\n\n$ docker service update --network-rm my-network my-web\n\n\nFor more information on overlay networking and service discovery, refer to Attach services to an overlay network and Docker swarm mode overlay network security model.\n\nGrant a service access to secrets\n\nTo create a service with access to Docker-managed secrets, use the --secret flag. For more information, see Manage sensitive strings (secrets) for Docker services\n\nCustomize a service's isolation mode\nImportant\n\nThis setting applies to Windows hosts only and is ignored for Linux hosts.\n\nDocker allows you to specify a swarm service's isolation mode. The isolation mode can be one of the following:\n\ndefault: Use the default isolation mode configured for the Docker host, as configured by the -exec-opt flag or exec-opts array in daemon.json. If the daemon does not specify an isolation technology, process is the default for Windows Server, and hyperv is the default (and only) choice for Windows 10.\n\nprocess: Run the service tasks as a separate process on the host.\n\nNote\n\nprocess isolation mode is only supported on Windows Server. Windows 10 only supports hyperv isolation mode.\n\nhyperv: Run the service tasks as isolated hyperv tasks. This increases overhead but provides more isolation.\n\nYou can specify the isolation mode when creating or updating a new service using the --isolation flag.\n\nControl service placement\n\nSwarm services provide a few different ways for you to control scale and placement of services on different nodes.\n\nYou can specify whether the service needs to run a specific number of replicas or should run globally on every worker node. See Replicated or global services.\n\nYou can configure the service's CPU or memory requirements, and the service only runs on nodes which can meet those requirements.\n\nPlacement constraints let you configure the service to run only on nodes with specific (arbitrary) metadata set, and cause the deployment to fail if appropriate nodes do not exist. For instance, you can specify that your service should only run on nodes where an arbitrary label pci_compliant is set to true.\n\nPlacement preferences let you apply an arbitrary label with a range of values to each node, and spread your service's tasks across those nodes using an algorithm. Currently, the only supported algorithm is spread, which tries to place them evenly. For instance, if you label each node with a label rack which has a value from 1-10, then specify a placement preference keyed on rack, then service tasks are placed as evenly as possible across all nodes with the label rack, after taking other placement constraints, placement preferences, and other node-specific limitations into account.\n\nUnlike constraints, placement preferences are best-effort, and a service does not fail to deploy if no nodes can satisfy the preference. If you specify a placement preference for a service, nodes that match that preference are ranked higher when the swarm managers decide which nodes should run the service tasks. Other factors, such as high availability of the service, also factor into which nodes are scheduled to run service tasks. For example, if you have N nodes with the rack label (and then some others), and your service is configured to run N+1 replicas, the +1 is scheduled on a node that doesn't already have the service on it if there is one, regardless of whether that node has the rack label or not.\n\nReplicated or global services\n\nSwarm mode has two types of services: replicated and global. For replicated services, you specify the number of replica tasks for the swarm manager to schedule onto available nodes. For global services, the scheduler places one task on each available node that meets the service's placement constraints and resource requirements.\n\nYou control the type of service using the --mode flag. If you don't specify a mode, the service defaults to replicated. For replicated services, you specify the number of replica tasks you want to start using the --replicas flag. For example, to start a replicated nginx service with 3 replica tasks:\n\n$ docker service create \\\n\n  --name my_web \\\n\n  --replicas 3 \\\n\n  nginx\n\n\nTo start a global service on each available node, pass --mode global to docker service create. Every time a new node becomes available, the scheduler places a task for the global service on the new node. For example to start a service that runs alpine on every node in the swarm:\n\n$ docker service create \\\n\n  --name myservice \\\n\n  --mode global \\\n\n  alpine top\n\n\nService constraints let you set criteria for a node to meet before the scheduler deploys a service to the node. You can apply constraints to the service based upon node attributes and metadata or engine metadata. For more information on constraints, refer to the docker service create CLI reference.\n\nReserve memory or CPUs for a service\n\nTo reserve a given amount of memory or number of CPUs for a service, use the --reserve-memory or --reserve-cpu flags. If no available nodes can satisfy the requirement (for instance, if you request 4 CPUs and no node in the swarm has 4 CPUs), the service remains in a pending state until an appropriate node is available to run its tasks.\n\nOut Of Memory Exceptions (OOME)\n\nIf your service attempts to use more memory than the swarm node has available, you may experience an Out Of Memory Exception (OOME) and a container, or the Docker daemon, might be killed by the kernel OOM killer. To prevent this from happening, ensure that your application runs on hosts with adequate memory and see Understand the risks of running out of memory.\n\nSwarm services allow you to use resource constraints, placement preferences, and labels to ensure that your service is deployed to the appropriate swarm nodes.\n\nPlacement constraints\n\nUse placement constraints to control the nodes a service can be assigned to. In the following example, the service only runs on nodes with the label region set to east. If no appropriately-labelled nodes are available, tasks will wait in Pending until they become available. The --constraint flag uses an equality operator (== or !=). For replicated services, it is possible that all services run on the same node, or each node only runs one replica, or that some nodes don't run any replicas. For global services, the service runs on every node that meets the placement constraint and any resource requirements.\n\n$ docker service create \\\n\n  --name my-nginx \\\n\n  --replicas 5 \\\n\n  --constraint node.labels.region==east \\\n\n  nginx\n\n\nYou can also use the constraint service-level key in a compose.yaml file.\n\nIf you specify multiple placement constraints, the service only deploys onto nodes where they are all met. The following example limits the service to run on all nodes where region is set to east and type is not set to devel:\n\n$ docker service create \\\n\n  --name my-nginx \\\n\n  --mode global \\\n\n  --constraint node.labels.region==east \\\n\n  --constraint node.labels.type!=devel \\\n\n  nginx\n\n\nYou can also use placement constraints in conjunction with placement preferences and CPU/memory constraints. Be careful not to use settings that are not possible to fulfill.\n\nFor more information on constraints, refer to the docker service create CLI reference.\n\nPlacement preferences\n\nWhile placement constraints limit the nodes a service can run on, placement preferences try to place tasks on appropriate nodes in an algorithmic way (currently, only spread evenly). For instance, if you assign each node a rack label, you can set a placement preference to spread the service evenly across nodes with the rack label, by value. This way, if you lose a rack, the service is still running on nodes on other racks.\n\nPlacement preferences are not strictly enforced. If no node has the label you specify in your preference, the service is deployed as though the preference were not set.\n\nNote\n\nPlacement preferences are ignored for global services.\n\nThe following example sets a preference to spread the deployment across nodes based on the value of the datacenter label. If some nodes have datacenter=us-east and others have datacenter=us-west, the service is deployed as evenly as possible across the two sets of nodes.\n\n$ docker service create \\\n\n  --replicas 9 \\\n\n  --name redis_2 \\\n\n  --placement-pref 'spread=node.labels.datacenter' \\\n\n  redis:7.4.0\n\nNote\n\nNodes which are missing the label used to spread still receive task assignments. As a group, these nodes receive tasks in equal proportion to any of the other groups identified by a specific label value. In a sense, a missing label is the same as having the label with a null value attached to it. If the service should only run on nodes with the label being used for the spread preference, the preference should be combined with a constraint.\n\nYou can specify multiple placement preferences, and they are processed in the order they are encountered. The following example sets up a service with multiple placement preferences. Tasks are spread first over the various datacenters, and then over racks (as indicated by the respective labels):\n\n$ docker service create \\\n\n  --replicas 9 \\\n\n  --name redis_2 \\\n\n  --placement-pref 'spread=node.labels.datacenter' \\\n\n  --placement-pref 'spread=node.labels.rack' \\\n\n  redis:7.4.0\n\n\nYou can also use placement preferences in conjunction with placement constraints or CPU/memory constraints. Be careful not to use settings that are not possible to fulfill.\n\nThis diagram illustrates how placement preferences work:\n\nWhen updating a service with docker service update, --placement-pref-add appends a new placement preference after all existing placement preferences. --placement-pref-rm removes an existing placement preference that matches the argument.\n\nConfigure a service's update behavior\n\nWhen you create a service, you can specify a rolling update behavior for how the swarm should apply changes to the service when you run docker service update. You can also specify these flags as part of the update, as arguments to docker service update.\n\nThe --update-delay flag configures the time delay between updates to a service task or sets of tasks. You can describe the time T as a combination of the number of seconds Ts, minutes Tm, or hours Th. So 10m30s indicates a 10 minute 30 second delay.\n\nBy default the scheduler updates 1 task at a time. You can pass the --update-parallelism flag to configure the maximum number of service tasks that the scheduler updates simultaneously.\n\nWhen an update to an individual task returns a state of RUNNING, the scheduler continues the update by continuing to another task until all tasks are updated. If at any time during an update a task returns FAILED, the scheduler pauses the update. You can control the behavior using the --update-failure-action flag for docker service create or docker service update.\n\nIn the example service below, the scheduler applies updates to a maximum of 2 replicas at a time. When an updated task returns either RUNNING or FAILED, the scheduler waits 10 seconds before stopping the next task to update:\n\n$ docker service create \\\n\n  --replicas 10 \\\n\n  --name my_web \\\n\n  --update-delay 10s \\\n\n  --update-parallelism 2 \\\n\n  --update-failure-action continue \\\n\n  alpine\n\n\nThe --update-max-failure-ratio flag controls what fraction of tasks can fail during an update before the update as a whole is considered to have failed. For example, with --update-max-failure-ratio 0.1 --update-failure-action pause, after 10% of the tasks being updated fail, the update is paused.\n\nAn individual task update is considered to have failed if the task doesn't start up, or if it stops running within the monitoring period specified with the --update-monitor flag. The default value for --update-monitor is 30 seconds, which means that a task failing in the first 30 seconds after it's started counts towards the service update failure threshold, and a failure after that is not counted.\n\nRoll back to the previous version of a service\n\nIn case the updated version of a service doesn't function as expected, it's possible to manually roll back to the previous version of the service using docker service update's --rollback flag. This reverts the service to the configuration that was in place before the most recent docker service update command.\n\nOther options can be combined with --rollback; for example, --update-delay 0s, to execute the rollback without a delay between tasks:\n\n$ docker service update \\\n\n  --rollback \\\n\n  --update-delay 0s\n\n  my_web\n\n\nYou can configure a service to roll back automatically if a service update fails to deploy. See Automatically roll back if an update fails.\n\nManual rollback is handled at the server side, which allows manually-initiated rollbacks to respect the new rollback parameters. Note that --rollback cannot be used in conjunction with other flags to docker service update.\n\nAutomatically roll back if an update fails\n\nYou can configure a service in such a way that if an update to the service causes redeployment to fail, the service can automatically roll back to the previous configuration. This helps protect service availability. You can set one or more of the following flags at service creation or update. If you do not set a value, the default is used.\n\nFlag\tDefault\tDescription\n--rollback-delay\t0s\tAmount of time to wait after rolling back a task before rolling back the next one. A value of 0 means to roll back the second task immediately after the first rolled-back task deploys.\n--rollback-failure-action\tpause\tWhen a task fails to roll back, whether to pause or continue trying to roll back other tasks.\n--rollback-max-failure-ratio\t0\tThe failure rate to tolerate during a rollback, specified as a floating-point number between 0 and 1. For instance, given 5 tasks, a failure ratio of .2 would tolerate one task failing to roll back. A value of 0 means no failure are tolerated, while a value of 1 means any number of failure are tolerated.\n--rollback-monitor\t5s\tDuration after each task rollback to monitor for failure. If a task stops before this time period has elapsed, the rollback is considered to have failed.\n--rollback-parallelism\t1\tThe maximum number of tasks to roll back in parallel. By default, one task is rolled back at a time. A value of 0 causes all tasks to be rolled back in parallel.\n\nThe following example configures a redis service to roll back automatically if a docker service update fails to deploy. Two tasks can be rolled back in parallel. Tasks are monitored for 20 seconds after rollback to be sure they do not exit, and a maximum failure ratio of 20% is tolerated. Default values are used for --rollback-delay and --rollback-failure-action.\n\n$ docker service create --name=my_redis \\\n\n                        --replicas=5 \\\n\n                        --rollback-parallelism=2 \\\n\n                        --rollback-monitor=20s \\\n\n                        --rollback-max-failure-ratio=.2 \\\n\n                        redis:latest\n\nGive a service access to volumes or bind mounts\n\nFor best performance and portability, you should avoid writing important data directly into a container's writable layer. You should instead use data volumes or bind mounts. This principle also applies to services.\n\nYou can create two types of mounts for services in a swarm, volume mounts or bind mounts. Regardless of which type of mount you use, configure it using the --mount flag when you create a service, or the --mount-add or --mount-rm flag when updating an existing service. The default is a data volume if you don't specify a type.\n\nData volumes\n\nData volumes are storage that exist independently of a container. The lifecycle of data volumes under swarm services is similar to that under containers. Volumes outlive tasks and services, so their removal must be managed separately. Volumes can be created before deploying a service, or if they don't exist on a particular host when a task is scheduled there, they are created automatically according to the volume specification on the service.\n\nTo use existing data volumes with a service use the --mount flag:\n\n$ docker service create \\\n\n  --mount src=<VOLUME-NAME>,dst=<CONTAINER-PATH> \\\n\n  --name myservice \\\n\n  IMAGE\n\n\nIf a volume with the name <VOLUME-NAME> doesn't exist when a task is scheduled to a particular host, then one is created. The default volume driver is local. To use a different volume driver with this create-on-demand pattern, specify the driver and its options with the --mount flag:\n\n$ docker service create \\\n\n  --mount type=volume,src=<VOLUME-NAME>,dst=<CONTAINER-PATH>,volume-driver=DRIVER,volume-opt=<KEY0>=<VALUE0>,volume-opt=<KEY1>=<VALUE1>\n\n  --name myservice \\\n\n  IMAGE\n\n\nFor more information on how to create data volumes and the use of volume drivers, see Use volumes.\n\nBind mounts\n\nBind mounts are file system paths from the host where the scheduler deploys the container for the task. Docker mounts the path into the container. The file system path must exist before the swarm initializes the container for the task.\n\nThe following examples show bind mount syntax:\n\nTo mount a read-write bind:\n\n$ docker service create \\\n\n  --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH> \\\n\n  --name myservice \\\n\n  IMAGE\n\n\nTo mount a read-only bind:\n\n$ docker service create \\\n\n  --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH>,readonly \\\n\n  --name myservice \\\n\n  IMAGE\n\nImportant\n\nBind mounts can be useful but they can also cause problems. In most cases, it is recommended that you architect your application such that mounting paths from the host is unnecessary. The main risks include the following:\n\nIf you bind mount a host path into your service‚Äôs containers, the path must exist on every swarm node. The Docker swarm mode scheduler can schedule containers on any machine that meets resource availability requirements and satisfies all constraints and placement preferences you specify.\n\nThe Docker swarm mode scheduler may reschedule your running service containers at any time if they become unhealthy or unreachable.\n\nHost bind mounts are non-portable. When you use bind mounts, there is no guarantee that your application runs the same way in development as it does in production.\n\nCreate services using templates\n\nYou can use templates for some flags of service create, using the syntax provided by the Go's text/template package.\n\nThe following flags are supported:\n\n--hostname\n--mount\n--env\n\nValid placeholders for the Go template are:\n\nPlaceholder\tDescription\n.Service.ID\tService ID\n.Service.Name\tService name\n.Service.Labels\tService labels\n.Node.ID\tNode ID\n.Node.Hostname\tNode hostname\n.Task.Name\tTask name\n.Task.Slot\tTask slot\nTemplate example\n\nThis example sets the template of the created containers based on the service's name and the ID of the node where the container is running:\n\n$ docker service create --name hosttempl \\\n\n                        --hostname=\"{{.Node.ID}}-{{.Service.Name}}\"\\\n\n                         busybox top\n\n\nTo see the result of using the template, use the docker service ps and docker inspect commands.\n\n$ docker service ps va8ew30grofhjoychbr6iot8c\n\n\n\nID            NAME         IMAGE                                                                                   NODE          DESIRED STATE  CURRENT STATE               ERROR  PORTS\n\nwo41w8hg8qan  hosttempl.1  busybox:latest@sha256:29f5d56d12684887bdfa50dcd29fc31eea4aaf4ad3bec43daf19026a7ce69912  2e7a8a9c4da2  Running        Running about a minute ago\n\n$ docker inspect --format=\"{{.Config.Hostname}}\" hosttempl.1.wo41w8hg8qanxwjwsg4kxpprj\n\nLearn More\nSwarm administration guide\nDocker Engine command line reference\nSwarm mode tutorial\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCreate a service\ngMSA for Swarm\nCreate a service using an image on a private registry\nProvide credential specs for managed service accounts\nUpdate a service\nRemove a service\nService configuration details\nConfigure the runtime environment\nUpdate the command an existing service runs\nSpecify the image version a service should use\nUpdate a service's image after creation\nIf the manager resolves the tag\nIf the manager cannot resolve the tag\nPublish ports\nPublish a service's ports using the routing mesh\nPublish a service's ports directly on the swarm node\nConnect the service to an overlay network\nGrant a service access to secrets\nCustomize a service's isolation mode\nControl service placement\nReplicated or global services\nReserve memory or CPUs for a service\nPlacement constraints\nPlacement preferences\nConfigure a service's update behavior\nRoll back to the previous version of a service\nAutomatically roll back if an update fails\nGive a service access to volumes or bind mounts\nData volumes\nBind mounts\nCreate services using templates\nTemplate example\nLearn More\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995479,
    "timestamp": "2026-02-07T06:33:19.798Z",
    "title": "Create a swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nCreate a swarm\nAdd nodes to the swarm\nDeploy a service to the swarm\nInspect a service on the swarm\nScale the service in the swarm\nDelete the service running on the swarm\nApply rolling updates to a service\nDrain a node on the swarm\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nGetting started with Swarm mode\n/\nCreate a swarm\nCreate a swarm\nCopy as Markdown\n\nAfter you complete the tutorial setup steps, you're ready to create a swarm. Make sure the Docker Engine daemon is started on the host machines.\n\nOpen a terminal and ssh into the machine where you want to run your manager node. This tutorial uses a machine named manager1.\n\nRun the following command to create a new swarm:\n\n$ docker swarm init --advertise-addr <MANAGER-IP>\n\n\nIn the tutorial, the following command creates a swarm on the manager1 machine:\n\n$ docker swarm init --advertise-addr 192.168.99.100\n\nSwarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.\n\n\n\nTo add a worker to this swarm, run the following command:\n\n\n\n    docker swarm join \\\n\n    --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\\n\n    192.168.99.100:2377\n\n\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n\n\nThe --advertise-addr flag configures the manager node to publish its address as 192.168.99.100. The other nodes in the swarm must be able to access the manager at the IP address.\n\nThe output includes the commands to join new nodes to the swarm. Nodes will join as managers or workers depending on the value for the --token flag.\n\nRun docker info to view the current state of the swarm:\n\n$ docker info\n\n\n\nContainers: 2\n\nRunning: 0\n\nPaused: 0\n\nStopped: 2\n\n  ...snip...\n\nSwarm: active\n\n  NodeID: dxn1zf6l61qsb1josjja83ngz\n\n  Is Manager: true\n\n  Managers: 1\n\n  Nodes: 1\n\n  ...snip...\n\n\nRun the docker node ls command to view information about nodes:\n\n$ docker node ls\n\n\n\nID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS\n\ndxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader\n\n\nThe * next to the node ID indicates that you're currently connected on this node.\n\nDocker Engine Swarm mode automatically names the node with the machine host name. The tutorial covers other columns in later steps.\n\nNext steps\n\nNext, you'll add two more nodes to the cluster.\n\nAdd two more nodes\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995482,
    "timestamp": "2026-02-07T06:33:19.801Z",
    "title": "Add nodes to the swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-tutorial/add-nodes/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nCreate a swarm\nAdd nodes to the swarm\nDeploy a service to the swarm\nInspect a service on the swarm\nScale the service in the swarm\nDelete the service running on the swarm\nApply rolling updates to a service\nDrain a node on the swarm\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nGetting started with Swarm mode\n/\nAdd nodes to the swarm\nAdd nodes to the swarm\nCopy as Markdown\n\nOnce you've created a swarm with a manager node, you're ready to add worker nodes.\n\nOpen a terminal and ssh into the machine where you want to run a worker node. This tutorial uses the name worker1.\n\nRun the command produced by the docker swarm init output from the Create a swarm tutorial step to create a worker node joined to the existing swarm:\n\n$ docker swarm join \\\n\n  --token  SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\\n\n  192.168.99.100:2377\n\n\n\nThis node joined a swarm as a worker.\n\n\nIf you don't have the command available, you can run the following command on a manager node to retrieve the join command for a worker:\n\n$ docker swarm join-token worker\n\n\n\nTo add a worker to this swarm, run the following command:\n\n\n\n    docker swarm join \\\n\n    --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\\n\n    192.168.99.100:2377\n\n\nOpen a terminal and ssh into the machine where you want to run a second worker node. This tutorial uses the name worker2.\n\nRun the command produced by the docker swarm init output from the Create a swarm tutorial step to create a second worker node joined to the existing swarm:\n\n$ docker swarm join \\\n\n  --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\\n\n  192.168.99.100:2377\n\n\n\nThis node joined a swarm as a worker.\n\n\nOpen a terminal and ssh into the machine where the manager node runs and run the docker node ls command to see the worker nodes:\n\n$ docker node ls\n\nID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS\n\n03g1y59jwfg7cf99w4lt0f662    worker2   Ready   Active\n\n9j68exjopxe7wfl6yuxml7a7j    worker1   Ready   Active\n\ndxn1zf6l61qsb1josjja83ngz *  manager1  Ready   Active        Leader\n\n\nThe MANAGER column identifies the manager nodes in the swarm. The empty status in this column for worker1 and worker2 identifies them as worker nodes.\n\nSwarm management commands like docker node ls only work on manager nodes.\n\nWhat's next?\n\nNow your swarm consists of a manager and two worker nodes. Next, you'll deploy a service.\n\nDeploy a service\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995485,
    "timestamp": "2026-02-07T06:33:19.801Z",
    "title": "Deploy a service to the swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-tutorial/deploy-service/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nCreate a swarm\nAdd nodes to the swarm\nDeploy a service to the swarm\nInspect a service on the swarm\nScale the service in the swarm\nDelete the service running on the swarm\nApply rolling updates to a service\nDrain a node on the swarm\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nGetting started with Swarm mode\n/\nDeploy a service to the swarm\nDeploy a service to the swarm\nCopy as Markdown\n\nAfter you create a swarm, you can deploy a service to the swarm. For this tutorial, you also added worker nodes, but that is not a requirement to deploy a service.\n\nOpen a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named manager1.\n\nRun the following command:\n\n$ docker service create --replicas 1 --name helloworld alpine ping docker.com\n\n\n\n9uk4639qpg7npwf3fn2aasksr\n\nThe docker service create command creates the service.\nThe --name flag names the service helloworld.\nThe --replicas flag specifies the desired state of 1 running instance.\nThe arguments alpine ping docker.com define the service as an Alpine Linux container that executes the command ping docker.com.\n\nRun docker service ls to see the list of running services:\n\n$ docker service ls\n\n\n\nID            NAME        SCALE  IMAGE   COMMAND\n\n9uk4639qpg7n  helloworld  1/1    alpine  ping docker.com\n\nNext steps\n\nNow you're ready to inspect the service.\n\nInspect the service\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995488,
    "timestamp": "2026-02-07T06:33:19.806Z",
    "title": "Inspect a service on the swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-tutorial/inspect-service/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nCreate a swarm\nAdd nodes to the swarm\nDeploy a service to the swarm\nInspect a service on the swarm\nScale the service in the swarm\nDelete the service running on the swarm\nApply rolling updates to a service\nDrain a node on the swarm\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nGetting started with Swarm mode\n/\nInspect a service on the swarm\nInspect a service on the swarm\nCopy as Markdown\n\nWhen you have deployed a service to your swarm, you can use the Docker CLI to see details about the service running in the swarm.\n\nIf you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named manager1.\n\nRun docker service inspect --pretty <SERVICE-ID> to display the details about a service in an easily readable format.\n\nTo see the details on the helloworld service:\n\n[manager1]$ docker service inspect --pretty helloworld\n\n\n\nID:\t\t9uk4639qpg7npwf3fn2aasksr\n\nName:\t\thelloworld\n\nService Mode:\tREPLICATED\n\n Replicas:\t\t1\n\nPlacement:\n\nUpdateConfig:\n\n Parallelism:\t1\n\nContainerSpec:\n\n Image:\t\talpine\n\n Args:\tping docker.com\n\nResources:\n\nEndpoint Mode:  vip\n\nTip\n\nTo return the service details in json format, run the same command without the --pretty flag.\n\n[manager1]$ docker service inspect helloworld\n\n[\n\n{\n\n    \"ID\": \"9uk4639qpg7npwf3fn2aasksr\",\n\n    \"Version\": {\n\n        \"Index\": 418\n\n    },\n\n    \"CreatedAt\": \"2016-06-16T21:57:11.622222327Z\",\n\n    \"UpdatedAt\": \"2016-06-16T21:57:11.622222327Z\",\n\n    \"Spec\": {\n\n        \"Name\": \"helloworld\",\n\n        \"TaskTemplate\": {\n\n            \"ContainerSpec\": {\n\n                \"Image\": \"alpine\",\n\n                \"Args\": [\n\n                    \"ping\",\n\n                    \"docker.com\"\n\n                ]\n\n            },\n\n            \"Resources\": {\n\n                \"Limits\": {},\n\n                \"Reservations\": {}\n\n            },\n\n            \"RestartPolicy\": {\n\n                \"Condition\": \"any\",\n\n                \"MaxAttempts\": 0\n\n            },\n\n            \"Placement\": {}\n\n        },\n\n        \"Mode\": {\n\n            \"Replicated\": {\n\n                \"Replicas\": 1\n\n            }\n\n        },\n\n        \"UpdateConfig\": {\n\n            \"Parallelism\": 1\n\n        },\n\n        \"EndpointSpec\": {\n\n            \"Mode\": \"vip\"\n\n        }\n\n    },\n\n    \"Endpoint\": {\n\n        \"Spec\": {}\n\n    }\n\n}\n\n]\n\n\nRun docker service ps <SERVICE-ID> to see which nodes are running the service:\n\n[manager1]$ docker service ps helloworld\n\n\n\nNAME                                    IMAGE   NODE     DESIRED STATE  CURRENT STATE           ERROR               PORTS\n\nhelloworld.1.8p1vev3fq5zm0mi8g0as41w35  alpine  worker2  Running        Running 3 minutes\n\n\nIn this case, the one instance of the helloworld service is running on the worker2 node. You may see the service running on your manager node. By default, manager nodes in a swarm can execute tasks just like worker nodes.\n\nSwarm also shows you the DESIRED STATE and CURRENT STATE of the service task so you can see if tasks are running according to the service definition.\n\nRun docker ps on the node where the task is running to see details about the container for the task.\n\nTip\n\nIf helloworld is running on a node other than your manager node, you must ssh to that node.\n\n[worker2]$ docker ps\n\n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\ne609dde94e47        alpine:latest       \"ping docker.com\"   3 minutes ago       Up 3 minutes                            helloworld.1.8p1vev3fq5zm0mi8g0as41w35\n\nNext steps\n\nNext, you'll change the scale for the service running in the swarm.\n\nChange the scale\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995494,
    "timestamp": "2026-02-07T06:33:19.816Z",
    "title": "Delete the service running on the swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-tutorial/delete-service/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nCreate a swarm\nAdd nodes to the swarm\nDeploy a service to the swarm\nInspect a service on the swarm\nScale the service in the swarm\nDelete the service running on the swarm\nApply rolling updates to a service\nDrain a node on the swarm\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nGetting started with Swarm mode\n/\nDelete the service running on the swarm\nDelete the service running on the swarm\nCopy as Markdown\n\nThe remaining steps in the tutorial don't use the helloworld service, so now you can delete the service from the swarm.\n\nIf you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named manager1.\n\nRun docker service rm helloworld to remove the helloworld service.\n\n$ docker service rm helloworld\n\n\n\nhelloworld\n\n\nRun docker service inspect <SERVICE-ID> to verify that the swarm manager removed the service. The CLI returns a message that the service is not found:\n\n$ docker service inspect helloworld\n\n[]\n\nStatus: Error: no such service: helloworld, Code: 1\n\n\nEven though the service no longer exists, the task containers take a few seconds to clean up. You can use docker ps on the nodes to verify when the tasks have been removed.\n\n$ docker ps\n\n\n\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS     NAMES\n\ndb1651f50347        alpine:latest       \"ping docker.com\"        44 minutes ago      Up 46 seconds                 helloworld.5.9lkmos2beppihw95vdwxy1j3w\n\n43bf6e532a92        alpine:latest       \"ping docker.com\"        44 minutes ago      Up 46 seconds                 helloworld.3.a71i8rp6fua79ad43ycocl4t2\n\n5a0fb65d8fa7        alpine:latest       \"ping docker.com\"        44 minutes ago      Up 45 seconds                 helloworld.2.2jpgensh7d935qdc857pxulfr\n\nafb0ba67076f        alpine:latest       \"ping docker.com\"        44 minutes ago      Up 46 seconds                 helloworld.4.1c47o7tluz7drve4vkm2m5olx\n\n688172d3bfaa        alpine:latest       \"ping docker.com\"        45 minutes ago      Up About a minute             helloworld.1.74nbhb3fhud8jfrhigd7s29we\n\n\n\n$ docker ps\n\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS     NAMES\n\nNext steps\n\nNext, you'll set up a new service and apply a rolling update.\n\nApply rolling updates\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995491,
    "timestamp": "2026-02-07T06:33:19.816Z",
    "title": "Scale the service in the swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-tutorial/scale-service/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nCreate a swarm\nAdd nodes to the swarm\nDeploy a service to the swarm\nInspect a service on the swarm\nScale the service in the swarm\nDelete the service running on the swarm\nApply rolling updates to a service\nDrain a node on the swarm\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nGetting started with Swarm mode\n/\nScale the service in the swarm\nScale the service in the swarm\nCopy as Markdown\n\nOnce you have deployed a service to a swarm, you are ready to use the Docker CLI to scale the number of containers in the service. Containers running in a service are called tasks.\n\nIf you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named manager1.\n\nRun the following command to change the desired state of the service running in the swarm:\n\n$ docker service scale <SERVICE-ID>=<NUMBER-OF-TASKS>\n\n\nFor example:\n\n$ docker service scale helloworld=5\n\n\n\nhelloworld scaled to 5\n\n\nRun docker service ps <SERVICE-ID> to see the updated task list:\n\n$ docker service ps helloworld\n\n\n\nNAME                                    IMAGE   NODE      DESIRED STATE  CURRENT STATE\n\nhelloworld.1.8p1vev3fq5zm0mi8g0as41w35  alpine  worker2   Running        Running 7 minutes\n\nhelloworld.2.c7a7tcdq5s0uk3qr88mf8xco6  alpine  worker1   Running        Running 24 seconds\n\nhelloworld.3.6crl09vdcalvtfehfh69ogfb1  alpine  worker1   Running        Running 24 seconds\n\nhelloworld.4.auky6trawmdlcne8ad8phb0f1  alpine  manager1  Running        Running 24 seconds\n\nhelloworld.5.ba19kca06l18zujfwxyc5lkyn  alpine  worker2   Running        Running 24 seconds\n\n\nYou can see that swarm has created 4 new tasks to scale to a total of 5 running instances of Alpine Linux. The tasks are distributed between the three nodes of the swarm. One is running on manager1.\n\nRun docker ps to see the containers running on the node where you're connected. The following example shows the tasks running on manager1:\n\n$ docker ps\n\n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\n528d68040f95        alpine:latest       \"ping docker.com\"   About a minute ago   Up About a minute                       helloworld.4.auky6trawmdlcne8ad8phb0f1\n\n\nIf you want to see the containers running on other nodes, ssh into those nodes and run the docker ps command.\n\nNext steps\n\nAt this point in the tutorial, you're finished with the helloworld service. Next, you'll delete the service\n\nDelete the service\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995497,
    "timestamp": "2026-02-07T06:33:19.819Z",
    "title": "Apply rolling updates to a service | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nCreate a swarm\nAdd nodes to the swarm\nDeploy a service to the swarm\nInspect a service on the swarm\nScale the service in the swarm\nDelete the service running on the swarm\nApply rolling updates to a service\nDrain a node on the swarm\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nGetting started with Swarm mode\n/\nApply rolling updates to a service\nApply rolling updates to a service\nCopy as Markdown\n\nIn a previous step of the tutorial, you scaled the number of instances of a service. In this part of the tutorial, you deploy a service based on the Redis 7.4.0 container tag. Then you upgrade the service to use the Redis 7.4.1 container image using rolling updates.\n\nIf you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named manager1.\n\nDeploy your Redis tag to the swarm and configure the swarm with a 10 second update delay. Note that the following example shows an older Redis tag:\n\n$ docker service create \\\n\n  --replicas 3 \\\n\n  --name redis \\\n\n  --update-delay 10s \\\n\n  redis:7.4.0\n\n\n\n0u6a4s31ybk7yw2wyvtikmu50\n\n\nYou configure the rolling update policy at service deployment time.\n\nThe --update-delay flag configures the time delay between updates to a service task or sets of tasks. You can describe the time T as a combination of the number of seconds Ts, minutes Tm, or hours Th. So 10m30s indicates a 10 minute 30 second delay.\n\nBy default the scheduler updates 1 task at a time. You can pass the --update-parallelism flag to configure the maximum number of service tasks that the scheduler updates simultaneously.\n\nBy default, when an update to an individual task returns a state of RUNNING, the scheduler schedules another task to update until all tasks are updated. If at any time during an update a task returns FAILED, the scheduler pauses the update. You can control the behavior using the --update-failure-action flag for docker service create or docker service update.\n\nInspect the redis service:\n\n$ docker service inspect --pretty redis\n\n\n\nID:             0u6a4s31ybk7yw2wyvtikmu50\n\nName:           redis\n\nService Mode:   Replicated\n\n Replicas:      3\n\nPlacement:\n\n Strategy:\t    Spread\n\nUpdateConfig:\n\n Parallelism:   1\n\n Delay:         10s\n\nContainerSpec:\n\n Image:         redis:7.4.0\n\nResources:\n\nEndpoint Mode:  vip\n\n\nNow you can update the container image for redis. The swarm manager applies the update to nodes according to the UpdateConfig policy:\n\n$ docker service update --image redis:7.4.1 redis\n\nredis\n\n\nThe scheduler applies rolling updates as follows by default:\n\nStop the first task.\nSchedule update for the stopped task.\nStart the container for the updated task.\nIf the update to a task returns RUNNING, wait for the specified delay period then start the next task.\nIf, at any time during the update, a task returns FAILED, pause the update.\n\nRun docker service inspect --pretty redis to see the new image in the desired state:\n\n$ docker service inspect --pretty redis\n\n\n\nID:             0u6a4s31ybk7yw2wyvtikmu50\n\nName:           redis\n\nService Mode:   Replicated\n\n Replicas:      3\n\nPlacement:\n\n Strategy:\t    Spread\n\nUpdateConfig:\n\n Parallelism:   1\n\n Delay:         10s\n\nContainerSpec:\n\n Image:         redis:7.4.1\n\nResources:\n\nEndpoint Mode:  vip\n\n\nThe output of service inspect shows if your update paused due to failure:\n\n$ docker service inspect --pretty redis\n\n\n\nID:             0u6a4s31ybk7yw2wyvtikmu50\n\nName:           redis\n\n...snip...\n\nUpdate status:\n\n State:      paused\n\n Started:    11 seconds ago\n\n Message:    update paused due to failure or early termination of task 9p7ith557h8ndf0ui9s0q951b\n\n...snip...\n\n\nTo restart a paused update run docker service update <SERVICE-ID>. For example:\n\n$ docker service update redis\n\n\nTo avoid repeating certain update failures, you may need to reconfigure the service by passing flags to docker service update.\n\nRun docker service ps <SERVICE-ID> to watch the rolling update:\n\n$ docker service ps redis\n\n\n\nNAME                                   IMAGE        NODE       DESIRED STATE  CURRENT STATE            ERROR\n\nredis.1.dos1zffgeofhagnve8w864fco      redis:7.4.1  worker1    Running        Running 37 seconds\n\n \\_ redis.1.88rdo6pa52ki8oqx6dogf04fh  redis:7.4.0  worker2    Shutdown       Shutdown 56 seconds ago\n\nredis.2.9l3i4j85517skba5o7tn5m8g0      redis:7.4.1  worker2    Running        Running About a minute\n\n \\_ redis.2.66k185wilg8ele7ntu8f6nj6i  redis:7.4.0  worker1    Shutdown       Shutdown 2 minutes ago\n\nredis.3.egiuiqpzrdbxks3wxgn8qib1g      redis:7.4.1  worker1    Running        Running 48 seconds\n\n \\_ redis.3.ctzktfddb2tepkr45qcmqln04  redis:7.4.0  mmanager1  Shutdown       Shutdown 2 minutes ago\n\n\nBefore Swarm updates all of the tasks, you can see that some are running redis:7.4.0 while others are running redis:7.4.1. The output above shows the state once the rolling updates are done.\n\nNext steps\n\nNext, you'll learn how to drain a node in the swarm.\n\nDrain a node\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995503,
    "timestamp": "2026-02-07T06:33:19.828Z",
    "title": "How nodes work | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nHow nodes work\nHow services work\nManage swarm security with public key infrastructure (PKI)\nSwarm task states\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nHow swarm works\n/\nHow nodes work\nHow nodes work\nCopy as Markdown\n\nSwarm mode lets you create a cluster of one or more Docker Engines called a swarm. A swarm consists of one or more nodes: physical or virtual machines running Docker Engine.\n\nThere are two types of nodes: managers and workers.\n\nIf you haven't already, read through the Swarm mode overview and key concepts.\n\nManager nodes\n\nManager nodes handle cluster management tasks:\n\nMaintaining cluster state\nScheduling services\nServing Swarm mode HTTP API endpoints\n\nUsing a Raft implementation, the managers maintain a consistent internal state of the entire swarm and all the services running on it. For testing purposes it is OK to run a swarm with a single manager. If the manager in a single-manager swarm fails, your services continue to run, but you need to create a new cluster to recover.\n\nTo take advantage of Swarm mode's fault-tolerance features, we recommend you implement an odd number of nodes according to your organization's high-availability requirements. When you have multiple managers you can recover from the failure of a manager node without downtime.\n\nA three-manager swarm tolerates a maximum loss of one manager.\n\nA five-manager swarm tolerates a maximum simultaneous loss of two manager nodes.\n\nAn odd number N of manager nodes in the cluster tolerates the loss of at most (N-1)/2 managers. Docker recommends a maximum of seven manager nodes for a swarm.\n\nImportant\n\nAdding more managers does NOT mean increased scalability or higher performance. In general, the opposite is true.\n\nWorker nodes\n\nWorker nodes are also instances of Docker Engine whose sole purpose is to execute containers. Worker nodes don't participate in the Raft distributed state, make scheduling decisions, or serve the swarm mode HTTP API.\n\nYou can create a swarm of one manager node, but you cannot have a worker node without at least one manager node. By default, all managers are also workers. In a single manager node cluster, you can run commands like docker service create and the scheduler places all tasks on the local engine.\n\nTo prevent the scheduler from placing tasks on a manager node in a multi-node swarm, set the availability for the manager node to Drain. The scheduler gracefully stops tasks on nodes in Drain mode and schedules the tasks on an Active node. The scheduler does not assign new tasks to nodes with Drain availability.\n\nRefer to the docker node update command line reference to see how to change node availability.\n\nChange roles\n\nYou can promote a worker node to be a manager by running docker node promote. For example, you may want to promote a worker node when you take a manager node offline for maintenance. See node promote.\n\nYou can also demote a manager node to a worker node. See node demote.\n\nLearn more\nRead about how Swarm mode services work.\nLearn how PKI works in Swarm mode.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nManager nodes\nWorker nodes\nChange roles\nLearn more\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995500,
    "timestamp": "2026-02-07T06:33:19.828Z",
    "title": "Drain a node on the swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-tutorial/drain-node/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nCreate a swarm\nAdd nodes to the swarm\nDeploy a service to the swarm\nInspect a service on the swarm\nScale the service in the swarm\nDelete the service running on the swarm\nApply rolling updates to a service\nDrain a node on the swarm\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nGetting started with Swarm mode\n/\nDrain a node on the swarm\nDrain a node on the swarm\nCopy as Markdown\n\nIn earlier steps of the tutorial, all the nodes have been running with Active availability. The swarm manager can assign tasks to any Active node, so up to now all nodes have been available to receive tasks.\n\nSometimes, such as planned maintenance times, you need to set a node to Drain availability. Drain availability prevents a node from receiving new tasks from the swarm manager. It also means the manager stops tasks running on the node and launches replica tasks on a node with Active availability.\n\nImportant\n\nSetting a node to Drain does not remove standalone containers from that node, such as those created with docker run, docker compose up, or the Docker Engine API. A node's status, including Drain, only affects the node's ability to schedule swarm service workloads.\n\nIf you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named manager1.\n\nVerify that all your nodes are actively available.\n\n$ docker node ls\n\n\n\nID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS\n\n1bcef6utixb0l0ca7gxuivsj0    worker2   Ready   Active\n\n38ciaotwjuritcdtn9npbnkuz    worker1   Ready   Active\n\ne216jshn25ckzbvmwlnh5jr3g *  manager1  Ready   Active        Leader\n\n\nIf you aren't still running the redis service from the rolling update tutorial, start it now:\n\n$ docker service create --replicas 3 --name redis --update-delay 10s redis:7.4.0\n\n\n\nc5uo6kdmzpon37mgj9mwglcfw\n\n\nRun docker service ps redis to see how the swarm manager assigned the tasks to different nodes:\n\n$ docker service ps redis\n\n\n\nNAME                               IMAGE        NODE     DESIRED STATE  CURRENT STATE\n\nredis.1.7q92v0nr1hcgts2amcjyqg3pq  redis:7.4.0  manager1 Running        Running 26 seconds\n\nredis.2.7h2l8h3q3wqy5f66hlv9ddmi6  redis:7.4.0  worker1  Running        Running 26 seconds\n\nredis.3.9bg7cezvedmkgg6c8yzvbhwsd  redis:7.4.0  worker2  Running        Running 26 seconds\n\n\nIn this case the swarm manager distributed one task to each node. You may see the tasks distributed differently among the nodes in your environment.\n\nRun docker node update --availability drain <NODE-ID> to drain a node that had a task assigned to it:\n\n$ docker node update --availability drain worker1\n\n\n\nworker1\n\n\nInspect the node to check its availability:\n\n$ docker node inspect --pretty worker1\n\n\n\nID:\t\t\t38ciaotwjuritcdtn9npbnkuz\n\nHostname:\t\tworker1\n\nStatus:\n\n State:\t\t\tReady\n\n Availability:\t\tDrain\n\n...snip...\n\n\nThe drained node shows Drain for Availability.\n\nRun docker service ps redis to see how the swarm manager updated the task assignments for the redis service:\n\n$ docker service ps redis\n\n\n\nNAME                                    IMAGE        NODE      DESIRED STATE  CURRENT STATE           ERROR\n\nredis.1.7q92v0nr1hcgts2amcjyqg3pq       redis:7.4.0  manager1  Running        Running 4 minutes\n\nredis.2.b4hovzed7id8irg1to42egue8       redis:7.4.0  worker2   Running        Running About a minute\n\n \\_ redis.2.7h2l8h3q3wqy5f66hlv9ddmi6   redis:7.4.0  worker1   Shutdown       Shutdown 2 minutes ago\n\nredis.3.9bg7cezvedmkgg6c8yzvbhwsd       redis:7.4.0  worker2   Running        Running 4 minutes\n\n\nThe swarm manager maintains the desired state by ending the task on a node with Drain availability and creating a new task on a node with Active availability.\n\nRun docker node update --availability active <NODE-ID> to return the drained node to an active state:\n\n$ docker node update --availability active worker1\n\n\n\nworker1\n\n\nInspect the node to see the updated state:\n\n$ docker node inspect --pretty worker1\n\n\n\nID:\t\t\t38ciaotwjuritcdtn9npbnkuz\n\nHostname:\t\tworker1\n\nStatus:\n\n State:\t\t\tReady\n\n Availability:\t\tActive\n\n...snip...\n\n\nWhen you set the node back to Active availability, it can receive new tasks:\n\nduring a service update to scale up\nduring a rolling update\nwhen you set another node to Drain availability\nwhen a task fails on another active node\nNext steps\n\nNext, you'll learn how to use a Swarm mode routing mesh\n\nUse a Swarm mode routing mesh\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995506,
    "timestamp": "2026-02-07T06:33:19.832Z",
    "title": "How services work | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nHow nodes work\nHow services work\nManage swarm security with public key infrastructure (PKI)\nSwarm task states\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nHow swarm works\n/\nHow services work\nHow services work\nCopy as Markdown\n\nTo deploy an application image when Docker Engine is in Swarm mode, you create a service. Frequently a service is the image for a microservice within the context of some larger application. Examples of services might include an HTTP server, a database, or any other type of executable program that you wish to run in a distributed environment.\n\nWhen you create a service, you specify which container image to use and which commands to execute inside running containers. You also define options for the service including:\n\nThe port where the swarm makes the service available outside the swarm\nAn overlay network for the service to connect to other services in the swarm\nCPU and memory limits and reservations\nA rolling update policy\nThe number of replicas of the image to run in the swarm\nServices, tasks, and containers\n\nWhen you deploy the service to the swarm, the swarm manager accepts your service definition as the desired state for the service. Then it schedules the service on nodes in the swarm as one or more replica tasks. The tasks run independently of each other on nodes in the swarm.\n\nFor example, imagine you want to load balance between three instances of an HTTP listener. The diagram below shows an HTTP listener service with three replicas. Each of the three instances of the listener is a task in the swarm.\n\nA container is an isolated process. In the Swarm mode model, each task invokes exactly one container. A task is analogous to a ‚Äúslot‚Äù where the scheduler places a container. Once the container is live, the scheduler recognizes that the task is in a running state. If the container fails health checks or terminates, the task terminates.\n\nTasks and scheduling\n\nA task is the atomic unit of scheduling within a swarm. When you declare a desired service state by creating or updating a service, the orchestrator realizes the desired state by scheduling tasks. For instance, you define a service that instructs the orchestrator to keep three instances of an HTTP listener running at all times. The orchestrator responds by creating three tasks. Each task is a slot that the scheduler fills by spawning a container. The container is the instantiation of the task. If an HTTP listener task subsequently fails its health check or crashes, the orchestrator creates a new replica task that spawns a new container.\n\nA task is a one-directional mechanism. It progresses monotonically through a series of states: assigned, prepared, running, etc. If the task fails, the orchestrator removes the task and its container and then creates a new task to replace it according to the desired state specified by the service.\n\nThe underlying logic of Docker's Swarm mode is a general purpose scheduler and orchestrator. The service and task abstractions themselves are unaware of the containers they implement. Hypothetically, you could implement other types of tasks such as virtual machine tasks or non-containerized process tasks. The scheduler and orchestrator are agnostic about the type of the task. However, the current version of Docker only supports container tasks.\n\nThe diagram below shows how Swarm mode accepts service create requests and schedules tasks to worker nodes.\n\nPending services\n\nA service may be configured in such a way that no node currently in the swarm can run its tasks. In this case, the service remains in state pending. Here are a few examples of when a service might remain in state pending.\n\nTip\n\nIf your only intention is to prevent a service from being deployed, scale the service to 0 instead of trying to configure it in such a way that it remains in pending.\n\nIf all nodes are paused or drained, and you create a service, it is pending until a node becomes available. In reality, the first node to become available gets all of the tasks, so this is not a good thing to do in a production environment.\n\nYou can reserve a specific amount of memory for a service. If no node in the swarm has the required amount of memory, the service remains in a pending state until a node is available which can run its tasks. If you specify a very large value, such as 500 GB, the task stays pending forever, unless you really have a node which can satisfy it.\n\nYou can impose placement constraints on the service, and the constraints may not be able to be honored at a given time.\n\nThis behavior illustrates that the requirements and configuration of your tasks are not tightly tied to the current state of the swarm. As the administrator of a swarm, you declare the desired state of your swarm, and the manager works with the nodes in the swarm to create that state. You do not need to micro-manage the tasks on the swarm.\n\nReplicated and global services\n\nThere are two types of service deployments, replicated and global.\n\nFor a replicated service, you specify the number of identical tasks you want to run. For example, you decide to deploy an HTTP service with three replicas, each serving the same content.\n\nA global service is a service that runs one task on every node. There is no pre-specified number of tasks. Each time you add a node to the swarm, the orchestrator creates a task and the scheduler assigns the task to the new node. Good candidates for global services are monitoring agents, anti-virus scanners or other types of containers that you want to run on every node in the swarm.\n\nThe diagram below shows a three-service replica in gray and a global service in black.\n\nLearn more\nRead about how Swarm mode nodes work.\nLearn how PKI works in Swarm mode.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nServices, tasks, and containers\nTasks and scheduling\nPending services\nReplicated and global services\nLearn more\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995512,
    "timestamp": "2026-02-07T06:33:19.840Z",
    "title": "Swarm task states | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/how-swarm-mode-works/swarm-task-states/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nHow nodes work\nHow services work\nManage swarm security with public key infrastructure (PKI)\nSwarm task states\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nHow swarm works\n/\nSwarm task states\nSwarm task states\nCopy as Markdown\n\nDocker lets you create services, which can start tasks. A service is a description of a desired state, and a task does the work. Work is scheduled on swarm nodes in this sequence:\n\nCreate a service by using docker service create.\nThe request goes to a Docker manager node.\nThe Docker manager node schedules the service to run on particular nodes.\nEach service can start multiple tasks.\nEach task has a life cycle, with states like NEW, PENDING, and COMPLETE.\n\nTasks are execution units that run once to completion. When a task stops, it isn't executed again, but a new task may take its place.\n\nTasks advance through a number of states until they complete or fail. Tasks are initialized in the NEW state. The task progresses forward through a number of states, and its state doesn't go backward. For example, a task never goes from COMPLETE to RUNNING.\n\nTasks go through the states in the following order:\n\nTask state\tDescription\nNEW\tThe task was initialized.\nPENDING\tResources for the task were allocated.\nASSIGNED\tDocker assigned the task to nodes.\nACCEPTED\tThe task was accepted by a worker node. If a worker node rejects the task, the state changes to REJECTED.\nREADY\tThe worker node is ready to start the task\nPREPARING\tDocker is preparing the task.\nSTARTING\tDocker is starting the task.\nRUNNING\tThe task is executing.\nCOMPLETE\tThe task exited without an error code.\nFAILED\tThe task exited with an error code.\nSHUTDOWN\tDocker requested the task to shut down.\nREJECTED\tThe worker node rejected the task.\nORPHANED\tThe node was down for too long.\nREMOVE\tThe task is not terminal but the associated service was removed or scaled down.\nView task state\n\nRun docker service ps <service-name> to get the state of a task. The CURRENT STATE field shows the task's state and how long it's been there.\n\n$ docker service ps webserver\n\nID             NAME              IMAGE    NODE        DESIRED STATE  CURRENT STATE            ERROR                              PORTS\n\nowsz0yp6z375   webserver.1       nginx    UbuntuVM    Running        Running 44 seconds ago\n\nj91iahr8s74p    \\_ webserver.1   nginx    UbuntuVM    Shutdown       Failed 50 seconds ago    \"No such container: webserver.‚Ä¶\"\n\n7dyaszg13mw2    \\_ webserver.1   nginx    UbuntuVM    Shutdown       Failed 5 hours ago       \"No such container: webserver.‚Ä¶\"\n\nWhere to go next\nLearn about swarm tasks\n\nEdit this page\n\nRequest changes\n\nTable of contents\nView task state\nWhere to go next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995509,
    "timestamp": "2026-02-07T06:33:19.840Z",
    "title": "Manage swarm security with public key infrastructure (PKI) | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/how-swarm-mode-works/pki/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nHow nodes work\nHow services work\nManage swarm security with public key infrastructure (PKI)\nSwarm task states\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nHow swarm works\n/\nManage swarm security with public key infrastructure (PKI)\nManage swarm security with public key infrastructure (PKI)\nCopy as Markdown\n\nThe Swarm mode public key infrastructure (PKI) system built into Docker makes it simple to securely deploy a container orchestration system. The nodes in a swarm use mutual Transport Layer Security (TLS) to authenticate, authorize, and encrypt the communications with other nodes in the swarm.\n\nWhen you create a swarm by running docker swarm init, Docker designates itself as a manager node. By default, the manager node generates a new root Certificate Authority (CA) along with a key pair, which are used to secure communications with other nodes that join the swarm. If you prefer, you can specify your own externally-generated root CA, using the --external-ca flag of the docker swarm init command.\n\nThe manager node also generates two tokens to use when you join additional nodes to the swarm: one worker token and one manager token. Each token includes the digest of the root CA's certificate and a randomly generated secret. When a node joins the swarm, the joining node uses the digest to validate the root CA certificate from the remote manager. The remote manager uses the secret to ensure the joining node is an approved node.\n\nEach time a new node joins the swarm, the manager issues a certificate to the node. The certificate contains a randomly generated node ID to identify the node under the certificate common name (CN) and the role under the organizational unit (OU). The node ID serves as the cryptographically secure node identity for the lifetime of the node in the current swarm.\n\nThe diagram below illustrates how manager nodes and worker nodes encrypt communications using a minimum of TLS 1.2.\n\nThe example below shows the information from a certificate from a worker node:\n\nCertificate:\n\n    Data:\n\n        Version: 3 (0x2)\n\n        Serial Number:\n\n            3b:1c:06:91:73:fb:16:ff:69:c3:f7:a2:fe:96:c1:73:e2:80:97:3b\n\n        Signature Algorithm: ecdsa-with-SHA256\n\n        Issuer: CN=swarm-ca\n\n        Validity\n\n            Not Before: Aug 30 02:39:00 2016 GMT\n\n            Not After : Nov 28 03:39:00 2016 GMT\n\n        Subject: O=ec2adilxf4ngv7ev8fwsi61i7, OU=swarm-worker, CN=dw02poa4vqvzxi5c10gm4pq2g\n\n...snip...\n\nBy default, each node in the swarm renews its certificate every three months. You can configure this interval by running the docker swarm update --cert-expiry <TIME PERIOD> command. The minimum rotation value is 1 hour. Refer to the docker swarm update CLI reference for details.\n\nRotating the CA certificate\nNote\n\nMirantis Kubernetes Engine (MKE), formerly known as Docker UCP, provides an external certificate manager service for the swarm. If you run swarm on MKE, you shouldn't rotate the CA certificates manually. Instead, contact Mirantis support if you need to rotate a certificate.\n\nIn the event that a cluster CA key or a manager node is compromised, you can rotate the swarm root CA so that none of the nodes trust certificates signed by the old root CA anymore.\n\nRun docker swarm ca --rotate to generate a new CA certificate and key. If you prefer, you can pass the --ca-cert and --external-ca flags to specify the root certificate and to use a root CA external to the swarm. Alternately, you can pass the --ca-cert and --ca-key flags to specify the exact certificate and key you would like the swarm to use.\n\nWhen you issue the docker swarm ca --rotate command, the following things happen in sequence:\n\nDocker generates a cross-signed certificate. This means that a version of the new root CA certificate is signed with the old root CA certificate. This cross-signed certificate is used as an intermediate certificate for all new node certificates. This ensures that nodes that still trust the old root CA can still validate a certificate signed by the new CA.\n\nDocker also tells all nodes to immediately renew their TLS certificates. This process may take several minutes, depending on the number of nodes in the swarm.\n\nAfter every node in the swarm has a new TLS certificate signed by the new CA, Docker forgets about the old CA certificate and key material, and tells all the nodes to trust the new CA certificate only.\n\nThis also causes a change in the swarm's join tokens. The previous join tokens are no longer valid.\n\nFrom this point on, all new node certificates issued are signed with the new root CA, and do not contain any intermediates.\n\nLearn More\nRead about how nodes work.\nLearn how Swarm mode services work.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nRotating the CA certificate\nLearn More\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995513,
    "timestamp": "2026-02-07T06:33:19.845Z",
    "title": "Join nodes to a swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/join-nodes/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nJoin nodes to a swarm\nJoin nodes to a swarm\nCopy as Markdown\n\nWhen you first create a swarm, you place a single Docker Engine into Swarm mode. To take full advantage of Swarm mode you can add nodes to the swarm:\n\nAdding worker nodes increases capacity. When you deploy a service to a swarm, the engine schedules tasks on available nodes whether they are worker nodes or manager nodes. When you add workers to your swarm, you increase the scale of the swarm to handle tasks without affecting the manager raft consensus.\nManager nodes increase fault-tolerance. Manager nodes perform the orchestration and cluster management functions for the swarm. Among manager nodes, a single leader node conducts orchestration tasks. If a leader node goes down, the remaining manager nodes elect a new leader and resume orchestration and maintenance of the swarm state. By default, manager nodes also run tasks.\n\nDocker Engine joins the swarm depending on the join-token you provide to the docker swarm join command. The node only uses the token at join time. If you subsequently rotate the token, it doesn't affect existing swarm nodes. Refer to Run Docker Engine in swarm mode.\n\nJoin as a worker node\n\nTo retrieve the join command including the join token for worker nodes, run the following command on a manager node:\n\n$ docker swarm join-token worker\n\n\n\nTo add a worker to this swarm, run the following command:\n\n\n\n    docker swarm join \\\n\n    --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\\n\n    192.168.99.100:2377\n\n\nRun the command from the output on the worker to join the swarm:\n\n$ docker swarm join \\\n\n  --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\\n\n  192.168.99.100:2377\n\n\n\nThis node joined a swarm as a worker.\n\n\nThe docker swarm join command does the following:\n\nSwitches Docker Engine on the current node into Swarm mode.\nRequests a TLS certificate from the manager.\nNames the node with the machine hostname.\nJoins the current node to the swarm at the manager listen address based upon the swarm token.\nSets the current node to Active availability, meaning it can receive tasks from the scheduler.\nExtends the ingress overlay network to the current node.\nJoin as a manager node\n\nWhen you run docker swarm join and pass the manager token, Docker Engine switches into Swarm mode the same as for workers. Manager nodes also participate in the raft consensus. The new nodes should be Reachable, but the existing manager remains the swarm Leader.\n\nDocker recommends three or five manager nodes per cluster to implement high availability. Because Swarm-mode manager nodes share data using Raft, there must be an odd number of managers. The swarm can continue to function after as long as a quorum of more than half of the manager nodes are available.\n\nFor more detail about swarm managers and administering a swarm, see Administer and maintain a swarm of Docker Engines.\n\nTo retrieve the join command including the join token for manager nodes, run the following command on a manager node:\n\n$ docker swarm join-token manager\n\n\n\nTo add a manager to this swarm, run the following command:\n\n\n\n    docker swarm join \\\n\n    --token SWMTKN-1-61ztec5kyafptydic6jfc1i33t37flcl4nuipzcusor96k7kby-5vy9t8u35tuqm7vh67lrz9xp6 \\\n\n    192.168.99.100:2377\n\n\nRun the command from the output on the new manager node to join it to the swarm:\n\n$ docker swarm join \\\n\n  --token SWMTKN-1-61ztec5kyafptydic6jfc1i33t37flcl4nuipzcusor96k7kby-5vy9t8u35tuqm7vh67lrz9xp6 \\\n\n  192.168.99.100:2377\n\n\n\nThis node joined a swarm as a manager.\n\nLearn More\nswarm join command line reference\nSwarm mode tutorial\n\nEdit this page\n\nRequest changes\n\nTable of contents\nJoin as a worker node\nJoin as a manager node\nLearn More\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995518,
    "timestamp": "2026-02-07T06:33:19.850Z",
    "title": "Lock your swarm to protect its encryption key | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm_manager_locking/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nLock your swarm to protect its encryption key\nLock your swarm to protect its encryption key\nCopy as Markdown\n\nThe Raft logs used by swarm managers are encrypted on disk by default. This at-rest encryption protects your service's configuration and data from attackers who gain access to the encrypted Raft logs. One of the reasons this feature was introduced was in support of the Docker secrets feature.\n\nWhen Docker restarts, both the TLS key used to encrypt communication among swarm nodes and the key used to encrypt and decrypt Raft logs on disk are loaded into each manager node's memory. Docker has the ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock.\n\nWhen Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time.\n\nNote\n\nYou don't need to unlock the swarm when a new node joins the swarm, because the key is propagated to it over mutual TLS.\n\nInitialize a swarm with autolocking enabled\n\nWhen you initialize a new swarm, you can use the --autolock flag to enable autolocking of swarm manager nodes when Docker restarts.\n\n$ docker swarm init --autolock\n\n\n\nSwarm initialized: current node (k1q27tfyx9rncpixhk69sa61v) is now a manager.\n\n\n\nTo add a worker to this swarm, run the following command:\n\n\n\n    docker swarm join \\\n\n    --token SWMTKN-1-0j52ln6hxjpxk2wgk917abcnxywj3xed0y8vi1e5m9t3uttrtu-7bnxvvlz2mrcpfonjuztmtts9 \\\n\n    172.31.46.109:2377\n\n\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n\n\n\nTo unlock a swarm manager after it restarts, run the `docker swarm unlock`\n\ncommand and provide the following key:\n\n\n\n    SWMKEY-1-WuYH/IX284+lRcXuoVf38viIDK3HJEKY13MIHX+tTt8\n\n\nStore the key in a safe place, such as in a password manager.\n\nWhen Docker restarts, you need to unlock the swarm. A locked swarm causes an error like the following when you try to start or restart a service:\n\n$ sudo service docker restart\n\n\n\n$ docker service ls\n\n\n\nError response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. Use \"docker swarm unlock\" to unlock it.\n\nEnable or disable autolock on an existing swarm\n\nTo enable autolock on an existing swarm, set the autolock flag to true.\n\n$ docker swarm update --autolock=true\n\n\n\nSwarm updated.\n\nTo unlock a swarm manager after it restarts, run the `docker swarm unlock`\n\ncommand and provide the following key:\n\n\n\n    SWMKEY-1-+MrE8NgAyKj5r3NcR4FiQMdgu+7W72urH0EZeSmP/0Y\n\n\n\nPlease remember to store this key in a password manager, since without it you\n\nwill not be able to restart the manager.\n\n\nTo disable autolock, set --autolock to false. The mutual TLS key and the encryption key used to read and write Raft logs are stored unencrypted on disk. There is a trade-off between the risk of storing the encryption key unencrypted at rest and the convenience of restarting a swarm without needing to unlock each manager.\n\n$ docker swarm update --autolock=false\n\n\nKeep the unlock key around for a short time after disabling autolocking, in case a manager goes down while it is still configured to lock using the old key.\n\nUnlock a swarm\n\nTo unlock a locked swarm, use docker swarm unlock.\n\n$ docker swarm unlock\n\n\n\nPlease enter unlock key:\n\n\nEnter the encryption key that was generated and shown in the command output when you locked the swarm or rotated the key, and the swarm unlocks.\n\nView the current unlock key for a running swarm\n\nConsider a situation where your swarm is running as expected, then a manager node becomes unavailable. You troubleshoot the problem and bring the physical node back online, but you need to unlock the manager by providing the unlock key to read the encrypted credentials and Raft logs.\n\nIf the key has not been rotated since the node left the swarm, and you have a quorum of functional manager nodes in the swarm, you can view the current unlock key using docker swarm unlock-key without any arguments.\n\n$ docker swarm unlock-key\n\n\n\nTo unlock a swarm manager after it restarts, run the `docker swarm unlock`\n\ncommand and provide the following key:\n\n\n\n    SWMKEY-1-8jDgbUNlJtUe5P/lcr9IXGVxqZpZUXPzd+qzcGp4ZYA\n\n\n\nPlease remember to store this key in a password manager, since without it you\n\nwill not be able to restart the manager.\n\n\nIf the key was rotated after the swarm node became unavailable and you do not have a record of the previous key, you may need to force the manager to leave the swarm and join it back to the swarm as a new manager.\n\nRotate the unlock key\n\nYou should rotate the locked swarm's unlock key on a regular schedule.\n\n$ docker swarm unlock-key --rotate\n\n\n\nSuccessfully rotated manager unlock key.\n\n\n\nTo unlock a swarm manager after it restarts, run the `docker swarm unlock`\n\ncommand and provide the following key:\n\n\n\n    SWMKEY-1-8jDgbUNlJtUe5P/lcr9IXGVxqZpZUXPzd+qzcGp4ZYA\n\n\n\nPlease remember to store this key in a password manager, since without it you\n\nwill not be able to restart the manager.\n\nWarning\n\nWhen you rotate the unlock key, keep a record of the old key around for a few minutes, so that if a manager goes down before it gets the new key, it may still be unlocked with the old one.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nInitialize a swarm with autolocking enabled\nEnable or disable autolock on an existing swarm\nUnlock a swarm\nView the current unlock key for a running swarm\nRotate the unlock key\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995519,
    "timestamp": "2026-02-07T06:33:19.856Z",
    "title": "Manage nodes in a swarm | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/manage-nodes/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nManage nodes in a swarm\nManage nodes in a swarm\nCopy as Markdown\n\nAs part of the swarm management lifecycle, you may need to:\n\nList nodes in the swarm\nInspect an individual node\nUpdate a node\nLeave the swarm\nList nodes\n\nTo view a list of nodes in the swarm run docker node ls from a manager node:\n\n$ docker node ls\n\n\n\nID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS\n\n46aqrk4e473hjbt745z53cr3t    node-5    Ready   Active        Reachable\n\n61pi3d91s0w3b90ijw3deeb2q    node-4    Ready   Active        Reachable\n\na5b2m3oghd48m8eu391pefq5u    node-3    Ready   Active\n\ne7p8btxeu3ioshyuj6lxiv6g0    node-2    Ready   Active\n\nehkv3bcimagdese79dn78otj5 *  node-1    Ready   Active        Leader\n\n\nThe AVAILABILITY column shows whether or not the scheduler can assign tasks to the node:\n\nActive means that the scheduler can assign tasks to the node.\nPause means the scheduler doesn't assign new tasks to the node, but existing tasks remain running.\nDrain means the scheduler doesn't assign new tasks to the node. The scheduler shuts down any existing tasks and schedules them on an available node.\n\nThe MANAGER STATUS column shows node participation in the Raft consensus:\n\nNo value indicates a worker node that does not participate in swarm management.\nLeader means the node is the primary manager node that makes all swarm management and orchestration decisions for the swarm.\nReachable means the node is a manager node participating in the Raft consensus quorum. If the leader node becomes unavailable, the node is eligible for election as the new leader.\nUnavailable means the node is a manager that can't communicate with other managers. If a manager node becomes unavailable, you should either join a new manager node to the swarm or promote a worker node to be a manager.\n\nFor more information on swarm administration refer to the Swarm administration guide.\n\nInspect an individual node\n\nYou can run docker node inspect <NODE-ID> on a manager node to view the details for an individual node. The output defaults to JSON format, but you can pass the --pretty flag to print the results in human-readable format. For example:\n\n$ docker node inspect self --pretty\n\n\n\nID:                     ehkv3bcimagdese79dn78otj5\n\nHostname:               node-1\n\nJoined at:              2016-06-16 22:52:44.9910662 +0000 utc\n\nStatus:\n\n State:                 Ready\n\n Availability:          Active\n\nManager Status:\n\n Address:               172.17.0.2:2377\n\n Raft Status:           Reachable\n\n Leader:                Yes\n\nPlatform:\n\n Operating System:      linux\n\n Architecture:          x86_64\n\nResources:\n\n CPUs:                  2\n\n Memory:                1.954 GiB\n\nPlugins:\n\n  Network:              overlay, host, bridge, overlay, null\n\n  Volume:               local\n\nEngine Version:         1.12.0-dev\n\nUpdate a node\n\nYou can modify node attributes to:\n\nChange node availability\nAdd or remove label metadata\nChange a node role\nChange node availability\n\nChanging node availability lets you:\n\nDrain a manager node so that it only performs swarm management tasks and is unavailable for task assignment.\nDrain a node so you can take it down for maintenance.\nPause a node so it can't receive new tasks.\nRestore unavailable or paused nodes availability status.\n\nFor example, to change a manager node to Drain availability:\n\n$ docker node update --availability drain node-1\n\n\n\nnode-1\n\n\nSee list nodes for descriptions of the different availability options.\n\nAdd or remove label metadata\n\nNode labels provide a flexible method of node organization. You can also use node labels in service constraints. Apply constraints when you create a service to limit the nodes where the scheduler assigns tasks for the service.\n\nRun docker node update --label-add on a manager node to add label metadata to a node. The --label-add flag supports either a <key> or a <key>=<value> pair.\n\nPass the --label-add flag once for each node label you want to add:\n\n$ docker node update --label-add foo --label-add bar=baz node-1\n\n\n\nnode-1\n\n\nThe labels you set for nodes using docker node update apply only to the node entity within the swarm. Do not confuse them with the Docker daemon labels for dockerd.\n\nTherefore, node labels can be used to limit critical tasks to nodes that meet certain requirements. For example, schedule only on machines where special workloads should be run, such as machines that meet PCI-SS compliance.\n\nA compromised worker could not compromise these special workloads because it cannot change node labels.\n\nEngine labels, however, are still useful because some features that do not affect secure orchestration of containers might be better off set in a decentralized manner. For instance, an engine could have a label to indicate that it has a certain type of disk device, which may not be relevant to security directly. These labels are more easily \"trusted\" by the swarm orchestrator.\n\nRefer to the docker service create CLI reference for more information about service constraints.\n\nPromote or demote a node\n\nYou can promote a worker node to the manager role. This is useful when a manager node becomes unavailable or if you want to take a manager offline for maintenance. Similarly, you can demote a manager node to the worker role.\n\nNote\n\nRegardless of your reason to promote or demote a node, you must always maintain a quorum of manager nodes in the swarm. For more information refer to the Swarm administration guide.\n\nTo promote a node or set of nodes, run docker node promote from a manager node:\n\n$ docker node promote node-3 node-2\n\n\n\nNode node-3 promoted to a manager in the swarm.\n\nNode node-2 promoted to a manager in the swarm.\n\n\nTo demote a node or set of nodes, run docker node demote from a manager node:\n\n$ docker node demote node-3 node-2\n\n\n\nManager node-3 demoted in the swarm.\n\nManager node-2 demoted in the swarm.\n\n\ndocker node promote and docker node demote are convenience commands for docker node update --role manager and docker node update --role worker respectively.\n\nInstall plugins on swarm nodes\n\nIf your swarm service relies on one or more plugins, these plugins need to be available on every node where the service could potentially be deployed. You can manually install the plugin on each node or script the installation. You can also deploy the plugin in a similar way as a global service using the Docker API, by specifying a PluginSpec instead of a ContainerSpec.\n\nNote\n\nThere is currently no way to deploy a plugin to a swarm using the Docker CLI or Docker Compose. In addition, it is not possible to install plugins from a private repository.\n\nThe PluginSpec is defined by the plugin developer. To add the plugin to all Docker nodes, use the service/create API, passing the PluginSpec JSON defined in the TaskTemplate.\n\nLeave the swarm\n\nRun the docker swarm leave command on a node to remove it from the swarm.\n\nFor example to leave the swarm on a worker node:\n\n$ docker swarm leave\n\n\n\nNode left the swarm.\n\n\nWhen a node leaves the swarm, Docker Engine stops running in Swarm mode. The orchestrator no longer schedules tasks to the node.\n\nIf the node is a manager node, you receive a warning about maintaining the quorum. To override the warning, pass the --force flag. If the last manager node leaves the swarm, the swarm becomes unavailable requiring you to take disaster recovery measures.\n\nFor information about maintaining a quorum and disaster recovery, refer to the Swarm administration guide.\n\nAfter a node leaves the swarm, you can run docker node rm on a manager node to remove the node from the node list.\n\nFor instance:\n\n$ docker node rm node-2\n\nLearn more\nSwarm administration guide\nDocker Engine command line reference\nSwarm mode tutorial\n\nEdit this page\n\nRequest changes\n\nTable of contents\nList nodes\nInspect an individual node\nUpdate a node\nChange node availability\nAdd or remove label metadata\nPromote or demote a node\nInstall plugins on swarm nodes\nLeave the swarm\nLearn more\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995524,
    "timestamp": "2026-02-07T06:33:19.862Z",
    "title": "Manage sensitive data with Docker secrets | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/secrets/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nManage sensitive data with Docker secrets\nManage sensitive data with Docker secrets\nCopy as Markdown\nAbout secrets\n\nIn terms of Docker Swarm services, a secret is a blob of data, such as a password, SSH private key, SSL certificate, or another piece of data that should not be transmitted over a network or stored unencrypted in a Dockerfile or in your application's source code. You can use Docker secrets to centrally manage this data and securely transmit it to only those containers that need access to it. Secrets are encrypted during transit and at rest in a Docker swarm. A given secret is only accessible to those services which have been granted explicit access to it, and only while those service tasks are running.\n\nYou can use secrets to manage any sensitive data which a container needs at runtime but you don't want to store in the image or in source control, such as:\n\nUsernames and passwords\nTLS certificates and keys\nSSH keys\nOther important data such as the name of a database or internal server\nGeneric strings or binary content (up to 500 kb in size)\nNote\n\nDocker secrets are only available to swarm services, not to standalone containers. To use this feature, consider adapting your container to run as a service. Stateful containers can typically run with a scale of 1 without changing the container code.\n\nAnother use case for using secrets is to provide a layer of abstraction between the container and a set of credentials. Consider a scenario where you have separate development, test, and production environments for your application. Each of these environments can have different credentials, stored in the development, test, and production swarms with the same secret name. Your containers only need to know the name of the secret to function in all three environments.\n\nYou can also use secrets to manage non-sensitive data, such as configuration files. However, Docker supports the use of configs for storing non-sensitive data. Configs are mounted into the container's filesystem directly, without the use of a RAM disk.\n\nWindows support\n\nDocker includes support for secrets on Windows containers. Where there are differences in the implementations, they are called out in the examples below. Keep the following notable differences in mind:\n\nMicrosoft Windows has no built-in driver for managing RAM disks, so within running Windows containers, secrets are persisted in clear text to the container's root disk. However, the secrets are explicitly removed when a container stops. In addition, Windows does not support persisting a running container as an image using docker commit or similar commands.\n\nOn Windows, we recommend enabling BitLocker on the volume containing the Docker root directory on the host machine to ensure that secrets for running containers are encrypted at rest.\n\nSecret files with custom targets are not directly bind-mounted into Windows containers, since Windows does not support non-directory file bind-mounts. Instead, secrets for a container are all mounted in C:\\ProgramData\\Docker\\internal\\secrets (an implementation detail which should not be relied upon by applications) within the container. Symbolic links are used to point from there to the desired target of the secret within the container. The default target is C:\\ProgramData\\Docker\\secrets.\n\nWhen creating a service which uses Windows containers, the options to specify UID, GID, and mode are not supported for secrets. Secrets are currently only accessible by administrators and users with system access within the container.\n\nHow Docker manages secrets\n\nWhen you add a secret to the swarm, Docker sends the secret to the swarm manager over a mutual TLS connection. The secret is stored in the Raft log, which is encrypted. The entire Raft log is replicated across the other managers, ensuring the same high availability guarantees for secrets as for the rest of the swarm management data.\n\nWhen you grant a newly-created or running service access to a secret, the decrypted secret is mounted into the container in an in-memory filesystem. The location of the mount point within the container defaults to /run/secrets/<secret_name> in Linux containers, or C:\\ProgramData\\Docker\\secrets in Windows containers. You can also specify a custom location.\n\nYou can update a service to grant it access to additional secrets or revoke its access to a given secret at any time.\n\nA node only has access to (encrypted) secrets if the node is a swarm manager or if it is running service tasks which have been granted access to the secret. When a container task stops running, the decrypted secrets shared to it are unmounted from the in-memory filesystem for that container and flushed from the node's memory.\n\nIf a node loses connectivity to the swarm while it is running a task container with access to a secret, the task container still has access to its secrets, but cannot receive updates until the node reconnects to the swarm.\n\nYou can add or inspect an individual secret at any time, or list all secrets. You cannot remove a secret that a running service is using. See Rotate a secret for a way to remove a secret without disrupting running services.\n\nTo update or roll back secrets more easily, consider adding a version number or date to the secret name. This is made easier by the ability to control the mount point of the secret within a given container.\n\nRead more about docker secret commands\n\nUse these links to read about specific commands, or continue to the example about using secrets with a service.\n\ndocker secret create\ndocker secret inspect\ndocker secret ls\ndocker secret rm\n--secret flag for docker service create\n--secret-add and --secret-rm flags for docker service update\nExamples\n\nThis section includes three graduated examples which illustrate how to use Docker secrets. The images used in these examples have been updated to make it easier to use Docker secrets. To find out how to modify your own images in a similar way, see Build support for Docker Secrets into your images.\n\nNote\n\nThese examples use a single-Engine swarm and unscaled services for simplicity. The examples use Linux containers, but Windows containers also support secrets. See Windows support.\n\nDefining and using secrets in compose files\n\nBoth the docker-compose and docker stack commands support defining secrets in a compose file. See the Compose file reference for details.\n\nSimple example: Get started with secrets\n\nThis simple example shows how secrets work in just a few commands. For a real-world example, continue to Intermediate example: Use secrets with a Nginx service.\n\nAdd a secret to Docker. The docker secret create command reads standard input because the last argument, which represents the file to read the secret from, is set to -.\n\n$ printf \"This is a secret\" | docker secret create my_secret_data -\n\n\nCreate a redis service and grant it access to the secret. By default, the container can access the secret at /run/secrets/<secret_name>, but you can customize the file name on the container using the target option.\n\n$ docker service  create --name redis --secret my_secret_data redis:alpine\n\n\nVerify that the task is running without issues using docker service ps. If everything is working, the output looks similar to this:\n\n$ docker service ps redis\n\n\n\nID            NAME     IMAGE         NODE              DESIRED STATE  CURRENT STATE          ERROR  PORTS\n\nbkna6bpn8r1a  redis.1  redis:alpine  ip-172-31-46-109  Running        Running 8 seconds ago  \n\n\nIf there were an error, and the task were failing and repeatedly restarting, you would see something like this:\n\n$ docker service ps redis\n\n\n\nNAME                      IMAGE         NODE  DESIRED STATE  CURRENT STATE          ERROR                      PORTS\n\nredis.1.siftice35gla      redis:alpine  moby  Running        Running 4 seconds ago                             \n\n \\_ redis.1.whum5b7gu13e  redis:alpine  moby  Shutdown       Failed 20 seconds ago      \"task: non-zero exit (1)\"  \n\n \\_ redis.1.2s6yorvd9zow  redis:alpine  moby  Shutdown       Failed 56 seconds ago      \"task: non-zero exit (1)\"  \n\n \\_ redis.1.ulfzrcyaf6pg  redis:alpine  moby  Shutdown       Failed about a minute ago  \"task: non-zero exit (1)\"  \n\n \\_ redis.1.wrny5v4xyps6  redis:alpine  moby  Shutdown       Failed 2 minutes ago       \"task: non-zero exit (1)\"\n\n\nGet the ID of the redis service task container using docker ps , so that you can use docker container exec to connect to the container and read the contents of the secret data file, which defaults to being readable by all and has the same name as the name of the secret. The first command below illustrates how to find the container ID, and the second and third commands use shell completion to do this automatically.\n\n$ docker ps --filter name=redis -q\n\n\n\n5cb1c2348a59\n\n\n\n$ docker container exec $(docker ps --filter name=redis -q) ls -l /run/secrets\n\n\n\ntotal 4\n\n-r--r--r--    1 root     root            17 Dec 13 22:48 my_secret_data\n\n\n\n$ docker container exec $(docker ps --filter name=redis -q) cat /run/secrets/my_secret_data\n\n\n\nThis is a secret\n\n\nVerify that the secret is not available if you commit the container.\n\n$ docker commit $(docker ps --filter name=redis -q) committed_redis\n\n\n\n$ docker run --rm -it committed_redis cat /run/secrets/my_secret_data\n\n\n\ncat: can't open '/run/secrets/my_secret_data': No such file or directory\n\n\nTry removing the secret. The removal fails because the redis service is running and has access to the secret.\n\n$ docker secret ls\n\n\n\nID                          NAME                CREATED             UPDATED\n\nwwwrxza8sxy025bas86593fqs   my_secret_data      4 hours ago         4 hours ago\n\n\n\n\n\n$ docker secret rm my_secret_data\n\n\n\nError response from daemon: rpc error: code = 3 desc = secret\n\n'my_secret_data' is in use by the following service: redis\n\n\nRemove access to the secret from the running redis service by updating the service.\n\n$ docker service update --secret-rm my_secret_data redis\n\n\nRepeat steps 3 and 4 again, verifying that the service no longer has access to the secret. The container ID is different, because the service update command redeploys the service.\n\n$ docker container exec -it $(docker ps --filter name=redis -q) cat /run/secrets/my_secret_data\n\n\n\ncat: can't open '/run/secrets/my_secret_data': No such file or directory\n\n\nStop and remove the service, and remove the secret from Docker.\n\n$ docker service rm redis\n\n\n\n$ docker secret rm my_secret_data\n\nSimple example: Use secrets in a Windows service\n\nThis is a very simple example which shows how to use secrets with a Microsoft IIS service running on Docker for Windows running Windows containers on Microsoft Windows 10. It is a naive example that stores the webpage in a secret.\n\nThis example assumes that you have PowerShell installed.\n\nSave the following into a new file index.html.\n\n<html lang=\"en\">\n\n  <head><title>Hello Docker</title></head>\n\n  <body>\n\n    <p>Hello Docker! You have deployed a HTML page.</p>\n\n  </body>\n\n</html>\n\nIf you have not already done so, initialize or join the swarm.\n\n> docker swarm init\n\n\nSave the index.html file as a swarm secret named homepage.\n\n> docker secret create homepage index.html\n\n\nCreate an IIS service and grant it access to the homepage secret.\n\n> docker service create `\n\n    --name my-iis `\n\n    --publish published=8000,target=8000 `\n\n    --secret src=homepage,target=\"\\inetpub\\wwwroot\\index.html\" `\n\n    microsoft/iis:nanoserver\n\nNote\n\nThere is technically no reason to use secrets for this example; configs are a better fit. This example is for illustration only.\n\nAccess the IIS service at http://localhost:8000/. It should serve the HTML content from the first step.\n\nRemove the service and the secret.\n\n> docker service rm my-iis\n\n> docker secret rm homepage\n\n> docker image remove secret-test\n\nIntermediate example: Use secrets with a Nginx service\n\nThis example is divided into two parts. The first part is all about generating the site certificate and does not directly involve Docker secrets at all, but it sets up the second part, where you store and use the site certificate and Nginx configuration as secrets.\n\nGenerate the site certificate\n\nGenerate a root CA and TLS certificate and key for your site. For production sites, you may want to use a service such as Let‚Äôs Encrypt to generate the TLS certificate and key, but this example uses command-line tools. This step is a little complicated, but is only a set-up step so that you have something to store as a Docker secret. If you want to skip these sub-steps, you can use Let's Encrypt to generate the site key and certificate, name the files site.key and site.crt, and skip to Configure the Nginx container.\n\nGenerate a root key.\n\n$ openssl genrsa -out \"root-ca.key\" 4096\n\n\nGenerate a CSR using the root key.\n\n$ openssl req \\\n\n          -new -key \"root-ca.key\" \\\n\n          -out \"root-ca.csr\" -sha256 \\\n\n          -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA'\n\n\nConfigure the root CA. Edit a new file called root-ca.cnf and paste the following contents into it. This constrains the root CA to signing leaf certificates and not intermediate CAs.\n\n[root_ca]\n\nbasicConstraints = critical,CA:TRUE,pathlen:1\n\nkeyUsage = critical, nonRepudiation, cRLSign, keyCertSign\n\nsubjectKeyIdentifier=hash\n\nSign the certificate.\n\n$ openssl x509 -req  -days 3650  -in \"root-ca.csr\" \\\n\n               -signkey \"root-ca.key\" -sha256 -out \"root-ca.crt\" \\\n\n               -extfile \"root-ca.cnf\" -extensions \\\n\n               root_ca\n\n\nGenerate the site key.\n\n$ openssl genrsa -out \"site.key\" 4096\n\n\nGenerate the site certificate and sign it with the site key.\n\n$ openssl req -new -key \"site.key\" -out \"site.csr\" -sha256 \\\n\n          -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost'\n\n\nConfigure the site certificate. Edit a new file called site.cnf and paste the following contents into it. This constrains the site certificate so that it can only be used to authenticate a server and can't be used to sign certificates.\n\n[server]\n\nauthorityKeyIdentifier=keyid,issuer\n\nbasicConstraints = critical,CA:FALSE\n\nextendedKeyUsage=serverAuth\n\nkeyUsage = critical, digitalSignature, keyEncipherment\n\nsubjectAltName = DNS:localhost, IP:127.0.0.1\n\nsubjectKeyIdentifier=hash\n\nSign the site certificate.\n\n$ openssl x509 -req -days 750 -in \"site.csr\" -sha256 \\\n\n    -CA \"root-ca.crt\" -CAkey \"root-ca.key\"  -CAcreateserial \\\n\n    -out \"site.crt\" -extfile \"site.cnf\" -extensions server\n\n\nThe site.csr and site.cnf files are not needed by the Nginx service, but you need them if you want to generate a new site certificate. Protect the root-ca.key file.\n\nConfigure the Nginx container\n\nProduce a very basic Nginx configuration that serves static files over HTTPS. The TLS certificate and key are stored as Docker secrets so that they can be rotated easily.\n\nIn the current directory, create a new file called site.conf with the following contents:\n\nserver {\n\n    listen                443 ssl;\n\n    server_name           localhost;\n\n    ssl_certificate       /run/secrets/site.crt;\n\n    ssl_certificate_key   /run/secrets/site.key;\n\n\n\n    location / {\n\n        root   /usr/share/nginx/html;\n\n        index  index.html index.htm;\n\n    }\n\n}\n\nCreate three secrets, representing the key, the certificate, and the site.conf. You can store any file as a secret as long as it is smaller than 500 KB. This allows you to decouple the key, certificate, and configuration from the services that use them. In each of these commands, the last argument represents the path to the file to read the secret from on the host machine's filesystem. In these examples, the secret name and the file name are the same.\n\n$ docker secret create site.key site.key\n\n\n\n$ docker secret create site.crt site.crt\n\n\n\n$ docker secret create site.conf site.conf\n\n$ docker secret ls\n\n\n\nID                          NAME                  CREATED             UPDATED\n\n2hvoi9mnnaof7olr3z5g3g7fp   site.key       58 seconds ago      58 seconds ago\n\naya1dh363719pkiuoldpter4b   site.crt       24 seconds ago      24 seconds ago\n\nzoa5df26f7vpcoz42qf2csth8   site.conf      11 seconds ago      11 seconds ago\n\n\nCreate a service that runs Nginx and has access to the three secrets. The last part of the docker service create command creates a symbolic link from the location of the site.conf secret to /etc/nginx.conf.d/, where Nginx looks for extra configuration files. This step happens before Nginx actually starts, so you don't need to rebuild your image if you change the Nginx configuration.\n\nNote\n\nNormally you would create a Dockerfile which copies the site.conf into place, build the image, and run a container using your custom image. This example does not require a custom image. It puts the site.conf into place and runs the container all in one step.\n\nSecrets are located within the /run/secrets/ directory in the container by default, which may require extra steps in the container to make the secret available in a different path. The example below creates a symbolic link to the true location of the site.conf file so that Nginx can read it:\n\n$ docker service create \\\n\n     --name nginx \\\n\n     --secret site.key \\\n\n     --secret site.crt \\\n\n     --secret site.conf \\\n\n     --publish published=3000,target=443 \\\n\n     nginx:latest \\\n\n     sh -c \"ln -s /run/secrets/site.conf /etc/nginx/conf.d/site.conf && exec nginx -g 'daemon off;'\"\n\n\nInstead of creating symlinks, secrets allow you to specify a custom location using the target option. The example below illustrates how the site.conf secret is made available at /etc/nginx/conf.d/site.conf inside the container without the use of symbolic links:\n\n$ docker service create \\\n\n     --name nginx \\\n\n     --secret site.key \\\n\n     --secret site.crt \\\n\n     --secret source=site.conf,target=/etc/nginx/conf.d/site.conf \\\n\n     --publish published=3000,target=443 \\\n\n     nginx:latest \\\n\n     sh -c \"exec nginx -g 'daemon off;'\"\n\n\nThe site.key and site.crt secrets use the short-hand syntax, without a custom target location set. The short syntax mounts the secrets in `/run/secrets/ with the same name as the secret. Within the running containers, the following three files now exist:\n\n/run/secrets/site.key\n/run/secrets/site.crt\n/etc/nginx/conf.d/site.conf\n\nVerify that the Nginx service is running.\n\n$ docker service ls\n\n\n\nID            NAME   MODE        REPLICAS  IMAGE\n\nzeskcec62q24  nginx  replicated  1/1       nginx:latest\n\n\n\n$ docker service ps nginx\n\n\n\nNAME                  IMAGE         NODE  DESIRED STATE  CURRENT STATE          ERROR  PORTS\n\nnginx.1.9ls3yo9ugcls  nginx:latest  moby  Running        Running 3 minutes ago\n\n\nVerify that the service is operational: you can reach the Nginx server, and that the correct TLS certificate is being used.\n\n$ curl --cacert root-ca.crt https://localhost:3000\n\n\n\n<!DOCTYPE html>\n\n<html>\n\n<head>\n\n<title>Welcome to nginx!</title>\n\n<style>\n\n    body {\n\n        width: 35em;\n\n        margin: 0 auto;\n\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n\n    }\n\n</style>\n\n</head>\n\n<body>\n\n<h1>Welcome to nginx!</h1>\n\n<p>If you see this page, the nginx web server is successfully installed and\n\nworking. Further configuration is required.</p>\n\n\n\n<p>For online documentation and support. refer to\n\n<a href=\"https://nginx.org\">nginx.org</a>.<br/>\n\nCommercial support is available at\n\n<a href=\"https://www.nginx.com\">nginx.com</a>.</p>\n\n\n\n<p><em>Thank you for using nginx.</em></p>\n\n</body>\n\n</html>\n\n$ openssl s_client -connect localhost:3000 -CAfile root-ca.crt\n\n\n\nCONNECTED(00000003)\n\ndepth=1 /C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA\n\nverify return:1\n\ndepth=0 /C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost\n\nverify return:1\n\n---\n\nCertificate chain\n\n 0 s:/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost\n\n   i:/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA\n\n---\n\nServer certificate\n\n-----BEGIN CERTIFICATE-----\n\n‚Ä¶\n\n-----END CERTIFICATE-----\n\nsubject=/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost\n\nissuer=/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA\n\n---\n\nNo client certificate CA names sent\n\n---\n\nSSL handshake has read 1663 bytes and written 712 bytes\n\n---\n\nNew, TLSv1/SSLv3, Cipher is AES256-SHA\n\nServer public key is 4096 bit\n\nSecure Renegotiation IS supported\n\nCompression: NONE\n\nExpansion: NONE\n\nSSL-Session:\n\n    Protocol  : TLSv1\n\n    Cipher    : AES256-SHA\n\n    Session-ID: A1A8BF35549C5715648A12FD7B7E3D861539316B03440187D9DA6C2E48822853\n\n    Session-ID-ctx:\n\n    Master-Key: F39D1B12274BA16D3A906F390A61438221E381952E9E1E05D3DD784F0135FB81353DA38C6D5C021CB926E844DFC49FC4\n\n    Key-Arg   : None\n\n    Start Time: 1481685096\n\n    Timeout   : 300 (sec)\n\n    Verify return code: 0 (ok)\n\n\nTo clean up after running this example, remove the nginx service and the stored secrets.\n\n$ docker service rm nginx\n\n\n\n$ docker secret rm site.crt site.key site.conf\n\nAdvanced example: Use secrets with a WordPress service\n\nIn this example, you create a single-node MySQL service with a custom root password, add the credentials as secrets, and create a single-node WordPress service which uses these credentials to connect to MySQL. The next example builds on this one and shows you how to rotate the MySQL password and update the services so that the WordPress service can still connect to MySQL.\n\nThis example illustrates some techniques to use Docker secrets to avoid saving sensitive credentials within your image or passing them directly on the command line.\n\nNote\n\nThis example uses a single-Engine swarm for simplicity, and uses a single-node MySQL service because a single MySQL server instance cannot be scaled by simply using a replicated service, and setting up a MySQL cluster is beyond the scope of this example.\n\nAlso, changing a MySQL root passphrase isn‚Äôt as simple as changing a file on disk. You must use a query or a mysqladmin command to change the password in MySQL.\n\nGenerate a random alphanumeric password for MySQL and store it as a Docker secret with the name mysql_password using the docker secret create command. To make the password shorter or longer, adjust the last argument of the openssl command. This is just one way to create a relatively random password. You can use another command to generate the password if you choose.\n\nNote\n\nAfter you create a secret, you cannot update it. You can only remove and re-create it, and you cannot remove a secret that a service is using. However, you can grant or revoke a running service's access to secrets using docker service update. If you need the ability to update a secret, consider adding a version component to the secret name, so that you can later add a new version, update the service to use it, then remove the old version.\n\nThe last argument is set to -, which indicates that the input is read from standard input.\n\n$ openssl rand -base64 20 | docker secret create mysql_password -\n\n\n\nl1vinzevzhj4goakjap5ya409\n\n\nThe value returned is not the password, but the ID of the secret. In the remainder of this tutorial, the ID output is omitted.\n\nGenerate a second secret for the MySQL root user. This secret isn't shared with the WordPress service created later. It's only needed to bootstrap the mysql service.\n\n$ openssl rand -base64 20 | docker secret create mysql_root_password -\n\n\nList the secrets managed by Docker using docker secret ls:\n\n$ docker secret ls\n\n\n\nID                          NAME                  CREATED             UPDATED\n\nl1vinzevzhj4goakjap5ya409   mysql_password        41 seconds ago      41 seconds ago\n\nyvsczlx9votfw3l0nz5rlidig   mysql_root_password   12 seconds ago      12 seconds ago\n\n\nThe secrets are stored in the encrypted Raft logs for the swarm.\n\nCreate a user-defined overlay network which is used for communication between the MySQL and WordPress services. There is no need to expose the MySQL service to any external host or container.\n\n$ docker network create -d overlay mysql_private\n\n\nCreate the MySQL service. The MySQL service has the following characteristics:\n\nBecause the scale is set to 1, only a single MySQL task runs. Load-balancing MySQL is left as an exercise to the reader and involves more than just scaling the service.\n\nOnly reachable by other containers on the mysql_private network.\n\nUses the volume mydata to store the MySQL data, so that it persists across restarts to the mysql service.\n\nThe secrets are each mounted in a tmpfs filesystem at /run/secrets/mysql_password and /run/secrets/mysql_root_password. They are never exposed as environment variables, nor can they be committed to an image if the docker commit command is run. The mysql_password secret is the one used by the non-privileged WordPress container to connect to MySQL.\n\nSets the environment variables MYSQL_PASSWORD_FILE and MYSQL_ROOT_PASSWORD_FILE to point to the files /run/secrets/mysql_password and /run/secrets/mysql_root_password. The mysql image reads the password strings from those files when initializing the system database for the first time. Afterward, the passwords are stored in the MySQL system database itself.\n\nSets environment variables MYSQL_USER and MYSQL_DATABASE. A new database called wordpress is created when the container starts, and the wordpress user has full permissions for this database only. This user cannot create or drop databases or change the MySQL configuration.\n\n$ docker service create \\\n\n     --name mysql \\\n\n     --replicas 1 \\\n\n     --network mysql_private \\\n\n     --mount type=volume,source=mydata,destination=/var/lib/mysql \\\n\n     --secret source=mysql_root_password,target=mysql_root_password \\\n\n     --secret source=mysql_password,target=mysql_password \\\n\n     -e MYSQL_ROOT_PASSWORD_FILE=\"/run/secrets/mysql_root_password\" \\\n\n     -e MYSQL_PASSWORD_FILE=\"/run/secrets/mysql_password\" \\\n\n     -e MYSQL_USER=\"wordpress\" \\\n\n     -e MYSQL_DATABASE=\"wordpress\" \\\n\n     mysql:latest\n\n\nVerify that the mysql container is running using the docker service ls command.\n\n$ docker service ls\n\n\n\nID            NAME   MODE        REPLICAS  IMAGE\n\nwvnh0siktqr3  mysql  replicated  1/1       mysql:latest\n\n\nNow that MySQL is set up, create a WordPress service that connects to the MySQL service. The WordPress service has the following characteristics:\n\nBecause the scale is set to 1, only a single WordPress task runs. Load-balancing WordPress is left as an exercise to the reader, because of limitations with storing WordPress session data on the container filesystem.\nExposes WordPress on port 30000 of the host machine, so that you can access it from external hosts. You can expose port 80 instead if you do not have a web server running on port 80 of the host machine.\nConnects to the mysql_private network so it can communicate with the mysql container, and also publishes port 80 to port 30000 on all swarm nodes.\nHas access to the mysql_password secret, but specifies a different target file name within the container. The WordPress container uses the mount point /run/secrets/wp_db_password.\nSets the environment variable WORDPRESS_DB_PASSWORD_FILE to the file path where the secret is mounted. The WordPress service reads the MySQL password string from that file and add it to the wp-config.php configuration file.\nConnects to the MySQL container using the username wordpress and the password in /run/secrets/wp_db_password and creates the wordpress database if it does not yet exist.\nStores its data, such as themes and plugins, in a volume called wpdata so these files persist when the service restarts.\n$ docker service create \\\n\n     --name wordpress \\\n\n     --replicas 1 \\\n\n     --network mysql_private \\\n\n     --publish published=30000,target=80 \\\n\n     --mount type=volume,source=wpdata,destination=/var/www/html \\\n\n     --secret source=mysql_password,target=wp_db_password \\\n\n     -e WORDPRESS_DB_USER=\"wordpress\" \\\n\n     -e WORDPRESS_DB_PASSWORD_FILE=\"/run/secrets/wp_db_password\" \\\n\n     -e WORDPRESS_DB_HOST=\"mysql:3306\" \\\n\n     -e WORDPRESS_DB_NAME=\"wordpress\" \\\n\n     wordpress:latest\n\n\nVerify the service is running using docker service ls and docker service ps commands.\n\n$ docker service ls\n\n\n\nID            NAME       MODE        REPLICAS  IMAGE\n\nwvnh0siktqr3  mysql      replicated  1/1       mysql:latest\n\nnzt5xzae4n62  wordpress  replicated  1/1       wordpress:latest\n\n$ docker service ps wordpress\n\n\n\nID            NAME         IMAGE             NODE  DESIRED STATE  CURRENT STATE           ERROR  PORTS\n\naukx6hgs9gwc  wordpress.1  wordpress:latest  moby  Running        Running 52 seconds ago   \n\n\nAt this point, you could actually revoke the WordPress service's access to the mysql_password secret, because WordPress has copied the secret to its configuration file wp-config.php. Don't do that for now, because we use it later to facilitate rotating the MySQL password.\n\nAccess http://localhost:30000/ from any swarm node and set up WordPress using the web-based wizard. All of these settings are stored in the MySQL wordpress database. WordPress automatically generates a password for your WordPress user, which is completely different from the password WordPress uses to access MySQL. Store this password securely, such as in a password manager. You need it to log into WordPress after rotating the secret.\n\nGo ahead and write a blog post or two and install a WordPress plugin or theme to verify that WordPress is fully operational and its state is saved across service restarts.\n\nDo not clean up any services or secrets if you intend to proceed to the next example, which demonstrates how to rotate the MySQL root password.\n\nExample: Rotate a secret\n\nThis example builds upon the previous one. In this scenario, you create a new secret with a new MySQL password, update the mysql and wordpress services to use it, then remove the old secret.\n\nNote\n\nChanging the password on a MySQL database involves running extra queries or commands, as opposed to just changing a single environment variable or a file, since the image only sets the MySQL password if the database doesn‚Äôt already exist, and MySQL stores the password within a MySQL database by default. Rotating passwords or other secrets may involve additional steps outside of Docker.\n\nCreate the new password and store it as a secret named mysql_password_v2.\n\n$ openssl rand -base64 20 | docker secret create mysql_password_v2 -\n\n\nUpdate the MySQL service to give it access to both the old and new secrets. Remember that you cannot update or rename a secret, but you can revoke a secret and grant access to it using a new target filename.\n\n$ docker service update \\\n\n     --secret-rm mysql_password mysql\n\n\n\n$ docker service update \\\n\n     --secret-add source=mysql_password,target=old_mysql_password \\\n\n     --secret-add source=mysql_password_v2,target=mysql_password \\\n\n     mysql\n\n\nUpdating a service causes it to restart, and when the MySQL service restarts the second time, it has access to the old secret under /run/secrets/old_mysql_password and the new secret under /run/secrets/mysql_password.\n\nEven though the MySQL service has access to both the old and new secrets now, the MySQL password for the WordPress user has not yet been changed.\n\nNote\n\nThis example does not rotate the MySQL root password.\n\nNow, change the MySQL password for the wordpress user using the mysqladmin CLI. This command reads the old and new password from the files in /run/secrets but does not expose them on the command line or save them in the shell history.\n\nDo this quickly and move on to the next step, because WordPress loses the ability to connect to MySQL.\n\nFirst, find the ID of the mysql container task.\n\n$ docker ps --filter name=mysql -q\n\n\n\nc7705cf6176f\n\n\nSubstitute the ID in the command below, or use the second variant which uses shell expansion to do it all in a single step.\n\n$ docker container exec CONTAINER_ID \\\n\n    bash -c 'mysqladmin --user=wordpress --password=\"$(< /run/secrets/old_mysql_password)\" password \"$(< /run/secrets/mysql_password)\"'\n\n\nOr:\n\n$ docker container exec $(docker ps --filter name=mysql -q) \\\n\n    bash -c 'mysqladmin --user=wordpress --password=\"$(< /run/secrets/old_mysql_password)\" password \"$(< /run/secrets/mysql_password)\"'\n\n\nUpdate the wordpress service to use the new password, keeping the target path at /run/secrets/wp_db_password. This triggers a rolling restart of the WordPress service and the new secret is used.\n\n$ docker service update \\\n\n     --secret-rm mysql_password \\\n\n     --secret-add source=mysql_password_v2,target=wp_db_password \\\n\n     wordpress    \n\n\nVerify that WordPress works by browsing to http://localhost:30000/ on any swarm node again. Use the WordPress username and password from when you ran through the WordPress wizard in the previous task.\n\nVerify that the blog post you wrote still exists, and if you changed any configuration values, verify that they are still changed.\n\nRevoke access to the old secret from the MySQL service and remove the old secret from Docker.\n\n$ docker service update \\\n\n     --secret-rm mysql_password \\\n\n     mysql\n\n\n\n$ docker secret rm mysql_password\n\n\nRun the following commands to remove the WordPress service, the MySQL container, the mydata and wpdata volumes, and the Docker secrets:\n\n$ docker service rm wordpress mysql\n\n\n\n$ docker volume rm mydata wpdata\n\n\n\n$ docker secret rm mysql_password_v2 mysql_root_password\n\nBuild support for Docker Secrets into your images\n\nIf you develop a container that can be deployed as a service and requires sensitive data, such as a credential, as an environment variable, consider adapting your image to take advantage of Docker secrets. One way to do this is to ensure that each parameter you pass to the image when creating the container can also be read from a file.\n\nMany of the Docker Official Images in the Docker library, such as the wordpress image used in the above examples, have been updated in this way.\n\nWhen you start a WordPress container, you provide it with the parameters it needs by setting them as environment variables. The WordPress image has been updated so that the environment variables which contain important data for WordPress, such as WORDPRESS_DB_PASSWORD, also have variants which can read their values from a file (WORDPRESS_DB_PASSWORD_FILE). This strategy ensures that backward compatibility is preserved, while allowing your container to read the information from a Docker-managed secret instead of being passed directly.\n\nNote\n\nDocker secrets do not set environment variables directly. This was a conscious decision, because environment variables can unintentionally be leaked between containers (for instance, if you use --link).\n\nUse Secrets in Compose\n\n\nservices:\n\n   db:\n\n     image: mysql:latest\n\n     volumes:\n\n       - db_data:/var/lib/mysql\n\n     environment:\n\n       MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password\n\n       MYSQL_DATABASE: wordpress\n\n       MYSQL_USER: wordpress\n\n       MYSQL_PASSWORD_FILE: /run/secrets/db_password\n\n     secrets:\n\n       - db_root_password\n\n       - db_password\n\n\n\n   wordpress:\n\n     depends_on:\n\n       - db\n\n     image: wordpress:latest\n\n     ports:\n\n       - \"8000:80\"\n\n     environment:\n\n       WORDPRESS_DB_HOST: db:3306\n\n       WORDPRESS_DB_USER: wordpress\n\n       WORDPRESS_DB_PASSWORD_FILE: /run/secrets/db_password\n\n     secrets:\n\n       - db_password\n\n\n\n\n\nsecrets:\n\n   db_password:\n\n     file: db_password.txt\n\n   db_root_password:\n\n     file: db_root_password.txt\n\n\n\nvolumes:\n\n    db_data:\n\nThis example creates a simple WordPress site using two secrets in a Compose file.\n\nThe top-level element secrets defines two secrets db_password and db_root_password.\n\nWhen deploying, Docker creates these two secrets and populates them with the content from the file specified in the Compose file.\n\nThe db service uses both secrets, and wordpress is using one.\n\nWhen you deploy, Docker mounts a file under /run/secrets/<secret_name> in the services. These files are never persisted on disk, but are managed in memory.\n\nEach service uses environment variables to specify where the service should look for that secret data.\n\nMore information on short and long syntax for secrets can be found in the Compose Specification.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nAbout secrets\nWindows support\nHow Docker manages secrets\nRead more about docker secret commands\nExamples\nDefining and using secrets in compose files\nSimple example: Get started with secrets\nSimple example: Use secrets in a Windows service\nIntermediate example: Use secrets with a Nginx service\nAdvanced example: Use secrets with a WordPress service\nExample: Rotate a secret\nBuild support for Docker Secrets into your images\nUse Secrets in Compose\nSecrets\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995527,
    "timestamp": "2026-02-07T06:33:19.863Z",
    "title": "Manage swarm service networks | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/networking/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nManage swarm service networks\nManage swarm service networks\nCopy as Markdown\n\nThis page describes networking for swarm services.\n\nSwarm and types of traffic\n\nA Docker swarm generates two different kinds of traffic:\n\nControl and management plane traffic: This includes swarm management messages, such as requests to join or leave the swarm. This traffic is always encrypted.\n\nApplication data plane traffic: This includes container traffic and traffic to and from external clients.\n\nKey network concepts\n\nThe following three network concepts are important to swarm services:\n\nOverlay networks manage communications among the Docker daemons participating in the swarm. You can create overlay networks, in the same way as user-defined networks for standalone containers. You can attach a service to one or more existing overlay networks as well, to enable service-to-service communication. Overlay networks are Docker networks that use the overlay network driver.\n\nThe ingress network is a special overlay network that facilitates load balancing among a service's nodes. When any swarm node receives a request on a published port, it hands that request off to a module called IPVS. IPVS keeps track of all the IP addresses participating in that service, selects one of them, and routes the request to it, over the ingress network.\n\nThe ingress network is created automatically when you initialize or join a swarm. Most users do not need to customize its configuration, but Docker allows you to do so.\n\nThe docker_gwbridge is a bridge network that connects the overlay networks (including the ingress network) to an individual Docker daemon's physical network. By default, each container a service is running is connected to its local Docker daemon host's docker_gwbridge network.\n\nThe docker_gwbridge network is created automatically when you initialize or join a swarm. Most users do not need to customize its configuration, but Docker allows you to do so.\n\nTip\n\nSee also Networking overview for more details about Swarm networking in general.\n\nFirewall considerations\n\nDocker daemons participating in a swarm need the ability to communicate with each other over the following ports:\n\nPort 7946 TCP/UDP for container network discovery.\nPort 4789 UDP (configurable) for the overlay network (including ingress) data path.\n\nWhen setting up networking in a Swarm, special care should be taken. Consult the tutorial for an overview.\n\nOverlay networking\n\nWhen you initialize a swarm or join a Docker host to an existing swarm, two new networks are created on that Docker host:\n\nAn overlay network called ingress, which handles the control and data traffic related to swarm services. When you create a swarm service and do not connect it to a user-defined overlay network, it connects to the ingress network by default.\nA bridge network called docker_gwbridge, which connects the individual Docker daemon to the other daemons participating in the swarm.\nCreate an overlay network\n\nTo create an overlay network, specify the overlay driver when using the docker network create command:\n\n$ docker network create \\\n\n  --driver overlay \\\n\n  my-network\n\n\nThe above command doesn't specify any custom options, so Docker assigns a subnet and uses default options. You can see information about the network using docker network inspect.\n\nWhen no containers are connected to the overlay network, its configuration is not very exciting:\n\n$ docker network inspect my-network\n\n[\n\n    {\n\n        \"Name\": \"my-network\",\n\n        \"Id\": \"fsf1dmx3i9q75an49z36jycxd\",\n\n        \"Created\": \"0001-01-01T00:00:00Z\",\n\n        \"Scope\": \"swarm\",\n\n        \"Driver\": \"overlay\",\n\n        \"EnableIPv6\": false,\n\n        \"IPAM\": {\n\n            \"Driver\": \"default\",\n\n            \"Options\": null,\n\n            \"Config\": []\n\n        },\n\n        \"Internal\": false,\n\n        \"Attachable\": false,\n\n        \"Ingress\": false,\n\n        \"Containers\": null,\n\n        \"Options\": {\n\n            \"com.docker.network.driver.overlay.vxlanid_list\": \"4097\"\n\n        },\n\n        \"Labels\": null\n\n    }\n\n]\n\n\nIn the above output, notice that the driver is overlay and that the scope is swarm, rather than local, host, or global scopes you might see in other types of Docker networks. This scope indicates that only hosts which are participating in the swarm can access this network.\n\nThe network's subnet and gateway are dynamically configured when a service connects to the network for the first time. The following example shows the same network as above, but with three containers of a redis service connected to it.\n\n$ docker network inspect my-network\n\n[\n\n    {\n\n        \"Name\": \"my-network\",\n\n        \"Id\": \"fsf1dmx3i9q75an49z36jycxd\",\n\n        \"Created\": \"2017-05-31T18:35:58.877628262Z\",\n\n        \"Scope\": \"swarm\",\n\n        \"Driver\": \"overlay\",\n\n        \"EnableIPv6\": false,\n\n        \"IPAM\": {\n\n            \"Driver\": \"default\",\n\n            \"Options\": null,\n\n            \"Config\": [\n\n                {\n\n                    \"Subnet\": \"10.0.0.0/24\",\n\n                    \"Gateway\": \"10.0.0.1\"\n\n                }\n\n            ]\n\n        },\n\n        \"Internal\": false,\n\n        \"Attachable\": false,\n\n        \"Ingress\": false,\n\n        \"Containers\": {\n\n            \"0e08442918814c2275c31321f877a47569ba3447498db10e25d234e47773756d\": {\n\n                \"Name\": \"my-redis.1.ka6oo5cfmxbe6mq8qat2djgyj\",\n\n                \"EndpointID\": \"950ce63a3ace13fe7ef40724afbdb297a50642b6d47f83a5ca8636d44039e1dd\",\n\n                \"MacAddress\": \"02:42:0a:00:00:03\",\n\n                \"IPv4Address\": \"10.0.0.3/24\",\n\n                \"IPv6Address\": \"\"\n\n            },\n\n            \"88d55505c2a02632c1e0e42930bcde7e2fa6e3cce074507908dc4b827016b833\": {\n\n                \"Name\": \"my-redis.2.s7vlybipal9xlmjfqnt6qwz5e\",\n\n                \"EndpointID\": \"dd822cb68bcd4ae172e29c321ced70b731b9994eee5a4ad1d807d9ae80ecc365\",\n\n                \"MacAddress\": \"02:42:0a:00:00:05\",\n\n                \"IPv4Address\": \"10.0.0.5/24\",\n\n                \"IPv6Address\": \"\"\n\n            },\n\n            \"9ed165407384f1276e5cfb0e065e7914adbf2658794fd861cfb9b991eddca754\": {\n\n                \"Name\": \"my-redis.3.hbz3uk3hi5gb61xhxol27hl7d\",\n\n                \"EndpointID\": \"f62c686a34c9f4d70a47b869576c37dffe5200732e1dd6609b488581634cf5d2\",\n\n                \"MacAddress\": \"02:42:0a:00:00:04\",\n\n                \"IPv4Address\": \"10.0.0.4/24\",\n\n                \"IPv6Address\": \"\"\n\n            }\n\n        },\n\n        \"Options\": {\n\n            \"com.docker.network.driver.overlay.vxlanid_list\": \"4097\"\n\n        },\n\n        \"Labels\": {},\n\n        \"Peers\": [\n\n            {\n\n                \"Name\": \"moby-e57c567e25e2\",\n\n                \"IP\": \"192.168.65.2\"\n\n            }\n\n        ]\n\n    }\n\n]\n\nCustomize an overlay network\n\nThere may be situations where you don't want to use the default configuration for an overlay network. For a full list of configurable options, run the command docker network create --help. The following are some of the most common options to change.\n\nConfigure the subnet and gateway\n\nBy default, the network's subnet and gateway are configured automatically when the first service is connected to the network. You can configure these when creating a network using the --subnet and --gateway flags. The following example extends the previous one by configuring the subnet and gateway.\n\n$ docker network create \\\n\n  --driver overlay \\\n\n  --subnet 10.0.9.0/24 \\\n\n  --gateway 10.0.9.99 \\\n\n  my-network\n\nUsing custom default address pools\n\nTo customize subnet allocation for your Swarm networks, you can optionally configure them during swarm init.\n\nFor example, the following command is used when initializing Swarm:\n\n$ docker swarm init --default-addr-pool 10.20.0.0/16 --default-addr-pool-mask-length 26\n\n\nWhenever a user creates a network, but does not use the --subnet command line option, the subnet for this network will be allocated sequentially from the next available subnet from the pool. If the specified network is already allocated, that network will not be used for Swarm.\n\nMultiple pools can be configured if discontiguous address space is required. However, allocation from specific pools is not supported. Network subnets will be allocated sequentially from the IP pool space and subnets will be reused as they are deallocated from networks that are deleted.\n\nThe default mask length can be configured and is the same for all networks. It is set to /24 by default. To change the default subnet mask length, use the --default-addr-pool-mask-length command line option.\n\nNote\n\nDefault address pools can only be configured on swarm init and cannot be altered after cluster creation.\n\nOverlay network size limitations\n\nDocker recommends creating overlay networks with /24 blocks. The /24 overlay network blocks limit the network to 256 IP addresses.\n\nThis recommendation addresses limitations with swarm mode. If you need more than 256 IP addresses, do not increase the IP block size. You can either use dnsrr endpoint mode with an external load balancer, or use multiple smaller overlay networks. See Configure service discovery for more information about different endpoint modes.\n\nConfigure encryption of application data\n\nManagement and control plane data related to a swarm is always encrypted. For more details about the encryption mechanisms, see the Docker swarm mode overlay network security model.\n\nApplication data among swarm nodes is not encrypted by default. To encrypt this traffic on a given overlay network, use the --opt encrypted flag on docker network create. This enables IPSEC encryption at the level of the vxlan. This encryption imposes a non-negligible performance penalty, so you should test this option before using it in production.\n\nNote\n\nYou must customize the automatically created ingress to enable encryption. By default, all ingress traffic is unencrypted, as encryption is a network-level option.\n\nAttach a service to an overlay network\n\nTo attach a service to an existing overlay network, pass the --network flag to docker service create, or the --network-add flag to docker service update.\n\n$ docker service create \\\n\n  --replicas 3 \\\n\n  --name my-web \\\n\n  --network my-network \\\n\n  nginx\n\n\nService containers connected to an overlay network can communicate with each other across it.\n\nTo see which networks a service is connected to, use docker service ls to find the name of the service, then docker service ps <service-name> to list the networks. Alternately, to see which services' containers are connected to a network, use docker network inspect <network-name>. You can run these commands from any swarm node which is joined to the swarm and is in a running state.\n\nConfigure service discovery\n\nService discovery is the mechanism Docker uses to route a request from your service's external clients to an individual swarm node, without the client needing to know how many nodes are participating in the service or their IP addresses or ports. You don't need to publish ports which are used between services on the same network. For instance, if you have a WordPress service that stores its data in a MySQL service, and they are connected to the same overlay network, you do not need to publish the MySQL port to the client, only the WordPress HTTP port.\n\nService discovery can work in two different ways: internal connection-based load-balancing at Layers 3 and 4 using the embedded DNS and a virtual IP (VIP), or external and customized request-based load-balancing at Layer 7 using DNS round robin (DNSRR). You can configure this per service.\n\nBy default, when you attach a service to a network and that service publishes one or more ports, Docker assigns the service a virtual IP (VIP), which is the \"front end\" for clients to reach the service. Docker keeps a list of all worker nodes in the service, and routes requests between the client and one of the nodes. Each request from the client might be routed to a different node.\n\nIf you configure a service to use DNS round-robin (DNSRR) service discovery, there is not a single virtual IP. Instead, Docker sets up DNS entries for the service such that a DNS query for the service name returns a list of IP addresses, and the client connects directly to one of these.\n\nDNS round-robin is useful in cases where you want to use your own load balancer, such as HAProxy. To configure a service to use DNSRR, use the flag --endpoint-mode dnsrr when creating a new service or updating an existing one.\n\nCustomize the ingress network\n\nMost users never need to configure the ingress network, but Docker allows you to do so. This can be useful if the automatically-chosen subnet conflicts with one that already exists on your network, or you need to customize other low-level network settings such as the MTU, or if you want to enable encryption.\n\nCustomizing the ingress network involves removing and recreating it. This is usually done before you create any services in the swarm. If you have existing services which publish ports, those services need to be removed before you can remove the ingress network.\n\nDuring the time that no ingress network exists, existing services which do not publish ports continue to function but are not load-balanced. This affects services which publish ports, such as a WordPress service which publishes port 80.\n\nInspect the ingress network using docker network inspect ingress, and remove any services whose containers are connected to it. These are services that publish ports, such as a WordPress service which publishes port 80. If all such services are not stopped, the next step fails.\n\nRemove the existing ingress network:\n\n$ docker network rm ingress\n\n\n\nWARNING! Before removing the routing-mesh network, make sure all the nodes\n\nin your swarm run the same docker engine version. Otherwise, removal may not\n\nbe effective and functionality of newly created ingress networks will be\n\nimpaired.\n\nAre you sure you want to continue? [y/N]\n\n\nCreate a new overlay network using the --ingress flag, along with the custom options you want to set. This example sets the MTU to 1200, sets the subnet to 10.11.0.0/16, and sets the gateway to 10.11.0.2.\n\n$ docker network create \\\n\n  --driver overlay \\\n\n  --ingress \\\n\n  --subnet=10.11.0.0/16 \\\n\n  --gateway=10.11.0.2 \\\n\n  --opt com.docker.network.driver.mtu=1200 \\\n\n  my-ingress\n\nNote\n\nYou can name your ingress network something other than ingress, but you can only have one. An attempt to create a second one fails.\n\nRestart the services that you stopped in the first step.\n\nCustomize the docker_gwbridge\n\nThe docker_gwbridge is a virtual bridge that connects the overlay networks (including the ingress network) to an individual Docker daemon's physical network. Docker creates it automatically when you initialize a swarm or join a Docker host to a swarm, but it is not a Docker device. It exists in the kernel of the Docker host. If you need to customize its settings, you must do so before joining the Docker host to the swarm, or after temporarily removing the host from the swarm.\n\nYou need to have the brctl application installed on your operating system in order to delete an existing bridge. The package name is bridge-utils.\n\nStop Docker.\n\nUse the brctl show docker_gwbridge command to check whether a bridge device exists called docker_gwbridge. If so, remove it using brctl delbr docker_gwbridge.\n\nStart Docker. Do not join or initialize the swarm.\n\nCreate or re-create the docker_gwbridge bridge with your custom settings. This example uses the subnet 10.11.0.0/16. For a full list of customizable options, see Bridge driver options.\n\n$ docker network create \\\n\n--subnet 10.11.0.0/16 \\\n\n--opt com.docker.network.bridge.name=docker_gwbridge \\\n\n--opt com.docker.network.bridge.enable_icc=false \\\n\n--opt com.docker.network.bridge.enable_ip_masquerade=true \\\n\ndocker_gwbridge\n\n\nInitialize or join the swarm.\n\nUse a separate interface for control and data traffic\n\nBy default, all swarm traffic is sent over the same interface, including control and management traffic for maintaining the swarm itself and data traffic to and from the service containers.\n\nYou can separate this traffic by passing the --data-path-addr flag when initializing or joining the swarm. If there are multiple interfaces, --advertise-addr must be specified explicitly, and --data-path-addr defaults to --advertise-addr if not specified. Traffic about joining, leaving, and managing the swarm is sent over the --advertise-addr interface, and traffic among a service's containers is sent over the --data-path-addr interface. These flags can take an IP address or a network device name, such as eth0.\n\nThis example initializes a swarm with a separate --data-path-addr. It assumes that your Docker host has two different network interfaces: 10.0.0.1 should be used for control and management traffic and 192.168.0.1 should be used for traffic relating to services.\n\n$ docker swarm init --advertise-addr 10.0.0.1 --data-path-addr 192.168.0.1\n\n\nThis example joins the swarm managed by host 192.168.99.100:2377 and sets the --advertise-addr flag to eth0 and the --data-path-addr flag to eth1.\n\n$ docker swarm join \\\n\n  --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2d7c \\\n\n  --advertise-addr eth0 \\\n\n  --data-path-addr eth1 \\\n\n  192.168.99.100:2377\n\nPublish ports on an overlay network\n\nSwarm services connected to the same overlay network effectively expose all ports to each other. For a port to be accessible outside of the service, that port must be published using the -p or --publish flag on docker service create or docker service update. Both the legacy colon-separated syntax and the newer comma-separated value syntax are supported. The longer syntax is preferred because it is somewhat self-documenting.\n\nFlag value\tDescription\n-p 8080:80 or\n-p published=8080,target=80\tMap TCP port 80 on the service to port 8080 on the routing mesh.\n-p 8080:80/udp or\n-p published=8080,target=80,protocol=udp\tMap UDP port 80 on the service to port 8080 on the routing mesh.\n-p 8080:80/tcp -p 8080:80/udp or\n-p published=8080,target=80,protocol=tcp -p published=8080,target=80,protocol=udp\tMap TCP port 80 on the service to TCP port 8080 on the routing mesh, and map UDP port 80 on the service to UDP port 8080 on the routing mesh.\nLearn more\nDeploy services to a swarm\nSwarm administration guide\nSwarm mode tutorial\nNetworking overview\nDocker CLI reference\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSwarm and types of traffic\nKey network concepts\nFirewall considerations\nOverlay networking\nCreate an overlay network\nCustomize an overlay network\nAttach a service to an overlay network\nConfigure service discovery\nCustomize the ingress network\nCustomize the docker_gwbridge\nUse a separate interface for control and data traffic\nPublish ports on an overlay network\nLearn more\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995530,
    "timestamp": "2026-02-07T06:33:19.866Z",
    "title": "Raft consensus in swarm mode | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/raft/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nRaft consensus in swarm mode\nRaft consensus in swarm mode\nCopy as Markdown\n\nWhen Docker Engine runs in Swarm mode, manager nodes implement the Raft Consensus Algorithm to manage the global cluster state.\n\nThe reason why Swarm mode is using a consensus algorithm is to make sure that all the manager nodes that are in charge of managing and scheduling tasks in the cluster are storing the same consistent state.\n\nHaving the same consistent state across the cluster means that in case of a failure, any Manager node can pick up the tasks and restore the services to a stable state. For example, if the Leader Manager which is responsible for scheduling tasks in the cluster dies unexpectedly, any other Manager can pick up the task of scheduling and re-balance tasks to match the desired state.\n\nSystems using consensus algorithms to replicate logs in a distributed systems do require special care. They ensure that the cluster state stays consistent in the presence of failures by requiring a majority of nodes to agree on values.\n\nRaft tolerates up to (N-1)/2 failures and requires a majority or quorum of (N/2)+1 members to agree on values proposed to the cluster. This means that in a cluster of 5 Managers running Raft, if 3 nodes are unavailable, the system cannot process any more requests to schedule additional tasks. The existing tasks keep running but the scheduler cannot rebalance tasks to cope with failures if the manager set is not healthy.\n\nThe implementation of the consensus algorithm in Swarm mode means it features the properties inherent to distributed systems:\n\nAgreement on values in a fault tolerant system. (Refer to FLP impossibility theorem and the Raft Consensus Algorithm paper)\nMutual exclusion through the leader election process\nCluster membership management\nGlobally consistent object sequencing and CAS (compare-and-swap) primitives\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995533,
    "timestamp": "2026-02-07T06:33:19.877Z",
    "title": "Run Docker Engine in swarm mode | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/swarm-mode/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nRun Docker Engine in swarm mode\nRun Docker Engine in swarm mode\nCopy as Markdown\n\nWhen you first install and start working with Docker Engine, Swarm mode is disabled by default. When you enable Swarm mode, you work with the concept of services managed through the docker service command.\n\nThere are two ways to run the engine in Swarm mode:\n\nCreate a new swarm, covered in this article.\nJoin an existing swarm.\n\nWhen you run the engine in Swarm mode on your local machine, you can create and test services based upon images you've created or other available images. In your production environment, Swarm mode provides a fault-tolerant platform with cluster management features to keep your services running and available.\n\nThese instructions assume you have installed the Docker Engine on a machine to serve as a manager node in your swarm.\n\nIf you haven't already, read through the Swarm mode key concepts and try the Swarm mode tutorial.\n\nCreate a swarm\n\nWhen you run the command to create a swarm, Docker Engine starts running in Swarm mode.\n\nRun docker swarm init to create a single-node swarm on the current node. The engine sets up the swarm as follows:\n\nSwitches the current node into Swarm mode.\nCreates a swarm named default.\nDesignates the current node as a leader manager node for the swarm.\nNames the node with the machine hostname.\nConfigures the manager to listen on an active network interface on port 2377.\nSets the current node to Active availability, meaning it can receive tasks from the scheduler.\nStarts an internal distributed data store for Engines participating in the swarm to maintain a consistent view of the swarm and all services running on it.\nBy default, generates a self-signed root CA for the swarm.\nBy default, generates tokens for worker and manager nodes to join the swarm.\nCreates an overlay network named ingress for publishing service ports external to the swarm.\nCreates an overlay default IP addresses and subnet mask for your networks\n\nThe output for docker swarm init provides the connection command to use when you join new worker nodes to the swarm:\n\n$ docker swarm init\n\nSwarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.\n\n\n\nTo add a worker to this swarm, run the following command:\n\n\n\n    docker swarm join \\\n\n    --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\\n\n    192.168.99.100:2377\n\n\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n\nConfiguring default address pools\n\nBy default Swarm mode uses a default address pool 10.0.0.0/8 for global scope (overlay) networks. Every network that does not have a subnet specified will have a subnet sequentially allocated from this pool. In some circumstances it may be desirable to use a different default IP address pool for networks.\n\nFor example, if the default 10.0.0.0/8 range conflicts with already allocated address space in your network, then it is desirable to ensure that networks use a different range without requiring swarm users to specify each subnet with the --subnet command.\n\nTo configure custom default address pools, you must define pools at swarm initialization using the --default-addr-pool command line option. This command line option uses CIDR notation for defining the subnet mask. To create the custom address pool for Swarm, you must define at least one default address pool, and an optional default address pool subnet mask. For example, for the 10.0.0.0/27, use the value 27.\n\nDocker allocates subnet addresses from the address ranges specified by the --default-addr-pool option. For example, a command line option --default-addr-pool 10.10.0.0/16 indicates that Docker will allocate subnets from that /16 address range. If --default-addr-pool-mask-len were unspecified or set explicitly to 24, this would result in 256 /24 networks of the form 10.10.X.0/24.\n\nThe subnet range comes from the --default-addr-pool, (such as 10.10.0.0/16). The size of 16 there represents the number of networks one can create within that default-addr-pool range. The --default-addr-pool option may occur multiple times with each option providing additional addresses for docker to use for overlay subnets.\n\nThe format of the command is:\n\n$ docker swarm init --default-addr-pool <IP range in CIDR> [--default-addr-pool <IP range in CIDR> --default-addr-pool-mask-length <CIDR value>]\n\n\nThe command to create a default IP address pool with a /16 (class B) for the 10.20.0.0 network looks like this:\n\n$ docker swarm init --default-addr-pool 10.20.0.0/16\n\n\nThe command to create a default IP address pool with a /16 (class B) for the 10.20.0.0 and 10.30.0.0 networks, and to create a subnet mask of /26 for each network looks like this:\n\n$ docker swarm init --default-addr-pool 10.20.0.0/16 --default-addr-pool 10.30.0.0/16 --default-addr-pool-mask-length 26\n\n\nIn this example, docker network create -d overlay net1 will result in 10.20.0.0/26 as the allocated subnet for net1, and docker network create -d overlay net2 will result in 10.20.0.64/26 as the allocated subnet for net2. This continues until all the subnets are exhausted.\n\nRefer to the following pages for more information:\n\nSwarm networking for more information about the default address pool usage\ndocker swarm init CLI reference for more detail on the --default-addr-pool flag.\nConfigure the advertise address\n\nManager nodes use an advertise address to allow other nodes in the swarm access to the Swarmkit API and overlay networking. The other nodes on the swarm must be able to access the manager node on its advertise address.\n\nIf you don't specify an advertise address, Docker checks if the system has a single IP address. If so, Docker uses the IP address with the listening port 2377 by default. If the system has multiple IP addresses, you must specify the correct --advertise-addr to enable inter-manager communication and overlay networking:\n\n$ docker swarm init --advertise-addr <MANAGER-IP>\n\n\nYou must also specify the --advertise-addr if the address where other nodes reach the first manager node is not the same address the manager sees as its own. For instance, in a cloud setup that spans different regions, hosts have both internal addresses for access within the region and external addresses that you use for access from outside that region. In this case, specify the external address with --advertise-addr so that the node can propagate that information to other nodes that subsequently connect to it.\n\nRefer to the docker swarm init CLI reference for more detail on the advertise address.\n\nView the join command or update a swarm join token\n\nNodes require a secret token to join the swarm. The token for worker nodes is different from the token for manager nodes. Nodes only use the join-token at the moment they join the swarm. Rotating the join token after a node has already joined a swarm does not affect the node's swarm membership. Token rotation ensures an old token cannot be used by any new nodes attempting to join the swarm.\n\nTo retrieve the join command including the join token for worker nodes, run:\n\n$ docker swarm join-token worker\n\n\n\nTo add a worker to this swarm, run the following command:\n\n\n\n    docker swarm join \\\n\n    --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\\n\n    192.168.99.100:2377\n\n\n\nThis node joined a swarm as a worker.\n\n\nTo view the join command and token for manager nodes, run:\n\n$ docker swarm join-token manager\n\n\n\nTo add a manager to this swarm, run the following command:\n\n\n\n    docker swarm join \\\n\n    --token SWMTKN-1-59egwe8qangbzbqb3ryawxzk3jn97ifahlsrw01yar60pmkr90-bdjfnkcflhooyafetgjod97sz \\\n\n    192.168.99.100:2377\n\n\nPass the --quiet flag to print only the token:\n\n$ docker swarm join-token --quiet worker\n\n\n\nSWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c\n\n\nBe careful with the join tokens because they are the secrets necessary to join the swarm. In particular, checking a secret into version control is a bad practice because it would allow anyone with access to the application source code to add new nodes to the swarm. Manager tokens are especially sensitive because they allow a new manager node to join and gain control over the whole swarm.\n\nWe recommend that you rotate the join tokens in the following circumstances:\n\nIf a token was checked-in by accident into a version control system, group chat or accidentally printed to your logs.\nIf you suspect a node has been compromised.\nIf you wish to guarantee that no new nodes can join the swarm.\n\nAdditionally, it is a best practice to implement a regular rotation schedule for any secret including swarm join tokens. We recommend that you rotate your tokens at least every 6 months.\n\nRun swarm join-token --rotate to invalidate the old token and generate a new token. Specify whether you want to rotate the token for worker or manager nodes:\n\n$ docker swarm join-token  --rotate worker\n\n\n\nTo add a worker to this swarm, run the following command:\n\n\n\n    docker swarm join \\\n\n    --token SWMTKN-1-2kscvs0zuymrsc9t0ocyy1rdns9dhaodvpl639j2bqx55uptag-ebmn5u927reawo27s3azntd44 \\\n\n    192.168.99.100:2377\n\nLearn more\nJoin nodes to a swarm\nswarm init command line reference\nSwarm mode tutorial\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCreate a swarm\nConfiguring default address pools\nConfigure the advertise address\nView the join command or update a swarm join token\nLearn more\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995536,
    "timestamp": "2026-02-07T06:33:19.879Z",
    "title": "Store configuration data using Docker Configs | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/configs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nStore configuration data using Docker Configs\nStore configuration data using Docker Configs\nCopy as Markdown\nAbout configs\n\nDocker swarm service configs allow you to store non-sensitive information, such as configuration files, outside a service's image or running containers. This allows you to keep your images as generic as possible, without the need to bind-mount configuration files into the containers or use environment variables.\n\nConfigs operate in a similar way to secrets, except that they are not encrypted at rest and are mounted directly into the container's filesystem without the use of RAM disks. Configs can be added or removed from a service at any time, and services can share a config. You can even use configs in conjunction with environment variables or labels, for maximum flexibility. Config values can be generic strings or binary content (up to 500 kb in size).\n\nNote\n\nDocker configs are only available to swarm services, not to standalone containers. To use this feature, consider adapting your container to run as a service with a scale of 1.\n\nConfigs are supported on both Linux and Windows services.\n\nWindows support\n\nDocker includes support for configs on Windows containers, but there are differences in the implementations, which are called out in the examples below. Keep the following notable differences in mind:\n\nConfig files with custom targets are not directly bind-mounted into Windows containers, since Windows does not support non-directory file bind-mounts. Instead, configs for a container are all mounted in C:\\ProgramData\\Docker\\internal\\configs (an implementation detail which should not be relied upon by applications) within the container. Symbolic links are used to point from there to the desired target of the config within the container. The default target is C:\\ProgramData\\Docker\\configs.\n\nWhen creating a service which uses Windows containers, the options to specify UID, GID, and mode are not supported for configs. Configs are currently only accessible by administrators and users with system access within the container.\n\nOn Windows, create or update a service using --credential-spec with the config://<config-name> format. This passes the gMSA credentials file directly to nodes before a container starts. No gMSA credentials are written to disk on worker nodes. For more information, refer to Deploy services to a swarm.\n\nHow Docker manages configs\n\nWhen you add a config to the swarm, Docker sends the config to the swarm manager over a mutual TLS connection. The config is stored in the Raft log, which is encrypted. The entire Raft log is replicated across the other managers, ensuring the same high availability guarantees for configs as for the rest of the swarm management data.\n\nWhen you grant a newly-created or running service access to a config, the config is mounted as a file in the container. The location of the mount point within the container defaults to /<config-name> in Linux containers. In Windows containers, configs are all mounted into C:\\ProgramData\\Docker\\configs and symbolic links are created to the desired location, which defaults to C:\\<config-name>.\n\nYou can set the ownership (uid and gid) for the config, using either the numerical ID or the name of the user or group. You can also specify the file permissions (mode). These settings are ignored for Windows containers.\n\nIf not set, the config is owned by the user running the container command (often root) and that user's default group (also often root).\nIf not set, the config has world-readable permissions (mode 0444), unless a umask is set within the container, in which case the mode is impacted by that umask value.\n\nYou can update a service to grant it access to additional configs or revoke its access to a given config at any time.\n\nA node only has access to configs if the node is a swarm manager or if it is running service tasks which have been granted access to the config. When a container task stops running, the configs shared to it are unmounted from the in-memory filesystem for that container and flushed from the node's memory.\n\nIf a node loses connectivity to the swarm while it is running a task container with access to a config, the task container still has access to its configs, but cannot receive updates until the node reconnects to the swarm.\n\nYou can add or inspect an individual config at any time, or list all configs. You cannot remove a config that a running service is using. See Rotate a config for a way to remove a config without disrupting running services.\n\nTo update or roll back configs more easily, consider adding a version number or date to the config name. This is made easier by the ability to control the mount point of the config within a given container.\n\nTo update a stack, make changes to your Compose file, then re-run docker stack deploy -c <new-compose-file> <stack-name>. If you use a new config in that file, your services start using them. Keep in mind that configurations are immutable, so you can't change the file for an existing service. Instead, you create a new config to use a different file\n\nYou can run docker stack rm to stop the app and take down the stack. This removes any config that was created by docker stack deploy with the same stack name. This removes all configs, including those not referenced by services and those remaining after a docker service update --config-rm.\n\nRead more about docker config commands\n\nUse these links to read about specific commands, or continue to the example about using configs with a service.\n\ndocker config create\ndocker config inspect\ndocker config ls\ndocker config rm\nExamples\n\nThis section includes graduated examples which illustrate how to use Docker configs.\n\nNote\n\nThese examples use a single-engine swarm and unscaled services for simplicity. The examples use Linux containers, but Windows containers also support configs.\n\nDefining and using configs in compose files\n\nThe docker stack command supports defining configs in a Compose file. However, the configs key is not supported for docker compose. See the Compose file reference for details.\n\nSimple example: Get started with configs\n\nThis simple example shows how configs work in just a few commands. For a real-world example, continue to Advanced example: Use configs with a Nginx service.\n\nAdd a config to Docker. The docker config create command reads standard input because the last argument, which represents the file to read the config from, is set to -.\n\n$ echo \"This is a config\" | docker config create my-config -\n\n\nCreate a redis service and grant it access to the config. By default, the container can access the config at /my-config, but you can customize the file name on the container using the target option.\n\n$ docker service create --name redis --config my-config redis:alpine\n\n\nVerify that the task is running without issues using docker service ps. If everything is working, the output looks similar to this:\n\n$ docker service ps redis\n\n\n\nID            NAME     IMAGE         NODE              DESIRED STATE  CURRENT STATE          ERROR  PORTS\n\nbkna6bpn8r1a  redis.1  redis:alpine  ip-172-31-46-109  Running        Running 8 seconds ago\n\n\nGet the ID of the redis service task container using docker ps, so that you can use docker container exec to connect to the container and read the contents of the config data file, which defaults to being readable by all and has the same name as the name of the config. The first command below illustrates how to find the container ID, and the second and third commands use shell completion to do this automatically.\n\n$ docker ps --filter name=redis -q\n\n\n\n5cb1c2348a59\n\n\n\n$ docker container exec $(docker ps --filter name=redis -q) ls -l /my-config\n\n\n\n-r--r--r--    1 root     root            12 Jun  5 20:49 my-config\n\n\n\n$ docker container exec $(docker ps --filter name=redis -q) cat /my-config\n\n\n\nThis is a config\n\n\nTry removing the config. The removal fails because the redis service is running and has access to the config.\n\n\n\n$ docker config ls\n\n\n\nID                          NAME                CREATED             UPDATED\n\nfzwcfuqjkvo5foqu7ts7ls578   hello               31 minutes ago      31 minutes ago\n\n\n\n\n\n$ docker config rm my-config\n\n\n\nError response from daemon: rpc error: code = 3 desc = config 'my-config' is\n\nin use by the following service: redis\n\n\nRemove access to the config from the running redis service by updating the service.\n\n$ docker service update --config-rm my-config redis\n\n\nRepeat steps 3 and 4 again, verifying that the service no longer has access to the config. The container ID is different, because the service update command redeploys the service.\n\n$ docker container exec -it $(docker ps --filter name=redis -q) cat /my-config\n\n\n\ncat: can't open '/my-config': No such file or directory\n\n\nStop and remove the service, and remove the config from Docker.\n\n$ docker service rm redis\n\n\n\n$ docker config rm my-config\n\nSimple example: Use configs in a Windows service\n\nThis is a very simple example which shows how to use configs with a Microsoft IIS service running on Docker for Windows running Windows containers on Microsoft Windows 10. It is a naive example that stores the webpage in a config.\n\nThis example assumes that you have PowerShell installed.\n\nSave the following into a new file index.html.\n\n<html lang=\"en\">\n\n  <head><title>Hello Docker</title></head>\n\n  <body>\n\n    <p>Hello Docker! You have deployed a HTML page.</p>\n\n  </body>\n\n</html>\n\nIf you have not already done so, initialize or join the swarm.\n\ndocker swarm init\n\nSave the index.html file as a swarm config named homepage.\n\ndocker config create homepage index.html\n\nCreate an IIS service and grant it access to the homepage config.\n\ndocker service create\n\n    --name my-iis\n\n    --publish published=8000,target=8000\n\n    --config src=homepage,target=\"\\inetpub\\wwwroot\\index.html\"\n\n    microsoft/iis:nanoserver\n\nAccess the IIS service at http://localhost:8000/. It should serve the HTML content from the first step.\n\nRemove the service and the config.\n\ndocker service rm my-iis\n\n\n\ndocker config rm homepage\nExample: Use a templated config\n\nTo create a configuration in which the content will be generated using a template engine, use the --template-driver parameter and specify the engine name as its argument. The template will be rendered when container is created.\n\nSave the following into a new file index.html.tmpl.\n\n<html lang=\"en\">\n\n  <head><title>Hello Docker</title></head>\n\n  <body>\n\n    <p>Hello {{ env \"HELLO\" }}! I'm service {{ .Service.Name }}.</p>\n\n  </body>\n\n</html>\n\nSave the index.html.tmpl file as a swarm config named homepage. Provide parameter --template-driver and specify golang as template engine.\n\n$ docker config create --template-driver golang homepage index.html.tmpl\n\n\nCreate a service that runs Nginx and has access to the environment variable HELLO and to the config.\n\n$ docker service create \\\n\n     --name hello-template \\\n\n     --env HELLO=\"Docker\" \\\n\n     --config source=homepage,target=/usr/share/nginx/html/index.html \\\n\n     --publish published=3000,target=80 \\\n\n     nginx:alpine\n\n\nVerify that the service is operational: you can reach the Nginx server, and that the correct output is being served.\n\n$ curl http://0.0.0.0:3000\n\n\n\n<html lang=\"en\">\n\n  <head><title>Hello Docker</title></head>\n\n  <body>\n\n    <p>Hello Docker! I'm service hello-template.</p>\n\n  </body>\n\n</html>\n\nAdvanced example: Use configs with a Nginx service\n\nThis example is divided into two parts. The first part is all about generating the site certificate and does not directly involve Docker configs at all, but it sets up the second part, where you store and use the site certificate as a series of secrets and the Nginx configuration as a config. The example shows how to set options on the config, such as the target location within the container and the file permissions (mode).\n\nGenerate the site certificate\n\nGenerate a root CA and TLS certificate and key for your site. For production sites, you may want to use a service such as Let‚Äôs Encrypt to generate the TLS certificate and key, but this example uses command-line tools. This step is a little complicated, but is only a set-up step so that you have something to store as a Docker secret. If you want to skip these sub-steps, you can use Let's Encrypt to generate the site key and certificate, name the files site.key and site.crt, and skip to Configure the Nginx container.\n\nGenerate a root key.\n\n$ openssl genrsa -out \"root-ca.key\" 4096\n\n\nGenerate a CSR using the root key.\n\n$ openssl req \\\n\n          -new -key \"root-ca.key\" \\\n\n          -out \"root-ca.csr\" -sha256 \\\n\n          -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA'\n\n\nConfigure the root CA. Edit a new file called root-ca.cnf and paste the following contents into it. This constrains the root CA to only sign leaf certificates and not intermediate CAs.\n\n[root_ca]\n\nbasicConstraints = critical,CA:TRUE,pathlen:1\n\nkeyUsage = critical, nonRepudiation, cRLSign, keyCertSign\n\nsubjectKeyIdentifier=hash\n\nSign the certificate.\n\n$ openssl x509 -req -days 3650 -in \"root-ca.csr\" \\\n\n               -signkey \"root-ca.key\" -sha256 -out \"root-ca.crt\" \\\n\n               -extfile \"root-ca.cnf\" -extensions \\\n\n               root_ca\n\n\nGenerate the site key.\n\n$ openssl genrsa -out \"site.key\" 4096\n\n\nGenerate the site certificate and sign it with the site key.\n\n$ openssl req -new -key \"site.key\" -out \"site.csr\" -sha256 \\\n\n          -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost'\n\n\nConfigure the site certificate. Edit a new file called site.cnf and paste the following contents into it. This constrains the site certificate so that it can only be used to authenticate a server and can't be used to sign certificates.\n\n[server]\n\nauthorityKeyIdentifier=keyid,issuer\n\nbasicConstraints = critical,CA:FALSE\n\nextendedKeyUsage=serverAuth\n\nkeyUsage = critical, digitalSignature, keyEncipherment\n\nsubjectAltName = DNS:localhost, IP:127.0.0.1\n\nsubjectKeyIdentifier=hash\n\nSign the site certificate.\n\n$ openssl x509 -req -days 750 -in \"site.csr\" -sha256 \\\n\n    -CA \"root-ca.crt\" -CAkey \"root-ca.key\" -CAcreateserial \\\n\n    -out \"site.crt\" -extfile \"site.cnf\" -extensions server\n\n\nThe site.csr and site.cnf files are not needed by the Nginx service, but you need them if you want to generate a new site certificate. Protect the root-ca.key file.\n\nConfigure the Nginx container\n\nProduce a very basic Nginx configuration that serves static files over HTTPS. The TLS certificate and key are stored as Docker secrets so that they can be rotated easily.\n\nIn the current directory, create a new file called site.conf with the following contents:\n\nserver {\n\n    listen                443 ssl;\n\n    server_name           localhost;\n\n    ssl_certificate       /run/secrets/site.crt;\n\n    ssl_certificate_key   /run/secrets/site.key;\n\n\n\n    location / {\n\n        root   /usr/share/nginx/html;\n\n        index  index.html index.htm;\n\n    }\n\n}\n\nCreate two secrets, representing the key and the certificate. You can store any file as a secret as long as it is smaller than 500 KB. This allows you to decouple the key and certificate from the services that use them. In these examples, the secret name and the file name are the same.\n\n$ docker secret create site.key site.key\n\n\n\n$ docker secret create site.crt site.crt\n\n\nSave the site.conf file in a Docker config. The first parameter is the name of the config, and the second parameter is the file to read it from.\n\n$ docker config create site.conf site.conf\n\n\nList the configs:\n\n$ docker config ls\n\n\n\nID                          NAME                CREATED             UPDATED\n\n4ory233120ccg7biwvy11gl5z   site.conf           4 seconds ago       4 seconds ago\n\n\nCreate a service that runs Nginx and has access to the two secrets and the config. Set the mode to 0440 so that the file is only readable by its owner and that owner's group, not the world.\n\n$ docker service create \\\n\n     --name nginx \\\n\n     --secret site.key \\\n\n     --secret site.crt \\\n\n     --config source=site.conf,target=/etc/nginx/conf.d/site.conf,mode=0440 \\\n\n     --publish published=3000,target=443 \\\n\n     nginx:latest \\\n\n     sh -c \"exec nginx -g 'daemon off;'\"\n\n\nWithin the running containers, the following three files now exist:\n\n/run/secrets/site.key\n/run/secrets/site.crt\n/etc/nginx/conf.d/site.conf\n\nVerify that the Nginx service is running.\n\n$ docker service ls\n\n\n\nID            NAME   MODE        REPLICAS  IMAGE\n\nzeskcec62q24  nginx  replicated  1/1       nginx:latest\n\n\n\n$ docker service ps nginx\n\n\n\nNAME                  IMAGE         NODE  DESIRED STATE  CURRENT STATE          ERROR  PORTS\n\nnginx.1.9ls3yo9ugcls  nginx:latest  moby  Running        Running 3 minutes ago\n\n\nVerify that the service is operational: you can reach the Nginx server, and that the correct TLS certificate is being used.\n\n$ curl --cacert root-ca.crt https://0.0.0.0:3000\n\n\n\n<!DOCTYPE html>\n\n<html>\n\n<head>\n\n<title>Welcome to nginx!</title>\n\n<style>\n\n    body {\n\n        width: 35em;\n\n        margin: 0 auto;\n\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n\n    }\n\n</style>\n\n</head>\n\n<body>\n\n<h1>Welcome to nginx!</h1>\n\n<p>If you see this page, the nginx web server is successfully installed and\n\nworking. Further configuration is required.</p>\n\n\n\n<p>For online documentation and support, refer to\n\n<a href=\"https://nginx.org\">nginx.org</a>.<br/>\n\nCommercial support is available at\n\n<a href=\"https://www.nginx.com\">www.nginx.com</a>.</p>\n\n\n\n<p><em>Thank you for using nginx.</em></p>\n\n</body>\n\n</html>\n\n$ openssl s_client -connect 0.0.0.0:3000 -CAfile root-ca.crt\n\n\n\nCONNECTED(00000003)\n\ndepth=1 /C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA\n\nverify return:1\n\ndepth=0 /C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost\n\nverify return:1\n\n---\n\nCertificate chain\n\n 0 s:/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost\n\n   i:/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA\n\n---\n\nServer certificate\n\n-----BEGIN CERTIFICATE-----\n\n‚Ä¶\n\n-----END CERTIFICATE-----\n\nsubject=/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost\n\nissuer=/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA\n\n---\n\nNo client certificate CA names sent\n\n---\n\nSSL handshake has read 1663 bytes and written 712 bytes\n\n---\n\nNew, TLSv1/SSLv3, Cipher is AES256-SHA\n\nServer public key is 4096 bit\n\nSecure Renegotiation IS supported\n\nCompression: NONE\n\nExpansion: NONE\n\nSSL-Session:\n\n    Protocol  : TLSv1\n\n    Cipher    : AES256-SHA\n\n    Session-ID: A1A8BF35549C5715648A12FD7B7E3D861539316B03440187D9DA6C2E48822853\n\n    Session-ID-ctx:\n\n    Master-Key: F39D1B12274BA16D3A906F390A61438221E381952E9E1E05D3DD784F0135FB81353DA38C6D5C021CB926E844DFC49FC4\n\n    Key-Arg   : None\n\n    Start Time: 1481685096\n\n    Timeout   : 300 (sec)\n\n    Verify return code: 0 (ok)\n\n\nUnless you are going to continue to the next example, clean up after running this example by removing the nginx service and the stored secrets and config.\n\n$ docker service rm nginx\n\n\n\n$ docker secret rm site.crt site.key\n\n\n\n$ docker config rm site.conf\n\n\nYou have now configured a Nginx service with its configuration decoupled from its image. You could run multiple sites with exactly the same image but separate configurations, without the need to build a custom image at all.\n\nExample: Rotate a config\n\nTo rotate a config, you first save a new config with a different name than the one that is currently in use. You then redeploy the service, removing the old config and adding the new config at the same mount point within the container. This example builds upon the previous one by rotating the site.conf configuration file.\n\nEdit the site.conf file locally. Add index.php to the index line, and save the file.\n\nserver {\n\n    listen                443 ssl;\n\n    server_name           localhost;\n\n    ssl_certificate       /run/secrets/site.crt;\n\n    ssl_certificate_key   /run/secrets/site.key;\n\n\n\n    location / {\n\n        root   /usr/share/nginx/html;\n\n        index  index.html index.htm index.php;\n\n    }\n\n}\n\nCreate a new Docker config using the new site.conf, called site-v2.conf.\n\n$ docker config create site-v2.conf site.conf\n\nUpdate the nginx service to use the new config instead of the old one.\n\n$ docker service update \\\n\n  --config-rm site.conf \\\n\n  --config-add source=site-v2.conf,target=/etc/nginx/conf.d/site.conf,mode=0440 \\\n\n  nginx\n\n\nVerify that the nginx service is fully re-deployed, using docker service ps nginx. When it is, you can remove the old site.conf config.\n\n$ docker config rm site.conf\n\n\nTo clean up, you can remove the nginx service, as well as the secrets and configs.\n\n$ docker service rm nginx\n\n\n\n$ docker secret rm site.crt site.key\n\n\n\n$ docker config rm site-v2.conf\n\n\nYou have now updated your nginx service's configuration without the need to rebuild its image.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nAbout configs\nWindows support\nHow Docker manages configs\nRead more about docker config commands\nExamples\nDefining and using configs in compose files\nSimple example: Get started with configs\nSimple example: Use configs in a Windows service\nExample: Use a templated config\nAdvanced example: Use configs with a Nginx service\nExample: Rotate a config\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995539,
    "timestamp": "2026-02-07T06:33:19.880Z",
    "title": "Swarm mode key concepts | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/key-concepts/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nSwarm mode key concepts\nSwarm mode key concepts\nCopy as Markdown\n\nThis topic introduces some of the concepts unique to the cluster management and orchestration features of Docker Engine 1.12.\n\nWhat is a swarm?\n\nThe cluster management and orchestration features embedded in Docker Engine are built using swarmkit. Swarmkit is a separate project which implements Docker's orchestration layer and is used directly within Docker.\n\nA swarm consists of multiple Docker hosts which run in Swarm mode and act as managers, to manage membership and delegation, and workers, which run swarm services. A given Docker host can be a manager, a worker, or perform both roles. When you create a service, you define its optimal state - number of replicas, network and storage resources available to it, ports the service exposes to the outside world, and more. Docker works to maintain that desired state. For instance, if a worker node becomes unavailable, Docker schedules that node's tasks on other nodes. A task is a running container which is part of a swarm service and is managed by a swarm manager, as opposed to a standalone container.\n\nOne of the key advantages of swarm services over standalone containers is that you can modify a service's configuration, including the networks and volumes it is connected to, without the need to manually restart the service. Docker will update the configuration, stop the service tasks with out of date configuration, and create new ones matching the desired configuration.\n\nWhen Docker is running in Swarm mode, you can still run standalone containers on any of the Docker hosts participating in the swarm, as well as swarm services. A key difference between standalone containers and swarm services is that only swarm managers can manage a swarm, while standalone containers can be started on any daemon. Docker daemons can participate in a swarm as managers, workers, or both.\n\nIn the same way that you can use Docker Compose to define and run containers, you can define and run Swarm service stacks.\n\nKeep reading for details about concepts related to Docker swarm services, including nodes, services, tasks, and load balancing.\n\nNodes\n\nA node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.\n\nTo deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.\n\nManager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes select a single leader to conduct orchestration tasks.\n\nWorker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.\n\nServices and tasks\n\nA service is the definition of the tasks to execute on the manager or worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.\n\nWhen you create a service, you specify which container image to use and which commands to execute inside running containers.\n\nIn the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.\n\nFor global services, the swarm runs one task for the service on every available node in the cluster.\n\nA task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.\n\nLoad balancing\n\nThe swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a published port or you can configure a published port for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.\n\nExternal components, such as cloud load balancers, can access the service on the published port of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.\n\nSwarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.\n\nWhat's next?\nRead the Swarm mode overview.\nGet started with the Swarm mode tutorial.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat is a swarm?\nNodes\nServices and tasks\nLoad balancing\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995542,
    "timestamp": "2026-02-07T06:33:19.885Z",
    "title": "Use Swarm mode routing mesh | Docker Docs",
    "url": "https://docs.docker.com/engine/swarm/ingress/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nAdminister and maintain a swarm of Docker Engines\nDeploy a stack to a swarm\nDeploy services to a swarm\nGetting started with Swarm mode\nHow swarm works\nJoin nodes to a swarm\nLock your swarm to protect its encryption key\nManage nodes in a swarm\nManage sensitive data with Docker secrets\nManage swarm service networks\nRaft consensus in swarm mode\nRun Docker Engine in swarm mode\nStore configuration data using Docker Configs\nSwarm mode key concepts\nUse Swarm mode routing mesh\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nSwarm mode\n/\nUse Swarm mode routing mesh\nUse Swarm mode routing mesh\nCopy as Markdown\n\nDocker Engine Swarm mode makes it easy to publish ports for services to make them available to resources outside the swarm. All nodes participate in an ingress routing mesh. The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there's no task running on the node. The routing mesh routes all incoming requests to published ports on available nodes to an active container.\n\nTo use the ingress network in the swarm, you need to have the following ports open between the swarm nodes before you enable Swarm mode:\n\nPort 7946 TCP/UDP for container network discovery.\nPort 4789 UDP (configurable) for the container ingress network.\n\nWhen setting up networking in a Swarm, special care should be taken. Consult the tutorial for an overview.\n\nYou must also open the published port between the swarm nodes and any external resources, such as an external load balancer, that require access to the port.\n\nYou can also bypass the routing mesh for a given service.\n\nPublish a port for a service\n\nUse the --publish flag to publish a port when you create a service. target is used to specify the port inside the container, and published is used to specify the port to bind on the routing mesh. If you leave off the published port, a random high-numbered port is bound for each service task. You need to inspect the task to determine the port.\n\n$ docker service create \\\n\n  --name <SERVICE-NAME> \\\n\n  --publish published=<PUBLISHED-PORT>,target=<CONTAINER-PORT> \\\n\n  IMAGE\n\nNote\n\nThe older form of this syntax is a colon-separated string, where the published port is first and the target port is second, such as -p 8080:80. The new syntax is preferred because it is easier to read and allows more flexibility.\n\nThe <PUBLISHED-PORT> is the port where the swarm makes the service available. If you omit it, a random high-numbered port is bound. The <CONTAINER-PORT> is the port where the container listens. This parameter is required.\n\nFor example, the following command publishes port 80 in the nginx container to port 8080 for any node in the swarm:\n\n$ docker service create \\\n\n  --name my-web \\\n\n  --publish published=8080,target=80 \\\n\n  --replicas 2 \\\n\n  nginx\n\n\nWhen you access port 8080 on any node, Docker routes your request to an active container. On the swarm nodes themselves, port 8080 may not actually be bound, but the routing mesh knows how to route the traffic and prevents any port conflicts from happening.\n\nThe routing mesh listens on the published port for any IP address assigned to the node. For externally routable IP addresses, the port is available from outside the host. For all other IP addresses the access is only available from within the host.\n\nYou can publish a port for an existing service using the following command:\n\n$ docker service update \\\n\n  --publish-add published=<PUBLISHED-PORT>,target=<CONTAINER-PORT> \\\n\n  SERVICE\n\n\nYou can use docker service inspect to view the service's published port. For instance:\n\n$ docker service inspect --format=\"{{json .Endpoint.Spec.Ports}}\" my-web\n\n\n\n[{\"Protocol\":\"tcp\",\"TargetPort\":80,\"PublishedPort\":8080}]\n\n\nThe output shows the <CONTAINER-PORT> (labeled TargetPort) from the containers and the <PUBLISHED-PORT> (labeled PublishedPort) where nodes listen for requests for the service.\n\nPublish a port for TCP only or UDP only\n\nBy default, when you publish a port, it is a TCP port. You can specifically publish a UDP port instead of or in addition to a TCP port. When you publish both TCP and UDP ports, if you omit the protocol specifier, the port is published as a TCP port. If you use the longer syntax (recommended), set the protocol key to either tcp or udp.\n\nTCP only\n\nLong syntax:\n\n$ docker service create --name dns-cache \\\n\n  --publish published=53,target=53 \\\n\n  dns-cache\n\n\nShort syntax:\n\n$ docker service create --name dns-cache \\\n\n  -p 53:53 \\\n\n  dns-cache\n\nTCP and UDP\n\nLong syntax:\n\n$ docker service create --name dns-cache \\\n\n  --publish published=53,target=53 \\\n\n  --publish published=53,target=53,protocol=udp \\\n\n  dns-cache\n\n\nShort syntax:\n\n$ docker service create --name dns-cache \\\n\n  -p 53:53 \\\n\n  -p 53:53/udp \\\n\n  dns-cache\n\nUDP only\n\nLong syntax:\n\n$ docker service create --name dns-cache \\\n\n  --publish published=53,target=53,protocol=udp \\\n\n  dns-cache\n\n\nShort syntax:\n\n$ docker service create --name dns-cache \\\n\n  -p 53:53/udp \\\n\n  dns-cache\n\nBypass the routing mesh\n\nBy default, swarm services which publish ports do so using the routing mesh. When you connect to a published port on any swarm node (whether it is running a given service or not), you are redirected to a worker which is running that service, transparently. Effectively, Docker acts as a load balancer for your swarm services.\n\nYou can bypass the routing mesh, so that when you access the bound port on a given node, you are always accessing the instance of the service running on that node. This is referred to as host mode. There are a few things to keep in mind.\n\nIf you access a node which is not running a service task, the service does not listen on that port. It is possible that nothing is listening, or that a completely different application is listening.\n\nIf you expect to run multiple service tasks on each node (such as when you have 5 nodes but run 10 replicas), you cannot specify a static target port. Either allow Docker to assign a random high-numbered port (by leaving off the published), or ensure that only a single instance of the service runs on a given node, by using a global service rather than a replicated one, or by using placement constraints.\n\nTo bypass the routing mesh, you must use the long --publish service and set mode to host. If you omit the mode key or set it to ingress, the routing mesh is used. The following command creates a global service using host mode and bypassing the routing mesh.\n\n$ docker service create --name dns-cache \\\n\n  --publish published=53,target=53,protocol=udp,mode=host \\\n\n  --mode global \\\n\n  dns-cache\n\nConfigure an external load balancer\n\nYou can configure an external load balancer for swarm services, either in combination with the routing mesh or without using the routing mesh at all.\n\nUsing the routing mesh\n\nYou can configure an external load balancer to route requests to a swarm service. For example, you could configure HAProxy to balance requests to an nginx service published to port 8080.\n\nIn this case, port 8080 must be open between the load balancer and the nodes in the swarm. The swarm nodes can reside on a private network that is accessible to the proxy server, but that is not publicly accessible.\n\nYou can configure the load balancer to balance requests between every node in the swarm even if there are no tasks scheduled on the node. For example, you could have the following HAProxy configuration in /etc/haproxy/haproxy.cfg:\n\nglobal\n\n        log /dev/log    local0\n\n        log /dev/log    local1 notice\n\n...snip...\n\n\n\n# Configure HAProxy to listen on port 80\n\nfrontend http_front\n\n   bind *:80\n\n   stats uri /haproxy?stats\n\n   default_backend http_back\n\n\n\n# Configure HAProxy to route requests to swarm nodes on port 8080\n\nbackend http_back\n\n   balance roundrobin\n\n   server node1 192.168.99.100:8080 check\n\n   server node2 192.168.99.101:8080 check\n\n   server node3 192.168.99.102:8080 check\n\nWhen you access the HAProxy load balancer on port 80, it forwards requests to nodes in the swarm. The swarm routing mesh routes the request to an active task. If, for any reason the swarm scheduler dispatches tasks to different nodes, you don't need to reconfigure the load balancer.\n\nYou can configure any type of load balancer to route requests to swarm nodes. To learn more about HAProxy, see the HAProxy documentation.\n\nWithout the routing mesh\n\nTo use an external load balancer without the routing mesh, set --endpoint-mode to dnsrr instead of the default value of vip. In this case, there is not a single virtual IP. Instead, Docker sets up DNS entries for the service such that a DNS query for the service name returns a list of IP addresses, and the client connects directly to one of these.\n\nYou can't use --endpoint-mode dnsrr together with --publish mode=ingress. You must run your own load balancer in front of the service. A DNS query for the service name on the Docker host returns a list of IP addresses for the nodes running the service. Configure your load balancer to consume this list and balance the traffic across the nodes. See Configure service discovery.\n\nLearn more\nDeploy services to a swarm\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPublish a port for a service\nPublish a port for TCP only or UDP only\nBypass the routing mesh\nConfigure an external load balancer\nUsing the routing mesh\nWithout the routing mesh\nLearn more\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995548,
    "timestamp": "2026-02-07T06:33:19.887Z",
    "title": "Docker Engine plugins | Docker Docs",
    "url": "https://docs.docker.com/engine/extend/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nAccess authorization plugin\nDocker log driver plugins\nDocker network driver plugins\nDocker Plugin API\nDocker volume plugins\nPlugin Config Version 1 of Plugin V2\nUse Docker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDocker Engine plugins\nDocker Engine managed plugin system\nCopy as Markdown\nInstalling and using a plugin\nDeveloping a plugin\nDebugging plugins\n\nDocker Engine's plugin system lets you install, start, stop, and remove plugins using Docker Engine.\n\nFor information about legacy (non-managed) plugins, refer to Understand legacy Docker Engine plugins.\n\nNote\n\nDocker Engine managed plugins are currently not supported on Windows daemons.\n\nInstalling and using a plugin\n\nPlugins are distributed as Docker images and can be hosted on Docker Hub or on a private registry.\n\nTo install a plugin, use the docker plugin install command, which pulls the plugin from Docker Hub or your private registry, prompts you to grant permissions or capabilities if necessary, and enables the plugin.\n\nTo check the status of installed plugins, use the docker plugin ls command. Plugins that start successfully are listed as enabled in the output.\n\nAfter a plugin is installed, you can use it as an option for another Docker operation, such as creating a volume.\n\nIn the following example, you install the rclone plugin, verify that it is enabled, and use it to create a volume.\n\nNote\n\nThis example is intended for instructional purposes only.\n\nSet up the pre-requisite directories. By default they must exist on the host at the following locations:\n\n/var/lib/docker-plugins/rclone/config. Reserved for the rclone.conf config file and must exist even if it's empty and the config file is not present.\n/var/lib/docker-plugins/rclone/cache. Holds the plugin state file as well as optional VFS caches.\n\nInstall the rclone plugin.\n\n$ docker plugin install rclone/docker-volume-rclone --alias rclone\n\n\n\nPlugin \"rclone/docker-volume-rclone\" is requesting the following privileges:\n\n - network: [host]\n\n - mount: [/var/lib/docker-plugins/rclone/config]\n\n - mount: [/var/lib/docker-plugins/rclone/cache]\n\n - device: [/dev/fuse]\n\n - capabilities: [CAP_SYS_ADMIN]\n\nDo you grant the above permissions? [y/N] \n\n\nThe plugin requests 5 privileges:\n\nIt needs access to the host network.\nAccess to pre-requisite directories to mount to store:\nYour Rclone config files\nTemporary cache data\nGives access to the FUSE (Filesystem in Userspace) device. This is required because Rclone uses FUSE to mount remote storage as if it were a local filesystem.\nIt needs the CAP_SYS_ADMIN capability, which allows the plugin to run the mount command.\n\nCheck that the plugin is enabled in the output of docker plugin ls.\n\n$ docker plugin ls\n\n\n\nID                    NAME                      DESCRIPTION                                ENABLED\n\naede66158353          rclone:latest             Rclone volume plugin for Docker            true\n\n\nCreate a volume using the plugin. This example mounts the /remote directory on host 1.2.3.4 into a volume named rclonevolume.\n\nThis volume can now be mounted into containers.\n\n$ docker volume create \\\n\n  -d rclone \\\n\n  --name rclonevolume \\\n\n  -o type=sftp \\\n\n  -o path=remote \\\n\n  -o sftp-host=1.2.3.4 \\\n\n  -o sftp-user=user \\\n\n  -o \"sftp-password=$(cat file_containing_password_for_remote_host)\"\n\n\nVerify that the volume was created successfully.\n\n$ docker volume ls\n\n\n\nDRIVER              NAME\n\nrclone         rclonevolume\n\n\nStart a container that uses the volume rclonevolume.\n\n$ docker run --rm -v rclonevolume:/data busybox ls /data\n\n\n\n<content of /remote on machine 1.2.3.4>\n\n\nRemove the volume rclonevolume\n\n$ docker volume rm rclonevolume\n\n\n\nsshvolume\n\n\nTo disable a plugin, use the docker plugin disable command. To completely remove it, use the docker plugin remove command. For other available commands and options, see the command line reference.\n\nDeveloping a plugin\nThe rootfs directory\n\nThe rootfs directory represents the root filesystem of the plugin. In this example, it was created from a Dockerfile:\n\nNote\n\nThe /run/docker/plugins directory is mandatory inside of the plugin's filesystem for Docker to communicate with the plugin.\n\n$ git clone https://github.com/vieux/docker-volume-sshfs\n\n$ cd docker-volume-sshfs\n\n$ docker build -t rootfsimage .\n\n$ id=$(docker create rootfsimage true) # id was cd851ce43a403 when the image was created\n\n$ sudo mkdir -p myplugin/rootfs\n\n$ sudo docker export \"$id\" | sudo tar -x -C myplugin/rootfs\n\n$ docker rm -vf \"$id\"\n\n$ docker rmi rootfsimage\n\nThe config.json file\n\nThe config.json file describes the plugin. See the plugins config reference.\n\nConsider the following config.json file.\n\n{\n\n  \"description\": \"sshFS plugin for Docker\",\n\n  \"documentation\": \"https://docs.docker.com/engine/extend/plugins/\",\n\n  \"entrypoint\": [\"/docker-volume-sshfs\"],\n\n  \"network\": {\n\n    \"type\": \"host\"\n\n  },\n\n  \"interface\": {\n\n    \"types\": [\"docker.volumedriver/1.0\"],\n\n    \"socket\": \"sshfs.sock\"\n\n  },\n\n  \"linux\": {\n\n    \"capabilities\": [\"CAP_SYS_ADMIN\"]\n\n  }\n\n}\n\nThis plugin is a volume driver. It requires a host network and the CAP_SYS_ADMIN capability. It depends upon the /docker-volume-sshfs entrypoint and uses the /run/docker/plugins/sshfs.sock socket to communicate with Docker Engine. This plugin has no runtime parameters.\n\nCreating the plugin\n\nA new plugin can be created by running docker plugin create <plugin-name> ./path/to/plugin/data where the plugin data contains a plugin configuration file config.json and a root filesystem in subdirectory rootfs.\n\nAfter that the plugin <plugin-name> will show up in docker plugin ls. Plugins can be pushed to remote registries with docker plugin push <plugin-name>.\n\nDebugging plugins\n\nStdout of a plugin is redirected to dockerd logs. Such entries have a plugin=<ID> suffix. Here are a few examples of commands for pluginID f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62 and their corresponding log entries in the docker daemon logs.\n\n$ docker plugin install tiborvass/sample-volume-plugin\n\n\n\nINFO[0036] Starting...       Found 0 volumes on startup  plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62\n\n$ docker volume create -d tiborvass/sample-volume-plugin samplevol\n\n\n\nINFO[0193] Create Called...  Ensuring directory /data/samplevol exists on host...  plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62\n\nINFO[0193] open /var/lib/docker/plugin-data/local-persist.json: no such file or directory  plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62\n\nINFO[0193]                   Created volume samplevol with mountpoint /data/samplevol  plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62\n\nINFO[0193] Path Called...    Returned path /data/samplevol  plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62\n\n$ docker run -v samplevol:/tmp busybox sh\n\n\n\nINFO[0421] Get Called...     Found samplevol                plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62\n\nINFO[0421] Mount Called...   Mounted samplevol              plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62\n\nINFO[0421] Path Called...    Returned path /data/samplevol  plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62\n\nINFO[0421] Unmount Called... Unmounted samplevol            plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62\n\nUsing runc to obtain logfiles and shell into the plugin.\n\nUse runc, the default docker container runtime, for debugging plugins by collecting plugin logs redirected to a file.\n\n$ sudo runc --root /run/docker/runtime-runc/plugins.moby list\n\n\n\nID                                                                 PID         STATUS      BUNDLE                                                                                                                                       CREATED                          OWNER\n\n93f1e7dbfe11c938782c2993628c895cf28e2274072c4a346a6002446c949b25   15806       running     /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby-plugins/93f1e7dbfe11c938782c2993628c895cf28e2274072c4a346a6002446c949b25   2018-02-08T21:40:08.621358213Z   root\n\n9b4606d84e06b56df84fadf054a21374b247941c94ce405b0a261499d689d9c9   14992       running     /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby-plugins/9b4606d84e06b56df84fadf054a21374b247941c94ce405b0a261499d689d9c9   2018-02-08T21:35:12.321325872Z   root\n\nc5bb4b90941efcaccca999439ed06d6a6affdde7081bb34dc84126b57b3e793d   14984       running     /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby-plugins/c5bb4b90941efcaccca999439ed06d6a6affdde7081bb34dc84126b57b3e793d   2018-02-08T21:35:12.321288966Z   root\n\n$ sudo runc --root /run/docker/runtime-runc/plugins.moby exec 93f1e7dbfe11c938782c2993628c895cf28e2274072c4a346a6002446c949b25 cat /var/log/plugin.log\n\n\nIf the plugin has a built-in shell, then exec into the plugin can be done as follows:\n\n$ sudo runc --root /run/docker/runtime-runc/plugins.moby exec -t 93f1e7dbfe11c938782c2993628c895cf28e2274072c4a346a6002446c949b25 sh\n\nUsing curl to debug plugin socket issues.\n\nTo verify if the plugin API socket that the docker daemon communicates with is responsive, use curl. In this example, we will make API calls from the docker host to volume and network plugins using curl 7.47.0 to ensure that the plugin is listening on the said socket. For a well functioning plugin, these basic requests should work. Note that plugin sockets are available on the host under /var/run/docker/plugins/<pluginID>\n\n$ curl -H \"Content-Type: application/json\" -XPOST -d '{}' --unix-socket /var/run/docker/plugins/e8a37ba56fc879c991f7d7921901723c64df6b42b87e6a0b055771ecf8477a6d/plugin.sock http:/VolumeDriver.List\n\n\n\n{\"Mountpoint\":\"\",\"Err\":\"\",\"Volumes\":[{\"Name\":\"myvol1\",\"Mountpoint\":\"/data/myvol1\"},{\"Name\":\"myvol2\",\"Mountpoint\":\"/data/myvol2\"}],\"Volume\":null}\n\n$ curl -H \"Content-Type: application/json\" -XPOST -d '{}' --unix-socket /var/run/docker/plugins/45e00a7ce6185d6e365904c8bcf62eb724b1fe307e0d4e7ecc9f6c1eb7bcdb70/plugin.sock http:/NetworkDriver.GetCapabilities\n\n\n\n{\"Scope\":\"local\"}\n\n\nWhen using curl 7.5 and above, the URL should be of the form http://hostname/APICall, where hostname is the valid hostname where the plugin is installed and APICall is the call to the plugin API.\n\nFor example, http://localhost/VolumeDriver.List\n\nRequest changes\n\nTable of contents\nInstalling and using a plugin\nDeveloping a plugin\nDebugging plugins\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995545,
    "timestamp": "2026-02-07T06:33:19.888Z",
    "title": "Deprecated features | Docker Docs",
    "url": "https://docs.docker.com/engine/deprecated/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDeprecated features\nDeprecated Docker Engine features\nCopy as Markdown\n\nThis page provides an overview of features that are deprecated in Engine. Changes in packaging, and supported (Linux) distributions are not included. To learn about end of support for Linux distributions, refer to the release notes.\n\nFeature deprecation policy\n\nAs changes are made to Docker there may be times when existing features need to be removed or replaced with newer features. Before an existing feature is removed it is labeled as \"deprecated\" within the documentation and remains in Docker for at least one stable release unless specified explicitly otherwise. After that time it may be removed.\n\nUsers are expected to take note of the list of deprecated features each release and plan their migration away from those features, and (if applicable) towards the replacement features as soon as possible.\n\nDeprecated engine features\n\nThe following table provides an overview of the current status of deprecated features:\n\nDeprecated: the feature is marked \"deprecated\" and should no longer be used.\n\nThe feature may be removed, disabled, or change behavior in a future release. The \"Deprecated\" column contains the release in which the feature was marked deprecated, whereas the \"Remove\" column contains a tentative release in which the feature is to be removed. If no release is included in the \"Remove\" column, the release is yet to be decided on.\n\nRemoved: the feature was removed, disabled, or hidden.\n\nRefer to the linked section for details. Some features are \"soft\" deprecated, which means that they remain functional for backward compatibility, and to allow users to migrate to alternatives. In such cases, a warning may be printed, and users should not rely on this feature.\n\nStatus\tFeature\tDeprecated\tRemove\nDeprecated\tSupport for cgroup v1\tv29.0\t-\nDeprecated\t--pause option on docker commit\tv29.0\tv30.0\nDeprecated\tLegacy links environment variables\tv28.4\tv30.0\nDeprecated\tSpecial handling for quoted values for TLS flags\tv28.4\tv29.0\nDeprecated\tEmpty/nil fields in image Config from inspect API\tv28.3\tv29.0\nDeprecated\tConfiguration for pushing non-distributable artifacts\tv28.0\tv29.0\nDeprecated\t--time option on docker stop and docker restart\tv28.0\t-\nRemoved\tNon-standard fields in image inspect\tv27.0\tv28.2\nRemoved\tAPI CORS headers\tv27.0\tv28.0\nRemoved\tGraphdriver plugins (experimental)\tv27.0\tv28.0\nDeprecated\tUnauthenticated TCP connections\tv26.0\tv28.0\nRemoved\tContainer and ContainerConfig fields in Image inspect\tv25.0\tv26.0\nRemoved\tDeprecate legacy API versions\tv25.0\tv26.0\nRemoved\tContainer short ID in network Aliases field\tv25.0\tv26.0\nRemoved\tMount bind-nonrecursive option\tv25.0\tv29.0\nRemoved\tIsAutomated field, and is-automated filter on docker search\tv25.0\tv28.2\nRemoved\tlogentries logging driver\tv24.0\tv25.0\nRemoved\tOOM-score adjust for the daemon\tv24.0\tv25.0\nRemoved\tBuildKit build information\tv23.0\tv24.0\nDeprecated\tLegacy builder for Linux images\tv23.0\t-\nDeprecated\tLegacy builder fallback\tv23.0\t-\nRemoved\tBtrfs storage driver on CentOS 7 and RHEL 7\tv20.10\tv23.0\nRemoved\tSupport for encrypted TLS private keys\tv20.10\tv23.0\nRemoved\tKubernetes stack and context support\tv20.10\tv23.0\nRemoved\tPulling images from non-compliant image registries\tv20.10\tv28.2\nRemoved\tLinux containers on Windows (LCOW)\tv20.10\tv23.0\nDeprecated\tBLKIO weight options with cgroups v1\tv20.10\t-\nRemoved\tKernel memory limit\tv20.10\tv23.0\nRemoved\tClassic Swarm and overlay networks using external key/value stores\tv20.10\tv23.0\nRemoved\tSupport for the legacy ~/.dockercfg configuration file for authentication\tv20.10\tv23.0\nDeprecated\tCLI plugins support\tv20.10\t-\nDeprecated\tDockerfile legacy ENV name value syntax\tv20.10\t-\nRemoved\tdocker build --stream flag (experimental)\tv20.10\tv20.10\nRemoved\tfluentd-async-connect log opt\tv20.10\tv28.0\nRemoved\tConfiguration options for experimental CLI features\tv19.03\tv23.0\nRemoved\tPushing and pulling with image manifest v2 schema 1\tv19.03\tv28.2\nRemoved\tdocker engine subcommands\tv19.03\tv20.10\nRemoved\tTop-level docker deploy subcommand (experimental)\tv19.03\tv20.10\nRemoved\tdocker stack deploy using \"dab\" files (experimental)\tv19.03\tv20.10\nRemoved\tSupport for the overlay2.override_kernel_check storage option\tv19.03\tv24.0\nRemoved\tAuFS storage driver\tv19.03\tv24.0\nRemoved\tLegacy \"overlay\" storage driver\tv18.09\tv24.0\nRemoved\tDevice mapper storage driver\tv18.09\tv25.0\nRemoved\tUse of reserved namespaces in engine labels\tv18.06\tv20.10\nRemoved\t--disable-legacy-registry override daemon option\tv17.12\tv19.03\nRemoved\tInteracting with V1 registries\tv17.06\tv17.12\nRemoved\tAsynchronous service create and service update as default\tv17.05\tv17.10\nRemoved\t-g and --graph flags on dockerd\tv17.05\tv23.0\nDeprecated\tTop-level network properties in NetworkSettings\tv1.13\tv17.12\nRemoved\tfilter option for /images/json endpoint\tv1.13\tv20.10\nRemoved\trepository:shortid image references\tv1.13\tv17.12\nRemoved\tdocker daemon subcommand\tv1.13\tv17.12\nRemoved\tDuplicate keys with conflicting values in engine labels\tv1.13\tv17.12\nDeprecated\tMAINTAINER in Dockerfile\tv1.13\t-\nDeprecated\tAPI calls without a version\tv1.13\tv17.12\nRemoved\tBacking filesystem without d_type support for overlay/overlay2\tv1.13\tv17.12\nRemoved\t--automated and --stars flags on docker search\tv1.12\tv20.10\nDeprecated\t-h shorthand for --help\tv1.12\tv17.09\nRemoved\t-e and --email flags on docker login\tv1.11\tv17.06\nDeprecated\tSeparator (:) of --security-opt flag on docker run\tv1.11\tv17.06\nDeprecated\tAmbiguous event fields in API\tv1.10\t-\nRemoved\t-f flag on docker tag\tv1.10\tv1.12\nRemoved\tHostConfig at API container start\tv1.10\tv1.12\nRemoved\t--before and --since flags on docker ps\tv1.10\tv1.12\nRemoved\tDriver-specific log tags\tv1.9\tv1.12\nRemoved\tDocker Content Trust ENV passphrase variables name change\tv1.9\tv1.12\nRemoved\t/containers/(id or name)/copy endpoint\tv1.8\tv1.12\nRemoved\tLXC built-in exec driver\tv1.8\tv1.10\nRemoved\tOld Command Line Options\tv1.8\tv1.10\nRemoved\t--api-enable-cors flag on dockerd\tv1.6\tv17.09\nRemoved\t--run flag on docker commit\tv0.10\tv1.13\nRemoved\tThree arguments form in docker import\tv0.6.7\tv1.12\nSupport for cgroup v1\n\nDeprecated in release: v29.0\n\nSupport for cgroup v1 is deprecated in the v29.0 release, however, it will continue to be supported until May 2029. The latest release in May 2029 may not necessarily support cgroup v1, but there will be at least one maintained branch with the support for cgroup v1.\n\nThe cgroup version currently in use can be checked by running the docker info command:\n\n$ docker info\n\n<...>\n\nServer:\n\n <...>\n\n Cgroup Version: 2\n\n <...>\n\n--pause option on docker commit\n\nDeprecated in release: v29.0\n\nTarget for removal in release: v30.0\n\nThe --pause option is enabled by default since Docker v1.1.0 to prevent committing containers in an inconsistent state, but can be disabled by setting the --pause=false option. In docker CLI v29.0 this flag is replaced by a --no-pause flag instead. The --pause option is still functional in the v29.0 release, printing a deprecation warning, but will be removed in docker CLI v30.\n\nLegacy links environment variables\n\nDeprecated in release: v28.4\n\nDisabled by default in release: v29.0\n\nTarget for removal in release: v30.0\n\nContainers attached to the default bridge network can specify \"legacy links\" (e.g. using --links on the CLI) to get access to other containers attached to that network. The linking container (i.e., the container created with --links) automatically gets environment variables that specify the IP address and port mappings of the linked container. However, these environment variables are prefixed with the linked container's names, making them impractical.\n\nStarting with Docker v29.0, these environment variables are no longer set by default. Users who still depend on them can start Docker Engine with the environment variable DOCKER_KEEP_DEPRECATED_LEGACY_LINKS_ENV_VARS=1 set.\n\nSupport for legacy links environment variables, as well as the DOCKER_KEEP_DEPRECATED_LEGACY_LINKS_ENV_VARS will be removed in Docker Engine v30.0.\n\nSpecial handling for quoted values for TLS flags\n\nDeprecated in release: v28.4\n\nTarget for removal in release: v29.0\n\nThe --tlscacert, --tlscert, and --tlskey command-line flags had non-standard behavior for handling values contained in quotes (\" or '). Normally, quotes are handled by the shell, for example, in the following example, the shell takes care of handling quotes before passing the values to the docker CLI:\n\ndocker --some-option \"some-value-in-quotes\" ...\n\n\nHowever, when passing values using an equal sign (=), this may not happen and values may be handled including quotes;\n\ndocker --some-option=\"some-value-in-quotes\" ...\n\n\nThis caused issues with \"Docker Machine\", which used this format as part of its docker-machine config output, and the CLI carried special, non-standard handling for these flags.\n\nDocker Machine reached EOL, and this special handling made the processing of flag values inconsistent with other flags used, so this behavior is deprecated. Users depending on this behavior are recommended to specify the quoted values using a space between the flag and its value, as illustrated above.\n\nEmpty/nil fields in image Config from inspect API\n\nDeprecated in release: v28.3\n\nTarget for removal in release: v29.0\n\nThe Config field returned by docker image inspect (and the GET /images/{name}/json API endpoint) currently includes certain fields even when they are empty or nil. Starting in Docker v29.0, the following fields will be omitted from the API response when they contain empty or default values:\n\nCmd\nEntrypoint\nEnv\nLabels\nOnBuild\nUser\nVolumes\nWorkingDir\n\nApplications consuming the image inspect API should be updated to handle the absence of these fields gracefully, treating missing fields as having their default/empty values.\n\nFor API version corresponding to Docker v29.0, these fields will be omitted when empty. They will continue to be included when using clients that request an older API version for backward compatibility.\n\nConfiguration for pushing non-distributable artifacts\n\nDeprecated in release: v28.0\n\nTarget for removal in release: v29.0\n\nNon-distributable artifacts (also called foreign layers) were introduced in docker v1.12 to accommodate Windows images for which the EULA did not allow layers to be distributed through registries other than those hosted by Microsoft. The concept of foreign / non-distributable layers was adopted by the OCI distribution spec in oci#233. These restrictions were relaxed later to allow distributing these images through non-public registries, for which a configuration was added in Docker v17.0.6.0.\n\nIn 2022, Microsoft updated the EULA and removed these restrictions, followed by the OCI distribution specification deprecating foreign layers in oci#965. In 2023, Microsoft removed the use of foreign data layers for their images, making this functionality obsolete.\n\nDocker v28.0 deprecates the --allow-nondistributable-artifacts daemon flag and corresponding allow-nondistributable-artifacts field in daemon.json. Setting either option no longer takes an effect, but a deprecation warning log is added to raise awareness about the deprecation. This warning is planned to become an error in the Docker v29.0.\n\nUsers currently using these options are therefore recommended to remove this option from their configuration to prevent the daemon from starting when upgrading to Docker v29.0.\n\nThe AllowNondistributableArtifactsCIDRs and AllowNondistributableArtifactsHostnames fields in the RegistryConfig of the GET /info API response are also deprecated. For API version v1.48 and lower, the fields are still included in the response but always null. In API version v1.49 and higher, the field will be omitted entirely.\n\n--time option on docker stop and docker restart\n\nDeprecated in release: v28.0\n\nThe --time option for the docker stop, docker container stop, docker restart, and docker container restart commands has been renamed to --timeout for consistency with other uses of timeout options. The --time option is now deprecated and hidden, but remains functional for backward compatibility. Users are encouraged to migrate to using the --timeout option instead.\n\nNon-standard fields in image inspect\n\nDeprecated in release: v27.0\n\nRemoved in release: v28.2\n\nThe Config field returned shown in docker image inspect (and as returned by the GET /images/{name}/json API endpoint) returns additional fields that are not part of the image's configuration and not part of the Docker image specification and OCI image specification.\n\nThese fields are never set (and always return the default value for the type), but are not omitted in the response when left empty. As these fields were not intended to be part of the image configuration response, they are deprecated, and will be removed from the API in thee next release.\n\nThe following fields are not part of the underlying image's Config field, and removed in the API response for API v1.50 and newer, corresponding with v28.2. They continue to be included when using clients that use an older API version:\n\nHostname\nDomainname\nAttachStdin\nAttachStdout\nAttachStderr\nTty\nOpenStdin\nStdinOnce\nImage\nNetworkDisabled (omitted unless set on older API versions)\nMacAddress (omitted unless set on older API versions)\nStopTimeout (omitted unless set on older API versions)\nGraphdriver plugins (experimental)\n\nDeprecated in: v27.0**.\n\nDisabled by default in release: v27.0\n\nTarget for removal in release: v28.0\n\nGraphdriver plugins were an experimental feature that allowed extending the Docker Engine with custom storage drivers for storing images and containers. This feature was not maintained since its inception.\n\nSupport for graphdriver plugins was disabled by default in v27.0, and removed in v28.0. Users of this feature are recommended to instead configure the Docker Engine to use the containerd image store and a custom snapshotter\n\nAPI CORS headers\n\nDeprecated in release: v27.0\n\nDisabled by default in release: v27.0\n\nRemoved in release: v28.0\n\nThe api-cors-header configuration option for the Docker daemon is insecure, and is therefore deprecated and scheduled for removal. Incorrectly setting this option could leave a window of opportunity for unauthenticated cross-origin requests to be accepted by the daemon.\n\nIn Docker Engine v27.0, this flag can still be set, but it has no effect unless the environment variable DOCKERD_DEPRECATED_CORS_HEADER is also set to a non-empty value.\n\nThis flag has been removed altogether in v28.0.\n\nThis is a breaking change for authorization plugins and other programs that depend on this option for accessing the Docker API from a browser. If you need to access the API through a browser, use a reverse proxy.\n\nUnauthenticated TCP connections\n\nDeprecated in release: v26.0\n\nTarget for removal in release: v28.0\n\nConfiguring the Docker daemon to listen on a TCP address will require mandatory TLS verification. This change aims to ensure secure communication by preventing unauthorized access to the Docker daemon over potentially insecure networks. This mandatory TLS requirement applies to all TCP addresses except tcp://localhost.\n\nIn version 27.0 and later, specifying --tls=false or --tlsverify=false CLI flags causes the daemon to fail to start if it's also configured to accept remote connections over TCP. This also applies to the equivalent configuration options in daemon.json.\n\nTo facilitate remote access to the Docker daemon over TCP, you'll need to implement TLS verification. This secures the connection by encrypting data in transit and providing a mechanism for mutual authentication.\n\nFor environments remote daemon access isn't required, we recommend binding the Docker daemon to a Unix socket. For daemons where remote access is required and where TLS encryption is not feasible, you may want to consider using SSH as an alternative solution.\n\nFor further information, assistance, and step-by-step instructions on configuring TLS (or SSH) for the Docker daemon, refer to Protect the Docker daemon socket.\n\nContainer and ContainerConfig fields in Image inspect\n\nDeprecated in release: v25.0\n\nRemoved in release: v26.0\n\nThe Container and ContainerConfig fields returned by docker inspect are mostly an implementation detail of the classic (non-BuildKit) image builder. These fields are not portable and are empty when using the BuildKit-based builder (enabled by default since v23.0). These fields are deprecated in v25.0 and are omitted starting from v26.0 ( API version v1.45 and up). If image configuration of an image is needed, you can obtain it from the Config field.\n\nDeprecate legacy API versions\n\nDeprecated in release: v25.0\n\nTarget for removal in release: v26.0\n\nThe Docker daemon provides a versioned API for backward compatibility with old clients. Docker clients can perform API-version negotiation to select the most recent API version supported by the daemon (downgrading to and older version of the API when necessary). API version negotiation was introduced in Docker v1.12.0 (API 1.24), and clients before that used a fixed API version.\n\nDocker Engine versions through v25.0 provide support for all API versions included in stable releases for a given platform. For Docker daemons on Linux, the earliest supported API version is 1.12 (corresponding with Docker Engine v1.0.0), whereas for Docker daemons on Windows, the earliest supported API version is 1.24 (corresponding with Docker Engine v1.12.0).\n\nSupport for legacy API versions (providing old API versions on current versions of the Docker Engine) is primarily intended to provide compatibility with recent, but still supported versions of the client, which is a common scenario (the Docker daemon may be updated to the latest release, but not all clients may be up-to-date or vice versa). Support for API versions before that (API versions provided by EOL versions of the Docker Daemon) is provided on a \"best effort\" basis.\n\nUse of old API versions is rare, and support for legacy API versions involves significant complexity (Docker 1.0.0 having been released 10 years ago). Because of this, we'll start deprecating support for legacy API versions.\n\nDocker Engine v25.0 by default disables API version older than 1.24 (aligning the minimum supported API version between Linux and Windows daemons). When connecting with a client that uses an API version older than 1.24, the daemon returns an error. The following example configures the Docker CLI to use API version 1.23, which produces an error:\n\nDOCKER_API_VERSION=1.23 docker version\n\nError response from daemon: client version 1.23 is too old. Minimum supported API version is 1.24,\n\nupgrade your client to a newer version\n\n\nSupport for API versions lower than 1.24 has been permanently removed in Docker Engine v26, and the minimum supported API version will be incrementally raised in releases following that.\n\nContainer short ID in network Aliases field\n\nDeprecated in release: v25.0\n\nRemoved in release: v26.0\n\nThe Aliases field returned by docker inspect contains the container short ID once the container is started. This behavior is deprecated in v25.0 but kept until the next release, v26.0. Starting with that version, the Aliases field will only contain the aliases set through the docker container create and docker run flag --network-alias.\n\nA new field DNSNames containing the container name (if one was specified), the hostname, the network aliases, as well as the container short ID, has been introduced in v25.0 and should be used instead of the Aliases field.\n\nMount bind-nonrecursive option\n\nDeprecated in release: v25.0\n\nRemoved in release: v29.0\n\nThe bind-nonrecursive option was replaced with the bind-recursive option (see cli-4316, cli-4671). The option was still accepted, but printed a deprecation warning:\n\nbind-nonrecursive is deprecated, use bind-recursive=disabled instead\n\n\nIn the v29.0 release, this warning is removed, and returned as an error. Users should use the equivalent bind-recursive=disabled option instead.\n\nIsAutomated field, and is-automated filter on docker search\n\nDeprecated in release: v25.0\n\nRemoved in release: v28.2\n\nThe is_automated field has been deprecated by Docker Hub's search API. Consequently, the IsAutomated field in image search will always be set to false in future, and searching for \"is-automated=true\" will yield no results.\n\nThe AUTOMATED column has been removed from the default docker search and docker image search output in v25.0, and the corresponding IsAutomated templating has been removed in v28.2.\n\nLogentries logging driver\n\nDeprecated in release: v24.0\n\nRemoved in release: v25.0\n\nThe logentries service SaaS was shut down on November 15, 2022, rendering this logging driver non-functional. Users should no longer use this logging driver, and the driver has been removed in Docker 25.0. Existing containers using this logging-driver are migrated to use the \"local\" logging driver after upgrading.\n\nOOM-score adjust for the daemon\n\nDeprecated in release: v24.0\n\nRemoved in release: v25.0\n\nThe oom-score-adjust option was added to prevent the daemon from being OOM-killed before other processes. This option was mostly added as a convenience, as running the daemon as a systemd unit was not yet common.\n\nHaving the daemon set its own limits is not best-practice, and something better handled by the process-manager starting the daemon.\n\nDocker v20.10 and newer no longer adjust the daemon's OOM score by default, instead setting the OOM-score to the systemd unit (OOMScoreAdjust) that's shipped with the packages.\n\nUsers currently depending on this feature are recommended to adjust the daemon's OOM score using systemd or through other means, when starting the daemon.\n\nBuildKit build information\n\nDeprecated in release: v23.0\n\nRemoved in release: v24.0\n\nBuild information structures have been introduced in BuildKit v0.10.0 and are generated with build metadata that allows you to see all the sources (images, Git repositories) that were used by the build with their exact versions and also the configuration that was passed to the build. This information is also embedded into the image configuration if one is generated.\n\nLegacy builder for Linux images\n\nDeprecated in release: v23.0\n\nDocker v23.0 now uses BuildKit by default to build Linux images, and uses the Buildx CLI component for docker build. With this change, docker build now exposes all advanced features that BuildKit provides and which were previously only available through the docker buildx subcommands.\n\nThe Buildx component is installed automatically when installing the docker CLI using our .deb or .rpm packages, and statically linked binaries are provided both on download.docker.com, and through the docker/buildx-bin image on Docker Hub. Refer the Buildx section for detailed instructions on installing the Buildx component.\n\nThis release marks the beginning of the deprecation cycle of the classic (\"legacy\") builder for Linux images. No active development will happen on the classic builder (except for bugfixes). BuildKit development started five Years ago, left the \"experimental\" phase since Docker 18.09, and is already the default builder for Docker Desktop. While we're comfortable that BuildKit is stable for general use, there may be some changes in behavior. If you encounter issues with BuildKit, we encourage you to report issues in the BuildKit issue tracker on GitHub{:target=\"blank\" rel=\"noopener\" class=\"\"}\n\nClassic builder for building Windows images\n\nBuildKit does not (yet) provide support for building Windows images, and docker build continues to use the classic builder to build native Windows images on Windows daemons.\n\nLegacy builder fallback\n\nDeprecated in release: v23.0\n\nDocker v23.0 now uses BuildKit by default to build Linux images, which requires the Buildx component to build images with BuildKit. There may be situations where the Buildx component is not available, and BuildKit cannot be used.\n\nTo provide a smooth transition to BuildKit as the default builder, Docker v23.0 has an automatic fallback for some situations, or produces an error to assist users to resolve the problem.\n\nIn situations where the user did not explicitly opt-in to use BuildKit (i.e., DOCKER_BUILDKIT=1 is not set), the CLI automatically falls back to the classic builder, but prints a deprecation warning:\n\nDEPRECATED: The legacy builder is deprecated and will be removed in a future release.\n\n            Install the buildx component to build images with BuildKit:\n\n            https://docs.docker.com/go/buildx/\n\nThis situation may occur if the docker CLI is installed using the static binaries, and the Buildx component is not installed or not installed correctly. This fallback will be removed in a future release, therefore we recommend to install the Buildx component and use BuildKit for your builds, or opt-out of using BuildKit with DOCKER_BUILDKIT=0.\n\nIf you opted-in to use BuildKit (DOCKER_BUILDKIT=1), but the Buildx component is missing, an error is printed instead, and the docker build command fails:\n\nERROR: BuildKit is enabled but the buildx component is missing or broken.\n\n       Install the buildx component to build images with BuildKit:\n\n       https://docs.docker.com/go/buildx/\n\nWe recommend to install the Buildx component to continue using BuildKit for your builds, but alternatively, users can either unset the DOCKER_BUILDKIT environment variable to fall back to the legacy builder, or opt-out of using BuildKit with DOCKER_BUILDKIT=0.\n\nBe aware that the classic builder is deprecated so both the automatic fallback and opting-out of using BuildKit will no longer be possible in a future release.\n\nBtrfs storage driver on CentOS 7 and RHEL 7\n\nRemoved in release: v23.0\n\nThe btrfs storage driver on CentOS and RHEL was provided as a technology preview by CentOS and RHEL, but has been deprecated since the Red Hat Enterprise Linux 7.4 release, and removed in CentOS 8 and RHEL 8. Users of the btrfs storage driver on CentOS are recommended to migrate to a different storage driver, such as overlay2, which is now the default storage driver. Docker 23.0 continues to provide the btrfs storage driver to allow users to migrate to an alternative driver. The next release of Docker will no longer provide this driver.\n\nSupport for encrypted TLS private keys\n\nDeprecated in release: v20.10\n\nRemoved in release: v23.0\n\nUse of encrypted TLS private keys has been deprecated, and has been removed. Golang has deprecated support for legacy PEM encryption (as specified in RFC 1423), as it is insecure by design (see https://go-review.googlesource.com/c/go/+/264159).\n\nThis feature allowed using an encrypted private key with a supplied password, but did not provide additional security as the encryption is known to be broken, and the key is sitting next to the password in the filesystem. Users are recommended to decrypt the private key, and store it un-encrypted to continue using it.\n\nKubernetes stack and context support\n\nDeprecated in release: v20.10\n\nRemoved in release: v23.0\n\nFollowing the deprecation of Compose on Kubernetes, support for Kubernetes in the stack and context commands has been removed from the CLI, and options related to this functionality are now either ignored, or may produce an error.\n\nThe following command-line flags are removed from the docker context subcommands:\n\n--default-stack-orchestrator - swarm is now the only (and default) orchestrator for stacks.\n--kubernetes - the Kubernetes endpoint can no longer be stored in docker context.\n--kubeconfig - exporting a context as a kubeconfig file is no longer supported.\n\nThe output produced by the docker context inspect subcommand no longer contains information about StackOrchestrator and Kubernetes endpoints for new contexts.\n\nThe following command-line flags are removed from the docker stack subcommands:\n\n--kubeconfig - using a kubeconfig file as context is no longer supported.\n--namespace - configuring the Kubernetes namespace for stacks is no longer supported.\n--orchestrator - swarm is now the only (and default) orchestrator for stacks.\n\nThe DOCKER_STACK_ORCHESTRATOR, DOCKER_ORCHESTRATOR, and KUBECONFIG environment variables, as well as the stackOrchestrator option in the ~/.docker/config.json CLI configuration file are no longer used, and ignored.\n\nPulling images from non-compliant image registries\n\nDeprecated in release: v20.10\n\nRemoved in release: v28.2\n\nDocker Engine v20.10 and up includes optimizations to verify if images in the local image cache need updating before pulling, preventing the Docker Engine from making unnecessary API requests. These optimizations require the container image registry to conform to the Open Container Initiative Distribution Specification.\n\nWhile most registries conform to the specification, we encountered some registries to be non-compliant, resulting in docker pull to fail.\n\nAs a temporary solution, Docker Engine v20.10 added a fallback mechanism to allow docker pull to be functional when using a non-compliant registry. A warning message is printed in this situation:\n\nWARNING Failed to pull manifest by the resolved digest. This registry does not\n        appear to conform to the distribution registry specification; falling back to\n        pull by tag. This fallback is DEPRECATED, and will be removed in a future\n        release.\n\n\nThe fallback was added to allow users to either migrate their images to a compliant registry, or for these registries to become compliant.\n\nGitHub deprecated the legacy docker.pkg.github.com registry, and it was sunset on Feb 24th, 2025 in favor of GitHub Container Registry (GHCR, ghcr.io), making this fallback no longer needed.\n\nLinux containers on Windows (LCOW) (experimental)\n\nDeprecated in release: v20.10\n\nRemoved in release: v23.0\n\nThe experimental feature to run Linux containers on Windows (LCOW) was introduced as a technical preview in Docker 17.09. While many enhancements were made after its introduction, the feature never reached completeness, and development has now stopped in favor of running Docker natively on Linux in WSL2.\n\nDevelopers who want to run Linux workloads on a Windows host are encouraged to use Docker Desktop with WSL2 instead.\n\nBLKIO weight options with cgroups v1\n\nDeprecated in release: v20.10\n\nSpecifying blkio weight (docker run --blkio-weight and docker run --blkio-weight-device) is now marked as deprecated when using cgroups v1 because the corresponding features were removed in Linux kernel v5.0 and up. When using cgroups v2, the --blkio-weight options are implemented using `io.weight.\n\nKernel memory limit\n\nDeprecated in release: v20.10\n\nRemoved in release: v23.0\n\nSpecifying kernel memory limit (docker run --kernel-memory) is no longer supported because the Linux kernel deprecated kmem.limit_in_bytes in v5.4. The OCI runtime specification now marks this option as \"NOT RECOMMENDED\", and OCI runtimes such as runc no longer support this option.\n\nThe Docker API no longer handles the kernel-memory fields, and Docker CLI v29.0 removes the --kernel-memory option.\n\nClassic Swarm and overlay networks using cluster store\n\nDeprecated in release: v20.10\n\nRemoved in release: v23.0\n\nStandalone (\"classic\") Swarm has been deprecated, and with that the use of overlay networks using an external key/value store. The corresponding--cluster-advertise, --cluster-store, and --cluster-store-opt daemon options have been removed.\n\nSupport for legacy ~/.dockercfg configuration files\n\nDeprecated in release: v20.10\n\nRemoved in release: v23.0\n\nThe Docker CLI up until v1.7.0 used the ~/.dockercfg file to store credentials after authenticating to a registry (docker login). Docker v1.7.0 replaced this file with a new CLI configuration file, located in ~/.docker/config.json. When implementing the new configuration file, the old file (and file-format) was kept as a fall-back, to assist existing users with migrating to the new file.\n\nGiven that the old file format encourages insecure storage of credentials (credentials are stored unencrypted), and that no version of the CLI since Docker v1.7.0 has created this file, support for this file, and its format has been removed.\n\nConfiguration options for experimental CLI features\n\nDeprecated in release: v19.03\n\nRemoved in release: v23.0\n\nThe DOCKER_CLI_EXPERIMENTAL environment variable and the corresponding experimental field in the CLI configuration file are deprecated. Experimental features are enabled by default, and these configuration options are no longer functional.\n\nStarting with v23.0, the Docker CLI no longer prints Experimental for the client in the output of docker version, and the field has been removed from the JSON format.\n\nCLI plugins support\n\nDeprecated in release: v20.10\n\nCLI Plugin API is now marked as deprecated.\n\nDockerfile legacy ENV name value syntax\n\nDeprecated in release: v20.10\n\nThe Dockerfile ENV instruction allows values to be set using either ENV name=value or ENV name value. The latter (ENV name value) form can be ambiguous, for example, the following defines a single env-variable (ONE) with value \"TWO= THREE=world\", but may have intended to be setting three env-vars:\n\nENV ONE TWO= THREE=world\n\nThis format also does not allow setting multiple environment-variables in a single ENV line in the Dockerfile.\n\nUse of the ENV name value syntax is discouraged, and may be removed in a future release. Users are encouraged to update their Dockerfiles to use the ENV name=value syntax, for example:\n\nENV ONE=\"\" TWO=\"\" THREE=\"world\"\ndocker build --stream flag (experimental)\n\nDeprecated in release: v20.10\n\nRemoved in release: v20.10\n\nDocker v17.07 introduced an experimental --stream flag on docker build which allowed the build-context to be incrementally sent to the daemon, instead of unconditionally sending the whole build-context.\n\nThis functionality has been reimplemented as part of BuildKit, which uses streaming by default and the --stream option will be ignored when using the classic builder, printing a deprecation warning instead.\n\nUsers that want to use this feature are encouraged to enable BuildKit by setting the DOCKER_BUILDKIT=1 environment variable or through the daemon or CLI configuration files.\n\nfluentd-async-connect log opt\n\nDeprecated in release: v20.10\n\nRemoved in release: v28.0\n\nThe --log-opt fluentd-async-connect option for the fluentd logging driver is deprecated in favor of --log-opt fluentd-async. A deprecation message is logged in the daemon logs if the old option is used:\n\nfluent#New: AsyncConnect is now deprecated, use Async instead\n\n\nUsers are encouraged to use the fluentd-async option going forward, as support for the old option has been removed.\n\nPushing and pulling with image manifest v2 schema 1\n\nDeprecated in release: v19.03\n\nDisabled by default in release: v26.0\n\nRemoved in release: v28.2\n\nThe image manifest v2 schema 1 and \"Docker Image v1\" formats were deprecated in favor of the v2 schema 2 and OCI image spec formats.\n\nThese legacy formats should no longer be used, and users are recommended to update images to use current formats, or to upgrade to more current images. Starting with Docker v26.0, pulling these images is disabled by default, and support has been removed in v28.2. Attempting to pull a legacy image now produces an error:\n\n$ docker pull ubuntu:10.04\n\nError response from daemon:\n\nDocker Image Format v1 and Docker Image manifest version 2, schema 1 support has been removed.\n\nSuggest the author of docker.io/library/ubuntu:10.04 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2.\n\nMore information at https://docs.docker.com/go/deprecated-image-specs/\n\ndocker engine subcommands\n\nDeprecated in release: v19.03\n\nRemoved in release: v20.10\n\nThe docker engine activate, docker engine check, and docker engine update provided an alternative installation method to upgrade Docker Community engines to Docker Enterprise, using an image-based distribution of the Docker Engine.\n\nThis feature was only available on Linux, and only when executed on a local node. Given the limitations of this feature, and the feature not getting widely adopted, the docker engine subcommands will be removed, in favor of installation through standard package managers.\n\nTop-level docker deploy subcommand (experimental)\n\nDeprecated in release: v19.03\n\nRemoved in release: v20.10\n\nThe top-level docker deploy command (using the \"Docker Application Bundle\" (.dab) file format was introduced as an experimental feature in Docker 1.13 / 17.03, but superseded by support for Docker Compose files using the docker stack deploy subcommand.\n\ndocker stack deploy using \"dab\" files (experimental)\n\nDeprecated in release: v19.03\n\nRemoved in release: v20.10\n\nWith no development being done on this feature, and no active use of the file format, support for the DAB file format and the top-level docker deploy command (hidden by default in 19.03), will be removed, in favour of docker stack deploy using compose files.\n\nSupport for the overlay2.override_kernel_check storage option\n\nDeprecated in release: v19.03\n\nRemoved in release: v24.0\n\nThis daemon configuration option disabled the Linux kernel version check used to detect if the kernel supported OverlayFS with multiple lower dirs, which is required for the overlay2 storage driver. Starting with Docker v19.03.7, the detection was improved to no longer depend on the kernel version, so this option was no longer used.\n\nAuFS storage driver\n\nDeprecated in release: v19.03\n\nRemoved in release: v24.0\n\nThe aufs storage driver is deprecated in favor of overlay2, and has been removed in a Docker Engine v24.0. Users of the aufs storage driver must migrate to a different storage driver, such as overlay2, before upgrading to Docker Engine v24.0.\n\nThe aufs storage driver facilitated running Docker on distros that have no support for OverlayFS, such as Ubuntu 14.04 LTS, which originally shipped with a 3.14 kernel.\n\nNow that Ubuntu 14.04 is no longer a supported distro for Docker, and overlay2 is available to all supported distros (as they are either on kernel 4.x, or have support for multiple lowerdirs backported), there is no reason to continue maintenance of the aufs storage driver.\n\nLegacy overlay storage driver\n\nDeprecated in release: v18.09\n\nRemoved in release: v24.0\n\nThe overlay storage driver is deprecated in favor of the overlay2 storage driver, which has all the benefits of overlay, without its limitations (excessive inode consumption). The legacy overlay storage driver has been removed in Docker Engine v24.0. Users of the overlay storage driver should migrate to the overlay2 storage driver before upgrading to Docker Engine v24.0.\n\nThe legacy overlay storage driver allowed using overlayFS-backed filesystems on kernels older than v4.x. Now that all supported distributions are able to run overlay2 (as they are either on kernel 4.x, or have support for multiple lowerdirs backported), there is no reason to keep maintaining the overlay storage driver.\n\nDevice mapper storage driver\n\nDeprecated in release: v18.09\n\nDisabled by default in release: v23.0\n\nRemoved in release: v25.0\n\nThe devicemapper storage driver is deprecated in favor of overlay2, and has been removed in Docker Engine v25.0. Users of the devicemapper storage driver must migrate to a different storage driver, such as overlay2, before upgrading to Docker Engine v25.0.\n\nThe devicemapper storage driver facilitates running Docker on older (3.x) kernels that have no support for other storage drivers (such as overlay2, or btrfs).\n\nNow that support for overlay2 is added to all supported distros (as they are either on kernel 4.x, or have support for multiple lowerdirs backported), there is no reason to continue maintenance of the devicemapper storage driver.\n\nUse of reserved namespaces in engine labels\n\nDeprecated in release: v18.06\n\nRemoved in release: v20.10\n\nThe namespaces com.docker.*, io.docker.*, and org.dockerproject.* in engine labels were always documented to be reserved, but there was never any enforcement.\n\nUsage of these namespaces will now cause a warning in the engine logs to discourage their use, and will error instead in v20.10 and above.\n\n--disable-legacy-registry override daemon option\n\nDisabled In Release: v17.12\n\nRemoved in release: v19.03\n\nThe --disable-legacy-registry flag was disabled in Docker 17.12 and will print an error when used. For this error to be printed, the flag itself is still present, but hidden. The flag has been removed in Docker 19.03.\n\nInteracting with V1 registries\n\nDisabled by default in release: v17.06\n\nRemoved in release: v17.12\n\nVersion 1.8.3 added a flag (--disable-legacy-registry=false) which prevents the Docker daemon from pull, push, and login operations against v1 registries. Though enabled by default, this signals the intent to deprecate the v1 protocol.\n\nSupport for the v1 protocol to the public registry was removed in 1.13. Any mirror configurations using v1 should be updated to use a v2 registry mirror.\n\nStarting with Docker 17.12, support for V1 registries has been removed, and the --disable-legacy-registry flag can no longer be used, and dockerd will fail to start when set.\n\nAsynchronous service create and service update as default\n\nDeprecated in release: v17.05\n\nDisabled by default in release: v17.10\n\nDocker 17.05 added an optional --detach=false option to make the docker service create and docker service update work synchronously. This option will be enabled by default in Docker 17.10, at which point the --detach flag can be used to use the previous (asynchronous) behavior.\n\nThe default for this option will also be changed accordingly for docker service rollback and docker service scale in Docker 17.10.\n\n-g and --graph flags on dockerd\n\nDeprecated in release: v17.05\n\nRemoved in release: v23.0\n\nThe -g or --graph flag for the dockerd or docker daemon command was used to indicate the directory in which to store persistent data and resource configuration and has been replaced with the more descriptive --data-root flag. These flags were deprecated and hidden in v17.05, and removed in v23.0.\n\nTop-level network properties in NetworkSettings\n\nDeprecated in release: v1.13.0\n\nTarget for removal in release: v17.12\n\nWhen inspecting a container, NetworkSettings contains top-level information about the default (\"bridge\") network;\n\nEndpointID, Gateway, GlobalIPv6Address, GlobalIPv6PrefixLen, IPAddress, IPPrefixLen, IPv6Gateway, and MacAddress.\n\nThese properties are deprecated in favor of per-network properties in NetworkSettings.Networks. These properties were already \"deprecated\" in Docker 1.9, but kept around for backward compatibility.\n\nRefer to #17538 for further information.\n\nfilter option for /images/json endpoint\n\nDeprecated in release: v1.13.0\n\nRemoved in release: v20.10\n\nThe filter option to filter the list of image by reference (name or name:tag) is now implemented as a regular filter, named reference.\n\nrepository:shortid image references\n\nDeprecated in release: v1.13.0\n\nRemoved in release: v17.12\n\nThe repository:shortid syntax for referencing images is very little used, collides with tag references, and can be confused with digest references.\n\nSupport for the repository:shortid notation to reference images was removed in Docker 17.12.\n\ndocker daemon subcommand\n\nDeprecated in release: v1.13.0\n\nRemoved in release: v17.12\n\nThe daemon is moved to a separate binary (dockerd), and should be used instead.\n\nDuplicate keys with conflicting values in engine labels\n\nDeprecated in release: v1.13.0\n\nRemoved in release: v17.12\n\nWhen setting duplicate keys with conflicting values, an error will be produced, and the daemon will fail to start.\n\nMAINTAINER in Dockerfile\n\nDeprecated in release: v1.13.0\n\nMAINTAINER was an early very limited form of LABEL which should be used instead.\n\nAPI calls without a version\n\nDeprecated in release: v1.13.0\n\nTarget for removal in release: v17.12\n\nAPI versions should be supplied to all API calls to ensure compatibility with future Engine versions. Instead of just requesting, for example, the URL /containers/json, you must now request /v1.25/containers/json.\n\nBacking filesystem without d_type support for overlay/overlay2\n\nDeprecated in release: v1.13.0\n\nRemoved in release: v17.12\n\nThe overlay and overlay2 storage driver does not work as expected if the backing filesystem does not support d_type. For example, XFS does not support d_type if it is formatted with the ftype=0 option.\n\nSupport for these setups has been removed, and Docker v23.0 and up now fails to start when attempting to use the overlay2 or overlay storage driver on a backing filesystem without d_type support.\n\nRefer to #27358 for details.\n\n--automated and --stars flags on docker search\n\nDeprecated in release: v1.12.0\n\nRemoved in release: v20.10\n\nThe docker search --automated and docker search --stars options are deprecated. Use docker search --filter=is-automated=<true|false> and docker search --filter=stars=... instead.\n\n-h shorthand for --help\n\nDeprecated in release: v1.12.0\n\nTarget for removal in release: v17.09\n\nThe shorthand (-h) is less common than --help on Linux and cannot be used on all subcommands (due to it conflicting with, e.g. -h / --hostname on docker create). For this reason, the -h shorthand was not printed in the \"usage\" output of subcommands, nor documented, and is now marked \"deprecated\".\n\n-e and --email flags on docker login\n\nDeprecated in release: v1.11.0\n\nRemoved in release: v17.06\n\nThe docker login no longer automatically registers an account with the target registry if the given username doesn't exist. Due to this change, the email flag is no longer required, and will be deprecated.\n\nSeparator (:) of --security-opt flag on docker run\n\nDeprecated in release: v1.11.0\n\nTarget for removal in release: v17.06\n\nThe flag --security-opt doesn't use the colon separator (:) anymore to divide keys and values, it uses the equal symbol (=) for consistency with other similar flags, like --storage-opt.\n\nAmbiguous event fields in API\n\nDeprecated in release: v1.10.0\n\nThe fields ID, Status and From in the events API have been deprecated in favor of a more rich structure. See the events API documentation for the new format.\n\n-f flag on docker tag\n\nDeprecated in release: v1.10.0\n\nRemoved in release: v1.12.0\n\nTo make tagging consistent across the various docker commands, the -f flag on the docker tag command is deprecated. It is no longer necessary to specify -f to move a tag from one image to another. Nor will docker generate an error if the -f flag is missing and the specified tag is already in use.\n\nHostConfig at API container start\n\nDeprecated in release: v1.10.0\n\nRemoved in release: v1.12.0\n\nPassing an HostConfig to POST /containers/{name}/start is deprecated in favor of defining it at container creation (POST /containers/create).\n\n--before and --since flags on docker ps\n\nDeprecated in release: v1.10.0\n\nRemoved in release: v1.12.0\n\nThe docker ps --before and docker ps --since options are deprecated. Use docker ps --filter=before=... and docker ps --filter=since=... instead.\n\nDriver-specific log tags\n\nDeprecated in release: v1.9.0\n\nRemoved in release: v1.12.0\n\nLog tags are now generated in a standard way across different logging drivers. Because of which, the driver specific log tag options syslog-tag, gelf-tag and fluentd-tag have been deprecated in favor of the generic tag option.\n\n$ docker --log-driver=syslog --log-opt tag=\"{{.ImageName}}/{{.Name}}/{{.ID}}\"\n\nDocker Content Trust ENV passphrase variables name change\n\nDeprecated in release: v1.9.0\n\nRemoved in release: v1.12.0\n\nSince 1.9, Docker Content Trust Offline key has been renamed to Root key and the Tagging key has been renamed to Repository key. Due to this renaming, we're also changing the corresponding environment variables\n\nDOCKER_CONTENT_TRUST_OFFLINE_PASSPHRASE is now named DOCKER_CONTENT_TRUST_ROOT_PASSPHRASE\nDOCKER_CONTENT_TRUST_TAGGING_PASSPHRASE is now named DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE\n/containers/(id or name)/copy endpoint\n\nDeprecated in release: v1.8.0\n\nRemoved in release: v1.12.0\n\nThe endpoint /containers/(id or name)/copy is deprecated in favor of /containers/(id or name)/archive.\n\nLXC built-in exec driver\n\nDeprecated in release: v1.8.0\n\nRemoved in release: v1.10.0\n\nThe built-in LXC execution driver, the lxc-conf flag, and API fields have been removed.\n\nOld Command Line Options\n\nDeprecated in release: v1.8.0\n\nRemoved in release: v1.10.0\n\nThe flags -d and --daemon are deprecated. Use the separate dockerd binary instead.\n\nThe following single-dash (-opt) variant of certain command line options are deprecated and replaced with double-dash options (--opt):\n\ndocker attach -nostdin\ndocker attach -sig-proxy\ndocker build -no-cache\ndocker build -rm\ndocker commit -author\ndocker commit -run\ndocker events -since\ndocker history -notrunc\ndocker images -notrunc\ndocker inspect -format\ndocker ps -beforeId\ndocker ps -notrunc\ndocker ps -sinceId\ndocker rm -link\ndocker run -cidfile\ndocker run -dns\ndocker run -entrypoint\ndocker run -expose\ndocker run -link\ndocker run -lxc-conf\ndocker run -n\ndocker run -privileged\ndocker run -volumes-from\ndocker search -notrunc\ndocker search -stars\ndocker search -t\ndocker search -trusted\ndocker tag -force\n\nThe following double-dash options are deprecated and have no replacement:\n\ndocker run --cpuset\ndocker run --networking\ndocker ps --since-id\ndocker ps --before-id\ndocker search --trusted\n\nDeprecated in release: v1.5.0\n\nRemoved in release: v1.12.0\n\nThe single-dash (-help) was removed, in favor of the double-dash --help\n\n--api-enable-cors flag on dockerd\n\nDeprecated in release: v1.6.0\n\nRemoved in release: v17.09\n\nThe flag --api-enable-cors is deprecated since v1.6.0. Use the flag --api-cors-header instead.\n\n--run flag on docker commit\n\nDeprecated in release: v0.10.0\n\nRemoved in release: v1.13.0\n\nThe flag --run of the docker commit command (and its short version -run) were deprecated in favor of the --changes flag that allows to pass Dockerfile commands.\n\nThree arguments form in docker import\n\nDeprecated in release: v0.6.7\n\nRemoved in release: v1.12.0\n\nThe docker import command format file|URL|- [REPOSITORY [TAG]] is deprecated since November 2013. It's no longer supported.\n\nRequest changes\n\nTable of contents\nFeature deprecation policy\nDeprecated engine features\nSupport for cgroup v1\n--pause option on docker commit\nLegacy links environment variables\nSpecial handling for quoted values for TLS flags\nEmpty/nil fields in image Config from inspect API\nConfiguration for pushing non-distributable artifacts\n--time option on docker stop and docker restart\nNon-standard fields in image inspect\nGraphdriver plugins (experimental)\nAPI CORS headers\nUnauthenticated TCP connections\nContainer and ContainerConfig fields in Image inspect\nDeprecate legacy API versions\nContainer short ID in network Aliases field\nMount bind-nonrecursive option\nIsAutomated field, and is-automated filter on docker search\nLogentries logging driver\nOOM-score adjust for the daemon\nBuildKit build information\nLegacy builder for Linux images\nLegacy builder fallback\nBtrfs storage driver on CentOS 7 and RHEL 7\nSupport for encrypted TLS private keys\nKubernetes stack and context support\nPulling images from non-compliant image registries\nLinux containers on Windows (LCOW) (experimental)\nBLKIO weight options with cgroups v1\nKernel memory limit\nClassic Swarm and overlay networks using cluster store\nSupport for legacy ~/.dockercfg configuration files\nConfiguration options for experimental CLI features\nCLI plugins support\nDockerfile legacy ENV name value syntax\ndocker build --stream flag (experimental)\nfluentd-async-connect log opt\nPushing and pulling with image manifest v2 schema 1\ndocker engine subcommands\nTop-level docker deploy subcommand (experimental)\ndocker stack deploy using \"dab\" files (experimental)\nSupport for the overlay2.override_kernel_check storage option\nAuFS storage driver\nLegacy overlay storage driver\nDevice mapper storage driver\nUse of reserved namespaces in engine labels\n--disable-legacy-registry override daemon option\nInteracting with V1 registries\nAsynchronous service create and service update as default\n-g and --graph flags on dockerd\nTop-level network properties in NetworkSettings\nfilter option for /images/json endpoint\nrepository:shortid image references\ndocker daemon subcommand\nDuplicate keys with conflicting values in engine labels\nMAINTAINER in Dockerfile\nAPI calls without a version\nBacking filesystem without d_type support for overlay/overlay2\n--automated and --stars flags on docker search\n-h shorthand for --help\n-e and --email flags on docker login\nSeparator (:) of --security-opt flag on docker run\nAmbiguous event fields in API\n-f flag on docker tag\nHostConfig at API container start\n--before and --since flags on docker ps\nDriver-specific log tags\nDocker Content Trust ENV passphrase variables name change\n/containers/(id or name)/copy endpoint\nLXC built-in exec driver\nOld Command Line Options\n--api-enable-cors flag on dockerd\n--run flag on docker commit\nThree arguments form in docker import\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995551,
    "timestamp": "2026-02-07T06:33:19.894Z",
    "title": "Access authorization plugin | Docker Docs",
    "url": "https://docs.docker.com/engine/extend/plugins_authorization/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nAccess authorization plugin\nDocker log driver plugins\nDocker network driver plugins\nDocker Plugin API\nDocker volume plugins\nPlugin Config Version 1 of Plugin V2\nUse Docker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDocker Engine plugins\n/\nAccess authorization plugin\nAccess authorization plugin\nCopy as Markdown\n\nThis document describes the Docker Engine plugins available in Docker Engine. To view information on plugins managed by Docker Engine, refer to Docker Engine plugin system.\n\nDocker's out-of-the-box authorization model is all or nothing. Any user with permission to access the Docker daemon can run any Docker client command. The same is true for callers using Docker's Engine API to contact the daemon. If you require greater access control, you can create authorization plugins and add them to your Docker daemon configuration. Using an authorization plugin, a Docker administrator can configure granular access policies for managing access to the Docker daemon.\n\nAnyone with the appropriate skills can develop an authorization plugin. These skills, at their most basic, are knowledge of Docker, understanding of REST, and sound programming knowledge. This document describes the architecture, state, and methods information available to an authorization plugin developer.\n\nBasic principles\n\nDocker's plugin infrastructure enables extending Docker by loading, removing and communicating with third-party components using a generic API. The access authorization subsystem was built using this mechanism.\n\nUsing this subsystem, you don't need to rebuild the Docker daemon to add an authorization plugin. You can add a plugin to an installed Docker daemon. You do need to restart the Docker daemon to add a new plugin.\n\nAn authorization plugin approves or denies requests to the Docker daemon based on both the current authentication context and the command context. The authentication context contains all user details and the authentication method. The command context contains all the relevant request data.\n\nAuthorization plugins must follow the rules described in Docker Plugin API. Each plugin must reside within directories described under the Plugin discovery section.\n\nNote\n\nThe abbreviations AuthZ and AuthN mean authorization and authentication respectively.\n\nDefault user authorization mechanism\n\nIf TLS is enabled in the Docker daemon, the default user authorization flow extracts the user details from the certificate subject name. That is, the User field is set to the client certificate subject common name, and the AuthenticationMethod field is set to TLS.\n\nBasic architecture\n\nYou are responsible for registering your plugin as part of the Docker daemon startup. You can install multiple plugins and chain them together. This chain can be ordered. Each request to the daemon passes in order through the chain. Only when all the plugins grant access to the resource, is the access granted.\n\nWhen an HTTP request is made to the Docker daemon through the CLI or via the Engine API, the authentication subsystem passes the request to the installed authentication plugin(s). The request contains the user (caller) and command context. The plugin is responsible for deciding whether to allow or deny the request.\n\nThe sequence diagrams below depict an allow and deny authorization flow:\n\nEach request sent to the plugin includes the authenticated user, the HTTP headers, and the request/response body. Only the user name and the authentication method used are passed to the plugin. Most importantly, no user credentials or tokens are passed. Finally, not all request/response bodies are sent to the authorization plugin. Only those request/response bodies where the Content-Type is either text/* or application/json are sent.\n\nFor commands that can potentially hijack the HTTP connection (HTTP Upgrade), such as exec, the authorization plugin is only called for the initial HTTP requests. Once the plugin approves the command, authorization is not applied to the rest of the flow. Specifically, the streaming data is not passed to the authorization plugins. For commands that return chunked HTTP response, such as logs and events, only the HTTP request is sent to the authorization plugins.\n\nDuring request/response processing, some authorization flows might need to do additional queries to the Docker daemon. To complete such flows, plugins can call the daemon API similar to a regular user. To enable these additional queries, the plugin must provide the means for an administrator to configure proper authentication and security policies.\n\nDocker client flows\n\nTo enable and configure the authorization plugin, the plugin developer must support the Docker client interactions detailed in this section.\n\nSetting up Docker daemon\n\nEnable the authorization plugin with a dedicated command line flag in the --authorization-plugin=PLUGIN_ID format. The flag supplies a PLUGIN_ID value. This value can be the plugin‚Äôs socket or a path to a specification file. Authorization plugins can be loaded without restarting the daemon. Refer to the dockerd documentation for more information.\n\n$ dockerd --authorization-plugin=plugin1 --authorization-plugin=plugin2,...\n\n\nDocker's authorization subsystem supports multiple --authorization-plugin parameters.\n\nCalling authorized command (allow)\n$ docker pull centos\n\n<...>\n\nf1b10cd84249: Pull complete\n\n<...>\n\nCalling unauthorized command (deny)\n$ docker pull centos\n\n<...>\n\ndocker: Error response from daemon: authorization denied by plugin PLUGIN_NAME: volumes are not allowed.\n\nError from plugins\n$ docker pull centos\n\n<...>\n\ndocker: Error response from daemon: plugin PLUGIN_NAME failed with error: AuthZPlugin.AuthZReq: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.\n\nAPI schema and implementation\n\nIn addition to Docker's standard plugin registration method, each plugin should implement the following two methods:\n\n/AuthZPlugin.AuthZReq This authorize request method is called before the Docker daemon processes the client request.\n\n/AuthZPlugin.AuthZRes This authorize response method is called before the response is returned from Docker daemon to the client.\n\n/AuthZPlugin.AuthZReq\n\nRequest\n\n{\n\n    \"User\":              \"The user identification\",\n\n    \"UserAuthNMethod\":   \"The authentication method used\",\n\n    \"RequestMethod\":     \"The HTTP method\",\n\n    \"RequestURI\":        \"The HTTP request URI\",\n\n    \"RequestBody\":       \"Byte array containing the raw HTTP request body\",\n\n    \"RequestHeader\":     \"Byte array containing the raw HTTP request header as a map[string][]string \"\n\n}\n\nResponse\n\n{\n\n    \"Allow\": \"Determined whether the user is allowed or not\",\n\n    \"Msg\":   \"The authorization message\",\n\n    \"Err\":   \"The error message if things go wrong\"\n\n}\n/AuthZPlugin.AuthZRes\n\nRequest:\n\n{\n\n    \"User\":              \"The user identification\",\n\n    \"UserAuthNMethod\":   \"The authentication method used\",\n\n    \"RequestMethod\":     \"The HTTP method\",\n\n    \"RequestURI\":        \"The HTTP request URI\",\n\n    \"RequestBody\":       \"Byte array containing the raw HTTP request body\",\n\n    \"RequestHeader\":     \"Byte array containing the raw HTTP request header as a map[string][]string\",\n\n    \"ResponseBody\":      \"Byte array containing the raw HTTP response body\",\n\n    \"ResponseHeader\":    \"Byte array containing the raw HTTP response header as a map[string][]string\",\n\n    \"ResponseStatusCode\":\"Response status code\"\n\n}\n\nResponse:\n\n{\n\n   \"Allow\":              \"Determined whether the user is allowed or not\",\n\n   \"Msg\":                \"The authorization message\",\n\n   \"Err\":                \"The error message if things go wrong\"\n\n}\nRequest authorization\n\nEach plugin must support two request authorization messages formats, one from the daemon to the plugin and then from the plugin to the daemon. The tables below detail the content expected in each message.\n\nDaemon -> Plugin\nName\tType\tDescription\nUser\tstring\tThe user identification\nAuthentication method\tstring\tThe authentication method used\nRequest method\tenum\tThe HTTP method (GET/DELETE/POST)\nRequest URI\tstring\tThe HTTP request URI including API version (e.g., v.1.17/containers/json)\nRequest headers\tmap[string]string\tRequest headers as key value pairs (without the authorization header)\nRequest body\t[]byte\tRaw request body\nPlugin -> Daemon\nName\tType\tDescription\nAllow\tbool\tBoolean value indicating whether the request is allowed or denied\nMsg\tstring\tAuthorization message (will be returned to the client in case the access is denied)\nErr\tstring\tError message (will be returned to the client in case the plugin encounter an error. The string value supplied may appear in logs, so should not include confidential information)\nResponse authorization\n\nThe plugin must support two authorization messages formats, one from the daemon to the plugin and then from the plugin to the daemon. The tables below detail the content expected in each message.\n\nDaemon -> Plugin\nName\tType\tDescription\nUser\tstring\tThe user identification\nAuthentication method\tstring\tThe authentication method used\nRequest method\tstring\tThe HTTP method (GET/DELETE/POST)\nRequest URI\tstring\tThe HTTP request URI including API version (e.g., v.1.17/containers/json)\nRequest headers\tmap[string]string\tRequest headers as key value pairs (without the authorization header)\nRequest body\t[]byte\tRaw request body\nResponse status code\tint\tStatus code from the Docker daemon\nResponse headers\tmap[string]string\tResponse headers as key value pairs\nResponse body\t[]byte\tRaw Docker daemon response body\nPlugin -> Daemon\nName\tType\tDescription\nAllow\tbool\tBoolean value indicating whether the response is allowed or denied\nMsg\tstring\tAuthorization message (will be returned to the client in case the access is denied)\nErr\tstring\tError message (will be returned to the client in case the plugin encounter an error. The string value supplied may appear in logs, so should not include confidential information)\n\nRequest changes\n\nTable of contents\nBasic principles\nDefault user authorization mechanism\nBasic architecture\nDocker client flows\nSetting up Docker daemon\nCalling authorized command (allow)\nCalling unauthorized command (deny)\nError from plugins\nAPI schema and implementation\nRequest authorization\nResponse authorization\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995557,
    "timestamp": "2026-02-07T06:33:19.896Z",
    "title": "Docker network driver plugins | Docker Docs",
    "url": "https://docs.docker.com/engine/extend/plugins_network/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nAccess authorization plugin\nDocker log driver plugins\nDocker network driver plugins\nDocker Plugin API\nDocker volume plugins\nPlugin Config Version 1 of Plugin V2\nUse Docker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDocker Engine plugins\n/\nDocker network driver plugins\nDocker network driver plugins\nCopy as Markdown\n\nThis document describes Docker Engine network driver plugins generally available in Docker Engine. To view information on plugins managed by Docker Engine, refer to Docker Engine plugin system.\n\nDocker Engine network plugins enable Engine deployments to be extended to support a wide range of networking technologies, such as VXLAN, IPVLAN, MACVLAN or something completely different. Network driver plugins are supported via the LibNetwork project. Each plugin is implemented as a \"remote driver\" for LibNetwork, which shares plugin infrastructure with Engine. Effectively, network driver plugins are activated in the same way as other plugins, and use the same kind of protocol.\n\nNetwork plugins and Swarm mode\n\nLegacy plugins do not work in Swarm mode. However, plugins written using the v2 plugin system do work in Swarm mode, as long as they are installed on each Swarm worker node.\n\nUse network driver plugins\n\nThe means of installing and running a network driver plugin depend on the particular plugin. So, be sure to install your plugin according to the instructions obtained from the plugin developer.\n\nOnce running however, network driver plugins are used just like the built-in network drivers: by being mentioned as a driver in network-oriented Docker commands. For example,\n\n$ docker network create --driver weave mynet\n\n\nSome network driver plugins are listed in plugins\n\nThe mynet network is now owned by weave, so subsequent commands referring to that network will be sent to the plugin,\n\n$ docker run --network=mynet busybox top\n\nFind network plugins\n\nNetwork plugins are written by third parties, and are published by those third parties, either on Docker Hub or on the third party's site.\n\nWrite a network plugin\n\nNetwork plugins implement the Docker plugin API and the network plugin protocol\n\nNetwork plugin protocol\n\nThe network driver protocol, in addition to the plugin activation call, is documented as part of libnetwork: https://github.com/moby/moby/blob/master/daemon/libnetwork/docs/remote.md.\n\nRequest changes\n\nTable of contents\nNetwork plugins and Swarm mode\nUse network driver plugins\nFind network plugins\nWrite a network plugin\nNetwork plugin protocol\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995554,
    "timestamp": "2026-02-07T06:33:19.896Z",
    "title": "Docker log driver plugins | Docker Docs",
    "url": "https://docs.docker.com/engine/extend/plugins_logging/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nAccess authorization plugin\nDocker log driver plugins\nDocker network driver plugins\nDocker Plugin API\nDocker volume plugins\nPlugin Config Version 1 of Plugin V2\nUse Docker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDocker Engine plugins\n/\nDocker log driver plugins\nDocker log driver plugins\nCopy as Markdown\n\nThis document describes logging driver plugins for Docker.\n\nLogging drivers enables users to forward container logs to another service for processing. Docker includes several logging drivers as built-ins, however can never hope to support all use-cases with built-in drivers. Plugins allow Docker to support a wide range of logging services without requiring to embed client libraries for these services in the main Docker codebase. See the plugin documentation for more information.\n\nCreate a logging plugin\n\nThe main interface for logging plugins uses the same JSON+HTTP RPC protocol used by other plugin types. See the example plugin for a reference implementation of a logging plugin. The example wraps the built-in jsonfilelog log driver.\n\nLogDriver protocol\n\nLogging plugins must register as a LogDriver during plugin activation. Once activated users can specify the plugin as a log driver.\n\nThere are two HTTP endpoints that logging plugins must implement:\n\n/LogDriver.StartLogging\n\nSignals to the plugin that a container is starting that the plugin should start receiving logs for.\n\nLogs will be streamed over the defined file in the request. On Linux this file is a FIFO. Logging plugins are not currently supported on Windows.\n\nRequest:\n\n{\n\n  \"File\": \"/path/to/file/stream\",\n\n  \"Info\": {\n\n          \"ContainerID\": \"123456\"\n\n  }\n\n}\n\nFile is the path to the log stream that needs to be consumed. Each call to StartLogging should provide a different file path, even if it's a container that the plugin has already received logs for prior. The file is created by Docker with a randomly generated name.\n\nInfo is details about the container that's being logged. This is fairly free-form, but is defined by the following struct definition:\n\ntype Info struct {\n\n\tConfig              map[string]string\n\n\tContainerID         string\n\n\tContainerName       string\n\n\tContainerEntrypoint string\n\n\tContainerArgs       []string\n\n\tContainerImageID    string\n\n\tContainerImageName  string\n\n\tContainerCreated    time.Time\n\n\tContainerEnv        []string\n\n\tContainerLabels     map[string]string\n\n\tLogPath             string\n\n\tDaemonName          string\n\n}\n\nContainerID will always be supplied with this struct, but other fields may be empty or missing.\n\nResponse:\n\n{\n\n  \"Err\": \"\"\n\n}\n\nIf an error occurred during this request, add an error message to the Err field in the response. If no error then you can either send an empty response ({}) or an empty value for the Err field.\n\nThe driver should at this point be consuming log messages from the passed in file. If messages are unconsumed, it may cause the container to block while trying to write to its stdio streams.\n\nLog stream messages are encoded as protocol buffers. The protobuf definitions are in the moby repository.\n\nSince protocol buffers are not self-delimited you must decode them from the stream using the following stream format:\n\n[size][message]\n\nWhere size is a 4-byte big endian binary encoded uint32. size in this case defines the size of the next message. message is the actual log entry.\n\nA reference golang implementation of a stream encoder/decoder can be found here\n\n/LogDriver.StopLogging\n\nSignals to the plugin to stop collecting logs from the defined file. Once a response is received, the file will be removed by Docker. You must make sure to collect all logs on the stream before responding to this request or risk losing log data.\n\nRequests on this endpoint does not mean that the container has been removed only that it has stopped.\n\nRequest:\n\n{\n\n  \"File\": \"/path/to/file/stream\"\n\n}\n\nResponse:\n\n{\n\n  \"Err\": \"\"\n\n}\n\nIf an error occurred during this request, add an error message to the Err field in the response. If no error then you can either send an empty response ({}) or an empty value for the Err field.\n\nOptional endpoints\n\nLogging plugins can implement two extra logging endpoints:\n\n/LogDriver.Capabilities\n\nDefines the capabilities of the log driver. You must implement this endpoint for Docker to be able to take advantage of any of the defined capabilities.\n\nRequest:\n\n{}\n\nResponse:\n\n{\n\n  \"ReadLogs\": true\n\n}\n\nSupported capabilities:\n\nReadLogs - this tells Docker that the plugin is capable of reading back logs to clients. Plugins that report that they support ReadLogs must implement the /LogDriver.ReadLogs endpoint\n/LogDriver.ReadLogs\n\nReads back logs to the client. This is used when docker logs <container> is called.\n\nIn order for Docker to use this endpoint, the plugin must specify as much when /LogDriver.Capabilities is called.\n\nRequest:\n\n{\n\n  \"ReadConfig\": {},\n\n  \"Info\": {\n\n    \"ContainerID\": \"123456\"\n\n  }\n\n}\n\nReadConfig is the list of options for reading, it is defined with the following golang struct:\n\ntype ReadConfig struct {\n\n\tSince  time.Time\n\n\tTail   int\n\n\tFollow bool\n\n}\nSince defines the oldest log that should be sent.\nTail defines the number of lines to read (e.g. like the command tail -n 10)\nFollow signals that the client wants to stay attached to receive new log messages as they come in once the existing logs have been read.\n\nInfo is the same type defined in /LogDriver.StartLogging. It should be used to determine what set of logs to read.\n\nResponse:\n\n{{ log stream }}\n\nThe response should be the encoded log message using the same format as the messages that the plugin consumed from Docker.\n\nRequest changes\n\nTable of contents\nCreate a logging plugin\nLogDriver protocol\n/LogDriver.StartLogging\n/LogDriver.StopLogging\nOptional endpoints\n/LogDriver.Capabilities\n/LogDriver.ReadLogs\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995560,
    "timestamp": "2026-02-07T06:33:19.902Z",
    "title": "Docker Plugin API | Docker Docs",
    "url": "https://docs.docker.com/engine/extend/plugin_api/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nAccess authorization plugin\nDocker log driver plugins\nDocker network driver plugins\nDocker Plugin API\nDocker volume plugins\nPlugin Config Version 1 of Plugin V2\nUse Docker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDocker Engine plugins\n/\nDocker Plugin API\nDocker Plugin API\nCopy as Markdown\n\nDocker plugins are out-of-process extensions which add capabilities to the Docker Engine.\n\nThis document describes the Docker Engine plugin API. To view information on plugins managed by Docker Engine, refer to Docker Engine plugin system.\n\nThis page is intended for people who want to develop their own Docker plugin. If you just want to learn about or use Docker plugins, look here.\n\nWhat plugins are\n\nA plugin is a process running on the same or a different host as the Docker daemon, which registers itself by placing a file on the daemon host in one of the plugin directories described in Plugin discovery.\n\nPlugins have human-readable names, which are short, lowercase strings. For example, flocker or weave.\n\nPlugins can run inside or outside containers. Currently running them outside containers is recommended.\n\nPlugin discovery\n\nDocker discovers plugins by looking for them in the plugin directory whenever a user or container tries to use one by name.\n\nThere are three types of files which can be put in the plugin directory.\n\n.sock files are Unix domain sockets.\n.spec files are text files containing a URL, such as unix:///other.sock or tcp://localhost:8080.\n.json files are text files containing a full json specification for the plugin.\n\nPlugins with Unix domain socket files must run on the same host as the Docker daemon. Plugins with .spec or .json files can run on a different host if you specify a remote URL.\n\nUnix domain socket files must be located under /run/docker/plugins, whereas spec files can be located either under /etc/docker/plugins or /usr/lib/docker/plugins.\n\nThe name of the file (excluding the extension) determines the plugin name.\n\nFor example, the flocker plugin might create a Unix socket at /run/docker/plugins/flocker.sock.\n\nYou can define each plugin into a separated subdirectory if you want to isolate definitions from each other. For example, you can create the flocker socket under /run/docker/plugins/flocker/flocker.sock and only mount /run/docker/plugins/flocker inside the flocker container.\n\nDocker always searches for Unix sockets in /run/docker/plugins first. It checks for spec or json files under /etc/docker/plugins and /usr/lib/docker/plugins if the socket doesn't exist. The directory scan stops as soon as it finds the first plugin definition with the given name.\n\nJSON specification\n\nThis is the JSON format for a plugin:\n\n{\n\n  \"Name\": \"plugin-example\",\n\n  \"Addr\": \"https://example.com/docker/plugin\",\n\n  \"TLSConfig\": {\n\n    \"InsecureSkipVerify\": false,\n\n    \"CAFile\": \"/usr/shared/docker/certs/example-ca.pem\",\n\n    \"CertFile\": \"/usr/shared/docker/certs/example-cert.pem\",\n\n    \"KeyFile\": \"/usr/shared/docker/certs/example-key.pem\"\n\n  }\n\n}\n\nThe TLSConfig field is optional and TLS will only be verified if this configuration is present.\n\nPlugin lifecycle\n\nPlugins should be started before Docker, and stopped after Docker. For example, when packaging a plugin for a platform which supports systemd, you might use systemd dependencies to manage startup and shutdown order.\n\nWhen upgrading a plugin, you should first stop the Docker daemon, upgrade the plugin, then start Docker again.\n\nPlugin activation\n\nWhen a plugin is first referred to -- either by a user referring to it by name (e.g. docker run --volume-driver=foo) or a container already configured to use a plugin being started -- Docker looks for the named plugin in the plugin directory and activates it with a handshake. See Handshake API below.\n\nPlugins are not activated automatically at Docker daemon startup. Rather, they are activated only lazily, or on-demand, when they are needed.\n\nSystemd socket activation\n\nPlugins may also be socket activated by systemd. The official Plugins helpers natively supports socket activation. In order for a plugin to be socket activated it needs a service file and a socket file.\n\nThe service file (for example /lib/systemd/system/your-plugin.service):\n\n[Unit]\n\nDescription=Your plugin\n\nBefore=docker.service\n\nAfter=network.target your-plugin.socket\n\nRequires=your-plugin.socket docker.service\n\n\n\n[Service]\n\nExecStart=/usr/lib/docker/your-plugin\n\n\n\n[Install]\n\nWantedBy=multi-user.target\n\nThe socket file (for example /lib/systemd/system/your-plugin.socket):\n\n[Unit]\n\nDescription=Your plugin\n\n\n\n[Socket]\n\nListenStream=/run/docker/plugins/your-plugin.sock\n\n\n\n[Install]\n\nWantedBy=sockets.target\n\nThis will allow plugins to be actually started when the Docker daemon connects to the sockets they're listening on (for instance the first time the daemon uses them or if one of the plugin goes down accidentally).\n\nAPI design\n\nThe Plugin API is RPC-style JSON over HTTP, much like webhooks.\n\nRequests flow from the Docker daemon to the plugin. The plugin needs to implement an HTTP server and bind this to the Unix socket mentioned in the \"plugin discovery\" section.\n\nAll requests are HTTP POST requests.\n\nThe API is versioned via an Accept header, which currently is always set to application/vnd.docker.plugins.v1+json.\n\nHandshake API\n\nPlugins are activated via the following \"handshake\" API call.\n\n/Plugin.Activate\n\nRequest: empty body\n\nResponse:\n\n{\n\n    \"Implements\": [\"VolumeDriver\"]\n\n}\n\nResponds with a list of Docker subsystems which this plugin implements. After activation, the plugin will then be sent events from this subsystem.\n\nPossible values are:\n\nauthz\nNetworkDriver\nVolumeDriver\nPlugin retries\n\nAttempts to call a method on a plugin are retried with an exponential backoff for up to 30 seconds. This may help when packaging plugins as containers, since it gives plugin containers a chance to start up before failing any user containers which depend on them.\n\nPlugins helpers\n\nTo ease plugins development, we're providing an sdk for each kind of plugins currently supported by Docker at docker/go-plugins-helpers.\n\nRequest changes\n\nTable of contents\nWhat plugins are\nPlugin discovery\nJSON specification\nPlugin lifecycle\nPlugin activation\nSystemd socket activation\nAPI design\nHandshake API\n/Plugin.Activate\nPlugin retries\nPlugins helpers\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995566,
    "timestamp": "2026-02-07T06:33:19.902Z",
    "title": "Plugin Config Version 1 of Plugin V2 | Docker Docs",
    "url": "https://docs.docker.com/engine/extend/config/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nAccess authorization plugin\nDocker log driver plugins\nDocker network driver plugins\nDocker Plugin API\nDocker volume plugins\nPlugin Config Version 1 of Plugin V2\nUse Docker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDocker Engine plugins\n/\nPlugin Config Version 1 of Plugin V2\nPlugin Config Version 1 of Plugin V2\nCopy as Markdown\n\nThis document outlines the format of the V0 plugin configuration.\n\nPlugin configs describe the various constituents of a Docker engine plugin. Plugin configs can be serialized to JSON format with the following media types:\n\nConfig Type\tMedia Type\nconfig\tapplication/vnd.docker.plugin.v1+json\nConfig Field Descriptions\n\nConfig provides the base accessible fields for working with V0 plugin format in the registry.\n\ndescription string\n\nDescription of the plugin\n\ndocumentation string\n\nLink to the documentation about the plugin\n\ninterface PluginInterface\n\nInterface implemented by the plugins, struct consisting of the following fields:\n\ntypes string array\n\nTypes indicate what interface(s) the plugin currently implements.\n\nSupported types:\n\ndocker.volumedriver/1.0\n\ndocker.networkdriver/1.0\n\ndocker.ipamdriver/1.0\n\ndocker.authz/1.0\n\ndocker.logdriver/1.0\n\ndocker.metricscollector/1.0\n\nsocket string\n\nSocket is the name of the socket the engine should use to communicate with the plugins. the socket will be created in /run/docker/plugins.\n\nentrypoint string array\n\nEntrypoint of the plugin, see ENTRYPOINT\n\nworkdir string\n\nWorking directory of the plugin, see WORKDIR\n\nnetwork PluginNetwork\n\nNetwork of the plugin, struct consisting of the following fields:\n\ntype string\n\nNetwork type.\n\nSupported types:\n\nbridge\nhost\nnone\n\nmounts PluginMount array\n\nMount of the plugin, struct consisting of the following fields. See MOUNTS.\n\nname string\n\nName of the mount.\n\ndescription string\n\nDescription of the mount.\n\nsource string\n\nSource of the mount.\n\ndestination string\n\nDestination of the mount.\n\ntype string\n\nMount type.\n\noptions string array\n\nOptions of the mount.\n\nipchost Boolean\n\nAccess to host ipc namespace.\n\npidhost Boolean\n\nAccess to host PID namespace.\n\npropagatedMount string\n\nPath to be mounted as rshared, so that mounts under that path are visible to Docker. This is useful for volume plugins. This path will be bind-mounted outside of the plugin rootfs so it's contents are preserved on upgrade.\n\nenv PluginEnv array\n\nEnvironment variables of the plugin, struct consisting of the following fields:\n\nname string\n\nName of the environment variable.\n\ndescription string\n\nDescription of the environment variable.\n\nvalue string\n\nValue of the environment variable.\n\nargs PluginArgs\n\nArguments of the plugin, struct consisting of the following fields:\n\nname string\n\nName of the arguments.\n\ndescription string\n\nDescription of the arguments.\n\nvalue string array\n\nValues of the arguments.\n\nlinux PluginLinux\n\ncapabilities string array\n\nCapabilities of the plugin (Linux only), see list here\n\nallowAllDevices Boolean\n\nIf /dev is bind mounted from the host, and allowAllDevices is set to true, the plugin will have rwm access to all devices on the host.\n\ndevices PluginDevice array\n\nDevice of the plugin, (Linux only), struct consisting of the following fields. See DEVICES.\n\nname string\n\nName of the device.\n\ndescription string\n\nDescription of the device.\n\npath string\n\nPath of the device.\n\nExample Config\n\nThe following example shows the 'tiborvass/sample-volume-plugin' plugin config.\n\n{\n\n  \"Args\": {\n\n    \"Description\": \"\",\n\n    \"Name\": \"\",\n\n    \"Settable\": null,\n\n    \"Value\": null\n\n  },\n\n  \"Description\": \"A sample volume plugin for Docker\",\n\n  \"Documentation\": \"https://docs.docker.com/engine/extend/plugins/\",\n\n  \"Entrypoint\": [\n\n    \"/usr/bin/sample-volume-plugin\",\n\n    \"/data\"\n\n  ],\n\n  \"Env\": [\n\n    {\n\n      \"Description\": \"\",\n\n      \"Name\": \"DEBUG\",\n\n      \"Settable\": [\n\n        \"value\"\n\n      ],\n\n      \"Value\": \"0\"\n\n    }\n\n  ],\n\n  \"Interface\": {\n\n    \"Socket\": \"plugin.sock\",\n\n    \"Types\": [\n\n      \"docker.volumedriver/1.0\"\n\n    ]\n\n  },\n\n  \"Linux\": {\n\n    \"Capabilities\": null,\n\n    \"AllowAllDevices\": false,\n\n    \"Devices\": null\n\n  },\n\n  \"Mounts\": null,\n\n  \"Network\": {\n\n    \"Type\": \"\"\n\n  },\n\n  \"PropagatedMount\": \"/data\",\n\n  \"User\": {},\n\n  \"Workdir\": \"\"\n\n}\n\nRequest changes\n\nTable of contents\nConfig Field Descriptions\nExample Config\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995563,
    "timestamp": "2026-02-07T06:33:19.902Z",
    "title": "Docker volume plugins | Docker Docs",
    "url": "https://docs.docker.com/engine/extend/plugins_volume/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nAccess authorization plugin\nDocker log driver plugins\nDocker network driver plugins\nDocker Plugin API\nDocker volume plugins\nPlugin Config Version 1 of Plugin V2\nUse Docker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDocker Engine plugins\n/\nDocker volume plugins\nDocker volume plugins\nCopy as Markdown\n\nDocker Engine volume plugins enable Engine deployments to be integrated with external storage systems such as Amazon EBS, and enable data volumes to persist beyond the lifetime of a single Docker host. See the plugin documentation for more information.\n\nChangelog\n1.13.0\nIf used as part of the v2 plugin architecture, mountpoints that are part of paths returned by the plugin must be mounted under the directory specified by PropagatedMount in the plugin configuration (#26398)\n1.12.0\nAdd Status field to VolumeDriver.Get response (#21006)\nAdd VolumeDriver.Capabilities to get capabilities of the volume driver (#22077)\n1.10.0\nAdd VolumeDriver.Get which gets the details about the volume (#16534)\nAdd VolumeDriver.List which lists all volumes owned by the driver (#16534)\n1.8.0\nInitial support for volume driver plugins (#14659)\nCommand-line changes\n\nTo give a container access to a volume, use the --volume and --volume-driver flags on the docker container run command. The --volume (or -v) flag accepts a volume name and path on the host, and the --volume-driver flag accepts a driver type.\n\n$ docker volume create --driver=flocker volumename\n\n\n\n$ docker container run -it --volume volumename:/data busybox sh\n\n--volume\n\nThe --volume (or -v) flag takes a value that is in the format <volume_name>:<mountpoint>. The two parts of the value are separated by a colon (:) character.\n\nThe volume name is a human-readable name for the volume, and cannot begin with a / character. It is referred to as volume_name in the rest of this topic.\nThe Mountpoint is the path on the host (v1) or in the plugin (v2) where the volume has been made available.\nvolumedriver\n\nSpecifying a volumedriver in conjunction with a volumename allows you to use plugins such as Flocker to manage volumes external to a single host, such as those on EBS.\n\nCreate a VolumeDriver\n\nThe container creation endpoint (/containers/create) accepts a VolumeDriver field of type string allowing to specify the name of the driver. If not specified, it defaults to \"local\" (the default driver for local volumes).\n\nVolume plugin protocol\n\nIf a plugin registers itself as a VolumeDriver when activated, it must provide the Docker Daemon with writeable paths on the host filesystem. The Docker daemon provides these paths to containers to consume. The Docker daemon makes the volumes available by bind-mounting the provided paths into the containers.\n\nNote\n\nVolume plugins should not write data to the /var/lib/docker/ directory, including /var/lib/docker/volumes. The /var/lib/docker/ directory is reserved for Docker.\n\n/VolumeDriver.Create\n\nRequest:\n\n{\n\n    \"Name\": \"volume_name\",\n\n    \"Opts\": {}\n\n}\n\nInstruct the plugin that the user wants to create a volume, given a user specified volume name. The plugin does not need to actually manifest the volume on the filesystem yet (until Mount is called). Opts is a map of driver specific options passed through from the user request.\n\nResponse:\n\n{\n\n    \"Err\": \"\"\n\n}\n\nRespond with a string error if an error occurred.\n\n/VolumeDriver.Remove\n\nRequest:\n\n{\n\n    \"Name\": \"volume_name\"\n\n}\n\nDelete the specified volume from disk. This request is issued when a user invokes docker rm -v to remove volumes associated with a container.\n\nResponse:\n\n{\n\n    \"Err\": \"\"\n\n}\n\nRespond with a string error if an error occurred.\n\n/VolumeDriver.Mount\n\nRequest:\n\n{\n\n    \"Name\": \"volume_name\",\n\n    \"ID\": \"b87d7442095999a92b65b3d9691e697b61713829cc0ffd1bb72e4ccd51aa4d6c\"\n\n}\n\nDocker requires the plugin to provide a volume, given a user specified volume name. Mount is called once per container start. If the same volume_name is requested more than once, the plugin may need to keep track of each new mount request and provision at the first mount request and deprovision at the last corresponding unmount request.\n\nID is a unique ID for the caller that is requesting the mount.\n\nResponse:\n\nv1\n\n{\n\n    \"Mountpoint\": \"/path/to/directory/on/host\",\n\n    \"Err\": \"\"\n\n}\n\nv2\n\n{\n\n    \"Mountpoint\": \"/path/under/PropagatedMount\",\n\n    \"Err\": \"\"\n\n}\n\nMountpoint is the path on the host (v1) or in the plugin (v2) where the volume has been made available.\n\nErr is either empty or contains an error string.\n\n/VolumeDriver.Path\n\nRequest:\n\n{\n\n    \"Name\": \"volume_name\"\n\n}\n\nRequest the path to the volume with the given volume_name.\n\nResponse:\n\nv1\n\n{\n\n    \"Mountpoint\": \"/path/to/directory/on/host\",\n\n    \"Err\": \"\"\n\n}\n\nv2\n\n{\n\n    \"Mountpoint\": \"/path/under/PropagatedMount\",\n\n    \"Err\": \"\"\n\n}\n\nRespond with the path on the host (v1) or inside the plugin (v2) where the volume has been made available, and/or a string error if an error occurred.\n\nMountpoint is optional. However, the plugin may be queried again later if one is not provided.\n\n/VolumeDriver.Unmount\n\nRequest:\n\n{\n\n    \"Name\": \"volume_name\",\n\n    \"ID\": \"b87d7442095999a92b65b3d9691e697b61713829cc0ffd1bb72e4ccd51aa4d6c\"\n\n}\n\nDocker is no longer using the named volume. Unmount is called once per container stop. Plugin may deduce that it is safe to deprovision the volume at this point.\n\nID is a unique ID for the caller that is requesting the mount.\n\nResponse:\n\n{\n\n    \"Err\": \"\"\n\n}\n\nRespond with a string error if an error occurred.\n\n/VolumeDriver.Get\n\nRequest:\n\n{\n\n    \"Name\": \"volume_name\"\n\n}\n\nGet info about volume_name.\n\nResponse:\n\nv1\n\n{\n\n  \"Volume\": {\n\n    \"Name\": \"volume_name\",\n\n    \"Mountpoint\": \"/path/to/directory/on/host\",\n\n    \"Status\": {}\n\n  },\n\n  \"Err\": \"\"\n\n}\n\nv2\n\n{\n\n  \"Volume\": {\n\n    \"Name\": \"volume_name\",\n\n    \"Mountpoint\": \"/path/under/PropagatedMount\",\n\n    \"Status\": {}\n\n  },\n\n  \"Err\": \"\"\n\n}\n\nRespond with a string error if an error occurred. Mountpoint and Status are optional.\n\n/VolumeDriver.List\n\nRequest:\n\n{}\n\nGet the list of volumes registered with the plugin.\n\nResponse:\n\nv1\n\n{\n\n  \"Volumes\": [\n\n    {\n\n      \"Name\": \"volume_name\",\n\n      \"Mountpoint\": \"/path/to/directory/on/host\"\n\n    }\n\n  ],\n\n  \"Err\": \"\"\n\n}\n\nv2\n\n{\n\n  \"Volumes\": [\n\n    {\n\n      \"Name\": \"volume_name\",\n\n      \"Mountpoint\": \"/path/under/PropagatedMount\"\n\n    }\n\n  ],\n\n  \"Err\": \"\"\n\n}\n\nRespond with a string error if an error occurred. Mountpoint is optional.\n\n/VolumeDriver.Capabilities\n\nRequest:\n\n{}\n\nGet the list of capabilities the driver supports.\n\nThe driver is not required to implement Capabilities. If it is not implemented, the default values are used.\n\nResponse:\n\n{\n\n  \"Capabilities\": {\n\n    \"Scope\": \"global\"\n\n  }\n\n}\n\nSupported scopes are global and local. Any other value in Scope will be ignored, and local is used. Scope allows cluster managers to handle the volume in different ways. For instance, a scope of global, signals to the cluster manager that it only needs to create the volume once instead of on each Docker host. More capabilities may be added in the future.\n\nRequest changes\n\nTable of contents\nChangelog\n1.13.0\n1.12.0\n1.10.0\n1.8.0\nCommand-line changes\n--volume\nvolumedriver\nCreate a VolumeDriver\nVolume plugin protocol\n/VolumeDriver.Create\n/VolumeDriver.Remove\n/VolumeDriver.Mount\n/VolumeDriver.Path\n/VolumeDriver.Unmount\n/VolumeDriver.Get\n/VolumeDriver.List\n/VolumeDriver.Capabilities\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995569,
    "timestamp": "2026-02-07T06:33:19.908Z",
    "title": "Use Docker Engine plugins | Docker Docs",
    "url": "https://docs.docker.com/engine/extend/legacy_plugins/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nInstall\nStorage\nNetworking\nContainers\nCLI\nDaemon\nManage resources\nLogs and metrics\nSecurity\nSwarm mode\nDeprecated features\nDocker Engine plugins\nAccess authorization plugin\nDocker log driver plugins\nDocker network driver plugins\nDocker Plugin API\nDocker volume plugins\nPlugin Config Version 1 of Plugin V2\nUse Docker Engine plugins\nRelease notes\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Engine\n/\nDocker Engine plugins\n/\nUse Docker Engine plugins\nUse Docker Engine plugins\nCopy as Markdown\n\nThis document describes the Docker Engine plugins generally available in Docker Engine. To view information on plugins managed by Docker, refer to Docker Engine plugin system.\n\nYou can extend the capabilities of the Docker Engine by loading third-party plugins. This page explains the types of plugins and provides links to several volume and network plugins for Docker.\n\nTypes of plugins\n\nPlugins extend Docker's functionality. They come in specific types. For example, a volume plugin might enable Docker volumes to persist across multiple Docker hosts and a network plugin might provide network plumbing.\n\nCurrently Docker supports authorization, volume and network driver plugins. In the future it will support additional plugin types.\n\nInstalling a plugin\n\nFollow the instructions in the plugin's documentation.\n\nFinding a plugin\n\nThe sections below provide an overview of available third-party plugins.\n\nNetwork plugins\nPlugin\tDescription\nContiv Networking\tAn open source network plugin to provide infrastructure and security policies for a multi-tenant micro services deployment, while providing an integration to physical network for non-container workload. Contiv Networking implements the remote driver and IPAM APIs available in Docker 1.9 onwards.\nKuryr Network Plugin\tA network plugin is developed as part of the OpenStack Kuryr project and implements the Docker networking (libnetwork) remote driver API by utilizing Neutron, the OpenStack networking service. It includes an IPAM driver as well.\nKathar√° Network Plugin\tDocker Network Plugin used by Kathar√°, an open source container-based network emulation system for showing interactive demos/lessons, testing production networks in a sandbox environment, or developing new network protocols.\nVolume plugins\nPlugin\tDescription\nAzure File Storage plugin\tLets you mount Microsoft Azure File Storage shares to Docker containers as volumes using the SMB 3.0 protocol. Learn more.\nBeeGFS Volume Plugin\tAn open source volume plugin to create persistent volumes in a BeeGFS parallel file system.\nBlockbridge plugin\tA volume plugin that provides access to an extensible set of container-based persistent storage options. It supports single and multi-host Docker environments with features that include tenant isolation, automated provisioning, encryption, secure deletion, snapshots and QoS.\nContiv Volume Plugin\tAn open source volume plugin that provides multi-tenant, persistent, distributed storage with intent based consumption. It has support for Ceph and NFS.\nConvoy plugin\tA volume plugin for a variety of storage back-ends including device mapper and NFS. It's a simple standalone executable written in Go and provides the framework to support vendor-specific extensions such as snapshots, backups and restore.\nDigitalOcean Block Storage plugin\tIntegrates DigitalOcean's block storage solution into the Docker ecosystem by automatically attaching a given block storage volume to a DigitalOcean droplet and making the contents of the volume available to Docker containers running on that droplet.\nDRBD plugin\tA volume plugin that provides highly available storage replicated by DRBD. Data written to the docker volume is replicated in a cluster of DRBD nodes.\nFlocker plugin\tA volume plugin that provides multi-host portable volumes for Docker, enabling you to run databases and other stateful containers and move them around across a cluster of machines.\nFuxi Volume Plugin\tA volume plugin that is developed as part of the OpenStack Kuryr project and implements the Docker volume plugin API by utilizing Cinder, the OpenStack block storage service.\ngce-docker plugin\tA volume plugin able to attach, format and mount Google Compute persistent-disks.\nGlusterFS plugin\tA volume plugin that provides multi-host volumes management for Docker using GlusterFS.\nHorcrux Volume Plugin\tA volume plugin that allows on-demand, version controlled access to your data. Horcrux is an open-source plugin, written in Go, and supports SCP, Minio and Amazon S3.\nHPE 3Par Volume Plugin\tA volume plugin that supports HPE 3Par and StoreVirtual iSCSI storage arrays.\nInfinit volume plugin\tA volume plugin that makes it easy to mount and manage Infinit volumes using Docker.\nIPFS Volume Plugin\tAn open source volume plugin that allows using an ipfs filesystem as a volume.\nKeywhiz plugin\tA plugin that provides credentials and secret management using Keywhiz as a central repository.\nLinode Volume Plugin\tA plugin that adds the ability to manage Linode Block Storage as Docker Volumes from within a Linode.\nLocal Persist Plugin\tA volume plugin that extends the default local driver's functionality by allowing you specify a mountpoint anywhere on the host, which enables the files to always persist, even if the volume is removed via docker volume rm.\nNetApp Plugin (nDVP)\tA volume plugin that provides direct integration with the Docker ecosystem for the NetApp storage portfolio. The nDVP package supports the provisioning and management of storage resources from the storage platform to Docker hosts, with a robust framework for adding additional platforms in the future.\nNetshare plugin\tA volume plugin that provides volume management for NFS 3/4, AWS EFS and CIFS file systems.\nNimble Storage Volume Plugin\tA volume plug-in that integrates with Nimble Storage Unified Flash Fabric arrays. The plug-in abstracts array volume capabilities to the Docker administrator to allow self-provisioning of secure multi-tenant volumes and clones.\nOpenStorage Plugin\tA cluster-aware volume plugin that provides volume management for file and block storage solutions. It implements a vendor neutral specification for implementing extensions such as CoS, encryption, and snapshots. It has example drivers based on FUSE, NFS, NBD and EBS to name a few.\nPortworx Volume Plugin\tA volume plugin that turns any server into a scale-out converged compute/storage node, providing container granular storage and highly available volumes across any node, using a shared-nothing storage backend that works with any docker scheduler.\nQuobyte Volume Plugin\tA volume plugin that connects Docker to Quobyte's data center file system, a general-purpose scalable and fault-tolerant storage platform.\nREX-Ray plugin\tA volume plugin which is written in Go and provides advanced storage functionality for many platforms including VirtualBox, EC2, Google Compute Engine, OpenStack, and EMC.\nVirtuozzo Storage and Ploop plugin\tA volume plugin with support for Virtuozzo Storage distributed cloud file system as well as ploop devices.\nVMware vSphere Storage Plugin\tDocker Volume Driver for vSphere enables customers to address persistent storage requirements for Docker containers in vSphere environments.\nAuthorization plugins\nPlugin\tDescription\nCasbin AuthZ Plugin\tAn authorization plugin based on Casbin, which supports access control models like ACL, RBAC, ABAC. The access control model can be customized. The policy can be persisted into file or DB.\nHBM plugin\tAn authorization plugin that prevents from executing commands with certains parameters.\nTwistlock AuthZ Broker\tA basic extendable authorization plugin that runs directly on the host or inside a container. This plugin allows you to define user policies that it evaluates during authorization. Basic authorization is provided if Docker daemon is started with the --tlsverify flag (username is extracted from the certificate common name).\nTroubleshooting a plugin\n\nIf you are having problems with Docker after loading a plugin, ask the authors of the plugin for help. The Docker team may not be able to assist you.\n\nWriting a plugin\n\nIf you are interested in writing a plugin for Docker, or seeing how they work under the hood, see the Docker plugins reference.\n\nRequest changes\n\nTable of contents\nTypes of plugins\nInstalling a plugin\nFinding a plugin\nNetwork plugins\nVolume plugins\nAuthorization plugins\nTroubleshooting a plugin\nWriting a plugin\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995624,
    "timestamp": "2026-02-07T06:34:33.228Z",
    "title": "Docker Build Overview | Docker Docs",
    "url": "https://docs.docker.com/build/concepts/overview/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nDocker Build Overview\nDockerfile overview\nBuild context\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCore concepts\n/\nDocker Build Overview\nDocker Build Overview\nCopy as Markdown\n\nDocker Build implements a client-server architecture, where:\n\nClient: Buildx is the client and the user interface for running and managing builds.\nServer: BuildKit is the server, or builder, that handles the build execution.\n\nWhen you invoke a build, the Buildx client sends a build request to the BuildKit backend. BuildKit resolves the build instructions and executes the build steps. The build output is either sent back to the client or uploaded to a registry, such as Docker Hub.\n\nBuildx and BuildKit are both installed with Docker Desktop and Docker Engine out-of-the-box. When you invoke the docker build command, you're using Buildx to run a build using the default BuildKit bundled with Docker.\n\nBuildx\n\nBuildx is the CLI tool that you use to run builds. The docker build command is a wrapper around Buildx. When you invoke docker build, Buildx interprets the build options and sends a build request to the BuildKit backend.\n\nThe Buildx client can do more than just run builds. You can also use Buildx to create and manage BuildKit backends, referred to as builders. It also supports features for managing images in registries, and for running multiple builds concurrently.\n\nDocker Buildx is installed by default with Docker Desktop. You can also build the CLI plugin from source, or grab a binary from the GitHub repository and install it manually. See Buildx README on GitHub for more information.\n\nNote\n\nWhile docker build invokes Buildx under the hood, there are subtle differences between this command and the canonical docker buildx build. For details, see Difference between docker build and docker buildx build.\n\nBuildKit\n\nBuildKit is the daemon process that executes the build workloads.\n\nA build execution starts with the invocation of a docker build command. Buildx interprets your build command and sends a build request to the BuildKit backend. The build request includes:\n\nThe Dockerfile\nBuild arguments\nExport options\nCaching options\n\nBuildKit resolves the build instructions and executes the build steps. While BuildKit is executing the build, Buildx monitors the build status and prints the progress to the terminal.\n\nIf the build requires resources from the client, such as local files or build secrets, BuildKit requests the resources that it needs from Buildx.\n\nThis is one way in which BuildKit is more efficient compared to the legacy builder used in earlier versions of Docker. BuildKit only requests the resources that the build needs when they're needed. The legacy builder, in comparison, always takes a copy of the local filesystem.\n\nExamples of resources that BuildKit can request from Buildx include:\n\nLocal filesystem build contexts\nBuild secrets\nSSH sockets\nRegistry authentication tokens\n\nFor more information about BuildKit, see BuildKit.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nBuildx\nBuildKit\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995627,
    "timestamp": "2026-02-07T06:34:33.229Z",
    "title": "Dockerfile overview | Docker Docs",
    "url": "https://docs.docker.com/build/concepts/dockerfile/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nDocker Build Overview\nDockerfile overview\nBuild context\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCore concepts\n/\nDockerfile overview\nDockerfile overview\nCopy as Markdown\nDockerfile\n\nIt all starts with a Dockerfile.\n\nDocker builds images by reading the instructions from a Dockerfile. A Dockerfile is a text file containing instructions for building your source code. The Dockerfile instruction syntax is defined by the specification reference in the Dockerfile reference.\n\nHere are the most common types of instructions:\n\nInstruction\tDescription\nFROM <image>\tDefines a base for your image.\nRUN <command>\tExecutes any commands in a new layer on top of the current image and commits the result. RUN also has a shell form for running commands.\nWORKDIR <directory>\tSets the working directory for any RUN, CMD, ENTRYPOINT, COPY, and ADD instructions that follow it in the Dockerfile.\nCOPY <src> <dest>\tCopies new files or directories from <src> and adds them to the filesystem of the container at the path <dest>.\nCMD <command>\tLets you define the default program that is run once you start the container based on this image. Each Dockerfile only has one CMD, and only the last CMD instance is respected when multiple exist.\n\nDockerfiles are crucial inputs for image builds and can facilitate automated, multi-layer image builds based on your unique configurations. Dockerfiles can start simple and grow with your needs to support more complex scenarios.\n\nFilename\n\nThe default filename to use for a Dockerfile is Dockerfile, without a file extension. Using the default name allows you to run the docker build command without having to specify additional command flags.\n\nSome projects may need distinct Dockerfiles for specific purposes. A common convention is to name these <something>.Dockerfile. You can specify the Dockerfile filename using the --file flag for the docker build command. Refer to the docker build CLI reference to learn about the --file flag.\n\nNote\n\nWe recommend using the default (Dockerfile) for your project's primary Dockerfile.\n\nDocker images\n\nDocker images consist of layers. Each layer is the result of a build instruction in the Dockerfile. Layers are stacked sequentially, and each one is a delta representing the changes applied to the previous layer.\n\nExample\n\nHere's what a typical workflow for building applications with Docker looks like.\n\nThe following example code shows a small \"Hello World\" application written in Python, using the Flask framework.\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n\n\n@app.route(\"/\")\n\ndef hello():\n\n    return \"Hello World!\"\n\nIn order to ship and deploy this application without Docker Build, you would need to make sure that:\n\nThe required runtime dependencies are installed on the server\nThe Python code gets uploaded to the server's filesystem\nThe server starts your application, using the necessary parameters\n\nThe following Dockerfile creates a container image, which has all the dependencies installed and that automatically starts your application.\n\n# syntax=docker/dockerfile:1\n\nFROM ubuntu:22.04\n\n\n\n# install app dependencies\n\nRUN apt-get update && apt-get install -y python3 python3-pip\n\nRUN pip install flask==3.0.*\n\n\n\n# install app\n\nCOPY hello.py /\n\n\n\n# final configuration\n\nENV FLASK_APP=hello\n\nEXPOSE 8000\n\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\nHere's a breakdown of what this Dockerfile does:\n\nDockerfile syntax\nBase image\nEnvironment setup\nComments\nInstalling dependencies\nCopying files\nSetting environment variables\nExposed ports\nStarting the application\nDockerfile syntax\n\nThe first line to add to a Dockerfile is a # syntax parser directive. While optional, this directive instructs the Docker builder what syntax to use when parsing the Dockerfile, and allows older Docker versions with BuildKit enabled to use a specific Dockerfile frontend before starting the build. Parser directives must appear before any other comment, whitespace, or Dockerfile instruction in your Dockerfile, and should be the first line in Dockerfiles.\n\n# syntax=docker/dockerfile:1\nTip\n\nWe recommend using docker/dockerfile:1, which always points to the latest release of the version 1 syntax. BuildKit automatically checks for updates of the syntax before building, making sure you are using the most current version.\n\nBase image\n\nThe line following the syntax directive defines what base image to use:\n\nFROM ubuntu:22.04\n\nThe FROM instruction sets your base image to the 22.04 release of Ubuntu. All instructions that follow are executed in this base image: an Ubuntu environment. The notation ubuntu:22.04, follows the name:tag standard for naming Docker images. When you build images, you use this notation to name your images. There are many public images you can leverage in your projects, by importing them into your build steps using the Dockerfile FROM instruction.\n\nDocker Hub contains a large set of official images that you can use for this purpose.\n\nEnvironment setup\n\nThe following line executes a build command inside the base image.\n\n# install app dependencies\n\nRUN apt-get update && apt-get install -y python3 python3-pip\n\nThis RUN instruction executes a shell in Ubuntu that updates the APT package index and installs Python tools in the container.\n\nComments\n\nNote the # install app dependencies line. This is a comment. Comments in Dockerfiles begin with the # symbol. As your Dockerfile evolves, comments can be instrumental to document how your Dockerfile works for any future readers and editors of the file, including your future self.\n\nNote\n\nYou might've noticed that comments are denoted using the same symbol as the syntax directive on the first line of the file. The symbol is only interpreted as a directive if the pattern matches a directive and appears at the beginning of the Dockerfile. Otherwise, it's treated as a comment.\n\nInstalling dependencies\n\nThe second RUN instruction installs the flask dependency required by the Python application.\n\nRUN pip install flask==3.0.*\n\nA prerequisite for this instruction is that pip is installed into the build container. The first RUN command installs pip, which ensures that we can use the command to install the flask web framework.\n\nCopying files\n\nThe next instruction uses the COPY instruction to copy the hello.py file from the local build context into the root directory of our image.\n\nCOPY hello.py /\n\nA build context is the set of files that you can access in Dockerfile instructions such as COPY and ADD.\n\nAfter the COPY instruction, the hello.py file is added to the filesystem of the build container.\n\nSetting environment variables\n\nIf your application uses environment variables, you can set environment variables in your Docker build using the ENV instruction.\n\nENV FLASK_APP=hello\n\nThis sets a Linux environment variable we'll need later. Flask, the framework used in this example, uses this variable to start the application. Without this, flask wouldn't know where to find our application to be able to run it.\n\nExposed ports\n\nThe EXPOSE instruction marks that our final image has a service listening on port 8000.\n\nEXPOSE 8000\n\nThis instruction isn't required, but it is a good practice and helps tools and team members understand what this application is doing.\n\nStarting the application\n\nFinally, CMD instruction sets the command that is run when the user starts a container based on this image.\n\nCMD [\"flask\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\nThis command starts the flask development server listening on all addresses on port 8000. The example here uses the \"exec form\" version of CMD. It's also possible to use the \"shell form\":\n\nCMD flask run --host 0.0.0.0 --port 8000\n\nThere are subtle differences between these two versions, for example in how they trap signals like SIGTERM and SIGKILL. For more information about these differences, see Shell and exec form\n\nBuilding\n\nTo build a container image using the Dockerfile example from the previous section, you use the docker build command:\n\n$ docker build -t test:latest .\n\n\nThe -t test:latest option specifies the name and tag of the image.\n\nThe single dot (.) at the end of the command sets the build context to the current directory. This means that the build expects to find the Dockerfile and the hello.py file in the directory where the command is invoked. If those files aren't there, the build fails.\n\nAfter the image has been built, you can run the application as a container with docker run, specifying the image name:\n\n$ docker run -p 127.0.0.1:8000:8000 test:latest\n\n\nThis publishes the container's port 8000 to http://localhost:8000 on the Docker host.\n\nTip\n\nTo improve linting, code navigation, and vulnerability scanning of your Dockerfiles in Visual Studio Code see Docker VS Code Extension.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDockerfile\nFilename\nDocker images\nExample\nDockerfile syntax\nBase image\nEnvironment setup\nComments\nInstalling dependencies\nCopying files\nSetting environment variables\nExposed ports\nStarting the application\nBuilding\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995630,
    "timestamp": "2026-02-07T06:34:33.233Z",
    "title": "Build context | Docker Docs",
    "url": "https://docs.docker.com/build/concepts/context/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nDocker Build Overview\nDockerfile overview\nBuild context\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCore concepts\n/\nBuild context\nBuild context\nCopy as Markdown\n\nThe docker build and docker buildx build commands build Docker images from a Dockerfile and a context.\n\nWhat is a build context?\n\nThe build context is the set of files that your build can access. The positional argument that you pass to the build command specifies the context that you want to use for the build:\n\n$ docker build [OPTIONS] PATH | URL | -\n\n                         ^^^^^^^^^^^^^^\n\n\nYou can pass any of the following inputs as the context for a build:\n\nThe relative or absolute path to a local directory\nA remote URL of a Git repository, tarball, or plain-text file\nA plain-text file or tarball piped to the docker build command through standard input\nFilesystem contexts\n\nWhen your build context is a local directory, a remote Git repository, or a tar file, then that becomes the set of files that the builder can access during the build. Build instructions such as COPY and ADD can refer to any of the files and directories in the context.\n\nA filesystem build context is processed recursively:\n\nWhen you specify a local directory or a tarball, all subdirectories are included\nWhen you specify a remote Git repository, the repository and all submodules are included\n\nFor more information about the different types of filesystem contexts that you can use with your builds, see:\n\nLocal files\nGit repositories\nRemote tarballs\nText file contexts\n\nWhen your build context is a plain-text file, the builder interprets the file as a Dockerfile. With this approach, the build doesn't use a filesystem context.\n\nFor more information, see empty build context.\n\nLocal context\n\nTo use a local build context, you can specify a relative or absolute filepath to the docker build command. The following example shows a build command that uses the current directory (.) as a build context:\n\n$ docker build .\n\n...\n\n#16 [internal] load build context\n\n#16 sha256:23ca2f94460dcbaf5b3c3edbaaa933281a4e0ea3d92fe295193e4df44dc68f85\n\n#16 transferring context: 13.16MB 2.2s done\n\n...\n\n\nThis makes files and directories in the current working directory available to the builder. The builder loads the files it needs from the build context when needed.\n\nYou can also use local tarballs as build context, by piping the tarball contents to the docker build command. See Tarballs.\n\nLocal directories\n\nConsider the following directory structure:\n\n.\n\n‚îú‚îÄ‚îÄ index.ts\n\n‚îú‚îÄ‚îÄ src/\n\n‚îú‚îÄ‚îÄ Dockerfile\n\n‚îú‚îÄ‚îÄ package.json\n\n‚îî‚îÄ‚îÄ package-lock.json\n\nDockerfile instructions can reference and include these files in the build if you pass this directory as a context.\n\n# syntax=docker/dockerfile:1\n\nFROM node:latest\n\nWORKDIR /src\n\nCOPY package.json package-lock.json .\n\nRUN npm ci\n\nCOPY index.ts src .\n$ docker build .\n\nLocal context with Dockerfile from stdin\n\nUse the following syntax to build an image using files on your local filesystem, while using a Dockerfile from stdin.\n\n$ docker build -f- PATH\n\n\nThe syntax uses the -f (or --file) option to specify the Dockerfile to use, and it uses a hyphen (-) as filename to instruct Docker to read the Dockerfile from stdin.\n\nThe following example uses the current directory (.) as the build context, and builds an image using a Dockerfile passed through stdin using a here-document.\n\n# create a directory to work in\n\nmkdir example\n\ncd example\n\n\n\n# create an example file\n\ntouch somefile.txt\n\n\n\n# build an image using the current directory as context\n\n# and a Dockerfile passed through stdin\n\ndocker build -t myimage:latest -f- . <<EOF\n\nFROM busybox\n\nCOPY somefile.txt ./\n\nRUN cat /somefile.txt\n\nEOF\nLocal tarballs\n\nWhen you pipe a tarball to the build command, the build uses the contents of the tarball as a filesystem context.\n\nFor example, given the following project directory:\n\n.\n\n‚îú‚îÄ‚îÄ Dockerfile\n\n‚îú‚îÄ‚îÄ Makefile\n\n‚îú‚îÄ‚îÄ README.md\n\n‚îú‚îÄ‚îÄ main.c\n\n‚îú‚îÄ‚îÄ scripts\n\n‚îú‚îÄ‚îÄ src\n\n‚îî‚îÄ‚îÄ test.Dockerfile\n\nYou can create a tarball of the directory and pipe it to the build for use as a context:\n\n$ tar czf foo.tar.gz *\n\n$ docker build - < foo.tar.gz\n\n\nThe build resolves the Dockerfile from the tarball context. You can use the --file flag to specify the name and location of the Dockerfile relative to the root of the tarball. The following command builds using test.Dockerfile in the tarball:\n\n$ docker build --file test.Dockerfile - < foo.tar.gz\n\nRemote context\n\nYou can specify the address of a remote Git repository, tarball, or plain-text file as your build context.\n\nFor Git repositories, the builder automatically clones the repository. See Git repositories.\nFor tarballs, the builder downloads and extracts the contents of the tarball. See Tarballs.\n\nIf the remote tarball is a text file, the builder receives no filesystem context, and instead assumes that the remote file is a Dockerfile. See Empty build context.\n\nGit repositories\n\nWhen you pass a URL pointing to the location of a Git repository as an argument to docker build, the builder uses the repository as the build context.\n\nThe builder performs a shallow clone of the repository, downloading only the HEAD commit, not the entire history.\n\nThe builder recursively clones the repository and any submodules it contains.\n\n$ docker build https://github.com/user/myrepo.git\n\n\nBy default, the builder clones the latest commit on the default branch of the repository that you specify.\n\nURL fragments\n\nYou can append URL fragments to the Git repository address to make the builder clone a specific branch, tag, and subdirectory of a repository.\n\nThe format of the URL fragment is #ref:dir, where:\n\nref is the name of the branch, tag, or commit hash\ndir is a subdirectory inside the repository\n\nFor example, the following command uses the container branch, and the docker subdirectory in that branch, as the build context:\n\n$ docker build https://github.com/user/myrepo.git#container:docker\n\n\nThe following table represents all the valid suffixes with their build contexts:\n\nBuild Syntax Suffix\tCommit Used\tBuild Context Used\nmyrepo.git\trefs/heads/<default branch>\t/\nmyrepo.git#mytag\trefs/tags/mytag\t/\nmyrepo.git#mybranch\trefs/heads/mybranch\t/\nmyrepo.git#pull/42/head\trefs/pull/42/head\t/\nmyrepo.git#:myfolder\trefs/heads/<default branch>\t/myfolder\nmyrepo.git#master:myfolder\trefs/heads/master\t/myfolder\nmyrepo.git#mytag:myfolder\trefs/tags/mytag\t/myfolder\nmyrepo.git#mybranch:myfolder\trefs/heads/mybranch\t/myfolder\n\nWhen you use a commit hash as the ref in the URL fragment, use the full, 40-character string SHA-1 hash of the commit. A short hash, for example a hash truncated to 7 characters, is not supported.\n\n# ‚úÖ The following works:\n\ndocker build github.com/docker/buildx#d4f088e689b41353d74f1a0bfcd6d7c0b213aed2\n\n# ‚ùå The following doesn't work because the commit hash is truncated:\n\ndocker build github.com/docker/buildx#d4f088e\nURL queries\nRequires:\nDocker Buildx 0.28.0 and later, Dockerfile 1.18.0 and later, and Docker Desktop 4.46.0 and later\n\nURL queries are more structured and recommended over URL fragments:\n\n$ docker buildx build 'https://github.com/user/myrepo.git?branch=container&subdir=docker'\n\nBuild syntax suffix\tCommit used\tBuild context used\nmyrepo.git\trefs/heads/<default branch>\t/\nmyrepo.git?tag=mytag\trefs/tags/mytag\t/\nmyrepo.git?branch=mybranch\trefs/heads/mybranch\t/\nmyrepo.git?ref=pull/42/head\trefs/pull/42/head\t/\nmyrepo.git?subdir=myfolder\trefs/heads/<default branch>\t/myfolder\nmyrepo.git?branch=master&subdir=myfolder\trefs/heads/master\t/myfolder\nmyrepo.git?tag=mytag&subdir=myfolder\trefs/tags/mytag\t/myfolder\nmyrepo.git?branch=mybranch&subdir=myfolder\trefs/heads/mybranch\t/myfolder\n\nA commit hash can be specified as a checksum (alias commit) query, along with tag, branch, or ref queries to verify that the reference resolves to the expected commit:\n\n$ docker buildx build 'https://github.com/moby/buildkit.git?tag=v0.21.1&checksum=66735c67'\n\n\nIf it doesn't match, the build fails:\n\n$ docker buildx build 'https://github.com/user/myrepo.git?tag=v0.1.0&commit=deadbeef'\n\n...\n\n#3 [internal] load git source https://github.com/user/myrepo.git?tag=v0.1.0-rc1&commit=deadbeef\n\n#3 0.484 bb41e835b6c3523c7c45b248cf4b45e7f862bc42       refs/tags/v0.1.0\n\n#3 ERROR: expected checksum to match deadbeef, got bb41e835b6c3523c7c45b248cf4b45e7f862bc42\n\nNote\n\nShort commit hash is supported with checksum (alias commit) query but for ref, only the full hash of the commit is supported.\n\nKeep .git directory\n\nBy default, BuildKit doesn't keep the .git directory when using Git contexts. You can configure BuildKit to keep the directory by setting the BUILDKIT_CONTEXT_KEEP_GIT_DIR build argument. This can be useful to if you want to retrieve Git information during your build:\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nWORKDIR /src\n\nRUN --mount=target=. \\\n\n  make REVISION=$(git rev-parse HEAD) build\n$ docker build \\\n\n  --build-arg BUILDKIT_CONTEXT_KEEP_GIT_DIR=1\n\n  https://github.com/user/myrepo.git#main\n\nPrivate repositories\n\nWhen you specify a Git context that's also a private repository, the builder needs you to provide the necessary authentication credentials. You can use either SSH or token-based authentication.\n\nBuildx automatically detects and uses SSH credentials if the Git context you specify is an SSH or Git address. By default, this uses $SSH_AUTH_SOCK. You can configure the SSH credentials to use with the --ssh flag.\n\n$ docker buildx build --ssh default git@github.com:user/private.git\n\n\nIf you want to use token-based authentication instead, you can pass the token using the --secret flag.\n\n$ GIT_AUTH_TOKEN=<token> docker buildx build \\\n\n  --secret id=GIT_AUTH_TOKEN \\\n\n  https://github.com/user/private.git\n\nNote\n\nDon't use --build-arg for secrets.\n\nRemote context with Dockerfile from stdin\n\nUse the following syntax to build an image using files on your local filesystem, while using a Dockerfile from stdin.\n\n$ docker build -f- URL\n\n\nThe syntax uses the -f (or --file) option to specify the Dockerfile to use, and it uses a hyphen (-) as filename to instruct Docker to read the Dockerfile from stdin.\n\nThis can be useful in situations where you want to build an image from a repository that doesn't contain a Dockerfile. Or if you want to build with a custom Dockerfile, without maintaining your own fork of the repository.\n\nThe following example builds an image using a Dockerfile from stdin, and adds the hello.c file from the hello-world repository on GitHub.\n\ndocker build -t myimage:latest -f- https://github.com/docker-library/hello-world.git <<EOF\n\nFROM busybox\n\nCOPY hello.c ./\n\nEOF\nRemote tarballs\n\nIf you pass the URL to a remote tarball, the URL itself is sent to the builder.\n\n$ docker build http://server/context.tar.gz\n\n#1 [internal] load remote build context\n\n#1 DONE 0.2s\n\n\n\n#2 copy /context /\n\n#2 DONE 0.1s\n\n...\n\n\nThe download operation will be performed on the host where the BuildKit daemon is running. Note that if you're using a remote Docker context or a remote builder, that's not necessarily the same machine as where you issue the build command. BuildKit fetches the context.tar.gz and uses it as the build context. Tarball contexts must be tar archives conforming to the standard tar Unix format and can be compressed with any one of the xz, bzip2, gzip or identity (no compression) formats.\n\nEmpty context\n\nWhen you use a text file as the build context, the builder interprets the file as a Dockerfile. Using a text file as context means that the build has no filesystem context.\n\nYou can build with an empty build context when your Dockerfile doesn't depend on any local files.\n\nHow to build without a context\n\nYou can pass the text file using a standard input stream, or by pointing at the URL of a remote text file.\n\nUnix pipe PowerShell Heredocs Remote file\n$ docker build - < Dockerfile\n\n\nWhen you build without a filesystem context, Dockerfile instructions such as COPY can't refer to local files:\n\n$ ls\n\nmain.c\n\n$ docker build -<<< $'FROM scratch\\nCOPY main.c .'\n\n[+] Building 0.0s (4/4) FINISHED\n\n => [internal] load build definition from Dockerfile       0.0s\n\n => => transferring dockerfile: 64B                        0.0s\n\n => [internal] load .dockerignore                          0.0s\n\n => => transferring context: 2B                            0.0s\n\n => [internal] load build context                          0.0s\n\n => => transferring context: 2B                            0.0s\n\n => ERROR [1/1] COPY main.c .                              0.0s\n\n------\n\n > [1/1] COPY main.c .:\n\n------\n\nDockerfile:2\n\n--------------------\n\n   1 |     FROM scratch\n\n   2 | >>> COPY main.c .\n\n   3 |\n\n--------------------\n\nERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 7ab2bb61-0c28-432e-abf5-a4c3440bc6b6::4lgfpdf54n5uqxnv9v6ymg7ih: \"/main.c\": not found\n\n.dockerignore files\n\nYou can use a .dockerignore file to exclude files or directories from the build context.\n\n# .dockerignore\n\nnode_modules\n\nbar\n\nThis helps avoid sending unwanted files and directories to the builder, improving build speed, especially when using a remote builder.\n\nFilename and location\n\nWhen you run a build command, the build client looks for a file named .dockerignore in the root directory of the context. If this file exists, the files and directories that match patterns in the files are removed from the build context before it's sent to the builder.\n\nIf you use multiple Dockerfiles, you can use different ignore-files for each Dockerfile. You do so using a special naming convention for the ignore-files. Place your ignore-file in the same directory as the Dockerfile, and prefix the ignore-file with the name of the Dockerfile, as shown in the following example.\n\n.\n\n‚îú‚îÄ‚îÄ index.ts\n\n‚îú‚îÄ‚îÄ src/\n\n‚îú‚îÄ‚îÄ docker\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ build.Dockerfile\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ build.Dockerfile.dockerignore\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ lint.Dockerfile\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ lint.Dockerfile.dockerignore\n\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test.Dockerfile\n\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ test.Dockerfile.dockerignore\n\n‚îú‚îÄ‚îÄ package.json\n\n‚îî‚îÄ‚îÄ package-lock.json\n\nA Dockerfile-specific ignore-file takes precedence over the .dockerignore file at the root of the build context if both exist.\n\nSyntax\n\nThe .dockerignore file is a newline-separated list of patterns similar to the file globs of Unix shells. Leading and trailing slashes in ignore patterns are disregarded. The following patterns all exclude a file or directory named bar in the subdirectory foo under the root of the build context:\n\n/foo/bar/\n/foo/bar\nfoo/bar/\nfoo/bar\n\nIf a line in .dockerignore file starts with # in column 1, then this line is considered as a comment and is ignored before interpreted by the CLI.\n\n#/this/is/a/comment\n\nIf you're interested in learning the precise details of the .dockerignore pattern matching logic, check out the moby/patternmatcher repository on GitHub, which contains the source code.\n\nMatching\n\nThe following code snippet shows an example .dockerignore file.\n\n# comment\n\n*/temp*\n\n*/*/temp*\n\ntemp?\n\nThis file causes the following build behavior:\n\nRule\tBehavior\n# comment\tIgnored.\n*/temp*\tExclude files and directories whose names start with temp in any immediate subdirectory of the root. For example, the plain file /somedir/temporary.txt is excluded, as is the directory /somedir/temp.\n*/*/temp*\tExclude files and directories starting with temp from any subdirectory that is two levels below the root. For example, /somedir/subdir/temporary.txt is excluded.\ntemp?\tExclude files and directories in the root directory whose names are a one-character extension of temp. For example, /tempa and /tempb are excluded.\n\nMatching is done using Go's filepath.Match function rules. A preprocessing step uses Go's filepath.Clean function to trim whitespace and remove . and ... Lines that are blank after preprocessing are ignored.\n\nNote\n\nFor historical reasons, the pattern . is ignored.\n\nBeyond Go's filepath.Match rules, Docker also supports a special wildcard string ** that matches any number of directories (including zero). For example, **/*.go excludes all files that end with .go found anywhere in the build context.\n\nYou can use the .dockerignore file to exclude the Dockerfile and .dockerignore files. These files are still sent to the builder as they're needed for running the build. But you can't copy the files into the image using ADD, COPY, or bind mounts.\n\nNegating matches\n\nYou can prepend lines with a ! (exclamation mark) to make exceptions to exclusions. The following is an example .dockerignore file that uses this mechanism:\n\n*.md\n\n!README.md\n\nAll markdown files right under the context directory except README.md are excluded from the context. Note that markdown files under subdirectories are still included.\n\nThe placement of ! exception rules influences the behavior: the last line of the .dockerignore that matches a particular file determines whether it's included or excluded. Consider the following example:\n\n*.md\n\n!README*.md\n\nREADME-secret.md\n\nNo markdown files are included in the context except README files other than README-secret.md.\n\nNow consider this example:\n\n*.md\n\nREADME-secret.md\n\n!README*.md\n\nAll of the README files are included. The middle line has no effect because !README*.md matches README-secret.md and comes last.\n\nNamed contexts\n\nIn addition to the default build context (the positional argument to the docker build command), you can also pass additional named contexts to builds.\n\nNamed contexts are specified using the --build-context flag, followed by a name-value pair. This lets you include files and directories from multiple sources during the build, while keeping them logically separated.\n\n$ docker build --build-context docs=./docs .\n\n\nIn this example:\n\nThe named docs context points to the ./docs directory.\nThe default context (.) points to the current working directory.\nUsing named contexts in a Dockerfile\n\nDockerfile instructions can reference named contexts as if they are stages in a multi-stage build.\n\nFor example, the following Dockerfile:\n\nUses a COPY instruction to copy files from the default context into the current build stage.\nBind mounts the files in a named context to process the files as part of the build.\n# syntax=docker/dockerfile:1\n\nFROM buildbase\n\nWORKDIR /app\n\n\n\n# Copy all files from the default context into /app/src in the build container\n\nCOPY . /app/src\n\nRUN make bin\n\n\n\n# Mount the files from the named \"docs\" context to build the documentation\n\nRUN --mount=from=docs,target=/app/docs \\\n\n    make manpages\nUse cases for named contexts\n\nUsing named contexts allows for greater flexibility and efficiency when building Docker images. Here are some scenarios where using named contexts can be useful:\n\nExample: combine local and remote sources\n\nYou can define separate named contexts for different types of sources. For example, consider a project where the application source code is local, but the deployment scripts are stored in a Git repository:\n\n$ docker build --build-context scripts=https://github.com/user/deployment-scripts.git .\n\n\nIn the Dockerfile, you can use these contexts independently:\n\n# syntax=docker/dockerfile:1\n\nFROM alpine:latest\n\n\n\n# Copy application code from the main context\n\nCOPY . /opt/app\n\n\n\n# Run deployment scripts using the remote \"scripts\" context\n\nRUN --mount=from=scripts,target=/scripts /scripts/main.sh\nExample: dynamic builds with custom dependencies\n\nIn some scenarios, you might need to dynamically inject configuration files or dependencies into the build from external sources. Named contexts make this straightforward by allowing you to mount different configurations without modifying the default build context.\n\n$ docker build --build-context config=./configs/prod .\n\n\nExample Dockerfile:\n\n# syntax=docker/dockerfile:1\n\nFROM nginx:alpine\n\n\n\n# Use the \"config\" context for environment-specific configurations\n\nCOPY --from=config nginx.conf /etc/nginx/nginx.conf\nExample: pin or override images\n\nYou can refer to named contexts in a Dockerfile the same way you can refer to an image. That means you can change an image reference in your Dockerfile by overriding it with a named context. For example, given the following Dockerfile:\n\nFROM alpine:3.23\n\nIf you want to force image reference to resolve to a different version, without changing the Dockerfile, you can pass a context with the same name to the build. For example:\n\ndocker buildx build --build-context alpine:3.23=docker-image://alpine:edge .\n\n\nThe docker-image:// prefix marks the context as an image reference. The reference can be a local image or an image in your registry.\n\nNamed contexts with Bake\n\nBake is a tool built into docker build that lets you manage your build configuration with a configuration file. Bake fully supports named contexts.\n\nTo define named contexts in a Bake file:\n\ndocker-bake.hcl\ntarget \"app\" {\n\n  contexts = {\n\n    docs = \"./docs\"\n\n  }\n\n}\n\nThis is equivalent to the following CLI invocation:\n\n$ docker build --build-context docs=./docs .\n\nLinking targets with named contexts\n\nIn addition to making complex builds more manageable, Bake also provides additional features on top of what you can do with docker build on the CLI. You can use named contexts to create build pipelines, where one target depends on and builds on top of another. For example, consider a Docker build setup where you have two Dockerfiles:\n\nbase.Dockerfile: for building a base image\napp.Dockerfile: for building an application image\n\nThe app.Dockerfile uses the image produced by base.Dockerfile as it's base image:\n\napp.Dockerfile\nFROM mybaseimage\n\nNormally, you would have to build the base image first, and then either load it to Docker Engine's local image store or push it to a registry. With Bake, you can reference other targets directly, creating a dependency between the app target and the base target.\n\ndocker-bake.hcl\ntarget \"base\" {\n\n  dockerfile = \"base.Dockerfile\"\n\n}\n\n\n\ntarget \"app\" {\n\n  dockerfile = \"app.Dockerfile\"\n\n  contexts = {\n\n    # the target: prefix indicates that 'base' is a Bake target\n\n    mybaseimage = \"target:base\"\n\n  }\n\n}\n\nWith this configuration, references to mybaseimage in app.Dockerfile use the results from building the base target. Building the app target will also trigger a rebuild of mybaseimage, if necessary:\n\n$ docker buildx bake app\n\nFurther reading\n\nFor more information about working with named contexts, see:\n\n--build-context CLI reference\nUsing Bake with additional contexts\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat is a build context?\nFilesystem contexts\nText file contexts\nLocal context\nLocal directories\nLocal context with Dockerfile from stdin\nLocal tarballs\nRemote context\nGit repositories\nRemote context with Dockerfile from stdin\nRemote tarballs\nEmpty context\nHow to build without a context\n.dockerignore files\nFilename and location\nSyntax\nNamed contexts\nUsing named contexts in a Dockerfile\nUse cases for named contexts\nNamed contexts with Bake\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995633,
    "timestamp": "2026-02-07T06:34:33.248Z",
    "title": "Build checks | Docker Docs",
    "url": "https://docs.docker.com/build/checks/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuild checks\nChecking your build configuration\nCopy as Markdown\nAvailability:\nBeta \nRequires:\nDocker Buildx 0.15.0 and later\n\nBuild checks are a feature introduced in Dockerfile 1.8. It lets you validate your build configuration and conduct a series of checks prior to executing your build. Think of it as an advanced form of linting for your Dockerfile and build options, or a dry-run mode for builds.\n\nYou can find the list of checks available, and a description of each, in the Build checks reference.\n\nHow build checks work\n\nTypically, when you run a build, Docker executes the build steps in your Dockerfile and build options as specified. With build checks, rather than executing the build steps, Docker checks the Dockerfile and options you provide and reports any issues it detects.\n\nBuild checks are useful for:\n\nValidating your Dockerfile and build options before running a build.\nEnsuring that your Dockerfile and build options are up-to-date with the latest best practices.\nIdentifying potential issues or anti-patterns in your Dockerfile and build options.\nTip\n\nTo improve linting, code navigation, and vulnerability scanning of your Dockerfiles in Visual Studio Code see Docker VS Code Extension.\n\nBuild with checks\n\nBuild checks are supported in:\n\nBuildx version 0.15.0 and later\ndocker/build-push-action version 6.6.0 and later\ndocker/bake-action version 5.6.0 and later\n\nInvoking a build runs the checks by default, and displays any violations in the build output. For example, the following command both builds the image and runs the checks:\n\n$ docker build .\n\n[+] Building 3.5s (11/11) FINISHED\n\n...\n\n\n\n1 warning found (use --debug to expand):\n\n  - Lint Rule 'JSONArgsRecommended': JSON arguments recommended for CMD to prevent unintended behavior related to OS signals (line 7)\n\n\nIn this example, the build ran successfully, but a JSONArgsRecommended warning was reported, because CMD instructions should use JSON array syntax.\n\nWith the GitHub Actions, the checks display in the diff view of pull requests.\n\nname: Build and push Docker images\n\non:\n\n  push:\n\n\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6.6.0\nMore verbose output\n\nCheck warnings for a regular docker build display a concise message containing the rule name, the message, and the line number of where in the Dockerfile the issue originated. If you want to see more detailed information about the checks, you can use the --debug flag. For example:\n\n$ docker --debug build .\n\n[+] Building 3.5s (11/11) FINISHED\n\n...\n\n\n\n 1 warning found:\n\n - JSONArgsRecommended: JSON arguments recommended for CMD to prevent unintended behavior related to OS signals (line 4)\n\nJSON arguments recommended for ENTRYPOINT/CMD to prevent unintended behavior related to OS signals\n\nMore info: https://docs.docker.com/go/dockerfile/rule/json-args-recommended/\n\nDockerfile:4\n\n--------------------\n\n   2 |\n\n   3 |     FROM alpine\n\n   4 | >>> CMD echo \"Hello, world!\"\n\n   5 |\n\n--------------------\n\n\nWith the --debug flag, the output includes a link to the documentation for the check, and a snippet of the Dockerfile where the issue was found.\n\nCheck a build without building\n\nTo run build checks without actually building, you can use the docker build command as you typically would, but with the addition of the --check flag. Here's an example:\n\n$ docker build --check .\n\n\nInstead of executing the build steps, this command only runs the checks and reports any issues it finds. If there are any issues, they will be reported in the output. For example:\n\nOutput with --check\n[+] Building 1.5s (5/5) FINISHED\n\n=> [internal] connecting to local controller\n\n=> [internal] load build definition from Dockerfile\n\n=> => transferring dockerfile: 253B\n\n=> [internal] load metadata for docker.io/library/node:22\n\n=> [auth] library/node:pull token for registry-1.docker.io\n\n=> [internal] load .dockerignore\n\n=> => transferring context: 50B\n\nJSONArgsRecommended - https://docs.docker.com/go/dockerfile/rule/json-args-recommended/\n\nJSON arguments recommended for ENTRYPOINT/CMD to prevent unintended behavior related to OS signals\n\nDockerfile:7\n\n--------------------\n\n5 |\n\n6 |     COPY index.js .\n\n7 | >>> CMD node index.js\n\n8 |\n\n--------------------\n\nThis output with --check shows the verbose message for the check.\n\nUnlike a regular build, if any violations are reported when using the --check flag, the command exits with a non-zero status code.\n\nFail build on check violations\n\nCheck violations for builds are reported as warnings, with exit code 0, by default. You can configure Docker to fail the build when violations are reported, using a check=error=true directive in your Dockerfile. This will cause the build to error out when after the build checks are run, before the actual build gets executed.\n\nDockerfile\n1\n\n2\n\n3\n4\n5\n\n\t\n# syntax=docker/dockerfile:1\n\n# check=error=true\n\n\n\nFROM alpine\n\nCMD echo \"Hello, world!\"\n\nWithout the # check=error=true directive, this build would complete with an exit code of 0. However, with the directive, build check violation results in non-zero exit code:\n\n$ docker build .\n\n[+] Building 1.5s (5/5) FINISHED\n\n...\n\n\n\n 1 warning found (use --debug to expand):\n\n - JSONArgsRecommended: JSON arguments recommended for CMD to prevent unintended behavior related to OS signals (line 5)\n\nDockerfile:1\n\n--------------------\n\n   1 | >>> # syntax=docker/dockerfile:1\n\n   2 |     # check=error=true\n\n   3 |\n\n--------------------\n\nERROR: lint violation found for rules: JSONArgsRecommended\n\n$ echo $?\n\n1\n\n\nYou can also set the error directive on the CLI by passing the BUILDKIT_DOCKERFILE_CHECK build argument:\n\n$ docker build --check --build-arg \"BUILDKIT_DOCKERFILE_CHECK=error=true\" .\n\nSkip checks\n\nBy default, all checks are run when you build an image. If you want to skip specific checks, you can use the check=skip directive in your Dockerfile. The skip parameter takes a CSV string of the check IDs you want to skip. For example:\n\nDockerfile\n# syntax=docker/dockerfile:1\n\n# check=skip=JSONArgsRecommended,StageNameCasing\n\n\n\nFROM alpine AS BASE_STAGE\n\nCMD echo \"Hello, world!\"\n\nBuilding this Dockerfile results in no check violations.\n\nYou can also skip checks by passing the BUILDKIT_DOCKERFILE_CHECK build argument with a CSV string of check IDs you want to skip. For example:\n\n$ docker build --check --build-arg \"BUILDKIT_DOCKERFILE_CHECK=skip=JSONArgsRecommended,StageNameCasing\" .\n\n\nTo skip all checks, use the skip=all parameter:\n\nDockerfile\n# syntax=docker/dockerfile:1\n\n# check=skip=all\nCombine error and skip parameters for check directives\n\nTo both skip specific checks and error on check violations, pass both the skip and error parameters separated by a semi-colon (;) to the check directive in your Dockerfile or in a build argument. For example:\n\nDockerfile\n# syntax=docker/dockerfile:1\n\n# check=skip=JSONArgsRecommended,StageNameCasing;error=true\nBuild argument\n$ docker build --check --build-arg \"BUILDKIT_DOCKERFILE_CHECK=skip=JSONArgsRecommended,StageNameCasing;error=true\" .\n\nExperimental checks\n\nBefore checks are promoted to stable, they may be available as experimental checks. Experimental checks are disabled by default. To see the list of experimental checks available, refer to the Build checks reference.\n\nTo enable all experimental checks, set the BUILDKIT_DOCKERFILE_CHECK build argument to experimental=all:\n\n$ docker build --check --build-arg \"BUILDKIT_DOCKERFILE_CHECK=experimental=all\" .\n\n\nYou can also enable experimental checks in your Dockerfile using the check directive:\n\nDockerfile\n# syntax=docker/dockerfile:1\n\n# check=experimental=all\n\nTo selectively enable experimental checks, you can pass a CSV string of the check IDs you want to enable, either to the check directive in your Dockerfile or as a build argument. For example:\n\nDockerfile\n# syntax=docker/dockerfile:1\n\n# check=experimental=JSONArgsRecommended,StageNameCasing\n\nNote that the experimental directive takes precedence over the skip directive, meaning that experimental checks will run regardless of the skip directive you have set. For example, if you set skip=all and enable experimental checks, the experimental checks will still run:\n\nDockerfile\n# syntax=docker/dockerfile:1\n\n# check=skip=all;experimental=all\nFurther reading\n\nFor more information about using build checks, see:\n\nBuild checks reference\nValidating build configuration with GitHub Actions\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow build checks work\nBuild with checks\nMore verbose output\nCheck a build without building\nFail build on check violations\nSkip checks\nCombine error and skip parameters for check directives\nExperimental checks\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995636,
    "timestamp": "2026-02-07T06:34:33.249Z",
    "title": "Multi-stage | Docker Docs",
    "url": "https://docs.docker.com/build/building/multi-stage/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nMulti-stage\nVariables\nSecrets\nMulti-platform\nExport binaries\nContainer Device Interface (CDI)\nBest practices\nBase images\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilding\n/\nMulti-stage\nMulti-stage builds\nCopy as Markdown\n\nMulti-stage builds are useful to anyone who has struggled to optimize Dockerfiles while keeping them easy to read and maintain.\n\nUse multi-stage builds\n\nWith multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don't want in the final image.\n\nThe following Dockerfile has two separate stages: one for building a binary, and another where the binary gets copied from the first stage into the next stage.\n\n# syntax=docker/dockerfile:1\n\nFROM golang:1.25\n\nWORKDIR /src\n\nCOPY <<EOF ./main.go\n\npackage main\n\n\n\nimport \"fmt\"\n\n\n\nfunc main() {\n\n  fmt.Println(\"hello, world\")\n\n}\n\nEOF\n\nRUN go build -o /bin/hello ./main.go\n\n\n\nFROM scratch\n\nCOPY --from=0 /bin/hello /bin/hello\n\nCMD [\"/bin/hello\"]\n\nYou only need the single Dockerfile. No need for a separate build script. Just run docker build.\n\n$ docker build -t hello .\n\n\nThe end result is a tiny production image with nothing but the binary inside. None of the build tools required to build the application are included in the resulting image.\n\nHow does it work? The second FROM instruction starts a new build stage with the scratch image as its base. The COPY --from=0 line copies just the built artifact from the previous stage into this new stage. The Go SDK and any intermediate artifacts are left behind, and not saved in the final image.\n\nName your build stages\n\nBy default, the stages aren't named, and you refer to them by their integer number, starting with 0 for the first FROM instruction. However, you can name your stages, by adding an AS <NAME> to the FROM instruction. This example improves the previous one by naming the stages and using the name in the COPY instruction. This means that even if the instructions in your Dockerfile are re-ordered later, the COPY doesn't break.\n\n# syntax=docker/dockerfile:1\n\nFROM golang:1.25 AS build\n\nWORKDIR /src\n\nCOPY <<EOF /src/main.go\n\npackage main\n\n\n\nimport \"fmt\"\n\n\n\nfunc main() {\n\n  fmt.Println(\"hello, world\")\n\n}\n\nEOF\n\nRUN go build -o /bin/hello ./main.go\n\n\n\nFROM scratch\n\nCOPY --from=build /bin/hello /bin/hello\n\nCMD [\"/bin/hello\"]\nStop at a specific build stage\n\nWhen you build your image, you don't necessarily need to build the entire Dockerfile including every stage. You can specify a target build stage. The following command assumes you are using the previous Dockerfile but stops at the stage named build:\n\n$ docker build --target build -t hello .\n\n\nA few scenarios where this might be useful are:\n\nDebugging a specific build stage\nUsing a debug stage with all debugging symbols or tools enabled, and a lean production stage\nUsing a testing stage in which your app gets populated with test data, but building for production using a different stage which uses real data\nUse an external image as a stage\n\nWhen using multi-stage builds, you aren't limited to copying from stages you created earlier in your Dockerfile. You can use the COPY --from instruction to copy from a separate image, either using the local image name, a tag available locally or on a Docker registry, or a tag ID. The Docker client pulls the image if necessary and copies the artifact from there. The syntax is:\n\nCOPY --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf\nUse a previous stage as a new stage\n\nYou can pick up where a previous stage left off by referring to it when using the FROM directive. For example:\n\n# syntax=docker/dockerfile:1\n\n\n\nFROM alpine:latest AS builder\n\nRUN apk --no-cache add build-base\n\n\n\nFROM builder AS build1\n\nCOPY source1.cpp source.cpp\n\nRUN g++ -o /binary source.cpp\n\n\n\nFROM builder AS build2\n\nCOPY source2.cpp source.cpp\n\nRUN g++ -o /binary source.cpp\nDifferences between legacy builder and BuildKit\n\nThe legacy Docker Engine builder processes all stages of a Dockerfile leading up to the selected --target. It will build a stage even if the selected target doesn't depend on that stage.\n\nBuildKit only builds the stages that the target stage depends on.\n\nFor example, given the following Dockerfile:\n\n# syntax=docker/dockerfile:1\n\nFROM ubuntu AS base\n\nRUN echo \"base\"\n\n\n\nFROM base AS stage1\n\nRUN echo \"stage1\"\n\n\n\nFROM base AS stage2\n\nRUN echo \"stage2\"\n\nWith BuildKit enabled, building the stage2 target in this Dockerfile means only base and stage2 are processed. There is no dependency on stage1, so it's skipped.\n\n$ DOCKER_BUILDKIT=1 docker build --no-cache -f Dockerfile --target stage2 .\n\n[+] Building 0.4s (7/7) FINISHED                                                                    \n\n => [internal] load build definition from Dockerfile                                            0.0s\n\n => => transferring dockerfile: 36B                                                             0.0s\n\n => [internal] load .dockerignore                                                               0.0s\n\n => => transferring context: 2B                                                                 0.0s\n\n => [internal] load metadata for docker.io/library/ubuntu:latest                                0.0s\n\n => CACHED [base 1/2] FROM docker.io/library/ubuntu                                             0.0s\n\n => [base 2/2] RUN echo \"base\"                                                                  0.1s\n\n => [stage2 1/1] RUN echo \"stage2\"                                                              0.2s\n\n => exporting to image                                                                          0.0s\n\n => => exporting layers                                                                         0.0s\n\n => => writing image sha256:f55003b607cef37614f607f0728e6fd4d113a4bf7ef12210da338c716f2cfd15    0.0s\n\n\nOn the other hand, building the same target without BuildKit results in all stages being processed:\n\n$ DOCKER_BUILDKIT=0 docker build --no-cache -f Dockerfile --target stage2 .\n\nSending build context to Docker daemon  219.1kB\n\nStep 1/6 : FROM ubuntu AS base\n\n ---> a7870fd478f4\n\nStep 2/6 : RUN echo \"base\"\n\n ---> Running in e850d0e42eca\n\nbase\n\nRemoving intermediate container e850d0e42eca\n\n ---> d9f69f23cac8\n\nStep 3/6 : FROM base AS stage1\n\n ---> d9f69f23cac8\n\nStep 4/6 : RUN echo \"stage1\"\n\n ---> Running in 758ba6c1a9a3\n\nstage1\n\nRemoving intermediate container 758ba6c1a9a3\n\n ---> 396baa55b8c3\n\nStep 5/6 : FROM base AS stage2\n\n ---> d9f69f23cac8\n\nStep 6/6 : RUN echo \"stage2\"\n\n ---> Running in bbc025b93175\n\nstage2\n\nRemoving intermediate container bbc025b93175\n\n ---> 09fc3770a9c4\n\nSuccessfully built 09fc3770a9c4\n\n\nThe legacy builder processes stage1, even if stage2 doesn't depend on it.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUse multi-stage builds\nName your build stages\nStop at a specific build stage\nUse an external image as a stage\nUse a previous stage as a new stage\nDifferences between legacy builder and BuildKit\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995639,
    "timestamp": "2026-02-07T06:34:33.253Z",
    "title": "Variables | Docker Docs",
    "url": "https://docs.docker.com/build/building/variables/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nMulti-stage\nVariables\nSecrets\nMulti-platform\nExport binaries\nContainer Device Interface (CDI)\nBest practices\nBase images\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilding\n/\nVariables\nBuild variables\nCopy as Markdown\n\nIn Docker Build, build arguments (ARG) and environment variables (ENV) both serve as a means to pass information into the build process. You can use them to parameterize the build, allowing for more flexible and configurable builds.\n\nWarning\n\nBuild arguments and environment variables are inappropriate for passing secrets to your build, because they're exposed in the final image. Instead, use secret mounts or SSH mounts, which expose secrets to your builds securely.\n\nSee Build secrets for more information.\n\nSimilarities and differences\n\nBuild arguments and environment variables are similar. They're both declared in the Dockerfile and can be set using flags for the docker build command. Both can be used to parameterize the build. But they each serve a distinct purpose.\n\nBuild arguments\n\nBuild arguments are variables for the Dockerfile itself. Use them to parameterize values of Dockerfile instructions. For example, you might use a build argument to specify the version of a dependency to install.\n\nBuild arguments have no effect on the build unless it's used in an instruction. They're not accessible or present in containers instantiated from the image unless explicitly passed through from the Dockerfile into the image filesystem or configuration. They may persist in the image metadata, as provenance attestations and in the image history, which is why they're not suitable for holding secrets.\n\nThey make Dockerfiles more flexible, and easier to maintain.\n\nFor an example on how you can use build arguments, see ARG usage example.\n\nEnvironment variables\n\nEnvironment variables are passed through to the build execution environment, and persist in containers instantiated from the image.\n\nEnvironment variables are primarily used to:\n\nConfigure the execution environment for builds\nSet default environment variables for containers\n\nEnvironment variables, if set, can directly influence the execution of your build, and the behavior or configuration of the application.\n\nYou can't override or set an environment variable at build-time. Values for environment variables must be declared in the Dockerfile. You can combine environment variables and build arguments to allow environment variables to be configured at build-time.\n\nFor an example on how to use environment variables for configuring builds, see ENV usage example.\n\nARG usage example\n\nBuild arguments are commonly used to specify versions of components, such as image variants or package versions, used in a build.\n\nSpecifying versions as build arguments lets you build with different versions without having to manually update the Dockerfile. It also makes it easier to maintain the Dockerfile, since it lets you declare versions at the top of the file.\n\nBuild arguments can also be a way to reuse a value in multiple places. For example, if you use multiple flavors of alpine in your build, you can ensure you're using the same version of alpine everywhere:\n\ngolang:1.22-alpine${ALPINE_VERSION}\npython:3.12-alpine${ALPINE_VERSION}\nnginx:1-alpine${ALPINE_VERSION}\n\nThe following example defines the version of node and alpine using build arguments.\n\n# syntax=docker/dockerfile:1\n\n\n\nARG NODE_VERSION=\"24\"\n\nARG ALPINE_VERSION=\"3.23\"\n\n\n\nFROM node:${NODE_VERSION}-alpine${ALPINE_VERSION} AS base\n\nWORKDIR /src\n\n\n\nFROM base AS build\n\nCOPY package*.json ./\n\nRUN npm ci\n\nRUN npm run build\n\n\n\nFROM base AS production\n\nCOPY package*.json ./\n\nRUN npm ci --omit=dev && npm cache clean --force\n\nCOPY --from=build /src/dist/ .\n\nCMD [\"node\", \"app.js\"]\n\nIn this case, the build arguments have default values. Specifying their values when you invoke a build is optional. To override the defaults, you would use the --build-arg CLI flag:\n\n$ docker build --build-arg NODE_VERSION=current .\n\n\nFor more information on how to use build arguments, refer to:\n\nARG Dockerfile reference\ndocker build --build-arg reference\nENV usage example\n\nDeclaring an environment variable with ENV makes the variable available to all subsequent instructions in the build stage. The following example shows an example setting NODE_ENV to production before installing JavaScript dependencies with npm. Setting the variable makes npm omits packages needed only for local development.\n\n# syntax=docker/dockerfile:1\n\n\n\nFROM node:20\n\nWORKDIR /app\n\nCOPY package*.json ./\n\nENV NODE_ENV=production\n\nRUN npm ci && npm cache clean --force\n\nCOPY . .\n\nCMD [\"node\", \"app.js\"]\n\nEnvironment variables aren't configurable at build-time by default. If you want to change the value of an ENV at build-time, you can combine environment variables and build arguments:\n\n# syntax=docker/dockerfile:1\n\n\n\nFROM node:20\n\nARG NODE_ENV=production\n\nENV NODE_ENV=$NODE_ENV\n\nWORKDIR /app\n\nCOPY package*.json ./\n\nRUN npm ci && npm cache clean --force\n\nCOPY . .\n\nCMD [\"node\", \"app.js\"]\n\nWith this Dockerfile, you can use --build-arg to override the default value of NODE_ENV:\n\n$ docker build --build-arg NODE_ENV=development .\n\n\nNote that, because the environment variables you set persist in containers, using them can lead to unintended side-effects for the application's runtime.\n\nFor more information on how to use environment variables in builds, refer to:\n\nENV Dockerfile reference\nScoping\n\nBuild arguments declared in the global scope of a Dockerfile aren't automatically inherited into the build stages. They're only accessible in the global scope.\n\n# syntax=docker/dockerfile:1\n\n\n\n# The following build argument is declared in the global scope:\n\nARG NAME=\"joe\"\n\n\n\nFROM alpine\n\n# The following instruction doesn't have access to the $NAME build argument\n\n# because the argument was defined in the global scope, not for this stage.\n\nRUN echo \"hello ${NAME}!\"\n\nThe echo command in this example evaluates to hello ! because the value of the NAME build argument is out of scope. To inherit global build arguments into a stage, you must consume them:\n\n# syntax=docker/dockerfile:1\n\n\n\n# Declare the build argument in the global scope\n\nARG NAME=\"joe\"\n\n\n\nFROM alpine\n\n# Consume the build argument in the build stage\n\nARG NAME\n\nRUN echo $NAME\n\nOnce a build argument is declared or consumed in a stage, it's automatically inherited by child stages.\n\n# syntax=docker/dockerfile:1\n\nFROM alpine AS base\n\n# Declare the build argument in the build stage\n\nARG NAME=\"joe\"\n\n\n\n# Create a new stage based on \"base\"\n\nFROM base AS build\n\n# The NAME build argument is available here\n\n# since it's declared in a parent stage\n\nRUN echo \"hello $NAME!\"\n\nThe following diagram further exemplifies how build argument and environment variable inheritance works for multi-stage builds.\n\nPre-defined build arguments\n\nThis section describes pre-defined build arguments available to all builds by default.\n\nMulti-platform build arguments\n\nMulti-platform build arguments describe the build and target platforms for the build.\n\nThe build platform is the operating system, architecture, and platform variant of the host system where the builder (the BuildKit daemon) is running.\n\nBUILDPLATFORM\nBUILDOS\nBUILDARCH\nBUILDVARIANT\n\nThe target platform arguments hold the same values for the target platforms for the build, specified using the --platform flag for the docker build command.\n\nTARGETPLATFORM\nTARGETOS\nTARGETARCH\nTARGETVARIANT\n\nThese arguments are useful for doing cross-compilation in multi-platform builds. They're available in the global scope of the Dockerfile, but they aren't automatically inherited by build stages. To use them inside stage, you must declare them:\n\n# syntax=docker/dockerfile:1\n\n\n\n# Pre-defined build arguments are available in the global scope\n\nFROM --platform=$BUILDPLATFORM golang\n\n# To inherit them to a stage, declare them with ARG\n\nARG TARGETOS\n\nRUN GOOS=$TARGETOS go build -o ./exe .\n\nFor more information about multi-platform build arguments, refer to Multi-platform arguments\n\nProxy arguments\n\nProxy build arguments let you specify proxies to use for your build. You don't need to declare or reference these arguments in the Dockerfile. Specifying a proxy with --build-arg is enough to make your build use the proxy.\n\nProxy arguments are automatically excluded from the build cache and the output of docker history by default. If you do reference the arguments in your Dockerfile, the proxy configuration ends up in the build cache.\n\nThe builder respects the following proxy build arguments. The variables are case insensitive.\n\nHTTP_PROXY\nHTTPS_PROXY\nFTP_PROXY\nNO_PROXY\nALL_PROXY\n\nTo configure a proxy for your build:\n\n$ docker build --build-arg HTTP_PROXY=https://my-proxy.example.com .\n\n\nFor more information about proxy build arguments, refer to Proxy arguments.\n\nBuild tool configuration variables\n\nThe following environment variables enable, disable, or change the behavior of Buildx and BuildKit. Note that these variables aren't used to configure the build container; they aren't available inside the build and they have no relation to the ENV instruction. They're used to configure the Buildx client, or the BuildKit daemon.\n\nVariable\tType\tDescription\nBUILDKIT_COLORS\tString\tConfigure text color for the terminal output.\nBUILDKIT_HOST\tString\tSpecify host to use for remote builders.\nBUILDKIT_PROGRESS\tString\tConfigure type of progress output.\nBUILDKIT_TTY_LOG_LINES\tString\tNumber of log lines (for active steps in TTY mode).\nBUILDX_BAKE_FILE\tString\tSpecify the build definition file(s) for docker buildx bake.\nBUILDX_BAKE_FILE_SEPARATOR\tString\tSpecify the file-path separator for BUILDX_BAKE_FILE.\nBUILDX_BAKE_GIT_AUTH_HEADER\tString\tHTTP authentication scheme for remote Bake files.\nBUILDX_BAKE_GIT_AUTH_TOKEN\tString\tHTTP authentication token for remote Bake files.\nBUILDX_BAKE_GIT_SSH\tString\tSSH authentication for remote Bake files.\nBUILDX_BUILDER\tString\tSpecify the builder instance to use.\nBUILDX_CONFIG\tString\tSpecify location for configuration, state, and logs.\nBUILDX_CPU_PROFILE\tString\tGenerate a pprof CPU profile at the specified location.\nBUILDX_EXPERIMENTAL\tBoolean\tTurn on experimental features.\nBUILDX_GIT_CHECK_DIRTY\tBoolean\tEnable dirty Git checkout detection.\nBUILDX_GIT_INFO\tBoolean\tRemove Git information in provenance attestations.\nBUILDX_GIT_LABELS\tString | Boolean\tAdd Git provenance labels to images.\nBUILDX_MEM_PROFILE\tString\tGenerate a pprof memory profile at the specified location.\nBUILDX_METADATA_PROVENANCE\tString | Boolean\tCustomize provenance information included in the metadata file.\nBUILDX_METADATA_WARNINGS\tString\tInclude build warnings in the metadata file.\nBUILDX_NO_DEFAULT_ATTESTATIONS\tBoolean\tTurn off default provenance attestations.\nBUILDX_NO_DEFAULT_LOAD\tBoolean\tTurn off loading images to image store by default.\nEXPERIMENTAL_BUILDKIT_SOURCE_POLICY\tString\tSpecify a BuildKit source policy file.\n\nBuildKit also supports a few additional configuration parameters. Refer to BuildKit built-in build args.\n\nYou can express Boolean values for environment variables in different ways. For example, true, 1, and T all evaluate to true. Evaluation is done using the strconv.ParseBool function in the Go standard library. See the reference documentation for details.\n\nBUILDKIT_COLORS\n\nChanges the colors of the terminal output. Set BUILDKIT_COLORS to a CSV string in the following format:\n\n$ export BUILDKIT_COLORS=\"run=123,20,245:error=yellow:cancel=blue:warning=white\"\n\n\nColor values can be any valid RGB hex code, or one of the BuildKit predefined colors.\n\nSetting NO_COLOR to anything turns off colorized output, as recommended by no-color.org.\n\nBUILDKIT_HOST\nRequires:\nDocker Buildx 0.9.0 and later\n\nYou use the BUILDKIT_HOST to specify the address of a BuildKit daemon to use as a remote builder. This is the same as specifying the address as a positional argument to docker buildx create.\n\nUsage:\n\n$ export BUILDKIT_HOST=tcp://localhost:1234\n\n$ docker buildx create --name=remote --driver=remote\n\n\nIf you specify both the BUILDKIT_HOST environment variable and a positional argument, the argument takes priority.\n\nBUILDKIT_PROGRESS\n\nSets the type of the BuildKit progress output. Valid values are:\n\nauto (default): automatically uses tty in interactive terminals, plain otherwise\nplain: displays build steps sequentially in simple text format\ntty: interactive output with formatted progress bars and build steps\nquiet: suppresses progress output, only shows errors and final image ID\nnone: no progress output, only shows errors\nrawjson: outputs build progress as raw JSON (useful for parsing by other tools)\n\nUsage:\n\n$ export BUILDKIT_PROGRESS=plain\n\nBUILDKIT_TTY_LOG_LINES\n\nYou can change how many log lines are visible for active steps in TTY mode by setting BUILDKIT_TTY_LOG_LINES to a number (default to 6).\n\n$ export BUILDKIT_TTY_LOG_LINES=8\n\nEXPERIMENTAL_BUILDKIT_SOURCE_POLICY\n\nLets you specify a BuildKit source policy file for creating reproducible builds with pinned dependencies.\n\n$ export EXPERIMENTAL_BUILDKIT_SOURCE_POLICY=./policy.json\n\n\nExample:\n\n{\n\n  \"rules\": [\n\n    {\n\n      \"action\": \"CONVERT\",\n\n      \"selector\": {\n\n        \"identifier\": \"docker-image://docker.io/library/alpine:latest\"\n\n      },\n\n      \"updates\": {\n\n        \"identifier\": \"docker-image://docker.io/library/alpine:latest@sha256:4edbd2beb5f78b1014028f4fbb99f3237d9561100b6881aabbf5acce2c4f9454\"\n\n      }\n\n    },\n\n    {\n\n      \"action\": \"CONVERT\",\n\n      \"selector\": {\n\n        \"identifier\": \"https://raw.githubusercontent.com/moby/buildkit/v0.10.1/README.md\"\n\n      },\n\n      \"updates\": {\n\n        \"attrs\": {\"http.checksum\": \"sha256:6e4b94fc270e708e1068be28bd3551dc6917a4fc5a61293d51bb36e6b75c4b53\"}\n\n      }\n\n    },\n\n    {\n\n      \"action\": \"DENY\",\n\n      \"selector\": {\n\n        \"identifier\": \"docker-image://docker.io/library/golang*\"\n\n      }\n\n    }\n\n  ]\n\n}\nBUILDX_BAKE_FILE\nRequires:\nDocker Buildx 0.26.0 and later\n\nSpecify one or more build definition files for docker buildx bake.\n\nThis environment variable provides an alternative to the -f / --file command-line flag.\n\nMultiple files can be specified by separating them with the system path separator (\":\" on Linux/macOS, \";\" on Windows):\n\nexport BUILDX_BAKE_FILE=file1.hcl:file2.hcl\n\n\nOr with a custom separator defined by the BUILDX_BAKE_FILE_SEPARATOR variable:\n\nexport BUILDX_BAKE_FILE_SEPARATOR=@\n\nexport BUILDX_BAKE_FILE=file1.hcl@file2.hcl\n\n\nIf both BUILDX_BAKE_FILE and the -f flag are set, only the files provided via -f are used.\n\nIf a listed file does not exist or is invalid, bake returns an error.\n\nBUILDX_BAKE_FILE_SEPARATOR\nRequires:\nDocker Buildx 0.26.0 and later\n\nControls the separator used between file paths in the BUILDX_BAKE_FILE environment variable.\n\nThis is useful if your file paths contain the default separator character or if you want to standardize separators across different platforms.\n\nexport BUILDX_BAKE_PATH_SEPARATOR=@\n\nexport BUILDX_BAKE_FILE=file1.hcl@file2.hcl\n\nBUILDX_BAKE_GIT_AUTH_HEADER\nRequires:\nDocker Buildx 0.14.0 and later\n\nSets the HTTP authentication scheme when using a remote Bake definition in a private Git repository. This is equivalent to the GIT_AUTH_HEADER secret, but facilitates the pre-flight authentication in Bake when loading the remote Bake file. Supported values are bearer (default) and basic.\n\nUsage:\n\n$ export BUILDX_BAKE_GIT_AUTH_HEADER=basic\n\nBUILDX_BAKE_GIT_AUTH_TOKEN\nRequires:\nDocker Buildx 0.14.0 and later\n\nSets the HTTP authentication token when using a remote Bake definition in a private Git repository. This is equivalent to the GIT_AUTH_TOKEN secret, but facilitates the pre-flight authentication in Bake when loading the remote Bake file.\n\nUsage:\n\n$ export BUILDX_BAKE_GIT_AUTH_TOKEN=$(cat git-token.txt)\n\nBUILDX_BAKE_GIT_SSH\nRequires:\nDocker Buildx 0.14.0 and later\n\nLets you specify a list of SSH agent socket filepaths to forward to Bake for authenticating to a Git server when using a remote Bake definition in a private repository. This is similar to SSH mounts for builds, but facilitates the pre-flight authentication in Bake when resolving the build definition.\n\nSetting this environment is typically not necessary, because Bake will use the SSH_AUTH_SOCK agent socket by default. You only need to specify this variable if you want to use a socket with a different filepath. This variable can take multiple paths using a comma-separated string.\n\nUsage:\n\n$ export BUILDX_BAKE_GIT_SSH=/run/foo/listener.sock,~/.creds/ssh.sock\n\nBUILDX_BUILDER\n\nOverrides the configured builder instance. Same as the docker buildx --builder CLI flag.\n\nUsage:\n\n$ export BUILDX_BUILDER=my-builder\n\nBUILDX_CONFIG\n\nYou can use BUILDX_CONFIG to specify the directory to use for build configuration, state, and logs. The lookup order for this directory is as follows:\n\n$BUILDX_CONFIG\n$DOCKER_CONFIG/buildx\n~/.docker/buildx (default)\n\nUsage:\n\n$ export BUILDX_CONFIG=/usr/local/etc\n\nBUILDX_CPU_PROFILE\nRequires:\nDocker Buildx 0.18.0 and later\n\nIf specified, Buildx generates a pprof CPU profile at the specified location.\n\nNote\n\nThis property is only useful for when developing Buildx. The profiling data is not relevant for analyzing a build's performance.\n\nUsage:\n\n$ export BUILDX_CPU_PROFILE=buildx_cpu.prof\n\nBUILDX_EXPERIMENTAL\n\nEnables experimental build features.\n\nUsage:\n\n$ export BUILDX_EXPERIMENTAL=1\n\nBUILDX_GIT_CHECK_DIRTY\nRequires:\nDocker Buildx 0.10.4 and later\n\nWhen set to true, checks for dirty state in source control information for provenance attestations.\n\nUsage:\n\n$ export BUILDX_GIT_CHECK_DIRTY=1\n\nBUILDX_GIT_INFO\nRequires:\nDocker Buildx 0.10.0 and later\n\nWhen set to false, removes source control information from provenance attestations.\n\nUsage:\n\n$ export BUILDX_GIT_INFO=0\n\nBUILDX_GIT_LABELS\nRequires:\nDocker Buildx 0.10.0 and later\n\nAdds provenance labels, based on Git information, to images that you build. The labels are:\n\ncom.docker.image.source.entrypoint: Location of the Dockerfile relative to the project root\norg.opencontainers.image.revision: Git commit revision\norg.opencontainers.image.source: SSH or HTTPS address of the repository\n\nExample:\n\n  \"Labels\": {\n\n    \"com.docker.image.source.entrypoint\": \"Dockerfile\",\n\n    \"org.opencontainers.image.revision\": \"5734329c6af43c2ae295010778cd308866b95d9b\",\n\n    \"org.opencontainers.image.source\": \"git@github.com:foo/bar.git\"\n\n  }\n\nUsage:\n\nSet BUILDX_GIT_LABELS=1 to include the entrypoint and revision labels.\nSet BUILDX_GIT_LABELS=full to include all labels.\n\nIf the repository is in a dirty state, the revision gets a -dirty suffix.\n\nBUILDX_MEM_PROFILE\nRequires:\nDocker Buildx 0.18.0 and later\n\nIf specified, Buildx generates a pprof memory profile at the specified location.\n\nNote\n\nThis property is only useful for when developing Buildx. The profiling data is not relevant for analyzing a build's performance.\n\nUsage:\n\n$ export BUILDX_MEM_PROFILE=buildx_mem.prof\n\nBUILDX_METADATA_PROVENANCE\nRequires:\nDocker Buildx 0.14.0 and later\n\nBy default, Buildx includes minimal provenance information in the metadata file through --metadata-file flag. This environment variable allows you to customize the provenance information included in the metadata file:\n\nmin sets minimal provenance (default).\nmax sets full provenance.\ndisabled, false or 0 does not set any provenance.\nBUILDX_METADATA_WARNINGS\nRequires:\nDocker Buildx 0.16.0 and later\n\nBy default, Buildx does not include build warnings in the metadata file through --metadata-file flag. You can set this environment variable to 1 or true to include them.\n\nBUILDX_NO_DEFAULT_ATTESTATIONS\nRequires:\nDocker Buildx 0.10.4 and later\n\nBy default, BuildKit v0.11 and later adds provenance attestations to images you build. Set BUILDX_NO_DEFAULT_ATTESTATIONS=1 to disable the default provenance attestations.\n\nUsage:\n\n$ export BUILDX_NO_DEFAULT_ATTESTATIONS=1\n\nBUILDX_NO_DEFAULT_LOAD\n\nWhen you build an image using the docker driver, the image is automatically loaded to the image store when the build finishes. Set BUILDX_NO_DEFAULT_LOAD to disable automatic loading of images to the local container store.\n\nUsage:\n\n$ export BUILDX_NO_DEFAULT_LOAD=1\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSimilarities and differences\nBuild arguments\nEnvironment variables\nARG usage example\nENV usage example\nScoping\nPre-defined build arguments\nMulti-platform build arguments\nProxy arguments\nBuild tool configuration variables\nBUILDKIT_COLORS\nBUILDKIT_HOST\nBUILDKIT_PROGRESS\nBUILDKIT_TTY_LOG_LINES\nEXPERIMENTAL_BUILDKIT_SOURCE_POLICY\nBUILDX_BAKE_FILE\nBUILDX_BAKE_FILE_SEPARATOR\nBUILDX_BAKE_GIT_AUTH_HEADER\nBUILDX_BAKE_GIT_AUTH_TOKEN\nBUILDX_BAKE_GIT_SSH\nBUILDX_BUILDER\nBUILDX_CONFIG\nBUILDX_CPU_PROFILE\nBUILDX_EXPERIMENTAL\nBUILDX_GIT_CHECK_DIRTY\nBUILDX_GIT_INFO\nBUILDX_GIT_LABELS\nBUILDX_MEM_PROFILE\nBUILDX_METADATA_PROVENANCE\nBUILDX_METADATA_WARNINGS\nBUILDX_NO_DEFAULT_ATTESTATIONS\nBUILDX_NO_DEFAULT_LOAD\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995642,
    "timestamp": "2026-02-07T06:34:33.267Z",
    "title": "Secrets | Docker Docs",
    "url": "https://docs.docker.com/build/building/secrets/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nMulti-stage\nVariables\nSecrets\nMulti-platform\nExport binaries\nContainer Device Interface (CDI)\nBest practices\nBase images\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilding\n/\nSecrets\nBuild secrets\nCopy as Markdown\n\nA build secret is any piece of sensitive information, such as a password or API token, consumed as part of your application's build process.\n\nBuild arguments and environment variables are inappropriate for passing secrets to your build, because they persist in the final image. Instead, you should use secret mounts or SSH mounts, which expose secrets to your builds securely.\n\nTypes of build secrets\nSecret mounts are general-purpose mounts for passing secrets into your build. A secret mount takes a secret from the build client and makes it temporarily available inside the build container, for the duration of the build instruction. This is useful if, for example, your build needs to communicate with a private artifact server or API.\nSSH mounts are special-purpose mounts for making SSH sockets or keys available inside builds. They're commonly used when you need to fetch private Git repositories in your builds.\nGit authentication for remote contexts is a set of pre-defined secrets for when you build with a remote Git context that's also a private repository. These secrets are \"pre-flight\" secrets: they are not consumed within your build instruction, but they're used to provide the builder with the necessary credentials to fetch the context.\nUsing build secrets\n\nFor secret mounts and SSH mounts, using build secrets is a two-step process. First you need to pass the secret into the docker build command, and then you need to consume the secret in your Dockerfile.\n\nTo pass a secret to a build, use the docker build --secret flag, or the equivalent options for Bake.\n\nCLI Bake\n$ docker build --secret id=aws,src=$HOME/.aws/credentials .\n\n\nTo consume a secret in a build and make it accessible to the RUN instruction, use the --mount=type=secret flag in the Dockerfile.\n\nRUN --mount=type=secret,id=aws \\\n\n    AWS_SHARED_CREDENTIALS_FILE=/run/secrets/aws \\\n\n    aws s3 cp ...\nSecret mounts\n\nSecret mounts expose secrets to the build containers, as files or environment variables. You can use secret mounts to pass sensitive information to your builds, such as API tokens, passwords, or SSH keys.\n\nSources\n\nThe source of a secret can be either a file or an environment variable. When you use the CLI or Bake, the type can be detected automatically. You can also specify it explicitly with type=file or type=env.\n\nThe following example mounts the environment variable KUBECONFIG to secret ID kube, as a file in the build container at /run/secrets/kube.\n\n$ docker build --secret id=kube,env=KUBECONFIG .\n\n\nWhen you use secrets from environment variables, you can omit the env parameter to bind the secret to a file with the same name as the variable. In the following example, the value of the API_TOKEN variable is mounted to /run/secrets/API_TOKEN in the build container.\n\n$ docker build --secret id=API_TOKEN .\n\nTarget\n\nWhen consuming a secret in a Dockerfile, the secret is mounted to a file by default. The default file path of the secret, inside the build container, is /run/secrets/<id>. You can customize how the secrets get mounted in the build container using the target and env options for the RUN --mount flag in the Dockerfile.\n\nThe following example takes secret id aws and mounts it to a file at /run/secrets/aws in the build container.\n\nRUN --mount=type=secret,id=aws \\\n\n    AWS_SHARED_CREDENTIALS_FILE=/run/secrets/aws \\\n\n    aws s3 cp ...\n\nTo mount a secret as a file with a different name, use the target option in the --mount flag.\n\nRUN --mount=type=secret,id=aws,target=/root/.aws/credentials \\\n\n    aws s3 cp ...\n\nTo mount a secret as an environment variable instead of a file, use the env option in the --mount flag.\n\nRUN --mount=type=secret,id=aws-key-id,env=AWS_ACCESS_KEY_ID \\\n\n    --mount=type=secret,id=aws-secret-key,env=AWS_SECRET_ACCESS_KEY \\\n\n    --mount=type=secret,id=aws-session-token,env=AWS_SESSION_TOKEN \\\n\n    aws s3 cp ...\n\nIt's possible to use the target and env options together to mount a secret as both a file and an environment variable.\n\nSSH mounts\n\nIf the credential you want to use in your build is an SSH agent socket or key, you can use the SSH mount instead of a secret mount. Cloning private Git repositories is a common use case for SSH mounts.\n\nThe following example clones a private GitHub repository using a Dockerfile SSH mount.\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nADD git@github.com:me/myprivaterepo.git /src/\n\nTo pass an SSH socket the build, you use the docker build --ssh flag, or equivalent options for Bake.\n\n$ docker buildx build --ssh default .\n\nGit authentication for remote contexts\n\nBuildKit supports two pre-defined build secrets, GIT_AUTH_TOKEN and GIT_AUTH_HEADER. Use them to specify HTTP authentication parameters when building with remote, private Git repositories, including:\n\nBuilding with a private Git repository as build context\nFetching private Git repositories in a build with ADD\n\nFor example, say you have a private GitLab project at https://gitlab.com/example/todo-app.git, and you want to run a build using that repository as the build context. An unauthenticated docker build command fails because the builder isn't authorized to pull the repository:\n\n$ docker build https://gitlab.com/example/todo-app.git\n\n[+] Building 0.4s (1/1) FINISHED\n\n => ERROR [internal] load git source https://gitlab.com/example/todo-app.git\n\n------\n\n > [internal] load git source https://gitlab.com/example/todo-app.git:\n\n0.313 fatal: could not read Username for 'https://gitlab.com': terminal prompts disabled\n\n------\n\n\nTo authenticate the builder to the Git server, set the GIT_AUTH_TOKEN environment variable to contain a valid GitLab access token, and pass it as a secret to the build:\n\n$ GIT_AUTH_TOKEN=$(cat gitlab-token.txt) docker build \\\n\n  --secret id=GIT_AUTH_TOKEN \\\n\n  https://gitlab.com/example/todo-app.git\n\n\nThe GIT_AUTH_TOKEN also works with ADD to fetch private Git repositories as part of your build:\n\nFROM alpine\n\nADD https://gitlab.com/example/todo-app.git /src\nHTTP authentication scheme\n\nBy default, Git authentication over HTTP uses the Bearer authentication scheme:\n\nAuthorization: Bearer GIT_AUTH_TOKEN\n\nIf you need to use a Basic scheme, with a username and password, you can set the GIT_AUTH_HEADER build secret:\n\n$ export GIT_AUTH_TOKEN=$(cat gitlab-token.txt)\n\n$ export GIT_AUTH_HEADER=basic\n\n$ docker build \\\n\n  --secret id=GIT_AUTH_TOKEN \\\n\n  --secret id=GIT_AUTH_HEADER \\\n\n  https://gitlab.com/example/todo-app.git\n\n\nBuildKit currently only supports the Bearer and Basic schemes.\n\nMultiple hosts\n\nYou can set the GIT_AUTH_TOKEN and GIT_AUTH_HEADER secrets on a per-host basis, which lets you use different authentication parameters for different hostnames. To specify a hostname, append the hostname as a suffix to the secret ID:\n\n$ export GITLAB_TOKEN=$(cat gitlab-token.txt)\n\n$ export GERRIT_TOKEN=$(cat gerrit-username-password.txt)\n\n$ export GERRIT_SCHEME=basic\n\n$ docker build \\\n\n  --secret id=GIT_AUTH_TOKEN.gitlab.com,env=GITLAB_TOKEN \\\n\n  --secret id=GIT_AUTH_TOKEN.gerrit.internal.example,env=GERRIT_TOKEN \\\n\n  --secret id=GIT_AUTH_HEADER.gerrit.internal.example,env=GERRIT_SCHEME \\\n\n  https://gitlab.com/example/todo-app.git\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nTypes of build secrets\nUsing build secrets\nSecret mounts\nSources\nTarget\nSSH mounts\nGit authentication for remote contexts\nHTTP authentication scheme\nMultiple hosts\nSecrets\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995645,
    "timestamp": "2026-02-07T06:34:33.274Z",
    "title": "Multi-platform | Docker Docs",
    "url": "https://docs.docker.com/build/building/multi-platform/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nMulti-stage\nVariables\nSecrets\nMulti-platform\nExport binaries\nContainer Device Interface (CDI)\nBest practices\nBase images\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilding\n/\nMulti-platform\nMulti-platform builds\nCopy as Markdown\n\nA multi-platform build refers to a single build invocation that targets multiple different operating system or CPU architecture combinations. When building images, this lets you create a single image that can run on multiple platforms, such as linux/amd64, linux/arm64, and windows/amd64.\n\nWhy multi-platform builds?\n\nDocker solves the \"it works on my machine\" problem by packaging applications and their dependencies into containers. This makes it easy to run the same application on different environments, such as development, testing, and production.\n\nBut containerization by itself only solves part of the problem. Containers share the host kernel, which means that the code that's running inside the container must be compatible with the host's architecture. This is why you can't run a linux/amd64 container on an arm64 host (without using emulation), or a Windows container on a Linux host.\n\nMulti-platform builds solve this problem by packaging multiple variants of the same application into a single image. This enables you to run the same image on different types of hardware, such as development machines running x86-64 or ARM-based Amazon EC2 instances in the cloud, without the need for emulation.\n\nDifference between single-platform and multi-platform images\n\nMulti-platform images have a different structure than single-platform images. Single-platform images contain a single manifest that points to a single configuration and a single set of layers. Multi-platform images contain a manifest list, pointing to multiple manifests, each of which points to a different configuration and set of layers.\n\nWhen you push a multi-platform image to a registry, the registry stores the manifest list and all the individual manifests. When you pull the image, the registry returns the manifest list, and Docker automatically selects the correct variant based on the host's architecture. For example, if you run a multi-platform image on an ARM-based Raspberry Pi, Docker selects the linux/arm64 variant. If you run the same image on an x86-64 laptop, Docker selects the linux/amd64 variant (if you're using Linux containers).\n\nPrerequisites\n\nTo build multi-platform images, you first need to make sure that your Docker environment is set up to support it. There are two ways you can do that:\n\nYou can switch from the \"classic\" image store to the containerd image store.\nYou can create and use a custom builder.\n\nThe \"classic\" image store of the Docker Engine does not support multi-platform images. Switching to the containerd image store ensures that your Docker Engine can push, pull, and build multi-platform images.\n\nCreating a custom builder that uses a driver with multi-platform support, such as the docker-container driver, will let you build multi-platform images without switching to a different image store. However, you still won't be able to load the multi-platform images you build into your Docker Engine image store. But you can push them to a container registry directly with docker build --push.\n\ncontainerd image store Custom builder\n\nThe steps for enabling the containerd image store depends on whether you're using Docker Desktop or Docker Engine standalone:\n\nIf you're using Docker Desktop, enable the containerd image store in the Docker Desktop settings.\n\nIf you're using Docker Engine standalone, enable the containerd image store using the daemon configuration file.\n\nIf you're using Docker Engine standalone and you need to build multi-platform images using emulation, you also need to install QEMU, see Install QEMU manually.\n\nBuild multi-platform images\n\nWhen triggering a build, use the --platform flag to define the target platforms for the build output, such as linux/amd64 and linux/arm64:\n\n$ docker buildx build --platform linux/amd64,linux/arm64 .\n\nStrategies\n\nYou can build multi-platform images using three different strategies, depending on your use case:\n\nUsing emulation, via QEMU\nUse a builder with multiple native nodes\nUse cross-compilation with multi-stage builds\nQEMU\n\nBuilding multi-platform images under emulation with QEMU is the easiest way to get started if your builder already supports it. Using emulation requires no changes to your Dockerfile, and BuildKit automatically detects the architectures that are available for emulation.\n\nNote\n\nEmulation with QEMU can be much slower than native builds, especially for compute-heavy tasks like compilation and compression or decompression.\n\nUse multiple native nodes or cross-compilation instead, if possible.\n\nDocker Desktop supports running and building multi-platform images under emulation by default. No configuration is necessary as the builder uses the QEMU that's bundled within the Docker Desktop VM.\n\nInstall QEMU manually\n\nIf you're using a builder outside of Docker Desktop, such as if you're using Docker Engine on Linux, or a custom remote builder, you need to install QEMU and register the executable types on the host OS. The prerequisites for installing QEMU are:\n\nLinux kernel version 4.8 or later\nbinfmt-support version 2.1.7 or later\nThe QEMU binaries must be statically compiled and registered with the fix_binary flag\n\nUse the tonistiigi/binfmt image to install QEMU and register the executable types on the host with a single command:\n\n$ docker run --privileged --rm tonistiigi/binfmt --install all\n\n\nThis installs the QEMU binaries and registers them with binfmt_misc, enabling QEMU to execute non-native file formats for emulation.\n\nOnce QEMU is installed and the executable types are registered on the host OS, they work transparently inside containers. You can verify your registration by checking if F is among the flags in /proc/sys/fs/binfmt_misc/qemu-*.\n\nMultiple native nodes\n\nUsing multiple native nodes provide better support for more complicated cases that QEMU can't handle, and also provides better performance.\n\nYou can add additional nodes to a builder using the --append flag.\n\nThe following command creates a multi-node builder from Docker contexts named node-amd64 and node-arm64. This example assumes that you've already added those contexts.\n\n$ docker buildx create --use --name mybuild node-amd64\n\nmybuild\n\n$ docker buildx create --append --name mybuild node-arm64\n\n$ docker buildx build --platform linux/amd64,linux/arm64 .\n\n\nWhile this approach has advantages over emulation, managing multi-node builders introduces some overhead of setting up and managing builder clusters. Alternatively, you can use Docker Build Cloud, a service that provides managed multi-node builders on Docker's infrastructure. With Docker Build Cloud, you get native multi-platform ARM and X86 builders without the burden of maintaining them. Using cloud builders also provides additional benefits, such as a shared build cache.\n\nAfter signing up for Docker Build Cloud, add the builder to your local environment and start building.\n\n$ docker buildx create --driver cloud ORG/BUILDER_NAME\n\ncloud-ORG-BUILDER_NAME\n\n$ docker build \\\n\n  --builder cloud-ORG-BUILDER_NAME \\\n\n  --platform linux/amd64,linux/arm64,linux/arm/v7 \\\n\n  --tag IMAGE_NAME \\\n\n  --push .\n\n\nFor more information, see Docker Build Cloud.\n\nCross-compilation\n\nDepending on your project, if the programming language you use has good support for cross-compilation, you can leverage multi-stage builds to build binaries for target platforms from the native architecture of the builder. Special build arguments, such as BUILDPLATFORM and TARGETPLATFORM, are automatically available for use in your Dockerfile.\n\nIn the following example, the FROM instruction is pinned to the native platform of the builder (using the --platform=$BUILDPLATFORM option) to prevent emulation from kicking in. Then the pre-defined $BUILDPLATFORM and $TARGETPLATFORM build arguments are interpolated in a RUN instruction. In this case, the values are just printed to stdout with echo, but this illustrates how you would pass them to the compiler for cross-compilation.\n\n# syntax=docker/dockerfile:1\n\nFROM --platform=$BUILDPLATFORM golang:alpine AS build\n\nARG TARGETPLATFORM\n\nARG BUILDPLATFORM\n\nRUN echo \"I am running on $BUILDPLATFORM, building for $TARGETPLATFORM\" > /log\n\nFROM alpine\n\nCOPY --from=build /log /log\nExamples\n\nHere are some examples of multi-platform builds:\n\nSimple multi-platform build using emulation\nMulti-platform Neovim build using Docker Build Cloud\nCross-compiling a Go application\nSimple multi-platform build using emulation\n\nThis example demonstrates how to build a simple multi-platform image using emulation with QEMU. The image contains a single file that prints the architecture of the container.\n\nPrerequisites:\n\nDocker Desktop, or Docker Engine with QEMU installed\ncontainerd image store enabled\n\nSteps:\n\nCreate an empty directory and navigate to it:\n\n$ mkdir multi-platform\n\n$ cd multi-platform\n\n\nCreate a simple Dockerfile that prints the architecture of the container:\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nRUN uname -m > /arch\n\nBuild the image for linux/amd64 and linux/arm64:\n\n$ docker build --platform linux/amd64,linux/arm64 -t multi-platform .\n\n\nRun the image and print the architecture:\n\n$ docker run --rm multi-platform cat /arch\n\nIf you're running on an x86-64 machine, you should see x86_64.\nIf you're running on an ARM machine, you should see aarch64.\nMulti-platform Neovim build using Docker Build Cloud\n\nThis example demonstrates how run a multi-platform build using Docker Build Cloud to compile and export Neovim binaries for the linux/amd64 and linux/arm64 platforms.\n\nDocker Build Cloud provides managed multi-node builders that support native multi-platform builds without the need for emulation, making it much faster to do CPU-intensive tasks like compilation.\n\nPrerequisites:\n\nYou've signed up for Docker Build Cloud and created a builder\n\nSteps:\n\nCreate an empty directory and navigate to it:\n\n$ mkdir docker-build-neovim\n\n$ cd docker-build-neovim\n\n\nCreate a Dockerfile that builds Neovim.\n\n# syntax=docker/dockerfile:1\n\nFROM debian:bookworm AS build\n\nWORKDIR /work\n\nRUN --mount=type=cache,target=/var/cache/apt,sharing=locked \\\n\n    --mount=type=cache,target=/var/lib/apt,sharing=locked \\\n\n    apt-get update && apt-get install -y \\\n\n    build-essential \\\n\n    cmake \\\n\n    curl \\\n\n    gettext \\\n\n    ninja-build \\\n\n    unzip\n\nADD https://github.com/neovim/neovim.git#stable .\n\nRUN make CMAKE_BUILD_TYPE=RelWithDebInfo\n\n\n\nFROM scratch\n\nCOPY --from=build /work/build/bin/nvim /\n\nBuild the image for linux/amd64 and linux/arm64 using Docker Build Cloud:\n\n$ docker build \\\n\n   --builder <cloud-builder> \\\n\n   --platform linux/amd64,linux/arm64 \\\n\n   --output ./bin .\n\n\nThis command builds the image using the cloud builder and exports the binaries to the bin directory.\n\nVerify that the binaries are built for both platforms. You should see the nvim binary for both linux/amd64 and linux/arm64.\n\n$ tree ./bin\n\n./bin\n\n‚îú‚îÄ‚îÄ linux_amd64\n\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ nvim\n\n‚îî‚îÄ‚îÄ linux_arm64\n\n    ‚îî‚îÄ‚îÄ nvim\n\n\n\n3 directories, 2 files\n\nCross-compiling a Go application\n\nThis example demonstrates how to cross-compile a Go application for multiple platforms using multi-stage builds. The application is a simple HTTP server that listens on port 8080 and returns the architecture of the container. This example uses Go, but the same principles apply to other programming languages that support cross-compilation.\n\nCross-compilation with Docker builds works by leveraging a series of pre-defined (in BuildKit) build arguments that give you information about platforms of the builder and the build targets. You can use these pre-defined arguments to pass the platform information to the compiler.\n\nIn Go, you can use the GOOS and GOARCH environment variables to specify the target platform to build for.\n\nPrerequisites:\n\nDocker Desktop or Docker Engine\n\nSteps:\n\nCreate an empty directory and navigate to it:\n\n$ mkdir go-server\n\n$ cd go-server\n\n\nCreate a base Dockerfile that builds the Go application:\n\n# syntax=docker/dockerfile:1\n\nFROM golang:alpine AS build\n\nWORKDIR /app\n\nADD https://github.com/dvdksn/buildme.git#eb6279e0ad8a10003718656c6867539bd9426ad8 .\n\nRUN go build -o server .\n\n\n\nFROM alpine\n\nCOPY --from=build /app/server /server\n\nENTRYPOINT [\"/server\"]\n\nThis Dockerfile can't build multi-platform with cross-compilation yet. If you were to try to build this Dockerfile with docker build, the builder would attempt to use emulation to build the image for the specified platforms.\n\nTo add cross-compilation support, update the Dockerfile to use the pre-defined BUILDPLATFORM, TARGETOS and TARGETARCH build arguments.\n\nPin the golang image to the platform of the builder using the --platform=$BUILDPLATFORM option.\nAdd ARG instructions for the Go compilation stages to make the TARGETOS and TARGETARCH build arguments available to the commands in this stage.\nSet the GOOS and GOARCH environment variables to the values of TARGETOS and TARGETARCH. The Go compiler uses these variables to do cross-compilation.\nUpdated Dockerfile Old Dockerfile Diff\n# syntax=docker/dockerfile:1\n\nFROM --platform=$BUILDPLATFORM golang:alpine AS build\n\nARG TARGETOS\n\nARG TARGETARCH\n\nWORKDIR /app\n\nADD https://github.com/dvdksn/buildme.git#eb6279e0ad8a10003718656c6867539bd9426ad8 .\n\nRUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o server .\n\n\n\nFROM alpine\n\nCOPY --from=build /app/server /server\n\nENTRYPOINT [\"/server\"]\n\nBuild the image for linux/amd64 and linux/arm64:\n\n$ docker build --platform linux/amd64,linux/arm64 -t go-server .\n\n\nThis example has shown how to cross-compile a Go application for multiple platforms with Docker builds. The specific steps on how to do cross-compilation may vary depending on the programming language you're using. Consult the documentation for your programming language to learn more about cross-compiling for different platforms.\n\nTip\n\nYou may also want to consider checking out xx - Dockerfile cross-compilation helpers. xx is a Docker image containing utility scripts that make cross-compiling with Docker builds easier.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhy multi-platform builds?\nDifference between single-platform and multi-platform images\nPrerequisites\nBuild multi-platform images\nStrategies\nQEMU\nMultiple native nodes\nCross-compilation\nExamples\nSimple multi-platform build using emulation\nMulti-platform Neovim build using Docker Build Cloud\nCross-compiling a Go application\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995648,
    "timestamp": "2026-02-07T06:34:33.279Z",
    "title": "Export binaries | Docker Docs",
    "url": "https://docs.docker.com/build/building/export/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nMulti-stage\nVariables\nSecrets\nMulti-platform\nExport binaries\nContainer Device Interface (CDI)\nBest practices\nBase images\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilding\n/\nExport binaries\nExport binaries\nCopy as Markdown\n\nDid you know that you can use Docker to build your application to standalone binaries? Sometimes, you don‚Äôt want to package and distribute your application as a Docker image. Use Docker to build your application, and use exporters to save the output to disk.\n\nThe default output format for docker build is a container image. That image is automatically loaded to your local image store, where you can run a container from that image, or push it to a registry. Under the hood, this uses the default exporter, called the docker exporter.\n\nTo export your build results as files instead, you can use the --output flag, or -o for short. the --output flag lets you change the output format of your build.\n\nExport binaries from a build\n\nIf you specify a filepath to the docker build --output flag, Docker exports the contents of the build container at the end of the build to the specified location on your host's filesystem. This uses the local exporter.\n\nThe neat thing about this is that you can use Docker's powerful isolation and build features to create standalone binaries. This works well for Go, Rust, and other languages that can compile to a single binary.\n\nThe following example creates a simple Rust program that prints \"Hello, World!\", and exports the binary to the host filesystem.\n\nCreate a new directory for this example, and navigate to it:\n\n$ mkdir hello-world-bin\n\n$ cd hello-world-bin\n\n\nCreate a Dockerfile with the following contents:\n\n# syntax=docker/dockerfile:1\n\nFROM rust:alpine AS build\n\nWORKDIR /src\n\nCOPY <<EOT hello.rs\n\nfn main() {\n\n    println!(\"Hello World!\");\n\n}\n\nEOT\n\nRUN rustc -o /bin/hello hello.rs\n\n\n\nFROM scratch\n\nCOPY --from=build /bin/hello /\n\nENTRYPOINT [\"/hello\"]\nTip\n\nThe COPY <<EOT syntax is a here-document. It lets you write multi-line strings in a Dockerfile. Here it's used to create a simple Rust program inline in the Dockerfile.\n\nThis Dockerfile uses a multi-stage build to compile the program in the first stage, and then copies the binary to a scratch image in the second. The final image is a minimal image that only contains the binary. This use case for the scratch image is common for creating minimal build artifacts for programs that don't require a full operating system to run.\n\nBuild the Dockerfile and export the binary to the current working directory:\n\n$ docker build --output=. .\n\n\nThis command builds the Dockerfile and exports the binary to the current working directory. The binary is named hello, and it's created in the current working directory.\n\nExporting multi-platform builds\n\nYou use the local exporter to export binaries in combination with multi-platform builds. This lets you compile multiple binaries at once, that can be run on any machine of any architecture, provided that the target platform is supported by the compiler you use.\n\nContinuing on the example Dockerfile in the Export binaries from a build section:\n\n# syntax=docker/dockerfile:1\n\nFROM rust:alpine AS build\n\nWORKDIR /src\n\nCOPY <<EOT hello.rs\n\nfn main() {\n\n    println!(\"Hello World!\");\n\n}\n\nEOT\n\nRUN rustc -o /bin/hello hello.rs\n\n\n\nFROM scratch\n\nCOPY --from=build /bin/hello /\n\nENTRYPOINT [\"/hello\"]\n\nYou can build this Rust program for multiple platforms using the --platform flag with the docker build command. In combination with the --output flag, the build exports the binaries for each target to the specified directory.\n\nFor example, to build the program for both linux/amd64 and linux/arm64:\n\n$ docker build --platform=linux/amd64,linux/arm64 --output=out .\n\n$ tree out/\n\nout/\n\n‚îú‚îÄ‚îÄ linux_amd64\n\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ hello\n\n‚îî‚îÄ‚îÄ linux_arm64\n\n    ‚îî‚îÄ‚îÄ hello\n\n\n\n3 directories, 2 files\n\nAdditional information\n\nIn addition to the local exporter, there are other exporters available. To learn more about the available exporters and how to use them, see the exporters documentation.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExport binaries from a build\nExporting multi-platform builds\nAdditional information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995649,
    "timestamp": "2026-02-07T06:34:33.285Z",
    "title": "Container Device Interface (CDI) | Docker Docs",
    "url": "https://docs.docker.com/build/building/cdi/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nMulti-stage\nVariables\nSecrets\nMulti-platform\nExport binaries\nContainer Device Interface (CDI)\nBest practices\nBase images\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilding\n/\nContainer Device Interface (CDI)\nContainer Device Interface (CDI)\nCopy as Markdown\n\nThe Container Device Interface (CDI) is a specification designed to standardize how devices (like GPUs, FPGAs, and other hardware accelerators) are exposed to and used by containers. The aim is to provide a more consistent and secure mechanism for using hardware devices in containerized environments, addressing the challenges associated with device-specific setups and configurations.\n\nIn addition to enabling the container to interact with the device node, CDI also lets you specify additional configuration for the device, such as environment variables, host mounts (such as shared objects), and executable hooks.\n\nGetting started\n\nTo get started with CDI, you need to have a compatible environment set up. This includes having Docker v27+ installed with CDI configured and Buildx v0.22+.\n\nYou also need to create the device specifications using JSON or YAML files in one of the following locations:\n\n/etc/cdi\n/var/run/cdi\n/etc/buildkit/cdi\nNote\n\nLocation can be changed by setting the specDirs option in the cdi section of the buildkitd.toml configuration file if you are using BuildKit directly. If you're building using the Docker Daemon with the docker driver, see Configure CDI devices documentation.\n\nNote\n\nIf you are creating a container builder on WSL, you need to ensure that Docker Desktop is installed and WSL 2 GPU Paravirtualization is enabled. Buildx v0.27+ is also required to mount the WSL libraries in the container.\n\nBuilding with a simple CDI specification\n\nLet's start with a simple CDI specification that injects an environment variable into the build environment and write it to /etc/cdi/foo.yaml:\n\n/etc/cdi/foo.yaml\ncdiVersion: \"0.6.0\"\n\nkind: \"vendor1.com/device\"\n\ndevices:\n\n- name: foo\n\n  containerEdits:\n\n    env:\n\n    - FOO=injected\n\nInspect the default builder to verify that vendor1.com/device is detected as a device:\n\n$ docker buildx inspect\n\nName:   default\n\nDriver: docker\n\n\n\nNodes:\n\nName:             default\n\nEndpoint:         default\n\nStatus:           running\n\nBuildKit version: v0.23.2\n\nPlatforms:        linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/amd64/v4, linux/386\n\nLabels:\n\n org.mobyproject.buildkit.worker.moby.host-gateway-ip: 172.17.0.1\n\nDevices:\n\n Name:                  vendor1.com/device=foo\n\n Automatically allowed: false\n\nGC Policy rule#0:\n\n All:            false\n\n Filters:        type==source.local,type==exec.cachemount,type==source.git.checkout\n\n Keep Duration:  48h0m0s\n\n Max Used Space: 658.9MiB\n\nGC Policy rule#1:\n\n All:            false\n\n Keep Duration:  1440h0m0s\n\n Reserved Space: 4.657GiB\n\n Max Used Space: 953.7MiB\n\n Min Free Space: 2.794GiB\n\nGC Policy rule#2:\n\n All:            false\n\n Reserved Space: 4.657GiB\n\n Max Used Space: 953.7MiB\n\n Min Free Space: 2.794GiB\n\nGC Policy rule#3:\n\n All:            true\n\n Reserved Space: 4.657GiB\n\n Max Used Space: 953.7MiB\n\n Min Free Space: 2.794GiB\n\n\nNow let's create a Dockerfile to use this device:\n\n# syntax=docker/dockerfile:1-labs\n\nFROM busybox\n\nRUN --device=vendor1.com/device \\\n\n  env | grep ^FOO=\n\nHere we use the RUN --device command and set vendor1.com/device which requests the first device available in the specification. In this case it uses foo, which is the first device in /etc/cdi/foo.yaml.\n\nNote\n\nRUN --device command is only featured in labs channel since Dockerfile frontend v1.14.0-labs and not yet available in stable syntax.\n\nNow let's build this Dockerfile:\n\n$ docker buildx build .\n\n[+] Building 0.4s (5/5) FINISHED                                                                                                        docker:default\n\n => [internal] load build definition from Dockerfile                                                                                    0.0s \n\n => => transferring dockerfile: 155B                                                                                                    0.0s\n\n => resolve image config for docker-image://docker/dockerfile:1-labs                                                                    0.1s \n\n => CACHED docker-image://docker/dockerfile:1-labs@sha256:9187104f31e3a002a8a6a3209ea1f937fb7486c093cbbde1e14b0fa0d7e4f1b5              0.0s\n\n => [internal] load metadata for docker.io/library/busybox:latest                                                                       0.1s \n\n => [internal] load .dockerignore                                                                                                       0.0s\n\n => => transferring context: 2B                                                                                                         0.0s \n\nERROR: failed to build: failed to solve: failed to load LLB: device vendor1.com/device=foo is requested by the build but not allowed\n\n\nIt fails because the device vendor1.com/device=foo is not automatically allowed by the build as shown in the buildx inspect output above:\n\nDevices:\n\n Name:                  vendor1.com/device=foo\n\n Automatically allowed: false\n\nTo allow the device, you can use the --allow flag with the docker buildx build command:\n\n$ docker buildx build --allow device .\n\n\nOr you can set the org.mobyproject.buildkit.device.autoallow annotation in the CDI specification to automatically allow the device for all builds:\n\n/etc/cdi/foo.yaml\ncdiVersion: \"0.6.0\"\n\nkind: \"vendor1.com/device\"\n\ndevices:\n\n- name: foo\n\n  containerEdits:\n\n    env:\n\n    - FOO=injected\n\nannotations:\n\n  org.mobyproject.buildkit.device.autoallow: true\n\nNow running the build again with the --allow device flag:\n\n$ docker buildx build --progress=plain --allow device .\n\n#0 building with \"default\" instance using docker driver\n\n\n\n#1 [internal] load build definition from Dockerfile\n\n#1 transferring dockerfile: 159B done\n\n#1 DONE 0.0s\n\n\n\n#2 resolve image config for docker-image://docker/dockerfile:1-labs\n\n#2 DONE 0.1s\n\n\n\n#3 docker-image://docker/dockerfile:1-labs@sha256:9187104f31e3a002a8a6a3209ea1f937fb7486c093cbbde1e14b0fa0d7e4f1b5\n\n#3 CACHED\n\n\n\n#4 [internal] load metadata for docker.io/library/busybox:latest\n\n#4 DONE 0.1s\n\n\n\n#5 [internal] load .dockerignore\n\n#5 transferring context: 2B done\n\n#5 DONE 0.0s\n\n\n\n#6 [1/2] FROM docker.io/library/busybox:latest@sha256:f85340bf132ae937d2c2a763b8335c9bab35d6e8293f70f606b9c6178d84f42b\n\n#6 CACHED\n\n\n\n#7 [2/2] RUN --device=vendor1.com/device   env | grep ^FOO=\n\n#7 0.155 FOO=injected\n\n#7 DONE 0.2s\n\n\nThe build is successful and the output shows that the FOO environment variable was injected into the build environment as specified in the CDI specification.\n\nSet up a container builder with GPU support\n\nIn this section, we will show you how to set up a container builder using NVIDIA GPUs. Since Buildx v0.22, when creating a new container builder, a GPU request is automatically added to the container builder if the host has GPU drivers installed in the kernel. This is similar to using --gpus=all with the docker run command.\n\nNow let's create a container builder named gpubuilder using Buildx:\n\n$ docker buildx create --name gpubuilder --driver-opt \"image=moby/buildkit:buildx-stable-1-gpu\" --bootstrap\n\n#1 [internal] booting buildkit\n\n#1 pulling image moby/buildkit:buildx-stable-1-gpu\n\n#1 pulling image moby/buildkit:buildx-stable-1-gpu 1.0s done\n\n#1 creating container buildx_buildkit_gpubuilder0\n\n#1 creating container buildx_buildkit_gpubuilder0 8.8s done\n\n#1 DONE 9.8s\n\ngpubuilder\n\nNote\n\nWe made a specially crafted BuildKit image because the current BuildKit release image is based on Alpine that doesn't support NVIDIA drivers. The following image is based on Ubuntu and installs the NVIDIA client libraries and generates the CDI specification for your GPU in the container builder if a device is requested during a build.\n\nLet's inspect this builder:\n\n$ docker buildx inspect gpubuilder\n\nName:          gpubuilder\n\nDriver:        docker-container\n\nLast Activity: 2025-07-10 08:18:09 +0000 UTC\n\n\n\nNodes:\n\nName:                  gpubuilder0\n\nEndpoint:              unix:///var/run/docker.sock\n\nDriver Options:        image=\"moby/buildkit:buildx-stable-1-gpu\"\n\nStatus:                running\n\nBuildKit daemon flags: --allow-insecure-entitlement=network.host\n\nBuildKit version:      v0.26.2\n\nPlatforms:             linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/arm64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/arm/v7, linux/arm/v6\n\nLabels:\n\n org.mobyproject.buildkit.worker.executor:         oci\n\n org.mobyproject.buildkit.worker.hostname:         d6aa9cbe8462\n\n org.mobyproject.buildkit.worker.network:          host\n\n org.mobyproject.buildkit.worker.oci.process-mode: sandbox\n\n org.mobyproject.buildkit.worker.selinux.enabled:  false\n\n org.mobyproject.buildkit.worker.snapshotter:      overlayfs\n\nDevices:\n\n Name:      nvidia.com/gpu\n\n On-Demand: true\n\nGC Policy rule#0:\n\n All:            false\n\n Filters:        type==source.local,type==exec.cachemount,type==source.git.checkout\n\n Keep Duration:  48h0m0s\n\n Max Used Space: 488.3MiB\n\nGC Policy rule#1:\n\n All:            false\n\n Keep Duration:  1440h0m0s\n\n Reserved Space: 9.313GiB\n\n Max Used Space: 93.13GiB\n\n Min Free Space: 188.1GiB\n\nGC Policy rule#2:\n\n All:            false\n\n Reserved Space: 9.313GiB\n\n Max Used Space: 93.13GiB\n\n Min Free Space: 188.1GiB\n\nGC Policy rule#3:\n\n All:            true\n\n Reserved Space: 9.313GiB\n\n Max Used Space: 93.13GiB\n\n Min Free Space: 188.1GiB\n\n\nWe can see nvidia.com/gpu vendor is detected as a device in the builder which means that drivers were detected.\n\nOptionally you can check if NVIDIA GPU devices are available in the container using nvidia-smi:\n\n$ docker exec -it buildx_buildkit_gpubuilder0 nvidia-smi -L\n\nGPU 0: Tesla T4 (UUID: GPU-6cf00fa7-59ac-16f2-3e83-d24ccdc56f84)\n\nBuilding with GPU support\n\nLet's create a simple Dockerfile that will use the GPU device:\n\n# syntax=docker/dockerfile:1-labs\n\nFROM ubuntu\n\nRUN --device=nvidia.com/gpu nvidia-smi -L\n\nNow run the build using the gpubuilder builder we created earlier:\n\n$ docker buildx --builder gpubuilder build --progress=plain .\n\n#0 building with \"gpubuilder\" instance using docker-container driver\n\n...\n\n\n\n#7 preparing device nvidia.com/gpu\n\n#7 0.000 > apt-get update\n\n...\n\n#7 4.872 > apt-get install -y gpg\n\n...\n\n#7 10.16 Downloading NVIDIA GPG key\n\n#7 10.21 > apt-get update\n\n...\n\n#7 12.15 > apt-get install -y --no-install-recommends nvidia-container-toolkit-base\n\n...\n\n#7 17.80 time=\"2025-04-15T08:58:16Z\" level=info msg=\"Generated CDI spec with version 0.8.0\"\n\n#7 DONE 17.8s\n\n\n\n#8 [2/2] RUN --device=nvidia.com/gpu nvidia-smi -L\n\n#8 0.527 GPU 0: Tesla T4 (UUID: GPU-6cf00fa7-59ac-16f2-3e83-d24ccdc56f84)\n\n#8 DONE 1.6s\n\n\nAs you might have noticed, the step #7 is preparing the nvidia.com/gpu device by installing client libraries and the toolkit to generate the CDI specifications for the GPU.\n\nThe nvidia-smi -L command is then executed in the container using the GPU device. The output shows the GPU UUID.\n\nYou can check the generated CDI specification within the container builder with the following command:\n\n$ docker exec -it buildx_buildkit_gpubuilder0 cat /etc/cdi/nvidia.yaml\n\n\nFor the EC2 instance g4dn.xlarge used here, it looks like this:\n\nShow more\ncdiVersion: 0.6.0\n\ncontainerEdits:\n\n  deviceNodes:\n\n  - path: /dev/nvidia-modeset\n\n  - path: /dev/nvidia-uvm\n\n  - path: /dev/nvidia-uvm-tools\n\n  - path: /dev/nvidiactl\n\n  env:\n\n  - NVIDIA_VISIBLE_DEVICES=void\n\n  hooks:\n\n  - args:\n\n    - nvidia-cdi-hook\n\n    - create-symlinks\n\n    - --link\n\n    - ../libnvidia-allocator.so.1::/usr/lib/x86_64-linux-gnu/gbm/nvidia-drm_gbm.so\n\n    hookName: createContainer\n\n    path: /usr/bin/nvidia-cdi-hook\n\n  - args:\n\n    - nvidia-cdi-hook\n\n    - create-symlinks\n\n    - --link\n\n    - libcuda.so.1::/usr/lib/x86_64-linux-gnu/libcuda.so\n\n    hookName: createContainer\n\n    path: /usr/bin/nvidia-cdi-hook\n\n  - args:\n\n    - nvidia-cdi-hook\n\n    - enable-cuda-compat\n\n    - --host-driver-version=570.133.20\n\n    hookName: createContainer\n\n    path: /usr/bin/nvidia-cdi-hook\n\n  - args:\n\n    - nvidia-cdi-hook\n\n    - update-ldcache\n\n    - --folder\n\n    - /usr/lib/x86_64-linux-gnu\n\n    hookName: createContainer\n\n    path: /usr/bin/nvidia-cdi-hook\n\n  mounts:\n\n  - containerPath: /run/nvidia-persistenced/socket\n\n    hostPath: /run/nvidia-persistenced/socket\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n    - noexec\n\n  - containerPath: /usr/bin/nvidia-cuda-mps-control\n\n    hostPath: /usr/bin/nvidia-cuda-mps-control\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/bin/nvidia-cuda-mps-server\n\n    hostPath: /usr/bin/nvidia-cuda-mps-server\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/bin/nvidia-debugdump\n\n    hostPath: /usr/bin/nvidia-debugdump\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/bin/nvidia-persistenced\n\n    hostPath: /usr/bin/nvidia-persistenced\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/bin/nvidia-smi\n\n    hostPath: /usr/bin/nvidia-smi\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libcuda.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libcuda.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libcudadebugger.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libcudadebugger.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-allocator.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-allocator.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-cfg.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-gpucomp.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-gpucomp.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-nscq.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-nscq.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-nvvm.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-nvvm.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-pkcs11-openssl3.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-pkcs11-openssl3.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-pkcs11.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-pkcs11.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.570.133.20\n\n    hostPath: /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.570.133.20\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /lib/firmware/nvidia/570.133.20/gsp_ga10x.bin\n\n    hostPath: /lib/firmware/nvidia/570.133.20/gsp_ga10x.bin\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\n  - containerPath: /lib/firmware/nvidia/570.133.20/gsp_tu10x.bin\n\n    hostPath: /lib/firmware/nvidia/570.133.20/gsp_tu10x.bin\n\n    options:\n\n    - ro\n\n    - nosuid\n\n    - nodev\n\n    - bind\n\ndevices:\n\n- containerEdits:\n\n    deviceNodes:\n\n    - path: /dev/nvidia0\n\n  name: \"0\"\n\n- containerEdits:\n\n    deviceNodes:\n\n    - path: /dev/nvidia0\n\n  name: GPU-6cf00fa7-59ac-16f2-3e83-d24ccdc56f84\n\n- containerEdits:\n\n    deviceNodes:\n\n    - path: /dev/nvidia0\n\n  name: all\n\nkind: nvidia.com/gpu\n\nCongrats on your first build using a GPU device with BuildKit and CDI.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nGetting started\nBuilding with a simple CDI specification\nSet up a container builder with GPU support\nBuilding with GPU support\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995652,
    "timestamp": "2026-02-07T06:34:33.290Z",
    "title": "Best practices | Docker Docs",
    "url": "https://docs.docker.com/build/building/best-practices/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nMulti-stage\nVariables\nSecrets\nMulti-platform\nExport binaries\nContainer Device Interface (CDI)\nBest practices\nBase images\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilding\n/\nBest practices\nBuilding best practices\nCopy as Markdown\nUse multi-stage builds\n\nMulti-stage builds let you reduce the size of your final image, by creating a cleaner separation between the building of your image and the final output. Split your Dockerfile instructions into distinct stages to make sure that the resulting output only contains the files that are needed to run the application.\n\nUsing multiple stages can also let you build more efficiently by executing build steps in parallel.\n\nSee Multi-stage builds for more information.\n\nCreate reusable stages\n\nIf you have multiple images with a lot in common, consider creating a reusable stage that includes the shared components, and basing your unique stages on that. Docker only needs to build the common stage once. This means that your derivative images use memory on the Docker host more efficiently and load more quickly.\n\nIt's also easier to maintain a common base stage (\"Don't repeat yourself\"), than it is to have multiple different stages doing similar things.\n\nChoose the right base image\n\nThe first step towards achieving a secure image is to choose the right base image. When choosing an image, ensure it's built from a trusted source and keep it small.\n\nDocker Official Images are a curated collection that have clear documentation, promote best practices, and are regularly updated. They provide a trusted starting point for many applications.\n\nVerified Publisher images are high-quality images published and maintained by the organizations partnering with Docker, with Docker verifying the authenticity of the content in their repositories.\n\nDocker-Sponsored Open Source are published and maintained by open source projects sponsored by Docker through an open source program.\n\nWhen you pick your base image, look out for the badges indicating that the image is part of these programs.\n\nWhen building your own image from a Dockerfile, ensure you choose a minimal base image that matches your requirements. A smaller base image not only offers portability and fast downloads, but also shrinks the size of your image and minimizes the number of vulnerabilities introduced through the dependencies.\n\nYou should also consider using two types of base image: one for building and unit testing, and another (typically slimmer) image for production. In the later stages of development, your image may not require build tools such as compilers, build systems, and debugging tools. A small image with minimal dependencies can considerably lower the attack surface.\n\nRebuild your images often\n\nDocker images are immutable. Building an image is taking a snapshot of that image at that moment. That includes any base images, libraries, or other software you use in your build. To keep your images up-to-date and secure, rebuild your images regularly with updated dependencies.\n\nUse --pull to get fresh base images\n\nThe following Dockerfile uses the 24.04 tag of the ubuntu image. Over time, that tag may resolve to a different underlying version of the ubuntu image, as the publisher rebuilds the image with new security patches and updated libraries.\n\n# syntax=docker/dockerfile:1\n\nFROM ubuntu:24.04\n\nRUN apt-get -y update && apt-get install -y --no-install-recommends python3\n\nTo get the latest version of the base image, use the --pull flag:\n\n$ docker build --pull -t my-image:my-tag .\n\n\nThe --pull flag forces Docker to check for and download a newer version of the base image, even if you have a version cached locally.\n\nUse --no-cache for clean builds\n\nThe --no-cache flag disables the build cache, forcing Docker to rebuild all layers from scratch:\n\n$ docker build --no-cache -t my-image:my-tag .\n\n\nThis gets the latest available versions of dependencies from package managers like apt-get or npm. However, --no-cache doesn't pull a fresh base image - it only prevents reusing cached layers. For a completely fresh build with the latest base image, combine both flags:\n\n$ docker build --pull --no-cache -t my-image:my-tag .\n\n\nAlso consider pinning base image versions.\n\nExclude with .dockerignore\n\nTo exclude files not relevant to the build, without restructuring your source repository, use a .dockerignore file. This file supports exclusion patterns similar to .gitignore files.\n\nFor example, to exclude all files with the .md extension:\n\n*.md\n\nFor information on creating one, see Dockerignore file.\n\nCreate ephemeral containers\n\nThe image defined by your Dockerfile should generate containers that are as ephemeral as possible. Ephemeral means that the container can be stopped and destroyed, then rebuilt and replaced with an absolute minimum set up and configuration.\n\nRefer to Processes under The Twelve-factor App methodology to get a feel for the motivations of running containers in such a stateless fashion.\n\nDon't install unnecessary packages\n\nAvoid installing extra or unnecessary packages just because they might be nice to have. For example, you don‚Äôt need to include a text editor in a database image.\n\nWhen you avoid installing extra or unnecessary packages, your images have reduced complexity, reduced dependencies, reduced file sizes, and reduced build times.\n\nDecouple applications\n\nEach container should have only one concern. Decoupling applications into multiple containers makes it easier to scale horizontally and reuse containers. For instance, a web application stack might consist of three separate containers, each with its own unique image, to manage the web application, database, and an in-memory cache in a decoupled manner.\n\nLimiting each container to one process is a good rule of thumb, but it's not a hard and fast rule. For example, not only can containers be spawned with an init process, some programs might spawn additional processes of their own accord. For instance, Celery can spawn multiple worker processes, and Apache can create one process per request.\n\nUse your best judgment to keep containers as clean and modular as possible. If containers depend on each other, you can use Docker container networks to ensure that these containers can communicate.\n\nSort multi-line arguments\n\nWhenever possible, sort multi-line arguments alphanumerically to make maintenance easier. This helps to avoid duplication of packages and make the list much easier to update. This also makes PRs a lot easier to read and review. Adding a space before a backslash (\\) helps as well.\n\nHere‚Äôs an example from the buildpack-deps image:\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\n  bzr \\\n\n  cvs \\\n\n  git \\\n\n  mercurial \\\n\n  subversion \\\n\n  && rm -rf /var/lib/apt/lists/*\nLeverage build cache\n\nWhen building an image, Docker steps through the instructions in your Dockerfile, executing each in the order specified. For each instruction, Docker checks whether it can reuse the instruction from the build cache.\n\nUnderstanding how the build cache works, and how cache invalidation occurs, is critical for ensuring faster builds. For more information about the Docker build cache and how to optimize your builds, see Docker build cache.\n\nPin base image versions\n\nImage tags are mutable, meaning a publisher can update a tag to point to a new image. This is useful because it lets publishers update tags to point to newer versions of an image. And as an image consumer, it means you automatically get the new version when you re-build your image.\n\nFor example, if you specify FROM alpine:3.21 in your Dockerfile, 3.21 resolves to the latest patch version for 3.21.\n\n# syntax=docker/dockerfile:1\n\nFROM alpine:3.21\n\nAt one point in time, the 3.21 tag might point to version 3.21.1 of the image. If you rebuild the image 3 months later, the same tag might point to a different version, such as 3.21.4. This publishing workflow is best practice, and most publishers use this tagging strategy, but it isn't enforced.\n\nThe downside with this is that you're not guaranteed to get the same for every build. This could result in breaking changes, and it means you also don't have an audit trail of the exact image versions that you're using.\n\nTo fully secure your supply chain integrity, you can pin the image version to a specific digest. By pinning your images to a digest, you're guaranteed to always use the same image version, even if a publisher replaces the tag with a new image. For example, the following Dockerfile pins the Alpine image to the same tag as earlier, 3.21, but this time with a digest reference as well.\n\n# syntax=docker/dockerfile:1\n\nFROM alpine:3.21@sha256:a8560b36e8b8210634f77d9f7f9efd7ffa463e380b75e2e74aff4511df3ef88c\n\nWith this Dockerfile, even if the publisher updates the 3.21 tag, your builds would still use the pinned image version: a8560b36e8b8210634f77d9f7f9efd7ffa463e380b75e2e74aff4511df3ef88c.\n\nWhile this helps you avoid unexpected changes, it's also more tedious to have to look up and include the image digest for base image versions manually each time you want to update it. And you're opting out of automated security fixes, which is likely something you want to get.\n\nDocker Scout's default Up-to-Date Base Images policy checks whether the base image version you're using is in fact the latest version. This policy also checks if pinned digests in your Dockerfile correspond to the correct version. If a publisher updates an image that you've pinned, the policy evaluation returns a non-compliant status, indicating that you should update your image.\n\nDocker Scout also supports an automated remediation workflow for keeping your base images up-to-date. When a new image digest is available, Docker Scout can automatically raise a pull request on your repository to update your Dockerfiles to use the latest version. This is better than using a tag that changes the version automatically, because you're in control and you have an audit trail of when and how the change occurred.\n\nFor more information about automatically updating your base images with Docker Scout, see Remediation.\n\nBuild and test your images in CI\n\nWhen you check in a change to source control or create a pull request, use GitHub Actions or another CI/CD pipeline to automatically build and tag a Docker image and test it.\n\nDockerfile instructions\n\nFollow these recommendations on how to properly use the Dockerfile instructions to create an efficient and maintainable Dockerfile.\n\nTip\n\nTo improve linting, code navigation, and vulnerability scanning of your Dockerfiles in Visual Studio Code see Docker VS Code Extension.\n\nFROM\n\nWhenever possible, use current official images as the basis for your images. Docker recommends the Alpine image as it is tightly controlled and small in size (currently under 6 MB), while still being a full Linux distribution.\n\nFor more information about the FROM instruction, see Dockerfile reference for the FROM instruction.\n\nLABEL\n\nYou can add labels to your image to help organize images by project, record licensing information, to aid in automation, or for other reasons. For each label, add a line beginning with LABEL with one or more key-value pairs. The following examples show the different acceptable formats. Explanatory comments are included inline.\n\nStrings with spaces must be quoted or the spaces must be escaped. Inner quote characters (\"), must also be escaped. For example:\n\n# Set one or more individual labels\n\nLABEL com.example.version=\"0.0.1-beta\"\n\nLABEL vendor1=\"ACME Incorporated\"\n\nLABEL vendor2=ZENITH\\ Incorporated\n\nLABEL com.example.release-date=\"2015-02-12\"\n\nLABEL com.example.version.is-production=\"\"\n\nAn image can have more than one label. Prior to Docker 1.10, it was recommended to combine all labels into a single LABEL instruction, to prevent extra layers from being created. This is no longer necessary, but combining labels is still supported. For example:\n\n# Set multiple labels on one line\n\nLABEL com.example.version=\"0.0.1-beta\" com.example.release-date=\"2015-02-12\"\n\nThe above example can also be written as:\n\n# Set multiple labels at once, using line-continuation characters to break long lines\n\nLABEL vendor=ACME\\ Incorporated \\\n\n      com.example.is-beta= \\\n\n      com.example.is-production=\"\" \\\n\n      com.example.version=\"0.0.1-beta\" \\\n\n      com.example.release-date=\"2015-02-12\"\n\nSee Understanding object labels for guidelines about acceptable label keys and values. For information about querying labels, refer to the items related to filtering in Managing labels on objects. See also LABEL in the Dockerfile reference.\n\nRUN\n\nSplit long or complex RUN statements on multiple lines separated with backslashes to make your Dockerfile more readable, understandable, and maintainable.\n\nFor example, you can chain commands with the && operator, and use escape characters to break long commands into multiple lines.\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\n    package-bar \\\n\n    package-baz \\\n\n    package-foo\n\nBy default, backslash escapes a newline character, but you can change it with the escape directive.\n\nYou can also use here documents to run multiple commands without chaining them with a pipeline operator:\n\nRUN <<EOF\n\napt-get update\n\napt-get install -y --no-install-recommends \\\n\n    package-bar \\\n\n    package-baz \\\n\n    package-foo\n\nEOF\n\nFor more information about RUN, see Dockerfile reference for the RUN instruction.\n\napt-get\n\nOne common use case for RUN instructions in Debian-based images is to install software using apt-get. Because apt-get installs packages, the RUN apt-get command has several counter-intuitive behaviors to look out for.\n\nAlways combine RUN apt-get update with apt-get install in the same RUN statement. For example:\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\n    package-bar \\\n\n    package-baz \\\n\n    package-foo\n\nUsing apt-get update alone in a RUN statement causes caching issues and subsequent apt-get install instructions to fail. For example, this issue will occur in the following Dockerfile:\n\n# syntax=docker/dockerfile:1\n\n\n\nFROM ubuntu:22.04\n\nRUN apt-get update\n\nRUN apt-get install -y --no-install-recommends curl\n\nAfter building the image, all layers are in the Docker cache. Suppose you later modify apt-get install by adding an extra package as shown in the following Dockerfile:\n\n# syntax=docker/dockerfile:1\n\n\n\nFROM ubuntu:22.04\n\nRUN apt-get update\n\nRUN apt-get install -y --no-install-recommends curl nginx\n\nDocker sees the initial and modified instructions as identical and reuses the cache from previous steps. As a result the apt-get update isn't executed because the build uses the cached version. Because the apt-get update isn't run, your build can potentially get an outdated version of the curl and nginx packages.\n\nUsing RUN apt-get update && apt-get install -y --no-install-recommends ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as cache busting. You can also achieve cache busting by specifying a package version. This is known as version pinning. For example:\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\n    package-bar \\\n\n    package-baz \\\n\n    package-foo=1.3.*\n\nVersion pinning forces the build to retrieve a particular version regardless of what‚Äôs in the cache. This technique can also reduce failures due to unanticipated changes in required packages.\n\nBelow is a well-formed RUN instruction that demonstrates all the apt-get recommendations.\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n\n    aufs-tools \\\n\n    automake \\\n\n    build-essential \\\n\n    curl \\\n\n    dpkg-sig \\\n\n    libcap-dev \\\n\n    libsqlite3-dev \\\n\n    mercurial \\\n\n    reprepro \\\n\n    ruby1.9.1 \\\n\n    ruby1.9.1-dev \\\n\n    s3cmd=1.1.* \\\n\n    && rm -rf /var/lib/apt/lists/*\n\nThe s3cmd argument specifies a version 1.1.*. If the image previously used an older version, specifying the new one causes a cache bust of apt-get update and ensures the installation of the new version. Listing packages on each line can also prevent mistakes in package duplication.\n\nIn addition, when you clean up the apt cache by removing /var/lib/apt/lists it reduces the image size, since the apt cache isn't stored in a layer. Since the RUN statement starts with apt-get update, the package cache is always refreshed prior to apt-get install.\n\nOfficial Debian and Ubuntu images automatically run apt-get clean, so explicit invocation is not required.\n\nUsing pipes\n\nSome RUN commands depend on the ability to pipe the output of one command into another, using the pipe character (|), as in the following example:\n\nRUN wget -O - https://some.site | wc -l > /number\n\nDocker executes these commands using the /bin/sh -c interpreter, which only evaluates the exit code of the last operation in the pipe to determine success. In the example above, this build step succeeds and produces a new image so long as the wc -l command succeeds, even if the wget command fails.\n\nIf you want the command to fail due to an error at any stage in the pipe, prepend set -o pipefail && to ensure that an unexpected error prevents the build from inadvertently succeeding. For example:\n\nRUN set -o pipefail && wget -O - https://some.site | wc -l > /number\nNote\n\nNot all shells support the -o pipefail option.\n\nIn cases such as the dash shell on Debian-based images, consider using the exec form of RUN to explicitly choose a shell that does support the pipefail option. For example:\n\nRUN [\"/bin/bash\", \"-c\", \"set -o pipefail && wget -O - https://some.site | wc -l > /number\"]\nCMD\n\nThe CMD instruction should be used to run the software contained in your image, along with any arguments. CMD should almost always be used in the form of CMD [\"executable\", \"param1\", \"param2\"]. Thus, if the image is for a service, such as Apache and Rails, you would run something like CMD [\"apache2\",\"-DFOREGROUND\"]. Indeed, this form of the instruction is recommended for any service-based image.\n\nIn most other cases, CMD should be given an interactive shell, such as bash, Python and perl. For example, CMD [\"perl\", \"-de0\"], CMD [\"python\"], or CMD [\"php\", \"-a\"]. Using this form means that when you execute something like docker run -it python, you‚Äôll get dropped into a usable shell, ready to go. CMD should rarely be used in the manner of CMD [\"param\", \"param\"] in conjunction with ENTRYPOINT, unless you and your expected users are already quite familiar with how ENTRYPOINT works.\n\nFor more information about CMD, see Dockerfile reference for the CMD instruction.\n\nEXPOSE\n\nThe EXPOSE instruction indicates the ports on which a container listens for connections. Consequently, you should use the common, traditional port for your application. For example, an image containing the Apache web server would use EXPOSE 80, while an image containing MongoDB would use EXPOSE 27017 and so on.\n\nFor external access, your users can execute docker run with a flag indicating how to map the specified port to the port of their choice. For container linking, Docker provides environment variables for the path from the recipient container back to the source (for example, MYSQL_PORT_3306_TCP).\n\nFor more information about EXPOSE, see Dockerfile reference for the EXPOSE instruction.\n\nENV\n\nTo make new software easier to run, you can use ENV to update the PATH environment variable for the software your container installs. For example, ENV PATH=/usr/local/nginx/bin:$PATH ensures that CMD [\"nginx\"] just works.\n\nThe ENV instruction is also useful for providing the required environment variables specific to services you want to containerize, such as Postgres‚Äôs PGDATA.\n\nLastly, ENV can also be used to set commonly used version numbers so that version bumps are easier to maintain, as seen in the following example:\n\nENV PG_MAJOR=9.3\n\nENV PG_VERSION=9.3.4\n\nRUN curl -SL https://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgres && ‚Ä¶\n\nENV PATH=/usr/local/postgres-$PG_MAJOR/bin:$PATH\n\nSimilar to having constant variables in a program, as opposed to hard-coding values, this approach lets you change a single ENV instruction to automatically bump the version of the software in your container.\n\nEach ENV line creates a new intermediate layer, just like RUN commands. This means that even if you unset the environment variable in a future layer, it still persists in this layer and its value can be dumped. You can test this by creating a Dockerfile like the following, and then building it.\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nENV ADMIN_USER=\"mark\"\n\nRUN echo $ADMIN_USER > ./mark\n\nRUN unset ADMIN_USER\n$ docker run --rm test sh -c 'echo $ADMIN_USER'\n\n\n\nmark\n\n\nTo prevent this and unset the environment variable, use a RUN command with shell commands, to set, use, and unset the variable all in a single layer. You can separate your commands with ; or &&. If you use the second method, and one of the commands fails, the docker build also fails. This is usually a good idea. Using \\ as a line continuation character for Linux Dockerfiles improves readability. You could also put all of the commands into a shell script and have the RUN command just run that shell script.\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nRUN export ADMIN_USER=\"mark\" \\\n\n    && echo $ADMIN_USER > ./mark \\\n\n    && unset ADMIN_USER\n\nCMD sh\n$ docker run --rm test sh -c 'echo $ADMIN_USER'\n\n\nFor more information about ENV, see Dockerfile reference for the ENV instruction.\n\nADD or COPY\n\nADD and COPY are functionally similar. COPY supports basic copying of files into the container, from the build context or from a stage in a multi-stage build. ADD supports features for fetching files from remote HTTPS and Git URLs, and extracting tar files automatically when adding files from the build context.\n\nYou'll mostly want to use COPY for copying files from one stage to another in a multi-stage build. If you need to add files from the build context to the container temporarily to execute a RUN instruction, you can often substitute the COPY instruction with a bind mount instead. For example, to temporarily add a requirements.txt file for a RUN pip install instruction:\n\nRUN --mount=type=bind,source=requirements.txt,target=/tmp/requirements.txt \\\n\n    pip install --requirement /tmp/requirements.txt\n\nBind mounts are more efficient than COPY for including files from the build context in the container. Note that bind-mounted files are only added temporarily for a single RUN instruction, and don't persist in the final image. If you need to include files from the build context in the final image, use COPY.\n\nThe ADD instruction is best for when you need to download a remote artifact as part of your build. ADD is better than manually adding files using something like wget and tar, because it ensures a more precise build cache. ADD also has built-in support for checksum validation of the remote resources, and a protocol for parsing branches, tags, and subdirectories from Git URLs.\n\nThe following example uses ADD to download a .NET installer. Combined with multi-stage builds, only the .NET runtime remains in the final stage, no intermediate files.\n\n# syntax=docker/dockerfile:1\n\n\n\nFROM scratch AS src\n\nARG DOTNET_VERSION=8.0.0-preview.6.23329.7\n\nADD --checksum=sha256:270d731bd08040c6a3228115de1f74b91cf441c584139ff8f8f6503447cebdbb \\\n\n    https://dotnetcli.azureedge.net/dotnet/Runtime/$DOTNET_VERSION/dotnet-runtime-$DOTNET_VERSION-linux-arm64.tar.gz /dotnet.tar.gz\n\n\n\nFROM mcr.microsoft.com/dotnet/runtime-deps:8.0.0-preview.6-bookworm-slim-arm64v8 AS installer\n\n\n\n# Retrieve .NET Runtime\n\nRUN --mount=from=src,target=/src <<EOF\n\nmkdir -p /dotnet\n\ntar -oxzf /src/dotnet.tar.gz -C /dotnet\n\nEOF\n\n\n\nFROM mcr.microsoft.com/dotnet/runtime-deps:8.0.0-preview.6-bookworm-slim-arm64v8\n\n\n\nCOPY --from=installer /dotnet /usr/share/dotnet\n\nRUN ln -s /usr/share/dotnet/dotnet /usr/bin/dotnet\n\nFor more information about ADD or COPY, see the following:\n\nDockerfile reference for the ADD instruction\nDockerfile reference for the COPY instruction\nENTRYPOINT\n\nThe best use for ENTRYPOINT is to set the image's main command, allowing that image to be run as though it was that command, and then use CMD as the default flags.\n\nThe following is an example of an image for the command line tool s3cmd:\n\nENTRYPOINT [\"s3cmd\"]\n\nCMD [\"--help\"]\n\nYou can use the following command to run the image and show the command's help:\n\n$ docker run s3cmd\n\n\nOr, you can use the right parameters to execute a command, like in the following example:\n\n$ docker run s3cmd ls s3://mybucket\n\n\nThis is useful because the image name can double as a reference to the binary as shown in the command above.\n\nThe ENTRYPOINT instruction can also be used in combination with a helper script, allowing it to function in a similar way to the command above, even when starting the tool may require more than one step.\n\nFor example, the Postgres Official Image uses the following script as its ENTRYPOINT:\n\n#!/bin/bash\n\nset -e\n\n\n\nif [ \"$1\" = 'postgres' ]; then\n\n    chown -R postgres \"$PGDATA\"\n\n\n\n    if [ -z \"$(ls -A \"$PGDATA\")\" ]; then\n\n        gosu postgres initdb\n\n    fi\n\n\n\n    exec gosu postgres \"$@\"\n\nfi\n\n\n\nexec \"$@\"\n\nThis script uses the exec Bash command so that the final running application becomes the container's PID 1. This allows the application to receive any Unix signals sent to the container. For more information, see the ENTRYPOINT reference.\n\nIn the following example, a helper script is copied into the container and run via ENTRYPOINT on container start:\n\nCOPY ./docker-entrypoint.sh /\n\nENTRYPOINT [\"/docker-entrypoint.sh\"]\n\nCMD [\"postgres\"]\n\nThis script lets you interact with Postgres in several ways.\n\nIt can simply start Postgres:\n\n$ docker run postgres\n\n\nOr, you can use it to run Postgres and pass parameters to the server:\n\n$ docker run postgres postgres --help\n\n\nLastly, you can use it to start a totally different tool, such as Bash:\n\n$ docker run --rm -it postgres bash\n\n\nFor more information about ENTRYPOINT, see Dockerfile reference for the ENTRYPOINT instruction.\n\nVOLUME\n\nYou should use the VOLUME instruction to expose any database storage area, configuration storage, or files and folders created by your Docker container. You are strongly encouraged to use VOLUME for any combination of mutable or user-serviceable parts of your image.\n\nFor more information about VOLUME, see Dockerfile reference for the VOLUME instruction.\n\nUSER\n\nIf a service can run without privileges, use USER to change to a non-root user. Start by creating the user and group in the Dockerfile with something like the following example:\n\nRUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres\nNote\n\nConsider an explicit UID/GID.\n\nUsers and groups in an image are assigned a non-deterministic UID/GID in that the \"next\" UID/GID is assigned regardless of image rebuilds. So, if it‚Äôs critical, you should assign an explicit UID/GID.\n\nNote\n\nDue to an unresolved bug in the Go archive/tar package's handling of sparse files, attempting to create a user with a significantly large UID inside a Docker container can lead to disk exhaustion because /var/log/faillog in the container layer is filled with NULL (\\0) characters. A workaround is to pass the --no-log-init flag to useradd. The Debian/Ubuntu adduser wrapper does not support this flag.\n\nAvoid installing or using sudo as it has unpredictable TTY and signal-forwarding behavior that can cause problems. If you absolutely need functionality similar to sudo, such as initializing the daemon as root but running it as non-root, consider using ‚Äúgosu‚Äù.\n\nLastly, to reduce layers and complexity, avoid switching USER back and forth frequently.\n\nFor more information about USER, see Dockerfile reference for the USER instruction.\n\nWORKDIR\n\nFor clarity and reliability, you should always use absolute paths for your WORKDIR. Also, you should use WORKDIR instead of proliferating instructions like RUN cd ‚Ä¶ && do-something, which are hard to read, troubleshoot, and maintain.\n\nFor more information about WORKDIR, see Dockerfile reference for the WORKDIR instruction.\n\nONBUILD\n\nAn ONBUILD command executes after the current Dockerfile build completes. ONBUILD executes in any child image derived FROM the current image. Think of the ONBUILD command as an instruction that the parent Dockerfile gives to the child Dockerfile.\n\nA Docker build executes ONBUILD commands before any command in a child Dockerfile.\n\nONBUILD is useful for images that are going to be built FROM a given image. For example, you would use ONBUILD for a language stack image that builds arbitrary user software written in that language within the Dockerfile, as you can see in Ruby‚Äôs ONBUILD variants.\n\nImages built with ONBUILD should get a separate tag. For example, ruby:1.9-onbuild or ruby:2.0-onbuild.\n\nBe careful when putting ADD or COPY in ONBUILD. The image fails catastrophically if the new build's context is missing the resource being added. Adding a separate tag, as recommended above, helps mitigate this by allowing the Dockerfile author to make a choice.\n\nFor more information about ONBUILD, see Dockerfile reference for the ONBUILD instruction.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUse multi-stage builds\nCreate reusable stages\nChoose the right base image\nRebuild your images often\nUse --pull to get fresh base images\nUse --no-cache for clean builds\nExclude with .dockerignore\nCreate ephemeral containers\nDon't install unnecessary packages\nDecouple applications\nSort multi-line arguments\nLeverage build cache\nPin base image versions\nBuild and test your images in CI\nDockerfile instructions\nFROM\nLABEL\nRUN\nCMD\nEXPOSE\nENV\nADD or COPY\nENTRYPOINT\nVOLUME\nUSER\nWORKDIR\nONBUILD\nBest practices\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995657,
    "timestamp": "2026-02-07T06:34:33.293Z",
    "title": "Base images | Docker Docs",
    "url": "https://docs.docker.com/build/building/base-images/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nMulti-stage\nVariables\nSecrets\nMulti-platform\nExport binaries\nContainer Device Interface (CDI)\nBest practices\nBase images\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilding\n/\nBase images\nBase images\nCopy as Markdown\n\nAll Dockerfiles start from a base image. A base is the image that your image extends. It refers to the contents of the FROM instruction in the Dockerfile.\n\nFROM debian\n\nFor most cases, you don't need to create your own base image. Docker Hub contains a vast library of Docker images that are suitable for use as a base image in your build. Docker Official Images have clear documentation, promote best practices, and are regularly updated. There are also Docker Verified Publisher images, created by trusted publishing partners, verified by Docker.\n\nCreate a base image\n\nIf you need to completely control the contents of your image, you can create your own base image from a Linux distribution of your choosing, or use the special FROM scratch base:\n\nFROM scratch\n\nThe scratch image is typically used to create minimal images containing only just what an application needs. See Create a minimal base image using scratch.\n\nTo create a distribution base image, you can use a root filesystem, packaged as a tar file, and import it to Docker with docker import. The process for creating your own base image depends on the Linux distribution you want to package. See Create a full image using tar.\n\nCreate a minimal base image using scratch\n\nThe reserved, minimal scratch image serves as a starting point for building containers. Using the scratch image signals to the build process that you want the next command in the Dockerfile to be the first filesystem layer in your image.\n\nWhile scratch appears in Docker's repository on Docker Hub, you can't pull it, run it, or tag any image with the name scratch. Instead, you can refer to it in your Dockerfile. For example, to create a minimal container using scratch:\n\n# syntax=docker/dockerfile:1\n\nFROM scratch\n\nADD hello /\n\nCMD [\"/hello\"]\n\nAssuming an executable binary named hello exists at the root of the build context. You can build this Docker image using the following docker build command:\n\n$ docker build --tag hello .\n\n\nTo run your new image, use the docker run command:\n\n$ docker run --rm hello\n\n\nThis example image can only be successfully executed as long as the hello binary doesn't have any runtime dependencies. Computer programs tend to depend on certain other programs or resources to exist in the runtime environment. For example:\n\nProgramming language runtimes\nDynamically linked C libraries\nCA certificates\n\nWhen building a base image, or any image, this is an important aspect to consider. And this is why creating a base image using FROM scratch can be difficult, for anything other than small, simple programs. On the other hand, it's also important to include only the things you need in your image, to reduce the image size and attack surface.\n\nCreate a full image using tar\n\nIn general, start with a working machine that is running the distribution you'd like to package as a base image, though that is not required for some tools like Debian's Debootstrap, which you can also use to build Ubuntu images.\n\nFor example, to create an Ubuntu base image:\n\n$ sudo debootstrap noble noble > /dev/null\n\n$ sudo tar -C noble -c . | docker import - noble\n\n\n\nsha256:81ec9a55a92a5618161f68ae691d092bf14d700129093158297b3d01593f4ee3\n\n\n\n$ docker run noble cat /etc/lsb-release\n\n\n\nDISTRIB_ID=Ubuntu\n\nDISTRIB_RELEASE=24.04\n\nDISTRIB_CODENAME=noble\n\nDISTRIB_DESCRIPTION=\"Ubuntu 24.04.2 LTS\"\n\nThere are more example scripts for creating base images in the Moby GitHub repository.\n\nMore resources\n\nFor more information about building images and writing Dockerfiles, see:\n\nDockerfile reference\nDockerfile best practices\nDocker Official Images\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCreate a base image\nCreate a minimal base image using scratch\nCreate a full image using tar\nMore resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995660,
    "timestamp": "2026-02-07T06:34:33.298Z",
    "title": "Builders | Docker Docs",
    "url": "https://docs.docker.com/build/builders/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBuild drivers\nManage builders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilders\nBuilders\nCopy as Markdown\n\nA builder is a BuildKit daemon that you can use to run your builds. BuildKit is the build engine that solves the build steps in a Dockerfile to produce a container image or other artifacts.\n\nYou can create and manage builders, inspect them, and even connect to builders running remotely. You interact with builders using the Docker CLI.\n\nDefault builder\n\nDocker Engine automatically creates a builder that becomes the default backend for your builds. This builder uses the BuildKit library bundled with the daemon. This builder requires no configuration.\n\nThe default builder is directly bound to the Docker daemon and its context. If you change the Docker context, your default builder refers to the new Docker context.\n\nBuild drivers\n\nBuildx implements a concept of build drivers to refer to different builder configurations. The default builder created by the daemon uses the docker driver.\n\nBuildx supports the following build drivers:\n\ndocker: uses the BuildKit library bundled into the Docker daemon.\ndocker-container: creates a dedicated BuildKit container using Docker.\nkubernetes: creates BuildKit pods in a Kubernetes cluster.\nremote: connects directly to a manually managed BuildKit daemon.\nSelected builder\n\nSelected builder refers to the builder that's used by default when you run build commands.\n\nWhen you run a build, or interact with builders in some way using the CLI, you can use the optional --builder flag, or the BUILDX_BUILDER environment variable, to specify a builder by name. If you don't specify a builder, the selected builder is used.\n\nUse the docker buildx ls command to see the available builder instances. The asterisk (*) next to a builder name indicates the selected builder.\n\n$ docker buildx ls\n\nNAME/NODE       DRIVER/ENDPOINT      STATUS   BUILDKIT PLATFORMS\n\ndefault *       docker\n\n  default       default              running  v0.11.6  linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386\n\nmy_builder      docker-container\n\n  my_builder0   default              running  v0.11.6  linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386\n\nSelect a different builder\n\nTo switch between builders, use the docker buildx use <name> command.\n\nAfter running this command, the builder you specify is automatically selected when you invoke builds.\n\nDifference between docker build and docker buildx build\n\nEven though docker build is an alias for docker buildx build, there are subtle differences between the two commands. With Buildx, the build client and the daemon (BuildKit) are decoupled. This means you can use multiple builders from a single client, even remote ones.\n\nThe docker build command always defaults to using the default builder that comes bundled with the Docker Engine, to ensure backwards compatibility with older versions of the Docker CLI. The docker buildx build command, on the other hand, checks whether you've set a different builder as the default builder before it sends your build to BuildKit.\n\nTo use the docker build command with a non-default builder, you must either specify the builder explicitly:\n\nUsing the --builder flag:\n\n$ docker build --builder my_builder .\n\n\nOr the BUILDX_BUILDER environment variable:\n\n$ BUILDX_BUILDER=my_builder docker build .\n\n\nIn general, we recommend that you use the docker buildx build command when you want to use custom builders. This ensures that your selected builder configuration is interpreted correctly.\n\nAdditional information\nFor information about how to interact with and manage builders, see Manage builders\nTo learn about different types of builders, see Build drivers\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDefault builder\nBuild drivers\nSelected builder\nSelect a different builder\nDifference between docker build and docker buildx build\nAdditional information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995663,
    "timestamp": "2026-02-07T06:34:33.303Z",
    "title": "Build drivers | Docker Docs",
    "url": "https://docs.docker.com/build/builders/drivers/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBuild drivers\nDocker container driver\nDocker driver\nKubernetes driver\nRemote driver\nManage builders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilders\n/\nBuild drivers\nBuild drivers\nCopy as Markdown\n\nBuild drivers are configurations for how and where the BuildKit backend runs. Driver settings are customizable and allow fine-grained control of the builder. Buildx supports the following drivers:\n\ndocker: uses the BuildKit library bundled into the Docker daemon.\ndocker-container: creates a dedicated BuildKit container using Docker.\nkubernetes: creates BuildKit pods in a Kubernetes cluster.\nremote: connects directly to a manually managed BuildKit daemon.\n\nDifferent drivers support different use cases. The default docker driver prioritizes simplicity and ease of use. It has limited support for advanced features like caching and output formats, and isn't configurable. Other drivers provide more flexibility and are better at handling advanced scenarios.\n\nThe following table outlines some differences between drivers.\n\nFeature\tdocker\tdocker-container\tkubernetes\tremote\nAutomatically load image\t‚úÖ\t\t\t\nCache export\t‚úÖ*\t‚úÖ\t‚úÖ\t‚úÖ\nTarball output\t\t‚úÖ\t‚úÖ\t‚úÖ\nMulti-arch images\t\t‚úÖ\t‚úÖ\t‚úÖ\nBuildKit configuration\t\t‚úÖ\t‚úÖ\tManaged externally\n\n* The docker driver doesn't support all cache export options. See Cache storage backends for more information.\n\nLoading to local image store\n\nUnlike when using the default docker driver, images built using other drivers aren't automatically loaded into the local image store. If you don't specify an output, the build result is exported to the build cache only.\n\nTo build an image using a non-default driver and load it to the image store, use the --load flag with the build command:\n\n$ docker buildx build --load -t <image> --builder=container .\n\n...\n\n=> exporting to oci image format                                                                                                      7.7s\n\n=> => exporting layers                                                                                                                4.9s\n\n=> => exporting manifest sha256:4e4ca161fa338be2c303445411900ebbc5fc086153a0b846ac12996960b479d3                                      0.0s\n\n=> => exporting config sha256:adf3eec768a14b6e183a1010cb96d91155a82fd722a1091440c88f3747f1f53f                                        0.0s\n\n=> => sending tarball                                                                                                                 2.8s\n\n=> importing to docker\n\n\nWith this option, the image is available in the image store after the build finishes:\n\n$ docker image ls\n\nREPOSITORY                       TAG               IMAGE ID       CREATED             SIZE\n\n<image>                          latest            adf3eec768a1   2 minutes ago       197MB\n\nLoad by default\nRequires:\nDocker Buildx 0.14.0 and later\n\nYou can configure the custom build drivers to behave in a similar way to the default docker driver, and load images to the local image store by default. To do so, set the default-load driver option when creating the builder:\n\n$ docker buildx create --driver-opt default-load=true\n\n\nNote that, just like with the docker driver, if you specify a different output format with --output, the result will not be loaded to the image store unless you also explicitly specify --output type=docker or use the --load flag.\n\nWhat's next\n\nRead about each driver:\n\nDocker driver\nDocker container driver\nKubernetes driver\nRemote driver\n\nEdit this page\n\nRequest changes\n\nTable of contents\nLoading to local image store\nLoad by default\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995666,
    "timestamp": "2026-02-07T06:34:33.305Z",
    "title": "Docker container driver | Docker Docs",
    "url": "https://docs.docker.com/build/builders/drivers/docker-container/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBuild drivers\nDocker container driver\nDocker driver\nKubernetes driver\nRemote driver\nManage builders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilders\n/\nBuild drivers\n/\nDocker container driver\nDocker container driver\nCopy as Markdown\n\nThe Docker container driver allows creation of a managed and customizable BuildKit environment in a dedicated Docker container.\n\nUsing the Docker container driver has a couple of advantages over the default Docker driver. For example:\n\nSpecify custom BuildKit versions to use.\nBuild multi-arch images, see QEMU\nAdvanced options for cache import and export\nSynopsis\n\nRun the following command to create a new builder, named container, that uses the Docker container driver:\n\n$ docker buildx create \\\n\n  --name container \\\n\n  --driver=docker-container \\\n\n  --driver-opt=[key=value,...]\n\ncontainer\n\n\nThe following table describes the available driver-specific options that you can pass to --driver-opt:\n\nParameter\tType\tDefault\tDescription\nimage\tString\t\tSets the BuildKit image to use for the container.\nmemory\tString\t\tSets the amount of memory the container can use.\nmemory-swap\tString\t\tSets the memory swap limit for the container.\ncpu-quota\tString\t\tImposes a CPU CFS quota on the container.\ncpu-period\tString\t\tSets the CPU CFS scheduler period for the container.\ncpu-shares\tString\t\tConfigures CPU shares (relative weight) of the container.\ncpuset-cpus\tString\t\tLimits the set of CPU cores the container can use.\ncpuset-mems\tString\t\tLimits the set of CPU memory nodes the container can use.\ndefault-load\tBoolean\tfalse\tAutomatically load images to the Docker Engine image store.\nnetwork\tString\t\tSets the network mode for the container.\ncgroup-parent\tString\t/docker/buildx\tSets the cgroup parent of the container if Docker is using the \"cgroupfs\" driver.\nrestart-policy\tString\tunless-stopped\tSets the container's restart policy.\nenv.<key>\tString\t\tSets the environment variable key to the specified value in the container.\nprovenance-add-gha\tBoolean\ttrue\tAutomatically writes GitHub Actions context into the builder for provenance.\n\nBefore you configure the resource limits for the container, read about configuring runtime resource constraints for containers.\n\nUsage\n\nWhen you run a build, Buildx pulls the specified image (by default, moby/buildkit). When the container has started, Buildx submits the build submitted to the containerized build server.\n\n$ docker buildx build -t <image> --builder=container .\n\nWARNING: No output specified with docker-container driver. Build result will only remain in the build cache. To push result image into registry use --push or to load image into docker use --load\n\n#1 [internal] booting buildkit\n\n#1 pulling image moby/buildkit:buildx-stable-1\n\n#1 pulling image moby/buildkit:buildx-stable-1 1.9s done\n\n#1 creating container buildx_buildkit_container0\n\n#1 creating container buildx_buildkit_container0 0.5s done\n\n#1 DONE 2.4s\n\n...\n\nCache persistence\n\nThe docker-container driver supports cache persistence, as it stores all the BuildKit state and related cache into a dedicated Docker volume.\n\nTo persist the docker-container driver's cache, even after recreating the driver using docker buildx rm and docker buildx create, you can destroy the builder using the --keep-state flag:\n\nFor example, to create a builder named container and then remove it while persisting state:\n\n# setup a builder\n\n$ docker buildx create --name=container --driver=docker-container --use --bootstrap\n\ncontainer\n\n$ docker buildx ls\n\nNAME/NODE       DRIVER/ENDPOINT              STATUS   BUILDKIT PLATFORMS\n\ncontainer *     docker-container\n\n  container0    desktop-linux                running  v0.10.5  linux/amd64\n\n$ docker volume ls\n\nDRIVER    VOLUME NAME\n\nlocal     buildx_buildkit_container0_state\n\n\n\n# remove the builder while persisting state\n\n$ docker buildx rm --keep-state container\n\n$ docker volume ls\n\nDRIVER    VOLUME NAME\n\nlocal     buildx_buildkit_container0_state\n\n\n\n# the newly created driver with the same name will have all the state of the previous one!\n\n$ docker buildx create --name=container --driver=docker-container --use --bootstrap\n\ncontainer\n\nQEMU\n\nThe docker-container driver supports using QEMU (user mode) to build non-native platforms. Use the --platform flag to specify which architectures that you want to build for.\n\nFor example, to build a Linux image for amd64 and arm64:\n\n$ docker buildx build \\\n\n  --builder=container \\\n\n  --platform=linux/amd64,linux/arm64 \\\n\n  -t <registry>/<image> \\\n\n  --push .\n\nNote\n\nEmulation with QEMU can be much slower than native builds, especially for compute-heavy tasks like compilation and compression or decompression.\n\nCustom network\n\nYou can customize the network that the builder container uses. This is useful if you need to use a specific network for your builds.\n\nFor example, let's create a network named foonet:\n\n$ docker network create foonet\n\n\nNow create a docker-container builder that will use this network:\n\n$ docker buildx create --use \\\n\n  --name mybuilder \\\n\n  --driver docker-container \\\n\n  --driver-opt \"network=foonet\"\n\n\nBoot and inspect mybuilder:\n\n$ docker buildx inspect --bootstrap\n\n\nInspect the builder container and see what network is being used:\n\n$ docker inspect buildx_buildkit_mybuilder0 --format={{.NetworkSettings.Networks}}\n\nmap[foonet:0xc00018c0c0]\n\nFurther reading\n\nFor more information on the Docker container driver, see the buildx reference.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nUsage\nCache persistence\nQEMU\nCustom network\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995667,
    "timestamp": "2026-02-07T06:34:33.310Z",
    "title": "Docker driver | Docker Docs",
    "url": "https://docs.docker.com/build/builders/drivers/docker/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBuild drivers\nDocker container driver\nDocker driver\nKubernetes driver\nRemote driver\nManage builders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilders\n/\nBuild drivers\n/\nDocker driver\nDocker driver\nCopy as Markdown\n\nThe Buildx Docker driver is the default driver. It uses the BuildKit server components built directly into the Docker Engine. The Docker driver requires no configuration.\n\nUnlike the other drivers, builders using the Docker driver can't be manually created. They're only created automatically from the Docker context.\n\nImages built with the Docker driver are automatically loaded to the local image store.\n\nSynopsis\n# The Docker driver is used by buildx by default\n\ndocker buildx build .\n\n\nIt's not possible to configure which BuildKit version to use, or to pass any additional BuildKit parameters to a builder using the Docker driver. The BuildKit version and parameters are preset by the Docker Engine internally.\n\nIf you need additional configuration and flexibility, consider using the Docker container driver.\n\nFurther reading\n\nFor more information on the Docker driver, see the buildx reference.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995670,
    "timestamp": "2026-02-07T06:34:33.320Z",
    "title": "Kubernetes driver | Docker Docs",
    "url": "https://docs.docker.com/build/builders/drivers/kubernetes/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBuild drivers\nDocker container driver\nDocker driver\nKubernetes driver\nRemote driver\nManage builders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilders\n/\nBuild drivers\n/\nKubernetes driver\nKubernetes driver\nCopy as Markdown\n\nThe Kubernetes driver lets you connect your local development or CI environments to builders in a Kubernetes cluster to allow access to more powerful compute resources, optionally on multiple native architectures.\n\nSynopsis\n\nRun the following command to create a new builder, named kube, that uses the Kubernetes driver:\n\n$ docker buildx create \\\n\n  --bootstrap \\\n\n  --name=kube \\\n\n  --driver=kubernetes \\\n\n  --driver-opt=[key=value,...]\n\n\nThe following table describes the available driver-specific options that you can pass to --driver-opt:\n\nParameter\tType\tDefault\tDescription\nimage\tString\t\tSets the image to use for running BuildKit.\nnamespace\tString\tNamespace in current Kubernetes context\tSets the Kubernetes namespace.\ndefault-load\tBoolean\tfalse\tAutomatically load images to the Docker Engine image store.\nreplicas\tInteger\t1\tSets the number of Pod replicas to create. See scaling BuildKit\nrequests.cpu\tCPU units\t\tSets the request CPU value specified in units of Kubernetes CPU. For example requests.cpu=100m or requests.cpu=2\nrequests.memory\tMemory size\t\tSets the request memory value specified in bytes or with a valid suffix. For example requests.memory=500Mi or requests.memory=4G\nrequests.ephemeral-storage\tStorage size\t\tSets the request ephemeral-storage value specified in bytes or with a valid suffix. For example requests.ephemeral-storage=2Gi\nlimits.cpu\tCPU units\t\tSets the limit CPU value specified in units of Kubernetes CPU. For example requests.cpu=100m or requests.cpu=2\nlimits.memory\tMemory size\t\tSets the limit memory value specified in bytes or with a valid suffix. For example requests.memory=500Mi or requests.memory=4G\nlimits.ephemeral-storage\tStorage size\t\tSets the limit ephemeral-storage value specified in bytes or with a valid suffix. For example requests.ephemeral-storage=100M\nbuildkit-root-volume-memory\tMemory size\tUsing regular file system\tMounts /var/lib/buildkit on an emptyDir memory-backed volume, with SizeLimit as the value. For example, buildkit-root-folder-memory=6G\nnodeselector\tCSV string\t\tSets the pod's nodeSelector label(s). See node assignment.\nannotations\tCSV string\t\tSets additional annotations on the deployments and pods.\nlabels\tCSV string\t\tSets additional labels on the deployments and pods.\ntolerations\tCSV string\t\tConfigures the pod's taint toleration. See node assignment.\nserviceaccount\tString\t\tSets the pod's serviceAccountName.\nschedulername\tString\t\tSets the scheduler responsible for scheduling the pod.\ntimeout\tTime\t120s\tSet the timeout limit that determines how long Buildx will wait for pods to be provisioned before a build.\nrootless\tBoolean\tfalse\tRun the container as a non-root user. See rootless mode.\nloadbalance\tString\tsticky\tLoad-balancing strategy (sticky or random). If set to sticky, the pod is chosen using the hash of the context path.\nqemu.install\tBoolean\tfalse\tInstall QEMU emulation for multi platforms support. See QEMU.\nqemu.image\tString\ttonistiigi/binfmt:latest\tSets the QEMU emulation image. See QEMU.\nScaling BuildKit\n\nOne of the main advantages of the Kubernetes driver is that you can scale the number of builder replicas up and down to handle increased build load. Scaling is configurable using the following driver options:\n\nreplicas=N\n\nThis scales the number of BuildKit pods to the desired size. By default, it only creates a single pod. Increasing the number of replicas lets you take advantage of multiple nodes in your cluster.\n\nrequests.cpu, requests.memory, requests.ephemeral-storage, limits.cpu, limits.memory, limits.ephemeral-storage\n\nThese options allow requesting and limiting the resources available to each BuildKit pod according to the official Kubernetes documentation.\n\nFor example, to create 4 replica BuildKit pods:\n\n$ docker buildx create \\\n\n  --bootstrap \\\n\n  --name=kube \\\n\n  --driver=kubernetes \\\n\n  --driver-opt=namespace=buildkit,replicas=4\n\n\nListing the pods, you get this:\n\n$ kubectl -n buildkit get deployments\n\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\n\nkube0   4/4     4            4           8s\n\n\n\n$ kubectl -n buildkit get pods\n\nNAME                     READY   STATUS    RESTARTS   AGE\n\nkube0-6977cdcb75-48ld2   1/1     Running   0          8s\n\nkube0-6977cdcb75-rkc6b   1/1     Running   0          8s\n\nkube0-6977cdcb75-vb4ks   1/1     Running   0          8s\n\nkube0-6977cdcb75-z4fzs   1/1     Running   0          8s\n\n\nAdditionally, you can use the loadbalance=(sticky|random) option to control the load-balancing behavior when there are multiple replicas. random selects random nodes from the node pool, providing an even workload distribution across replicas. sticky (the default) attempts to connect the same build performed multiple times to the same node each time, ensuring better use of local cache.\n\nFor more information on scalability, see the options for docker buildx create.\n\nNode assignment\n\nThe Kubernetes driver allows you to control the scheduling of BuildKit pods using the nodeSelector and tolerations driver options. You can also set the schedulername option if you want to use a custom scheduler altogether.\n\nYou can use the annotations and labels driver options to apply additional metadata to the deployments and pods that's hosting your builders.\n\nThe value of the nodeSelector parameter is a comma-separated string of key-value pairs, where the key is the node label and the value is the label text. For example: \"nodeselector=kubernetes.io/arch=arm64\"\n\nThe tolerations parameter is a semicolon-separated list of taints. It accepts the same values as the Kubernetes manifest. Each tolerations entry specifies a taint key and the value, operator, or effect. For example: \"tolerations=key=foo,value=bar;key=foo2,operator=exists;key=foo3,effect=NoSchedule\"\n\nThese options accept CSV-delimited strings as values. Due to quoting rules for shell commands, you must wrap the values in single quotes. You can even wrap all of --driver-opt in single quotes, for example:\n\n$ docker buildx create \\\n\n  --bootstrap \\\n\n  --name=kube \\\n\n  --driver=kubernetes \\\n\n  '--driver-opt=\"nodeselector=label1=value1,label2=value2\",\"tolerations=key=key1,value=value1\"'\n\nMulti-platform builds\n\nThe Kubernetes driver has support for creating multi-platform images, either using QEMU or by leveraging the native architecture of nodes.\n\nQEMU\n\nLike the docker-container driver, the Kubernetes driver also supports using QEMU (user mode) to build images for non-native platforms. Include the --platform flag and specify which platforms you want to output to.\n\nFor example, to build a Linux image for amd64 and arm64:\n\n$ docker buildx build \\\n\n  --builder=kube \\\n\n  --platform=linux/amd64,linux/arm64 \\\n\n  -t <user>/<image> \\\n\n  --push .\n\nWarning\n\nQEMU performs full-CPU emulation of non-native platforms, which is much slower than native builds. Compute-heavy tasks like compilation and compression/decompression will likely take a large performance hit.\n\nUsing a custom BuildKit image or invoking non-native binaries in builds may require that you explicitly turn on QEMU using the qemu.install option when creating the builder:\n\n$ docker buildx create \\\n\n  --bootstrap \\\n\n  --name=kube \\\n\n  --driver=kubernetes \\\n\n  --driver-opt=namespace=buildkit,qemu.install=true\n\nNative\n\nIf you have access to cluster nodes of different architectures, the Kubernetes driver can take advantage of these for native builds. To do this, use the --append flag of docker buildx create.\n\nFirst, create your builder with explicit support for a single architecture, for example amd64:\n\n$ docker buildx create \\\n\n  --bootstrap \\\n\n  --name=kube \\\n\n  --driver=kubernetes \\\n\n  --platform=linux/amd64 \\\n\n  --node=builder-amd64 \\\n\n  --driver-opt=namespace=buildkit,nodeselector=\"kubernetes.io/arch=amd64\"\n\n\nThis creates a Buildx builder named kube, containing a single builder node named builder-amd64. Assigning a node name using --node is optional. Buildx generates a random node name if you don't provide one.\n\nNote that the Buildx concept of a node isn't the same as the Kubernetes concept of a node. A Buildx node in this case could connect multiple Kubernetes nodes of the same architecture together.\n\nWith the kube builder created, you can now introduce another architecture into the mix using --append. For example, to add arm64:\n\n$ docker buildx create \\\n\n  --append \\\n\n  --bootstrap \\\n\n  --name=kube \\\n\n  --driver=kubernetes \\\n\n  --platform=linux/arm64 \\\n\n  --node=builder-arm64 \\\n\n  --driver-opt=namespace=buildkit,nodeselector=\"kubernetes.io/arch=arm64\"\n\n\nListing your builders shows both nodes for the kube builder:\n\n$ docker buildx ls\n\nNAME/NODE       DRIVER/ENDPOINT                                         STATUS   PLATFORMS\n\nkube            kubernetes\n\n  builder-amd64 kubernetes:///kube?deployment=builder-amd64&kubeconfig= running  linux/amd64*, linux/amd64/v2, linux/amd64/v3, linux/386\n\n  builder-arm64 kubernetes:///kube?deployment=builder-arm64&kubeconfig= running  linux/arm64*\n\n\nYou can now build multi-arch amd64 and arm64 images, by specifying those platforms together in your build command:\n\n$ docker buildx build --builder=kube --platform=linux/amd64,linux/arm64 -t <user>/<image> --push .\n\n\nYou can repeat the buildx create --append command for as many architectures that you want to support.\n\nRootless mode\n\nThe Kubernetes driver supports rootless mode. For more information on how rootless mode works, and its requirements, refer to the Rootless Buildkit documentation.\n\nTo turn it on in your cluster, you can use the rootless=true driver option:\n\n$ docker buildx create \\\n\n  --name=kube \\\n\n  --driver=kubernetes \\\n\n  --driver-opt=namespace=buildkit,rootless=true\n\n\nThis will create your pods without securityContext.privileged.\n\nRequires Kubernetes version 1.19 or later. Using Ubuntu as the host kernel is recommended.\n\nExample: Creating a Buildx builder in Kubernetes\n\nThis guide shows you how to:\n\nCreate a namespace for your Buildx resources\nCreate a Kubernetes builder.\nList the available builders\nBuild an image using your Kubernetes builders\n\nPrerequisites:\n\nYou have an existing Kubernetes cluster. If you don't already have one, you can follow along by installing minikube.\nThe cluster you want to connect to is accessible via the kubectl command, with the KUBECONFIG environment variable set appropriately if necessary.\n\nCreate a buildkit namespace.\n\nCreating a separate namespace helps keep your Buildx resources separate from other resources in the cluster.\n\n$ kubectl create namespace buildkit\n\nnamespace/buildkit created\n\n\nCreate a new builder with the Kubernetes driver:\n\n$ docker buildx create \\\n\n  --bootstrap \\\n\n  --name=kube \\\n\n  --driver=kubernetes \\\n\n  --driver-opt=namespace=buildkit\n\nNote\n\nRemember to specify the namespace in driver options.\n\nList available builders using docker buildx ls\n\n$ docker buildx ls\n\nNAME/NODE                DRIVER/ENDPOINT STATUS  PLATFORMS\n\nkube                     kubernetes\n\n  kube0-6977cdcb75-k9h9m                 running linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386\n\ndefault *                docker\n\n  default                default         running linux/amd64, linux/386\n\n\nInspect the running pods created by the build driver with kubectl.\n\n$ kubectl -n buildkit get deployments\n\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\n\nkube0   1/1     1            1           32s\n\n\n\n$ kubectl -n buildkit get pods\n\nNAME                     READY   STATUS    RESTARTS   AGE\n\nkube0-6977cdcb75-k9h9m   1/1     Running   0          32s\n\n\nThe build driver creates the necessary resources on your cluster in the specified namespace (in this case, buildkit), while keeping your driver configuration locally.\n\nUse your new builder by including the --builder flag when running buildx commands. For example: :\n\n# Replace <registry> with your Docker username\n\n# and <image> with the name of the image you want to build\n\ndocker buildx build \\\n\n  --builder=kube \\\n\n  -t <registry>/<image> \\\n\n  --push .\n\n\nThat's it: you've now built an image from a Kubernetes pod, using Buildx.\n\nFurther reading\n\nFor more information on the Kubernetes driver, see the buildx reference.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nScaling BuildKit\nNode assignment\nMulti-platform builds\nQEMU\nNative\nRootless mode\nExample: Creating a Buildx builder in Kubernetes\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995673,
    "timestamp": "2026-02-07T06:34:33.322Z",
    "title": "Remote driver | Docker Docs",
    "url": "https://docs.docker.com/build/builders/drivers/remote/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBuild drivers\nDocker container driver\nDocker driver\nKubernetes driver\nRemote driver\nManage builders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilders\n/\nBuild drivers\n/\nRemote driver\nRemote driver\nCopy as Markdown\n\nThe Buildx remote driver allows for more complex custom build workloads, allowing you to connect to externally managed BuildKit instances. This is useful for scenarios that require manual management of the BuildKit daemon, or where a BuildKit daemon is exposed from another source.\n\nSynopsis\n$ docker buildx create \\\n\n  --name remote \\\n\n  --driver remote \\\n\n  tcp://localhost:1234\n\n\nThe following table describes the available driver-specific options that you can pass to --driver-opt:\n\nParameter\tType\tDefault\tDescription\nkey\tString\t\tSets the TLS client key.\ncert\tString\t\tAbsolute path to the TLS client certificate to present to buildkitd.\ncacert\tString\t\tAbsolute path to the TLS certificate authority used for validation.\nservername\tString\tEndpoint hostname.\tTLS server name used in requests.\ndefault-load\tBoolean\tfalse\tAutomatically load images to the Docker Engine image store.\nExample: Remote BuildKit over Unix sockets\n\nThis guide shows you how to create a setup with a BuildKit daemon listening on a Unix socket, and have Buildx connect through it.\n\nEnsure that BuildKit is installed.\n\nFor example, you can launch an instance of buildkitd with:\n\n$ sudo ./buildkitd --group $(id -gn) --addr unix://$HOME/buildkitd.sock\n\n\nAlternatively, refer to the Rootless Buildkit documentation for running buildkitd in rootless mode, or the BuildKit systemd examples for running it as a systemd service.\n\nCheck that you have a Unix socket that you can connect to.\n\n$ ls -lh /home/user/buildkitd.sock\n\nsrw-rw---- 1 root user 0 May  5 11:04 /home/user/buildkitd.sock\n\n\nConnect Buildx to it using the remote driver:\n\n$ docker buildx create \\\n\n  --name remote-unix \\\n\n  --driver remote \\\n\n  unix://$HOME/buildkitd.sock\n\n\nList available builders with docker buildx ls. You should then see remote-unix among them:\n\n$ docker buildx ls\n\nNAME/NODE           DRIVER/ENDPOINT                        STATUS  PLATFORMS\n\nremote-unix         remote\n\n  remote-unix0      unix:///home/.../buildkitd.sock        running linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386\n\ndefault *           docker\n\n  default           default                                running linux/amd64, linux/386\n\n\nYou can switch to this new builder as the default using docker buildx use remote-unix, or specify it per build using --builder:\n\n$ docker buildx build --builder=remote-unix -t test --load .\n\n\nRemember that you need to use the --load flag if you want to load the build result into the Docker daemon.\n\nExample: Remote BuildKit in Docker container\n\nThis guide will show you how to create setup similar to the docker-container driver, by manually booting a BuildKit Docker container and connecting to it using the Buildx remote driver. This procedure will manually create a container and access it via it's exposed port. (You'd probably be better of just using the docker-container driver that connects to BuildKit through the Docker daemon, but this is for illustration purposes.)\n\nGenerate certificates for BuildKit.\n\nYou can use this bake definition as a starting point:\n\nSAN=\"localhost 127.0.0.1\" docker buildx bake \"https://github.com/moby/buildkit.git#master:examples/create-certs\"\n\n\nNote that while it's possible to expose BuildKit over TCP without using TLS, it's not recommended. Doing so allows arbitrary access to BuildKit without credentials.\n\nWith certificates generated in .certs/, startup the container:\n\n$ docker run -d --rm \\\n\n  --name=remote-buildkitd \\\n\n  --privileged \\\n\n  -p 1234:1234 \\\n\n  -v $PWD/.certs:/etc/buildkit/certs \\\n\n  moby/buildkit:latest \\\n\n  --addr tcp://0.0.0.0:1234 \\\n\n  --tlscacert /etc/buildkit/certs/daemon/ca.pem \\\n\n  --tlscert /etc/buildkit/certs/daemon/cert.pem \\\n\n  --tlskey /etc/buildkit/certs/daemon/key.pem\n\n\nThis command starts a BuildKit container and exposes the daemon's port 1234 to localhost.\n\nConnect to this running container using Buildx:\n\n$ docker buildx create \\\n\n  --name remote-container \\\n\n  --driver remote \\\n\n  --driver-opt cacert=${PWD}/.certs/client/ca.pem,cert=${PWD}/.certs/client/cert.pem,key=${PWD}/.certs/client/key.pem,servername=TLS_SERVER_NAME \\\n\n  tcp://localhost:1234\n\n\nAlternatively, use the docker-container:// URL scheme to connect to the BuildKit container without specifying a port:\n\n$ docker buildx create \\\n\n  --name remote-container \\\n\n  --driver remote \\\n\n  docker-container://remote-container\n\nExample: Remote BuildKit in Kubernetes\n\nThis guide will show you how to create a setup similar to the kubernetes driver by manually creating a BuildKit Deployment. While the kubernetes driver will do this under-the-hood, it might sometimes be desirable to scale BuildKit manually. Additionally, when executing builds from inside Kubernetes pods, the Buildx builder will need to be recreated from within each pod or copied between them.\n\nCreate a Kubernetes deployment of buildkitd by following the instructions in the BuildKit documentation.\n\nCreate certificates for the BuildKit daemon and client using the create-certs.sh, script and create a deployment of BuildKit pods with a service that connects to them.\n\nAssuming that the service is called buildkitd, create a remote builder in Buildx, ensuring that the listed certificate files are present:\n\n$ docker buildx create \\\n\n  --name remote-kubernetes \\\n\n  --driver remote \\\n\n  --driver-opt cacert=${PWD}/.certs/client/ca.pem,cert=${PWD}/.certs/client/cert.pem,key=${PWD}/.certs/client/key.pem \\\n\n  tcp://buildkitd.default.svc:1234\n\n\nNote that this only works internally, within the cluster, since the BuildKit setup guide only creates a ClusterIP service. To access a builder remotely, you can set up and use an ingress, which is outside the scope of this guide.\n\nDebug a remote builder in Kubernetes\n\nIf you're having trouble accessing a remote builder deployed in Kubernetes, you can use the kube-pod:// URL scheme to connect directly to a BuildKit pod through the Kubernetes API. Note that this method only connects to a single pod in the deployment.\n\n$ kubectl get pods --selector=app=buildkitd -o json | jq -r '.items[].metadata.name'\n\nbuildkitd-XXXXXXXXXX-xxxxx\n\n$ docker buildx create \\\n\n  --name remote-container \\\n\n  --driver remote \\\n\n  kube-pod://buildkitd-XXXXXXXXXX-xxxxx\n\n\nAlternatively, use the port forwarding mechanism of kubectl:\n\n$ kubectl port-forward svc/buildkitd 1234:1234\n\n\nThen you can point the remote driver at tcp://localhost:1234.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nExample: Remote BuildKit over Unix sockets\nExample: Remote BuildKit in Docker container\nExample: Remote BuildKit in Kubernetes\nDebug a remote builder in Kubernetes\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995676,
    "timestamp": "2026-02-07T06:34:33.325Z",
    "title": "Manage builders | Docker Docs",
    "url": "https://docs.docker.com/build/builders/manage/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBuild drivers\nManage builders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBuilders\n/\nManage builders\nManage builders\nCopy as Markdown\n\nYou can create, inspect, and manage builders using docker buildx commands, or using Docker Desktop.\n\nCreate a new builder\n\nThe default builder uses the docker driver. You can't manually create new docker builders, but you can create builders that use other drivers, such as the docker-container driver, which runs the BuildKit daemon in a container.\n\nUse the docker buildx create command to create a builder.\n\n$ docker buildx create --name=<builder-name>\n\n\nBuildx uses the docker-container driver by default if you omit the --driver flag. For more information about available drivers, see Build drivers.\n\nList available builders\n\nUse docker buildx ls to see builder instances available on your system, and the drivers they're using.\n\n$ docker buildx ls\n\nNAME/NODE       DRIVER/ENDPOINT      STATUS   BUILDKIT PLATFORMS\n\ndefault *       docker\n\n  default       default              running  v0.11.6  linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386\n\nmy_builder      docker-container\n\n  my_builder0   default              running  v0.11.6  linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386\n\n\nThe asterisk (*) next to the builder name indicates the selected builder.\n\nInspect a builder\n\nTo inspect a builder with the CLI, use docker buildx inspect <name>. You can only inspect a builder if the builder is active. You can add the --bootstrap flag to the command to start the builder.\n\n$ docker buildx inspect --bootstrap my_builder\n\n[+] Building 1.7s (1/1) FINISHED                                                                  \n\n => [internal] booting buildkit                                                              1.7s\n\n => => pulling image moby/buildkit:buildx-stable-1                                           1.3s\n\n => => creating container buildx_buildkit_my_builder0                                        0.4s\n\nName:          my_builder\n\nDriver:        docker-container\n\nLast Activity: 2023-06-21 18:28:37 +0000 UTC\n\n\n\nNodes:\n\nName:      my_builder0\n\nEndpoint:  unix:///var/run/docker.sock\n\nStatus:    running\n\nBuildkit:  v0.11.6\n\nPlatforms: linux/arm64, linux/amd64, linux/amd64/v2, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/mips64le, linux/mips64, linux/arm/v7, linux/arm/v6\n\n\nIf you want to see how much disk space a builder is using, use the docker buildx du command. By default, this command shows the total disk usage for all available builders. To see usage for a specific builder, use the --builder flag.\n\n$ docker buildx du --builder my_builder\n\nID                                        RECLAIMABLE SIZE        LAST ACCESSED\n\nolkri5gq6zsh8q2819i69aq6l                 true        797.2MB     37 seconds ago\n\n6km4kasxgsywxkm6cxybdumbb*                true        438.5MB     36 seconds ago\n\nqh3wwwda7gx2s5u4hsk0kp4w7                 true        213.8MB     37 seconds ago\n\n54qq1egqem8max3lxq6180cj8                 true        200.2MB     37 seconds ago\n\nndlp969ku0950bmrw9muolw0c*                true        116.7MB     37 seconds ago\n\nu52rcsnfd1brwc0chwsesb3io*                true        116.7MB     37 seconds ago\n\nrzoeay0s4nmss8ub59z6lwj7d                 true        46.25MB     4 minutes ago\n\nitk1iibhmv7awmidiwbef633q                 true        33.33MB     37 seconds ago\n\n4p78yqnbmgt6xhcxqitdieeln                 true        19.46MB     4 minutes ago\n\ndgkjvv4ay0szmr9bl7ynla7fy*                true        19.24MB     36 seconds ago\n\ntuep198kmcw299qc9e4d1a8q2                 true        8.663MB     4 minutes ago\n\nn1wzhauk9rpmt6ib1es7dktvj                 true        20.7kB      4 minutes ago\n\n0a2xfhinvndki99y69157udlm                 true        16.56kB     37 seconds ago\n\ngf0z1ypz54npfererqfeyhinn                 true        16.38kB     37 seconds ago\n\nnz505f12cnsu739dw2pw0q78c                 true        8.192kB     37 seconds ago\n\nhwpcyq5hdfvioltmkxu7fzwhb*                true        8.192kB     37 seconds ago\n\nacekq89snc7j6im1rjdizvsg1*                true        8.192kB     37 seconds ago\n\nReclaimable:  2.01GB\n\nTotal:        2.01GB\n\nRemove a builder\n\nUse the docker buildx remove command to remove a builder.\n\n$ docker buildx rm <builder-name>\n\n\nIf you remove your currently selected builder, the default docker builder is automatically selected. You can't remove the default builder.\n\nLocal build cache for the builder is also removed.\n\nRemoving remote builders\n\nRemoving a remote builder doesn't affect the remote build cache. It also doesn't stop the remote BuildKit daemon. It only removes your connection to the builder.\n\nManage builders with Docker Desktop\n\nIf you have turned on the Docker Desktop Builds view, you can inspect builders in Docker Desktop settings.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCreate a new builder\nList available builders\nInspect a builder\nRemove a builder\nRemoving remote builders\nManage builders with Docker Desktop\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995681,
    "timestamp": "2026-02-07T06:34:33.330Z",
    "title": "Bake | Docker Docs",
    "url": "https://docs.docker.com/build/bake/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\nBake\nCopy as Markdown\n\nBake is a feature of Docker Buildx that lets you define your build configuration using a declarative file, as opposed to specifying a complex CLI expression. It also lets you run multiple builds concurrently with a single invocation.\n\nA Bake file can be written in HCL, JSON, or YAML formats, where the YAML format is an extension of a Docker Compose file. Here's an example Bake file in HCL format:\n\ndocker-bake.hcl\ngroup \"default\" {\n\n  targets = [\"frontend\", \"backend\"]\n\n}\n\n\n\ntarget \"frontend\" {\n\n  context = \"./frontend\"\n\n  dockerfile = \"frontend.Dockerfile\"\n\n  args = {\n\n    NODE_VERSION = \"22\"\n\n  }\n\n  tags = [\"myapp/frontend:latest\"]\n\n}\n\n\n\ntarget \"backend\" {\n\n  context = \"./backend\"\n\n  dockerfile = \"backend.Dockerfile\"\n\n  args = {\n\n    GO_VERSION = \"1.25\"\n\n  }\n\n  tags = [\"myapp/backend:latest\"]\n\n}\n\nThe group block defines a group of targets that can be built concurrently. Each target block defines a build target with its own configuration, such as the build context, Dockerfile, and tags.\n\nTo invoke a build using the above Bake file, you can run:\n\n$ docker buildx bake\n\n\nThis executes the default group, which builds the frontend and backend targets concurrently.\n\nGet started\n\nTo learn how to get started with Bake, head over to the Bake introduction.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nGet started\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995682,
    "timestamp": "2026-02-07T06:34:33.333Z",
    "title": "Introduction | Docker Docs",
    "url": "https://docs.docker.com/build/bake/introduction/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nIntroduction\nIntroduction to Bake\nCopy as Markdown\n\nBake is an abstraction for the docker build command that lets you more easily manage your build configuration (CLI flags, environment variables, etc.) in a consistent way for everyone on your team.\n\nBake is a command built into the Buildx CLI, so as long as you have Buildx installed, you also have access to bake, via the docker buildx bake command.\n\nBuilding a project with Bake\n\nHere's a simple example of a docker build command:\n\n$ docker build -f Dockerfile -t myapp:latest .\n\n\nThis command builds the Dockerfile in the current directory and tags the resulting image as myapp:latest.\n\nTo express the same build configuration using Bake:\n\ndocker-bake.hcl\ntarget \"myapp\" {\n\n  context = \".\"\n\n  dockerfile = \"Dockerfile\"\n\n  tags = [\"myapp:latest\"]\n\n}\n\nBake provides a structured way to manage your build configuration, and it saves you from having to remember all the CLI flags for docker build every time. With this file, building the image is as simple as running:\n\n$ docker buildx bake myapp\n\n\nFor simple builds, the difference between docker build and docker buildx bake is minimal. However, as your build configuration grows more complex, Bake provides a more structured way to manage that complexity, that would be difficult to manage with CLI flags for the docker build. It also provides a way to share build configurations across your team, so that everyone is building images in a consistent way, with the same configuration.\n\nThe Bake file format\n\nYou can write Bake files in HCL, YAML (Docker Compose files), or JSON. In general, HCL is the most expressive and flexible format, which is why you'll see it used in most of the examples in this documentation, and in projects that use Bake.\n\nThe properties that can be set for a target closely resemble the CLI flags for docker build. For instance, consider the following docker build command:\n\n$ docker build \\\n\n  -f Dockerfile \\\n\n  -t myapp:latest \\\n\n  --build-arg foo=bar \\\n\n  --no-cache \\\n\n  --platform linux/amd64,linux/arm64 \\\n\n  .\n\n\nThe Bake equivalent would be:\n\ndocker-bake.hcl\ntarget \"myapp\" {\n\n  context = \".\"\n\n  dockerfile = \"Dockerfile\"\n\n  tags = [\"myapp:latest\"]\n\n  args = {\n\n    foo = \"bar\"\n\n  }\n\n  no-cache = true\n\n  platforms = [\"linux/amd64\", \"linux/arm64\"]\n\n}\nTip\n\nWant a better editing experience for Bake files in VS Code? Check out the Docker VS Code Extension (Beta) for linting, code navigation, and vulnerability scanning.\n\nNext steps\n\nTo learn more about using Bake, see the following topics:\n\nLearn how to define and use targets in Bake\nTo see all the properties that can be set for a target, refer to the Bake file reference.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nBuilding a project with Bake\nThe Bake file format\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995685,
    "timestamp": "2026-02-07T06:34:33.334Z",
    "title": "Targets | Docker Docs",
    "url": "https://docs.docker.com/build/bake/targets/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nTargets\nBake targets\nCopy as Markdown\n\nA target in a Bake file represents a build invocation. It holds all the information you would normally pass to a docker build command using flags.\n\ndocker-bake.hcl\ntarget \"webapp\" {\n\n  dockerfile = \"webapp.Dockerfile\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  context = \"https://github.com/username/webapp\"\n\n}\n\nTo build a target with Bake, pass name of the target to the bake command.\n\n$ docker buildx bake webapp\n\n\nYou can build multiple targets at once by passing multiple target names to the bake command.\n\n$ docker buildx bake webapp api tests\n\nDefault target\n\nIf you don't specify a target when running docker buildx bake, Bake will build the target named default.\n\ndocker-bake.hcl\ntarget \"default\" {\n\n  dockerfile = \"webapp.Dockerfile\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  context = \"https://github.com/username/webapp\"\n\n}\n\nTo build this target, run docker buildx bake without any arguments:\n\n$ docker buildx bake\n\nTarget properties\n\nThe properties you can set for a target closely resemble the CLI flags for docker build, with a few additional properties that are specific to Bake.\n\nFor all the properties you can set for a target, see the Bake reference.\n\nGrouping targets\n\nYou can group targets together using the group block. This is useful when you want to build multiple targets at once.\n\ndocker-bake.hcl\ngroup \"all\" {\n\n  targets = [\"webapp\", \"api\", \"tests\"]\n\n}\n\n\n\ntarget \"webapp\" {\n\n  dockerfile = \"webapp.Dockerfile\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  context = \"https://github.com/username/webapp\"\n\n}\n\n\n\ntarget \"api\" {\n\n  dockerfile = \"api.Dockerfile\"\n\n  tags = [\"docker.io/username/api:latest\"]\n\n  context = \"https://github.com/username/api\"\n\n}\n\n\n\ntarget \"tests\" {\n\n  dockerfile = \"tests.Dockerfile\"\n\n  contexts = {\n\n    webapp = \"target:webapp\"\n\n    api = \"target:api\"\n\n  }\n\n  output = [\"type=local,dest=build/tests\"]\n\n  context = \".\"\n\n}\n\nTo build all the targets in a group, pass the name of the group to the bake command.\n\n$ docker buildx bake all\n\nPattern matching for targets and groups\n\nBake supports shell-style wildcard patterns when specifying target or grouped targets. This makes it easier to build multiple targets without listing each one explicitly.\n\nSupported patterns:\n\n* matches any sequence of characters\n? matches any single character\n[abc] matches any character in brackets\nNote\n\nAlways wrap wildcard patterns in quotes. Without quotes, your shell will expand the wildcard to match files in the current directory, which usually causes errors.\n\nExamples:\n\n# Match all targets starting with 'foo-'\n\n$ docker buildx bake \"foo-*\"\n\n\n\n# Match all targets\n\n$ docker buildx bake \"*\"\n\n\n\n# Matches: foo-baz, foo-caz, foo-daz, etc.\n\n$ docker buildx bake \"foo-?az\"\n\n\n\n# Matches: foo-bar, boo-bar\n\n$ docker buildx bake \"[fb]oo-bar\"\n\n\n\n# Matches: mtx-a-b-d, mtx-a-b-e, mtx-a-b-f\n\n$ docker buildx bake \"mtx-a-b-*\"\n\n\nYou can also combine multiple patterns:\n\n$ docker buildx bake \"foo*\" \"tests\"\n\nAdditional resources\n\nRefer to the following pages to learn more about Bake's features:\n\nLearn how to use variables in Bake to make your build configuration more flexible.\nLearn how you can use matrices to build multiple images with different configurations in Matrices.\nHead to the Bake file reference to learn about all the properties you can set in a Bake file, and its syntax.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDefault target\nTarget properties\nGrouping targets\nPattern matching for targets and groups\nAdditional resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995688,
    "timestamp": "2026-02-07T06:34:33.342Z",
    "title": "Inheritance | Docker Docs",
    "url": "https://docs.docker.com/build/bake/inheritance/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nInheritance\nInheritance in Bake\nCopy as Markdown\n\nTargets can inherit attributes from other targets, using the inherits attribute. For example, imagine that you have a target that builds a Docker image for a development environment:\n\ndocker-bake.hcl\ntarget \"app-dev\" {\n\n  args = {\n\n    GO_VERSION = \"1.25\"\n\n  }\n\n  tags = [\"docker.io/username/myapp:dev\"]\n\n  labels = {\n\n    \"org.opencontainers.image.source\" = \"https://github.com/username/myapp\"\n\n    \"org.opencontainers.image.author\" = \"moby.whale@example.com\"\n\n  }\n\n}\n\nYou can create a new target that uses the same build configuration, but with slightly different attributes for a production build. In this example, the app-release target inherits the app-dev target, but overrides the tags attribute and adds a new platforms attribute:\n\ndocker-bake.hcl\ntarget \"app-release\" {\n\n  inherits = [\"app-dev\"]\n\n  tags = [\"docker.io/username/myapp:latest\"]\n\n  platforms = [\"linux/amd64\", \"linux/arm64\"]\n\n}\nCommon reusable targets\n\nOne common inheritance pattern is to define a common target that contains shared attributes for all or many of the build targets in the project. For example, the following _common target defines a common set of build arguments:\n\ndocker-bake.hcl\ntarget \"_common\" {\n\n  args = {\n\n    GO_VERSION = \"1.25\"\n\n    BUILDKIT_CONTEXT_KEEP_GIT_DIR = 1\n\n  }\n\n}\n\nYou can then inherit the _common target in other targets to apply the shared attributes:\n\ndocker-bake.hcl\ntarget \"lint\" {\n\n  inherits = [\"_common\"]\n\n  dockerfile = \"./dockerfiles/lint.Dockerfile\"\n\n  output = [{ type = \"cacheonly\" }]\n\n}\n\n\n\ntarget \"docs\" {\n\n  inherits = [\"_common\"]\n\n  dockerfile = \"./dockerfiles/docs.Dockerfile\"\n\n  output = [\"./docs/reference\"]\n\n}\n\n\n\ntarget \"test\" {\n\n  inherits = [\"_common\"]\n\n  target = \"test-output\"\n\n  output = [\"./test\"]\n\n}\n\n\n\ntarget \"binaries\" {\n\n  inherits = [\"_common\"]\n\n  target = \"binaries\"\n\n  output = [\"./build\"]\n\n  platforms = [\"local\"]\n\n}\nOverriding inherited attributes\n\nWhen a target inherits another target, it can override any of the inherited attributes. For example, the following target overrides the args attribute from the inherited target:\n\ndocker-bake.hcl\ntarget \"app-dev\" {\n\n  inherits = [\"_common\"]\n\n  args = {\n\n    GO_VERSION = \"1.17\"\n\n  }\n\n  tags = [\"docker.io/username/myapp:dev\"]\n\n}\n\nThe GO_VERSION argument in app-release is set to 1.17, overriding the GO_VERSION argument from the app-dev target.\n\nFor more information about overriding attributes, see the Overriding configurations page.\n\nInherit from multiple targets\n\nThe inherits attribute is a list, meaning you can reuse attributes from multiple other targets. In the following example, the app-release target reuses attributes from both the app-dev and _common targets.\n\ndocker-bake.hcl\ntarget \"_common\" {\n\n  args = {\n\n    GO_VERSION = \"1.25\"\n\n    BUILDKIT_CONTEXT_KEEP_GIT_DIR = 1\n\n  }\n\n}\n\n\n\ntarget \"app-dev\" {\n\n  inherits = [\"_common\"]\n\n  args = {\n\n    BUILDKIT_CONTEXT_KEEP_GIT_DIR = 0\n\n  }\n\n  tags = [\"docker.io/username/myapp:dev\"]\n\n  labels = {\n\n    \"org.opencontainers.image.source\" = \"https://github.com/username/myapp\"\n\n    \"org.opencontainers.image.author\" = \"moby.whale@example.com\"\n\n  }\n\n}\n\n\n\ntarget \"app-release\" {\n\n  inherits = [\"app-dev\", \"_common\"]\n\n  tags = [\"docker.io/username/myapp:latest\"]\n\n  platforms = [\"linux/amd64\", \"linux/arm64\"]\n\n}\n\nWhen inheriting attributes from multiple targets and there's a conflict, the target that appears last in the inherits list takes precedence. The previous example defines the BUILDKIT_CONTEXT_KEEP_GIT_DIR in the _common target and overrides it in the app-dev target.\n\nThe app-release target inherits both app-dev target and the _common target. The BUILDKIT_CONTEXT_KEEP_GIT_DIR argument is set to 0 in the app-dev target and 1 in the _common target. The BUILDKIT_CONTEXT_KEEP_GIT_DIR argument in the app-release target is set to 1, not 0, because the _common target appears last in the inherits list.\n\nReusing single attributes from targets\n\nIf you only want to inherit a single attribute from a target, you can reference an attribute from another target using dot notation. For example, in the following Bake file, the bar target reuses the tags attribute from the foo target:\n\ndocker-bake.hcl\ntarget \"foo\" {\n\n  dockerfile = \"foo.Dockerfile\"\n\n  tags       = [\"myapp:latest\"]\n\n}\n\ntarget \"bar\" {\n\n  dockerfile = \"bar.Dockerfile\"\n\n  tags       = target.foo.tags\n\n}\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCommon reusable targets\nOverriding inherited attributes\nInherit from multiple targets\nReusing single attributes from targets\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995691,
    "timestamp": "2026-02-07T06:34:33.350Z",
    "title": "Variables | Docker Docs",
    "url": "https://docs.docker.com/build/bake/variables/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nVariables\nVariables in Bake\nCopy as Markdown\n\nYou can define and use variables in a Bake file to set attribute values, interpolate them into other values, and perform arithmetic operations. Variables can be defined with default values, and can be overridden with environment variables.\n\nUsing variables as attribute values\n\nUse the variable block to define a variable.\n\ndocker-bake.hcl\nvariable \"TAG\" {\n\n  default = \"docker.io/username/webapp:latest\"\n\n}\n\nThe following example shows how to use the TAG variable in a target.\n\ndocker-bake.hcl\ntarget \"webapp\" {\n\n  context = \".\"\n\n  dockerfile = \"Dockerfile\"\n\n  tags = [ TAG ]\n\n}\nInterpolate variables into values\n\nBake supports string interpolation of variables into values. You can use the ${} syntax to interpolate a variable into a value. The following example defines a TAG variable with a value of latest.\n\ndocker-bake.hcl\nvariable \"TAG\" {\n\n  default = \"latest\"\n\n}\n\nTo interpolate the TAG variable into the value of an attribute, use the ${TAG} syntax.\n\ndocker-bake.hcl\ngroup \"default\" {\n\n  targets = [ \"webapp\" ]\n\n}\n\n\n\nvariable \"TAG\" {\n\n  default = \"latest\"\n\n}\n\n\n\ntarget \"webapp\" {\n\n  context = \".\"\n\n  dockerfile = \"Dockerfile\"\n\n  tags = [\"docker.io/username/webapp:${TAG}\"]\n\n}\n\nPrinting the Bake file with the --print flag shows the interpolated value in the resolved build configuration.\n\n$ docker buildx bake --print\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"webapp\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\"docker.io/username/webapp:latest\"]\n\n    }\n\n  }\n\n}\nValidating variables\n\nTo verify that the value of a variable conforms to an expected type, value range, or other condition, you can define custom validation rules using the validation block.\n\nIn the following example, validation is used to enforce a numeric constraint on a variable value; the PORT variable must be 1024 or greater.\n\ndocker-bake.hcl\n# Define a variable `PORT` with a default value and a validation rule\n\nvariable \"PORT\" {\n\n  default = 3000  # Default value assigned to `PORT`\n\n\n\n  # Validation block to ensure `PORT` is a valid number within the acceptable range\n\n  validation {\n\n    condition = PORT >= 1024  # Ensure `PORT` is at least 1024\n\n    error_message = \"The variable 'PORT' must be 1024 or greater.\"  # Error message for invalid values\n\n  }\n\n}\n\nIf the condition expression evaluates to false, the variable value is considered invalid, whereby the build invocation fails and error_message is emitted. For example, if PORT=443, the condition evaluates to false, and the error is raised.\n\nValues are coerced into the expected type before the validation is set. This ensures that any overrides set with environment variables work as expected.\n\nValidate multiple conditions\n\nTo evaluate more than one condition, define multiple validation blocks for the variable. All conditions must be true.\n\nHere‚Äôs an example:\n\ndocker-bake.hcl\n# Define a variable `VAR` with multiple validation rules\n\nvariable \"VAR\" {\n\n  # First validation block: Ensure the variable is not empty\n\n  validation {\n\n    condition = VAR != \"\"\n\n    error_message = \"The variable 'VAR' must not be empty.\"\n\n  }\n\n\n\n  # Second validation block: Ensure the value contains only alphanumeric characters\n\n  validation {\n\n    # VAR and the regex match must be identical:\n\n    condition = VAR == regex(\"[a-zA-Z0-9]+\", VAR)\n\n    error_message = \"The variable 'VAR' can only contain letters and numbers.\"\n\n  }\n\n}\n\nThis example enforces:\n\nThe variable must not be empty.\nThe variable must match a specific character set.\n\nFor invalid inputs like VAR=\"hello@world\", the validation would fail.\n\nValidating variable dependencies\n\nYou can reference other Bake variables in your condition expression, enabling validations that enforce dependencies between variables. This ensures that dependent variables are set correctly before proceeding.\n\nHere‚Äôs an example:\n\ndocker-bake.hcl\n# Define a variable `FOO`\n\nvariable \"FOO\" {}\n\n\n\n# Define a variable `BAR` with a validation rule that references `FOO`\n\nvariable \"BAR\" {\n\n  # Validation block to ensure `FOO` is set if `BAR` is used\n\n  validation {\n\n    condition = FOO != \"\"  # Check if `FOO` is not an empty string\n\n    error_message = \"The variable 'BAR' requires 'FOO' to be set.\"\n\n  }\n\n}\n\nThis configuration ensures that the BAR variable can only be used if FOO has been assigned a non-empty value. Attempting to build without setting FOO will trigger the validation error.\n\nEscape variable interpolation\n\nIf you want to bypass variable interpolation when parsing the Bake definition, use double dollar signs ($${VARIABLE}).\n\ndocker-bake.hcl\ntarget \"webapp\" {\n\n  dockerfile-inline = <<EOF\n\n  FROM alpine\n\n  ARG TARGETARCH\n\n  RUN echo \"Building for $${TARGETARCH/amd64/x64}\"\n\n  EOF\n\n  platforms = [\"linux/amd64\", \"linux/arm64\"]\n\n}\n$ docker buildx bake --progress=plain\n\n...\n\n#8 [linux/arm64 2/2] RUN echo \"Building for arm64\"\n\n#8 0.036 Building for arm64\n\n#8 DONE 0.0s\n\n\n\n#9 [linux/amd64 2/2] RUN echo \"Building for x64\"\n\n#9 0.046 Building for x64\n\n#9 DONE 0.1s\n\n...\n\nUsing variables in variables across files\n\nWhen multiple files are specified, one file can use variables defined in another file. In the following example, the vars.hcl file defines a BASE_IMAGE variable with a default value of docker.io/library/alpine.\n\nvars.hcl\nvariable \"BASE_IMAGE\" {\n\n  default = \"docker.io/library/alpine\"\n\n}\n\nThe following docker-bake.hcl file defines a BASE_LATEST variable that references the BASE_IMAGE variable.\n\ndocker-bake.hcl\nvariable \"BASE_LATEST\" {\n\n  default = \"${BASE_IMAGE}:latest\"\n\n}\n\n\n\ntarget \"webapp\" {\n\n  contexts = {\n\n    base = BASE_LATEST\n\n  }\n\n}\n\nWhen you print the resolved build configuration, using the -f flag to specify the vars.hcl and docker-bake.hcl files, you see that the BASE_LATEST variable is resolved to docker.io/library/alpine:latest.\n\n$ docker buildx bake -f vars.hcl -f docker-bake.hcl --print app\n\n{\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"context\": \".\",\n\n      \"contexts\": {\n\n        \"base\": \"docker.io/library/alpine:latest\"\n\n      },\n\n      \"dockerfile\": \"Dockerfile\"\n\n    }\n\n  }\n\n}\nAdditional resources\n\nHere are some additional resources that show how you can use variables in Bake:\n\nYou can override variable values using environment variables. See Overriding configurations for more information.\nYou can refer to and use global variables in functions. See HCL functions\nYou can use variable values when evaluating expressions. See Expression evaluation\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUsing variables as attribute values\nInterpolate variables into values\nValidating variables\nValidate multiple conditions\nValidating variable dependencies\nEscape variable interpolation\nUsing variables in variables across files\nAdditional resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995694,
    "timestamp": "2026-02-07T06:34:33.350Z",
    "title": "Expressions | Docker Docs",
    "url": "https://docs.docker.com/build/bake/expressions/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nExpressions\nExpression evaluation in Bake\nCopy as Markdown\n\nBake files in the HCL format support expression evaluation, which lets you perform arithmetic operations, conditionally set values, and more.\n\nArithmetic operations\n\nYou can perform arithmetic operations in expressions. The following example shows how to multiply two numbers.\n\ndocker-bake.hcl\nsum = 7*6\n\n\n\ntarget \"default\" {\n\n  args = {\n\n    answer = sum\n\n  }\n\n}\n\nPrinting the Bake file with the --print flag shows the evaluated value for the answer build argument.\n\n$ docker buildx bake --print\n\n{\n\n  \"target\": {\n\n    \"default\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"args\": {\n\n        \"answer\": \"42\"\n\n      }\n\n    }\n\n  }\n\n}\nTernary operators\n\nYou can use ternary operators to conditionally register a value.\n\nThe following example adds a tag only when a variable is not empty, using the built-in notequal function.\n\ndocker-bake.hcl\nvariable \"TAG\" {}\n\n\n\ntarget \"default\" {\n\n  context=\".\"\n\n  dockerfile=\"Dockerfile\"\n\n  tags = [\n\n    \"my-image:latest\",\n\n    notequal(\"\",TAG) ? \"my-image:${TAG}\": \"\"\n\n  ]\n\n}\n\nIn this case, TAG is an empty string, so the resulting build configuration only contains the hard-coded my-image:latest tag.\n\n$ docker buildx bake --print\n\n{\n\n  \"target\": {\n\n    \"default\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\"my-image:latest\"]\n\n    }\n\n  }\n\n}\nExpressions with variables\n\nYou can use expressions with variables to conditionally set values, or to perform arithmetic operations.\n\nThe following example uses expressions to set values based on the value of variables. The v1 build argument is set to \"higher\" if the variable FOO is greater than 5, otherwise it is set to \"lower\". The v2 build argument is set to \"yes\" if the IS_FOO variable is true, otherwise it is set to \"no\".\n\ndocker-bake.hcl\nvariable \"FOO\" {\n\n  default = 3\n\n}\n\n\n\nvariable \"IS_FOO\" {\n\n  default = true\n\n}\n\n\n\ntarget \"app\" {\n\n  args = {\n\n    v1 = FOO > 5 ? \"higher\" : \"lower\"\n\n    v2 = IS_FOO ? \"yes\" : \"no\"\n\n  }\n\n}\n\nPrinting the Bake file with the --print flag shows the evaluated values for the v1 and v2 build arguments.\n\n$ docker buildx bake --print app\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"app\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"app\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"args\": {\n\n        \"v1\": \"lower\",\n\n        \"v2\": \"yes\"\n\n      }\n\n    }\n\n  }\n\n}\n\nEdit this page\n\nRequest changes\n\nTable of contents\nArithmetic operations\nTernary operators\nExpressions with variables\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995697,
    "timestamp": "2026-02-07T06:34:33.353Z",
    "title": "Functions | Docker Docs",
    "url": "https://docs.docker.com/build/bake/funcs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nFunctions\nFunctions\nCopy as Markdown\n\nHCL functions are great for when you need to manipulate values in your build configuration in more complex ways than just concatenation or interpolation.\n\nStandard library\n\nBake ships with built-in support for the standard library functions.\n\nThe following example shows the add function:\n\ndocker-bake.hcl\nvariable \"TAG\" {\n\n  default = \"latest\"\n\n}\n\n\n\ngroup \"default\" {\n\n  targets = [\"webapp\"]\n\n}\n\n\n\ntarget \"webapp\" {\n\n  args = {\n\n    buildno = \"${add(123, 1)}\"\n\n  }\n\n}\n$ docker buildx bake --print webapp\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"webapp\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"args\": {\n\n        \"buildno\": \"124\"\n\n      }\n\n    }\n\n  }\n\n}\nUser-defined functions\n\nYou can create user-defined functions that do just what you want, if the built-in standard library functions don't meet your needs.\n\nThe following example defines an increment function.\n\ndocker-bake.hcl\nfunction \"increment\" {\n\n  params = [number]\n\n  result = number + 1\n\n}\n\n\n\ngroup \"default\" {\n\n  targets = [\"webapp\"]\n\n}\n\n\n\ntarget \"webapp\" {\n\n  args = {\n\n    buildno = \"${increment(123)}\"\n\n  }\n\n}\n$ docker buildx bake --print webapp\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"webapp\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"args\": {\n\n        \"buildno\": \"124\"\n\n      }\n\n    }\n\n  }\n\n}\nVariables in functions\n\nYou can make references to variables and standard library functions inside your functions.\n\nYou can't reference user-defined functions from other functions.\n\nThe following example uses a global variable (REPO) in a custom function.\n\ndocker-bake.hcl\n# docker-bake.hcl\n\nvariable \"REPO\" {\n\n  default = \"user/repo\"\n\n}\n\n\n\nfunction \"tag\" {\n\n  params = [tag]\n\n  result = [\"${REPO}:${tag}\"]\n\n}\n\n\n\ntarget \"webapp\" {\n\n  tags = tag(\"v1\")\n\n}\n\nPrinting the Bake file with the --print flag shows that the tag function uses the value of REPO to set the prefix of the tag.\n\n$ docker buildx bake --print webapp\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"webapp\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\"user/repo:v1\"]\n\n    }\n\n  }\n\n}\n\nEdit this page\n\nRequest changes\n\nTable of contents\nStandard library\nUser-defined functions\nVariables in functions\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995700,
    "timestamp": "2026-02-07T06:34:33.364Z",
    "title": "Matrix targets | Docker Docs",
    "url": "https://docs.docker.com/build/bake/matrices/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nMatrix targets\nMatrix targets\nCopy as Markdown\n\nA matrix strategy lets you fork a single target into multiple different variants, based on parameters that you specify. This works in a similar way to Matrix strategies for GitHub Actions. You can use this to reduce duplication in your Bake definition.\n\nThe matrix attribute is a map of parameter names to lists of values. Bake builds each possible combination of values as a separate target.\n\nEach generated target must have a unique name. To specify how target names should resolve, use the name attribute.\n\nThe following example resolves the app target to app-foo and app-bar. It also uses the matrix value to define the target build stage.\n\ndocker-bake.hcl\ntarget \"app\" {\n\n  name = \"app-${tgt}\"\n\n  matrix = {\n\n    tgt = [\"foo\", \"bar\"]\n\n  }\n\n  target = tgt\n\n}\n$ docker buildx bake --print app\n\n[+] Building 0.0s (0/0)\n\n{\n\n  \"group\": {\n\n    \"app\": {\n\n      \"targets\": [\n\n        \"app-foo\",\n\n        \"app-bar\"\n\n      ]\n\n    },\n\n    \"default\": {\n\n      \"targets\": [\n\n        \"app\"\n\n      ]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"app-bar\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"target\": \"bar\"\n\n    },\n\n    \"app-foo\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"target\": \"foo\"\n\n    }\n\n  }\n\n}\n\nMultiple axes\n\nYou can specify multiple keys in your matrix to fork a target on multiple axes. When using multiple matrix keys, Bake builds every possible variant.\n\nThe following example builds four targets:\n\napp-foo-1-0\napp-foo-2-0\napp-bar-1-0\napp-bar-2-0\ndocker-bake.hcl\ntarget \"app\" {\n\n  name = \"app-${tgt}-${replace(version, \".\", \"-\")}\"\n\n  matrix = {\n\n    tgt = [\"foo\", \"bar\"]\n\n    version = [\"1.0\", \"2.0\"]\n\n  }\n\n  target = tgt\n\n  args = {\n\n    VERSION = version\n\n  }\n\n}\nMultiple values per matrix target\n\nIf you want to differentiate the matrix on more than just a single value, you can use maps as matrix values. Bake creates a target for each map, and you can access the nested values using dot notation.\n\nThe following example builds two targets:\n\napp-foo-1-0\napp-bar-2-0\ndocker-bake.hcl\ntarget \"app\" {\n\n  name = \"app-${item.tgt}-${replace(item.version, \".\", \"-\")}\"\n\n  matrix = {\n\n    item = [\n\n      {\n\n        tgt = \"foo\"\n\n        version = \"1.0\"\n\n      },\n\n      {\n\n        tgt = \"bar\"\n\n        version = \"2.0\"\n\n      }\n\n    ]\n\n  }\n\n  target = item.tgt\n\n  args = {\n\n    VERSION = item.version\n\n  }\n\n}\n\nEdit this page\n\nRequest changes\n\nTable of contents\nMultiple axes\nMultiple values per matrix target\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995703,
    "timestamp": "2026-02-07T06:34:33.367Z",
    "title": "Contexts | Docker Docs",
    "url": "https://docs.docker.com/build/bake/contexts/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nContexts\nUsing Bake with additional contexts\nCopy as Markdown\n\nIn addition to the main context key that defines the build context, each target can also define additional named contexts with a map defined with key contexts. These values map to the --build-context flag in the build command.\n\nInside the Dockerfile these contexts can be used with the FROM instruction or --from flag.\n\nSupported context values are:\n\nLocal filesystem directories\nContainer images\nGit URLs\nHTTP URLs\nName of another target in the Bake file\nPinning alpine image\nDockerfile\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nRUN echo \"Hello world\"\ndocker-bake.hcl\ntarget \"app\" {\n\n  contexts = {\n\n    alpine = \"docker-image://alpine:3.13\"\n\n  }\n\n}\nUsing a secondary source directory\nDockerfile\nFROM golang\n\nCOPY --from=src . .\ndocker-bake.hcl\n# Running `docker buildx bake app` will result in `src` not pointing\n\n# to some previous build stage but to the client filesystem, not part of the context.\n\ntarget \"app\" {\n\n  contexts = {\n\n    src = \"../path/to/source\"\n\n  }\n\n}\nUsing a target as a build context\n\nTo use a result of one target as a build context of another, specify the target name with target: prefix.\n\nbaseapp.Dockerfile\nFROM scratch\nDockerfile\n# syntax=docker/dockerfile:1\n\nFROM baseapp\n\nRUN echo \"Hello world\"\ndocker-bake.hcl\ntarget \"base\" {\n\n  dockerfile = \"baseapp.Dockerfile\"\n\n}\n\n\n\ntarget \"app\" {\n\n  contexts = {\n\n    baseapp = \"target:base\"\n\n  }\n\n}\n\nIn most cases you should just use a single multi-stage Dockerfile with multiple targets for similar behavior. This case is only recommended when you have multiple Dockerfiles that can't be easily merged into one.\n\nDeduplicate context transfer\nNote\n\nAs of Buildx version 0.17.0 and later, Bake automatically de-duplicates context transfer for targets that share the same context. In addition to Buildx version 0.17.0, the builder must be running BuildKit version 0.16.0 or later, and the Dockerfile syntax must be docker/dockerfile:1.10 or later.\n\nIf you meet these requirements, you don't need to manually de-duplicate context transfer as described in this section.\n\nTo check your Buildx version, run docker buildx version.\nTo check your BuildKit version, run docker buildx inspect --bootstrap and look for the BuildKit version field.\nTo check your Dockerfile syntax version, check the syntax parser directive in your Dockerfile. If it's not present, the default version whatever comes bundled with your current version of BuildKit. To set the version explicitly, add #syntax=docker/dockerfile:1.10 at the top of your Dockerfile.\n\nWhen you build targets concurrently, using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it's used. This can result in significant impact on build time, depending on your build configuration. For example, say you have a Bake file that defines the following group of targets:\n\ndocker-bake.hcl\ngroup \"default\" {\n\n  targets = [\"target1\", \"target2\"]\n\n}\n\n\n\ntarget \"target1\" {\n\n  target = \"target1\"\n\n  context = \".\"\n\n}\n\n\n\ntarget \"target2\" {\n\n  target = \"target2\"\n\n  context = \".\"\n\n}\n\nIn this case, the context . is transferred twice when you build the default group: once for target1 and once for target2.\n\nIf your context is small, and if you are using a local builder, duplicate context transfers may not be a big deal. But if your build context is big, or you have a large number of targets, or you're transferring the context over a network to a remote builder, context transfer becomes a performance bottleneck.\n\nTo avoid transferring the same context multiple times, you can define a named context that only loads the context files, and have each target that needs those files reference that named context. For example, the following Bake file defines a named target ctx, which is used by both target1 and target2:\n\ndocker-bake.hcl\ngroup \"default\" {\n\n  targets = [\"target1\", \"target2\"]\n\n}\n\n\n\ntarget \"ctx\" {\n\n  context = \".\"\n\n  target = \"ctx\"\n\n}\n\n\n\ntarget \"target1\" {\n\n  target = \"target1\"\n\n  contexts = {\n\n    ctx = \"target:ctx\"\n\n  }\n\n}\n\n\n\ntarget \"target2\" {\n\n  target = \"target2\"\n\n  contexts = {\n\n    ctx = \"target:ctx\"\n\n  }\n\n}\n\nThe named context ctx represents a Dockerfile stage, which copies the files from its context (.). Other stages in the Dockerfile can now reference the ctx named context and, for example, mount its files with --mount=from=ctx.\n\nDockerfile\nFROM scratch AS ctx\n\nCOPY --link . .\n\n\n\nFROM golang:alpine AS target1\n\nWORKDIR /work\n\nRUN --mount=from=ctx \\\n\n    go build -o /out/client ./cmd/client \\\n\n\n\nFROM golang:alpine AS target2\n\nWORKDIR /work\n\nRUN --mount=from=ctx \\\n\n    go build -o /out/server ./cmd/server\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPinning alpine image\nUsing a secondary source directory\nUsing a target as a build context\nDeduplicate context transfer\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995706,
    "timestamp": "2026-02-07T06:34:33.370Z",
    "title": "Bake file reference | Docker Docs",
    "url": "https://docs.docker.com/build/bake/reference/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nBake file reference\nBake file reference\nCopy as Markdown\n\nThe Bake file is a file for defining workflows that you run using docker buildx bake.\n\nFile format\n\nYou can define your Bake file in the following file formats:\n\nHashiCorp Configuration Language (HCL)\nJSON\nYAML (Compose file)\n\nBy default, Bake uses the following lookup order to find the configuration file:\n\ncompose.yaml\ncompose.yml\ndocker-compose.yml\ndocker-compose.yaml\ndocker-bake.json\ndocker-bake.hcl\ndocker-bake.override.json\ndocker-bake.override.hcl\n\nYou can specify the file location explicitly using the --file flag:\n\n$ docker buildx bake --file ../docker/bake.hcl --print\n\n\nIf you don't specify a file explicitly, Bake searches for the file in the current working directory. If more than one Bake file is found, all files are merged into a single definition. Files are merged according to the lookup order. That means that if your project contains both a compose.yaml file and a docker-bake.hcl file, Bake loads the compose.yaml file first, and then the docker-bake.hcl file.\n\nIf merged files contain duplicate attribute definitions, those definitions are either merged or overridden by the last occurrence, depending on the attribute. The following attributes are overridden by the last occurrence:\n\ntarget.cache-to\ntarget.dockerfile-inline\ntarget.dockerfile\ntarget.outputs\ntarget.platforms\ntarget.pull\ntarget.tags\ntarget.target\n\nFor example, if compose.yaml and docker-bake.hcl both define the tags attribute, the docker-bake.hcl is used.\n\n$ cat compose.yaml\n\nservices:\n\n  webapp:\n\n    build:\n\n      context: .\n\n      tags:\n\n        - bar\n\n$ cat docker-bake.hcl\n\ntarget \"webapp\" {\n\n  tags = [\"foo\"]\n\n}\n\n$ docker buildx bake --print webapp\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\n\n        \"webapp\"\n\n      ]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\n\n        \"foo\"\n\n      ]\n\n    }\n\n  }\n\n}\n\n\nAll other attributes are merged. For example, if compose.yaml and docker-bake.hcl both define unique entries for the labels attribute, all entries are included. Duplicate entries for the same label are overridden.\n\n$ cat compose.yaml\n\nservices:\n\n  webapp:\n\n    build:\n\n      context: .\n\n      labels: \n\n        com.example.foo: \"foo\"\n\n        com.example.name: \"Alice\"\n\n$ cat docker-bake.hcl\n\ntarget \"webapp\" {\n\n  labels = {\n\n    \"com.example.bar\" = \"bar\"\n\n    \"com.example.name\" = \"Bob\"\n\n  }\n\n}\n\n$ docker buildx bake --print webapp\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\n\n        \"webapp\"\n\n      ]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"labels\": {\n\n        \"com.example.foo\": \"foo\",\n\n        \"com.example.bar\": \"bar\",\n\n        \"com.example.name\": \"Bob\"\n\n      }\n\n    }\n\n  }\n\n}\n\nSyntax\n\nThe Bake file supports the following property types:\n\ntarget: build targets\ngroup: collections of build targets\nvariable: build arguments and variables\nfunction: custom Bake functions\n\nYou define properties as hierarchical blocks in the Bake file. You can assign one or more attributes to a property.\n\nThe following snippet shows a JSON representation of a simple Bake file. This Bake file defines three properties: a variable, a group, and a target.\n\n{\n\n  \"variable\": {\n\n    \"TAG\": {\n\n      \"default\": \"latest\"\n\n    }\n\n  },\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"webapp\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\"docker.io/username/webapp:${TAG}\"]\n\n    }\n\n  }\n\n}\n\nIn the JSON representation of a Bake file, properties are objects, and attributes are values assigned to those objects.\n\nThe following example shows the same Bake file in the HCL format:\n\nvariable \"TAG\" {\n\n  default = \"latest\"\n\n}\n\n\n\ngroup \"default\" {\n\n  targets = [\"webapp\"]\n\n}\n\n\n\ntarget \"webapp\" {\n\n  dockerfile = \"Dockerfile\"\n\n  tags = [\"docker.io/username/webapp:${TAG}\"]\n\n}\n\nHCL is the preferred format for Bake files. Aside from syntactic differences, HCL lets you use features that the JSON and YAML formats don't support.\n\nThe examples in this document use the HCL format.\n\nTarget\n\nA target reflects a single docker build invocation. Consider the following build command:\n\n$ docker build \\\n\n  --file=Dockerfile.webapp \\\n\n  --tag=docker.io/username/webapp:latest \\\n\n  https://github.com/username/webapp\n\n\nYou can express this command in a Bake file as follows:\n\ntarget \"webapp\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  context = \"https://github.com/username/webapp\"\n\n}\n\nThe following table shows the complete list of attributes that you can assign to a target:\n\nName\tType\tDescription\nargs\tMap\tBuild arguments\nannotations\tList\tExporter annotations\nattest\tList\tBuild attestations\ncache-from\tList\tExternal cache sources\ncache-to\tList\tExternal cache destinations\ncall\tString\tSpecify the frontend method to call for the target.\ncontext\tString\tSet of files located in the specified path or URL\ncontexts\tMap\tAdditional build contexts\ndescription\tString\tDescription of a target\ndockerfile-inline\tString\tInline Dockerfile string\ndockerfile\tString\tDockerfile location\nentitlements\tList\tPermissions that the build process requires to run\nextra-hosts\tList\tCustoms host-to-IP mapping\ninherits\tList\tInherit attributes from other targets\nlabels\tMap\tMetadata for images\nmatrix\tMap\tDefine a set of variables that forks a target into multiple targets.\nname\tString\tOverride the target name when using a matrix.\nno-cache-filter\tList\tDisable build cache for specific stages\nno-cache\tBoolean\tDisable build cache completely\noutput\tList\tOutput destinations\npolicy\tList\tPolicies to validate build sources and metadata\nplatforms\tList\tTarget platforms\npull\tBoolean\tAlways pull images\nsecret\tList\tSecrets to expose to the build\nshm-size\tList\tSize of /dev/shm\nssh\tList\tSSH agent sockets or keys to expose to the build\ntags\tList\tImage names and tags\ntarget\tString\tTarget build stage\nulimits\tList\tUlimit options\ntarget.args\n\nUse the args attribute to define build arguments for the target. This has the same effect as passing a --build-arg flag to the build command.\n\ntarget \"default\" {\n\n  args = {\n\n    VERSION = \"0.0.0+unknown\"\n\n  }\n\n}\n\nYou can set args attributes to use null values. Doing so forces the target to use the ARG value specified in the Dockerfile.\n\nvariable \"GO_VERSION\" {\n\n  default = \"1.20.3\"\n\n}\n\n\n\ntarget \"webapp\" {\n\n  dockerfile = \"webapp.Dockerfile\"\n\n  tags = [\"docker.io/username/webapp\"]\n\n}\n\n\n\ntarget \"db\" {\n\n  args = {\n\n    GO_VERSION = null\n\n  }\n\n  dockerfile = \"db.Dockerfile\"\n\n  tags = [\"docker.io/username/db\"]\n\n}\ntarget.annotations\n\nThe annotations attribute lets you add annotations to images built with bake. The key takes a list of annotations, in the format of KEY=VALUE.\n\ntarget \"default\" {\n\n  output = [{ type = \"image\", name = \"foo\" }]\n\n  annotations = [\"org.opencontainers.image.authors=dvdksn\"]\n\n}\n\nBy default, the annotation is added to image manifests. You can configure the level of the annotations by adding a prefix to the annotation, containing a comma-separated list of all the levels that you want to annotate. The following example adds annotations to both the image index and manifests.\n\ntarget \"default\" {\n\n  output = [\n\n    {\n\n      type = \"image\"\n\n      name = \"foo\"\n\n    }\n\n  ]\n\n  annotations = [\"index,manifest:org.opencontainers.image.authors=dvdksn\"]\n\n}\n\nRead about the supported levels in Specifying annotation levels.\n\ntarget.attest\n\nThe attest attribute lets you apply build attestations to the target. This attribute accepts the long-form CSV version of attestation parameters.\n\ntarget \"default\" {\n\n  attest = [\n\n    {\n\n      type = \"provenance\"\n\n      mode = \"max\"\n\n    },\n\n    {\n\n      type = \"sbom\"\n\n    }\n\n  ]\n\n}\ntarget.cache-from\n\nBuild cache sources. The builder imports cache from the locations you specify. It uses the Buildx cache storage backends, and it works the same way as the --cache-from flag. This takes a list value, so you can specify multiple cache sources.\n\ntarget \"app\" {\n\n  cache-from = [\n\n    {\n\n      type = \"s3\"\n\n      region = \"eu-west-1\"\n\n      bucket = \"mybucket\"\n\n    },\n\n    {\n\n      type = \"registry\"\n\n      ref = \"user/repo:cache\"\n\n    }\n\n  ]\n\n}\ntarget.cache-to\n\nBuild cache export destinations. The builder exports its build cache to the locations you specify. It uses the Buildx cache storage backends, and it works the same way as the --cache-to flag. This takes a list value, so you can specify multiple cache export targets.\n\ntarget \"app\" {\n\n  cache-to = [\n\n    {\n\n      type = \"s3\"\n\n      region = \"eu-west-1\"\n\n      bucket = \"mybucket\"\n\n    },\n\n    {\n\n      type = \"inline\"\n\n    }\n\n  ]\n\n}\ntarget.call\n\nSpecifies the frontend method to use. Frontend methods let you, for example, execute build checks only, instead of running a build. This is the same as the --call flag.\n\ntarget \"app\" {\n\n  call = \"check\"\n\n}\n\nSupported values are:\n\nbuild builds the target (default)\ncheck: evaluates build checks for the target\noutline: displays the target's build arguments and their default values if available\ntargets: lists all Bake targets in the loaded definition, along with its description.\n\nFor more information about frontend methods, refer to the CLI reference for docker buildx build --call.\n\ntarget.context\n\nSpecifies the location of the build context to use for this target. Accepts a URL or a directory path. This is the same as the build context positional argument that you pass to the build command.\n\ntarget \"app\" {\n\n  context = \"./src/www\"\n\n}\n\nThis resolves to the current working directory (\".\") by default.\n\n$ docker buildx bake --print -f - <<< 'target \"default\" {}'\n\n[+] Building 0.0s (0/0)\n\n{\n\n  \"target\": {\n\n    \"default\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\"\n\n    }\n\n  }\n\n}\n\ntarget.contexts\n\nAdditional build contexts. This is the same as the --build-context flag. This attribute takes a map, where keys result in named contexts that you can reference in your builds.\n\nYou can specify different types of contexts, such local directories, Git URLs, and even other Bake targets. Bake automatically determines the type of a context based on the pattern of the context value.\n\nContext type\tExample\nContainer image\tdocker-image://alpine@sha256:0123456789\nGit URL\thttps://github.com/user/proj.git\nHTTP URL\thttps://example.com/files\nLocal directory\t../path/to/src\nBake target\ttarget:base\nPin an image version\n# docker-bake.hcl\n\ntarget \"app\" {\n\n  contexts = {\n\n    alpine = \"docker-image://alpine:3.13\"\n\n  }\n\n}\n# Dockerfile\n\nFROM alpine\n\nRUN echo \"Hello world\"\nUse a local directory\n# docker-bake.hcl\n\ntarget \"app\" {\n\n  contexts = {\n\n    src = \"../path/to/source\"\n\n  }\n\n}\n# Dockerfile\n\nFROM scratch AS src\n\nFROM golang\n\nCOPY --from=src . .\nUse another target as base\nNote\n\nYou should prefer to use regular multi-stage builds over this option. You can Use this feature when you have multiple Dockerfiles that can't be easily merged into one.\n\n# docker-bake.hcl\n\ntarget \"base\" {\n\n  dockerfile = \"baseapp.Dockerfile\"\n\n}\n\n\n\ntarget \"app\" {\n\n  contexts = {\n\n    baseapp = \"target:base\"\n\n  }\n\n}\n# Dockerfile\n\nFROM baseapp\n\nRUN echo \"Hello world\"\ntarget.description\n\nDefines a human-readable description for the target, clarifying its purpose or functionality.\n\ntarget \"lint\" {\n\n  description = \"Runs golangci-lint to detect style errors\"\n\n  args = {\n\n    GOLANGCI_LINT_VERSION = null\n\n  }\n\n  dockerfile = \"lint.Dockerfile\"\n\n}\n\nThis attribute is useful when combined with the docker buildx bake --list=targets option, providing a more informative output when listing the available build targets in a Bake file.\n\ntarget.dockerfile-inline\n\nUses the string value as an inline Dockerfile for the build target.\n\ntarget \"default\" {\n\n  dockerfile-inline = \"FROM alpine\\nENTRYPOINT [\\\"echo\\\", \\\"hello\\\"]\"\n\n}\n\nThe dockerfile-inline takes precedence over the dockerfile attribute. If you specify both, Bake uses the inline version.\n\ntarget.dockerfile\n\nName of the Dockerfile to use for the build. This is the same as the --file flag for the docker build command.\n\ntarget \"default\" {\n\n  dockerfile = \"./src/www/Dockerfile\"\n\n}\n\nResolves to \"Dockerfile\" by default.\n\n$ docker buildx bake --print -f - <<< 'target \"default\" {}'\n\n[+] Building 0.0s (0/0)\n\n{\n\n  \"target\": {\n\n    \"default\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\"\n\n    }\n\n  }\n\n}\n\ntarget.entitlements\n\nEntitlements are permissions that the build process requires to run.\n\nCurrently supported entitlements are:\n\nnetwork.host: Allows the build to use commands that access the host network. In Dockerfile, use RUN --network=host to run a command with host network enabled.\n\nsecurity.insecure: Allows the build to run commands in privileged containers that are not limited by the default security sandbox. Such container may potentially access and modify system resources. In Dockerfile, use RUN --security=insecure to run a command in a privileged container.\n\ntarget \"integration-tests\" {\n\n  # this target requires privileged containers to run nested containers\n\n  entitlements = [\"security.insecure\"]\n\n}\n\nEntitlements are enabled with a two-step process. First, a target must declare the entitlements it requires. Secondly, when invoking the bake command, the user must grant the entitlements by passing the --allow flag or confirming the entitlements when prompted in an interactive terminal. This is to ensure that the user is aware of the possibly insecure permissions they are granting to the build process.\n\ntarget.extra-hosts\n\nUse the extra-hosts attribute to define customs host-to-IP mapping for the target. This has the same effect as passing a --add-host flag to the build command.\n\ntarget \"default\" {\n\n  extra-hosts = {\n\n    my_hostname = \"8.8.8.8\"\n\n  }\n\n}\ntarget.inherits\n\nA target can inherit attributes from other targets. Use inherits to reference from one target to another.\n\nIn the following example, the app-dev target specifies an image name and tag. The app-release target uses inherits to reuse the tag name.\n\nvariable \"TAG\" {\n\n  default = \"latest\"\n\n}\n\n\n\ntarget \"app-dev\" {\n\n  tags = [\"docker.io/username/myapp:${TAG}\"]\n\n}\n\n\n\ntarget \"app-release\" {\n\n  inherits = [\"app-dev\"]\n\n  platforms = [\"linux/amd64\", \"linux/arm64\"]\n\n}\n\nThe inherits attribute is a list, meaning you can reuse attributes from multiple other targets. In the following example, the app-release target reuses attributes from both the app-dev and _release targets.\n\ntarget \"app-dev\" {\n\n  args = {\n\n    GO_VERSION = \"1.20\"\n\n    BUILDX_EXPERIMENTAL = 1\n\n  }\n\n  tags = [\"docker.io/username/myapp\"]\n\n  dockerfile = \"app.Dockerfile\"\n\n  labels = {\n\n    \"org.opencontainers.image.source\" = \"https://github.com/username/myapp\"\n\n  }\n\n}\n\n\n\ntarget \"_release\" {\n\n  args = {\n\n    BUILDKIT_CONTEXT_KEEP_GIT_DIR = 1\n\n    BUILDX_EXPERIMENTAL = 0\n\n  }\n\n}\n\n\n\ntarget \"app-release\" {\n\n  inherits = [\"app-dev\", \"_release\"]\n\n  platforms = [\"linux/amd64\", \"linux/arm64\"]\n\n}\n\nWhen inheriting attributes from multiple targets and there's a conflict, the target that appears last in the inherits list takes precedence. The previous example defines the BUILDX_EXPERIMENTAL argument twice for the app-release target. It resolves to 0 because the _release target appears last in the inheritance chain:\n\n$ docker buildx bake --print app-release\n\n[+] Building 0.0s (0/0)\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\n\n        \"app-release\"\n\n      ]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"app-release\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"app.Dockerfile\",\n\n      \"args\": {\n\n        \"BUILDKIT_CONTEXT_KEEP_GIT_DIR\": \"1\",\n\n        \"BUILDX_EXPERIMENTAL\": \"0\",\n\n        \"GO_VERSION\": \"1.20\"\n\n      },\n\n      \"labels\": {\n\n        \"org.opencontainers.image.source\": \"https://github.com/username/myapp\"\n\n      },\n\n      \"tags\": [\n\n        \"docker.io/username/myapp\"\n\n      ],\n\n      \"platforms\": [\n\n        \"linux/amd64\",\n\n        \"linux/arm64\"\n\n      ]\n\n    }\n\n  }\n\n}\n\ntarget.labels\n\nAssigns image labels to the build. This is the same as the --label flag for docker build.\n\ntarget \"default\" {\n\n  labels = {\n\n    \"org.opencontainers.image.source\" = \"https://github.com/username/myapp\"\n\n    \"com.docker.image.source.entrypoint\" = \"Dockerfile\"\n\n  }\n\n}\n\nIt's possible to use a null value for labels. If you do, the builder uses the label value specified in the Dockerfile.\n\ntarget.matrix\n\nA matrix strategy lets you fork a single target into multiple different variants, based on parameters that you specify. This works in a similar way to [Matrix strategies for GitHub Actions]. You can use this to reduce duplication in your bake definition.\n\nThe matrix attribute is a map of parameter names to lists of values. Bake builds each possible combination of values as a separate target.\n\nEach generated target must have a unique name. To specify how target names should resolve, use the name attribute.\n\nThe following example resolves the app target to app-foo and app-bar. It also uses the matrix value to define the target build stage.\n\ntarget \"app\" {\n\n  name = \"app-${tgt}\"\n\n  matrix = {\n\n    tgt = [\"foo\", \"bar\"]\n\n  }\n\n  target = tgt\n\n}\n$ docker buildx bake --print app\n\n[+] Building 0.0s (0/0)\n\n{\n\n  \"group\": {\n\n    \"app\": {\n\n      \"targets\": [\n\n        \"app-foo\",\n\n        \"app-bar\"\n\n      ]\n\n    },\n\n    \"default\": {\n\n      \"targets\": [\n\n        \"app\"\n\n      ]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"app-bar\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"target\": \"bar\"\n\n    },\n\n    \"app-foo\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"target\": \"foo\"\n\n    }\n\n  }\n\n}\n\nMultiple axes\n\nYou can specify multiple keys in your matrix to fork a target on multiple axes. When using multiple matrix keys, Bake builds every possible variant.\n\nThe following example builds four targets:\n\napp-foo-1-0\napp-foo-2-0\napp-bar-1-0\napp-bar-2-0\ntarget \"app\" {\n\n  name = \"app-${tgt}-${replace(version, \".\", \"-\")}\"\n\n  matrix = {\n\n    tgt = [\"foo\", \"bar\"]\n\n    version = [\"1.0\", \"2.0\"]\n\n  }\n\n  target = tgt\n\n  args = {\n\n    VERSION = version\n\n  }\n\n}\nMultiple values per matrix target\n\nIf you want to differentiate the matrix on more than just a single value, you can use maps as matrix values. Bake creates a target for each map, and you can access the nested values using dot notation.\n\nThe following example builds two targets:\n\napp-foo-1-0\napp-bar-2-0\ntarget \"app\" {\n\n  name = \"app-${item.tgt}-${replace(item.version, \".\", \"-\")}\"\n\n  matrix = {\n\n    item = [\n\n      {\n\n        tgt = \"foo\"\n\n        version = \"1.0\"\n\n      },\n\n      {\n\n        tgt = \"bar\"\n\n        version = \"2.0\"\n\n      }\n\n    ]\n\n  }\n\n  target = item.tgt\n\n  args = {\n\n    VERSION = item.version\n\n  }\n\n}\ntarget.name\n\nSpecify name resolution for targets that use a matrix strategy. The following example resolves the app target to app-foo and app-bar.\n\ntarget \"app\" {\n\n  name = \"app-${tgt}\"\n\n  matrix = {\n\n    tgt = [\"foo\", \"bar\"]\n\n  }\n\n  target = tgt\n\n}\ntarget.network\n\nSpecify the network mode for the whole build request. This will override the default network mode for all the RUN instructions in the Dockerfile. Accepted values are default, host, and none.\n\nUsually, a better approach to set the network mode for your build steps is to instead use RUN --network=<value> in your Dockerfile. This way, you can set the network mode for individual build steps and everyone building the Dockerfile gets consistent behavior without needing to pass additional flags to the build command.\n\nIf you set network mode to host in your Bake file, you must also grant network.host entitlement when invoking the bake command. This is because host network mode requires elevated privileges and can be a security risk. You can pass --allow=network.host to the docker buildx bake command to grant the entitlement, or you can confirm the entitlement when prompted if you are using an interactive terminal.\n\ntarget \"app\" {\n\n  # make sure this build does not access internet\n\n  network = \"none\"\n\n}\ntarget.no-cache-filter\n\nDon't use build cache for the specified stages. This is the same as the --no-cache-filter flag for docker build. The following example avoids build cache for the foo build stage.\n\ntarget \"default\" {\n\n  no-cache-filter = [\"foo\"]\n\n}\ntarget.no-cache\n\nDon't use cache when building the image. This is the same as the --no-cache flag for docker build.\n\ntarget \"default\" {\n\n  no-cache = true\n\n}\ntarget.output\n\nConfiguration for exporting the build output. This is the same as the --output flag. The following example configures the target to use a cache-only output,\n\ntarget \"default\" {\n\n  output = [{ type = \"cacheonly\" }]\n\n}\ntarget.policy\n\nPolicies to validate build sources and metadata. Each entry uses the same keys as the --policy flag for docker buildx build (filename, reset, disabled, strict, log-level). Bake also automatically loads Dockerfile.rego alongside the target Dockerfile when present.\n\ntarget \"default\" {\n\n  policy = [\n\n    { filename = \"extra.rego\" },\n\n  ]\n\n}\ntarget.platforms\n\nSet target platforms for the build target. This is the same as the --platform flag. The following example creates a multi-platform build for three architectures.\n\ntarget \"default\" {\n\n  platforms = [\"linux/amd64\", \"linux/arm64\", \"linux/arm/v7\"]\n\n}\ntarget.pull\n\nConfigures whether the builder should attempt to pull images when building the target. This is the same as the --pull flag for docker build. The following example forces the builder to always pull all images referenced in the build target.\n\ntarget \"default\" {\n\n  pull = true\n\n}\ntarget.secret\n\nDefines secrets to expose to the build target. This is the same as the --secret flag.\n\nvariable \"HOME\" {\n\n  default = null\n\n}\n\n\n\ntarget \"default\" {\n\n  secret = [\n\n    {\n\n      type = \"env\"\n\n      id = \"KUBECONFIG\"\n\n    },\n\n    {\n\n      type = \"file\"\n\n      id = \"aws\"\n\n      src = \"${HOME}/.aws/credentials\"\n\n    }\n\n  ]\n\n}\n\nThis lets you mount the secret in your Dockerfile.\n\nRUN --mount=type=secret,id=aws,target=/root/.aws/credentials \\\n\n    aws cloudfront create-invalidation ...\n\nRUN --mount=type=secret,id=KUBECONFIG,env=KUBECONFIG \\\n\n    helm upgrade --install\ntarget.shm-size\n\nSets the size of the shared memory allocated for build containers when using RUN instructions.\n\nThe format is <number><unit>. number must be greater than 0. Unit is optional and can be b (bytes), k (kilobytes), m (megabytes), or g (gigabytes). If you omit the unit, the system uses bytes.\n\nThis is the same as the --shm-size flag for docker build.\n\ntarget \"default\" {\n\n  shm-size = \"128m\"\n\n}\nNote\n\nIn most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.\n\ntarget.ssh\n\nDefines SSH agent sockets or keys to expose to the build. This is the same as the --ssh flag. This can be useful if you need to access private repositories during a build.\n\ntarget \"default\" {\n\n  ssh = [{ id = \"default\" }]\n\n}\nFROM alpine\n\nRUN --mount=type=ssh \\\n\n    apk add git openssh-client \\\n\n    && install -m 0700 -d ~/.ssh \\\n\n    && ssh-keyscan github.com >> ~/.ssh/known_hosts \\\n\n    && git clone git@github.com:user/my-private-repo.git\ntarget.tags\n\nImage names and tags to use for the build target. This is the same as the --tag flag.\n\ntarget \"default\" {\n\n  tags = [\n\n    \"org/repo:latest\",\n\n    \"myregistry.azurecr.io/team/image:v1\"\n\n  ]\n\n}\ntarget.target\n\nSet the target build stage to build. This is the same as the --target flag.\n\ntarget \"default\" {\n\n  target = \"binaries\"\n\n}\ntarget.ulimits\n\nUlimits overrides the default ulimits of build's containers when using RUN instructions and are specified with a soft and hard limit as such: <type>=<soft limit>[:<hard limit>], for example:\n\ntarget \"app\" {\n\n  ulimits = [\n\n    \"nofile=1024:1024\"\n\n  ]\n\n}\nNote\n\nIf you do not provide a hard limit, the soft limit is used for both values. If no ulimits are set, they are inherited from the default ulimits set on the daemon.\n\nNote\n\nIn most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.\n\nGroup\n\nGroups allow you to invoke multiple builds (targets) at once.\n\ngroup \"default\" {\n\n  targets = [\"db\", \"webapp-dev\"]\n\n}\n\n\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n}\n\n\n\ntarget \"db\" {\n\n  dockerfile = \"Dockerfile.db\"\n\n  tags = [\"docker.io/username/db\"]\n\n}\n\nGroups take precedence over targets, if both exist with the same name. The following bake file builds the default group. Bake ignores the default target.\n\ntarget \"default\" {\n\n  dockerfile-inline = \"FROM ubuntu\"\n\n}\n\n\n\ngroup \"default\" {\n\n  targets = [\"alpine\", \"debian\"]\n\n}\n\ntarget \"alpine\" {\n\n  dockerfile-inline = \"FROM alpine\"\n\n}\n\ntarget \"debian\" {\n\n  dockerfile-inline = \"FROM debian\"\n\n}\nVariable\n\nThe HCL file format supports variable block definitions. You can use variables as build arguments in your Dockerfile, or interpolate them in attribute values in your Bake file.\n\nvariable \"TAG\" {\n\n  type = string\n\n  default = \"latest\"\n\n  description = \"Tag to use for build\"\n\n}\n\n\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:${TAG}\"]\n\n}\n\nYou can assign a default value for a variable in the Bake file, or assign a null value to it. If you assign a null value, Buildx uses the default value from the Dockerfile instead.\n\nYou can also add a description of the variable's purpose with the description field. This attribute is useful when combined with the docker buildx bake --list=variables option, providing a more informative output when listing the available variables in a Bake file.\n\nYou can override variable defaults set in the Bake file using environment variables. The following example sets the TAG variable to dev, overriding the default latest value shown in the previous example.\n\n$ TAG=dev docker buildx bake webapp-dev\n\n\nVariables can also be assigned an explicit type. If provided, it will be used to validate the default value (if set), as well as any overrides. This is particularly useful when using complex types which are intended to be overridden. The previous example could be expanded to apply an arbitrary series of tags.\n\nvariable \"TAGS\" {\n\n  default = [\"latest\"]\n\n  type = list(string)\n\n}\n\n\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [for tag in TAGS: \"docker.io/username/webapp:${tag}\"]\n\n}\n\nThis example shows how to generate three tags without changing the file or using custom functions/parsing:\n\n$ TAGS=dev,latest,2 docker buildx bake webapp-dev\n\nVariable typing\n\nThe following primitive types are available:\n\nstring\nnumber\nbool\n\nThe type is expressed like a keyword; it must be expressed as a literal:\n\nvariable \"OK\" {\n\n  type = string\n\n}\n\n\n\n# cannot be an actual string\n\nvariable \"BAD\" {\n\n  type = \"string\"\n\n}\n\n\n\n# cannot be the result of an expression\n\nvariable \"ALSO_BAD\" {\n\n  type = lower(\"string\")\n\n}\n\nSpecifying primitive types can be valuable to show intent (especially when a default is not provided), but bake will generally behave as expected without explicit typing.\n\nComplex types are expressed with \"type constructors\"; they are:\n\ntuple([<type>,...])\nlist(<type>)\nset(<type>)\nmap(<type>)\nobject({<attr>=<type>},...})\n\nThe following are examples of each of those, as well as how the (optional) default value would be expressed:\n\n# structured way to express \"1.2.3-alpha\"\n\nvariable \"MY_VERSION\" {\n\n  type = tuple([number, number, number, string])\n\n  default = [1, 2, 3, \"alpha\"]\n\n}\n\n\n\n# JDK versions used in a matrix build\n\nvariable \"JDK_VERSIONS\" {\n\n  type = list(number)\n\n  default = [11, 17, 21]\n\n}\n\n\n\n# better way to express the previous example; this will also\n\n# enforce set semantics and allow use of set-based functions\n\nvariable \"JDK_VERSIONS\" {\n\n  type = set(number)\n\n  default = [11, 17, 21]\n\n}\n\n\n\n# with the help of lookup(), translate a 'feature' to a tag\n\nvariable \"FEATURE_TO_NAME\" {\n\n  type = map(string)\n\n  default = {featureA = \"slim\", featureB = \"tiny\"}\n\n}\n\n\n\n# map a branch name to a registry location\n\nvariable \"PUSH_DESTINATION\" {\n\n  type = object({branch = string, registry = string})\n\n  default = {branch = \"main\", registry = \"prod-registry.invalid.com\"}\n\n}\n\n\n\n# make the previous example more useful with composition\n\nvariable \"PUSH_DESTINATIONS\" {\n\n  type = list(object({branch = string, registry = string}))\n\n  default = [\n\n    {branch = \"develop\", registry = \"test-registry.invalid.com\"},\n\n    {branch = \"main\", registry = \"prod-registry.invalid.com\"},\n\n  ]\n\n}\n\nNote that in each example, the default value would be valid even if typing was not present. If typing was omitted, the first three would all be considered tuple; you would be restricted to functions that operate on tuple and, for example, not be able to add elements. Similarly, the third and fourth would both be considered object, with the limits and semantics of that type. In short, in the absence of a type, any value delimited with [] is a tuple and value delimited with {} is an object. Explicit typing for complex types not only opens up the ability to use functions applicable to that specialized type, but is also a precondition for providing overrides.\n\nNote\n\nSee HCL Type Expressions page for more details.\n\nOverriding variables\n\nAs mentioned in the intro to variables, primitive types (string, number, and bool) can be overridden without typing and will generally behave as expected. (When explicit typing is not provided, a variable is assumed to be primitive when the default value lacks {} or [] delimiters; a variable with neither typing nor a default value is treated as string.) Naturally, these same overrides can be used alongside explicit typing too; they may help in edge cases where you want VAR=true to be a string, where without typing, it may be a string or a bool depending on how/where it's used. Overriding a variable with a complex type can only be done when the type is provided. This is still done via environment variables, but the values can be provided via CSV or JSON.\n\nCSV overrides\n\nThis is considered the canonical method and is well suited to interactive usage. It is assumed that list and set will be the most common complex type, as well as the most common complex type designed to be overridden. Thus, there is full CSV support for list and set (and tuple; despite being considered a structural type, it is more like a collection type in this regard).\n\nThere is limited support for map and object and no support for composite types; for these advanced cases, an alternative mechanism using JSON is available.\n\nJSON overrides\n\nOverrides can also be provided via JSON. This is the only method available for providing some complex types and may be convenient if overrides are already JSON (for example, if they come from a JSON API). It can also be used when dealing with values are difficult or impossible to specify using CSV (e.g., values containing quotes or commas). To use JSON, simply append _JSON to the variable name. In this contrived example, CSV cannot handle the second value; despite being a supported CSV type, JSON must be used:\n\nvariable \"VALS\" {\n\n  type = list(string)\n\n  default = [\"some\", \"list\"]\n\n}\n$ cat data.json\n\n[\"hello\",\"with,comma\",\"with\\\"quote\"]\n\n$ VALS_JSON=$(< data.json) docker buildx bake\n\n\n\n# CSV equivalent, though the second value cannot be expressed at all \n\n$ VALS='hello,\"with\"\"quote\"' docker buildx bake\n\n\nThis example illustrates some precedence and usage rules:\n\nvariable \"FOO\" {\n\n  type = string\n\n  default = \"foo\"\n\n}\n\n\n\nvariable \"FOO_JSON\" {\n\n  type = string\n\n  default = \"foo\"\n\n}\n\nThe variable FOO can only be overridden using CSV because FOO_JSON, which would typically used for a JSON override, is already a defined variable. Since FOO_JSON is an actual variable, setting that environment variable would be expected to a CSV value. A JSON override is possible for this variable, using environment variable FOO_JSON_JSON.\n\n# These three are all equivalent, setting variable FOO=bar\n\n$ FOO=bar docker buildx bake <...>\n\n$ FOO='bar' docker buildx bake <...>\n\n$ FOO=\"bar\" docker buildx bake <...>\n\n\n\n# Sets *only* variable FOO_JSON; FOO is untouched\n\n$ FOO_JSON=bar docker buildx bake <...>\n\n\n\n# This also sets FOO_JSON, but will fail due to not being valid JSON\n\n$ FOO_JSON_JSON=bar docker buildx bake <...>\n\n\n\n# These are all equivalent\n\n$ cat data.json\n\n\"bar\"\n\n$ FOO_JSON_JSON=$(< data.json) docker buildx bake <...>\n\n$ FOO_JSON_JSON='\"bar\"' docker buildx bake <...>\n\n$ FOO_JSON=bar docker buildx bake <...>\n\n\n\n# This results in setting two different variables, both specified as CSV (FOO=bar and FOO_JSON=\"baz\")\n\n$ FOO=bar FOO_JSON='\"baz\"' docker buildx bake <...>\n\n\n\n# These refer to the same variable with FOO_JSON_JSON having precedence and read as JSON (FOO_JSON=baz)\n\n$ FOO_JSON=bar FOO_JSON_JSON='\"baz\"' docker buildx bake <...>\n\nBuilt-in variables\n\nThe following variables are built-ins that you can use with Bake without having to define them.\n\nVariable\tDescription\nBAKE_CMD_CONTEXT\tHolds the main context when building using a remote Bake file.\nBAKE_LOCAL_PLATFORM\tReturns the current platform‚Äôs default platform specification (e.g. linux/amd64).\nUse environment variable as default\n\nYou can set a Bake variable to use the value of an environment variable as a default value:\n\nvariable \"HOME\" {\n\n  default = \"$HOME\"\n\n}\nInterpolate variables into attributes\n\nTo interpolate a variable into an attribute string value, you must use curly brackets. The following doesn't work:\n\nvariable \"HOME\" {\n\n  default = \"$HOME\"\n\n}\n\n\n\ntarget \"default\" {\n\n  ssh = [\"default=$HOME/.ssh/id_rsa\"]\n\n}\n\nWrap the variable in curly brackets where you want to insert it:\n\n  variable \"HOME\" {\n\n    default = \"$HOME\"\n\n  }\n\n\n\n  target \"default\" {\n\n-   ssh = [\"default=$HOME/.ssh/id_rsa\"]\n\n+   ssh = [\"default=${HOME}/.ssh/id_rsa\"]\n\n  }\n\n\nBefore you can interpolate a variable into an attribute, first you must declare it in the bake file, as demonstrated in the following example.\n\n$ cat docker-bake.hcl\n\ntarget \"default\" {\n\n  dockerfile-inline = \"FROM ${BASE_IMAGE}\"\n\n}\n\n$ docker buildx bake\n\n[+] Building 0.0s (0/0)\n\ndocker-bake.hcl:2\n\n--------------------\n\n   1 |     target \"default\" {\n\n   2 | >>>   dockerfile-inline = \"FROM ${BASE_IMAGE}\"\n\n   3 |     }\n\n   4 |\n\n--------------------\n\nERROR: docker-bake.hcl:2,31-41: Unknown variable; There is no variable named \"BASE_IMAGE\"., and 1 other diagnostic(s)\n\n$ cat >> docker-bake.hcl\n\n\n\nvariable \"BASE_IMAGE\" {\n\n  default = \"alpine\"\n\n}\n\n\n\n$ docker buildx bake\n\n[+] Building 0.6s (5/5) FINISHED\n\nFunction\n\nA set of general-purpose functions provided by go-cty are available for use in HCL files:\n\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    buildno = \"${add(123, 1)}\"\n\n  }\n\n}\n\nIn addition, user defined functions are also supported:\n\n# docker-bake.hcl\n\nfunction \"increment\" {\n\n  params = [number]\n\n  result = number + 1\n\n}\n\n\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    buildno = \"${increment(123)}\"\n\n  }\n\n}\nNote\n\nSee User defined HCL functions page for more details.\n\nRequest changes\n\nTable of contents\nFile format\nSyntax\nTarget\ntarget.args\ntarget.annotations\ntarget.attest\ntarget.cache-from\ntarget.cache-to\ntarget.call\ntarget.context\ntarget.contexts\ntarget.description\ntarget.dockerfile-inline\ntarget.dockerfile\ntarget.entitlements\ntarget.extra-hosts\ntarget.inherits\ntarget.labels\ntarget.matrix\ntarget.name\ntarget.network\ntarget.no-cache-filter\ntarget.no-cache\ntarget.output\ntarget.policy\ntarget.platforms\ntarget.pull\ntarget.secret\ntarget.shm-size\ntarget.ssh\ntarget.tags\ntarget.target\ntarget.ulimits\nGroup\nVariable\nVariable typing\nOverriding variables\nBuilt-in variables\nUse environment variable as default\nInterpolate variables into attributes\nFunction\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995712,
    "timestamp": "2026-02-07T06:34:33.381Z",
    "title": "Building with Bake from a Compose file | Docker Docs",
    "url": "https://docs.docker.com/build/bake/compose-file/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nBuilding with Bake from a Compose file\nBuilding with Bake from a Compose file\nCopy as Markdown\n\nBake supports the Compose file format to parse a Compose file and translate each service to a target.\n\n# compose.yaml\n\nservices:\n\n  webapp-dev:\n\n    build: &build-dev\n\n      dockerfile: Dockerfile.webapp\n\n      tags:\n\n        - docker.io/username/webapp:latest\n\n      cache_from:\n\n        - docker.io/username/webapp:cache\n\n      cache_to:\n\n        - docker.io/username/webapp:cache\n\n\n\n  webapp-release:\n\n    build:\n\n      <<: *build-dev\n\n      x-bake:\n\n        platforms:\n\n          - linux/amd64\n\n          - linux/arm64\n\n\n\n  db:\n\n    image: docker.io/username/db\n\n    build:\n\n      dockerfile: Dockerfile.db\n$ docker buildx bake --print\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"db\", \"webapp-dev\", \"webapp-release\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"db\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile.db\",\n\n      \"tags\": [\"docker.io/username/db\"]\n\n    },\n\n    \"webapp-dev\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile.webapp\",\n\n      \"tags\": [\"docker.io/username/webapp:latest\"],\n\n      \"cache-from\": [\n\n        {\n\n          \"ref\": \"docker.io/username/webapp:cache\",\n\n          \"type\": \"registry\"\n\n        }\n\n      ],\n\n      \"cache-to\": [\n\n        {\n\n          \"ref\": \"docker.io/username/webapp:cache\",\n\n          \"type\": \"registry\"\n\n        }\n\n      ]\n\n    },\n\n    \"webapp-release\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile.webapp\",\n\n      \"tags\": [\"docker.io/username/webapp:latest\"],\n\n      \"cache-from\": [\n\n        {\n\n          \"ref\": \"docker.io/username/webapp:cache\",\n\n          \"type\": \"registry\"\n\n        }\n\n      ],\n\n      \"cache-to\": [\n\n        {\n\n          \"ref\": \"docker.io/username/webapp:cache\",\n\n          \"type\": \"registry\"\n\n        }\n\n      ],\n\n      \"platforms\": [\"linux/amd64\", \"linux/arm64\"]\n\n    }\n\n  }\n\n}\n\nThe compose format has some limitations compared to the HCL format:\n\nSpecifying variables or global scope attributes is not yet supported\ninherits service field is not supported, but you can use YAML anchors to reference other services, as demonstrated in the previous example with &build-dev.\n.env file\n\nYou can declare default environment variables in an environment file named .env. This file will be loaded from the current working directory, where the command is executed and applied to compose definitions passed with -f.\n\n# compose.yaml\n\nservices:\n\n  webapp:\n\n    image: docker.io/username/webapp:${TAG:-v1.0.0}\n\n    build:\n\n      dockerfile: Dockerfile\n# .env\n\nTAG=v1.1.0\n$ docker buildx bake --print\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"webapp\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\"docker.io/username/webapp:v1.1.0\"]\n\n    }\n\n  }\n\n}\nNote\n\nSystem environment variables take precedence over environment variables in .env file.\n\nExtension field with x-bake\n\nWhere some fields are not available in the compose specification, you can use the special extension field x-bake in your compose file to evaluate extra fields:\n\n# compose.yaml\n\nservices:\n\n  addon:\n\n    image: ct-addon:bar\n\n    build:\n\n      context: .\n\n      dockerfile: ./Dockerfile\n\n      args:\n\n        CT_ECR: foo\n\n        CT_TAG: bar\n\n      x-bake:\n\n        tags:\n\n          - ct-addon:foo\n\n          - ct-addon:alp\n\n        platforms:\n\n          - linux/amd64\n\n          - linux/arm64\n\n        cache-from:\n\n          - user/app:cache\n\n          - type=local,src=path/to/cache\n\n        cache-to:\n\n          - type=local,dest=path/to/cache\n\n        pull: true\n\n\n\n  aws:\n\n    image: ct-fake-aws:bar\n\n    build:\n\n      dockerfile: ./aws.Dockerfile\n\n      args:\n\n        CT_ECR: foo\n\n        CT_TAG: bar\n\n      x-bake:\n\n        secret:\n\n          - id=mysecret,src=./secret\n\n          - id=mysecret2,src=./secret2\n\n        platforms: linux/arm64\n\n        output: type=docker\n\n        no-cache: true\n$ docker buildx bake --print\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"addon\", \"aws\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"addon\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"./Dockerfile\",\n\n      \"args\": {\n\n        \"CT_ECR\": \"foo\",\n\n        \"CT_TAG\": \"bar\"\n\n      },\n\n      \"tags\": [\"ct-addon:foo\", \"ct-addon:alp\"],\n\n      \"cache-from\": [\n\n        {\n\n          \"ref\": \"user/app:cache\",\n\n          \"type\": \"registry\"\n\n        },\n\n        {\n\n          \"src\": \"path/to/cache\",\n\n          \"type\": \"local\"\n\n        }\n\n      ],\n\n      \"cache-to\": [\n\n        {\n\n          \"dest\": \"path/to/cache\",\n\n          \"type\": \"local\"\n\n        }\n\n      ],\n\n      \"platforms\": [\"linux/amd64\", \"linux/arm64\"],\n\n      \"pull\": true\n\n    },\n\n    \"aws\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"./aws.Dockerfile\",\n\n      \"args\": {\n\n        \"CT_ECR\": \"foo\",\n\n        \"CT_TAG\": \"bar\"\n\n      },\n\n      \"tags\": [\"ct-fake-aws:bar\"],\n\n      \"secret\": [\n\n        {\n\n          \"id\": \"mysecret\",\n\n          \"src\": \"./secret\"\n\n        },\n\n        {\n\n          \"id\": \"mysecret2\",\n\n          \"src\": \"./secret2\"\n\n        }\n\n      ],\n\n      \"platforms\": [\"linux/arm64\"],\n\n      \"output\": [\n\n        {\n\n          \"type\": \"docker\"\n\n        }\n\n      ],\n\n      \"no-cache\": true\n\n    }\n\n  }\n\n}\n\nComplete list of valid fields for x-bake:\n\ncache-from\ncache-to\ncontexts\nno-cache\nno-cache-filter\noutput\nplatforms\npull\nsecret\nssh\ntags\n\nEdit this page\n\nRequest changes\n\nTable of contents\n.env file\nExtension field with x-bake\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995711,
    "timestamp": "2026-02-07T06:34:33.388Z",
    "title": "Bake standard library functions | Docker Docs",
    "url": "https://docs.docker.com/build/bake/stdlib/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nBake standard library functions\nBake standard library functions\nCopy as Markdown\nName\tDescription\nabsolute\tIf the given number is negative then returns its positive equivalent, or otherwise returns the given number unchanged.\nadd\tReturns the sum of the two given numbers.\nand\tApplies the logical AND operation to the given boolean values.\nbase64decode\tDecodes a string containing a base64 sequence.\nbase64encode\tEncodes a string to a base64 sequence.\nbasename\tReturns the last element of a path.\nbcrypt\tComputes a hash of the given string using the Blowfish cipher.\nbyteslen\tReturns the total number of bytes in the given buffer.\nbytesslice\tExtracts a subslice from the given buffer.\ncan\tTries to evaluate the expression given in its first argument.\nceil\tReturns the smallest whole number that is greater than or equal to the given value.\nchomp\tRemoves one or more newline characters from the end of the given string.\nchunklist\tSplits a single list into multiple lists where each has at most the given number of elements.\ncidrhost\tCalculates a full host IP address within a given IP network address prefix.\ncidrnetmask\tConverts an IPv4 address prefix given in CIDR notation into a subnet mask address.\ncidrsubnet\tCalculates a subnet address within a given IP network address prefix.\ncidrsubnets\tCalculates many consecutive subnet addresses at once, rather than just a single subnet extension.\ncoalesce\tReturns the first of the given arguments that isn't null, or raises an error if there are no non-null arguments.\ncoalescelist\tReturns the first of the given sequences that has a length greater than zero.\ncompact\tRemoves all empty string elements from the given list of strings.\nconcat\tConcatenates together all of the given lists or tuples into a single sequence, preserving the input order.\ncontains\tReturns true if the given value is a value in the given list, tuple, or set, or false otherwise.\nconvert\tConverts a value to a specified type constraint, using HCL's customdecode extension for type expression support.\ncsvdecode\tParses the given string as Comma Separated Values (as defined by RFC 4180) and returns a map of objects representing the table of data, using the first row as a header row to define the object attributes.\ndirname\tReturns the directory of a path.\ndistinct\tRemoves any duplicate values from the given list, preserving the order of remaining elements.\ndivide\tDivides the first given number by the second.\nelement\tReturns the element with the given index from the given list or tuple, applying the modulo operation to the given index if it's greater than the number of elements.\nequal\tReturns true if the two given values are equal, or false otherwise.\nflatten\tTransforms a list, set, or tuple value into a tuple by replacing any given elements that are themselves sequences with a flattened tuple of all of the nested elements concatenated together.\nfloor\tReturns the greatest whole number that is less than or equal to the given value.\nformat\tConstructs a string by applying formatting verbs to a series of arguments, using a similar syntax to the C function \"printf\".\nformatdate\tFormats a timestamp given in RFC 3339 syntax into another timestamp in some other machine-oriented time syntax, as described in the format string.\nformatlist\tConstructs a list of strings by applying formatting verbs to a series of arguments, using a similar syntax to the C function \"printf\".\ngreaterthan\tReturns true if and only if the second number is greater than the first.\ngreaterthanorequalto\tReturns true if and only if the second number is greater than or equal to the first.\nhasindex\tReturns true if if the given collection can be indexed with the given key without producing an error, or false otherwise.\nhomedir\tReturns the current user's home directory.\nindent\tAdds a given number of spaces after each newline character in the given string.\nindex\tReturns the element with the given key from the given collection, or raises an error if there is no such element.\nindexof\tFinds the element index for a given value in a list.\nint\tDiscards any fractional portion of the given number.\njoin\tConcatenates together the elements of all given lists with a delimiter, producing a single string.\njsondecode\tParses the given string as JSON and returns a value corresponding to what the JSON document describes.\njsonencode\tReturns a string containing a JSON representation of the given value.\nkeys\tReturns a list of the keys of the given map in lexicographical order.\nlength\tReturns the number of elements in the given collection.\nlessthan\tReturns true if and only if the second number is less than the first.\nlessthanorequalto\tReturns true if and only if the second number is less than or equal to the first.\nlog\tReturns the logarithm of the given number in the given base.\nlookup\tReturns the value of the element with the given key from the given map, or returns the default value if there is no such element.\nlower\tReturns the given string with all Unicode letters translated to their lowercase equivalents.\nmax\tReturns the numerically greatest of all of the given numbers.\nmd5\tComputes the MD5 hash of a given string and encodes it with hexadecimal digits.\nmerge\tMerges all of the elements from the given maps into a single map, or the attributes from given objects into a single object.\nmin\tReturns the numerically smallest of all of the given numbers.\nmodulo\tDivides the first given number by the second and then returns the remainder.\nmultiply\tReturns the product of the two given numbers.\nnegate\tMultiplies the given number by -1.\nnot\tApplies the logical NOT operation to the given boolean value.\nnotequal\tReturns false if the two given values are equal, or true otherwise.\nor\tApplies the logical OR operation to the given boolean values.\nparseint\tParses the given string as a number of the given base, or raises an error if the string contains invalid characters.\npow\tReturns the given number raised to the given power (exponentiation).\nrange\tReturns a list of numbers spread evenly over a particular range.\nregex\tApplies the given regular expression pattern to the given string and returns information about a single match, or raises an error if there is no match.\nregex_replace\tApplies the given regular expression pattern to the given string and replaces all matches with the given replacement string.\nregexall\tApplies the given regular expression pattern to the given string and returns a list of information about all non-overlapping matches, or an empty list if there are no matches.\nreplace\tReplaces all instances of the given substring in the given string with the given replacement string.\nreverse\tReturns the given string with all of its Unicode characters in reverse order.\nreverselist\tReturns the given list with its elements in reverse order.\nrsadecrypt\tDecrypts an RSA-encrypted ciphertext.\nsanitize\tReplaces all non-alphanumeric characters with a underscore, leaving only characters that are valid for a Bake target name.\nsemvercmp\tReturns true if version satisfies a constraint.\nsethaselement\tReturns true if the given set contains the given element, or false otherwise.\nsetintersection\tReturns the intersection of all given sets.\nsetproduct\tCalculates the cartesian product of two or more sets.\nsetsubtract\tReturns the relative complement of the two given sets.\nsetsymmetricdifference\tReturns the symmetric difference of the two given sets.\nsetunion\tReturns the union of all given sets.\nsha1\tComputes the SHA1 hash of a given string and encodes it with hexadecimal digits.\nsha256\tComputes the SHA256 hash of a given string and encodes it with hexadecimal digits.\nsha512\tComputes the SHA512 hash of a given string and encodes it with hexadecimal digits.\nsignum\tReturns 0 if the given number is zero, 1 if the given number is positive, or -1 if the given number is negative.\nslice\tExtracts a subslice of the given list or tuple value.\nsort\tApplies a lexicographic sort to the elements of the given list.\nsplit\tProduces a list of one or more strings by splitting the given string at all instances of a given separator substring.\nstrlen\tReturns the number of Unicode characters (technically: grapheme clusters) in the given string.\nsubstr\tExtracts a substring from the given string.\nsubtract\tReturns the difference between the two given numbers.\ntimeadd\tAdds the duration represented by the given duration string to the given RFC 3339 timestamp string, returning another RFC 3339 timestamp.\ntimestamp\tReturns a string representation of the current date and time.\ntitle\tReplaces one letter after each non-letter and non-digit character with its uppercase equivalent.\ntrim\tRemoves consecutive sequences of characters in \"cutset\" from the start and end of the given string.\ntrimprefix\tRemoves the given prefix from the start of the given string, if present.\ntrimspace\tRemoves any consecutive space characters (as defined by Unicode) from the start and end of the given string.\ntrimsuffix\tRemoves the given suffix from the start of the given string, if present.\ntry\tVariadic function that tries to evaluate all of is arguments in sequence until one succeeds, in which case it returns that result, or returns an error if none of them succeed.\nupper\tReturns the given string with all Unicode letters translated to their uppercase equivalents.\nurlencode\tApplies URL encoding to a given string.\nuuidv4\tGenerates and returns a Type-4 UUID in the standard hexadecimal string format.\nuuidv5\tGenerates and returns a Type-5 UUID in the standard hexadecimal string format.\nvalues\tReturns the values of elements of a given map, or the values of attributes of a given object, in lexicographic order by key or attribute name.\nzipmap\tConstructs a map from a list of keys and a corresponding list of values, which must both be of the same length.\nabsolute\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    val = \"${absolute(-42)}\" # => 42\n\n  }\n\n}\nadd\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${add(123, 1)}\" # => 124\n\n  }\n\n}\nand\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${and(true, false)}\" # => false\n\n  }\n\n}\nbase64decode\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    decoded = \"${base64decode(\"SGVsbG8=\")}\" # => \"Hello\"\n\n  }\n\n}\nbase64encode\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    encoded = \"${base64encode(\"Hello\")}\" # => \"SGVsbG8=\"\n\n  }\n\n}\nbasename\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    file = \"${basename(\"/usr/local/bin/docker\")}\" # => \"docker\"\n\n  }\n\n}\nbcrypt\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    hash = \"${bcrypt(\"mypassword\")}\" # => \"$2a$10$...\"\n\n  }\n\n}\nbyteslen\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    length = \"${byteslen(\"Docker\")}\" # => 6\n\n  }\n\n}\nbytesslice\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    part = \"${bytesslice(\"Docker\", 0, 3)}\" # => \"Doc\"\n\n  }\n\n}\ncan\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    safe = \"${can(parseint(\"123\", 10))}\" # => true\n\n  }\n\n}\nceil\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    rounded = \"${ceil(3.14)}\" # => 4\"\n\n  }\n\n}\nchomp\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${chomp(\"Hello\\n\\n\")}\" # => \"Hello\"\n\n  }\n\n}\nchunklist\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${chunklist([1,2,3,4,5], 2)}\"     # => [[1,2],[3,4],[5]]\n\n  }\n\n}\ncidrhost\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${cidrhost(\"10.0.0.0/16\", 5)}\"   # => \"10.0.0.5\"\n\n  }\n\n}\ncidrnetmask\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    mask = \"${cidrnetmask(\"10.0.0.0/16\")}\"     # => \"255.255.0.0\"\n\n  }\n\n}\ncidrsubnet\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    subnet = \"${cidrsubnet(\"10.0.0.0/16\", 4, 2)}\" # => \"10.0.32.0/20\"\n\n  }\n\n}\ncidrsubnets\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    subs = \"${cidrsubnets(\"10.0.0.0/16\", 4, 4)}\" # => [\"10.0.0.0/20\",\"10.0.16.0/20\",...]\n\n  }\n\n}\ncoalesce\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    first = \"${coalesce(null, \"\", \"docker\")}\"  # => \"docker\"\n\n  }\n\n}\ncoalescelist\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    first = \"${coalescelist([], [1,2], [3])}\" # => [1,2]\n\n  }\n\n}\ncompact\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    list = \"${compact([\"a\", \"\", \"b\", \"\"])}\" # => [\"a\",\"b\"]\n\n  }\n\n}\nconcat\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    list = \"${concat([1,2],[3,4])}\" # => [1,2,3,4]\n\n  }\n\n}\ncontains\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    check = \"${contains([1,2,3], 2)}\" # => true\n\n  }\n\n}\nconvert\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${convert(\"42\", number)}\" # => 42\n\n  }\n\n}\ncsvdecode\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    data = \"${csvdecode(\"name,age\\nAlice,30\\nBob,40\")}\" # => [{\"name\":\"Alice\",\"age\":\"30\"},{\"name\":\"Bob\",\"age\":\"40\"}]\n\n  }\n\n}\ndirname\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    dir = \"${dirname(\"/usr/local/bin/docker\")}\" # => \"/usr/local/bin\"\n\n  }\n\n}\ndistinct\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${distinct([1,2,2,3,3,3])}\" # => [1,2,3]\n\n  }\n\n}\ndivide\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${divide(10, 2)}\" # => 5\n\n  }\n\n}\nelement\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    val = \"${element([\"a\",\"b\",\"c\"], 1)}\" # => \"b\"\n\n  }\n\n}\nequal\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    check = \"${equal(2, 2)}\" # => true\n\n  }\n\n}\nflatten\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    flat = \"${flatten([[1,2],[3,4],[5]])}\" # => [1,2,3,4,5]\n\n  }\n\n}\nfloor\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${floor(3.99)}\" # => 3\n\n  }\n\n}\nformat\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${format(\"Hello, %s!\", \"World\")}\" # => \"Hello, World!\"\n\n  }\n\n}\nformatdate\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    date = \"${formatdate(\"YYYY-MM-DD\", \"2025-09-16T12:00:00Z\")}\" # => \"2025-09-16\"\n\n  }\n\n}\nformatlist\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    list = \"${formatlist(\"Hi %s\", [\"Alice\", \"Bob\"])}\" # => [\"Hi Alice\",\"Hi Bob\"]\n\n  }\n\n}\ngreaterthan\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${greaterthan(2, 5)}\" # => true\n\n  }\n\n}\ngreaterthanorequalto\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${greaterthanorequalto(5, 5)}\" # => true\n\n  }\n\n}\nhasindex\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    exists = \"${hasindex([10, 20, 30], 1)}\"  # => true\n\n    missing = \"${hasindex([10, 20, 30], 5)}\" # => false\n\n  }\n\n}\nhomedir\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    home = \"${homedir()}\" # => e.g., \"/home/user\"\n\n  }\n\n}\nindent\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    text = \"${indent(4, \"Hello\\nWorld\")}\" \n\n    # => \"    Hello\\n    World\"\n\n  }\n\n}\nindex\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    val = \"${index({foo = \"bar\", baz = \"qux\"}, \"baz\")}\" # => \"qux\"\n\n  }\n\n}\nindexof\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    pos = \"${indexof([\"a\",\"b\",\"c\"], \"b\")}\" # => 1\n\n  }\n\n}\nint\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    number = \"${int(3.75)}\" # => 3\n\n  }\n\n}\njoin\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    csv = \"${join(\",\", [\"a\",\"b\",\"c\"])}\" # => \"a,b,c\"\n\n  }\n\n}\njsondecode\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    obj = \"${jsondecode(\"{\\\"name\\\":\\\"Docker\\\",\\\"stars\\\":5}\")}\" # => {\"name\":\"Docker\",\"stars\":5}\n\n  }\n\n}\njsonencode\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    str = \"${jsonencode({name=\"Docker\", stars=5})}\" # => \"{\\\"name\\\":\\\"Docker\\\",\\\"stars\\\":5}\"\n\n  }\n\n}\nkeys\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    list = \"${keys({foo = 1, bar = 2, baz = 3})}\" \n\n    # => [\"bar\",\"baz\",\"foo\"] (sorted order)\n\n  }\n\n}\nlength\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    size = \"${length([1,2,3,4])}\" # => 4\n\n  }\n\n}\nlessthan\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${lessthan(10, 3)}\" # => false\n\n  }\n\n}\nlessthanorequalto\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${lessthanorequalto(5, 7)}\" # => true\n\n  }\n\n}\nlog\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    val = \"${log(100, 10)}\" # => 2\n\n  }\n\n}\nlookup\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    found    = \"${lookup({a=\"apple\", b=\"banana\"}, \"a\", \"none\")}\" # => \"apple\"\n\n    fallback = \"${lookup({a=\"apple\", b=\"banana\"}, \"c\", \"none\")}\" # => \"none\"\n\n  }\n\n}\nlower\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    val = \"${lower(\"HELLO\")}\" # => \"hello\"\n\n  }\n\n}\nmax\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${max(3, 9, 7)}\" # => 9\n\n  }\n\n}\nmd5\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    hash = \"${md5(\"docker\")}\" # => \"597dd5f6a...\" (hex string)\n\n  }\n\n}\nmerge\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    combined = \"${merge({a=1, b=2}, {b=3, c=4})}\" # => {a=1, b=3, c=4}\n\n  }\n\n}\nmin\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${min(3, 9, 7)}\" # => 3\n\n  }\n\n}\nmodulo\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${modulo(10, 3)}\" # => 1\n\n  }\n\n}\nmultiply\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${multiply(6, 7)}\" # => 42\n\n  }\n\n}\nnegate\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${negate(7)}\" # => -7\n\n  }\n\n}\nnot\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${not(true)}\" # => false\n\n  }\n\n}\nnotequal\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${notequal(4, 5)}\" # => true\n\n  }\n\n}\nor\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${or(true, false)}\" # => true\n\n  }\n\n}\nparseint\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${parseint(\"ff\", 16)}\" # => 255\n\n  }\n\n}\npow\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${pow(3, 2)}\" # => 9\n\n  }\n\n}\nrange\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${range(0, 5)}\" # => [0,1,2,3,4]\n\n  }\n\n}\nregex\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${regex(\"h.llo\", \"hello\")}\" # => \"hello\"\n\n  }\n\n}\nregex_replace\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${regex_replace(\"[0-9]+\", \"abc123xyz\", \"NUM\")}\" # => \"abcNUMxyz\"\n\n  }\n\n}\nregexall\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = matches = \"${regexall(\"[a-z]+\", \"abc123xyz\")}\" # => [\"abc\",\"xyz\"]\n\n  }\n\n}\nreplace\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${replace(\"banana\", \"na\", \"--\")}\" # => \"ba-- --\"\n\n  }\n\n}\nreverse\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${reverse(\"stressed\")}\" # => \"desserts\"\n\n  }\n\n}\nreverselist\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${reverselist([1,2,3])}\" # => [3,2,1]\n\n  }\n\n}\nrsadecrypt\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${rsadecrypt(\"eczGaDhXDbOFRZ...\", \"MIIEowIBAAKCAQEAgUElV5...\")}\"\n\n  }\n\n}\nsanitize\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${sanitize(\"My App! v1.0\")}\" # => \"My_App__v1_0\"\n\n  }\n\n}\nsemvercmp\n\nThis function checks if a semantic version fits within a set of constraints. See Checking Version Constraints for details.\n\n# docker-bake.hcl\n\nvariable \"ALPINE_VERSION\" {\n\n  default = \"3.23\"\n\n}\n\n\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  platforms = semvercmp(ALPINE_VERSION, \">= 3.20\") ? [\n\n    \"linux/amd64\",\n\n    \"linux/arm64\",\n\n    \"linux/riscv64\"\n\n  ] : [\n\n    \"linux/amd64\",\n\n    \"linux/arm64\"\n\n  ]\n\n}\nsethaselement\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${sethaselement([1,2,3], 2)}\"  # => true\n\n  }\n\n}\nsetintersection\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${setintersection([\"a\",\"b\",\"c\"], [\"b\",\"c\",\"d\"])}\" # => [\"b\",\"c\"]\n\n  }\n\n}\nsetproduct\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${setproduct([\"x\",\"y\"], [1,2])}\" # => [[\"x\",1],[\"x\",2],[\"y\",1],[\"y\",2]]\n\n  }\n\n}\nsetsubtract\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${setsubtract([1,2,3], [2])}\" # => [1,3]\n\n  }\n\n}\nsetsymmetricdifference\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${setsymmetricdifference([1,2,3], [3,4])}\" # => [1,2,4]\n\n  }\n\n}\nsetunion\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${setunion([\"a\",\"b\"], [\"b\",\"c\"])}\" # => [\"a\",\"b\",\"c\"]\n\n  }\n\n}\nsha1\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${sha1(\"hello\")}\" # => \"aaf4c61d...\" (hex)\n\n  }\n\n}\nsha256\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${sha256(\"hello\")}\" # => \"2cf24dba...\" (hex)\n\n  }\n\n}\nsha512\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${sha512(\"hello\")}\" # => \"9b71d224...\" (hex)\n\n  }\n\n}\nsignum\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    zero = \"${signum(0)}\"    # => 0\n\n    pos  = \"${signum(12)}\"   # => 1\n\n    neg  = \"${signum(-12)}\"  # => -1\n\n  }\n\n}\nslice\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${slice([\"a\",\"b\",\"c\",\"d\"], 1, 3)}\" # => [\"b\",\"c\"]\n\n  }\n\n}\nsort\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${sort([\"b\",\"c\",\"a\"])}\" # => [\"a\",\"b\",\"c\"]\n\n  }\n\n}\nsplit\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${split(\",\", \"a,b,c\")}\" # => [\"a\",\"b\",\"c\"]\n\n  }\n\n}\nstrlen\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${strlen(\"Docker\")}\" # => 6\n\n  }\n\n}\nsubstr\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${substr(\"abcdef\", 2, 3)}\" # => \"cde\"\n\n  }\n\n}\nsubtract\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${subtract(10, 3)}\" # => 7\n\n  }\n\n}\ntimeadd\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${timeadd(\"2025-09-24T12:00:00Z\", \"1h30m\")}\" # => \"2025-09-24T13:30:00Z\"\n\n  }\n\n}\ntimestamp\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${timestamp()}\" # => current RFC3339 time at evaluation\n\n  }\n\n}\ntitle\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${title(\"hello world-from_docker\")}\" # => \"Hello World-From_Docker\"\n\n  }\n\n}\ntrim\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${trim(\"--hello--\", \"-\")}\" # => \"hello\"\n\n  }\n\n}\ntrimprefix\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${trimprefix(\"docker-build\", \"docker-\")}\" # => \"build\"\n\n  }\n\n}\ntrimspace\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${trimspace(\"   hello   \")}\" # => \"hello\"\n\n  }\n\n}\ntrimsuffix\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${trimsuffix(\"filename.txt\", \".txt\")}\" # => \"filename\"\n\n  }\n\n}\ntry\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    # First expr errors (invalid hex), second succeeds ‚Üí returns 255\n\n    val1 = \"${try(parseint(\"zz\", 16), parseint(\"ff\", 16))}\" # => 255\n\n\n\n    # First expr errors (missing key), fallback string is used\n\n    val2 = \"${try(index({a=\"apple\"}, \"b\"), \"fallback\")}\"    # => \"fallback\"\n\n  }\n\n}\nupper\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    result = \"${upper(\"hello\")}\" # => \"HELLO\"\n\n  }\n\n}\nurlencode\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    url = \"${urlencode(\"a b=c&d\")}\" # => \"a+b%3Dc%26d\"\n\n  }\n\n}\nuuidv4\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    id = \"${uuidv4()}\" # => random v4 UUID each evaluation\n\n  }\n\n}\nuuidv5\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    # Uses the DNS namespace UUID per RFC 4122\n\n    # \"6ba7b810-9dad-11d1-80b4-00c04fd430c8\"\n\n    id = \"${uuidv5(\"6ba7b810-9dad-11d1-80b4-00c04fd430c8\", \"example.com\")}\"\n\n    # => always \"9073926b-929f-31c2-abc9-fad77ae3e8eb\" for \"example.com\"\n\n  }\n\n}\nvalues\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    vals = \"${values({a=1, c=3, b=2})}\" # => [1,2,3] (ordered by key: a,b,c)\n\n  }\n\n}\nzipmap\n# docker-bake.hcl\n\ntarget \"webapp-dev\" {\n\n  dockerfile = \"Dockerfile.webapp\"\n\n  tags = [\"docker.io/username/webapp:latest\"]\n\n  args = {\n\n    obj = \"${zipmap([\"name\",\"stars\"], [\"Docker\", 5])}\" # => {name=\"Docker\", stars=5}\n\n  }\n\n}\n\nRequest changes\n\nTable of contents\nabsolute\nadd\nand\nbase64decode\nbase64encode\nbasename\nbcrypt\nbyteslen\nbytesslice\ncan\nceil\nchomp\nchunklist\ncidrhost\ncidrnetmask\ncidrsubnet\ncidrsubnets\ncoalesce\ncoalescelist\ncompact\nconcat\ncontains\nconvert\ncsvdecode\ndirname\ndistinct\ndivide\nelement\nequal\nflatten\nfloor\nformat\nformatdate\nformatlist\ngreaterthan\ngreaterthanorequalto\nhasindex\nhomedir\nindent\nindex\nindexof\nint\njoin\njsondecode\njsonencode\nkeys\nlength\nlessthan\nlessthanorequalto\nlog\nlookup\nlower\nmax\nmd5\nmerge\nmin\nmodulo\nmultiply\nnegate\nnot\nnotequal\nor\nparseint\npow\nrange\nregex\nregex_replace\nregexall\nreplace\nreverse\nreverselist\nrsadecrypt\nsanitize\nsemvercmp\nsethaselement\nsetintersection\nsetproduct\nsetsubtract\nsetsymmetricdifference\nsetunion\nsha1\nsha256\nsha512\nsignum\nslice\nsort\nsplit\nstrlen\nsubstr\nsubtract\ntimeadd\ntimestamp\ntitle\ntrim\ntrimprefix\ntrimspace\ntrimsuffix\ntry\nupper\nurlencode\nuuidv4\nuuidv5\nvalues\nzipmap\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995717,
    "timestamp": "2026-02-07T06:34:33.388Z",
    "title": "Overriding configurations | Docker Docs",
    "url": "https://docs.docker.com/build/bake/overrides/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nOverriding configurations\nOverriding configurations\nCopy as Markdown\n\nBake supports loading build definitions from files, but sometimes you need even more flexibility to configure these definitions. For example, you might want to override an attribute when building in a particular environment or for a specific target.\n\nThe following list of attributes can be overridden:\n\nargs\nattest\ncache-from\ncache-to\ncontext\ncontexts\ndockerfile\nentitlements\nlabels\nnetwork\nno-cache\noutput\nplatform\npull\nsecrets\nssh\ntags\ntarget\n\nTo override these attributes, you can use the following methods:\n\nFile overrides\nCLI overrides\nEnvironment variable overrides\nFile overrides\n\nYou can load multiple Bake files that define build configurations for your targets. This is useful when you want to separate configurations into different files for better organization, or to conditionally override configurations based on which files are loaded.\n\nDefault file lookup\n\nYou can use the --file or -f flag to specify which files to load. If you don't specify any files, Bake will use the following lookup order:\n\ncompose.yaml\ncompose.yml\ndocker-compose.yml\ndocker-compose.yaml\ndocker-bake.json\ndocker-bake.hcl\ndocker-bake.override.json\ndocker-bake.override.hcl\n\nIf more than one Bake file is found, all files are loaded and merged into a single definition. Files are merged according to the lookup order.\n\n$ docker buildx bake --print\n\n[+] Building 0.0s (1/1) FINISHED                                                                                                                                                                                            \n\n => [internal] load local bake definitions                                                                                                                                                                             0.0s\n\n => => reading compose.yaml 45B / 45B                                                                                                                                                                                  0.0s\n\n => => reading docker-bake.hcl 113B / 113B                                                                                                                                                                             0.0s\n\n => => reading docker-bake.override.hcl 65B / 65B\n\n\nIf merged files contain duplicate attribute definitions, those definitions are either merged or overridden by the last occurrence, depending on the attribute.\n\nBake will attempt to load all of the files in the order they are found. If multiple files define the same target, attributes are either merged or overridden. In the case of overrides, the last one loaded takes precedence.\n\nFor example, given the following files:\n\ndocker-bake.hcl\nvariable \"TAG\" {\n\n  default = \"foo\"\n\n}\n\n\n\ntarget \"default\" {\n\n  tags = [\"username/my-app:${TAG}\"]\n\n}\ndocker-bake.override.hcl\nvariable \"TAG\" {\n\n  default = \"bar\"\n\n}\n\nSince docker-bake.override.hcl is loaded last in the default lookup order, the TAG variable is overridden with the value bar.\n\n$ docker buildx bake --print\n\n{\n\n  \"target\": {\n\n    \"default\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\"username/my-app:bar\"]\n\n    }\n\n  }\n\n}\n\nManual file overrides\n\nYou can use the --file flag to explicitly specify which files to load, and use this as a way to conditionally apply override files.\n\nFor example, you can create a file that defines a set of configurations for a specific environment, and load it only when building for that environment. The following example shows how to load an override.hcl file that sets the TAG variable to bar. The TAG variable is then used in the default target.\n\ndocker-bake.hcl\nvariable \"TAG\" {\n\n  default = \"foo\"\n\n}\n\n\n\ntarget \"default\" {\n\n  tags = [\"username/my-app:${TAG}\"]\n\n}\noverrides.hcl\nvariable \"TAG\" {\n\n  default = \"bar\"\n\n}\n\nPrinting the build configuration without the --file flag shows the TAG variable is set to the default value foo.\n\n$ docker buildx bake --print\n\n{\n\n  \"target\": {\n\n    \"default\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\n\n        \"username/my-app:foo\"\n\n      ]\n\n    }\n\n  }\n\n}\n\n\nUsing the --file flag to load the overrides.hcl file overrides the TAG variable with the value bar.\n\n$ docker buildx bake -f docker-bake.hcl -f overrides.hcl --print\n\n{\n\n  \"target\": {\n\n    \"default\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\n\n        \"username/my-app:bar\"\n\n      ]\n\n    }\n\n  }\n\n}\n\nCommand line\n\nYou can also override target configurations from the command line with the --set flag:\n\n# docker-bake.hcl\n\ntarget \"app\" {\n\n  args = {\n\n    mybuildarg = \"foo\"\n\n  }\n\n}\n$ docker buildx bake --set app.args.mybuildarg=bar --set app.platform=linux/arm64 app --print\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"app\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"app\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"args\": {\n\n        \"mybuildarg\": \"bar\"\n\n      },\n\n      \"platforms\": [\"linux/arm64\"]\n\n    }\n\n  }\n\n}\nNote\n\n--set is a repeatable flag. For array fields such as tags, repeat --set to provide multiple values or use the += operator to append without replacing. Array literal syntax like --set target.tags=[a,b] is not supported.\n\nPattern matching syntax defined in https://golang.org/pkg/path/#Match is also supported:\n\n$ docker buildx bake --set foo*.args.mybuildarg=value  # overrides build arg for all targets starting with \"foo\"\n\n$ docker buildx bake --set *.platform=linux/arm64      # overrides platform for all targets\n\n$ docker buildx bake --set foo*.no-cache               # bypass caching only for targets starting with \"foo\"\n\n\nComplete list of attributes that can be overridden with --set are:\n\nargs\nattest\ncache-from\ncache-to\ncontext\ncontexts\ndockerfile\nentitlements\nlabels\nnetwork\nno-cache\noutput\nplatform\npull\nsecrets\nssh\ntags\ntarget\nEnvironment variables\n\nYou can also use environment variables to override configurations.\n\nBake lets you use environment variables to override the value of a variable block. Only variable blocks can be overridden with environment variables. This means you need to define the variables in the bake file and then set the environment variable with the same name to override it.\n\nThe following example shows how you can define a TAG variable with a default value in the Bake file, and override it with an environment variable.\n\nvariable \"TAG\" {\n\n  default = \"latest\"\n\n}\n\n\n\ntarget \"default\" {\n\n  context = \".\"\n\n  dockerfile = \"Dockerfile\"\n\n  tags = [\"docker.io/username/webapp:${TAG}\"]\n\n}\n$ export TAG=$(git rev-parse --short HEAD)\n\n$ docker buildx bake --print webapp\n\n\nThe TAG variable is overridden with the value of the environment variable, which is the short commit hash generated by git rev-parse --short HEAD.\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"webapp\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"webapp\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"tags\": [\"docker.io/username/webapp:985e9e9\"]\n\n    }\n\n  }\n\n}\nType coercion\n\nOverriding non-string variables with environment variables is supported. Values passed as environment variables are coerced into suitable types first.\n\nThe following example defines a PORT variable. The backend target uses the PORT variable as-is, and the frontend target uses the value of PORT incremented by one.\n\nvariable \"PORT\" {\n\n  default = 3000\n\n}\n\n\n\ngroup \"default\" {\n\n  targets = [\"backend\", \"frontend\"]\n\n}\n\n\n\ntarget \"backend\" {\n\n  args = {\n\n    PORT = PORT\n\n  }\n\n}\n\n\n\ntarget \"frontend\" {\n\n  args = {\n\n    PORT = add(PORT, 1)\n\n  }\n\n}\n\nOverriding PORT using an environment variable will first coerce the value into the expected type, an integer, before the expression in the frontend target runs.\n\n$ PORT=7070 docker buildx bake --print\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\n\n        \"backend\",\n\n        \"frontend\"\n\n      ]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"backend\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"args\": {\n\n        \"PORT\": \"7070\"\n\n      }\n\n    },\n\n    \"frontend\": {\n\n      \"context\": \".\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"args\": {\n\n        \"PORT\": \"7071\"\n\n      }\n\n    }\n\n  }\n\n}\n\nEdit this page\n\nRequest changes\n\nTable of contents\nFile overrides\nDefault file lookup\nManual file overrides\nCommand line\nEnvironment variables\nType coercion\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995720,
    "timestamp": "2026-02-07T06:34:33.393Z",
    "title": "Remote Bake file definition | Docker Docs",
    "url": "https://docs.docker.com/build/bake/remote-definition/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nIntroduction\nTargets\nInheritance\nVariables\nExpressions\nFunctions\nMatrix targets\nContexts\nBake file reference\nBake standard library functions\nBuilding with Bake from a Compose file\nOverriding configurations\nRemote Bake file definition\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nBake\n/\nRemote Bake file definition\nRemote Bake file definition\nCopy as Markdown\n\nYou can build Bake files directly from a remote Git repository or HTTPS URL:\n\n$ docker buildx bake \"https://github.com/docker/cli.git#v20.10.11\" --print\n\n#1 [internal] load git source https://github.com/docker/cli.git#v20.10.11\n\n#1 0.745 e8f1871b077b64bcb4a13334b7146492773769f7       refs/tags/v20.10.11\n\n#1 2.022 From https://github.com/docker/cli\n\n#1 2.022  * [new tag]         v20.10.11  -> v20.10.11\n\n#1 DONE 2.9s\n\n\nThis fetches the Bake definition from the specified remote location and executes the groups or targets defined in that file. If the remote Bake definition doesn't specify a build context, the context is automatically set to the Git remote. For example, this case uses https://github.com/docker/cli.git:\n\n{\n\n  \"group\": {\n\n    \"default\": {\n\n      \"targets\": [\"binary\"]\n\n    }\n\n  },\n\n  \"target\": {\n\n    \"binary\": {\n\n      \"context\": \"https://github.com/docker/cli.git#v20.10.11\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"args\": {\n\n        \"BASE_VARIANT\": \"alpine\",\n\n        \"GO_STRIP\": \"\",\n\n        \"VERSION\": \"\"\n\n      },\n\n      \"target\": \"binary\",\n\n      \"platforms\": [\"local\"],\n\n      \"output\": [\"build\"]\n\n    }\n\n  }\n\n}\nUse the local context with a remote definition\n\nWhen building with a remote Bake definition, you may want to consume local files relative to the directory where the Bake command is executed. You can define contexts as relative to the command context using a cwd:// prefix.\n\nhttps://github.com/dvdksn/buildx/blob/bake-remote-example/docker-bake.hcl\ntarget \"default\" {\n\n  context = \"cwd://\"\n\n  dockerfile-inline = <<EOT\n\nFROM alpine\n\nWORKDIR /src\n\nCOPY . .\n\nRUN ls -l && stop\n\nEOT\n\n}\n$ touch foo bar\n\n$ docker buildx bake \"https://github.com/dvdksn/buildx.git#bake-remote-example\"\n\n...\n\n > [4/4] RUN ls -l && stop:\n\n#8 0.101 total 0\n\n#8 0.102 -rw-r--r--    1 root     root             0 Jul 27 18:47 bar\n\n#8 0.102 -rw-r--r--    1 root     root             0 Jul 27 18:47 foo\n\n#8 0.102 /bin/sh: stop: not found\n\nYou can append a path to the cwd:// prefix if you want to use a specific local directory as a context. Note that if you do specify a path, it must be within the working directory where the command gets executed. If you use an absolute path, or a relative path leading outside of the working directory, Bake will throw an error.\n\nLocal named contexts\n\nYou can also use the cwd:// prefix to define local directories in the Bake execution context as named contexts.\n\nThe following example defines the docs context as ./src/docs/content, relative to the current working directory where Bake is run as a named context.\n\ndocker-bake.hcl\ntarget \"default\" {\n\n  contexts = {\n\n    docs = \"cwd://src/docs/content\"\n\n  }\n\n  dockerfile = \"Dockerfile\"\n\n}\n\nBy contrast, if you omit the cwd:// prefix, the path would be resolved relative to the build context.\n\nSpecify the Bake definition to use\n\nWhen loading a Bake file from a remote Git repository, if the repository contains more than one Bake file, you can specify which Bake definition to use with the --file or -f flag:\n\ndocker buildx bake -f bake.hcl \"https://github.com/crazy-max/buildx.git#remote-with-local\"\n\n...\n\n#4 [2/2] RUN echo \"hello world\"\n\n#4 0.270 hello world\n\n#4 DONE 0.3s\nCombine local and remote Bake definitions\n\nYou can also combine remote definitions with local ones using the cwd:// prefix with -f.\n\nGiven the following local Bake definition in the current working directory:\n\n# local.hcl\n\ntarget \"default\" {\n\n  args = {\n\n    HELLO = \"foo\"\n\n  }\n\n}\n\nThe following example uses -f to specify two Bake definitions:\n\n-f bake.hcl: this definition is loaded relative to the Git URL.\n-f cwd://local.hcl: this definition is loaded relative to the current working directory where the Bake command is executed.\ndocker buildx bake -f bake.hcl -f cwd://local.hcl \"https://github.com/crazy-max/buildx.git#remote-with-local\" --print\n\n{\n\n  \"target\": {\n\n    \"default\": {\n\n      \"context\": \"https://github.com/crazy-max/buildx.git#remote-with-local\",\n\n      \"dockerfile\": \"Dockerfile\",\n\n      \"args\": {\n\n        \"HELLO\": \"foo\"\n\n      },\n\n      \"target\": \"build\",\n\n      \"output\": [\n\n        {\n\n          \"type\": \"cacheonly\"\n\n        }\n\n      ]\n\n    }\n\n  }\n\n}\n\nOne case where combining local and remote Bake definitions becomes necessary is when you're building with a remote Bake definition in GitHub Actions and want to use the metadata-action to generate tags, annotations, or labels. The metadata action generates a Bake file available in the runner's local Bake execution context. To use both the remote definition and the local \"metadata-only\" Bake file, specify both files and use the cwd:// prefix for the metadata Bake file:\n\n      - name: Build\n\n        uses: docker/bake-action@v6\n\n        with:\n\n          files: |\n\n            ./docker-bake.hcl\n\n            cwd://${{ steps.meta.outputs.bake-file }}\n\n          targets: build\nRemote definition in a private repository\n\nIf you want to use a remote definition that lives in a private repository, you may need to specify credentials for Bake to use when fetching the definition.\n\nIf you can authenticate to the private repository using the default SSH_AUTH_SOCK, then you don't need to specify any additional authentication parameters for Bake. Bake automatically uses your default agent socket.\n\nFor authentication using an HTTP token, or custom SSH agents, use the following environment variables to configure Bake's authentication strategy:\n\nBUILDX_BAKE_GIT_AUTH_TOKEN\nBUILDX_BAKE_GIT_AUTH_HEADER\nBUILDX_BAKE_GIT_SSH\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUse the local context with a remote definition\nLocal named contexts\nSpecify the Bake definition to use\nCombine local and remote Bake definitions\nRemote definition in a private repository\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995726,
    "timestamp": "2026-02-07T06:34:33.397Z",
    "title": "Build cache invalidation | Docker Docs",
    "url": "https://docs.docker.com/build/cache/invalidation/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nBuild cache invalidation\nBuild cache invalidation\nCopy as Markdown\n\nWhen building an image, Docker steps through the instructions in your Dockerfile, executing each in the order specified. For each instruction, the builder checks whether it can reuse the instruction from the build cache.\n\nGeneral rules\n\nThe basic rules of build cache invalidation are as follows:\n\nThe builder begins by checking if the base image is already cached. Each subsequent instruction is compared against the cached layers. If no cached layer matches the instruction exactly, the cache is invalidated.\n\nIn most cases, comparing the Dockerfile instruction with the corresponding cached layer is sufficient. However, some instructions require additional checks and explanations.\n\nFor the ADD and COPY instructions, and for RUN instructions with bind mounts (RUN --mount=type=bind), the builder calculates a cache checksum from file metadata to determine whether cache is valid. During cache lookup, cache is invalidated if the file metadata has changed for any of the files involved.\n\nThe modification time of a file (mtime) is not taken into account when calculating the cache checksum. If only the mtime of the copied files have changed, the cache is not invalidated.\n\nAside from the ADD and COPY commands, cache checking doesn't look at the files in the container to determine a cache match. For example, when processing a RUN apt-get -y update command the files updated in the container aren't examined to determine if a cache hit exists. In that case just the command string itself is used to find a match.\n\nOnce the cache is invalidated, all subsequent Dockerfile commands generate new images and the cache isn't used.\n\nIf your build contains several layers and you want to ensure the build cache is reusable, order the instructions from less frequently changed to more frequently changed where possible.\n\nWORKDIR and SOURCE_DATE_EPOCH\n\nThe WORKDIR instruction respects the SOURCE_DATE_EPOCH build argument when determining cache validity. Changing SOURCE_DATE_EPOCH between builds invalidates the cache for WORKDIR and all subsequent instructions.\n\nSOURCE_DATE_EPOCH sets timestamps for files created during the build. If you set this to a dynamic value like a Git commit timestamp, the cache breaks with each commit. This is expected behavior when tracking build provenance.\n\nFor reproducible builds without frequent cache invalidation, use a fixed timestamp:\n\n$ docker build --build-arg SOURCE_DATE_EPOCH=0 .\n\nRUN instructions\n\nThe cache for RUN instructions isn't invalidated automatically between builds. Suppose you have a step in your Dockerfile to install curl:\n\nFROM alpine:3.23 AS install\n\nRUN apk add curl\n\nThis doesn't mean that the version of curl in your image is always up-to-date. Rebuilding the image one week later will still get you the same packages as before. To force a re-execution of the RUN instruction, you can:\n\nMake sure that a layer before it has changed\nClear the build cache ahead of the build using docker builder prune\nUse the --no-cache or --no-cache-filter options\n\nThe --no-cache-filter option lets you specify a specific build stage to invalidate the cache for:\n\n$ docker build --no-cache-filter install .\n\nBuild secrets\n\nThe contents of build secrets are not part of the build cache. Changing the value of a secret doesn't result in cache invalidation.\n\nIf you want to force cache invalidation after changing a secret value, you can pass a build argument with an arbitrary value that you also change when changing the secret. Build arguments do result in cache invalidation.\n\nFROM alpine\n\nARG CACHEBUST\n\nRUN --mount=type=secret,id=TOKEN,env=TOKEN \\\n\n    some-command ...\n$ TOKEN=\"tkn_pat123456\" docker build --secret id=TOKEN --build-arg CACHEBUST=1 .\n\n\nProperties of secrets such as IDs and mount paths do participate in the cache checksum, and result in cache invalidation if changed.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nGeneral rules\nWORKDIR and SOURCE_DATE_EPOCH\nRUN instructions\nBuild secrets\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995723,
    "timestamp": "2026-02-07T06:34:33.397Z",
    "title": "Cache | Docker Docs",
    "url": "https://docs.docker.com/build/cache/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\nDocker build cache\nCopy as Markdown\n\nWhen you build the same Docker image multiple times, knowing how to optimize the build cache is a great tool for making sure the builds run fast.\n\nHow the build cache works\n\nUnderstanding Docker's build cache helps you write better Dockerfiles that result in faster builds.\n\nThe following example shows a small Dockerfile for a program written in C.\n\n# syntax=docker/dockerfile:1\n\nFROM ubuntu:latest\n\n\n\nRUN apt-get update && apt-get install -y build-essentials\n\nCOPY main.c Makefile /src/\n\nWORKDIR /src/\n\nRUN make build\n\nEach instruction in this Dockerfile translates to a layer in your final image. You can think of image layers as a stack, with each layer adding more content on top of the layers that came before it:\n\nWhenever a layer changes, that layer will need to be re-built. For example, suppose you make a change to your program in the main.c file. After this change, the COPY command will have to run again in order for those changes to appear in the image. In other words, Docker will invalidate the cache for this layer.\n\nIf a layer changes, all other layers that come after it are also affected. When the layer with the COPY command gets invalidated, all layers that follow will need to run again, too:\n\nAnd that's the Docker build cache in a nutshell. Once a layer changes, then all downstream layers need to be rebuilt as well. Even if they wouldn't build anything differently, they still need to re-run.\n\nOther resources\n\nFor more information on using cache to do efficient builds, see:\n\nCache invalidation\nOptimize build cache\nGarbage collection\nCache storage backends\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow the build cache works\nOther resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995729,
    "timestamp": "2026-02-07T06:34:33.404Z",
    "title": "Cache storage backends | Docker Docs",
    "url": "https://docs.docker.com/build/cache/backends/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nAmazon S3 cache\nAzure Blob Storage cache\nGitHub Actions cache\nInline cache\nLocal cache\nRegistry cache\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nCache storage backends\nCache storage backends\nCopy as Markdown\n\nTo ensure fast builds, BuildKit automatically caches the build result in its own internal cache. Additionally, BuildKit also supports exporting build cache to an external location, making it possible to import in future builds.\n\nAn external cache becomes almost essential in CI/CD build environments. Such environments usually have little-to-no persistence between runs, but it's still important to keep the runtime of image builds as low as possible.\n\nThe default docker driver supports the inline, local, registry, and gha cache backends, but only if you have enabled the containerd image store. Other cache backends require you to select a different driver.\n\nWarning\n\nIf you use secrets or credentials inside your build process, ensure you manipulate them using the dedicated --secret option. Manually managing secrets using COPY or ARG could result in leaked credentials.\n\nBackends\n\nBuildx supports the following cache storage backends:\n\ninline: embeds the build cache into the image.\n\nThe inline cache gets pushed to the same location as the main output result. This only works with the image exporter.\n\nregistry: embeds the build cache into a separate image, and pushes to a dedicated location separate from the main output.\n\nlocal: writes the build cache to a local directory on the filesystem.\n\ngha: uploads the build cache to GitHub Actions cache (beta).\n\ns3: uploads the build cache to an AWS S3 bucket (unreleased).\n\nazblob: uploads the build cache to Azure Blob Storage (unreleased).\n\nCommand syntax\n\nTo use any of the cache backends, you first need to specify it on build with the --cache-to option to export the cache to your storage backend of choice. Then, use the --cache-from option to import the cache from the storage backend into the current build. Unlike the local BuildKit cache (which is always enabled), all of the cache storage backends must be explicitly exported to, and explicitly imported from.\n\nExample buildx command using the registry backend, using import and export cache:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=registry,ref=<registry>/<cache-image>[,parameters...] \\\n\n  --cache-from type=registry,ref=<registry>/<cache-image>[,parameters...] .\n\nWarning\n\nAs a general rule, each cache writes to some location. No location can be written to twice, without overwriting the previously cached data. If you want to maintain multiple scoped caches (for example, a cache per Git branch), then ensure that you use different locations for exported cache.\n\nMultiple caches\n\nBuildKit supports multiple cache exporters, allowing you to push cache to more than one destination. You can also import from as many remote caches as you'd like. For example, a common pattern is to use the cache of both the current branch and the main branch. The following example shows importing cache from multiple locations using the registry cache backend:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=registry,ref=<registry>/<cache-image>:<branch> \\\n\n  --cache-from type=registry,ref=<registry>/<cache-image>:<branch> \\\n\n  --cache-from type=registry,ref=<registry>/<cache-image>:main .\n\nConfiguration options\n\nThis section describes some configuration options available when generating cache exports. The options described here are common for at least two or more backend types. Additionally, the different backend types support specific parameters as well. See the detailed page about each backend type for more information about which configuration parameters apply.\n\nThe common parameters described here are:\n\nCache mode\nCache compression\nOCI media type\nCache mode\n\nWhen generating a cache output, the --cache-to argument accepts a mode option for defining which layers to include in the exported cache. This is supported by all cache backends except for the inline cache.\n\nMode can be set to either of two options: mode=min or mode=max. For example, to build the cache with mode=max with the registry backend:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=registry,ref=<registry>/<cache-image>,mode=max \\\n\n  --cache-from type=registry,ref=<registry>/<cache-image> .\n\n\nThis option is only set when exporting a cache, using --cache-to. When importing a cache (--cache-from) the relevant parameters are automatically detected.\n\nIn min cache mode (the default), only layers that are exported into the resulting image are cached, while in max cache mode, all layers are cached, even those of intermediate steps.\n\nWhile min cache is typically smaller (which speeds up import/export times, and reduces storage costs), max cache is more likely to get more cache hits. Depending on the complexity and location of your build, you should experiment with both parameters to find the results that work best for you.\n\nCache compression\n\nThe cache compression options are the same as the exporter compression options. This is supported by the local and registry cache backends.\n\nFor example, to compress the registry cache with zstd compression:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=registry,ref=<registry>/<cache-image>,compression=zstd \\\n\n  --cache-from type=registry,ref=<registry>/<cache-image> .\n\nOCI media types\n\nThe cache OCI options are the same as the exporter OCI options. These are supported by the local and registry cache backends.\n\nFor example, to export OCI media type cache, use the oci-mediatypes property:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=registry,ref=<registry>/<cache-image>,oci-mediatypes=true \\\n\n  --cache-from type=registry,ref=<registry>/<cache-image> .\n\n\nThis property is only meaningful with the --cache-to flag. When fetching cache, BuildKit will auto-detect the correct media types to use.\n\nBy default, the OCI media type generates an image index for the cache image. Some OCI registries, such as Amazon ECR, don't support the image index media type: application/vnd.oci.image.index.v1+json. If you export cache images to ECR, or any other registry that doesn't support image indices, set the image-manifest parameter to true to generate a single image manifest instead of an image index for the cache image:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=registry,ref=<registry>/<cache-image>,oci-mediatypes=true,image-manifest=true \\\n\n  --cache-from type=registry,ref=<registry>/<cache-image> .\n\nNote\n\nSince BuildKit v0.21, image-manifest is enabled by default.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nBackends\nCommand syntax\nMultiple caches\nConfiguration options\nCache mode\nCache compression\nOCI media types\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995732,
    "timestamp": "2026-02-07T06:34:33.409Z",
    "title": "Build garbage collection | Docker Docs",
    "url": "https://docs.docker.com/build/cache/garbage-collection/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nBuild garbage collection\nBuild garbage collection\nCopy as Markdown\n\nWhile docker builder prune or docker buildx prune commands run at once, Garbage Collection (GC) runs periodically and follows an ordered list of prune policies. The BuildKit daemon clears the build cache when the cache size becomes too big, or when the cache age expires.\n\nFor most users, the default GC behavior is sufficient and doesn't require any intervention. Advanced users, particularly those working with large-scale builds, self-managed builders, or constrained storage environments, might benefit from customizing these settings to better align with their workflow needs. The following sections explain how GC works and provide guidance on tailoring its behavior through custom configuration.\n\nGarbage collection policies\n\nGC policies define a set of rules that determine how the build cache is managed and cleaned up. These policies include criteria for when to remove cache entries, such as the age of the cache, the amount of space being used, and the type of cache records to prune.\n\nEach GC policy is evaluated in sequence, starting with the most specific criteria, and proceeds to broader rules if previous policies do not free up enough cache. This lets BuildKit prioritize cache entries, preserving the most valuable cache while ensuring the system maintains performance and availability.\n\nFor example, say you have the following GC policies:\n\nFind \"stale\" cache records that haven't been used in the past 48 hours, and delete records until there's maximum 5GB of \"stale\" cache left.\nIf the build cache size exceeds 10GB, delete records until the total cache size is no more than 10GB.\n\nThe first rule is more specific, prioritizing stale cache records and setting a lower limit for a less valuable type of cache. The second rule imposes a higher hard limit that applies to any type of cache records. With these policies, if you have 11GB worth of build cache, where:\n\n7GB of which is \"stale\" cache\n4GB is other, more valuable cache\n\nA GC sweep would delete 5GB of stale cache as part of the 1st policy, with a remainder of 6GB, meaning the 2nd policy does not need to clear any more cache.\n\nThe default GC policies are (approximately):\n\nRemove cache that can be easily regenerated, such as build contexts from local directories or remote Git repositories, and cache mounts, if hasn't been used for more than 48 hours.\nRemove cache that hasn't been used in a build for more than 60 days.\nRemove unshared cache that exceeds the build cache size limit. Unshared cache records refers to layer blobs that are not used by other resources (typically, as image layers).\nRemove any build cache that exceeds the build cache size limit.\n\nThe precise algorithm and the means of configuring the policies differ slightly depending on what kind of builder you're using. Refer to Configuration for more details.\n\nConfiguration\nNote\n\nIf you're satisfied with the default garbage collection behavior and don't need to fine-tune its settings, you can skip this section. Default configurations work well for most use cases and require no additional setup.\n\nDepending on the type of build driver you use, you will use different configuration files to change the builder's GC settings:\n\nIf you use the default builder for Docker Engine (the docker driver), use the Docker daemon configuration file.\nIf you use a custom builder, use a BuildKit configuration file.\nDocker daemon configuration file\n\nIf you're using the default docker driver, GC is configured in the daemon.json configuration file, or if you use Docker Desktop, in Settings > Docker Engine.\n\nThe following snippet shows the default builder configuration for the docker driver for Docker Desktop users:\n\n{\n\n  \"builder\": {\n\n    \"gc\": {\n\n      \"defaultKeepStorage\": \"20GB\",\n\n      \"enabled\": true\n\n    }\n\n  }\n\n}\n\nThe defaultKeepStorage option configures the size limit of the build cache, which influences the GC policies. The default policies for the docker driver work as follows:\n\nRemove ephemeral, unused build cache older than 48 hours if it exceeds 13.8% of defaultKeepStorage, or at minimum 512MB.\nRemove unused build cache older than 60 days.\nRemove unshared build cache that exceeds the defaultKeepStorage limit.\nRemove any build cache that exceeds the defaultKeepStorage limit.\n\nGiven the Docker Desktop default value for defaultKeepStorage of 20GB, the default GC policies resolve to:\n\n{\n\n  \"builder\": {\n\n    \"gc\": {\n\n      \"enabled\": true,\n\n      \"policy\": [\n\n        {\n\n          \"reservedSpace\": \"2.764GB\",\n\n          \"keepDuration\": \"48h\",\n\n          \"filter\": [\n\n            \"type=source.local,type=exec.cachemount,type=source.git.checkout\"\n\n          ]\n\n        },\n\n        { \"reservedSpace\": \"20GB\", \"keepDuration\": [\"1440h\"] },\n\n        { \"reservedSpace\": \"20GB\" },\n\n        { \"reservedSpace\": \"20GB\", \"all\": true }\n\n      ]\n\n    }\n\n  }\n\n}\n\nThe easiest way to tweak the build cache configuration for the docker driver is to adjust the defaultKeepStorage option:\n\nIncrease the limit if you feel like you think the GC is too aggressive.\nDecrease the limit if you need to preserve space.\nCustom GC policies in the Docker daemon configuration file\n\nIf you need even more control, you can define your own GC policies directly. The following example defines a more conservative GC configuration with the following policies:\n\nRemove unused cache entries older than 1440 hours, or 60 days, if build cache exceeds 50GB.\nRemove unshared cache entries if build cache exceeds 50GB.\nRemove any cache entries if build cache exceeds 100GB.\n{\n\n  \"builder\": {\n\n    \"gc\": {\n\n      \"enabled\": true,\n\n      \"policy\": [\n\n        { \"reservedSpace\": \"50GB\", \"keepDuration\": [\"1440h\"] },\n\n        { \"reservedSpace\": \"50GB\" },\n\n        { \"reservedSpace\": \"100GB\", \"all\": true }\n\n      ]\n\n    }\n\n  }\n\n}\nNote\n\nIn the Docker daemon configuration file, the \"equals\" operator in GC filters is denoted using a single =, whereas BuildKit's configuration file uses ==:\n\ndaemon.json\tbuildkitd.toml\ntype=source.local\ttype==source.local\nprivate=true\tprivate==true\nshared=true\tshared==true\n\nSee prune filters for information about available GC filters. GC configuration in daemon.json supports all filters except mutable and immutable.\n\nBuildKit configuration file\n\nFor build drivers other than docker, GC is configured using a buildkitd.toml configuration file. This file uses the following high-level configuration options that you can use to tweak the thresholds for how much disk space BuildKit should use for cache:\n\nOption\tDescription\tDefault value\nreservedSpace\tThe minimum amount of disk space BuildKit is allowed to allocate for cache. Usage below this threshold will not be reclaimed during garbage collection.\t10% of total disk space or 10GB (whichever is lower)\nmaxUsedSpace\tThe maximum amount of disk space that BuildKit is allowed to use. Usage above this threshold will be reclaimed during garbage collection.\t60% of total disk space or 100GB (whichever is lower)\nminFreeSpace\tThe amount of disk space that must be kept free.\t20GB\n\nYou can set these options either as number of bytes, a unit string (for example, 512MB), or as a percentage of the total disk size. Changing these options influences the default GC policies used by the BuildKit worker. With the default thresholds, the GC policies resolve as follows:\n\n# Global defaults\n\n[worker.oci]\n\n  gc = true\n\n  reservedSpace = \"10GB\"\n\n  maxUsedSpace = \"100GB\"\n\n  minFreeSpace = \"20%\"\n\n\n\n# Policy 1\n\n[[worker.oci.gcpolicy]]\n\n  filters = [ \"type==source.local\", \"type==exec.cachemount\", \"type==source.git.checkout\" ]\n\n  keepDuration = \"48h\"\n\n  maxUsedSpace = \"512MB\"\n\n\n\n# Policy 2\n\n[[worker.oci.gcpolicy]]\n\n  keepDuration = \"1440h\" # 60 days\n\n  reservedSpace = \"10GB\"\n\n  maxUsedSpace = \"100GB\"\n\n\n\n# Policy 3\n\n[[worker.oci.gcpolicy]]\n\n  reservedSpace = \"10GB\"\n\n  maxUsedSpace = \"100GB\"\n\n\n\n# Policy 4\n\n[[worker.oci.gcpolicy]]\n\n  all = true\n\n  reservedSpace = \"10GB\"\n\n  maxUsedSpace = \"100GB\"\n\nIn practical terms, this means:\n\nPolicy 1: If the build cache exceeds 512MB, BuildKit removes cache records for local build contexts, remote Git contexts, and cache mounts that haven‚Äôt been used in the last 48 hours.\nPolicy 2: If disk usage exceeds 100GB, unshared build cache older than 60 days is removed, ensuring at least 10GB of disk space is reserved for cache.\nPolicy 3: If disk usage exceeds 100GB, any unshared cache is removed, ensuring at least 10GB of disk space is reserved for cache.\nPolicy 4: If disk usage exceeds 100GB, all cache‚Äîincluding shared and internal records‚Äîis removed, ensuring at least 10GB of disk space is reserved for cache.\n\nreservedSpace has the highest priority in defining the lower limit for build cache size. If maxUsedSpace or minFreeSpace would define a lower value, the minimum cache size would never be brought below reservedSpace.\n\nIf both reservedSpace and maxUsedSpace are set, a GC sweep results in a cache size between those thresholds. For example, if reservedSpace is set to 10GB, and maxUsedSpace is set to 20GB, the resulting amount of cache after a GC run is less than 20GB, but at least 10GB.\n\nYou can also define completely custom GC policies. Custom policies also let you define filters, which lets you pinpoint the types of cache entries that a given policy is allowed to prune.\n\nCustom GC policies in BuildKit\n\nCustom GC policies let you fine-tune how BuildKit manages its cache, and gives you full control over cache retention based on criteria such as cache type, duration, or disk space thresholds. If you need full control over the cache thresholds and how cache records should be prioritized, defining custom GC policies is the way to go.\n\nTo define a custom GC policy, use the [[worker.oci.gcpolicy]] configuration block in buildkitd.toml. Each policy define the thresholds that will be used for that policy. The global values for reservedSpace, maxUsedSpace, and minFreeSpace do not apply if you use custom policies.\n\nHere‚Äôs an example configuration:\n\n# Custom GC Policy 1: Remove unused local contexts older than 24 hours\n\n[[worker.oci.gcpolicy]]\n\n  filters = [\"type==source.local\"]\n\n  keepDuration = \"24h\"\n\n  reservedSpace = \"5GB\"\n\n  maxUsedSpace = \"50GB\"\n\n\n\n# Custom GC Policy 2: Remove remote Git contexts older than 30 days\n\n[[worker.oci.gcpolicy]]\n\n  filters = [\"type==source.git.checkout\"]\n\n  keepDuration = \"720h\"\n\n  reservedSpace = \"5GB\"\n\n  maxUsedSpace = \"30GB\"\n\n\n\n# Custom GC Policy 3: Aggressively clean all cache if disk usage exceeds 90GB\n\n[[worker.oci.gcpolicy]]\n\n  all = true\n\n  reservedSpace = \"5GB\"\n\n  maxUsedSpace = \"90GB\"\n\nIn addition to the reservedSpace, maxUsedSpace, and minFreeSpace threshold, when defining a GC policy you have two additional configuration options:\n\nall: By default, BuildKit will exclude some cache records from being pruned during GC. Setting this option to true will allow any cache records to be pruned.\nfilters: Filters let you specify specific types of cache records that a GC policy is allowed to prune.\n\nSee buildx prune filters for information about available GC filters.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nGarbage collection policies\nConfiguration\nDocker daemon configuration file\nBuildKit configuration file\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995735,
    "timestamp": "2026-02-07T06:34:33.411Z",
    "title": "Optimize cache usage in builds | Docker Docs",
    "url": "https://docs.docker.com/build/cache/optimize/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nOptimize cache usage in builds\nOptimize cache usage in builds\nCopy as Markdown\n\nWhen building with Docker, a layer is reused from the build cache if the instruction and the files it depends on hasn't changed since it was previously built. Reusing layers from the cache speeds up the build process because Docker doesn't have to rebuild the layer again.\n\nHere are a few techniques you can use to optimize build caching and speed up the build process:\n\nOrder your layers: Putting the commands in your Dockerfile into a logical order can help you avoid unnecessary cache invalidation.\nKeep the context small: The context is the set of files and directories that are sent to the builder to process a build instruction. Keeping the context as small as possible reduces the amount of data that needs to be sent to the builder, and reduces the likelihood of cache invalidation.\nUse bind mounts: Bind mounts let you mount a file or directory from the host machine into the build container. Using bind mounts can help you avoid unnecessary layers in the image, which can slow down the build process.\nUse cache mounts: Cache mounts let you specify a persistent package cache to be used during builds. The persistent cache helps speed up build steps, especially steps that involve installing packages using a package manager. Having a persistent cache for packages means that even if you rebuild a layer, you only download new or changed packages.\nUse an external cache: An external cache lets you store build cache at a remote location. The external cache image can be shared between multiple builds, and across different environments.\nOrder your layers\n\nPutting the commands in your Dockerfile into a logical order is a great place to start. Because a change causes a rebuild for steps that follow, try to make expensive steps appear near the beginning of the Dockerfile. Steps that change often should appear near the end of the Dockerfile, to avoid triggering rebuilds of layers that haven't changed.\n\nConsider the following example. A Dockerfile snippet that runs a JavaScript build from the source files in the current directory:\n\n# syntax=docker/dockerfile:1\n\nFROM node\n\nWORKDIR /app\n\nCOPY . .          # Copy over all files in the current directory\n\nRUN npm install   # Install dependencies\n\nRUN npm build     # Run build\n\nThis Dockerfile is rather inefficient. Updating any file causes a reinstall of all dependencies every time you build the Docker image even if the dependencies didn't change since last time.\n\nInstead, the COPY command can be split in two. First, copy over the package management files (in this case, package.json and yarn.lock). Then, install the dependencies. Finally, copy over the project source code, which is subject to frequent change.\n\n# syntax=docker/dockerfile:1\n\nFROM node\n\nWORKDIR /app\n\nCOPY package.json yarn.lock .    # Copy package management files\n\nRUN npm install                  # Install dependencies\n\nCOPY . .                         # Copy over project files\n\nRUN npm build                    # Run build\n\nBy installing dependencies in earlier layers of the Dockerfile, there is no need to rebuild those layers when a project file has changed.\n\nKeep the context small\n\nThe easiest way to make sure your context doesn't include unnecessary files is to create a .dockerignore file in the root of your build context. The .dockerignore file works similarly to .gitignore files, and lets you exclude files and directories from the build context.\n\nHere's an example .dockerignore file that excludes the node_modules directory, all files and directories that start with tmp:\n\n.dockerignore\nnode_modules\n\ntmp*\n\nIgnore-rules specified in the .dockerignore file apply to the entire build context, including subdirectories. This means it's a rather coarse-grained mechanism, but it's a good way to exclude files and directories that you know you don't need in the build context, such as temporary files, log files, and build artifacts.\n\nUse bind mounts\n\nYou might be familiar with bind mounts for when you run containers with docker run or Docker Compose. Bind mounts let you mount a file or directory from the host machine into a container.\n\n# bind mount using the -v flag\n\ndocker run -v $(pwd):/path/in/container image-name\n\n# bind mount using the --mount flag\n\ndocker run --mount=type=bind,src=.,dst=/path/in/container image-name\n\nTo use bind mounts in a build, you can use the --mount flag with the RUN instruction in your Dockerfile:\n\nFROM golang:latest\n\nWORKDIR /app\n\nRUN --mount=type=bind,target=. go build -o /app/hello\n\nIn this example, the current directory is mounted into the build container before the go build command gets executed. The source code is available in the build container for the duration of that RUN instruction. When the instruction is done executing, the mounted files are not persisted in the final image, or in the build cache. Only the output of the go build command remains.\n\nThe COPY and ADD instructions in a Dockerfile lets you copy files from the build context into the build container. Using bind mounts is beneficial for build cache optimization because you're not adding unnecessary layers to the cache. If you have build context that's on the larger side, and it's only used to generate an artifact, you're better off using bind mounts to temporarily mount the source code required to generate the artifact into the build. If you use COPY to add the files to the build container, BuildKit will include all of those files in the cache, even if the files aren't used in the final image.\n\nThere are a few things to be aware of when using bind mounts in a build:\n\nBind mounts are read-only by default. If you need to write to the mounted directory, you need to specify the rw option. However, even with the rw option, the changes are not persisted in the final image or the build cache. The file writes are sustained for the duration of the RUN instruction, and are discarded after the instruction is done.\n\nMounted files are not persisted in the final image. Only the output of the RUN instruction is persisted in the final image. If you need to include files from the build context in the final image, you need to use the COPY or ADD instructions.\n\nIf the target directory is not empty, the contents of the target directory are hidden by the mounted files. The original contents are restored after the RUN instruction is done.\n\nExample\nUse cache mounts\n\nRegular cache layers in Docker correspond to an exact match of the instruction and the files it depends on. If the instruction and the files it depends on have changed since the layer was built, the layer is invalidated, and the build process has to rebuild the layer.\n\nCache mounts are a way to specify a persistent cache location to be used during builds. The cache is cumulative across builds, so you can read and write to the cache multiple times. This persistent caching means that even if you need to rebuild a layer, you only download new or changed packages. Any unchanged packages are reused from the cache mount.\n\nTo use cache mounts in a build, you can use the --mount flag with the RUN instruction in your Dockerfile:\n\nFROM node:latest\n\nWORKDIR /app\n\nRUN --mount=type=cache,target=/root/.npm npm install\n\nIn this example, the npm install command uses a cache mount for the /root/.npm directory, the default location for the npm cache. The cache mount is persisted across builds, so even if you end up rebuilding the layer, you only download new or changed packages. Any changes to the cache are persisted across builds, and the cache is shared between multiple builds.\n\nHow you specify cache mounts depends on the build tool you're using. If you're unsure how to specify cache mounts, refer to the documentation for the build tool you're using. Here are a few examples:\n\nGo Apt Python Ruby Rust .NET PHP\nRUN --mount=type=cache,target=/go/pkg/mod \\\n\n    --mount=type=cache,target=/root/.cache/go-build \\\n\n    go build -o /app/hello\n\nIt's important that you read the documentation for the build tool you're using to make sure you're using the correct cache mount options. Package managers have different requirements for how they use the cache, and using the wrong options can lead to unexpected behavior. For example, Apt needs exclusive access to its data, so the caches use the option sharing=locked to ensure parallel builds using the same cache mount wait for each other and not access the same cache files at the same time.\n\nUse an external cache\n\nThe default cache storage for builds is internal to the builder (BuildKit instance) you're using. Each builder uses its own cache storage. When you switch between different builders, the cache is not shared between them. Using an external cache lets you define a remote location for pushing and pulling cache data.\n\nExternal caches are especially useful for CI/CD pipelines, where the builders are often ephemeral, and build minutes are precious. Reusing the cache between builds can drastically speed up the build process and reduce cost. You can even make use of the same cache in your local development environment.\n\nTo use an external cache, you specify the --cache-to and --cache-from options with the docker buildx build command.\n\n--cache-to exports the build cache to the specified location.\n--cache-from specifies remote caches for the build to use.\n\nThe following example shows how to set up a GitHub Actions workflow using docker/build-push-action, and push the build cache layers to an OCI registry image:\n\n.github/workflows/ci.yml\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: true\n\n          tags: user/app:latest\n\n          cache-from: type=registry,ref=user/app:buildcache\n\n          cache-to: type=registry,ref=user/app:buildcache,mode=max\n\nThis setup tells BuildKit to look for cache in the user/app:buildcache image. And when the build is done, the new build cache is pushed to the same image, overwriting the old cache.\n\nThis cache can be used locally as well. To pull the cache in a local build, you can use the --cache-from option with the docker buildx build command:\n\n$ docker buildx build --cache-from type=registry,ref=user/app:buildcache .\n\nSummary\n\nOptimizing cache usage in builds can significantly speed up the build process. Keeping the build context small, using bind mounts, cache mounts, and external caches are all techniques you can use to make the most of the build cache and speed up the build process.\n\nFor more information about the concepts discussed in this guide, see:\n\n.dockerignore files\nCache invalidation\nCache mounts\nCache backend types\nBuilding best practices\n\nEdit this page\n\nRequest changes\n\nTable of contents\nOrder your layers\nKeep the context small\nUse bind mounts\nUse cache mounts\nUse an external cache\nSummary\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995738,
    "timestamp": "2026-02-07T06:34:33.413Z",
    "title": "Amazon S3 cache | Docker Docs",
    "url": "https://docs.docker.com/build/cache/backends/s3/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nAmazon S3 cache\nAzure Blob Storage cache\nGitHub Actions cache\nInline cache\nLocal cache\nRegistry cache\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nCache storage backends\n/\nAmazon S3 cache\nAmazon S3 cache\nCopy as Markdown\nAvailability:\nExperimental \n\nThe s3 cache storage uploads your resulting build cache to Amazon S3 file storage service or other S3-compatible services, such as MinIO.\n\nThis cache storage backend is not supported with the default docker driver. To use this feature, create a new builder using a different driver. See Build drivers for more information.\n\nSynopsis\n$ docker buildx build --push -t <user>/<image> \\\n\n  --cache-to type=s3,region=<region>,bucket=<bucket>,name=<cache-image>[,parameters...] \\\n\n  --cache-from type=s3,region=<region>,bucket=<bucket>,name=<cache-image> .\n\n\nThe following table describes the available CSV parameters that you can pass to --cache-to and --cache-from.\n\nName\tOption\tType\tDefault\tDescription\nregion\tcache-to,cache-from\tString\t\tRequired. Geographic location.\nbucket\tcache-to,cache-from\tString\t\tRequired. Name of the S3 bucket.\nname\tcache-to,cache-from\tString\tbuildkit\tName of the cache image.\nendpoint_url\tcache-to,cache-from\tString\t\tEndpoint of the S3 bucket.\nprefix\tcache-to,cache-from\tString\t\tPrefix to prepend to all filenames.\nblobs_prefix\tcache-to,cache-from\tString\tblobs/\tPrefix to prepend to blob filenames.\nupload_parallelism\tcache-to\tInteger\t4\tNumber of parallel layer uploads.\ntouch_refresh\tcache-to\tTime\t24h\tInterval for updating the timestamp of unchanged cache layers.\nmanifests_prefix\tcache-to,cache-from\tString\tmanifests/\tPrefix to prepend to manifest filenames.\nuse_path_style\tcache-to,cache-from\tBoolean\tfalse\tWhen true, uses bucket in the URL instead of hostname.\naccess_key_id\tcache-to,cache-from\tString\t\tSee authentication.\nsecret_access_key\tcache-to,cache-from\tString\t\tSee authentication.\nsession_token\tcache-to,cache-from\tString\t\tSee authentication.\nmode\tcache-to\tmin,max\tmin\tCache layers to export, see cache mode.\nignore-error\tcache-to\tBoolean\tfalse\tIgnore errors caused by failed cache exports.\nAuthentication\n\nBuildx can reuse existing AWS credentials, configured either using a credentials file or environment variables, for pushing and pulling cache to S3. Alternatively, you can use the access_key_id, secret_access_key, and session_token attributes to specify credentials directly on the CLI.\n\nRefer to AWS Go SDK, Specifying Credentials for details about authentication using environment variables and credentials file.\n\nFurther reading\n\nFor an introduction to caching see Docker build cache.\n\nFor more information on the s3 cache backend, see the BuildKit README.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nAuthentication\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995741,
    "timestamp": "2026-02-07T06:34:33.425Z",
    "title": "Azure Blob Storage cache | Docker Docs",
    "url": "https://docs.docker.com/build/cache/backends/azblob/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nAmazon S3 cache\nAzure Blob Storage cache\nGitHub Actions cache\nInline cache\nLocal cache\nRegistry cache\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nCache storage backends\n/\nAzure Blob Storage cache\nAzure Blob Storage cache\nCopy as Markdown\nAvailability:\nExperimental \n\nThe azblob cache store uploads your resulting build cache to Azure's blob storage service.\n\nThis cache storage backend is not supported with the default docker driver. To use this feature, create a new builder using a different driver. See Build drivers for more information.\n\nSynopsis\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=azblob,name=<cache-image>[,parameters...] \\\n\n  --cache-from type=azblob,name=<cache-image>[,parameters...] .\n\n\nThe following table describes the available CSV parameters that you can pass to --cache-to and --cache-from.\n\nName\tOption\tType\tDefault\tDescription\nname\tcache-to,cache-from\tString\t\tRequired. The name of the cache image.\naccount_url\tcache-to,cache-from\tString\t\tBase URL of the storage account.\nsecret_access_key\tcache-to,cache-from\tString\t\tBlob storage account key, see authentication.\nmode\tcache-to\tmin,max\tmin\tCache layers to export, see cache mode.\nignore-error\tcache-to\tBoolean\tfalse\tIgnore errors caused by failed cache exports.\nAuthentication\n\nThe secret_access_key, if left unspecified, is read from environment variables on the BuildKit server following the scheme for the Azure Go SDK. The environment variables are read from the server, not the Buildx client.\n\nFurther reading\n\nFor an introduction to caching see Docker build cache.\n\nFor more information on the azblob cache backend, see the BuildKit README.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nAuthentication\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995742,
    "timestamp": "2026-02-07T06:34:33.430Z",
    "title": "GitHub Actions cache | Docker Docs",
    "url": "https://docs.docker.com/build/cache/backends/gha/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nAmazon S3 cache\nAzure Blob Storage cache\nGitHub Actions cache\nInline cache\nLocal cache\nRegistry cache\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nCache storage backends\n/\nGitHub Actions cache\nGitHub Actions cache\nCopy as Markdown\nAvailability:\nExperimental \n\nThe GitHub Actions cache utilizes the GitHub-provided Action's cache or other cache services supporting the GitHub Actions cache protocol. This is the recommended cache to use inside your GitHub Actions workflows, as long as your use case falls within the size and usage limits set by GitHub.\n\nThis cache storage backend is not supported with the default docker driver. To use this feature, create a new builder using a different driver. See Build drivers for more information.\n\nSynopsis\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=gha[,parameters...] \\\n\n  --cache-from type=gha[,parameters...] .\n\n\nThe following table describes the available CSV parameters that you can pass to --cache-to and --cache-from.\n\nName\tOption\tType\tDefault\tDescription\nurl\tcache-to,cache-from\tString\t$ACTIONS_CACHE_URL or $ACTIONS_RESULTS_URL\tCache server URL, see authentication. Ignored when version=2.\nurl_v2\tcache-to,cache-from\tString\t$ACTIONS_RESULTS_URL\tCache v2 server URL, see authentication.\ntoken\tcache-to,cache-from\tString\t$ACTIONS_RUNTIME_TOKEN\tAccess token, see authentication.\nscope\tcache-to,cache-from\tString\tbuildkit\tWhich scope cache object belongs to, see scope\nmode\tcache-to\tmin,max\tmin\tCache layers to export, see cache mode.\nignore-error\tcache-to\tBoolean\tfalse\tIgnore errors caused by failed cache exports.\ntimeout\tcache-to,cache-from\tString\t10m\tMax duration for importing or exporting cache before it's timed out.\nrepository\tcache-to\tString\t\tGitHub repository used for cache storage.\nghtoken\tcache-to\tString\t\tGitHub token required for accessing the GitHub API.\nversion\tcache-to,cache-from\tString\t1 unless $ACTIONS_CACHE_SERVICE_V2 is set, then 2\tSelects GitHub Actions cache version, see version\nAuthentication\n\nIf the url, url_v2 or token parameters are left unspecified, the gha cache backend will fall back to using environment variables. If you invoke the docker buildx command manually from an inline step, then the variables must be manually exposed. Consider using the crazy-max/ghaction-github-runtime, GitHub Action as a helper for exposing the variables.\n\nScope\n\nScope is a key used to identify the cache object. By default, it is set to buildkit. If you build multiple images, each build will overwrite the cache of the previous, leaving only the final cache.\n\nTo preserve the cache for multiple builds, you can specify this scope attribute with a specific name. In the following example, the cache is set to the image name, to ensure each image gets its own cache:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=gha,url=...,token=...,scope=image \\\n\n  --cache-from type=gha,url=...,token=...,scope=image .\n\n$ docker buildx build --push -t <registry>/<image2> \\\n\n  --cache-to type=gha,url=...,token=...,scope=image2 \\\n\n  --cache-from type=gha,url=...,token=...,scope=image2 .\n\n\nGitHub's cache access restrictions, still apply. Only the cache for the current branch, the base branch and the default branch is accessible by a workflow.\n\nVersion\n\nIf you don‚Äôt set version explicitly, the default is v1. However, if the environment variable $ACTIONS_CACHE_SERVICE_V2 is set to a value interpreted as true ( 1, true, yes), then v2 is used automatically.\n\nOnly one URL is relevant at a time:\n\nWith v1, use url (defaults to $ACTIONS_CACHE_URL).\nWith v2, use url_v2 (defaults to $ACTIONS_RESULTS_URL).\nUsing docker/build-push-action\n\nWhen using the docker/build-push-action, the url and token parameters are automatically populated. No need to manually specify them, or include any additional workarounds.\n\nFor example:\n\n- name: Build and push\n\n  uses: docker/build-push-action@v6\n\n  with:\n\n    context: .\n\n    push: true\n\n    tags: \"<registry>/<image>:latest\"\n\n    cache-from: type=gha\n\n    cache-to: type=gha,mode=max\nAvoid GitHub Actions cache API throttling\n\nGitHub's usage limits and eviction policy causes stale cache entries to be removed after a certain period of time. By default, the gha cache backend uses the GitHub Actions cache API to check the status of cache entries.\n\nThe GitHub Actions cache API is subject to rate limiting if you make too many requests in a short period of time, which may happen as a result of cache lookups during a build using the gha cache backend.\n\n#31 exporting to GitHub Actions Cache\n\n#31 preparing build cache for export\n\n#31 preparing build cache for export 600.3s done\n\n#31 ERROR: maximum timeout reached\n\n------\n\n > exporting to GitHub Actions Cache:\n\n------\n\nERROR: failed to solve: maximum timeout reached\n\nmake: *** [Makefile:35: release] Error 1\n\nError: Process completed with exit code 2.\n\nTo mitigate this issue, you can supply a GitHub token to BuildKit. This lets BuildKit utilize the standard GitHub API for checking cache keys, thereby reducing the number of requests made to the cache API.\n\nTo provide a GitHub token, you can use the ghtoken parameter, and a repository parameter to specify the repository to use for cache storage. The ghtoken parameter is a GitHub token with the repo scope, which is required to access the GitHub Actions cache API.\n\nThe ghtoken parameter is automatically set to the value of secrets.GITHUB_TOKEN when you build with the docker/build-push-action action. You can also set the ghtoken parameter manually using the github-token input, as shown in the following example:\n\n- name: Build and push\n\n  uses: docker/build-push-action@v6\n\n  with:\n\n    context: .\n\n    push: true\n\n    tags: \"<registry>/<image>:latest\"\n\n    cache-from: type=gha\n\n    cache-to: type=gha,mode=max\n\n    github-token: ${{ secrets.MY_CUSTOM_TOKEN }}\nFurther reading\n\nFor an introduction to caching see Docker build cache.\n\nFor more information on the gha cache backend, see the BuildKit README.\n\nFor more information about using GitHub Actions with Docker, see Introduction to GitHub Actions\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nAuthentication\nScope\nVersion\nUsing docker/build-push-action\nAvoid GitHub Actions cache API throttling\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995745,
    "timestamp": "2026-02-07T06:34:33.431Z",
    "title": "Inline cache | Docker Docs",
    "url": "https://docs.docker.com/build/cache/backends/inline/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nAmazon S3 cache\nAzure Blob Storage cache\nGitHub Actions cache\nInline cache\nLocal cache\nRegistry cache\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nCache storage backends\n/\nInline cache\nInline cache\nCopy as Markdown\n\nThe inline cache storage backend is the simplest way to get an external cache and is easy to get started using if you're already building and pushing an image.\n\nThe downside of inline cache is that it doesn't scale with multi-stage builds as well as the other drivers do. It also doesn't offer separation between your output artifacts and your cache output. This means that if you're using a particularly complex build flow, or not exporting your images directly to a registry, then you may want to consider the registry cache.\n\nSynopsis\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=inline \\\n\n  --cache-from type=registry,ref=<registry>/<image> .\n\n\nNo additional parameters are supported for the inline cache.\n\nTo export cache using inline storage, pass type=inline to the --cache-to option:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=inline .\n\n\nAlternatively, you can also export inline cache by setting the build argument BUILDKIT_INLINE_CACHE=1, instead of using the --cache-to flag:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --build-arg BUILDKIT_INLINE_CACHE=1 .\n\n\nTo import the resulting cache on a future build, pass type=registry to --cache-from which lets you extract the cache from inside a Docker image in the specified registry:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-from type=registry,ref=<registry>/<image> .\n\nFurther reading\n\nFor an introduction to caching see Docker build cache.\n\nFor more information on the inline cache backend, see the BuildKit README.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995748,
    "timestamp": "2026-02-07T06:34:33.434Z",
    "title": "Local cache | Docker Docs",
    "url": "https://docs.docker.com/build/cache/backends/local/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nAmazon S3 cache\nAzure Blob Storage cache\nGitHub Actions cache\nInline cache\nLocal cache\nRegistry cache\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nCache storage backends\n/\nLocal cache\nLocal cache\nCopy as Markdown\n\nThe local cache store is a simple cache option that stores your cache as files in a directory on your filesystem, using an OCI image layout for the underlying directory structure. Local cache is a good choice if you're just testing, or if you want the flexibility to self-manage a shared storage solution.\n\nSynopsis\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=local,dest=path/to/local/dir[,parameters...] \\\n\n  --cache-from type=local,src=path/to/local/dir .\n\n\nThe following table describes the available CSV parameters that you can pass to --cache-to and --cache-from.\n\nName\tOption\tType\tDefault\tDescription\nsrc\tcache-from\tString\t\tPath of the local directory where cache gets imported from.\ndigest\tcache-from\tString\t\tDigest of manifest to import, see cache versioning.\ndest\tcache-to\tString\t\tPath of the local directory where cache gets exported to.\nmode\tcache-to\tmin,max\tmin\tCache layers to export, see cache mode.\noci-mediatypes\tcache-to\ttrue,false\ttrue\tUse OCI media types in exported manifests, see OCI media types.\nimage-manifest\tcache-to\ttrue,false\ttrue\tWhen using OCI media types, generate an image manifest instead of an image index for the cache image, see OCI media types.\ncompression\tcache-to\tgzip,estargz,zstd\tgzip\tCompression type, see cache compression.\ncompression-level\tcache-to\t0..22\t\tCompression level, see cache compression.\nforce-compression\tcache-to\ttrue,false\tfalse\tForcibly apply compression, see cache compression.\nignore-error\tcache-to\tBoolean\tfalse\tIgnore errors caused by failed cache exports.\n\nIf the src cache doesn't exist, then the cache import step will fail, but the build continues.\n\nCache versioning\n\nThis section describes how versioning works for caches on a local filesystem, and how you can use the digest parameter to use older versions of cache.\n\nIf you inspect the cache directory manually, you can see the resulting OCI image layout:\n\n$ ls cache\n\nblobs  index.json  ingest\n\n$ cat cache/index.json | jq\n\n{\n\n  \"schemaVersion\": 2,\n\n  \"manifests\": [\n\n    {\n\n      \"mediaType\": \"application/vnd.oci.image.index.v1+json\",\n\n      \"digest\": \"sha256:6982c70595cb91769f61cd1e064cf5f41d5357387bab6b18c0164c5f98c1f707\",\n\n      \"size\": 1560,\n\n      \"annotations\": {\n\n        \"org.opencontainers.image.ref.name\": \"latest\"\n\n      }\n\n    }\n\n  ]\n\n}\n\n\nLike other cache types, local cache gets replaced on export, by replacing the contents of the index.json file. However, previous caches will still be available in the blobs directory. These old caches are addressable by digest, and kept indefinitely. Therefore, the size of the local cache will continue to grow (see moby/buildkit#1896 for more information).\n\nWhen importing cache using --cache-from, you can specify the digest parameter to force loading an older version of the cache, for example:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=local,dest=path/to/local/dir \\\n\n  --cache-from type=local,ref=path/to/local/dir,digest=sha256:6982c70595cb91769f61cd1e064cf5f41d5357387bab6b18c0164c5f98c1f707 .\n\nFurther reading\n\nFor an introduction to caching see Docker build cache.\n\nFor more information on the local cache backend, see the BuildKit README.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nCache versioning\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995753,
    "timestamp": "2026-02-07T06:34:33.450Z",
    "title": "Registry cache | Docker Docs",
    "url": "https://docs.docker.com/build/cache/backends/registry/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nBuild cache invalidation\nBuild garbage collection\nCache storage backends\nAmazon S3 cache\nAzure Blob Storage cache\nGitHub Actions cache\nInline cache\nLocal cache\nRegistry cache\nOptimize cache usage in builds\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCache\n/\nCache storage backends\n/\nRegistry cache\nRegistry cache\nCopy as Markdown\n\nThe registry cache storage can be thought of as an extension to the inline cache. Unlike the inline cache, the registry cache is entirely separate from the image, which allows for more flexible usage - registry-backed cache can do everything that the inline cache can do, and more:\n\nAllows for separating the cache and resulting image artifacts so that you can distribute your final image without the cache inside.\nIt can efficiently cache multi-stage builds in max mode, instead of only the final stage.\nIt works with other exporters for more flexibility, instead of only the image exporter.\n\nThis cache storage backend is not supported with the default docker driver. To use this feature, create a new builder using a different driver. See Build drivers for more information.\n\nSynopsis\n\nUnlike the simpler inline cache, the registry cache supports several configuration parameters:\n\n$ docker buildx build --push -t <registry>/<image> \\\n\n  --cache-to type=registry,ref=<registry>/<cache-image>[,parameters...] \\\n\n  --cache-from type=registry,ref=<registry>/<cache-image> .\n\n\nThe following table describes the available CSV parameters that you can pass to --cache-to and --cache-from.\n\nName\tOption\tType\tDefault\tDescription\nref\tcache-to,cache-from\tString\t\tFull name of the cache image to import.\nmode\tcache-to\tmin,max\tmin\tCache layers to export, see cache mode.\noci-mediatypes\tcache-to\ttrue,false\ttrue\tUse OCI media types in exported manifests, see OCI media types.\nimage-manifest\tcache-to\ttrue,false\ttrue\tWhen using OCI media types, generate an image manifest instead of an image index for the cache image, see OCI media types.\ncompression\tcache-to\tgzip,estargz,zstd\tgzip\tCompression type, see cache compression.\ncompression-level\tcache-to\t0..22\t\tCompression level, see cache compression.\nforce-compression\tcache-to\ttrue,false\tfalse\tForcibly apply compression, see cache compression.\nignore-error\tcache-to\tBoolean\tfalse\tIgnore errors caused by failed cache exports.\n\nYou can choose any valid value for ref, as long as it's not the same as the target location that you push your image to. You might choose different tags (e.g. foo/bar:latest and foo/bar:build-cache), separate image names (e.g. foo/bar and foo/bar-cache), or even different repositories (e.g. docker.io/foo/bar and ghcr.io/foo/bar). It's up to you to decide the strategy that you want to use for separating your image from your cache images.\n\nIf the --cache-from target doesn't exist, then the cache import step will fail, but the build continues.\n\nFurther reading\n\nFor an introduction to caching see Docker build cache.\n\nFor more information on the registry cache backend, see the BuildKit README.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995756,
    "timestamp": "2026-02-07T06:34:33.451Z",
    "title": "CI | Docker Docs",
    "url": "https://docs.docker.com/build/ci/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\nContinuous integration with Docker\nCopy as Markdown\n\nContinuous Integration (CI) is the part of the development process where you're looking to get your code changes merged with the main branch of the project. At this point, development teams run tests and builds to vet that the code changes don't cause any unwanted or unexpected behaviors.\n\nThere are several uses for Docker at this stage of development, even if you don't end up packaging your application as a container image.\n\nDocker as a build environment\n\nContainers are reproducible, isolated environments that yield predictable results. Building and testing your application in a Docker container makes it easier to prevent unexpected behaviors from occurring. Using a Dockerfile, you define the exact requirements for the build environment, including programming runtimes, operating system, binaries, and more.\n\nUsing Docker to manage your build environment also eases maintenance. For example, updating to a new version of a programming runtime can be as simple as changing a tag or digest in a Dockerfile. No need to SSH into a pet VM to manually reinstall a newer version and update the related configuration files.\n\nAdditionally, just as you expect third-party open source packages to be secure, the same should go for your build environment. You can scan and index a builder image, just like you would for any other containerized application.\n\nThe following links provide instructions for how you can get started using Docker for building your applications in CI:\n\nGitHub Actions\nGitLab\nCircle CI\nRender\nDocker in Docker\n\nYou can also use a Dockerized build environment to build container images using Docker. That is, your build environment runs inside a container which itself is equipped to run Docker builds. This method is referred to as \"Docker in Docker\".\n\nDocker provides an official Docker image that you can use for this purpose.\n\nWhat's next\n\nDocker maintains a set of official GitHub Actions that you can use to build, annotate, and push container images on the GitHub Actions platform. See Introduction to GitHub Actions to learn more and get started.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDocker as a build environment\nDocker in Docker\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995757,
    "timestamp": "2026-02-07T06:34:33.458Z",
    "title": "GitHub Actions | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\nDocker Build GitHub Actions\nCopy as Markdown\n\nGitHub Actions is a popular CI/CD platform for automating your build, test, and deployment pipeline. Docker provides a set of official GitHub Actions for you to use in your workflows. These official actions are reusable, easy-to-use components for building, annotating, and pushing images.\n\nThe following GitHub Actions are available:\n\nBuild and push Docker images: build and push Docker images with BuildKit.\nDocker Buildx Bake: enables using high-level builds with Bake.\nDocker Login: sign in to a Docker registry.\nDocker Setup Buildx: creates and boots a BuildKit builder.\nDocker Metadata action: extracts metadata from Git reference and GitHub events to generate tags, labels, and annotations.\nDocker Setup Compose: installs and sets up Compose.\nDocker Setup Docker: installs Docker Engine.\nDocker Setup QEMU: installs QEMU static binaries for multi-platform builds.\nDocker Scout: analyze Docker images for security vulnerabilities.\n\nUsing Docker's actions provides an easy-to-use interface, while still allowing flexibility for customizing build parameters.\n\nExamples\n\nIf you're looking for examples on how to use the Docker GitHub Actions, refer to the following sections:\n\nAdd image annotations with GitHub Actions\n\nAdd SBOM and provenance attestations with GitHub Actions\n\nValidating build configuration with GitHub Actions\n\nUsing secrets with GitHub Actions\n\nGitHub Actions build summary\n\nConfiguring your GitHub Actions builder\n\nCache management with GitHub Actions\n\nCopy image between registries with GitHub Actions\n\nExport to Docker with GitHub Actions\n\nLocal registry with GitHub Actions\n\nMulti-platform image with GitHub Actions\n\nNamed contexts with GitHub Actions\n\nPush to multiple registries with GitHub Actions\n\nReproducible builds with GitHub Actions\n\nShare built image between jobs with GitHub Actions\n\nManage tags and labels with GitHub Actions\n\nTest before push with GitHub Actions\n\nUpdate Docker Hub description with GitHub Actions\n\nGet started with GitHub Actions\n\nThe Introduction to GitHub Actions with Docker guide walks you through the process of setting up and using Docker GitHub Actions for building Docker images, and pushing images to Docker Hub.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExamples\nGet started with GitHub Actions\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995762,
    "timestamp": "2026-02-07T06:34:33.465Z",
    "title": "Annotations | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/annotations/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nAnnotations\nAdd image annotations with GitHub Actions\nCopy as Markdown\n\nAnnotations let you specify arbitrary metadata for OCI image components, such as manifests, indexes, and descriptors.\n\nTo add annotations when building images with GitHub Actions, use the metadata-action to automatically create OCI-compliant annotations. The metadata action creates an annotations output that you can reference, both with build-push-action and bake-action.\n\nbuild-push-action bake-action\nname: ci\n\n\n\non:\n\n  push:\n\n\n\nenv:\n\n  IMAGE_NAME: user/app\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Extract metadata\n\n        id: meta\n\n        uses: docker/metadata-action@v5\n\n        with:\n\n          images: ${{ env.IMAGE_NAME }}\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          tags: ${{ steps.meta.outputs.tags }}\n\n          annotations: ${{ steps.meta.outputs.annotations }}\n\n          push: true\nConfigure annotation level\n\nBy default, annotations are placed on image manifests. To configure the annotation level, set the DOCKER_METADATA_ANNOTATIONS_LEVELS environment variable on the metadata-action step to a comma-separated list of all the levels that you want to annotate. For example, setting DOCKER_METADATA_ANNOTATIONS_LEVELS to index results in annotations on the image index instead of the manifests.\n\nThe following example creates annotations on both the image index and manifests.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\nenv:\n\n  IMAGE_NAME: user/app\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      \n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Extract metadata\n\n        id: meta\n\n        uses: docker/metadata-action@v5\n\n        with:\n\n          images: ${{ env.IMAGE_NAME }}\n\n        env:\n\n          DOCKER_METADATA_ANNOTATIONS_LEVELS: manifest,index\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          tags: ${{ steps.meta.outputs.tags }}\n\n          annotations: ${{ steps.meta.outputs.annotations }}\n\n          push: true\nNote\n\nThe build must produce the components that you want to annotate. For example, to annotate an image index, the build must produce an index. If the build produces only a manifest and you specify index or index-descriptor, the build fails.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nConfigure annotation level\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995765,
    "timestamp": "2026-02-07T06:34:33.466Z",
    "title": "Attestations | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/attestations/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nAttestations\nAdd SBOM and provenance attestations with GitHub Actions\nCopy as Markdown\n\nSoftware Bill of Material (SBOM) and provenance attestations add metadata about the contents of your image, and how it was built.\n\nAttestations are supported with version 4 and later of the docker/build-push-action.\n\nDefault provenance\n\nThe docker/build-push-action GitHub Action automatically adds provenance attestations to your image, with the following conditions:\n\nIf the GitHub repository is public, provenance attestations with mode=max are automatically added to the image.\nIf the GitHub repository is private, provenance attestations with mode=min are automatically added to the image.\nIf you're using the docker exporter, or you're loading the build results to the runner with load: true, no attestations are added to the image. These output formats don't support attestations.\nWarning\n\nIf you're using docker/build-push-action to build images for code in a public GitHub repository, the provenance attestations attached to your image by default contains the values of build arguments. If you're misusing build arguments to pass secrets to your build, such as user credentials or authentication tokens, those secrets are exposed in the provenance attestation. Refactor your build to pass those secrets using secret mounts instead. Also remember to rotate any secrets you may have exposed.\n\nMax-level provenance\n\nIt's recommended that you build your images with max-level provenance attestations. Private repositories only add min-level provenance by default, but you can manually override the provenance level by setting the provenance input on the docker/build-push-action GitHub Action to mode=max.\n\nNote that adding attestations to an image means you must push the image to a registry directly, as opposed to loading the image to the local image store of the runner. This is because the local image store doesn't support loading images with attestations.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\nenv:\n\n  IMAGE_NAME: user/app\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      \n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Extract metadata\n\n        id: meta\n\n        uses: docker/metadata-action@v5\n\n        with:\n\n          images: ${{ env.IMAGE_NAME }}\n\n\n\n      - name: Build and push image\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: true\n\n          provenance: mode=max\n\n          tags: ${{ steps.meta.outputs.tags }}\nSBOM\n\nSBOM attestations aren't automatically added to the image. To add SBOM attestations, set the sbom input of the docker/build-push-action to true.\n\nNote that adding attestations to an image means you must push the image to a registry directly, as opposed to loading the image to the local image store of the runner. This is because the local image store doesn't support loading images with attestations.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\nenv:\n\n  IMAGE_NAME: user/app\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Extract metadata\n\n        id: meta\n\n        uses: docker/metadata-action@v5\n\n        with:\n\n          images: ${{ env.IMAGE_NAME }}\n\n\n\n      - name: Build and push image\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          sbom: true\n\n          push: true\n\n          tags: ${{ steps.meta.outputs.tags }}\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDefault provenance\nMax-level provenance\nSBOM\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995768,
    "timestamp": "2026-02-07T06:34:33.480Z",
    "title": "Build checks | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/checks/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nBuild checks\nValidating build configuration with GitHub Actions\nCopy as Markdown\n\nBuild checks let you validate your docker build configuration without actually running the build.\n\nRun checks with docker/build-push-action\n\nTo run build checks in a GitHub Actions workflow with the build-push-action, set the call input parameter to check. With this set, the workflow fails if any check warnings are detected for your build's configuration.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      \n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Validate build configuration\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          call: check\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: true\n\n          tags: user/app:latest\nRun checks with docker/bake-action\n\nIf you're using Bake and docker/bake-action to run your builds, you don't need to specify any special inputs in your GitHub Actions workflow configuration. Instead, define a Bake target that calls the check method, and invoke that target in your CI.\n\ntarget \"build\" {\n\n  dockerfile = \"Dockerfile\"\n\n  args = {\n\n    FOO = \"bar\"\n\n  }\n\n}\n\ntarget \"validate-build\" {\n\n  inherits = [\"build\"]\n\n  call = \"check\"\n\n}\nname: ci\n\n\n\non:\n\n  push:\n\n\n\nenv:\n\n  IMAGE_NAME: user/app\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Validate build configuration\n\n        uses: docker/bake-action@v6\n\n        with:\n\n          targets: validate-build\n\n\n\n      - name: Build\n\n        uses: docker/bake-action@v6\n\n        with:\n\n          targets: build\n\n          push: true\nUsing the call input directly\n\nYou can also set the build method with the call input which is equivalent to using the --call flag with docker buildx bake\n\nFor example, to run a check without defining call in your Bake file:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Validate build configuration\n\n        uses: docker/bake-action@v6\n\n        with:\n\n          targets: build\n\n          call: check\n\nEdit this page\n\nRequest changes\n\nTable of contents\nRun checks with docker/build-push-action\nRun checks with docker/bake-action\nUsing the call input directly\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995774,
    "timestamp": "2026-02-07T06:34:33.482Z",
    "title": "Build summary | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/build-summary/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nBuild summary\nGitHub Actions build summary\nCopy as Markdown\n\nDocker's GitHub Actions for building and pushing images generate a job summary for your build that outlines the execution and materials used:\n\nA summary showing the Dockerfile used, the build duration, and cache utilization\nInputs for the build, such as build arguments, tags, labels, and build contexts\nFor builds with Bake, the full bake definition for the build\n\nJob summaries for Docker builds appear automatically if you use the following versions of the Build and push Docker images or Docker Buildx Bake GitHub Actions:\n\ndocker/build-push-action@v6\ndocker/bake-action@v6\n\nTo view the job summary, open the details page for the job in GitHub after the job has finished. The summary is available for both failed and successful builds. In the case of a failed build, the summary also displays the error message that caused the build to fail:\n\nImport build records to Docker Desktop\nAvailability:\nBeta \nRequires:\nDocker Desktop 4.31 and later\n\nThe job summary includes a link for downloading a build record archive for the run. The build record archive is a ZIP file containing the details about a build (or builds, if you use docker/bake-action to build multiple targets). You can import this build record archive into Docker Desktop, which gives you a powerful, graphical interface for further analyzing the build's performance via the Docker Desktop Builds view.\n\nTo import the build record archive into Docker Desktop:\n\nDownload and install Docker Desktop.\n\nDownload the build record archive from the job summary in GitHub Actions.\n\nOpen the Builds view in Docker Desktop.\n\nSelect the Import build button, and then browse for the .zip archive job summary that you downloaded. Alternatively, you can drag-and-drop the build record archive ZIP file onto the Docker Desktop window after opening the import build dialog.\n\nSelect Import to add the build records.\n\nAfter a few seconds, the builds from the GitHub Actions run appear under the Completed builds tab in the Builds view. To inspect a build and see a detailed view of all the inputs, results, build steps, and cache utilization, select the item in the list.\n\nDisable job summary\n\nTo disable job summaries, set the DOCKER_BUILD_SUMMARY environment variable in the YAML configuration for your build step:\n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\n        env:\n\n          DOCKER_BUILD_SUMMARY: false\n\n        with:\n\n          tags: ${{ steps.meta.outputs.tags }}\n\n          labels: ${{ steps.meta.outputs.labels }}\nDisable build record upload\n\nTo disable the upload of the build record archive to GitHub, set the DOCKER_BUILD_RECORD_UPLOAD environment variable in the YAML configuration for your build step:\n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\n        env:\n\n          DOCKER_BUILD_RECORD_UPLOAD: false\n\n        with:\n\n          tags: ${{ steps.meta.outputs.tags }}\n\n          labels: ${{ steps.meta.outputs.labels }}\n\nWith this configuration, the build summary is still generated, but does not contain a link to download the build record archive.\n\nLimitations\n\nBuild summaries are currently not supported for:\n\nRepositories hosted on GitHub Enterprise Servers. Summaries can only be viewed for repositories hosted on GitHub.com.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nImport build records to Docker Desktop\nDisable job summary\nDisable build record upload\nLimitations\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995769,
    "timestamp": "2026-02-07T06:34:33.483Z",
    "title": "Build secrets | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/secrets/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nBuild secrets\nUsing secrets with GitHub Actions\nCopy as Markdown\n\nA build secret is sensitive information, such as a password or API token, consumed as part of the build process. Docker Build supports two forms of secrets:\n\nSecret mounts add secrets as files in the build container (under /run/secrets by default).\nSSH mounts add SSH agent sockets or keys into the build container.\n\nThis page shows how to use secrets with GitHub Actions. For an introduction to secrets in general, see Build secrets.\n\nSecret mounts\n\nIn the following example uses and exposes the GITHUB_TOKEN secret as provided by GitHub in your workflow.\n\nFirst, create a Dockerfile that uses the secret:\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nRUN --mount=type=secret,id=github_token,env=GITHUB_TOKEN ...\n\nIn this example, the secret name is github_token. The following workflow exposes this secret using the secrets input:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          platforms: linux/amd64,linux/arm64\n\n          tags: user/app:latest\n\n          secrets: |\n\n            \"github_token=${{ secrets.GITHUB_TOKEN }}\"\nNote\n\nYou can also expose a secret file to the build with the secret-files input:\n\nsecret-files: |\n\n  \"MY_SECRET=./secret.txt\"\n\nIf you're using GitHub secrets and need to handle multi-line value, you will need to place the key-value pair between quotes:\n\nsecrets: |\n\n  \"MYSECRET=${{ secrets.GPG_KEY }}\"\n\n  GIT_AUTH_TOKEN=abcdefghi,jklmno=0123456789\n\n  \"MYSECRET=aaaaaaaa\n\n  bbbbbbb\n\n  ccccccccc\"\n\n  FOO=bar\n\n  \"EMPTYLINE=aaaa\n\n\n\n  bbbb\n\n  ccc\"\n\n  \"JSON_SECRET={\"\"key1\"\":\"\"value1\"\",\"\"key2\"\":\"\"value2\"\"}\"\nKey\tValue\nMYSECRET\t***********************\nGIT_AUTH_TOKEN\tabcdefghi,jklmno=0123456789\nMYSECRET\taaaaaaaa\\nbbbbbbb\\nccccccccc\nFOO\tbar\nEMPTYLINE\taaaa\\n\\nbbbb\\nccc\nJSON_SECRET\t{\"key1\":\"value1\",\"key2\":\"value2\"}\nNote\n\nDouble escapes are needed for quote signs.\n\nSSH mounts\n\nSSH mounts let you authenticate with SSH servers. For example to perform a git clone, or to fetch application packages from a private repository.\n\nThe following Dockerfile example uses an SSH mount to fetch Go modules from a private GitHub repository.\n\nShow more\n# syntax=docker/dockerfile:1\n\n\n\nARG GO_VERSION=\"1.25\"\n\n\n\nFROM golang:${GO_VERSION}-alpine AS base\n\nENV CGO_ENABLED=0\n\nENV GOPRIVATE=\"github.com/foo/*\"\n\nRUN apk add --no-cache file git rsync openssh-client\n\nRUN mkdir -p -m 0700 ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts\n\nWORKDIR /src\n\n\n\nFROM base AS vendor\n\n# this step configure git and checks the ssh key is loaded\n\nRUN --mount=type=ssh <<EOT\n\n  set -e\n\n  echo \"Setting Git SSH protocol\"\n\n  git config --global url.\"git@github.com:\".insteadOf \"https://github.com/\"\n\n  (\n\n    set +e\n\n    ssh -T git@github.com\n\n    if [ ! \"$?\" = \"1\" ]; then\n\n      echo \"No GitHub SSH key loaded exiting...\"\n\n      exit 1\n\n    fi\n\n  )\n\nEOT\n\n# this one download go modules\n\nRUN --mount=type=bind,target=. \\\n\n    --mount=type=cache,target=/go/pkg/mod \\\n\n    --mount=type=ssh \\\n\n    go mod download -x\n\n\n\nFROM vendor AS build\n\nRUN --mount=type=bind,target=. \\\n\n    --mount=type=cache,target=/go/pkg/mod \\\n\n    --mount=type=cache,target=/root/.cache \\\n\n    go build ...\n\nTo build this Dockerfile, you must specify an SSH mount that the builder can use in the steps with --mount=type=ssh.\n\nThe following GitHub Action workflow uses the MrSquaare/ssh-setup-action third-party action to bootstrap SSH setup on the GitHub runner. The action creates a private key defined by the GitHub Action secret SSH_GITHUB_PPK and adds it to the SSH agent socket file at SSH_AUTH_SOCK. The SSH mount in the build step assume SSH_AUTH_SOCK by default, so there's no need to specify the ID or path for the SSH agent socket explicitly.\n\ndocker/build-push-action docker/bake-action\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up SSH\n\n        uses: MrSquaare/ssh-setup-action@2d028b70b5e397cf8314c6eaea229a6c3e34977a # v3.1.0\n\n        with:\n\n          host: github.com\n\n          private-key: ${{ secrets.SSH_GITHUB_PPK }}\n\n          private-key-name: github-ppk\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          ssh: default\n\n          push: true\n\n          tags: user/app:latest\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSecret mounts\nSSH mounts\nSecrets\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995775,
    "timestamp": "2026-02-07T06:34:33.496Z",
    "title": "BuildKit configuration | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/configure-builder/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nBuildKit configuration\nConfiguring your GitHub Actions builder\nCopy as Markdown\n\nThis page contains instructions on configuring your BuildKit instances when using our Setup Buildx Action.\n\nVersion pinning\n\nBy default, the action will attempt to use the latest version of Buildx available on the GitHub Runner (the build client) and the latest release of BuildKit (the build server).\n\nTo pin to a specific version of Buildx, use the version input. For example, to pin to Buildx v0.10.0:\n\n- name: Set up Docker Buildx\n\n  uses: docker/setup-buildx-action@v3\n\n  with:\n\n    version: v0.10.0\n\nTo pin to a specific version of BuildKit, use the image option in the driver-opts input. For example, to pin to BuildKit v0.11.0:\n\n- name: Set up Docker Buildx\n\n  uses: docker/setup-buildx-action@v3\n\n  with:\n\n    driver-opts: image=moby/buildkit:v0.11.0\nBuildKit container logs\n\nTo display BuildKit container logs when using the docker-container driver, you must either enable step debug logging, or set the --debug buildkitd flag in the Docker Setup Buildx action:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  buildx:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          buildkitd-flags: --debug\n\n      \n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\nLogs will be available at the end of a job:\n\nBuildKit Daemon configuration\n\nYou can provide a BuildKit configuration to your builder if you're using the docker-container driver (default) with the config or buildkitd-config-inline inputs:\n\nRegistry mirror\n\nYou can configure a registry mirror using an inline block directly in your workflow with the buildkitd-config-inline input:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  buildx:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          buildkitd-config-inline: |\n\n            [registry.\"docker.io\"]\n\n              mirrors = [\"mirror.gcr.io\"]\n\nFor more information about using a registry mirror, see Registry mirror.\n\nMax parallelism\n\nYou can limit the parallelism of the BuildKit solver which is particularly useful for low-powered machines.\n\nYou can use the buildkitd-config-inline input like the previous example, or you can use a dedicated BuildKit config file from your repository if you want with the config input:\n\n# .github/buildkitd.toml\n\n[worker.oci]\n\n  max-parallelism = 4\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  buildx:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          config: .github/buildkitd.toml\nAppend additional nodes to the builder\n\nBuildx supports running builds on multiple machines. This is useful for building multi-platform images on native nodes for more complicated cases that aren't handled by QEMU. Building on native nodes generally has better performance, and allows you to distribute the build across multiple machines.\n\nYou can append nodes to the builder you're creating using the append option. It takes input in the form of a YAML string document to remove limitations intrinsically linked to GitHub Actions: you can only use strings in the input fields:\n\nName\tType\tDescription\nname\tString\tName of the node. If empty, it's the name of the builder it belongs to, with an index number suffix. This is useful to set it if you want to modify/remove a node in an underlying step of you workflow.\nendpoint\tString\tDocker context or endpoint of the node to add to the builder\ndriver-opts\tList\tList of additional driver-specific options\nbuildkitd-flags\tString\tFlags for buildkitd daemon\nplatforms\tString\tFixed platforms for the node. If not empty, values take priority over the detected ones.\n\nHere is an example using remote nodes with the remote driver and TLS authentication:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  buildx:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          driver: remote\n\n          endpoint: tcp://oneprovider:1234\n\n          append: |\n\n            - endpoint: tcp://graviton2:1234\n\n              platforms: linux/arm64\n\n            - endpoint: tcp://linuxone:1234\n\n              platforms: linux/s390x\n\n        env:\n\n          BUILDER_NODE_0_AUTH_TLS_CACERT: ${{ secrets.ONEPROVIDER_CA }}\n\n          BUILDER_NODE_0_AUTH_TLS_CERT: ${{ secrets.ONEPROVIDER_CERT }}\n\n          BUILDER_NODE_0_AUTH_TLS_KEY: ${{ secrets.ONEPROVIDER_KEY }}\n\n          BUILDER_NODE_1_AUTH_TLS_CACERT: ${{ secrets.GRAVITON2_CA }}\n\n          BUILDER_NODE_1_AUTH_TLS_CERT: ${{ secrets.GRAVITON2_CERT }}\n\n          BUILDER_NODE_1_AUTH_TLS_KEY: ${{ secrets.GRAVITON2_KEY }}\n\n          BUILDER_NODE_2_AUTH_TLS_CACERT: ${{ secrets.LINUXONE_CA }}\n\n          BUILDER_NODE_2_AUTH_TLS_CERT: ${{ secrets.LINUXONE_CERT }}\n\n          BUILDER_NODE_2_AUTH_TLS_KEY: ${{ secrets.LINUXONE_KEY }}\nAuthentication for remote builders\n\nThe following examples show how to handle authentication for remote builders, using SSH or TLS.\n\nSSH authentication\n\nTo be able to connect to an SSH endpoint using the docker-container driver, you have to set up the SSH private key and configuration on the GitHub Runner:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  buildx:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up SSH\n\n        uses: MrSquaare/ssh-setup-action@2d028b70b5e397cf8314c6eaea229a6c3e34977a # v3.1.0\n\n        with:\n\n          host: graviton2\n\n          private-key: ${{ secrets.SSH_PRIVATE_KEY }}\n\n          private-key-name: aws_graviton2\n\n      \n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          endpoint: ssh://me@graviton2\nTLS authentication\n\nYou can also set up a remote BuildKit instance using the remote driver. To ease the integration in your workflow, you can use an environment variables that sets up authentication using the BuildKit client certificates for the tcp://:\n\nBUILDER_NODE_<idx>_AUTH_TLS_CACERT\nBUILDER_NODE_<idx>_AUTH_TLS_CERT\nBUILDER_NODE_<idx>_AUTH_TLS_KEY\n\nThe <idx> placeholder is the position of the node in the list of nodes.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  buildx:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          driver: remote\n\n          endpoint: tcp://graviton2:1234\n\n        env:\n\n          BUILDER_NODE_0_AUTH_TLS_CACERT: ${{ secrets.GRAVITON2_CA }}\n\n          BUILDER_NODE_0_AUTH_TLS_CERT: ${{ secrets.GRAVITON2_CERT }}\n\n          BUILDER_NODE_0_AUTH_TLS_KEY: ${{ secrets.GRAVITON2_KEY }}\nStandalone mode\n\nIf you don't have the Docker CLI installed on the GitHub Runner, the Buildx binary gets invoked directly, instead of calling it as a Docker CLI plugin. This can be useful if you want to use the kubernetes driver in your self-hosted runner:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  buildx:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Checkout\n\n        uses: actions/checkout@v4\n\n      \n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          driver: kubernetes\n\n      \n\n      - name: Build\n\n        run: |\n\n          buildx build .\nIsolated builders\n\nThe following example shows how you can select different builders for different jobs.\n\nAn example scenario where this might be useful is when you are using a monorepo, and you want to pinpoint different packages to specific builders. For example, some packages may be particularly resource-intensive to build and require more compute. Or they require a builder equipped with a particular capability or hardware.\n\nFor more information about remote builder, see remote driver and the append builder nodes example.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up builder1\n\n        uses: docker/setup-buildx-action@v3\n\n        id: builder1\n\n      \n\n      - name: Set up builder2\n\n        uses: docker/setup-buildx-action@v3\n\n        id: builder2\n\n      \n\n      - name: Build against builder1\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          builder: ${{ steps.builder1.outputs.name }}\n\n          target: mytarget1\n\n      \n\n      - name: Build against builder2\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          builder: ${{ steps.builder2.outputs.name }}\n\n          target: mytarget2\n\nEdit this page\n\nRequest changes\n\nTable of contents\nVersion pinning\nBuildKit container logs\nBuildKit Daemon configuration\nRegistry mirror\nMax parallelism\nAppend additional nodes to the builder\nAuthentication for remote builders\nSSH authentication\nTLS authentication\nStandalone mode\nIsolated builders\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995783,
    "timestamp": "2026-02-07T06:34:33.496Z",
    "title": "Copy image between registries | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/copy-image-registries/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nCopy image between registries\nCopy image between registries with GitHub Actions\nCopy as Markdown\n\nMulti-platform images built using Buildx can be copied from one registry to another using the buildx imagetools create command:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Login to GitHub Container Registry\n\n        uses: docker/login-action@v3\n\n        with:\n\n          registry: ghcr.io\n\n          username: ${{ github.repository_owner }}\n\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n      \n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          platforms: linux/amd64,linux/arm64\n\n          push: true\n\n          tags: |\n\n            user/app:latest\n\n            user/app:1.0.0\n\n\n\n      - name: Push image to GHCR\n\n        run: |\n\n          docker buildx imagetools create \\\n\n            --tag ghcr.io/user/app:latest \\\n\n            --tag ghcr.io/user/app:1.0.0 \\\n\n            user/app:latest\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995780,
    "timestamp": "2026-02-07T06:34:33.497Z",
    "title": "Cache management | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/cache/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nCache management\nCache management with GitHub Actions\nCopy as Markdown\n\nThis page contains examples on using the cache storage backends with GitHub Actions.\n\nNote\n\nSee Cache storage backends for more details about cache storage backends.\n\nInline cache\n\nIn most cases you want to use the inline cache exporter. However, note that the inline cache exporter only supports min cache mode. To use max cache mode, push the image and the cache separately using the registry cache exporter with the cache-to option, as shown in the registry cache example.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: true\n\n          tags: user/app:latest\n\n          cache-from: type=registry,ref=user/app:latest\n\n          cache-to: type=inline\nRegistry cache\n\nYou can import/export cache from a cache manifest or (special) image configuration on the registry with the registry cache exporter.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: true\n\n          tags: user/app:latest\n\n          cache-from: type=registry,ref=user/app:buildcache\n\n          cache-to: type=registry,ref=user/app:buildcache,mode=max\nGitHub cache\nCache backend API\nAvailability:\nExperimental \n\nThe GitHub Actions cache exporter backend uses the GitHub Cache service API to fetch and upload cache blobs. That's why you should only use this cache backend in a GitHub Action workflow, as the url ($ACTIONS_RESULTS_URL) and token ($ACTIONS_RUNTIME_TOKEN) attributes only get populated in a workflow context.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: true\n\n          tags: user/app:latest\n\n          cache-from: type=gha\n\n          cache-to: type=gha,mode=max\nImportant\n\nStarting April 15th, 2025, only GitHub Cache service API v2 will be supported.\n\nIf you encounter the following error during your build:\n\nERROR: failed to solve: This legacy service is shutting down, effective April 15, 2025. Migrate to the new service ASAP. For more information: https://gh.io/gha-cache-sunset\n\n\nYou're probably using outdated tools that only support the legacy GitHub Cache service API v1. Here are the minimum versions you need to upgrade to depending on your use case:\n\nDocker Buildx >= v0.21.0\nBuildKit >= v0.20.0\nDocker Compose >= v2.33.1\nDocker Engine >= v28.0.0 (if you're building using the Docker driver with containerd image store enabled)\n\nIf you're building using the docker/build-push-action or docker/bake-action actions on GitHub hosted runners, Docker Buildx and BuildKit are already up to date but on self-hosted runners, you may need to update them yourself. Alternatively, you can use the docker/setup-buildx-action action to install the latest version of Docker Buildx:\n\n- name: Set up Docker Buildx\n\n  uses: docker/setup-buildx-action@v3\n\n  with:\n\n   version: latest\n\nIf you're building using Docker Compose, you can use the docker/setup-compose-action action:\n\n- name: Set up Docker Compose\n\n  uses: docker/setup-compose-action@v1\n\n  with:\n\n   version: latest\n\nIf you're building using the Docker Engine with the containerd image store enabled, you can use the docker/setup-docker-action action:\n\n-\n\n  name: Set up Docker\n\n  uses: docker/setup-docker-action@v4\n\n  with:\n\n    version: latest\n\n    daemon-config: |\n\n      {\n\n        \"features\": {\n\n          \"containerd-snapshotter\": true\n\n        }\n\n      }\nCache mounts\n\nBuildKit doesn't preserve cache mounts in the GitHub Actions cache by default. To put your cache mounts into GitHub Actions cache and reuse it between builds, you can use a workaround provided by reproducible-containers/buildkit-cache-dance.\n\nThis GitHub Action creates temporary containers to extract and inject the cache mount data with your Docker build steps.\n\nThe following example shows how to use this workaround with a Go project.\n\nExample Dockerfile in build/package/Dockerfile\n\nFROM golang:1.21.1-alpine as base-build\n\n\n\nWORKDIR /build\n\n\n\nRUN --mount=type=cache,target=/go/pkg/mod \\\n\n    --mount=type=bind,source=go.mod,target=go.mod \\\n\n    --mount=type=bind,source=go.sum,target=go.sum \\\n\n    go mod download\n\n\n\nRUN --mount=type=cache,target=/go/pkg/mod \\\n\n    --mount=type=cache,target=/root/.cache/go-build \\\n\n    --mount=type=bind,target=. \\\n\n    go build -o /bin/app ./src\n\n...\n\nExample CI action\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Docker meta\n\n        id: meta\n\n        uses: docker/metadata-action@v5\n\n        with:\n\n          images: user/app\n\n          tags: |\n\n            type=ref,event=branch\n\n            type=ref,event=pr\n\n            type=semver,pattern={{version}}\n\n            type=semver,pattern={{major}}.{{minor}}\n\n\n\n      - name: Go Build Cache for Docker\n\n        uses: actions/cache@v4\n\n        with:\n\n          path: go-build-cache\n\n          key: ${{ runner.os }}-go-build-cache-${{ hashFiles('**/go.sum') }}\n\n\n\n      - name: Inject go-build-cache\n\n        uses: reproducible-containers/buildkit-cache-dance@4b2444fec0c0fb9dbf175a96c094720a692ef810 # v2.1.4\n\n        with:\n\n          cache-source: go-build-cache\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          cache-from: type=gha\n\n          cache-to: type=gha,mode=max\n\n          file: build/package/Dockerfile\n\n          push: ${{ github.event_name != 'pull_request' }}\n\n          tags: ${{ steps.meta.outputs.tags }}\n\n          labels: ${{ steps.meta.outputs.labels }}\n\n          platforms: linux/amd64,linux/arm64\n\nFor more information about this workaround, refer to the GitHub repository.\n\nLocal cache\nWarning\n\nAt the moment, old cache entries aren't deleted, so the cache size keeps growing. The following example uses the Move cache step as a workaround (see moby/buildkit#1896 for more info).\n\nYou can also leverage GitHub cache using the actions/cache and local cache exporter with this action:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Cache Docker layers\n\n        uses: actions/cache@v4\n\n        with:\n\n          path: ${{ runner.temp }}/.buildx-cache\n\n          key: ${{ runner.os }}-buildx-${{ github.sha }}\n\n          restore-keys: |\n\n            ${{ runner.os }}-buildx-\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: true\n\n          tags: user/app:latest\n\n          cache-from: type=local,src=${{ runner.temp }}/.buildx-cache\n\n          cache-to: type=local,dest=${{ runner.temp }}/.buildx-cache-new,mode=max\n\n\n\n      - # Temp fix\n\n        # https://github.com/docker/build-push-action/issues/252\n\n        # https://github.com/moby/buildkit/issues/1896\n\n        name: Move cache\n\n        run: |\n\n          rm -rf ${{ runner.temp }}/.buildx-cache\n\n          mv ${{ runner.temp }}/.buildx-cache-new ${{ runner.temp }}/.buildx-cache\n\nEdit this page\n\nRequest changes\n\nTable of contents\nInline cache\nRegistry cache\nGitHub cache\nCache backend API\nCache mounts\nLocal cache\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995787,
    "timestamp": "2026-02-07T06:34:33.515Z",
    "title": "Local registry | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/local-registry/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nLocal registry\nLocal registry with GitHub Actions\nCopy as Markdown\n\nFor testing purposes you may need to create a local registry to push images into:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    services:\n\n      registry:\n\n        image: registry:3\n\n        ports:\n\n          - 5000:5000\n\n    steps:\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n      \n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          driver-opts: network=host\n\n      \n\n      - name: Build and push to local registry\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: true\n\n          tags: localhost:5000/name/app:latest\n\n      \n\n      - name: Inspect\n\n        run: |\n\n          docker buildx imagetools inspect localhost:5000/name/app:latest\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995792,
    "timestamp": "2026-02-07T06:34:33.517Z",
    "title": "Multi-platform image | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/multi-platform/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nMulti-platform image\nMulti-platform image with GitHub Actions\nCopy as Markdown\n\nYou can build multi-platform images using the platforms option, as shown in the following example:\n\nNote\nFor a list of available platforms, see the Docker Setup Buildx action.\nIf you want support for more platforms, you can use QEMU with the Docker Setup QEMU action.\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          platforms: linux/amd64,linux/arm64\n\n          push: true\n\n          tags: user/app:latest\nBuild and load multi-platform images\n\nThe default Docker setup for GitHub Actions runners does not support loading multi-platform images to the local image store of the runner after building them. To load a multi-platform image, you need to enable the containerd image store option for the Docker Engine.\n\nThere is no way to configure the default Docker setup in the GitHub Actions runners directly, but you can use docker/setup-docker-action to customize the Docker Engine and CLI settings for a job.\n\nThe following example workflow enables the containerd image store, builds a multi-platform image, and loads the results into the GitHub runner's local image store.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker\n\n        uses: docker/setup-docker-action@v4\n\n        with:\n\n          daemon-config: |\n\n            {\n\n              \"debug\": true,\n\n              \"features\": {\n\n                \"containerd-snapshotter\": true\n\n              }\n\n            }\n\n\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          platforms: linux/amd64,linux/arm64\n\n          load: true\n\n          tags: user/app:latest\nDistribute build across multiple runners\n\nBuilding multiple platforms on the same runner can significantly extend build times, particularly when dealing with complex Dockerfiles or a high number of target platforms. By distributing platform-specific builds across multiple runners using a matrix strategy, you can drastically reduce build durations and streamline your CI pipeline. These examples demonstrate how to allocate each platform build to a dedicated runner, including ARM-native runners where applicable, and create a unified manifest list using the buildx imagetools create command.\n\nThe following workflow will build the image for each platform on a dedicated runner using a matrix strategy and push by digest. Then, the merge job will create manifest lists and push them to Docker Hub. The metadata action is used to set tags and labels.\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\nenv:\n\n  REGISTRY_IMAGE: user/app\n\n\n\njobs:\n\n  build:\n\n    strategy:\n\n      fail-fast: false\n\n      matrix:\n\n        include:\n\n        - platform: linux/amd64\n\n          runner: ubuntu-latest\n\n        - platform: linux/arm64\n\n          runner: ubuntu-24.04-arm\n\n    runs-on: ${{ matrix.runner }}\n\n    steps:\n\n      - name: Prepare\n\n        run: |\n\n          platform=${{ matrix.platform }}\n\n          echo \"PLATFORM_PAIR=${platform//\\//-}\" >> $GITHUB_ENV\n\n\n\n      - name: Docker meta\n\n        id: meta\n\n        uses: docker/metadata-action@v5\n\n        with:\n\n          images: ${{ env.REGISTRY_IMAGE }}\n\n\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push by digest\n\n        id: build\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          platforms: ${{ matrix.platform }}\n\n          labels: ${{ steps.meta.outputs.labels }}\n\n          tags: ${{ env.REGISTRY_IMAGE }}\n\n          outputs: type=image,push-by-digest=true,name-canonical=true,push=true\n\n\n\n      - name: Export digest\n\n        run: |\n\n          mkdir -p ${{ runner.temp }}/digests\n\n          digest=\"${{ steps.build.outputs.digest }}\"\n\n          touch \"${{ runner.temp }}/digests/${digest#sha256:}\"\n\n\n\n      - name: Upload digest\n\n        uses: actions/upload-artifact@v4\n\n        with:\n\n          name: digests-${{ env.PLATFORM_PAIR }}\n\n          path: ${{ runner.temp }}/digests/*\n\n          if-no-files-found: error\n\n          retention-days: 1\n\n\n\n  merge:\n\n    runs-on: ubuntu-latest\n\n    needs:\n\n      - build\n\n    steps:\n\n      - name: Download digests\n\n        uses: actions/download-artifact@v4\n\n        with:\n\n          path: ${{ runner.temp }}/digests\n\n          pattern: digests-*\n\n          merge-multiple: true\n\n\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Docker meta\n\n        id: meta\n\n        uses: docker/metadata-action@v5\n\n        with:\n\n          images: ${{ env.REGISTRY_IMAGE }}\n\n          tags: |\n\n            type=ref,event=branch\n\n            type=ref,event=pr\n\n            type=semver,pattern={{version}}\n\n            type=semver,pattern={{major}}.{{minor}}\n\n\n\n      - name: Create manifest list and push\n\n        working-directory: ${{ runner.temp }}/digests\n\n        run: |\n\n          docker buildx imagetools create $(jq -cr '.tags | map(\"-t \" + .) | join(\" \")' <<< \"$DOCKER_METADATA_OUTPUT_JSON\") \\\n\n            $(printf '${{ env.REGISTRY_IMAGE }}@sha256:%s ' *)\n\n\n\n      - name: Inspect image\n\n        run: |\n\n          docker buildx imagetools inspect ${{ env.REGISTRY_IMAGE }}:${{ steps.meta.outputs.version }}\nWith Bake\n\nIt's also possible to build on multiple runners using Bake, with the bake action.\n\nYou can find a live example in this GitHub repository.\n\nThe following example achieves the same results as described in the previous section.\n\nvariable \"DEFAULT_TAG\" {\n\n  default = \"app:local\"\n\n}\n\n\n\n// Special target: https://github.com/docker/metadata-action#bake-definition\n\ntarget \"docker-metadata-action\" {\n\n  tags = [\"${DEFAULT_TAG}\"]\n\n}\n\n\n\n// Default target if none specified\n\ngroup \"default\" {\n\n  targets = [\"image-local\"]\n\n}\n\n\n\ntarget \"image\" {\n\n  inherits = [\"docker-metadata-action\"]\n\n}\n\n\n\ntarget \"image-local\" {\n\n  inherits = [\"image\"]\n\n  output = [\"type=docker\"]\n\n}\n\n\n\ntarget \"image-all\" {\n\n  inherits = [\"image\"]\n\n  platforms = [\n\n    \"linux/amd64\",\n\n    \"linux/arm/v6\",\n\n    \"linux/arm/v7\",\n\n    \"linux/arm64\"\n\n  ]\n\n}\nname: ci\n\n\n\non:\n\n  push:\n\n\n\nenv:\n\n  REGISTRY_IMAGE: user/app\n\n\n\njobs:\n\n  prepare:\n\n    runs-on: ubuntu-latest\n\n    outputs:\n\n      matrix: ${{ steps.platforms.outputs.matrix }}\n\n    steps:\n\n      - name: Checkout\n\n        uses: actions/checkout@v4\n\n\n\n      - name: Create matrix\n\n        id: platforms\n\n        run: |\n\n          echo \"matrix=$(docker buildx bake image-all --print | jq -cr '.target.\"image-all\".platforms')\" >>${GITHUB_OUTPUT}\n\n\n\n      - name: Show matrix\n\n        run: |\n\n          echo ${{ steps.platforms.outputs.matrix }}\n\n\n\n      - name: Docker meta\n\n        id: meta\n\n        uses: docker/metadata-action@v5\n\n        with:\n\n          images: ${{ env.REGISTRY_IMAGE }}\n\n\n\n      - name: Rename meta bake definition file\n\n        run: |\n\n          mv \"${{ steps.meta.outputs.bake-file }}\" \"${{ runner.temp }}/bake-meta.json\"\n\n\n\n      - name: Upload meta bake definition\n\n        uses: actions/upload-artifact@v4\n\n        with:\n\n          name: bake-meta\n\n          path: ${{ runner.temp }}/bake-meta.json\n\n          if-no-files-found: error\n\n          retention-days: 1\n\n\n\n  build:\n\n    needs:\n\n      - prepare\n\n    strategy:\n\n      fail-fast: false\n\n      matrix:\n\n        platform: ${{ fromJson(needs.prepare.outputs.matrix) }}\n\n    runs-on: ${{ startsWith(matrix.platform, 'linux/arm') && 'ubuntu-24.04-arm' || 'ubuntu-latest' }}\n\n    steps:\n\n      - name: Prepare\n\n        run: |\n\n          platform=${{ matrix.platform }}\n\n          echo \"PLATFORM_PAIR=${platform//\\//-}\" >> $GITHUB_ENV\n\n\n\n      - name: Download meta bake definition\n\n        uses: actions/download-artifact@v4\n\n        with:\n\n          name: bake-meta\n\n          path: ${{ runner.temp }}\n\n\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build\n\n        id: bake\n\n        uses: docker/bake-action@v6\n\n        with:\n\n          files: |\n\n            ./docker-bake.hcl\n\n            cwd://${{ runner.temp }}/bake-meta.json\n\n          targets: image\n\n          set: |\n\n            *.tags=${{ env.REGISTRY_IMAGE }}\n\n            *.platform=${{ matrix.platform }}\n\n            *.output=type=image,push-by-digest=true,name-canonical=true,push=true\n\n\n\n      - name: Export digest\n\n        run: |\n\n          mkdir -p ${{ runner.temp }}/digests\n\n          digest=\"${{ fromJSON(steps.bake.outputs.metadata).image['containerimage.digest'] }}\"\n\n          touch \"${{ runner.temp }}/digests/${digest#sha256:}\"\n\n\n\n      - name: Upload digest\n\n        uses: actions/upload-artifact@v4\n\n        with:\n\n          name: digests-${{ env.PLATFORM_PAIR }}\n\n          path: ${{ runner.temp }}/digests/*\n\n          if-no-files-found: error\n\n          retention-days: 1\n\n\n\n  merge:\n\n    runs-on: ubuntu-latest\n\n    needs:\n\n      - build\n\n    steps:\n\n      - name: Download meta bake definition\n\n        uses: actions/download-artifact@v4\n\n        with:\n\n          name: bake-meta\n\n          path: ${{ runner.temp }}\n\n\n\n      - name: Download digests\n\n        uses: actions/download-artifact@v4\n\n        with:\n\n          path: ${{ runner.temp }}/digests\n\n          pattern: digests-*\n\n          merge-multiple: true\n\n\n\n      - name: Login to DockerHub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Create manifest list and push\n\n        working-directory: ${{ runner.temp }}/digests\n\n        run: |\n\n          docker buildx imagetools create $(jq -cr '.target.\"docker-metadata-action\".tags | map(select(startswith(\"${{ env.REGISTRY_IMAGE }}\")) | \"-t \" + .) | join(\" \")' ${{ runner.temp }}/bake-meta.json) \\\n\n            $(printf '${{ env.REGISTRY_IMAGE }}@sha256:%s ' *)\n\n\n\n      - name: Inspect image\n\n        run: |\n\n          docker buildx imagetools inspect ${{ env.REGISTRY_IMAGE }}:$(jq -r '.target.\"docker-metadata-action\".args.DOCKER_META_VERSION' ${{ runner.temp }}/bake-meta.json)\n\nEdit this page\n\nRequest changes\n\nTable of contents\nBuild and load multi-platform images\nDistribute build across multiple runners\nWith Bake\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995784,
    "timestamp": "2026-02-07T06:34:33.517Z",
    "title": "Export to Docker | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/export-docker/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nExport to Docker\nExport to Docker with GitHub Actions\nCopy as Markdown\n\nYou may want your build result to be available in the Docker client through docker images to be able to use it in another step of your workflow:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n      \n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          load: true\n\n          tags: myimage:latest\n\n      \n\n      - name: Inspect\n\n        run: |\n\n          docker image inspect myimage:latest\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995795,
    "timestamp": "2026-02-07T06:34:33.534Z",
    "title": "Named contexts | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/named-contexts/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nNamed contexts\nNamed contexts with GitHub Actions\nCopy as Markdown\n\nYou can define additional build contexts, and access them in your Dockerfile with FROM name or --from=name. When Dockerfile defines a stage with the same name it's overwritten.\n\nThis can be useful with GitHub Actions to reuse results from other builds or pin an image to a specific tag in your workflow.\n\nPin image to a tag\n\nReplace alpine:latest with a pinned one:\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nRUN echo \"Hello World\"\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          build-contexts: |\n\n            alpine=docker-image://alpine:3.23\n\n          tags: myimage:latest\nUse image in subsequent steps\n\nBy default, the Docker Setup Buildx action uses docker-container as a build driver, so built Docker images aren't loaded automatically.\n\nWith named contexts you can reuse the built image:\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nRUN echo \"Hello World\"\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          driver: docker\n\n\n\n      - name: Build base image\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          context: \"{{defaultContext}}:base\"\n\n          load: true\n\n          tags: my-base-image:latest\n\n\n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          build-contexts: |\n\n            alpine=docker-image://my-base-image:latest\n\n          tags: myimage:latest\nUsing with a container builder\n\nAs shown in the previous section we are not using the default docker-container driver for building with named contexts. That's because this driver can't load an image from the Docker store as it's isolated. To solve this problem you can use a local registry to push your base image in your workflow:\n\n# syntax=docker/dockerfile:1\n\nFROM alpine\n\nRUN echo \"Hello World\"\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    services:\n\n      registry:\n\n        image: registry:3\n\n        ports:\n\n          - 5000:5000\n\n    steps:\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n        with:\n\n          # network=host driver-opt needed to push to local registry\n\n          driver-opts: network=host\n\n\n\n      - name: Build base image\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          context: \"{{defaultContext}}:base\"\n\n          tags: localhost:5000/my-base-image:latest\n\n          push: true\n\n\n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          build-contexts: |\n\n            alpine=docker-image://localhost:5000/my-base-image:latest\n\n          tags: myimage:latest\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPin image to a tag\nUse image in subsequent steps\nUsing with a container builder\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995796,
    "timestamp": "2026-02-07T06:34:33.535Z",
    "title": "Push to multiple registries | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/push-multi-registries/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nPush to multiple registries\nPush to multiple registries with GitHub Actions\nCopy as Markdown\n\nThe following workflow will connect you to Docker Hub and GitHub Container Registry, and push the image to both registries:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Login to GitHub Container Registry\n\n        uses: docker/login-action@v3\n\n        with:\n\n          registry: ghcr.io\n\n          username: ${{ github.repository_owner }}\n\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          platforms: linux/amd64,linux/arm64\n\n          push: true\n\n          tags: |\n\n            user/app:latest\n\n            user/app:1.0.0\n\n            ghcr.io/user/app:latest\n\n            ghcr.io/user/app:1.0.0\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995801,
    "timestamp": "2026-02-07T06:34:33.535Z",
    "title": "Reproducible builds | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/reproducible-builds/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nReproducible builds\nReproducible builds with GitHub Actions\nCopy as Markdown\n\nSOURCE_DATE_EPOCH is a standardized environment variable for instructing build tools to produce a reproducible output. Setting the environment variable for a build makes the timestamps in the image index, config, and file metadata reflect the specified Unix time.\n\nTo set the environment variable in GitHub Actions, use the built-in env property on the build step.\n\nUnix epoch timestamps\n\nThe following example sets the SOURCE_DATE_EPOCH variable to 0, Unix epoch.\n\ndocker/build-push-action docker/bake-action\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          tags: user/app:latest\n\n        env:\n\n          SOURCE_DATE_EPOCH: 0\nGit commit timestamps\n\nThe following example sets SOURCE_DATE_EPOCH to the Git commit timestamp.\n\ndocker/build-push-action docker/bake-action\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Get Git commit timestamps\n\n        run: echo \"TIMESTAMP=$(git log -1 --pretty=%ct)\" >> $GITHUB_ENV\n\n\n\n      - name: Build\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          tags: user/app:latest\n\n        env:\n\n          SOURCE_DATE_EPOCH: ${{ env.TIMESTAMP }}\nAdditional information\n\nFor more information about the SOURCE_DATE_EPOCH support in BuildKit, see BuildKit documentation.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUnix epoch timestamps\nGit commit timestamps\nAdditional information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995810,
    "timestamp": "2026-02-07T06:34:33.545Z",
    "title": "Test before push | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/test-before-push/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nTest before push\nTest before push with GitHub Actions\nCopy as Markdown\n\nIn some cases, you might want to validate that the image works as expected before pushing it. The following workflow implements several steps to achieve this:\n\nBuild and export the image to Docker\nTest your image\nMulti-platform build and push the image\nname: ci\n\n\n\non:\n\n  push:\n\n\n\nenv:\n\n  TEST_TAG: user/app:test\n\n  LATEST_TAG: user/app:latest\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and export to Docker\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          load: true\n\n          tags: ${{ env.TEST_TAG }}\n\n\n\n      - name: Test\n\n        run: |\n\n          docker run --rm ${{ env.TEST_TAG }}\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          platforms: linux/amd64,linux/arm64\n\n          push: true\n\n          tags: ${{ env.LATEST_TAG }}\nNote\n\nThe linux/amd64 image is only built once in this workflow. The image is built once, and the following steps use the internal cache from the first Build and push step. The second Build and push step only builds linux/arm64.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995807,
    "timestamp": "2026-02-07T06:34:33.547Z",
    "title": "Tags and labels | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/manage-tags-labels/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nTags and labels\nManage tags and labels with GitHub Actions\nCopy as Markdown\n\nIf you want an \"automatic\" tag management and OCI Image Format Specification for labels, you can do it in a dedicated setup step. The following workflow will use the Docker Metadata Action to handle tags and labels based on GitHub Actions events and Git metadata:\n\nname: ci\n\n\n\non:\n\n  schedule:\n\n    - cron: \"0 10 * * *\"\n\n  push:\n\n    branches:\n\n      - \"**\"\n\n    tags:\n\n      - \"v*.*.*\"\n\n  pull_request:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Docker meta\n\n        id: meta\n\n        uses: docker/metadata-action@v5\n\n        with:\n\n          # list of Docker images to use as base name for tags\n\n          images: |\n\n            name/app\n\n            ghcr.io/username/app\n\n          # generate Docker tags based on the following events/attributes\n\n          tags: |\n\n            type=schedule\n\n            type=ref,event=branch\n\n            type=ref,event=pr\n\n            type=semver,pattern={{version}}\n\n            type=semver,pattern={{major}}.{{minor}}\n\n            type=semver,pattern={{major}}\n\n            type=sha\n\n\n\n      - name: Login to Docker Hub\n\n        if: github.event_name != 'pull_request'\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Login to GHCR\n\n        if: github.event_name != 'pull_request'\n\n        uses: docker/login-action@v3\n\n        with:\n\n          registry: ghcr.io\n\n          username: ${{ github.repository_owner }}\n\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: ${{ github.event_name != 'pull_request' }}\n\n          tags: ${{ steps.meta.outputs.tags }}\n\n          labels: ${{ steps.meta.outputs.labels }}\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995804,
    "timestamp": "2026-02-07T06:34:33.548Z",
    "title": "Share image between jobs | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/share-image-jobs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nShare image between jobs\nShare built image between jobs with GitHub Actions\nCopy as Markdown\n\nAs each job is isolated in its own runner, you can't use your built image between jobs, except if you're using self-hosted runners or Docker Build Cloud. However, you can pass data between jobs in a workflow using the actions/upload-artifact and actions/download-artifact actions:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and export\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          tags: myimage:latest\n\n          outputs: type=docker,dest=${{ runner.temp }}/myimage.tar\n\n\n\n      - name: Upload artifact\n\n        uses: actions/upload-artifact@v4\n\n        with:\n\n          name: myimage\n\n          path: ${{ runner.temp }}/myimage.tar\n\n\n\n  use:\n\n    runs-on: ubuntu-latest\n\n    needs: build\n\n    steps:\n\n      - name: Download artifact\n\n        uses: actions/download-artifact@v4\n\n        with:\n\n          name: myimage\n\n          path: ${{ runner.temp }}\n\n\n\n      - name: Load image\n\n        run: |\n\n          docker load --input ${{ runner.temp }}/myimage.tar\n\n          docker image ls -a\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995813,
    "timestamp": "2026-02-07T06:34:33.555Z",
    "title": "Update Docker Hub description | Docker Docs",
    "url": "https://docs.docker.com/build/ci/github-actions/update-dockerhub-desc/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nGitHub Actions\nAnnotations\nAttestations\nBuild checks\nBuild secrets\nBuild summary\nBuildKit configuration\nCache management\nCopy image between registries\nExport to Docker\nLocal registry\nMulti-platform image\nNamed contexts\nPush to multiple registries\nReproducible builds\nShare image between jobs\nTags and labels\nTest before push\nUpdate Docker Hub description\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nCI\n/\nGitHub Actions\n/\nUpdate Docker Hub description\nUpdate Docker Hub description with GitHub Actions\nCopy as Markdown\n\nYou can update the Docker Hub repository description using a third party action called Docker Hub Description with this action:\n\nname: ci\n\n\n\non:\n\n  push:\n\n\n\njobs:\n\n  docker:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Login to Docker Hub\n\n        uses: docker/login-action@v3\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n\n\n      - name: Set up QEMU\n\n        uses: docker/setup-qemu-action@v3\n\n\n\n      - name: Set up Docker Buildx\n\n        uses: docker/setup-buildx-action@v3\n\n\n\n      - name: Build and push\n\n        uses: docker/build-push-action@v6\n\n        with:\n\n          push: true\n\n          tags: user/app:latest\n\n\n\n      - name: Update repo description\n\n        uses: peter-evans/dockerhub-description@e98e4d1628a5f3be2be7c231e50981aee98723ae # v4.0.0\n\n        with:\n\n          username: ${{ vars.DOCKERHUB_USERNAME }}\n\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n          repository: user/app\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995880,
    "timestamp": "2026-02-07T06:37:09.634Z",
    "title": "Validating builds | Docker Docs",
    "url": "https://docs.docker.com/build/policies/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\nValidating build inputs with policies\nCopy as Markdown\n\nBuilding with Docker often involves downloading remote resources. These external dependencies, such as Docker images, Git repositories, remote files, and other artifacts, are called build inputs.\n\nFor example:\n\nPulling images from a registry\nCloning a source code repository\nFetching files from a server over HTTPS\n\nWhen consuming build inputs, it's a good idea to verify the contents are what you expect them to be. One way to do this is to use the --checksum option for the ADD Dockerfile instruction. This lets you verify the SHA256 checksum of a remote resource when pulling it into a build:\n\nADD --checksum=sha256:c0ff3312345‚Ä¶ https://example.com/archive.tar.gz /\n\nIf the remote archive.tar.gz file does not match the checksum that the Dockerfile expects, the build fails.\n\nChecksums verify that content matches what you expect, but only for the ADD instruction. They don't tell you anything about where the content came from or how it was produced. You can't use checksums to enforce constraints like \"images must be signed\" or \"dependencies must come from approved sources.\"\n\nBuild policies solve this problem. They let you define rules that validate all your build inputs, enforcing requirements like provenance attestations, approved registries, and signed Git tags across your entire build process.\n\nPrerequisites\n\nBuild policies is currently an experimental feature. To try it out, you'll need:\n\nBuildx 0.31.0 or later - Check your version: docker buildx version\nBuildKit 0.27.0 or later - Verify with: docker buildx inspect --bootstrap\n\nIf you're using Docker Desktop, ensure you're on a version that includes these updates.\n\nBuild policies\n\nBuildx version 0.31.0 added support for build policies. Build policies are rules for securing your Docker build supply chain, and help protect against upstream compromises, malicious dependencies, and unauthorized modifications to your build inputs.\n\nBuild policies let you enforce extended verifications on inputs used to build your projects, such as:\n\nDocker images must use digest references (not tags alone)\nImages must have provenance attestations and cosign signatures\nGit tags are signed by maintainers with a PGP public key\nAll remote artifacts must use HTTPS and include a checksum for verification\n\nBuild policies are defined in a declarative policy language, called Rego, created for the Open Policy Agent (OPA). The following example shows a minimal build policy in Rego.\n\nDockerfile.rego\npackage docker\n\n\n\ndefault allow := false\n\n\n\n# Allow any local inputs for this build\n\n# For example: a local build context, or a local Dockerfile\n\nallow if input.local\n\n\n\n# Allow images, but only if they have provenance attestations\n\nallow if {\n\n    input.image.hasProvenance\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nIf the Dockerfile associated with this policy references an image with no provenance attestation in a FROM instruction, the policy would be violated and the build would fail.\n\nHow policies work\n\nWhen you run docker buildx build, Buildx:\n\nResolves all build inputs (images, Git repos, HTTP downloads)\nLooks for a policy file matching your Dockerfile name (e.g., Dockerfile.rego)\nEvaluates each input against the policy before the build starts\nAllows the build to proceed only if all inputs pass the policy\n\nPolicies are written in Rego (Open Policy Agent's policy language). You don't need to be a Rego expert - the Introduction tutorial teaches you everything needed.\n\nPolicy files live alongside your Dockerfile:\n\nproject/\n\n‚îú‚îÄ‚îÄ Dockerfile\n\n‚îú‚îÄ‚îÄ Dockerfile.rego\n\n‚îî‚îÄ‚îÄ src/\n\nNo additional configuration is needed - Buildx automatically finds and loads the policy when you build.\n\nUse cases\n\nBuild policies help you enforce security and compliance requirements on your Docker builds. Common scenarios where policies provide value:\n\nEnforce base image standards\n\nRequire all production Dockerfiles to use specific, approved base images with digest references. Prevent developers from using arbitrary images that haven't been vetted by your security team.\n\nValidate third-party dependencies\n\nWhen your build downloads files, libraries, or tools from the internet, verify they come from trusted sources and match expected checksums or signatures. This protects against supply chain attacks where an upstream dependency is compromised.\n\nEnsure signed releases\n\nRequire that all dependencies have valid signatures from trusted parties.\n\nCheck GPG signatures for Git repositories you clone in your builds\nVerify provenance attestation signatures with Sigstore\nMeet compliance requirements\n\nSome regulatory frameworks require evidence that you validate your build inputs. Build policies give you an auditable, declarative way to demonstrate you're checking dependencies against security standards.\n\nSeparate development and production rules\n\nApply stricter validation for production builds while allowing more flexibility during development. The same policy file can contain conditional rules based on build context or target.\n\nGet started\n\nReady to start writing policies? The Introduction tutorial walks you through creating your first policy and teaches the Rego basics you need.\n\nFor practical usage guidance, see Using build policies.\n\nFor practical examples you can copy and adapt, see the Example policies library.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nBuild policies\nHow policies work\nUse cases\nEnforce base image standards\nValidate third-party dependencies\nEnsure signed releases\nMeet compliance requirements\nSeparate development and production rules\nGet started\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995884,
    "timestamp": "2026-02-07T06:37:09.637Z",
    "title": "Usage | Docker Docs",
    "url": "https://docs.docker.com/build/policies/usage/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\n/\nUsage\nUsing build policies\nCopy as Markdown\n\nBuild policies validate inputs before builds execute. This guide covers how to develop policies iteratively and apply them to real builds with docker buildx build and docker buildx bake.\n\nPrerequisites\nBuildx 0.31.0 or later - Check your version: docker buildx version\nBuildKit 0.26.0 or later - Verify with: docker buildx inspect --bootstrap\n\nIf you're using Docker Desktop, ensure you're on a version that includes these updates.\n\nPolicy development workflow\n\nBuildx automatically loads policies that match your Dockerfile name. When you build with Dockerfile, Buildx looks for Dockerfile.rego in the same directory. For a file named app.Dockerfile, it looks for app.Dockerfile.rego. See the Advanced: Policy configuration section for configuration options and manual policy loading.\n\nWriting policies is an iterative process:\n\nStart with a basic deny-all policy.\nBuild with debug logging to see what inputs your Dockerfile uses.\nAdd rules to allow specific sources based on the debug output.\nTest and refine.\nViewing inputs from your Dockerfile\n\nTo see the inputs that your Dockerfile references (images, Git repos, HTTP downloads), build with debug logging:\n\n$ docker buildx build --progress=plain --policy log-level=debug .\n\n\nExample output for an image source:\n\n#1 0.010 checking policy for source docker-image://alpine:3.19 (linux/arm64)\n\n#1 0.011 policy input: {\n\n#1 0.011   \"env\": {\n\n#1 0.011     \"filename\": \".\"\n\n#1 0.011   },\n\n#1 0.011   \"image\": {\n\n#1 0.011     \"ref\": \"docker.io/library/alpine:3.19\",\n\n#1 0.011     \"host\": \"docker.io\",\n\n#1 0.011     \"repo\": \"alpine\",\n\n#1 0.011     \"tag\": \"3.19\",\n\n#1 0.011     \"platform\": \"linux/arm64\"\n\n#1 0.011   }\n\n#1 0.011 }\n\n#1 0.011 unknowns for policy evaluation: [input.image.checksum input.image.labels ...]\n\n#1 0.012 policy decision for source docker-image://alpine:3.19: ALLOW\n\nThis shows the complete input structure, which fields are unresolved, and the policy decision for each source. See Input reference for all available fields.\n\nTesting policies with policy eval\n\nUse docker buildx policy eval to test whether your policy allows a specific source without running a full build.\n\nNote: docker buildx policy eval tests the source specified as the argument. It doesn't parse your Dockerfile to evaluate all inputs - for that, build with --progress=plain.\n\nTest if your policy allows the local context:\n\n$ docker buildx policy eval .\n\n\nNo output means the policy allowed the source. If denied, you see:\n\nERROR: policy denied\n\n\nTest other sources:\n\n$ docker buildx policy eval https://example.com              # Test HTTP\n\n$ docker buildx policy eval https://github.com/org/repo.git  # Test Git\n\n\nBy default, --print shows reference information parsed from the source string (like repo, tag, host) without fetching from registries. To inspect metadata that requires fetching the source (like labels, checksum, or hasProvenance), specify which fields to fetch with --fields:\n\n$ docker buildx policy eval --print --fields image.labels docker-image://alpine:3.19\n\n\nMultiple fields can be specified as a comma-separated list.\n\nIterative development example\n\nHere's a practical workflow for developing policies:\n\nStart with basic deny-all policy:\n\nDockerfile.rego\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\ndecision := {\"allow\": allow}\n\nBuild with debug logging to see what inputs your Dockerfile uses:\n\n$ docker buildx build --progress=plain --policy log-level=debug .\n\n\nThe output shows the denied image and its input structure:\n\n#1 0.026 checking policy for source docker-image://docker.io/library/alpine:3.19\n\n#1 0.027 policy input: {\n\n#1 0.027   \"image\": {\n\n#1 0.027     \"repo\": \"alpine\",\n\n#1 0.027     \"tag\": \"3.19\",\n\n#1 0.027     ...\n\n#1 0.027   }\n\n#1 0.027 }\n\n#1 0.028 policy decision for source docker-image://alpine:3.19: DENY\n\n#1 ERROR: source \"docker-image://alpine:3.19\" not allowed by policy\n\nAdd a rule allowing the alpine image:\n\nallow if {\n\n    input.image.repo == \"alpine\"\n\n}\n\nBuild again to verify the policy works:\n\n$ docker buildx build .\n\n\nIf it fails, see Debugging for troubleshooting guidance.\n\nUsing policies with docker build\n\nOnce you've developed and tested your policy, apply it to real builds.\n\nBasic usage\n\nCreate a policy alongside your Dockerfile:\n\nDockerfile\nFROM alpine:3.19\n\nRUN echo \"hello\"\nDockerfile.rego\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n    input.image.repo == \"alpine\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nBuild normally:\n\n$ docker buildx build .\n\n\nBuildx loads the policy automatically and validates the alpine:3.19 image before building.\n\nBuild with different Dockerfile names\n\nSpecify the Dockerfile with -f:\n\n$ docker buildx build -f app.Dockerfile .\n\n\nBuildx looks for app.Dockerfile.rego in the same directory.\n\nBuild with manual policy\n\nAdd an extra policy to the automatic one:\n\n$ docker buildx build --policy filename=extra-checks.rego .\n\n\nBoth Dockerfile.rego (automatic) and extra-checks.rego (manual) must pass.\n\nBuild without automatic policy\n\nUse only your specified policy:\n\n$ docker buildx build --policy reset=true,filename=strict.rego .\n\nUsing policies with bake\n\nBake supports automatic policy loading just like docker buildx build. Place Dockerfile.rego alongside your Dockerfile and run:\n\n$ docker buildx bake\n\nManual policy in bake files\n\nSpecify additional policies in your docker-bake.hcl:\n\ndocker-bake.hcl\ntarget \"default\" {\n\n  dockerfile = \"Dockerfile\"\n\n  policy = [\"extra.rego\"]\n\n}\n\nThe policy attribute takes a list of policy files. Bake loads these in addition to the automatic Dockerfile.rego (if it exists).\n\nMultiple policies in bake\ndocker-bake.hcl\ntarget \"webapp\" {\n\n  dockerfile = \"Dockerfile\"\n\n  policy = [\n\n    \"shared/base-policy.rego\",\n\n    \"security/image-signing.rego\"\n\n  ]\n\n}\n\nAll policies must pass for the target to build successfully.\n\nDifferent policies per target\n\nApply different validation rules to different targets:\n\ndocker-bake.hcl\ntarget \"development\" {\n\n  dockerfile = \"dev.Dockerfile\"\n\n  policy = [\"policies/permissive.rego\"]\n\n}\n\n\n\ntarget \"production\" {\n\n  dockerfile = \"prod.Dockerfile\"\n\n  policy = [\"policies/strict.rego\", \"policies/signing-required.rego\"]\n\n}\n\nBuild with the appropriate target:\n\n$ docker buildx bake development  # Uses permissive policy\n\n$ docker buildx bake production   # Uses strict policies\n\nBake with policy options\n\nCurrently, bake doesn't support policy options (reset, strict, disabled) in the HCL file. Use command-line flags instead:\n\n$ docker buildx bake --policy disabled=true production\n\nTesting in CI/CD\n\nValidate policies in continuous integration by running builds with the --policy flag. For unit testing policies before running builds, see Test build policies.\n\nTest policies during CI builds:\n\n.github/workflows/test-policies.yml\nname: Test Build Policies\n\non: [push, pull_request]\n\n\n\njobs:\n\n  test:\n\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - uses: actions/checkout@v4\n\n      - uses: docker/setup-buildx-action@v3\n\n      - name: Test build with policy\n\n        run: docker buildx build --policy strict=true .\n\nThis ensures policy changes don't break builds and that new rules work as intended. The strict=true flag fails the build if policies aren't loaded (for example, if the BuildKit instance used by the build is too old and doesn't support policies).\n\nAdvanced: Policy configuration\n\nThis section covers advanced policy loading mechanisms and configuration options.\n\nAutomatic policy loading\n\nBuildx automatically loads policies that match your Dockerfile name. When you build with Dockerfile, Buildx looks for Dockerfile.rego in the same directory. For a file named app.Dockerfile, it looks for app.Dockerfile.rego.\n\nproject/\n\n‚îú‚îÄ‚îÄ Dockerfile\n\n‚îú‚îÄ‚îÄ Dockerfile.rego          # Loaded automatically for Dockerfile\n\n‚îú‚îÄ‚îÄ app.Dockerfile\n\n‚îú‚îÄ‚îÄ app.Dockerfile.rego      # Loaded automatically for app.Dockerfile\n\n‚îî‚îÄ‚îÄ src/\n\nThis automatic loading means you don't need command-line flags in most cases. Create the policy file alongside your Dockerfile and build:\n\n$ docker buildx build .\n\n\nBuildx detects Dockerfile.rego and evaluates it before running the build.\n\nNote\n\nPolicy files must be in the same directory as the Dockerfile they validate. Buildx doesn't search parent directories or subdirectories.\n\nWhen policies don't load\n\nIf buildx can't find a matching .rego file, the build proceeds without policy evaluation. To require policies and fail if none are found, use strict mode:\n\n$ docker buildx build --policy strict=true .\n\n\nThis fails the build if no policy loads or if the BuildKit daemon doesn't support policies.\n\nManual policy configuration\n\nThe --policy flag lets you specify additional policies, override automatic loading, or control policy behavior.\n\nBasic syntax:\n\n$ docker buildx build --policy filename=custom.rego .\n\n\nThis loads custom.rego in addition to the automatic Dockerfile.rego (if it exists).\n\nMultiple policies:\n\n$ docker buildx build --policy filename=policy1.rego --policy filename=policy2.rego .\n\n\nAll policies must pass for the build to succeed. Use this to enforce layered requirements (base policy + project-specific rules).\n\nAvailable options:\n\nOption\tDescription\tExample\nfilename=<path>\tLoad policy from specified file\tfilename=custom.rego\nreset=true\tIgnore automatic policies, use only specified ones\treset=true\ndisabled=true\tDisable all policy evaluation\tdisabled=true\nstrict=true\tFail if BuildKit doesn't support policies\tstrict=true\nlog-level=<level>\tControl policy logging (error, warn, info, debug, none). Use debug to see complete input JSON and unresolved fields\tlog-level=debug\n\nCombine options with commas:\n\n$ docker buildx build --policy filename=extra.rego,strict=true .\n\nExploring sources with policy eval\n\nThe docker buildx policy eval command lets you quickly explore and test sources without running a build.\n\nInspect input structure with --print\n\nUse --print to see the input structure for any source without running policy evaluation:\n\n$ docker buildx policy eval --print https://github.com/moby/buildkit.git\n\n{\n\n  \"git\": {\n\n    \"schema\": \"https\",\n\n    \"host\": \"github.com\",\n\n    \"remote\": \"https://github.com/moby/buildkit.git\"\n\n  }\n\n}\n\nTest different source types:\n\n# HTTP downloads\n\n$ docker buildx policy eval --print https://releases.hashicorp.com/terraform/1.5.0/terraform.zip\n\n\n\n# Images (requires docker-image:// prefix)\n\n$ docker buildx policy eval --print docker-image://alpine:3.19\n\n\n\n# Local context\n\n$ docker buildx policy eval --print .\n\n\nShows information parsed from the source without fetching. Use --fields to fetch specific metadata (see above).\n\nTest with specific policy files\n\nThe --filename flag specifies which policy file to load by providing the base Dockerfile name (without the .rego extension). This is useful for testing sources against policies associated with different Dockerfiles.\n\nFor example, to test a source against the policy for app.Dockerfile:\n\n$ docker buildx policy eval --filename app.Dockerfile .\n\n\nThis loads app.Dockerfile.rego and tests whether it allows the source . (the local directory). The flag defaults to Dockerfile if not specified.\n\nTest different sources against your policy:\n\n$ docker buildx policy eval --filename app.Dockerfile https://github.com/org/repo.git\n\n$ docker buildx policy eval --filename app.Dockerfile docker-image://alpine:3.19\n\nReset automatic loading\n\nTo use only your specified policies and ignore automatic .rego files:\n\n$ docker buildx build --policy reset=true,filename=custom.rego .\n\n\nThis skips Dockerfile.rego and loads only custom.rego.\n\nDisable policies temporarily\n\nDisable policy evaluation for testing or emergencies:\n\n$ docker buildx build --policy disabled=true .\n\n\nThe build proceeds without any policy checks. Use this carefully - you're bypassing security controls.\n\nNext steps\nWrite unit tests for your policies: Test build policies\nDebug policy failures: Debugging\nBrowse working examples: Example policies\nReference all input fields: Input reference\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nPolicy development workflow\nViewing inputs from your Dockerfile\nTesting policies with policy eval\nIterative development example\nUsing policies with docker build\nBasic usage\nBuild with different Dockerfile names\nBuild with manual policy\nBuild without automatic policy\nUsing policies with bake\nManual policy in bake files\nMultiple policies in bake\nDifferent policies per target\nBake with policy options\nTesting in CI/CD\nAdvanced: Policy configuration\nAutomatic policy loading\nWhen policies don't load\nManual policy configuration\nExploring sources with policy eval\nReset automatic loading\nDisable policies temporarily\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995881,
    "timestamp": "2026-02-07T06:37:09.650Z",
    "title": "Introduction | Docker Docs",
    "url": "https://docs.docker.com/build/policies/intro/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\n/\nIntroduction\nIntroduction to build policies\nCopy as Markdown\n\nBuild policies let you validate the inputs to your Docker builds before they run. This tutorial walks you through creating your first policy, teaching the Rego basics you need along the way.\n\nWhat you'll learn\n\nBy the end of this tutorial, you'll understand:\n\nHow to create and organize policy files\nBasic Rego syntax and patterns\nHow to write policies that validate URLs, checksums, and images\nHow policies evaluate during builds\nPrerequisites\nBuildx version 0.31 or later\nBasic familiarity with Dockerfiles and building images\nHow policies work\n\nWhen you build an image, Buildx resolves all the inputs your Dockerfile references: base images from FROM instructions, files from ADD or COPY or build contexts, and Git repositories. Before running the build, Buildx evaluates your policies against these inputs. If any input violates a policy, the build fails before any instructions execute.\n\nPolicies are written in Rego, a declarative language designed for expressing rules and constraints. You don't need to know Rego to get started - this tutorial teaches you what you need.\n\nCreate your first policy\n\nCreate a new directory for this tutorial and add a Dockerfile:\n\n$ mkdir policy-tutorial\n\n$ cd policy-tutorial\n\n\nCreate a Dockerfile that downloads a file with ADD:\n\nFROM scratch\n\nADD https://example.com/index.html /index.html\n\nNow create a policy file. Policies use the .rego extension and live alongside your Dockerfile. Create Dockerfile.rego:\n\nDockerfile.rego\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\nallow if {\n\n  input.http.host == \"example.com\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nSave this file as Dockerfile.rego in the same directory as your Dockerfile.\n\nLet's break down what this policy does:\n\npackage docker - All build policies must start with this package declaration\ndefault allow := false - This example uses a deny-by-default rule: if inputs do not match an allow rule, the policy check fails\nallow if input.local - The first rule allows any local files (your build context)\nallow if { input.http.host == \"example.com\" } - The second rule allows HTTP downloads from example.com\ndecision := {\"allow\": allow} - The final decision object tells Buildx whether to allow or deny the input\n\nThis policy says: \"Only allow local files and HTTP downloads from example.com\". Rego evaluates all the policy rules to figure out the return value for the decision variable, for each build input. The evaluations happen in parallel and on-demand; the order of the policy rules has no significance.\n\nAbout input.local\n\nYou'll see allow if input.local in nearly every policy. This rule allows local file access, which includes your build context (typically, the . directory) and importantly, the Dockerfile itself. Without this rule, Buildx can't read your Dockerfile to start the build.\n\nEven builds that don't reference any files from the build context often need input.local because the Dockerfile is a local file. The policy evaluates before the build starts, and denying local access means denying access to the Dockerfile.\n\nIn rare cases, you might want stricter local file policies - for example, in CI builds where the build context uses a Git URL as a context directly. In these cases, you may want to deny local sources to prevent untracked files or changes from making their way into your build.\n\nAutomatic policy loading\n\nBuildx automatically loads policies that match your Dockerfile name. When you build with Dockerfile, Buildx looks for Dockerfile.rego in the same directory. For a file named app.Dockerfile, it looks for app.Dockerfile.rego.\n\nThis automatic loading means you don't need any command-line flags in most cases - create the policy file and build.\n\nThe policy file must be in the same directory as the Dockerfile. If Buildx can't find a matching policy, the build proceeds without policy evaluation (unless you use --policy strict=true).\n\nFor more control over policy loading, see the Usage guide.\n\nRun a build with your policy\n\nBuild the image with policy evaluation enabled:\n\n$ docker build .\n\n\nThe build succeeds because the URL in your Dockerfile matches the policy. Now try changing the URL in your Dockerfile to something else:\n\nFROM scratch\n\nADD https://api.github.com/users/octocat /user.json\n\nBuild again:\n\n$ docker build .\n\n\nThis time the build fails with a policy violation. The api.github.com hostname doesn't match the rule in your policy, so Buildx rejects it before running any build steps.\n\nDebugging policy failures\n\nIf your build fails with a policy violation, use --progress=plain to see exactly what went wrong:\n\n$ docker buildx build --progress=plain .\n\n\nThis shows all policy checks, the input data for each source, and allow/deny decisions. For complete debugging guidance, see Debugging.\n\nAdd helpful error messages\n\nWhen a policy denies an input, users see a generic error message. You can provide custom messages that explain why the build was denied:\n\nDockerfile.rego\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\nallow if {\n\n  input.http.host == \"example.com\"\n\n  input.http.schema == \"https\"\n\n}\n\n\n\ndeny_msg contains msg if {\n\n  not allow\n\n  input.http\n\n  msg := \"only HTTPS downloads from example.com are allowed\"\n\n}\n\n\n\ndecision := {\"allow\": allow, \"deny_msg\": deny_msg}\n\nNow when a build is denied, users see your custom message explaining what went wrong:\n\n$ docker buildx build .\n\nPolicy: only HTTPS downloads from example.com are allowed\n\nERROR: failed to build: ... source not allowed by policy\n\n\nThe deny_msg rule uses contains to add messages to a set. You can add multiple deny messages for different failure conditions to help users understand exactly what needs to change.\n\nUnderstand Rego rules\n\nRego policies are built from rules. A rule defines when something is allowed. The basic pattern is:\n\nallow if {\n\n    condition_one\n\n    condition_two\n\n    condition_three\n\n}\n\nAll conditions must be true for the rule to match. Think of it as an AND operation - the URL must match AND the checksum must match AND any other conditions you specify.\n\nYou can have multiple allow rules in one policy. If any rule matches, the input is allowed:\n\n# Allow downloads from example.com\n\nallow if {\n\n    input.http.host == \"example.com\"\n\n}\n\n\n\n# Also allow downloads from api.github.com\n\nallow if {\n\n    input.http.host == \"api.github.com\"\n\n}\n\nThis works like OR - the input can match the first rule OR the second rule.\n\nAccess input fields\n\nThe input object gives you access to information about build inputs. The structure depends on the input type:\n\ninput.http - Files downloaded with ADD https://...\ninput.image - Container images from FROM or COPY --from\ninput.git - Git repositories from ADD git://... or build context\ninput.local - Local file context\n\nRefer to the Input reference for all available input fields.\n\nFor HTTP downloads, you can access:\n\nKey\tDescription\tExample\ninput.http.url\tThe full URL\thttps://example.com/index.html\ninput.http.schema\tThe protocol (HTTP/HTTPS)\thttps\ninput.http.host\tThe hostname\texample.com\ninput.http.path\tThe URL path, including parameters\t/index.html\n\nUpdate your policy to require HTTPS:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if {\n\n    input.http.host == \"example.com\"\n\n    input.http.schema == \"https\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nNow the policy requires both the hostname to be example.com and the protocol to be HTTPS. HTTP URLs (without TLS) would fail the policy check.\n\nPattern matching and strings\n\nRego provides built-in functions for pattern matching. Use startswith() to match URL prefixes:\n\nallow if {\n\n    startswith(input.http.url, \"https://example.com/\")\n\n}\n\nThis allows any URL that starts with https://example.com/.\n\nUse regex.match() for complex patterns:\n\nallow if {\n\n    regex.match(`^https://example\\.com/.+\\.json$`, input.http.url)\n\n}\n\nThis matches URLs that:\n\nStart with https://example.com/\nEnd with .json\nHave at least one character between the domain and extension\nPolicy file location\n\nPolicy files live adjacent to the Dockerfile they validate, using the naming pattern <dockerfile-name>.rego:\n\nproject/\n\n‚îú‚îÄ‚îÄ Dockerfile           # Main Dockerfile\n\n‚îú‚îÄ‚îÄ Dockerfile.rego      # Policy for Dockerfile\n\n‚îú‚îÄ‚îÄ lint.Dockerfile      # Linting Dockerfile\n\n‚îî‚îÄ‚îÄ lint.Dockerfile.rego # Policy for lint.Dockerfile\n\nWhen you build, Buildx automatically loads the corresponding policy file:\n\n$ docker buildx build -f Dockerfile .        # Loads Dockerfile.rego\n\n$ docker buildx build -f lint.Dockerfile .   # Loads lint.Dockerfile.rego\n\nNext steps\n\nYou now understand how to write basic build policies for HTTP resources. To continue learning:\n\nApply and test policies: Using build policies\nLearn Image validation to validate container images from FROM instructions\nLearn Git validation to validate Git repositories used in builds\nSee Example policies for copy-paste-ready policies covering common scenarios\nWrite unit tests for your policies: Test build policies\nDebug policy failures: Debugging\nRead the Input reference for all available input fields\nCheck the Built-in functions for signature verification, attestations, and other security checks\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat you'll learn\nPrerequisites\nHow policies work\nCreate your first policy\nAbout input.local\nAutomatic policy loading\nRun a build with your policy\nDebugging policy failures\nAdd helpful error messages\nUnderstand Rego rules\nAccess input fields\nPattern matching and strings\nPolicy file location\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995887,
    "timestamp": "2026-02-07T06:37:09.658Z",
    "title": "Image validation | Docker Docs",
    "url": "https://docs.docker.com/build/policies/validate-images/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\n/\nImage validation\nValidating image inputs\nCopy as Markdown\n\nContainer images are the most common build inputs. Every FROM instruction pulls an image, and COPY --from references pull additional images. Validating these images protects your build supply chain from compromised registries, unexpected updates, and unauthorized base images.\n\nThis guide teaches you to write policies that validate image inputs, progressing from basic allowlisting to advanced attestation checks.\n\nPrerequisites\n\nYou should understand the policy basics from the Introduction: creating policy files, basic Rego syntax, and how policies evaluate during builds.\n\nWhat are image inputs?\n\nImage inputs come from two Dockerfile instructions:\n\n# FROM instructions\n\nFROM alpine:3.22\n\nFROM golang:1.25-alpine AS builder\n\n\n\n# COPY --from references\n\nCOPY --from=builder /app /app\n\nCOPY --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf\n\nEach of these references triggers a policy evaluation. Your policy can inspect image metadata, verify attestations, and enforce constraints before the build proceeds.\n\nAllowlist specific repositories\n\nThe simplest image policy restricts which repositories can be used. This prevents developers from using arbitrary images that haven't been vetted.\n\nCreate a policy that only allows Alpine:\n\nDockerfile.rego\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.image.repo == \"alpine\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis policy:\n\nDenies all inputs by default\nAllows local build context\nAllows any image from the alpine repository (any tag or digest)\n\nTest it with a Dockerfile:\n\nDockerfile\nFROM alpine\n\nRUN echo \"hello\"\n$ docker build .\n\n\nThe build succeeds. Try changing to FROM ubuntu:\n\n$ docker build .\n\n\nThe build fails because ubuntu doesn't match the allowed repository.\n\nCompare semantic versions\n\nRestrict images to specific version ranges using Rego's semver functions:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# Allow Go 1.21 or newer\n\nallow if {\n\n  input.image.repo == \"golang\"\n\n  semver.is_valid(input.image.tag)\n\n  semver.compare(input.image.tag, \"1.21.0\") >= 0\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThe semver.compare(a, b) function compares semantic versions and returns:\n\n-1 if version a is less than b\n0 if versions are equal\n1 if version a is greater than b\n\nUse semver.is_valid() to check if a tag is a valid semantic version before comparing.\n\nRestrict to specific version ranges:\n\nallow if {\n\n  input.image.repo == \"node\"\n\n  version := input.image.tag\n\n  semver.is_valid(version)\n\n  semver.compare(version, \"20.0.0\") >= 0  # 20.0.0 or newer\n\n  semver.compare(version, \"21.0.0\") < 0   # older than 21.0.0\n\n}\n\nThis allows only Node.js 20.x versions. The pattern works for any image using semantic versioning.\n\nThese semver functions are standard Rego built-ins documented in the OPA policy reference.\n\nRequire digest references\n\nTags like alpine:3.22 can change - someone could push a new image with the same tag. Digests like alpine@sha256:abc123... are immutable.\n\nRequiring users to provide digests\n\nYou can require that users always specify digests in their Dockerfiles:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.image.isCanonical\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThe isCanonical field is true when the user's reference includes a digest. This policy would allow:\n\nFROM alpine@sha256:4b7ce07002c69e8f3d704a9c5d6fd3053be500b7f1c69fc0d80990c2ad8dd412\n\nBut reject tag-only references like FROM alpine:3.22.\n\nPinning to specific digests\n\nAlternatively (or additionally), you can validate that an image's actual digest matches a specific value, regardless of how the user wrote the reference:\n\nallow if {\n\n  input.image.repo == \"alpine\"\n\n  input.image.checksum == \"sha256:4b7ce07002c69e8f3d704a9c5d6fd3053be500b7f1c69fc0d80990c2ad8dd412\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis checks the actual content digest of the pulled image. It would allow both:\n\nFROM alpine:3.22\n\nFROM alpine@sha256:4b7ce...\n\nAs long as the resolved image has the specified digest. This is useful for pinning critical base images to known-good versions.\n\nRestrict registries\n\nControl which registries your builds can pull from. This helps enforce corporate policies or restrict to trusted sources.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# Allow Docker Hub images\n\nallow if {\n\n  input.image.host == \"docker.io\"  # Docker Hub\n\n  input.image.repo == \"alpine\"\n\n}\n\n\n\n# Allow images from internal registry\n\nallow if {\n\n  input.image.host == \"registry.company.com\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThe host field contains the registry hostname. Docker Hub images use \"docker.io\" as the host value. Test with:\n\nFROM alpine                                    # Allowed (Docker Hub)\n\nFROM registry.company.com/myapp:latest         # Allowed (company registry)\n\nFROM ghcr.io/someorg/image:latest              # Denied (wrong registry)\n\nUse fullRepo when you need the complete path including registry:\n\nallow if {\n\n  input.image.fullRepo == \"docker.io/library/alpine\"\n\n}\nValidate platform constraints\n\nMulti-architecture images support different operating systems and CPU architectures. You can restrict builds to specific platforms:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.image.os == \"linux\"\n\n  input.image.arch in [\"amd64\", \"arm64\"]\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis policy:\n\nDefines supported architectures in a list\nChecks input.image.os matches Linux\nVerifies input.image.arch is in the supported list\n\nThe os and arch fields come from the image manifest, reflecting the actual image platform. This works with Docker's automatic platform selection - policies validate what Buildx resolves, not what you specify.\n\nInspect image metadata\n\nImages contain metadata like environment variables, labels, and working directories. You can validate these to ensure images meet requirements.\n\nCheck for specific environment variables:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.image.repo == \"golang\"\n\n  input.image.workingDir == \"/go\"\n\n  some ver in input.image.env\n\n  startswith(ver, \"GOLANG_VERSION=\")\n\n  some toolchain in input.image.env\n\n  toolchain == \"GOTOOLCHAIN=local\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis policy validates the official Go image by checking:\n\nThe working directory is /go\nThe environment has GOLANG_VERSION set\nThe environment includes GOTOOLCHAIN=local\n\nThe input.image.env field is an array of strings in KEY=VALUE format. Use Rego's some iteration to search the array.\n\nCheck image labels:\n\nallow if {\n\n  input.image.labels[\"org.opencontainers.image.vendor\"] == \"Example Corp\"\n\n  input.image.labels[\"org.opencontainers.image.version\"] != \"\"\n\n}\n\nThe labels field is a map, so you access values with bracket notation.\n\nRequire attestations and provenance\n\nModern images include attestations: machine-readable metadata about how the image was built. Provenance attestations describe the build process, and SBOMs list the software inside.\n\nRequire provenance:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.image.hasProvenance\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThe hasProvenance field is true when the image has provenance or SBOM attestations.\n\nVerify GitHub Actions signatures\n\nFor images built with GitHub Actions, verify they came from trusted workflows by inspecting signature metadata:\n\nallow if {\n\n  input.image.repo == \"myapp\"\n\n  input.image.hasProvenance\n\n  some sig in input.image.signatures\n\n  valid_github_signature(sig)\n\n}\n\n\n\n# Helper to validate GitHub Actions signature\n\nvalid_github_signature(sig) if {\n\n  sig.signer.certificateIssuer == \"CN=sigstore-intermediate,O=sigstore.dev\"\n\n  sig.signer.issuer == \"https://token.actions.githubusercontent.com\"\n\n  startswith(sig.signer.buildSignerURI, \"https://github.com/myorg/\")\n\n  sig.signer.runnerEnvironment == \"github-hosted\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis pattern works with any GitHub Actions workflow using Sigstore keyless signing. The signature metadata provides cryptographic proof of the build's origin. For complete signature verification examples, see Example policies.\n\nCombine multiple checks\n\nReal policies often combine several checks. Multiple conditions in one allow rule means AND - all must be true:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# Production images need everything\n\nallow if {\n\n  input.image.repo == \"alpine\"\n\n  input.image.isCanonical\n\n  input.image.hasProvenance\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nMultiple allow rules means OR - any rule can match:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# Allow Alpine with strict checks\n\nallow if {\n\n  input.image.repo == \"alpine\"\n\n  input.image.isCanonical\n\n}\n\n\n\n# Allow Go with different checks\n\nallow if {\n\n  input.image.repo == \"golang\"\n\n  input.image.workingDir == \"/go\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nUse this pattern to apply different requirements to different base images.\n\nNext steps\n\nYou now understand how to validate container images in build policies. To continue learning:\n\nLearn Git repository validation for source code inputs\nBrowse Example policies for complete policy patterns\nRead Built-in functions for signature verification and attestation checking\nCheck the Input reference for all available image fields\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nWhat are image inputs?\nAllowlist specific repositories\nCompare semantic versions\nRequire digest references\nRequiring users to provide digests\nPinning to specific digests\nRestrict registries\nValidate platform constraints\nInspect image metadata\nRequire attestations and provenance\nVerify GitHub Actions signatures\nCombine multiple checks\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995890,
    "timestamp": "2026-02-07T06:37:09.667Z",
    "title": "Git validation | Docker Docs",
    "url": "https://docs.docker.com/build/policies/validate-git/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\n/\nGit validation\nValidating Git repositories\nCopy as Markdown\n\nGit repositories often appear in Docker builds as source code inputs. The ADD instruction can clone repositories, and build contexts can reference Git URLs. Validating these inputs ensures you're building from trusted sources with verified versions.\n\nThis guide teaches you to write policies that validate Git inputs, from basic version pinning to verifying signed commits and tags.\n\nPrerequisites\n\nYou should understand the policy basics from the Introduction: creating policy files, basic Rego syntax, and how policies evaluate during builds.\n\nWhat are Git inputs?\n\nGit inputs come from ADD instructions that reference Git repositories:\n\n# Clone a specific tag\n\nADD https://github.com/moby/buildkit.git#v0.26.1 /buildkit\n\n\n\n# Clone a branch\n\nADD https://github.com/user/repo.git#main /src\n\n\n\n# Clone a commit\n\nADD https://github.com/user/repo.git#abcde123 /src\n\nThe build context can also be a Git repository when you build with:\n\n$ docker build https://github.com/user/repo.git#main\n\n\nEach Git reference triggers a policy evaluation. Your policy can inspect repository URLs, validate versions, check commit metadata, and verify signatures.\n\nMatch specific repositories\n\nThe simplest Git policy restricts which repositories can be used:\n\nDockerfile.rego\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.git.host == \"github.com\"\n\n  input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis policy:\n\nDenies all inputs by default\nAllows local build context\nAllows only the BuildKit repository from GitHub\n\nThe host field contains the Git server hostname, and remote contains the full repository URL. Test it:\n\nDockerfile\nFROM scratch\n\nADD https://github.com/moby/buildkit.git#v0.26.1 /\n$ docker build .\n\n\nThe build succeeds. Try a different repository and it fails.\n\nYou can match multiple repositories with additional rules:\n\nallow if {\n\n  input.git.host == \"github.com\"\n\n  input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n}\n\n\n\nallow if {\n\n  input.git.host == \"github.com\"\n\n  input.git.remote == \"https://github.com/docker/cli.git\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\nPin to specific versions\n\nTags and branches can change over time. Pin to specific versions to ensure reproducible builds:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n  input.git.tagName == \"v0.26.1\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThe tagName field contains the tag name when the Git reference points to a tag. Use branch for branches:\n\nallow if {\n\n  input.git.remote == \"https://github.com/user/repo.git\"\n\n  input.git.branch == \"main\"\n\n}\n\nOr use ref for any type of reference (branch, tag, or commit SHA):\n\nallow if {\n\n  input.git.ref == \"v0.26.1\"\n\n}\nUse version allowlists\n\nFor repositories you trust but want to control versions, maintain an allowlist:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallowed_versions = [\n\n    {\"tag\": \"v0.26.1\", \"annotated\": true, \"sha\": \"abc123\"},\n\n]\n\n\n\nis_buildkit if {\n\n    input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n}\n\n\n\nallow if {\n\n    not is_buildkit\n\n}\n\n\n\nallow if {\n\n    is_buildkit\n\n    some version in allowed_versions\n\n    input.git.tagName == version.tag\n\n    input.git.isAnnotatedTag == version.annotated\n\n    startswith(input.git.commitChecksum, version.sha)\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis policy:\n\nDefines an allowlist of approved versions with metadata\nUses a helper rule (is_buildkit) for readability\nAllows all non-BuildKit inputs\nFor BuildKit, checks the tag name, whether it's an annotated tag, and the commit SHA against the allowlist\n\nThe helper rule makes complex policies more maintainable. You can expand the allowlist as new versions are approved:\n\nallowed_versions = [\n\n    {\"tag\": \"v0.26.1\", \"annotated\": true, \"sha\": \"abc123\"},\n\n    {\"tag\": \"v0.27.0\", \"annotated\": true, \"sha\": \"def456\"},\n\n    {\"tag\": \"v0.27.1\", \"annotated\": true, \"sha\": \"789abc\"},\n\n]\nValidate with regex patterns\n\nUse pattern matching for semantic versioning:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n  regex.match(`^v[0-9]+\\.[0-9]+\\.[0-9]+$`, input.git.tagName)\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis allows any BuildKit tag matching the pattern vX.Y.Z where X, Y, and Z are numbers. The regex ensures you're using release versions, not pre-release tags like v0.26.0-rc1.\n\nMatch major versions:\n\n# Only allow v0.x releases\n\nallow if {\n\n  input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n  regex.match(`^v0\\.[0-9]+\\.[0-9]+$`, input.git.tagName)\n\n}\nInspect commit metadata\n\nThe commit object provides detailed information about commits:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# Check commit author\n\nallow if {\n\n  input.git.remote == \"https://github.com/user/repo.git\"\n\n  input.git.commit.author.email == \"trusted@example.com\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThe commit object includes:\n\nauthor.name: Author's name\nauthor.email: Author's email\nauthor.when: When the commit was authored\ncommitter.name: Committer's name\ncommitter.email: Committer's email\ncommitter.when: When the commit was committed\nmessage: Commit message\n\nValidate commit messages:\n\nallow if {\n\n  input.git.commit\n\n  contains(input.git.commit.message, \"Signed-off-by:\")\n\n}\n\nPin to specific commit SHA:\n\nallow if {\n\n  input.git.commitChecksum == \"abc123def456...\"\n\n}\nRequire signed commits\n\nGPG-signed commits prove authenticity. Check for commit signatures:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n  input.git.commit.pgpSignature != null\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThe pgpSignature field is null for unsigned commits. For signed commits, it contains signature details.\n\nSSH signatures work similarly:\n\nallow if {\n\n  input.git.commit.sshSignature != null\n\n}\nRequire signed tags\n\nAnnotated tags can be signed, providing a cryptographic guarantee of the release:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n  input.git.tag.pgpSignature != null\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThe tag object is only available for annotated tags. It includes:\n\ntagger.name: Who created the tag\ntagger.email: Tagger's email\ntagger.when: When the tag was created\nmessage: Tag message\npgpSignature: GPP signature (if signed)\nsshSignature: SSH signature (if signed)\n\nLightweight tags don't have a tag object, so this policy effectively requires annotated, signed tags.\n\nVerify signatures with public keys\n\nUse the verify_git_signature() function to cryptographically verify Git signatures against trusted public keys:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if {\n\n  input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n  input.git.tagName != \"\"\n\n  verify_git_signature(input.git.tag, \"keys.asc\")\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis verifies that Git tags are signed by keys in the keys.asc public key file. To set this up:\n\nExport maintainer public keys:\n$ curl https://github.com/user.gpg > keys.asc\n\nPlace keys.asc alongside your policy file\n\nThe function verifies PGP signatures on commits or tags. See Built-in functions for more details.\n\nApply conditional rules\n\nUse different rules for different contexts. Allow unsigned refs during development but require signing for production:\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nis_buildkit if {\n\n    input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n}\n\n\n\nis_version_tag if {\n\n    is_buildkit\n\n    regex.match(`^v[0-9]+\\.[0-9]+\\.[0-9]+$`, input.git.tagName)\n\n}\n\n\n\n# Version tags must be signed\n\nallow if {\n\n    is_version_tag\n\n    input.git.tagName != \"\"\n\n    verify_git_signature(input.git.tag, \"keys.asc\")\n\n}\n\n\n\n# Non-version refs allowed in development\n\nallow if {\n\n    is_buildkit\n\n    not is_version_tag\n\n    input.env.target != \"release\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis policy:\n\nDefines helper rules for readability\nRequires signed version tags from maintainers\nAllows unsigned refs (branches, commits) unless building the release target\nUses input.env.target to detect the build target\n\nBuild a development target without signatures:\n\n$ docker buildx build --target=dev .\n\n\nBuild the release target, and signing is enforced:\n\n$ docker buildx build --target=release .\n\nNext steps\n\nYou now understand how to validate Git repositories in build policies. To continue learning:\n\nBrowse Example policies for complete policy patterns\nRead Built-in functions for Git signature verification functions\nCheck the Input reference for all available Git fields\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nWhat are Git inputs?\nMatch specific repositories\nPin to specific versions\nUse version allowlists\nValidate with regex patterns\nInspect commit metadata\nRequire signed commits\nRequire signed tags\nVerify signatures with public keys\nApply conditional rules\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995896,
    "timestamp": "2026-02-07T06:37:09.674Z",
    "title": "Testing | Docker Docs",
    "url": "https://docs.docker.com/build/policies/testing/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\n/\nTesting\nTest build policies\nCopy as Markdown\n\nThe docker buildx policy test command runs unit tests for build policies using OPA's standard test framework.\n\n$ docker buildx policy test <path>\n\n\nThis validates policy logic with mocked inputs.\n\nFor testing against real sources (actual image metadata, Git repositories), use docker buildx policy eval instead. You can use the eval --print option to resolve input for a specific source for writing a test case.\n\nBasic example\n\nStart with a simple policy that only allows alpine images:\n\nDockerfile.rego\npackage docker\n\n\n\ndefault allow = false\n\n\n\nallow if {\n\n    input.image.repo == \"alpine\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nCreate a test file with the *_test.rego suffix. Test functions must start with test_:\n\nDockerfile_test.rego\npackage docker\n\n\n\ntest_alpine_allowed if {\n\n    decision.allow with input as {\"image\": {\"repo\": \"alpine\"}}\n\n}\n\n\n\ntest_ubuntu_denied if {\n\n    not decision.allow with input as {\"image\": {\"repo\": \"ubuntu\"}}\n\n}\n\nRun the tests:\n\n$ docker buildx policy test .\n\ntest_alpine_allowed: PASS (allow=true)\n\ntest_ubuntu_denied: PASS (allow=false)\n\n\nPASS indicates that the tests defined in Dockerfile_test.rego executed successfully and all assertions were satisfied.\n\nCommand options\n\nFilter tests by name with --run:\n\n$ docker buildx policy test --run alpine .\n\ntest_alpine_allowed: PASS (allow=true)\n\n\nTest policies with non-default filenames using --filename:\n\n$ docker buildx policy test --filename app.Dockerfile .\n\n\nThis loads app.Dockerfile.rego and runs *_test.rego files against it.\n\nTest output\n\nPassed tests show the allow status and any deny messages:\n\ntest_alpine_allowed: PASS (allow=true)\n\ntest_ubuntu_denied: PASS (allow=false, deny_msg=only alpine images are allowed)\n\n\nFailed tests show input, decision output, and missing fields:\n\ntest_invalid: FAIL (allow=false)\n\ninput:\n\n  {\n\n    \"image\": {}\n\n  }\n\ndecision:\n\n  {\n\n    \"allow\": false,\n\n    \"deny_msg\": [\n\n      \"only alpine images are allowed\"\n\n    ]\n\n  }\n\nmissing_input: input.image.repo\n\nTest deny messages\n\nTo test custom error messages, capture the full decision result and assert on the deny_msg field.\n\nFor a policy with deny messages:\n\nDockerfile.rego\npackage docker\n\n\n\ndefault allow = false\n\n\n\nallow if {\n\n    input.image.repo == \"alpine\"\n\n}\n\n\n\ndeny_msg contains msg if {\n\n    not allow\n\n    msg := \"only alpine images are allowed\"\n\n}\n\n\n\ndecision := {\"allow\": allow, \"deny_msg\": deny_msg}\n\nTest the deny message:\n\nDockerfile_test.rego\ntest_deny_message if {\n\n    result := decision with input as {\"image\": {\"repo\": \"ubuntu\"}}\n\n    not result.allow\n\n    \"only alpine images are allowed\" in result.deny_msg\n\n}\nTest patterns\n\nTest environment-specific rules:\n\ntest_production_requires_digest if {\n\n    decision.allow with input as {\n\n        \"env\": {\"target\": \"production\"},\n\n        \"image\": {\"isCanonical\": true}\n\n    }\n\n}\n\n\n\ntest_development_allows_tags if {\n\n    decision.allow with input as {\n\n        \"env\": {\"target\": \"development\"},\n\n        \"image\": {\"isCanonical\": false}\n\n    }\n\n}\n\nTest multiple registries:\n\ntest_dockerhub_allowed if {\n\n    decision.allow with input as {\n\n        \"image\": {\n\n            \"ref\": \"docker.io/library/alpine\",\n\n            \"host\": \"docker.io\",\n\n            \"repo\": \"alpine\"\n\n        }\n\n    }\n\n}\n\n\n\ntest_ghcr_allowed if {\n\n    decision.allow with input as {\n\n        \"image\": {\n\n            \"ref\": \"ghcr.io/myorg/myapp\",\n\n            \"host\": \"ghcr.io\",\n\n            \"repo\": \"myorg/myapp\"\n\n        }\n\n    }\n\n}\n\nFor available input fields, see the Input reference.\n\nOrganize test files\n\nThe test runner discovers all *_test.rego files recursively:\n\nbuild-policies/\n\n‚îú‚îÄ‚îÄ Dockerfile.rego\n\n‚îú‚îÄ‚îÄ Dockerfile_test.rego\n\n‚îî‚îÄ‚îÄ tests/\n\n    ‚îú‚îÄ‚îÄ registries_test.rego\n\n    ‚îú‚îÄ‚îÄ signatures_test.rego\n\n    ‚îî‚îÄ‚îÄ environments_test.rego\n\nRun all tests:\n\n$ docker buildx policy test .\n\n\nOr test specific files:\n\n$ docker buildx policy test tests/registries_test.rego\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nBasic example\nCommand options\nTest output\nTest deny messages\nTest patterns\nOrganize test files\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995893,
    "timestamp": "2026-02-07T06:37:09.683Z",
    "title": "Templates & examples | Docker Docs",
    "url": "https://docs.docker.com/build/policies/examples/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\n/\nTemplates & examples\nPolicy templates and examples\nCopy as Markdown\n\nThis page provides complete, working policy examples you can copy and adapt. The examples are organized into two sections: getting started policies for quick adoption, and production templates for comprehensive security.\n\nIf you're new to policies, start with the tutorials: Introduction, Image validation, and Git validation. Those pages teach individual techniques. This page shows complete policies combining those techniques.\n\nHow to use these examples\nCopy the policy code into a Dockerfile.rego file next to your Dockerfile\nCustomize any todo comments with your specific values\nTest by running docker build . and verifying the policy works as expected\nRefine based on your team's needs\nUsing examples with bake\n\nThese policies work with both docker buildx build and docker buildx bake. For bake, place the policy alongside your Dockerfile and it loads automatically. To use additional policies:\n\ntarget \"default\" {\n\n  dockerfile = \"Dockerfile\"\n\n  policy = [\"extra.rego\"]\n\n}\n\nSee the Usage guide for complete bake integration details.\n\nGetting started\n\nThese policies work immediately with minimal or no customization. Use them to adopt policies quickly and demonstrate value to your team.\n\nDevelopment-friendly baseline\n\nA permissive policy that allows typical development workflows while blocking obvious security issues.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\nallow if input.git\n\n\n\n# Allow common public registries\n\nallow if {\n\n  input.image.host == \"docker.io\"  # Docker Hub\n\n}\n\n\n\nallow if {\n\n  input.image.host == \"ghcr.io\"  # GitHub Container Registry\n\n}\n\n\n\nallow if {\n\n  input.image.host == \"dhi.io\"  # Docker Hardened Images\n\n}\n\n\n\n# Require HTTPS for all downloads\n\nallow if {\n\n  input.http.schema == \"https\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis policy allows local and Git contexts, images from Docker Hub, GitHub Container Registry, and Docker Hardened Images, and ADD downloads over HTTPS. It blocks HTTP downloads and non-standard registries.\n\nWhen to use: Starting point for teams new to policies. Provides basic security without disrupting development workflows.\n\nRegistry allowlist\n\nControl which registries your builds can pull images from.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# TODO: Add your internal registry hostname\n\nallowed_registries := [\"docker.io\", \"ghcr.io\", \"dhi.io\", \"registry.company.com\"]\n\n\n\nallow if {\n\n  input.image.host in allowed_registries\n\n}\n\n\n\n# Allow mirrored DHI images from Docker Hub (DHI Enterprise users)\n\n# TODO: Replace with your organization namespace\n\nallow if {\n\n  input.image.host == \"docker.io\"\n\n  startswith(input.image.repo, \"myorg/dhi-\")\n\n}\n\n\n\ndeny_msg contains msg if {\n\n  not allow\n\n  input.image\n\n  msg := sprintf(\"registry %s is not in the allowlist\", [input.image.host])\n\n}\n\n\n\ndecision := {\"allow\": allow, \"deny_msg\": deny_msg}\n\nThis policy restricts image pulls to approved registries. Customize and add your internal registry to the list. If you have a DHI Enterprise subscription and have mirrored Docker Hardened Images to Docker Hub, add a rule to allow images from your organization's namespace.\n\nWhen to use: Enforce corporate policies about approved image sources. Prevents developers from using arbitrary public registries.\n\nPin base images to digests\n\nRequire digest references for reproducible builds.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# Require digest references for all images\n\nallow if {\n\n  input.image.isCanonical\n\n}\n\n\n\ndeny_msg contains msg if {\n\n  not allow\n\n  input.image\n\n  msg := sprintf(\"image %s must use digest reference (e.g., @sha256:...)\", [input.image.ref])\n\n}\n\n\n\ndecision := {\"allow\": allow, \"deny_msg\": deny_msg}\n\nThis policy requires images use digest references like alpine@sha256:abc123... instead of tags like alpine:3.19. Digests are immutable - the same digest always resolves to the same image content.\n\nWhen to use: Ensure build reproducibility. Prevents builds from breaking when upstream tags are updated. Required for compliance in some environments.\n\nControl external dependencies\n\nPin specific versions of dependencies downloaded during builds.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# Allow any image (add restrictions as needed)\n\nallow if input.image\n\n\n\n# TODO: Add your allowed Git repositories and tags\n\nallowed_repos := {\n\n  \"https://github.com/moby/buildkit.git\": [\"v0.26.1\", \"v0.27.0\"],\n\n}\n\n# Only allow Git input from allowed_repos\n\nallow if {\n\n  some repo, versions in allowed_repos\n\n  input.git.remote == repo\n\n  input.git.tagName in versions\n\n}\n\n\n\n# TODO: Add your allowed downloads\n\nallow if {\n\n  input.http.url == \"https://example.com/app-v1.0.tar.gz\"\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis policy creates allowlists for external dependencies. Add your Git repositories with approved version tags, and URLs.\n\nWhen to use: Control which external dependencies can be used in builds. Prevents builds from pulling arbitrary versions or unverified downloads.\n\nProduction templates\n\nThese templates demonstrate comprehensive security patterns. They require customization but show best practices for production environments.\n\nImage attestation and provenance\n\nRequire images have provenance attestations from trusted builders.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# TODO: Add your repository names\n\nallowed_repos := [\"myorg/backend\", \"myorg/frontend\", \"myorg/worker\"]\n\n\n\n# Production images need full attestations\n\nallow if {\n\n  some repo in allowed_repos\n\n  input.image.repo == repo\n\n  input.image.hasProvenance\n\n  some sig in input.image.signatures\n\n  trusted_github_builder(sig, repo)\n\n}\n\n\n\n# Helper to validate GitHub Actions build from main branch\n\ntrusted_github_builder(sig, repo) if {\n\n  sig.signer.certificateIssuer == \"CN=sigstore-intermediate,O=sigstore.dev\"\n\n  sig.signer.issuer == \"https://token.actions.githubusercontent.com\"\n\n  startswith(sig.signer.buildSignerURI, sprintf(\"https://github.com/myorg/%s/.github/workflows/\", [repo]))\n\n  sig.signer.sourceRepositoryRef == \"refs/heads/main\"\n\n  sig.signer.runnerEnvironment == \"github-hosted\"\n\n}\n\n\n\n# Allow Docker Hardened Images with built-in attestations\n\nallow if {\n\n  input.image.host == \"dhi.io\"\n\n  input.image.isCanonical\n\n  input.image.hasProvenance\n\n}\n\n\n\n# Allow official base images with digests\n\nallow if {\n\n  input.image.repo == \"alpine\"\n\n  input.image.host == \"docker.io\"\n\n  input.image.isCanonical\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis template validates that your application images have provenance attestations, and were built by GitHub Actions from your main branch. Docker Hardened Images are allowed when using digests since they include comprehensive attestations by default. Other base images must use digests.\n\nCustomize:\n\nReplace allowed_repos with your image names\nUpdate the organization name in trusted_github_builder()\nAdd rules for other base images you use\n\nWhen to use: Enforce supply chain security for production deployments. Ensures images are built by trusted CI/CD pipelines with auditable provenance.\n\nSigned Git releases\n\nEnforce signed tags from trusted maintainers for Git dependencies.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\nallow if input.image\n\n\n\n# TODO: Replace with your repository URL\n\nis_buildkit if {\n\n    input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n}\n\n\n\nis_version_tag if {\n\n    is_buildkit\n\n    regex.match(`^v[0-9]+\\.[0-9]+\\.[0-9]+$`, input.git.tagName)\n\n}\n\n\n\n# Version tags must be signed\n\nallow if {\n\n    is_version_tag\n\n    input.git.tagName != \"\"\n\n    verify_git_signature(input.git.tag, \"maintainers.asc\")\n\n}\n\n\n\n# Allow unsigned refs for development\n\nallow if {\n\n    is_buildkit\n\n    not is_version_tag\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis template requires production release tags to be signed by trusted maintainers. Development branches and commits can be unsigned.\n\nSetup:\n\nExport maintainer PGP public keys to maintainers.asc:\n$ gpg --export --armor user1@example.com user2@example.com > maintainers.asc\n\nPlace maintainers.asc in the same directory as your policy file\n\nCustomize:\n\nReplace the repository URL in is_buildkit\nUpdate the maintainers in the PGP keyring file\nAdjust the version tag regex pattern if needed\n\nWhen to use: Validate that production dependencies come from signed releases. Protects against compromised releases or unauthorized updates.\n\nMulti-registry policy\n\nApply different validation rules for internal and external registries.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# TODO: Replace with your internal registry hostname\n\ninternal_registry := \"registry.company.com\"\n\n\n\n# Internal registry: basic validation\n\nallow if {\n\n  input.image.host == internal_registry\n\n}\n\n\n\n# External registries: strict validation\n\nallow if {\n\n  input.image.host != internal_registry\n\n  input.image.host != \"\"\n\n  input.image.isCanonical\n\n  input.image.hasProvenance\n\n}\n\n\n\n# Docker Hub: allowlist specific images\n\nallow if {\n\n  input.image.host == \"docker.io\"\n\n  # TODO: Add your approved base images\n\n  input.image.repo in [\"alpine\", \"golang\", \"node\"]\n\n  input.image.isCanonical\n\n}\n\n\n\n# Docker Hardened Images: trusted by default with built-in attestations\n\nallow if {\n\n  input.image.host == \"dhi.io\"\n\n  input.image.isCanonical\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis template defines a trust boundary between internal and external image sources. Internal images require minimal validation, while external images need digests and provenance. Docker Hardened Images from dhi.io are treated as trusted since they include comprehensive attestations and security guarantees.\n\nCustomize:\n\nSet your internal registry hostname\nAdd your approved Docker Hub base images\nAdjust validation requirements based on your security policies\n\nWhen to use: Organizations with internal registries that need different rules for internal and external sources. Balances security with practical workflow needs.\n\nMulti-environment policy\n\nApply different rules based on the build target or stage. For example,\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# TODO: Define your environment detection logic\n\nis_production if {\n\n  input.env.target == \"production\"\n\n}\n\n\n\nis_development if {\n\n  input.env.target == \"development\"\n\n}\n\n\n\n# Production: strict rules - only digest images with provenance\n\nallow if {\n\n  is_production\n\n  input.image.isCanonical\n\n  input.image.hasProvenance\n\n}\n\n\n\n# Development: permissive rules - any image\n\nallow if {\n\n  is_development\n\n  input.image\n\n}\n\n\n\n# Staging inherits production rules (default target detection)\n\nallow if {\n\n  not is_production\n\n  not is_development\n\n  input.image.isCanonical\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis template uses build targets to apply different validation levels. Production requires attestations and digests, development is permissive, and staging uses moderate rules.\n\nCustomize:\n\nUpdate environment detection logic (target names, build args, etc.)\nAdjust validation requirements for each environment\nAdd more environments as needed\n\nWhen to use: Teams with separate build configurations for different deployment stages. Allows flexibility in development while enforcing strict rules for production.\n\nComplete dependency pinning\n\nPin all external dependencies to specific versions across all input types.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# TODO: Add your pinned images with exact digests\n\n# Docker Hub images use docker.io as host\n\nallowed_dockerhub := {\n\n  \"alpine\": \"sha256:4b7ce07002c69e8f3d704a9c5d6fd3053be500b7f1c69fc0d80990c2ad8dd412\",\n\n  \"golang\": \"sha256:abc123...\",\n\n}\n\n\n\nallow if {\n\n  input.image.host == \"docker.io\"\n\n  some repo, digest in allowed_dockerhub\n\n  input.image.repo == repo\n\n  input.image.checksum == digest\n\n}\n\n\n\n# TODO: Add your pinned DHI images\n\nallowed_dhi := {\n\n  \"python\": \"sha256:def456...\",\n\n  \"node\": \"sha256:ghi789...\",\n\n}\n\n\n\nallow if {\n\n  input.image.host == \"dhi.io\"\n\n  some repo, digest in allowed_dhi\n\n  input.image.repo == repo\n\n  input.image.checksum == digest\n\n}\n\n\n\n# TODO: Add your pinned Git dependencies\n\nallowed_git := {\n\n  \"https://github.com/moby/buildkit.git\": {\n\n    \"tag\": \"v0.26.1\",\n\n    \"commit\": \"abc123...\",\n\n  },\n\n}\n\n\n\nallow if {\n\n  some url, version in allowed_git\n\n  input.git.remote == url\n\n  input.git.tagName == version.tag\n\n  input.git.commitChecksum == version.commit\n\n}\n\n\n\n# TODO: Add your pinned HTTP downloads\n\nallowed_downloads := {\n\n  \"https://releases.example.com/app-v1.0.tar.gz\": \"sha256:def456...\",\n\n}\n\n\n\nallow if {\n\n  some url, checksum in allowed_downloads\n\n  input.http.url == url\n\n  input.http.checksum == checksum\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis template pins every external dependency to exact versions with cryptographic verification. Images use digests, Git repos use commit SHAs, and downloads use checksums.\n\nCustomize:\n\nAdd all your dependencies with exact versions/checksums\nMaintain this file when updating dependencies\nConsider automating updates through CI/CD\n\nWhen to use: Maximum reproducibility and security. Ensures builds always use exact versions of all dependencies. Required for high-security or regulated environments.\n\nManual signature verification\n\nVerify image signatures by inspecting signature metadata fields.\n\npackage docker\n\n\n\ndefault allow := false\n\n\n\nallow if input.local\n\n\n\n# Require valid GitHub Actions signatures\n\nallow if {\n\n    input.image\n\n    input.image.hasProvenance\n\n    some sig in input.image.signatures\n\n    valid_github_signature(sig)\n\n}\n\n\n\n# Helper function to validate GitHub Actions signature\n\nvalid_github_signature(sig) if {\n\n    # Sigstore keyless signing\n\n    sig.signer.certificateIssuer == \"CN=sigstore-intermediate,O=sigstore.dev\"\n\n    sig.signer.issuer == \"https://token.actions.githubusercontent.com\"\n\n\n\n    # TODO: Replace with your organization\n\n    startswith(sig.signer.buildSignerURI, \"https://github.com/myorg/.github/workflows/\")\n\n    startswith(sig.signer.sourceRepositoryURI, \"https://github.com/myorg/\")\n\n\n\n    # Verify GitHub hosted runner\n\n    sig.signer.runnerEnvironment == \"github-hosted\"\n\n\n\n    # Require timestamp\n\n    count(sig.timestamps) > 0\n\n}\n\n\n\ndecision := {\"allow\": allow}\n\nThis policy validates that images were built by GitHub Actions using Sigstore keyless signing.\n\nCustomize:\n\nReplace myorg with your GitHub organization\nAdjust workflow path restrictions\nAdd additional signature field checks as needed\n\nWhen to use: Enforce that images are built by CI/CD with verifiable signatures, not manually pushed by developers.\n\nNext steps\nWrite unit tests for your policies: Test build policies\nReview Built-in functions for signature verification and attestation checking\nCheck the Input reference for all available fields you can validate\nRead the tutorials for detailed explanations: Introduction, Image validation, Git validation\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow to use these examples\nUsing examples with bake\nGetting started\nDevelopment-friendly baseline\nRegistry allowlist\nPin base images to digests\nControl external dependencies\nProduction templates\nImage attestation and provenance\nSigned Git releases\nMulti-registry policy\nMulti-environment policy\nComplete dependency pinning\nManual signature verification\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995899,
    "timestamp": "2026-02-07T06:37:09.684Z",
    "title": "Debugging | Docker Docs",
    "url": "https://docs.docker.com/build/policies/debugging/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\n/\nDebugging\nDebugging build policies\nCopy as Markdown\n\nWhen policies don't work as expected, use the tools available to inspect policy evaluation and understand what's happening. This guide covers the debugging techniques and common gotchas.\n\nQuick reference\n\nEssential debugging commands:\n\n# See complete input data during builds (recommended)\n\n$ docker buildx build --progress=plain --policy log-level=debug .\n\n\n\n# See policy checks and decisions\n\n$ docker buildx build --progress=plain .\n\n\n\n# Explore input structure for different sources\n\n$ docker buildx policy eval --print .\n\n$ docker buildx policy eval --print https://github.com/org/repo.git\n\n$ docker buildx policy eval --print docker-image://alpine:3.19\n\n\n\n# Test if policy allows a source\n\n$ docker buildx policy eval .\n\nPolicy output with --progress=plain\n\nTo see policy evaluation during builds, use --progress=plain:\n\n$ docker buildx build --progress=plain .\n\n\nThis shows all policy checks, decisions, and print() output. Without --progress=plain, policy evaluation is silent unless there's an error.\n\n#1 loading policies Dockerfile.rego\n\n#1 0.010 checking policy for source docker-image://alpine:3.19 (linux/arm64)\n\n#1 0.011 Dockerfile.rego:8: image: {\"ref\":\"alpine:3.19\",\"repo\":\"alpine\",\"tag\":\"3.19\"}\n\n#1 0.012 policy decision for source docker-image://alpine:3.19: ALLOW\n\nIf a policy denies a source, you'll see:\n\n#1 0.012 policy decision for source docker-image://nginx:latest: DENY\n\nERROR: source \"docker-image://nginx:latest\" not allowed by policy\nDebug logging\n\nFor detailed debugging, add --policy log-level=debug to see the full input JSON, unresolved fields, and policy responses:\n\n$ docker buildx build --progress=plain --policy log-level=debug .\n\n\nThis shows significantly more information than the default level, including the complete input structure for each source without needing print() statements in your policy.\n\nComplete input JSON:\n\n#1 0.007 policy input: {\n\n#1 0.007   \"env\": {\n\n#1 0.007     \"filename\": \".\"\n\n#1 0.007   },\n\n#1 0.007   \"image\": {\n\n#1 0.007     \"ref\": \"docker.io/library/alpine:3.19\",\n\n#1 0.007     \"host\": \"docker.io\",\n\n#1 0.007     \"repo\": \"alpine\",\n\n#1 0.007     \"fullRepo\": \"docker.io/library/alpine\",\n\n#1 0.007     \"tag\": \"3.19\",\n\n#1 0.007     \"platform\": \"linux/arm64\",\n\n#1 0.007     \"os\": \"linux\",\n\n#1 0.007     \"arch\": \"arm64\"\n\n#1 0.007   }\n\n#1 0.007 }\n\nUnresolved fields:\n\n#1 0.007 unknowns for policy evaluation: [input.image.checksum input.image.labels input.image.user input.image.volumes input.image.workingDir input.image.env input.image.hasProvenance input.image.signatures]\n\nPolicy response:\n\n#1 0.008 policy response: map[allow:true]\n\nThis detailed output is invaluable for understanding exactly what data your policy receives and which fields are not yet resolved. Use debug logging when developing policies to avoid needing extensive print() statements.\n\nConditional debugging with print()\n\nWhile --policy log-level=debug shows all input data automatically, the print() function is useful for debugging specific rule logic and conditional flows:\n\nallow if {\n\n    input.image\n\n    print(\"Checking image:\", input.image.repo, \"isCanonical:\", input.image.isCanonical)\n\n    input.image.repo == \"alpine\"\n\n    input.image.isCanonical\n\n}\n\nUse print() to debug conditional logic within rules or track which rules are evaluating. For general input inspection during development, use --policy log-level=debug instead - it requires no policy modifications.\n\nNote\n\nPrint statements only execute when their containing rule evaluates. A rule like allow if { input.image; print(...) } only prints for image inputs, not for Git repos, HTTP downloads, or local files.\n\nCommon issues\nFull repository path or repository name\n\nSymptom: Policy checking repository names doesn't match as expected.\n\nCause: Docker Hub images use input.image.repo for the short name (\"alpine\") but input.image.fullRepo includes the full path (\"docker.io/library/alpine\").\n\nSolution:\n\n# Match just the repo name (works for Docker Hub and other registries)\n\nallow if {\n\n    input.image\n\n    input.image.repo == \"alpine\"\n\n}\n\n\n\n# Or match the full repository path\n\nallow if {\n\n    input.image\n\n    input.image.fullRepo == \"docker.io/library/alpine\"\n\n}\nPolicy evaluation happens multiple times\n\nSymptom: Build output shows the same source evaluated multiple times.\n\nCause: BuildKit may evaluate policies at different stages (reference resolution, actual pull) or for different platforms.\n\nThis is normal behavior. Policies should be idempotent (produce same result each time for the same input).\n\nFields missing with policy eval --print\n\nSymptom: docker buildx policy eval --print doesn't show expected fields like hasProvenance, labels, or checksum.\n\nCause: --print shows only reference information by default, without fetching from registries.\n\nSolution: Use --fields to fetch specific metadata fields:\n\n$ docker buildx policy eval --print --fields image.labels docker-image://alpine:3.19\n\n\nSee Using build policies for details.\n\nNext steps\nSee complete field reference: Input reference\nReview example policies: Examples\nLearn policy usage patterns: Using build policies\n\nEdit this page\n\nRequest changes\n\nTable of contents\nQuick reference\nPolicy output with --progress=plain\nDebug logging\nConditional debugging with print()\nCommon issues\nFull repository path or repository name\nPolicy evaluation happens multiple times\nFields missing with policy eval --print\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995902,
    "timestamp": "2026-02-07T06:37:09.704Z",
    "title": "Input reference | Docker Docs",
    "url": "https://docs.docker.com/build/policies/inputs/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\n/\nInput reference\nInput reference\nCopy as Markdown\n\nWhen Buildx evaluates policies, it provides information about build inputs through the input object. The structure of input depends on the type of resource your Dockerfile references.\n\nInput types\n\nBuild inputs correspond to Dockerfile instructions:\n\nDockerfile instruction\tInput type\tAccess pattern\nFROM alpine:latest\tImage\tinput.image\nCOPY --from=builder /app /app\tImage\tinput.image\nADD https://example.com/file.tar.gz /\tHTTP\tinput.http\nADD git@github.com:user/repo.git /src\tGit\tinput.git\nBuild context (.)\tLocal\tinput.local\n\nEach input type has specific fields available for policy evaluation.\n\nHTTP inputs\n\nHTTP inputs represent files downloaded over HTTP or HTTPS using the ADD instruction.\n\nExample Dockerfile\nFROM alpine\n\nADD --checksum=sha256:abc123... https://example.com/app.tar.gz /app.tar.gz\nAvailable fields\ninput.http.url\n\nThe complete URL of the resource.\n\nallow if {\n\n    input.http.url == \"https://example.com/app.tar.gz\"\n\n}\ninput.http.schema\n\nThe URL scheme (http or https).\n\n# Require HTTPS for all downloads\n\nallow if {\n\n    input.http.schema == \"https\"\n\n}\ninput.http.host\n\nThe hostname from the URL.\n\n# Allow downloads from approved domains\n\nallow if {\n\n    input.http.host == \"cdn.example.com\"\n\n}\ninput.http.path\n\nThe path component of the URL.\n\nallow if {\n\n    startswith(input.http.path, \"/releases/\")\n\n}\ninput.http.checksum\n\nThe checksum specified with ADD --checksum=..., if present. Empty string if no checksum was provided.\n\n# Require checksums for all downloads\n\nallow if {\n\n    input.http.checksum != \"\"\n\n}\ninput.http.hasAuth\n\nBoolean indicating if the request includes authentication (HTTP basic auth or bearer token).\n\n# Require authentication for internal servers\n\nallow if {\n\n    input.http.host == \"internal.company.com\"\n\n    input.http.hasAuth\n\n}\nImage inputs\n\nImage inputs represent container images from FROM instructions or COPY --from references.\n\nExample Dockerfile\nFROM alpine:3.19@sha256:abc123...\n\nCOPY --from=builder:latest /app /app\nAvailable fields\ninput.image.ref\n\nThe complete image reference as written in the Dockerfile.\n\nallow if {\n\n    input.image.ref == \"alpine:3.19@sha256:abc123...\"\n\n}\ninput.image.host\n\nThe registry hostname. Docker Hub images use \"docker.io\".\n\n# Only allow Docker Hub images\n\nallow if {\n\n    input.image.host == \"docker.io\"\n\n}\n\n\n\n# Only allow images from GitHub Container Registry\n\nallow if {\n\n    input.image.host == \"ghcr.io\"\n\n}\ninput.image.repo\n\nThe repository name without the registry host.\n\nallow if {\n\n    input.image.repo == \"library/alpine\"\n\n}\ninput.image.fullRepo\n\nThe full repository path including registry host.\n\nallow if {\n\n    input.image.fullRepo == \"docker.io/library/alpine\"\n\n}\ninput.image.tag\n\nThe tag portion of the reference. Empty if using a digest reference.\n\n# Allow only specific tags\n\nallow if {\n\n    input.image.tag == \"3.19\"\n\n}\ninput.image.isCanonical\n\nBoolean indicating if the reference uses a digest (@sha256:...).\n\n# Require digest references\n\nallow if {\n\n    input.image.isCanonical\n\n}\ninput.image.checksum\n\nThe SHA256 digest of the image manifest.\n\nallow if {\n\n    input.image.checksum == \"sha256:abc123...\"\n\n}\ninput.image.platform\n\nThe target platform for multi-platform images.\n\nallow if {\n\n    input.image.platform == \"linux/amd64\"\n\n}\ninput.image.os\n\nThe operating system from the image configuration.\n\nallow if {\n\n    input.image.os == \"linux\"\n\n}\ninput.image.arch\n\nThe CPU architecture from the image configuration.\n\nallow if {\n\n    input.image.arch == \"amd64\"\n\n}\ninput.image.hasProvenance\n\nBoolean indicating if the image has provenance attestations.\n\n# Require provenance for production images\n\nallow if {\n\n    input.image.hasProvenance\n\n}\ninput.image.labels\n\nA map of image labels from the image configuration.\n\n# Check for specific labels\n\nallow if {\n\n    input.image.labels[\"org.opencontainers.image.vendor\"] == \"Example Corp\"\n\n}\ninput.image.signatures\n\nArray of attestation signatures. Each signature in the array has the following fields:\n\nkind: Signature kind (e.g., \"docker-github-builder\", \"self-signed\")\ntype: Signature type (e.g., \"bundle-v0.3\", \"simplesigning-v1\")\ntimestamps: Trusted timestamps from transparency logs\ndockerReference: Docker image reference\nisDHI: Boolean indicating if this is a Docker Hardened Image\nsigner: Sigstore certificate details\n# Require at least one signature\n\nallow if {\n\n    count(input.image.signatures) > 0\n\n}\n\nFor Sigstore signatures, the signer object provides detailed certificate information from the signing workflow:\n\ncertificateIssuer: Certificate issuer\nsubjectAlternativeName: Subject alternative name from certificate\nbuildSignerURI: URI of the build signer\nbuildSignerDigest: Digest of the build signer\nrunnerEnvironment: CI/CD runner environment\nsourceRepositoryURI: Source repository URL\nsourceRepositoryDigest: Source repository digest\nsourceRepositoryRef: Source repository ref (branch/tag)\nsourceRepositoryIdentifier: Source repository identifier\nsourceRepositoryOwnerURI: Repository owner URI\nbuildConfigURI: Build configuration URI\nbuildTrigger: What triggered the build\nrunInvocationURI: CI/CD run invocation URI\n# Require signatures from GitHub Actions\n\nallow if {\n\n    some sig in input.image.signatures\n\n    sig.signer.runnerEnvironment == \"github-hosted\"\n\n    startswith(sig.signer.sourceRepositoryURI, \"https://github.com/myorg/\")\n\n}\nGit inputs\n\nGit inputs represent Git repositories referenced in ADD instructions or used as build context.\n\nExample Dockerfile\nADD git@github.com:moby/buildkit.git#v0.12.0 /src\nAvailable fields\ninput.git.schema\n\nThe URL scheme (https, http, git, or ssh).\n\n# Require HTTPS for Git clones\n\nallow if {\n\n    input.git.schema == \"https\"\n\n}\ninput.git.host\n\nThe Git host (e.g., github.com, gitlab.com).\n\nallow if {\n\n    input.git.host == \"github.com\"\n\n}\ninput.git.remote\n\nThe complete Git URL.\n\nallow if {\n\n    input.git.remote == \"https://github.com/moby/buildkit.git\"\n\n}\ninput.git.ref\n\nThe Git reference.\n\nallow if {\n\n    input.git.ref == \"refs/heads/master\"\n\n}\ninput.git.tagName\n\nThe tag name if the reference is a tag.\n\n# Only allow version tags\n\nallow if {\n\n    regex.match(`^v[0-9]+\\.[0-9]+\\.[0-9]+$`, input.git.tagName)\n\n}\ninput.git.branch\n\nThe branch name if the reference is a branch.\n\nallow if {\n\n    input.git.branch == \"main\"\n\n}\ninput.git.subDir\n\nThe subdirectory path within the repository, if specified.\n\n# Ensure clones are from the root\n\nallow if {\n\n    input.git.subDir == \"\"\n\n}\ninput.git.isCommitRef\n\nBoolean indicating if the reference is a commit SHA (as opposed to a branch or tag name).\n\n# Require commit SHAs for production\n\nallow if {\n\n    input.env.target == \"production\"\n\n    input.git.isCommitRef\n\n}\ninput.git.checksum\n\nThe checksum of the Git reference. For commit references and branches, this is the commit hash. For annotated tags, this is the tag object hash.\n\nallow if {\n\n    input.git.checksum == \"abc123...\"\n\n}\ninput.git.commitChecksum\n\nThe commit hash that the reference points to. For annotated tags, this differs from checksum (which is the tag object hash). For commit references and branches, this is the same as checksum.\n\nallow if {\n\n    input.git.commitChecksum == \"abc123...\"\n\n}\ninput.git.isAnnotatedTag\n\nBoolean indicating if the reference is an annotated tag (as opposed to a lightweight tag).\n\n# Require annotated tags\n\nallow if {\n\n    input.git.tagName != \"\"\n\n    input.git.isAnnotatedTag\n\n}\ninput.git.commit\n\nObject containing commit metadata:\n\nauthor: Author name, email, when\ncommitter: Committer name, email, when\nmessage: Commit message\npgpSignature: PGP signature details if signed\nsshSignature: SSH signature details if signed\n# Check commit author\n\nallow if {\n\n    input.git.commit.author.email == \"maintainer@example.com\"\n\n}\ninput.git.tag\n\nObject containing tag metadata for annotated tags:\n\ntagger: Tagger name, email, when\nmessage: Tag message\npgpSignature: PGP signature details if signed\nsshSignature: SSH signature details if signed\n# Require signed tags\n\nallow if {\n\n    input.git.tag.pgpSignature != null\n\n}\nLocal inputs\n\nLocal inputs represent the build context directory.\n\nAvailable fields\ninput.local.name\n\nThe name or path of the local context.\n\nallow if {\n\n    input.local.name == \".\"\n\n}\n\nLocal inputs are typically less restricted than remote inputs, but you can still write policies to enforce context requirements.\n\nEnvironment fields\n\nThe input.env object provides build configuration information set by user on invoking the build, not specific to a resource type.\n\nAvailable fields\ninput.env.filename\n\nThe name of the Dockerfile being built.\n\n# Stricter rules for production Dockerfile\n\nallow if {\n\n    input.env.filename == \"Dockerfile\"\n\n    input.image.isCanonical\n\n}\n\n\n\n# Relaxed rules for development\n\nallow if {\n\n    input.env.filename == \"Dockerfile.dev\"\n\n}\ninput.env.target\n\nThe build target from multi-stage builds.\n\n# Require signing only for release builds\n\nallow if {\n\n    input.env.target == \"release\"\n\n    input.git.tagName != \"\"\n\n    verify_git_signature(input.git.tag, \"maintainer.asc\")\n\n}\ninput.env.args\n\nBuild arguments passed with --build-arg. Access specific arguments by key.\n\n# Check build argument values\n\nallow if {\n\n    input.env.args.ENVIRONMENT == \"production\"\n\n    input.image.hasProvenance\n\n}\nNext steps\nSee Built-in functions for built-in helper functions to check and validate input properties\nBrowse Example policies for common patterns\nRead about Rego for advanced policy logic\n\nEdit this page\n\nRequest changes\n\nTable of contents\nInput types\nHTTP inputs\nExample Dockerfile\nAvailable fields\nImage inputs\nExample Dockerfile\nAvailable fields\nGit inputs\nExample Dockerfile\nAvailable fields\nLocal inputs\nAvailable fields\nEnvironment fields\nAvailable fields\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995905,
    "timestamp": "2026-02-07T06:37:09.708Z",
    "title": "Built-in functions | Docker Docs",
    "url": "https://docs.docker.com/build/policies/built-ins/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nIntroduction\nUsage\nImage validation\nGit validation\nTemplates & examples\nTesting\nDebugging\nInput reference\nBuilt-in functions\nMetadata\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nValidating builds\n/\nBuilt-in functions\nBuilt-in functions\nCopy as Markdown\n\nBuildx provides built-in functions, in addition to the Rego built-ins, to extend Rego policies with Docker-specific operations like loading local files, verifying Git signatures, and pinning image digests.\n\nRego built-in functions\n\nThe functions documented on this page are Buildx-specific functions, distinct from Rego's standard built-in functions\n\nBuildx also supports standard Rego built-in functions, but only a subset. To see the exact list of supported functions, refer to the Buildx source code.\n\nBuildx built-in functions\n\nBuildx provides the following custom built-in functions for policy development:\n\nprint\nload_json\nverify_git_signature\npin_image\nprint\n\nOutputs debug information during policy evaluation.\n\nParameters:\n\nAny number of values to print\n\nReturns: The values (pass-through)\n\nExample:\n\nallow if {\n\n    input.image.repo == \"alpine\"\n\n    print(\"Allowing alpine image:\", input.image.tag)\n\n}\n\nDebug output appears when building with --progress=plain.\n\nload_json\n\nLoads and parses JSON data from local files in the build context.\n\nParameters:\n\nfilename (string) - Path to JSON file relative to policy directory\n\nReturns: Parsed JSON data as Rego value\n\nExample:\n\n# Load approved versions from external file\n\napproved_versions = load_json(\"versions.json\")\n\n\n\nallow if {\n\n    input.image.repo == \"alpine\"\n\n    some version in approved_versions.alpine\n\n    input.image.tag == version\n\n}\n\nFile structure:\n\nproject/\n\n‚îú‚îÄ‚îÄ Dockerfile\n\n‚îú‚îÄ‚îÄ Dockerfile.rego\n\n‚îî‚îÄ‚îÄ versions.json\n\nversions.json:\n\n{\n\n  \"alpine\": [\"3.19\", \"3.20\"],\n\n  \"golang\": [\"1.21\", \"1.22\"]\n\n}\n\nThe JSON file must be in the same directory as the policy or in a subdirectory accessible from the policy location.\n\nverify_git_signature\n\nVerifies PGP signatures on Git commits or tags.\n\nParameters:\n\ngit_object (object) - Either input.git.commit or input.git.tag\nkeyfile (string) - Path to PGP public key file (relative to policy directory)\n\nReturns: Boolean - true if signature is valid, false otherwise\n\nExample:\n\n# Require signed Git tags\n\nallow if {\n\n    input.git.tagName != \"\"\n\n    verify_git_signature(input.git.tag, \"maintainer.asc\")\n\n}\n\n\n\n# Require signed commits\n\nallow if {\n\n    input.git.commit\n\n    verify_git_signature(input.git.commit, \"keys/team.asc\")\n\n}\n\nDirectory structure:\n\nproject/\n\n‚îú‚îÄ‚îÄ Dockerfile.rego\n\n‚îî‚îÄ‚îÄ maintainer.asc          # PGP public key\n\nOr with subdirectory:\n\nproject/\n\n‚îú‚îÄ‚îÄ Dockerfile.rego\n\n‚îî‚îÄ‚îÄ keys/\n\n    ‚îú‚îÄ‚îÄ maintainer.asc\n\n    ‚îî‚îÄ‚îÄ team.asc\n\nObtaining public keys:\n\n$ gpg --export --armor user@example.com > maintainer.asc\n\npin_image\n\nPins an image to a specific digest, overriding the tag-based reference. Use this to force builds to use specific image versions.\n\nParameters:\n\nimage_object (object) - Must be input.image (the current image being evaluated)\ndigest (string) - Target digest in format sha256:...\n\nReturns: Boolean - true if pinning succeeds\n\nExample:\n\n# Pin alpine 3.19 to specific digest\n\nalpine_3_19_digest = \"sha256:4b7ce07002c69e8f3d704a9c5d6fd3053be500b7f1c69fc0d80990c2ad8dd412\"\n\n\n\nallow if {\n\n    input.image.repo == \"alpine\"\n\n    input.image.tag == \"3.19\"\n\n    pin_image(input.image, alpine_3_19_digest)\n\n}\n\nAutomatic digest replacement:\n\n# Replace old digests with patched versions\n\nreplace_map = {\n\n  \"3.22.0\": \"3.22.2\",\n\n  \"3.22.1\": \"3.22.2\",\n\n}\n\n\n\nalpine_digests = {\n\n  \"3.22.0\": \"sha256:8a1f59ffb675680d47db6337b49d22281a139e9d709335b492be023728e11715\",\n\n  \"3.22.2\": \"sha256:4b7ce07002c69e8f3d704a9c5d6fd3053be500b7f1c69fc0d80990c2ad8dd412\",\n\n}\n\n\n\nallow if {\n\n    input.image.repo == \"alpine\"\n\n    some old_version, new_version in replace_map\n\n    input.image.checksum == alpine_digests[old_version]\n\n    print(\"Replacing\", old_version, \"with\", new_version)\n\n    pin_image(input.image, alpine_digests[new_version])\n\n}\n\nThis pattern automatically upgrades old image versions to patched releases.\n\nNext steps\nBrowse complete examples: Example policies\nLearn policy development workflow: Using build policies\nReference input fields: Input reference\n\nEdit this page\n\nRequest changes\n\nTable of contents\nRego built-in functions\nBuildx built-in functions\nprint\nload_json\nverify_git_signature\npin_image\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995910,
    "timestamp": "2026-02-07T06:37:09.711Z",
    "title": "Annotations | Docker Docs",
    "url": "https://docs.docker.com/build/metadata/annotations/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nAnnotations\nBuild attestations\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nMetadata\n/\nAnnotations\nAnnotations\nCopy as Markdown\n\nAnnotations provide descriptive metadata for images. Use annotations to record arbitrary information and attach it to your image, which helps consumers and tools understand the origin, contents, and how to use the image.\n\nAnnotations are similar to, and in some sense overlap with, labels. Both serve the same purpose: to attach metadata to a resource. As a general principle, you can think of the difference between annotations and labels as follows:\n\nAnnotations describe OCI image components, such as manifests, indexes, and descriptors.\nLabels describe Docker resources, such as images, containers, networks, and volumes.\n\nThe OCI image specification defines the format of annotations, as well as a set of pre-defined annotation keys. Adhering to the specified standards ensures that metadata about images can be surfaced automatically and consistently, by tools like Docker Scout.\n\nAnnotations are not to be confused with attestations:\n\nAttestations contain information about how an image was built and what it contains. An attestation is attached as a separate manifest on the image index. Attestations are not standardized by the Open Container Initiative.\nAnnotations contain arbitrary metadata about an image. Annotations attach to the image config as labels, or on the image index or manifest as properties.\nAdd annotations\n\nYou can add annotations to an image at build-time, or when creating the image manifest or index.\n\nNote\n\nThe Docker Engine image store doesn't support loading images with annotations. To build with annotations, make sure to push the image directly to a registry, using the --push CLI flag or the registry exporter.\n\nTo specify annotations on the command line, use the --annotation flag for the docker build command:\n\n$ docker build --push --annotation \"foo=bar\" .\n\n\nIf you're using Bake, you can use the annotations attribute to specify annotations for a given target:\n\ntarget \"default\" {\n\n  output = [\"type=registry\"]\n\n  annotations = [\"foo=bar\"]\n\n}\n\nFor examples on how to add annotations to images built with GitHub Actions, see Add image annotations with GitHub Actions\n\nYou can also add annotations to an image created using docker buildx imagetools create. This command only supports adding annotations to an index or manifest descriptors, see CLI reference.\n\nInspect annotations\n\nTo view annotations on an image index, use the docker buildx imagetools inspect command. This shows you any annotations for the index and descriptors (references to manifests) that the index contains. The following example shows an org.opencontainers.image.documentation annotation on a descriptor, and an org.opencontainers.image.authors annotation on the index.\n\n$ docker buildx imagetools inspect IMAGE --raw\n\n{\n\n  \"schemaVersion\": 2,\n\n  \"mediaType\": \"application/vnd.oci.image.index.v1+json\",\n\n  \"manifests\": [\n\n    {\n\n      \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n\n      \"digest\": \"sha256:d20246ef744b1d05a1dd69d0b3fa907db007c07f79fe3e68c17223439be9fefb\",\n\n      \"size\": 911,\n\n      \"annotations\": {\n\n        \"org.opencontainers.image.documentation\": \"https://foo.example/docs\",\n\n      },\n\n      \"platform\": {\n\n        \"architecture\": \"amd64\",\n\n        \"os\": \"linux\"\n\n      }\n\n    },\n\n  ],\n\n  \"annotations\": {\n\n    \"org.opencontainers.image.authors\": \"dvdksn\"\n\n  }\n\n}\n\n\nTo inspect annotations on a manifest, use the docker buildx imagetools inspect command and specify <IMAGE>@<DIGEST>, where <DIGEST> is the digest of the manifest:\n\n$ docker buildx imagetools inspect IMAGE@sha256:d20246ef744b1d05a1dd69d0b3fa907db007c07f79fe3e68c17223439be9fefb --raw\n\n{\n\n  \"schemaVersion\": 2,\n\n  \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n\n  \"config\": {\n\n    \"mediaType\": \"application/vnd.oci.image.config.v1+json\",\n\n    \"digest\": \"sha256:4368b6959a78b412efa083c5506c4887e251f1484ccc9f0af5c406d8f76ece1d\",\n\n    \"size\": 850\n\n  },\n\n  \"layers\": [\n\n    {\n\n      \"mediaType\": \"application/vnd.oci.image.layer.v1.tar+gzip\",\n\n      \"digest\": \"sha256:2c03dbb20264f09924f9eab176da44e5421e74a78b09531d3c63448a7baa7c59\",\n\n      \"size\": 3333033\n\n    },\n\n    {\n\n      \"mediaType\": \"application/vnd.oci.image.layer.v1.tar+gzip\",\n\n      \"digest\": \"sha256:4923ad480d60a548e9b334ca492fa547a3ce8879676685b6718b085de5aaf142\",\n\n      \"size\": 61887305\n\n    }\n\n  ],\n\n  \"annotations\": {\n\n    \"index,manifest:org.opencontainers.image.vendor\": \"foocorp\",\n\n    \"org.opencontainers.image.source\": \"https://git.example/foo.git\",\n\n  }\n\n}\n\nSpecify annotation level\n\nBy default, annotations are added to the image manifest. You can specify which level (OCI image component) to attach the annotation to by prefixing the annotation string with a special type declaration:\n\n$ docker build --annotation \"TYPE:KEY=VALUE\" .\n\n\nThe following types are supported:\n\nmanifest: annotates manifests.\nindex: annotates the root index.\nmanifest-descriptor: annotates manifest descriptors in the index.\nindex-descriptor: annotates the index descriptor in the image layout.\n\nFor example, to build an image with the annotation foo=bar attached to the image index:\n\n$ docker build --tag IMAGE --push --annotation \"index:foo=bar\" .\n\n\nNote that the build must produce the component that you specify, or else the build will fail. For example, the following does not work, because the docker exporter does not produce an index:\n\n$ docker build --output type=docker --annotation \"index:foo=bar\" .\n\n\nLikewise, the following example also does not work, because buildx creates a docker output by default under some circumstances, such as when provenance attestations are explicitly disabled:\n\n$ docker build --provenance=false --annotation \"index:foo=bar\" .\n\n\nIt is possible to specify types, separated by a comma, to add the annotation to more than one level. The following example creates an image with the annotation foo=bar on both the image index and the image manifest:\n\n$ docker build --tag IMAGE --push --annotation \"index,manifest:foo=bar\" .\n\n\nYou can also specify a platform qualifier within square brackets in the type prefix, to annotate only components matching specific OS and architectures. The following example adds the foo=bar annotation only to the linux/amd64 manifest:\n\n$ docker build --tag IMAGE --push --annotation \"manifest[linux/amd64]:foo=bar\" .\n\nRelated information\n\nRelated articles:\n\nAdd image annotations with GitHub Actions\nAnnotations OCI specification\n\nReference information:\n\ndocker buildx build --annotation\nBake file reference: annotations\ndocker buildx imagetools create --annotation\n\nEdit this page\n\nRequest changes\n\nTable of contents\nAdd annotations\nInspect annotations\nSpecify annotation level\nRelated information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995913,
    "timestamp": "2026-02-07T06:37:09.728Z",
    "title": "Build attestations | Docker Docs",
    "url": "https://docs.docker.com/build/metadata/attestations/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nAnnotations\nBuild attestations\nImage attestation storage\nProvenance attestations\nSBOM attestations\nSLSA definitions\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nMetadata\n/\nBuild attestations\nBuild attestations\nCopy as Markdown\n\nBuild attestations describe how an image was built, and what it contains. The attestations are created at build-time by BuildKit, and become attached to the final image as metadata.\n\nThe purpose of attestations is to make it possible to inspect an image and see where it comes from, who created it and how, and what it contains. This enables you to make informed decisions about how an image impacts the supply chain security of your application. It also enables the use of policy engines for validating images based on policy rules you've defined.\n\nTwo types of build attestations are available:\n\nSoftware Bill of Material (SBOM): list of software artifacts that an image contains, or that were used to build the image.\nProvenance: how an image was built.\nPurpose of attestations\n\nThe use of open source and third-party packages is more widespread than ever before. Developers share and reuse code because it helps increase productivity, allowing teams to create better products, faster.\n\nImporting and using code created elsewhere without vetting it introduces a severe security risk. Even if you do review the software that you consume, new zero-day vulnerabilities are frequently discovered, requiring development teams take action to remediate them.\n\nBuild attestations make it easier to see the contents of an image, and where it comes from. Use attestations to analyze and decide whether to use an image, or to see if images you are already using are exposed to vulnerabilities.\n\nCreating attestations\n\nWhen you build an image with docker buildx build, you can add attestation records to the resulting image using the --provenance and --sbom options. You can opt in to add either the SBOM or provenance attestation type, or both.\n\n$ docker buildx build --sbom=true --provenance=true .\n\nNote\n\nThe default image store doesn't support attestations. If you're using the default image store and you build an image using the default docker driver, or using a different driver with the --load flag, the attestations are lost.\n\nTo make sure the attestations are preserved, you can:\n\nUse a docker-container driver with the --push flag to push the image to a registry directly.\nEnable the containerd image store.\nNote\n\nProvenance attestations are enabled by default, with the mode=min option. You can disable provenance attestations using the --provenance=false flag, or by setting the BUILDX_NO_DEFAULT_ATTESTATIONS environment variable.\n\nUsing the --provenance=true flag attaches provenance attestations with mode=min by default. See Provenance attestation for more details.\n\nBuildKit generates the attestations when building the image. The attestation records are wrapped in the in-toto JSON format and attached to the image index in a manifest for the final image.\n\nStorage\n\nBuildKit produces attestations in the in-toto format, as defined by the in-toto framework, a standard supported by the Linux Foundation.\n\nAttestations attach to images as a manifest in the image index. The data records of the attestations are stored as JSON blobs.\n\nBecause attestations attach to images as a manifest, it means that you can inspect the attestations for any image in a registry without having to pull the whole image.\n\nAll BuildKit exporters support attestations. The local and tar can't save the attestations to an image manifest, since it's outputting a directory of files or a tarball, not an image. Instead, these exporters write the attestations to one or more JSON files in the root directory of the export.\n\nExample\n\nThe following example shows a truncated in-toto JSON representation of an SBOM attestation.\n\n{\n\n  \"_type\": \"https://in-toto.io/Statement/v0.1\",\n\n  \"predicateType\": \"https://spdx.dev/Document\",\n\n  \"subject\": [\n\n    {\n\n      \"name\": \"pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>\",\n\n      \"digest\": {\n\n        \"sha256\": \"e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862\"\n\n      }\n\n    }\n\n  ],\n\n  \"predicate\": {\n\n    \"SPDXID\": \"SPDXRef-DOCUMENT\",\n\n    \"creationInfo\": {\n\n      \"created\": \"2022-12-15T11:47:54.546747383Z\",\n\n      \"creators\": [\"Organization: Anchore, Inc\", \"Tool: syft-v0.60.3\"],\n\n      \"licenseListVersion\": \"3.18\"\n\n    },\n\n    \"dataLicense\": \"CC0-1.0\",\n\n    \"documentNamespace\": \"https://anchore.com/syft/dir/run/src/core-da0f600b-7f0a-4de0-8432-f83703e6bc4f\",\n\n    \"name\": \"/run/src/core\",\n\n    // list of files that the image contains, e.g.:\n\n    \"files\": [\n\n      {\n\n        \"SPDXID\": \"SPDXRef-1ac501c94e2f9f81\",\n\n        \"comment\": \"layerID: sha256:9b18e9b68314027565b90ff6189d65942c0f7986da80df008b8431276885218e\",\n\n        \"fileName\": \"/bin/busybox\",\n\n        \"licenseConcluded\": \"NOASSERTION\"\n\n      }\n\n    ],\n\n    // list of packages that were identified for this image:\n\n    \"packages\": [\n\n      {\n\n        \"name\": \"busybox\",\n\n        \"originator\": \"Person: S√∂ren Tempel <soeren+alpine@soeren-tempel.net>\",\n\n        \"sourceInfo\": \"acquired package info from APK DB: lib/apk/db/installed\",\n\n        \"versionInfo\": \"1.35.0-r17\",\n\n        \"SPDXID\": \"SPDXRef-980737451f148c56\",\n\n        \"description\": \"Size optimized toolbox of many common UNIX utilities\",\n\n        \"downloadLocation\": \"https://busybox.net/\",\n\n        \"licenseConcluded\": \"GPL-2.0-only\",\n\n        \"licenseDeclared\": \"GPL-2.0-only\"\n\n        // ...\n\n      }\n\n    ],\n\n    // files-packages relationship\n\n    \"relationships\": [\n\n      {\n\n        \"relatedSpdxElement\": \"SPDXRef-1ac501c94e2f9f81\",\n\n        \"relationshipType\": \"CONTAINS\",\n\n        \"spdxElementId\": \"SPDXRef-980737451f148c56\"\n\n      },\n\n      ...\n\n    ],\n\n    \"spdxVersion\": \"SPDX-2.2\"\n\n  }\n\n}\n\nTo deep-dive into the specifics about how attestations are stored, see Image Attestation Storage (BuildKit).\n\nAttestation manifest format\n\nAttestations are stored as manifests, referenced by the image's index. Each attestation manifest refers to a single image manifest (one platform-variant of the image). Attestation manifests contain a single layer, the \"value\" of the attestation.\n\nThe following example shows the structure of an attestation manifest:\n\n{\n\n  \"schemaVersion\": 2,\n\n  \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n\n  \"config\": {\n\n    \"mediaType\": \"application/vnd.oci.image.config.v1+json\",\n\n    \"size\": 167,\n\n    \"digest\": \"sha256:916d7437a36dd0e258e64d9c5a373ca5c9618eeb1555e79bd82066e593f9afae\"\n\n  },\n\n  \"layers\": [\n\n    {\n\n      \"mediaType\": \"application/vnd.in-toto+json\",\n\n      \"size\": 1833349,\n\n      \"digest\": \"sha256:3138024b98ed5aa8e3008285a458cd25a987202f2500ce1a9d07d8e1420f5491\",\n\n      \"annotations\": {\n\n        \"in-toto.io/predicate-type\": \"https://spdx.dev/Document\"\n\n      }\n\n    }\n\n  ]\n\n}\nAttestations as OCI artifacts\n\nYou can configure the format of the attestation manifest using the oci-artifact option for the image and registry exporters. If set to true, the structure of the attestation manifest changes as follows:\n\nAn artifactType field is added to the attestation manifest, with a value of application/vnd.docker.attestation.manifest.v1+json.\nThe config field is an empty descriptor instead of a \"dummy\" config.\nA subject field is also added, pointing to the image manifest that the attestation refers to.\n\nThe following example shows an attestation with the OCI artifact format:\n\n{\n\n  \"schemaVersion\": 2,\n\n  \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n\n  \"artifactType\": \"application/vnd.docker.attestation.manifest.v1+json\",\n\n  \"config\": {\n\n    \"mediaType\": \"application/vnd.oci.empty.v1+json\",\n\n    \"size\": 2,\n\n    \"digest\": \"sha256:44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\",\n\n    \"data\": \"e30=\"\n\n  },\n\n  \"layers\": [\n\n    {\n\n      \"mediaType\": \"application/vnd.in-toto+json\",\n\n      \"size\": 2208,\n\n      \"digest\": \"sha256:6d2f2c714a6bee3cf9e4d3cb9a966b629efea2dd8556ed81f19bd597b3325286\",\n\n      \"annotations\": {\n\n        \"in-toto.io/predicate-type\": \"https://slsa.dev/provenance/v0.2\"\n\n      }\n\n    }\n\n  ],\n\n  \"subject\": {\n\n    \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n\n    \"size\": 1054,\n\n    \"digest\": \"sha256:bc2046336420a2852ecf915786c20f73c4c1b50d7803aae1fd30c971a7d1cead\",\n\n    \"platform\": {\n\n      \"architecture\": \"amd64\",\n\n      \"os\": \"linux\"\n\n    }\n\n  }\n\n}\nWhat's next\n\nLearn more about the available attestation types and how to use them:\n\nProvenance\nSBOM\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPurpose of attestations\nCreating attestations\nStorage\nExample\nAttestation manifest format\nAttestations as OCI artifacts\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995916,
    "timestamp": "2026-02-07T06:37:09.731Z",
    "title": "Image attestation storage | Docker Docs",
    "url": "https://docs.docker.com/build/metadata/attestations/attestation-storage/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nAnnotations\nBuild attestations\nImage attestation storage\nProvenance attestations\nSBOM attestations\nSLSA definitions\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nMetadata\n/\nBuild attestations\n/\nImage attestation storage\nImage attestation storage\nCopy as Markdown\n\nBuildkit supports creating and attaching attestations to build artifacts. These attestations can provide valuable information from the build process, including, but not limited to: SBOMs, SLSA Provenance, build logs, etc.\n\nThis document describes the current custom format used to store attestations, which is designed to be compatible with current registry implementations today. In the future, we may support exporting attestations in additional formats.\n\nAttestations are stored as manifest objects in the image index, similar in style to OCI artifacts.\n\nProperties\nAttestation Manifest\n\nAttestation manifests are attached to the root image index object, under a separate OCI image manifest. Each attestation manifest can contain multiple attestation blobs, with all the of the attestations in a manifest applying to a single platform manifest. All properties of standard OCI and Docker manifests continue to apply.\n\nThe image config descriptor will point to a valid image config, however, it will not contain attestation-specific details, and should be ignored as it is only included for compatibility purposes.\n\nEach image layer in layers will contain a descriptor for a single attestation blob. The mediaType of each layer will be set in accordance to its contents, one of:\n\napplication/vnd.in-toto+json (currently, the only supported option)\n\nIndicates an in-toto attestation blob\n\nAny unknown mediaTypes should be ignored.\n\nTo assist attestation traversal, the following annotations may be set on each layer descriptor:\n\nin-toto.io/predicate-type\n\nThis annotation will be set if the enclosed attestation is an in-toto attestation (currently, the only supported option). The annotation will be set to contain the same value as the predicateType property present inside the attestation.\n\nWhen present, this annotation may be used to find the specific attestation(s) they are looking for to avoid pulling the contents of the others.\n\nAttestation Blob\n\nThe contents of each layer will be a blob dependent on its mediaType.\n\napplication/vnd.in-toto+json\n\nThe blob contents will contain a full in-toto attestation statement:\n\n{\n\n  \"_type\": \"https://in-toto.io/Statement/v0.1\",\n\n  \"subject\": [\n\n    {\n\n      \"name\": \"NAME\",\n\n      \"digest\": {\"ALGORITHM\": \"HEX_VALUE\"}\n\n    },\n\n    ...\n\n  ],\n\n  \"predicateType\": \"URI\",\n\n  \"predicate\": { ... }\n\n}\n\nThe subject of the attestation should be set to be the same digest as the target manifest described in the Attestation Manifest Descriptor, or some object within.\n\nAttestation Manifest Descriptor\n\nAttestation manifests are attached to the root image index, in the manifests key, after all the original runnable manifests. All properties of standard OCI and Docker manifest descriptors continue to apply.\n\nTo prevent container runtimes from accidentally pulling or running the image described in the manifest, the platform property of the attestation manifest will be set to unknown/unknown, as follows:\n\n\"platform\": {\n\n  \"architecture\": \"unknown\",\n\n  \"os\": \"unknown\"\n\n}\n\nTo assist index traversal, the following annotations will be set on the manifest descriptor descriptor:\n\nvnd.docker.reference.type\n\nThis annotation describes the type of the artifact, and will be set to attestation-manifest. If any other value is specified, the entire manifest should be ignored.\n\nvnd.docker.reference.digest\n\nThis annotation will contain the digest of the object in the image index that the attestation manifest refers to.\n\nWhen present, this annotation can be used to find the matching attestation manifest for a selected image manifest.\n\nExamples\n\nExample showing an SBOM attestation attached to a linux/amd64 image\n\nImage index (sha256:94acc2ca70c40f3f6291681f37ce9c767e3d251ce01c7e4e9b98ccf148c26260):\n\nThis image index defines two descriptors: an AMD64 image sha256:23678f31.. and an attestation manifest sha256:02cb9aa7.. for that image.\n\n{\n\n  \"mediaType\": \"application/vnd.oci.image.index.v1+json\",\n\n  \"schemaVersion\": 2,\n\n  \"manifests\": [\n\n    {\n\n      \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n\n      \"digest\": \"sha256:23678f31b3b3586c4fb318aecfe64a96a1f0916ba8faf9b2be2abee63fa9e827\",\n\n      \"size\": 1234,\n\n      \"platform\": {\n\n        \"architecture\": \"amd64\",\n\n        \"os\": \"linux\"\n\n      }\n\n    },\n\n    {\n\n      \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n\n      \"digest\": \"sha256:02cb9aa7600e73fcf41ee9f0f19cc03122b2d8be43d41ce4b21335118f5dd943\",\n\n      \"size\": 1234,\n\n      \"annotations\": {\n\n        \"vnd.docker.reference.digest\": \"sha256:23678f31b3b3586c4fb318aecfe64a96a1f0916ba8faf9b2be2abee63fa9e827\",\n\n        \"vnd.docker.reference.type\": \"attestation-manifest\"\n\n      },\n\n      \"platform\": {\n\n         \"architecture\": \"unknown\",\n\n         \"os\": \"unknown\"\n\n      }\n\n    }\n\n  ]\n\n}\nAttestation manifest (sha256:02cb9aa7600e73fcf41ee9f0f19cc03122b2d8be43d41ce4b21335118f5dd943):\n\nThis attestation manifest contains one attestation that is an in-toto attestation that contains a \"https://spdx.dev/Document\" predicate, signifying that it is defining a SBOM for the image.\n\n{\n\n  \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n\n  \"schemaVersion\": 2,\n\n  \"config\": {\n\n    \"mediaType\": \"application/vnd.oci.image.config.v1+json\",\n\n    \"digest\": \"sha256:a781560066f20ec9c28f2115a95a886e5e71c7c7aa9d8fd680678498b82f3ea3\",\n\n    \"size\": 123\n\n  },\n\n  \"layers\": [\n\n    {\n\n      \"mediaType\": \"application/vnd.in-toto+json\",\n\n      \"digest\": \"sha256:133ae3f9bcc385295b66c2d83b28c25a9f294ce20954d5cf922dda860429734a\",\n\n      \"size\": 1234,\n\n      \"annotations\": {\n\n        \"in-toto.io/predicate-type\": \"https://spdx.dev/Document\"\n\n      }\n\n    }\n\n  ]\n\n}\nImage config (sha256:a781560066f20ec9c28f2115a95a886e5e71c7c7aa9d8fd680678498b82f3ea3):\n{\n\n  \"architecture\": \"unknown\",\n\n  \"os\": \"unknown\",\n\n  \"config\": {},\n\n  \"rootfs\": {\n\n    \"type\": \"layers\",\n\n    \"diff_ids\": [\n\n      \"sha256:133ae3f9bcc385295b66c2d83b28c25a9f294ce20954d5cf922dda860429734a\"\n\n    ]\n\n  }\n\n}\nLayer content (sha256:1ea07d5e55eb47ad0e6bbfa2ec180fb580974411e623814e519064c88f022f5c):\n\nAttestation body containing the SBOM data listing the packages used during the build in SPDX format.\n\n{\n\n  \"_type\": \"https://in-toto.io/Statement/v0.1\",\n\n  \"predicateType\": \"https://spdx.dev/Document\",\n\n  \"subject\": [\n\n    {\n\n      \"name\": \"_\",\n\n      \"digest\": {\n\n        \"sha256\": \"23678f31b3b3586c4fb318aecfe64a96a1f0916ba8faf9b2be2abee63fa9e827\"\n\n      }\n\n    }\n\n  ],\n\n  \"predicate\": {\n\n    \"SPDXID\": \"SPDXRef-DOCUMENT\",\n\n    \"spdxVersion\": \"SPDX-2.2\",\n\n    ...\n\nRequest changes\n\nTable of contents\nProperties\nAttestation Manifest\nAttestation Blob\nAttestation Manifest Descriptor\nExamples\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995917,
    "timestamp": "2026-02-07T06:37:09.744Z",
    "title": "Provenance attestations | Docker Docs",
    "url": "https://docs.docker.com/build/metadata/attestations/slsa-provenance/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nAnnotations\nBuild attestations\nImage attestation storage\nProvenance attestations\nSBOM attestations\nSLSA definitions\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nMetadata\n/\nBuild attestations\n/\nProvenance attestations\nProvenance attestations\nCopy as Markdown\n\nThe provenance attestations include facts about the build process, including details such as:\n\nBuild timestamps\nBuild parameters and environment\nVersion control metadata\nSource code details\nMaterials (files, scripts) consumed during the build\n\nBy default, provenance attestations follow the SLSA provenance schema, version 0.2. You can optionally enable SLSA Provenance v1 using the version parameter.\n\nFor more information about how BuildKit populates these provenance properties, refer to SLSA definitions.\n\nCreate provenance attestations\n\nTo create a provenance attestation, pass the --attest type=provenance option to the docker buildx build command:\n\n$ docker buildx build --tag <namespace>/<image>:<version> \\\n\n    --attest type=provenance,mode=[min,max],version=[v0.2,v1] .\n\n\nAlternatively, you can use the shorthand --provenance=true option instead of --attest type=provenance. To specify the mode or version parameters using the shorthand option, use: --provenance=mode=max,version=v1.\n\nFor an example on how to add provenance attestations with GitHub Actions, see Add attestations with GitHub Actions.\n\nMode\n\nYou can use the mode parameter to define the level of detail to be included in the provenance attestation. Supported values are mode=min (default) and mode=max.\n\nMin\n\nIn min mode, the provenance attestations include a minimal set of information, such as:\n\nBuild timestamps\nThe frontend used\nBuild materials\nSource repository and revision\nBuild platform\nReproducibility\n\nValues of build arguments, the identities of secrets, and rich layer metadata is not included mode=min. The min-level provenance is safe to use for all builds, as it doesn't leak information from any part of the build environment.\n\nThe following JSON example shows the information included in a provenance attestations created using the min mode:\n\n{\n\n  \"_type\": \"https://in-toto.io/Statement/v0.1\",\n\n  \"predicateType\": \"https://slsa.dev/provenance/v0.2\",\n\n  \"subject\": [\n\n    {\n\n      \"name\": \"pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>\",\n\n      \"digest\": {\n\n        \"sha256\": \"e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862\"\n\n      }\n\n    }\n\n  ],\n\n  \"predicate\": {\n\n    \"builder\": { \"id\": \"\" },\n\n    \"buildType\": \"https://mobyproject.org/buildkit@v1\",\n\n    \"materials\": [\n\n      {\n\n        \"uri\": \"pkg:docker/docker/dockerfile@1\",\n\n        \"digest\": {\n\n          \"sha256\": \"9ba7531bd80fb0a858632727cf7a112fbfd19b17e94c4e84ced81e24ef1a0dbc\"\n\n        }\n\n      },\n\n      {\n\n        \"uri\": \"pkg:docker/golang@1.19.4-alpine?platform=linux%2Farm64\",\n\n        \"digest\": {\n\n          \"sha256\": \"a9b24b67dc83b3383d22a14941c2b2b2ca6a103d805cac6820fd1355943beaf1\"\n\n        }\n\n      }\n\n    ],\n\n    \"invocation\": {\n\n      \"configSource\": { \"entryPoint\": \"Dockerfile\" },\n\n      \"parameters\": {\n\n        \"frontend\": \"gateway.v0\",\n\n        \"args\": {\n\n          \"cmdline\": \"docker/dockerfile:1\",\n\n          \"source\": \"docker/dockerfile:1\",\n\n          \"target\": \"binaries\"\n\n        },\n\n        \"locals\": [{ \"name\": \"context\" }, { \"name\": \"dockerfile\" }]\n\n      },\n\n      \"environment\": { \"platform\": \"linux/arm64\" }\n\n    },\n\n    \"metadata\": {\n\n      \"buildInvocationID\": \"c4a87v0sxhliuewig10gnsb6v\",\n\n      \"buildStartedOn\": \"2022-12-16T08:26:28.651359794Z\",\n\n      \"buildFinishedOn\": \"2022-12-16T08:26:29.625483253Z\",\n\n      \"reproducible\": false,\n\n      \"completeness\": {\n\n        \"parameters\": true,\n\n        \"environment\": true,\n\n        \"materials\": false\n\n      },\n\n      \"https://mobyproject.org/buildkit@v1#metadata\": {\n\n        \"vcs\": {\n\n          \"revision\": \"a9ba846486420e07d30db1107411ac3697ecab68\",\n\n          \"source\": \"git@github.com:<org>/<repo>.git\"\n\n        }\n\n      }\n\n    }\n\n  }\n\n}\nMax\n\nThe max mode includes all of the information included in the min mode, as well as:\n\nThe LLB definition of the build. These show the exact steps taken to produce the image.\nInformation about the Dockerfile, including a full base64-encoded version of the file.\nSource maps describing the relationship between build steps and image layers.\n\nWhen possible, you should prefer mode=max as it contains significantly more detailed information for analysis.\n\nWarning\n\nNote that mode=max exposes the values of build arguments.\n\nIf you're misusing build arguments to pass credentials, authentication tokens, or other secrets, you should refactor your build to pass the secrets using secret mounts instead. Secret mounts don't leak outside of the build and are never included in provenance attestations.\n\nVersion\n\nThe version parameter lets you specify which SLSA provenance schema version to use. Supported values are version=v0.2 (default) and version=v1.\n\nTo use SLSA Provenance v1:\n\n$ docker buildx build --tag <namespace>/<image>:<version> \\\n\n    --attest type=provenance,mode=max,version=v1 .\n\n\nFor more information about SLSA Provenance v1, see the SLSA specification. To see the difference between SLSA v0.2 and v1 provenance attestations, refer to SLSA definitions\n\nInspecting Provenance\n\nTo explore created Provenance exported through the image exporter, you can use imagetools inspect.\n\nUsing the --format option, you can specify a template for the output. All provenance-related data is available under the .Provenance attribute. For example, to get the raw contents of the Provenance in the SLSA format:\n\n$ docker buildx imagetools inspect <namespace>/<image>:<version> \\\n\n    --format \"{{ json .Provenance.SLSA }}\"\n\n{\n\n  \"buildType\": \"https://mobyproject.org/buildkit@v1\",\n\n  ...\n\n}\n\n\nYou can also construct more complex expressions using the full functionality of Go templates. For example, for provenance generated with mode=max, you can extract the full source code of the Dockerfile used to build the image:\n\n$ docker buildx imagetools inspect <namespace>/<image>:<version> \\\n\n    --format '{{ range (index .Provenance.SLSA.metadata \"https://mobyproject.org/buildkit@v1#metadata\").source.infos }}{{ if eq .filename \"Dockerfile\" }}{{ .data }}{{ end }}{{ end }}' | base64 -d\n\nFROM ubuntu:24.04\n\nRUN apt-get update\n\n...\n\nProvenance attestation example\n\nThe following example shows what a JSON representation of a provenance attestation with mode=max looks like:\n\n{\n\n  \"_type\": \"https://in-toto.io/Statement/v0.1\",\n\n  \"predicateType\": \"https://slsa.dev/provenance/v0.2\",\n\n  \"subject\": [\n\n    {\n\n      \"name\": \"pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>\",\n\n      \"digest\": {\n\n        \"sha256\": \"e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862\"\n\n      }\n\n    }\n\n  ],\n\n  \"predicate\": {\n\n    \"builder\": { \"id\": \"\" },\n\n    \"buildType\": \"https://mobyproject.org/buildkit@v1\",\n\n    \"materials\": [\n\n      {\n\n        \"uri\": \"pkg:docker/docker/dockerfile@1\",\n\n        \"digest\": {\n\n          \"sha256\": \"9ba7531bd80fb0a858632727cf7a112fbfd19b17e94c4e84ced81e24ef1a0dbc\"\n\n        }\n\n      },\n\n      {\n\n        \"uri\": \"pkg:docker/golang@1.19.4-alpine?platform=linux%2Farm64\",\n\n        \"digest\": {\n\n          \"sha256\": \"a9b24b67dc83b3383d22a14941c2b2b2ca6a103d805cac6820fd1355943beaf1\"\n\n        }\n\n      }\n\n    ],\n\n    \"buildConfig\": {\n\n      \"llbDefinition\": [\n\n        {\n\n          \"id\": \"step4\",\n\n          \"op\": {\n\n            \"Op\": {\n\n              \"exec\": {\n\n                \"meta\": {\n\n                  \"args\": [\"/bin/sh\", \"-c\", \"go mod download -x\"],\n\n                  \"env\": [\n\n                    \"PATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n\n                    \"GOLANG_VERSION=1.19.4\",\n\n                    \"GOPATH=/go\",\n\n                    \"CGO_ENABLED=0\"\n\n                  ],\n\n                  \"cwd\": \"/src\"\n\n                },\n\n                \"mounts\": [\n\n                  { \"input\": 0, \"dest\": \"/\", \"output\": 0 },\n\n                  {\n\n                    \"input\": -1,\n\n                    \"dest\": \"/go/pkg/mod\",\n\n                    \"output\": -1,\n\n                    \"mountType\": 3,\n\n                    \"cacheOpt\": { \"ID\": \"//go/pkg/mod\" }\n\n                  },\n\n                  {\n\n                    \"input\": 1,\n\n                    \"selector\": \"/go.mod\",\n\n                    \"dest\": \"/src/go.mod\",\n\n                    \"output\": -1,\n\n                    \"readonly\": true\n\n                  },\n\n                  {\n\n                    \"input\": 1,\n\n                    \"selector\": \"/go.sum\",\n\n                    \"dest\": \"/src/go.sum\",\n\n                    \"output\": -1,\n\n                    \"readonly\": true\n\n                  }\n\n                ]\n\n              }\n\n            },\n\n            \"platform\": { \"Architecture\": \"arm64\", \"OS\": \"linux\" },\n\n            \"constraints\": {}\n\n          },\n\n          \"inputs\": [\"step3:0\", \"step1:0\"]\n\n        }\n\n      ]\n\n    },\n\n    \"metadata\": {\n\n      \"buildInvocationID\": \"edf52vxjyf9b6o5qd7vgx0gru\",\n\n      \"buildStartedOn\": \"2022-12-15T15:38:13.391980297Z\",\n\n      \"buildFinishedOn\": \"2022-12-15T15:38:14.274565297Z\",\n\n      \"reproducible\": false,\n\n      \"completeness\": {\n\n        \"parameters\": true,\n\n        \"environment\": true,\n\n        \"materials\": false\n\n      },\n\n      \"https://mobyproject.org/buildkit@v1#metadata\": {\n\n        \"vcs\": {\n\n          \"revision\": \"a9ba846486420e07d30db1107411ac3697ecab68-dirty\",\n\n          \"source\": \"git@github.com:<org>/<repo>.git\"\n\n        },\n\n        \"source\": {\n\n          \"locations\": {\n\n            \"step4\": {\n\n              \"locations\": [\n\n                {\n\n                  \"ranges\": [\n\n                    { \"start\": { \"line\": 5 }, \"end\": { \"line\": 5 } },\n\n                    { \"start\": { \"line\": 6 }, \"end\": { \"line\": 6 } },\n\n                    { \"start\": { \"line\": 7 }, \"end\": { \"line\": 7 } },\n\n                    { \"start\": { \"line\": 8 }, \"end\": { \"line\": 8 } }\n\n                  ]\n\n                }\n\n              ]\n\n            }\n\n          },\n\n          \"infos\": [\n\n            {\n\n              \"filename\": \"Dockerfile\",\n\n              \"data\": \"RlJPTSBhbHBpbmU6bGF0ZXN0Cg==\",\n\n              \"llbDefinition\": [\n\n                {\n\n                  \"id\": \"step0\",\n\n                  \"op\": {\n\n                    \"Op\": {\n\n                      \"source\": {\n\n                        \"identifier\": \"local://dockerfile\",\n\n                        \"attrs\": {\n\n                          \"local.differ\": \"none\",\n\n                          \"local.followpaths\": \"[\\\"Dockerfile\\\",\\\"Dockerfile.dockerignore\\\",\\\"dockerfile\\\"]\",\n\n                          \"local.session\": \"s4j58ngehdal1b5hn7msiqaqe\",\n\n                          \"local.sharedkeyhint\": \"dockerfile\"\n\n                        }\n\n                      }\n\n                    },\n\n                    \"constraints\": {}\n\n                  }\n\n                },\n\n                { \"id\": \"step1\", \"op\": { \"Op\": null }, \"inputs\": [\"step0:0\"] }\n\n              ]\n\n            }\n\n          ]\n\n        },\n\n        \"layers\": {\n\n          \"step2:0\": [\n\n            [\n\n              {\n\n                \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n\n                \"digest\": \"sha256:261da4162673b93e5c0e7700a3718d40bcc086dbf24b1ec9b54bca0b82300626\",\n\n                \"size\": 3259190\n\n              },\n\n              {\n\n                \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n\n                \"digest\": \"sha256:bc729abf26b5aade3c4426d388b5ea6907fe357dec915ac323bb2fa592d6288f\",\n\n                \"size\": 286218\n\n              },\n\n              {\n\n                \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n\n                \"digest\": \"sha256:7f1d6579712341e8062db43195deb2d84f63b0f2d1ed7c3d2074891085ea1b56\",\n\n                \"size\": 116878653\n\n              },\n\n              {\n\n                \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n\n                \"digest\": \"sha256:652874aefa1343799c619d092ab9280b25f96d97939d5d796437e7288f5599c9\",\n\n                \"size\": 156\n\n              }\n\n            ]\n\n          ]\n\n        }\n\n      }\n\n    }\n\n  }\n\n}\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCreate provenance attestations\nMode\nMin\nMax\nVersion\nInspecting Provenance\nProvenance attestation example\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995923,
    "timestamp": "2026-02-07T06:37:09.754Z",
    "title": "SLSA definitions | Docker Docs",
    "url": "https://docs.docker.com/build/metadata/attestations/slsa-definitions/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nAnnotations\nBuild attestations\nImage attestation storage\nProvenance attestations\nSBOM attestations\nSLSA definitions\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nMetadata\n/\nBuild attestations\n/\nSLSA definitions\nSLSA definitions\nCopy as Markdown\n\nBuildKit supports the creation of SLSA Provenance for builds that it runs.\n\nThe provenance format generated by BuildKit is defined by the SLSA Provenance format (supports both v0.2 and v1).\n\nThis page describes how BuildKit populate each field, and whether the field gets included when you generate attestations mode=min and mode=max.\n\nSLSA v1\nbuildDefinition.buildType\nRef: https://slsa.dev/spec/v1.1/provenance#buildType\nIncluded with mode=min and mode=max.\n\nThe buildDefinition.buildType field is set to https://github.com/moby/buildkit/blob/master/docs/attestations/slsa-definitions.md and can be used to determine the structure of the provenance content.\n\n    \"buildDefinition\": {\n\n      \"buildType\": \"https://github.com/moby/buildkit/blob/master/docs/attestations/slsa-definitions.md\",\n\n      ...\n\n    }\nbuildDefinition.externalParameters.configSource\nRef: https://slsa.dev/spec/v1.1/provenance#externalParameters\nIncluded with mode=min and mode=max.\n\nDescribes the config that initialized the build.\n\n    \"buildDefinition\": {\n\n      \"externalParameters\": {\n\n        \"configSource\": {\n\n          \"uri\": \"https://github.com/moby/buildkit.git#refs/tags/v0.11.0\",\n\n          \"digest\": {\n\n            \"sha1\": \"4b220de5058abfd01ff619c9d2ff6b09a049bea0\"\n\n          },\n\n          \"path\": \"Dockerfile\"\n\n        },\n\n        ...\n\n      },\n\n    }\n\nFor builds initialized from a remote context, like a Git or HTTP URL, this object defines the context URL and its immutable digest in the uri and digest fields. For builds using a local frontend, such as a Dockerfile, the path field defines the path for the frontend file that initialized the build (filename frontend option).\n\nbuildDefinition.externalParameters.request\nRef: https://slsa.dev/spec/v1.1/provenance#externalParameters\nPartially included with mode=min.\n\nDescribes build inputs passed to the build.\n\n    \"buildDefinition\": {\n\n      \"externalParameters\": {\n\n        \"request\": {\n\n          \"frontend\": \"gateway.v0\",\n\n          \"args\": {\n\n            \"build-arg:BUILDKIT_CONTEXT_KEEP_GIT_DIR\": \"1\",\n\n            \"label:FOO\": \"bar\",\n\n            \"source\": \"docker/dockerfile-upstream:master\",\n\n            \"target\": \"release\"\n\n          },\n\n          \"secrets\": [\n\n            {\n\n              \"id\": \"GIT_AUTH_HEADER\",\n\n              \"optional\": true\n\n            },\n\n            ...\n\n          ],\n\n          \"ssh\": [],\n\n          \"locals\": []\n\n        },\n\n        ...\n\n      },\n\n    }\n\nThe following fields are included with both mode=min and mode=max:\n\nlocals lists any local sources used in the build, including the build context and frontend file.\n\nfrontend defines type of BuildKit frontend used for the build. Currently, this can be dockerfile.v0 or gateway.v0.\n\nargs defines the build arguments passed to the BuildKit frontend.\n\nThe keys inside the args object reflect the options as BuildKit receives them. For example, build-arg and label prefixes are used for build arguments and labels, and target key defines the target stage that was built. The source key defines the source image for the Gateway frontend, if used.\n\nThe following fields are only included with mode=max:\n\nsecrets defines secrets used during the build. Note that actual secret values are not included.\nssh defines the ssh forwards used during the build.\nbuildDefinition.internalParameters.buildConfig\nRef: https://slsa.dev/spec/v1.1/provenance#internalParameters\nOnly included with mode=max.\n\nDefines the build steps performed during the build.\n\nBuildKit internally uses LLB definition to execute the build steps. The LLB definition of the build steps is defined in the buildDefinition.internalParameters.buildConfig.llbDefinition field.\n\nEach LLB step is the JSON definition of the LLB ProtoBuf API. The dependencies for a vertex in the LLB graph can be found in the inputs field for every step.\n\n    \"buildDefinition\": {\n\n      \"internalParameters\": {\n\n        \"buildConfig\": {\n\n          \"llbDefinition\": [\n\n            {\n\n              \"id\": \"step0\",\n\n              \"op\": {\n\n                \"Op\": {\n\n                  \"exec\": {\n\n                    \"meta\": {\n\n                      \"args\": [\n\n                        \"/bin/sh\",\n\n                        \"-c\",\n\n                        \"go build .\"\n\n                      ],\n\n                      \"env\": [\n\n                        \"PATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n\n                        \"GOPATH=/go\",\n\n                        \"GOFLAGS=-mod=vendor\",\n\n                      ],\n\n                      \"cwd\": \"/src\",\n\n                    },\n\n                    \"mounts\": [...]\n\n                  }\n\n                },\n\n                \"platform\": {...},\n\n              },\n\n              \"inputs\": [\n\n                \"step8:0\",\n\n                \"step2:0\",\n\n              ]\n\n            },\n\n            ...\n\n          ]\n\n        },\n\n      }\n\n    }\nbuildDefinition.internalParameters.builderPlatform\nRef: https://slsa.dev/spec/v1.1/provenance#internalParameters\nIncluded with mode=min and mode=max.\n    \"buildDefinition\": {\n\n      \"internalParameters\": {\n\n        \"builderPlatform\": \"linux/amd64\"\n\n        ...\n\n      },\n\n    }\n\nBuildKit sets the builderPlatform of the build machine. Note that this is not necessarily the platform of the build result that can be determined from the in-toto subject field.\n\nbuildDefinition.resolvedDependencies\nRef: https://slsa.dev/spec/v1.1/provenance#resolvedDependencies\nIncluded with mode=min and mode=max.\n\nDefines all the external artifacts that were part of the build. The value depends on the type of artifact:\n\nThe URL of Git repositories containing source code for the image\nHTTP URLs if you are building from a remote tarball, or that was included using an ADD command in Dockerfile\nAny Docker images used during the build\n\nThe URLs to the Docker images will be in Package URL format.\n\nAll the build materials will include the immutable checksum of the artifact. When building from a mutable tag, you can use the digest information to determine if the artifact has been updated compared to when the build ran.\n\n    \"buildDefinition\": {\n\n      \"resolvedDependencies\": [\n\n        {\n\n          \"uri\": \"pkg:docker/alpine@3.17?platform=linux%2Famd64\",\n\n          \"digest\": {\n\n            \"sha256\": \"8914eb54f968791faf6a8638949e480fef81e697984fba772b3976835194c6d4\"\n\n          }\n\n        },\n\n        {\n\n          \"uri\": \"https://github.com/moby/buildkit.git#refs/tags/v0.11.0\",\n\n          \"digest\": {\n\n            \"sha1\": \"4b220de5058abfd01ff619c9d2ff6b09a049bea0\"\n\n          }\n\n        },\n\n        ...\n\n      ],\n\n      ...\n\n    }\nrunDetails.builder.id\nRef: https://slsa.dev/spec/v1.1/provenance#builder.id\nIncluded with mode=min and mode=max.\n\nThe field is set to the URL of the build, if available.\n\n    \"runDetails\": {\n\n      \"builder\": {\n\n        \"id\": \"https://github.com/docker/buildx/actions/runs/3709599520\"\n\n        ...\n\n      },\n\n      ...\n\n    }\nNote\n\nThis value can be set using the builder-id attestation parameter.\n\nrunDetails.metadata.invocationID\nRef: https://slsa.dev/spec/v1.1/provenance#invocationId\nIncluded with mode=min and mode=max.\n\nUnique identifier for the build invocation. When building a multi-platform image with a single build request, this value will be the shared by all the platform versions of the image.\n\n    \"runDetails\": {\n\n      \"metadata\": {\n\n        \"invocationID\": \"rpv7a389uzil5lqmrgwhijwjz\",\n\n        ...\n\n      },\n\n      ...\n\n    }\nrunDetails.metadata.startedOn\nRef: https://slsa.dev/spec/v1.1/provenance#startedOn\nIncluded with mode=min and mode=max.\n\nTimestamp when the build started.\n\n    \"runDetails\": {\n\n      \"metadata\": {\n\n        \"startedOn\": \"2021-11-17T15:00:00Z\",\n\n        ...\n\n      },\n\n      ...\n\n    }\nrunDetails.metadata.finishedOn\nRef: https://slsa.dev/spec/v1.1/provenance#finishedOn\nIncluded with mode=min and mode=max.\n\nTimestamp when the build finished.\n\n    \"runDetails\": {\n\n      \"metadata\": {\n\n        \"finishedOn\": \"2021-11-17T15:01:00Z\",\n\n        ...\n\n      },\n\n    }\nrunDetails.metadata.buildkit_metadata\nRef: https://slsa.dev/spec/v1.1/provenance#extension-fields\nPartially included with mode=min.\n\nThis extension field defines BuildKit-specific additional metadata that is not part of the SLSA provenance spec.\n\n    \"runDetails\": {\n\n      \"metadata\": {\n\n        \"buildkit_metadata\": {\n\n          \"source\": {...},\n\n          \"layers\": {...},\n\n          \"vcs\": {...},\n\n        },\n\n        ...\n\n      },\n\n    }\nsource\n\nOnly included with mode=max.\n\nDefines a source mapping of LLB build steps, defined in the buildDefinition.internalParameters.buildConfig.llbDefinition field, to their original source code (for example, Dockerfile commands). The source.locations field contains the ranges of all the Dockerfile commands ran in an LLB step. source.infos array contains the source code itself. This mapping is present if the BuildKit frontend provided it when creating the LLB definition.\n\nlayers\n\nOnly included with mode=max.\n\nDefines the layer mapping of LLB build step mounts defined in buildDefinition.internalParameters.buildConfig.llbDefinition to the OCI descriptors of equivalent layers. This mapping is present if the layer data was available, usually when attestation is for an image or if the build step pulled in image data as part of the build.\n\nvcs\n\nIncluded with mode=min and mode=max.\n\nDefines optional metadata for the version control system used for the build. If a build uses a remote context from Git repository, BuildKit extracts the details of the version control system automatically and displays it in the buildDefinition.externalParameters.configSource field. But if the build uses a source from a local directory, the VCS information is lost even if the directory contained a Git repository. In this case, the build client can send additional vcs:source and vcs:revision build options and BuildKit will add them to the provenance attestations as extra metadata. Note that, contrary to the buildDefinition.externalParameters.configSource field, BuildKit doesn't verify the vcs values, and as such they can't be trusted and should only be used as a metadata hint.\n\nrunDetails.metadata.buildkit_hermetic\nRef: https://slsa.dev/spec/v1.1/provenance#extension-fields\nIncluded with mode=min and mode=max.\n\nThis extension field is set to true if the build was hermetic and did not access the network. In Dockerfiles, a build is hermetic if it does not use RUN commands or disables network with --network=none flag.\n\n    \"runDetails\": {\n\n      \"metadata\": {\n\n        \"buildkit_hermetic\": true,\n\n        ...\n\n      },\n\n    }\nrunDetails.metadata.buildkit_completeness\nRef: https://slsa.dev/spec/v1.1/provenance#extension-fields\nIncluded with mode=min and mode=max.\n\nThis extension field defines if the provenance information is complete. It is similar to metadata.completeness field in SLSA v0.2.\n\nbuildkit_completeness.request is true if all the build arguments are included in the buildDefinition.externalParameters.request field. When building with min mode, the build arguments are not included in the provenance information and request is not complete. Request is also not complete on direct LLB builds that did not use a frontend.\n\nbuildkit_completeness.resolvedDependencies is true if buildDefinition.resolvedDependencies field includes all the dependencies of the build. When building from un-tracked source in a local directory, the dependencies are not complete, while when building from a remote Git repository all dependencies can be tracked by BuildKit and buildkit_completeness.resolvedDependencies is true.\n\n    \"runDetails\": {\n\n      \"metadata\": {\n\n        \"buildkit_completeness\": {\n\n          \"request\": true,\n\n          \"resolvedDependencies\": true\n\n        },\n\n        ...\n\n      },\n\n    }\nrunDetails.metadata.buildkit_reproducible\nRef: https://slsa.dev/spec/v1.1/provenance#extension-fields\nIncluded with mode=min and mode=max.\n\nThis extension field defines if the build result is supposed to be byte-by-byte reproducible. It is similar to metadata.reproducible field in SLSA v0.2. This value can be set by the user with the reproducible=true attestation parameter.\n\n    \"runDetails\": {\n\n      \"metadata\": {\n\n        \"buildkit_reproducible\": false,\n\n        ...\n\n      },\n\n    }\nSLSA v0.2\nbuilder.id\nRef: https://slsa.dev/spec/v0.2/provenance#builder.id\nIncluded with mode=min and mode=max.\n\nThe field is set to the URL of the build, if available.\n\n    \"builder\": {\n\n      \"id\": \"https://github.com/docker/buildx/actions/runs/3709599520\"\n\n    },\nNote\n\nThis value can be set using the builder-id attestation parameter.\n\nbuildType\nRef: https://slsa.dev/spec/v0.2/provenance#buildType\nIncluded with mode=min and mode=max.\n\nThe buildType field is set to https://mobyproject.org/buildkit@v1 and can be used to determine the structure of the provenance content.\n\n    \"buildType\": \"https://mobyproject.org/buildkit@v1\",\ninvocation.configSource\nRef: https://slsa.dev/spec/v0.2/provenance#invocation.configSource\nIncluded with mode=min and mode=max.\n\nDescribes the config that initialized the build.\n\n    \"invocation\": {\n\n      \"configSource\": {\n\n        \"uri\": \"https://github.com/moby/buildkit.git#refs/tags/v0.11.0\",\n\n        \"digest\": {\n\n          \"sha1\": \"4b220de5058abfd01ff619c9d2ff6b09a049bea0\"\n\n        },\n\n        \"entryPoint\": \"Dockerfile\"\n\n      },\n\n      ...\n\n    },\n\nFor builds initialized from a remote context, like a Git or HTTP URL, this object defines the context URL and its immutable digest in the uri and digest fields. For builds using a local frontend, such as a Dockerfile, the entryPoint field defines the path for the frontend file that initialized the build (filename frontend option).\n\ninvocation.parameters\nRef: https://slsa.dev/spec/v0.2/provenance#invocation.parameters\nPartially included with mode=min.\n\nDescribes build inputs passed to the build.\n\n    \"invocation\": {\n\n      \"parameters\": {\n\n        \"frontend\": \"gateway.v0\",\n\n        \"args\": {\n\n          \"build-arg:BUILDKIT_CONTEXT_KEEP_GIT_DIR\": \"1\",\n\n          \"label:FOO\": \"bar\",\n\n          \"source\": \"docker/dockerfile-upstream:master\",\n\n          \"target\": \"release\"\n\n        },\n\n        \"secrets\": [\n\n          {\n\n            \"id\": \"GIT_AUTH_HEADER\",\n\n            \"optional\": true\n\n          },\n\n          ...\n\n        ],\n\n        \"ssh\": [],\n\n        \"locals\": []\n\n      },\n\n      ...\n\n    },\n\nThe following fields are included with both mode=min and mode=max:\n\nlocals lists any local sources used in the build, including the build context and frontend file.\n\nfrontend defines type of BuildKit frontend used for the build. Currently, this can be dockerfile.v0 or gateway.v0.\n\nargs defines the build arguments passed to the BuildKit frontend.\n\nThe keys inside the args object reflect the options as BuildKit receives them. For example, build-arg and label prefixes are used for build arguments and labels, and target key defines the target stage that was built. The source key defines the source image for the Gateway frontend, if used.\n\nThe following fields are only included with mode=max:\n\nsecrets defines secrets used during the build. Note that actual secret values are not included.\nssh defines the ssh forwards used during the build.\ninvocation.environment\nRef: https://slsa.dev/spec/v0.2/provenance#invocation.environment\nIncluded with mode=min and mode=max.\n    \"invocation\": {\n\n      \"environment\": {\n\n        \"platform\": \"linux/amd64\"\n\n      },\n\n      ...\n\n    },\n\nThe only value BuildKit currently sets is the platform of the current build machine. Note that this is not necessarily the platform of the build result that can be determined from the in-toto subject field.\n\nmaterials\nRef: https://slsa.dev/spec/v0.2/provenance#materials\nIncluded with mode=min and mode=max.\n\nDefines all the external artifacts that were part of the build. The value depends on the type of artifact:\n\nThe URL of Git repositories containing source code for the image\nHTTP URLs if you are building from a remote tarball, or that was included using an ADD command in Dockerfile\nAny Docker images used during the build\n\nThe URLs to the Docker images will be in Package URL format.\n\nAll the build materials will include the immutable checksum of the artifact. When building from a mutable tag, you can use the digest information to determine if the artifact has been updated compared to when the build ran.\n\n    \"materials\": [\n\n      {\n\n        \"uri\": \"pkg:docker/alpine@3.17?platform=linux%2Famd64\",\n\n        \"digest\": {\n\n          \"sha256\": \"8914eb54f968791faf6a8638949e480fef81e697984fba772b3976835194c6d4\"\n\n        }\n\n      },\n\n      {\n\n        \"uri\": \"https://github.com/moby/buildkit.git#refs/tags/v0.11.0\",\n\n        \"digest\": {\n\n          \"sha1\": \"4b220de5058abfd01ff619c9d2ff6b09a049bea0\"\n\n        }\n\n      },\n\n      ...\n\n    ],\nbuildConfig\nRef: https://slsa.dev/spec/v0.2/provenance#buildConfig\nOnly included with mode=max.\n\nDefines the build steps performed during the build.\n\nBuildKit internally uses LLB definition to execute the build steps. The LLB definition of the build steps is defined in buildConfig.llbDefinition field.\n\nEach LLB step is the JSON definition of the LLB ProtoBuf API. The dependencies for a vertex in the LLB graph can be found in the inputs field for every step.\n\n  \"buildConfig\": {\n\n    \"llbDefinition\": [\n\n      {\n\n        \"id\": \"step0\",\n\n        \"op\": {\n\n          \"Op\": {\n\n            \"exec\": {\n\n              \"meta\": {\n\n                \"args\": [\n\n                  \"/bin/sh\",\n\n                  \"-c\",\n\n                  \"go build .\"\n\n                ],\n\n                \"env\": [\n\n                  \"PATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n\n                  \"GOPATH=/go\",\n\n                  \"GOFLAGS=-mod=vendor\",\n\n                ],\n\n                \"cwd\": \"/src\",\n\n              },\n\n              \"mounts\": [...]\n\n            }\n\n          },\n\n          \"platform\": {...},\n\n        },\n\n        \"inputs\": [\n\n          \"step8:0\",\n\n          \"step2:0\",\n\n        ]\n\n      },\n\n      ...\n\n    ]\n\n  },\nmetadata.buildInvocationId\nRef: https://slsa.dev/spec/v0.2/provenance#buildInvocationId\nIncluded with mode=min and mode=max.\n\nUnique identifier for the build invocation. When building a multi-platform image with a single build request, this value will be the shared by all the platform versions of the image.\n\n    \"metadata\": {\n\n      \"buildInvocationID\": \"rpv7a389uzil5lqmrgwhijwjz\",\n\n      ...\n\n    },\nmetadata.buildStartedOn\nRef: https://slsa.dev/spec/v0.2/provenance#buildStartedOn\nIncluded with mode=min and mode=max.\n\nTimestamp when the build started.\n\n    \"metadata\": {\n\n      \"buildStartedOn\": \"2021-11-17T15:00:00Z\",\n\n      ...\n\n    },\nmetadata.buildFinishedOn\nRef: https://slsa.dev/spec/v0.2/provenance#buildFinishedOn\nIncluded with mode=min and mode=max.\n\nTimestamp when the build finished.\n\n    \"metadata\": {\n\n      \"buildFinishedOn\": \"2021-11-17T15:01:00Z\",\n\n      ...\n\n    },\nmetadata.completeness\nRef: https://slsa.dev/spec/v0.2/provenance#metadata.completeness\nIncluded with mode=min and mode=max.\n\nDefines if the provenance information is complete.\n\ncompleteness.parameters is true if all the build arguments are included in the parameters field. When building with min mode, the build arguments are not included in the provenance information and parameters are not complete. Parameters are also not complete on direct LLB builds that did not use a frontend.\n\ncompleteness.environment is always true for BuildKit builds.\n\ncompleteness.materials is true if materials field includes all the dependencies of the build. When building from un-tracked source in a local directory, the materials are not complete, while when building from a remote Git repository all materials can be tracked by BuildKit and completeness.materials is true.\n\n    \"metadata\": {\n\n      \"completeness\": {\n\n        \"parameters\": true,\n\n        \"environment\": true,\n\n        \"materials\": true\n\n      },\n\n      ...\n\n    },\nmetadata.reproducible\nRef: https://slsa.dev/spec/v0.2/provenance#metadata.reproducible\nIncluded with mode=min and mode=max.\n\nDefines if the build result is supposed to be byte-by-byte reproducible. This value can be set by the user with the reproducible=true attestation parameter.\n\n    \"metadata\": {\n\n      \"reproducible\": false,\n\n      ...\n\n    },\nmetadata.https://mobyproject.org/buildkit@v1#hermetic\n\nIncluded with mode=min and mode=max.\n\nThis extension field is set to true if the build was hermetic and did not access the network. In Dockerfiles, a build is hermetic if it does not use RUN commands or disables network with --network=none flag.\n\n    \"metadata\": {\n\n      \"https://mobyproject.org/buildkit@v1#hermetic\": true,\n\n      ...\n\n    },\nmetadata.https://mobyproject.org/buildkit@v1#metadata\n\nPartially included with mode=min.\n\nThis extension field defines BuildKit-specific additional metadata that is not part of the SLSA provenance spec.\n\n    \"metadata\": {\n\n      \"https://mobyproject.org/buildkit@v1#metadata\": {\n\n        \"source\": {...},\n\n        \"layers\": {...},\n\n        \"vcs\": {...},\n\n      },\n\n      ...\n\n    },\nsource\n\nOnly included with mode=max.\n\nDefines a source mapping of LLB build steps, defined in the buildConfig.llbDefinition field, to their original source code (for example, Dockerfile commands). The source.locations field contains the ranges of all the Dockerfile commands ran in an LLB step. source.infos array contains the source code itself. This mapping is present if the BuildKit frontend provided it when creating the LLB definition.\n\nlayers\n\nOnly included with mode=max.\n\nDefines the layer mapping of LLB build step mounts defined in buildConfig.llbDefinition to the OCI descriptors of equivalent layers. This mapping is present if the layer data was available, usually when attestation is for an image or if the build step pulled in image data as part of the build.\n\nvcs\n\nIncluded with mode=min and mode=max.\n\nDefines optional metadata for the version control system used for the build. If a build uses a remote context from Git repository, BuildKit extracts the details of the version control system automatically and displays it in the invocation.configSource field. But if the build uses a source from a local directory, the VCS information is lost even if the directory contained a Git repository. In this case, the build client can send additional vcs:source and vcs:revision build options and BuildKit will add them to the provenance attestations as extra metadata. Note that, contrary to the invocation.configSource field, BuildKit doesn't verify the vcs values, and as such they can't be trusted and should only be used as a metadata hint.\n\nRequest changes\n\nTable of contents\nSLSA v1\nbuildDefinition.buildType\nbuildDefinition.externalParameters.configSource\nbuildDefinition.externalParameters.request\nbuildDefinition.internalParameters.buildConfig\nbuildDefinition.internalParameters.builderPlatform\nbuildDefinition.resolvedDependencies\nrunDetails.builder.id\nrunDetails.metadata.invocationID\nrunDetails.metadata.startedOn\nrunDetails.metadata.finishedOn\nrunDetails.metadata.buildkit_metadata\nrunDetails.metadata.buildkit_hermetic\nrunDetails.metadata.buildkit_completeness\nrunDetails.metadata.buildkit_reproducible\nSLSA v0.2\nbuilder.id\nbuildType\ninvocation.configSource\ninvocation.parameters\ninvocation.environment\nmaterials\nbuildConfig\nmetadata.buildInvocationId\nmetadata.buildStartedOn\nmetadata.buildFinishedOn\nmetadata.completeness\nmetadata.reproducible\nmetadata.https://mobyproject.org/buildkit@v1#hermetic\nmetadata.https://mobyproject.org/buildkit@v1#metadata\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995922,
    "timestamp": "2026-02-07T06:37:09.759Z",
    "title": "SBOM attestations | Docker Docs",
    "url": "https://docs.docker.com/build/metadata/attestations/sbom/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nAnnotations\nBuild attestations\nImage attestation storage\nProvenance attestations\nSBOM attestations\nSLSA definitions\nExporters\nBuildKit\nDebugging\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nMetadata\n/\nBuild attestations\n/\nSBOM attestations\nSBOM attestations\nCopy as Markdown\n\nSBOM attestations help ensure software supply chain transparency by verifying the software artifacts an image contains and the artifacts used to create the image. Metadata included in an SBOM for describing software artifacts may include:\n\nName of the artifact\nVersion\nLicense type\nAuthors\nUnique package identifier\n\nIndexing the contents of an image during the build has benefits over scanning a final image. When scanning happens as part of the build, you can detect software you used to build the image, which may not show up in the final image.\n\nDocker supports SBOM generation and attestation through an SLSA-compliant build process using BuildKit and attestations. The SBOMs generated by BuildKit follow the SPDX standard and attach to the final image as a JSON-encoded SPDX document, using the format defined by the in-toto SPDX predicate. On this page, you‚Äôll learn how to create, manage, and verify SBOM attestations using Docker tooling.\n\nCreate SBOM attestations\n\nTo create an SBOM attestation, pass the --attest type=sbom option to the docker buildx build command:\n\n$ docker buildx build --tag <namespace>/<image>:<version> \\\n\n    --attest type=sbom --push .\n\n\nAlternatively, you can use the shorthand --sbom=true option instead of --attest type=sbom.\n\nFor an example on how to add SBOM attestations with GitHub Actions, see Add attestations with GitHub Actions.\n\nVerify SBOM attestations\n\nAlways validate the generated SBOM for your image before you push your image to a registry.\n\nTo validate, you can build the image using the local exporter. Building with the local exporter saves the build result to your local filesystem instead of creating an image. Attestations are written to a JSON file in the root directory of your export.\n\n$ docker buildx build \\\n\n  --sbom=true \\\n\n  --output type=local,dest=out .\n\n\nThe SBOM file appears in the root directory of the output, named sbom.spdx.json:\n\n$ ls -1 ./out | grep sbom\n\nsbom.spdx.json\n\nArguments\n\nBy default, BuildKit only scans the final stage of an image. The resulting SBOM doesn't include build-time dependencies installed in earlier stages, or that exist in the build context. This may cause you to overlook vulnerabilities in those dependencies, which could impact the security of your final build artifacts.\n\nFor instance, you might use multi-stage builds, with a FROM scratch stanza for your final stage to achieve a smaller image size.\n\nFROM alpine AS build\n\n# build the software ...\n\n\n\nFROM scratch\n\nCOPY --from=build /path/to/bin /bin\n\nENTRYPOINT [ \"/bin\" ]\n\nScanning the resulting image built using this Dockerfile example would not reveal build-time dependencies used in the build stage.\n\nTo include build-time dependencies from your Dockerfile, you can set the build arguments BUILDKIT_SBOM_SCAN_CONTEXT and BUILDKIT_SBOM_SCAN_STAGE. This expands the scanning scope to include the build context and additional stages.\n\nYou can set the arguments as global arguments (after declaring the Dockerfile syntax directive, before the first FROM command) or individually in each stage. If set globally, the value propagates to each stage in the Dockerfile.\n\nThe BUILDKIT_SBOM_SCAN_CONTEXT and BUILDKIT_SBOM_SCAN_STAGE build arguments are special values. You can't perform variable substitution using these arguments, and you can't set them using environment variables from within the Dockerfile. The only way to set these values is using explicit ARG command in the Dockerfile.\n\nScan build context\n\nTo scan the build context, set the BUILDKIT_SBOM_SCAN_CONTEXT to true.\n\n# syntax=docker/dockerfile:1\n\nARG BUILDKIT_SBOM_SCAN_CONTEXT=true\n\nFROM alpine AS build\n\n# ...\n\nYou can use the --build-arg CLI option to override the value specified in the Dockerfile.\n\n$ docker buildx build --tag <image>:<version> \\\n\n    --attest type=sbom \\\n\n    --build-arg BUILDKIT_SBOM_SCAN_CONTEXT=false .\n\n\nNote that passing the option as a CLI argument only, without having declared it using ARG in the Dockerfile, will have no effect. You must specify the ARG in the Dockerfile, whereby you can override the context scanning behavior using --build-arg.\n\nScan stages\n\nTo scan more than just the final stage, set the BUILDKIT_SBOM_SCAN_STAGE argument to true, either globally or in the specific stages that you want to scan. The following table demonstrates the different possible settings for this argument.\n\nValue\tDescription\nBUILDKIT_SBOM_SCAN_STAGE=true\tEnables scanning for the current stage\nBUILDKIT_SBOM_SCAN_STAGE=false\tDisables scanning for the current stage\nBUILDKIT_SBOM_SCAN_STAGE=base,bin\tEnables scanning for the stages named base and bin\n\nOnly stages that are built will be scanned. Stages that aren't dependencies of the target stage won't be built, or scanned.\n\nThe following Dockerfile example uses multi-stage builds to build a static website with Hugo.\n\n# syntax=docker/dockerfile:1\n\nFROM alpine as hugo\n\nARG BUILDKIT_SBOM_SCAN_STAGE=true\n\nWORKDIR /src\n\nCOPY <<config.yml ./\n\ntitle: My Hugo website\n\nconfig.yml\n\nRUN apk add --upgrade hugo && hugo\n\n\n\nFROM scratch\n\nCOPY --from=hugo /src/public /\n\nSetting ARG BUILDKIT_SBOM_SCAN_STAGE=true in the hugo stage ensures that the final SBOM includes the information that Alpine Linux and Hugo were used to create the website.\n\nBuilding this image with the local exporter creates two JSON files:\n\n$ docker buildx build \\\n\n  --sbom=true \\\n\n  --output type=local,dest=out .\n\n$ ls -1 out | grep sbom\n\nsbom-hugo.spdx.json\n\nsbom.spdx.json\n\nInspecting SBOMs\n\nTo explore created SBOMs exported through the image exporter, you can use imagetools inspect.\n\nUsing the --format option, you can specify a template for the output. All SBOM-related data is available under the .SBOM attribute. For example, to get the raw contents of an SBOM in SPDX format:\n\n$ docker buildx imagetools inspect <namespace>/<image>:<version> \\\n\n    --format \"{{ json .SBOM.SPDX }}\"\n\n{\n\n  \"SPDXID\": \"SPDXRef-DOCUMENT\",\n\n  ...\n\n}\n\nTip\n\nIf the image is multi-platform, you can check the SBOM for a platform-specific index using --format '{{ json (index .SBOM \"linux/amd64\").SPDX }}'.\n\nYou can also construct more complex expressions using the full functionality of Go templates. For example, you can list all the installed packages and their version identifiers:\n\n$ docker buildx imagetools inspect <namespace>/<image>:<version> \\\n\n    --format \"{{ range .SBOM.SPDX.packages }}{{ .name }}@{{ .versionInfo }}{{ println }}{{ end }}\"\n\nadduser@3.118ubuntu2\n\napt@2.0.9\n\nbase-files@11ubuntu5.6\n\nbase-passwd@3.5.47\n\n...\n\nSBOM generator\n\nBuildKit generates the SBOM using a scanner plugin. By default, it uses is the BuildKit Syft scanner plugin. This plugin is built on top of Anchore's Syft, an open source tool for generating an SBOM.\n\nYou can select a different plugin to use with the generator option, specifying an image that implements the BuildKit SBOM scanner protocol.\n\n$ docker buildx build --attest type=sbom,generator=<image> .\n\nTip\n\nThe Docker Scout SBOM generator is available. See Docker Scout SBOMs.\n\nSBOM attestation example\n\nThe following JSON example shows what an SBOM attestation might look like.\n\n{\n\n  \"_type\": \"https://in-toto.io/Statement/v0.1\",\n\n  \"predicateType\": \"https://spdx.dev/Document\",\n\n  \"subject\": [\n\n    {\n\n      \"name\": \"pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>\",\n\n      \"digest\": {\n\n        \"sha256\": \"e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862\"\n\n      }\n\n    }\n\n  ],\n\n  \"predicate\": {\n\n    \"SPDXID\": \"SPDXRef-DOCUMENT\",\n\n    \"creationInfo\": {\n\n      \"created\": \"2022-12-16T15:27:25.517047753Z\",\n\n      \"creators\": [\"Organization: Anchore, Inc\", \"Tool: syft-v0.60.3\"],\n\n      \"licenseListVersion\": \"3.18\"\n\n    },\n\n    \"dataLicense\": \"CC0-1.0\",\n\n    \"documentNamespace\": \"https://anchore.com/syft/dir/run/src/core/sbom-cba61a72-fa95-4b60-b63f-03169eac25ca\",\n\n    \"name\": \"/run/src/core/sbom\",\n\n    \"packages\": [\n\n      {\n\n        \"SPDXID\": \"SPDXRef-b074348b8f56ea64\",\n\n        \"downloadLocation\": \"NOASSERTION\",\n\n        \"externalRefs\": [\n\n          {\n\n            \"referenceCategory\": \"SECURITY\",\n\n            \"referenceLocator\": \"cpe:2.3:a:org:repo:\\\\(devel\\\\):*:*:*:*:*:*:*\",\n\n            \"referenceType\": \"cpe23Type\"\n\n          },\n\n          {\n\n            \"referenceCategory\": \"PACKAGE_MANAGER\",\n\n            \"referenceLocator\": \"pkg:golang/github.com/org/repo@(devel)\",\n\n            \"referenceType\": \"purl\"\n\n          }\n\n        ],\n\n        \"filesAnalyzed\": false,\n\n        \"licenseConcluded\": \"NONE\",\n\n        \"licenseDeclared\": \"NONE\",\n\n        \"name\": \"github.com/org/repo\",\n\n        \"sourceInfo\": \"acquired package info from go module information: bin/server\",\n\n        \"versionInfo\": \"(devel)\"\n\n      },\n\n      {\n\n        \"SPDXID\": \"SPDXRef-1b96f57f8fed62d8\",\n\n        \"checksums\": [\n\n          {\n\n            \"algorithm\": \"SHA256\",\n\n            \"checksumValue\": \"0c13f1f3c1636491f716c2027c301f21f9dbed7c4a2185461ba94e3e58443408\"\n\n          }\n\n        ],\n\n        \"downloadLocation\": \"NOASSERTION\",\n\n        \"externalRefs\": [\n\n          {\n\n            \"referenceCategory\": \"SECURITY\",\n\n            \"referenceLocator\": \"cpe:2.3:a:go-chi:chi\\\\/v5:v5.0.0:*:*:*:*:*:*:*\",\n\n            \"referenceType\": \"cpe23Type\"\n\n          },\n\n          {\n\n            \"referenceCategory\": \"SECURITY\",\n\n            \"referenceLocator\": \"cpe:2.3:a:go_chi:chi\\\\/v5:v5.0.0:*:*:*:*:*:*:*\",\n\n            \"referenceType\": \"cpe23Type\"\n\n          },\n\n          {\n\n            \"referenceCategory\": \"SECURITY\",\n\n            \"referenceLocator\": \"cpe:2.3:a:go:chi\\\\/v5:v5.0.0:*:*:*:*:*:*:*\",\n\n            \"referenceType\": \"cpe23Type\"\n\n          },\n\n          {\n\n            \"referenceCategory\": \"PACKAGE_MANAGER\",\n\n            \"referenceLocator\": \"pkg:golang/github.com/go-chi/chi/v5@v5.0.0\",\n\n            \"referenceType\": \"purl\"\n\n          }\n\n        ],\n\n        \"filesAnalyzed\": false,\n\n        \"licenseConcluded\": \"NONE\",\n\n        \"licenseDeclared\": \"NONE\",\n\n        \"name\": \"github.com/go-chi/chi/v5\",\n\n        \"sourceInfo\": \"acquired package info from go module information: bin/server\",\n\n        \"versionInfo\": \"v5.0.0\"\n\n      }\n\n    ],\n\n    \"relationships\": [\n\n      {\n\n        \"relatedSpdxElement\": \"SPDXRef-1b96f57f8fed62d8\",\n\n        \"relationshipType\": \"CONTAINS\",\n\n        \"spdxElementId\": \"SPDXRef-043f7360d3c66bc31ba45388f16423aa58693289126421b71d884145f8837fe1\"\n\n      },\n\n      {\n\n        \"relatedSpdxElement\": \"SPDXRef-b074348b8f56ea64\",\n\n        \"relationshipType\": \"CONTAINS\",\n\n        \"spdxElementId\": \"SPDXRef-043f7360d3c66bc31ba45388f16423aa58693289126421b71d884145f8837fe1\"\n\n      }\n\n    ],\n\n    \"spdxVersion\": \"SPDX-2.2\"\n\n  }\n\n}\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCreate SBOM attestations\nVerify SBOM attestations\nArguments\nScan build context\nScan stages\nInspecting SBOMs\nSBOM generator\nSBOM attestation example\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995928,
    "timestamp": "2026-02-07T06:37:09.762Z",
    "title": "OpenTelemetry support | Docker Docs",
    "url": "https://docs.docker.com/build/debug/opentelemetry/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nCore concepts\nBuild checks\nBuilding\nBuilders\nBake\nCache\nCI\nValidating builds Experimental\nMetadata\nExporters\nBuildKit\nDebugging\nOpenTelemetry support\nBuild release notes\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Build\n/\nDebugging\n/\nOpenTelemetry support\nOpenTelemetry support\nCopy as Markdown\n\nBoth Buildx and BuildKit support OpenTelemetry.\n\nTo capture the trace to Jaeger, set JAEGER_TRACE environment variable to the collection address using a driver-opt.\n\nFirst create a Jaeger container:\n\n$ docker run -d --name jaeger -p \"6831:6831/udp\" -p \"16686:16686\" --restart unless-stopped jaegertracing/all-in-one\n\n\nThen create a docker-container builder that will use the Jaeger instance via the JAEGER_TRACE environment variable:\n\n$ docker buildx create --use \\\n\n  --name mybuilder \\\n\n  --driver docker-container \\\n\n  --driver-opt \"network=host\" \\\n\n  --driver-opt \"env.JAEGER_TRACE=localhost:6831\"\n\n\nBoot and inspect mybuilder:\n\n$ docker buildx inspect --bootstrap\n\n\nBuildx commands should be traced at http://127.0.0.1:16686/:\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995931,
    "timestamp": "2026-02-07T06:37:09.766Z",
    "title": "Docker Compose | Docker Docs",
    "url": "https://docs.docker.com/compose/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\nDocker Compose\nCopy as Markdown\n\nDocker Compose is a tool for defining and running multi-container applications. It is the key to unlocking a streamlined and efficient development and deployment experience.\n\nCompose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single YAML configuration file. Then, with a single command, you create and start all the services from your configuration file.\n\nCompose works in all environments - production, staging, development, testing, as well as CI workflows. It also has commands for managing the whole lifecycle of your application:\n\nStart, stop, and rebuild services\nView the status of running services\nStream the log output of running services\nRun a one-off command on a service\nWhy use Compose?\n\nUnderstand Docker Compose's key benefits\n\nHow Compose works\n\nUnderstand how Compose works\n\nInstall Compose\n\nFollow the instructions on how to install Docker Compose.\n\nQuickstart\n\nLearn the key concepts of Docker Compose whilst building a simple Python web application.\n\nView the release notes\n\nFind out about the latest enhancements and bug fixes.\n\nExplore the Compose file reference\n\nFind information on defining services, networks, and volumes for a Docker application.\n\nUse Compose Bridge\n\nTransform your Compose configuration file into configuration files for different platforms, such as Kubernetes.\n\nBrowse common FAQs\n\nExplore general FAQs and find out how to give feedback.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995937,
    "timestamp": "2026-02-07T06:37:09.771Z",
    "title": "Why use Compose? | Docker Docs",
    "url": "https://docs.docker.com/compose/intro/features-uses/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nHow Compose works\nWhy use Compose?\nHistory and development\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nIntroduction to Compose\n/\nWhy use Compose?\nWhy use Compose?\nCopy as Markdown\nKey benefits of Docker Compose\n\nUsing Docker Compose offers several benefits that streamline the development, deployment, and management of containerized applications:\n\nSimplified control: Define and manage multi-container apps in one YAML file, streamlining orchestration and replication.\n\nEfficient collaboration: Shareable YAML files support smooth collaboration between developers and operations, improving workflows and issue resolution, leading to increased overall efficiency.\n\nRapid application development: Compose caches the configuration used to create a container. When you restart a service that has not changed, Compose re-uses the existing containers. Re-using containers means that you can make changes to your environment very quickly.\n\nPortability across environments: Compose supports variables in the Compose file. You can use these variables to customize your composition for different environments, or different users.\n\nCommon use cases of Docker Compose\n\nCompose can be used in many different ways. Some common use cases are outlined below.\n\nDevelopment environments\n\nWhen you're developing software, the ability to run an application in an isolated environment and interact with it is crucial. The Compose command line tool can be used to create the environment and interact with it.\n\nThe Compose file provides a way to document and configure all of the application's service dependencies (databases, queues, caches, web service APIs, etc). Using the Compose command line tool you can create and start one or more containers for each dependency with a single command (docker compose up).\n\nTogether, these features provide a convenient way for you to get started on a project. Compose can reduce a multi-page \"developer getting started guide\" to a single machine-readable Compose file and a few commands.\n\nAutomated testing environments\n\nAn important part of any Continuous Deployment or Continuous Integration process is the automated test suite. Automated end-to-end testing requires an environment in which to run tests. Compose provides a convenient way to create and destroy isolated testing environments for your test suite. By defining the full environment in a Compose file, you can create and destroy these environments in just a few commands:\n\n$ docker compose up -d\n\n$ ./run_tests\n\n$ docker compose down\n\nSingle host deployments\n\nCompose has traditionally been focused on development and testing workflows, but with each release we're making progress on more production-oriented features.\n\nFor details on using production-oriented features, see Compose in production.\n\nWhat's next?\nLearn about the history of Compose\nUnderstand how Compose works\nTry the Quickstart guide\n\nEdit this page\n\nRequest changes\n\nTable of contents\nKey benefits of Docker Compose\nCommon use cases of Docker Compose\nDevelopment environments\nAutomated testing environments\nSingle host deployments\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995934,
    "timestamp": "2026-02-07T06:37:09.772Z",
    "title": "How Compose works | Docker Docs",
    "url": "https://docs.docker.com/compose/intro/compose-application-model/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nHow Compose works\nWhy use Compose?\nHistory and development\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nIntroduction to Compose\n/\nHow Compose works\nHow Compose works\nCopy as Markdown\n\nWith Docker Compose you use a YAML configuration file, known as the Compose file, to configure your application‚Äôs services, and then you create and start all the services from your configuration with the Compose CLI.\n\nThe Compose file, or compose.yaml file, follows the rules provided by the Compose Specification in how to define multi-container applications. This is the Docker Compose implementation of the formal Compose Specification.\n\nThe Compose application model\nThe Compose file\n\nThe default path for a Compose file is compose.yaml (preferred) or compose.yml that is placed in the working directory. Compose also supports docker-compose.yaml and docker-compose.yml for backwards compatibility of earlier versions. If both files exist, Compose prefers the canonical compose.yaml.\n\nYou can use fragments and extensions to keep your Compose file efficient and easy to maintain.\n\nMultiple Compose files can be merged together to define the application model. The combination of YAML files is implemented by appending or overriding YAML elements based on the Compose file order you set. Simple attributes and maps get overridden by the highest order Compose file, lists get merged by appending. Relative paths are resolved based on the first Compose file's parent folder, whenever complementary files being merged are hosted in other folders. As some Compose file elements can both be expressed as single strings or complex objects, merges apply to the expanded form. For more information, see Working with multiple Compose files.\n\nIf you want to reuse other Compose files, or factor out parts of your application model into separate Compose files, you can also use include. This is useful if your Compose application is dependent on another application which is managed by a different team, or needs to be shared with others.\n\nCLI\n\nThe Docker CLI lets you interact with your Docker Compose applications through the docker compose command and its subcommands. If you're using Docker Desktop, the Docker Compose CLI is included by default.\n\nUsing the CLI, you can manage the lifecycle of your multi-container applications defined in the compose.yaml file. The CLI commands enable you to start, stop, and configure your applications effortlessly.\n\nKey commands\n\nTo start all the services defined in your compose.yaml file:\n\n$ docker compose up\n\n\nTo stop and remove the running services:\n\n$ docker compose down \n\n\nIf you want to monitor the output of your running containers and debug issues, you can view the logs with:\n\n$ docker compose logs\n\n\nTo list all the services along with their current status:\n\n$ docker compose ps\n\n\nFor a full list of all the Compose CLI commands, see the reference documentation.\n\nIllustrative example\n\nThe following example illustrates the Compose concepts outlined above. The example is non-normative.\n\nConsider an application split into a frontend web application and a backend service.\n\nThe frontend is configured at runtime with an HTTP configuration file managed by infrastructure, providing an external domain name, and an HTTPS server certificate injected by the platform's secured secret store.\n\nThe backend stores data in a persistent volume.\n\nBoth services communicate with each other on an isolated back-tier network, while the frontend is also connected to a front-tier network and exposes port 443 for external usage.\n\nThe example application is composed of the following parts:\n\nTwo services, backed by Docker images: webapp and database\nOne secret (HTTPS certificate), injected into the frontend\nOne configuration (HTTP), injected into the frontend\nOne persistent volume, attached to the backend\nTwo networks\nservices:\n\n  frontend:\n\n    image: example/webapp\n\n    ports:\n\n      - \"443:8043\"\n\n    networks:\n\n      - front-tier\n\n      - back-tier\n\n    configs:\n\n      - httpd-config\n\n    secrets:\n\n      - server-certificate\n\n\n\n  backend:\n\n    image: example/database\n\n    volumes:\n\n      - db-data:/etc/data\n\n    networks:\n\n      - back-tier\n\n\n\nvolumes:\n\n  db-data:\n\n    driver: flocker\n\n    driver_opts:\n\n      size: \"10GiB\"\n\n\n\nconfigs:\n\n  httpd-config:\n\n    external: true\n\n\n\nsecrets:\n\n  server-certificate:\n\n    external: true\n\n\n\nnetworks:\n\n  # The presence of these objects is sufficient to define them\n\n  front-tier: {}\n\n  back-tier: {}\n\nThe docker compose up command starts the frontend and backend services, creates the necessary networks and volumes, and injects the configuration and secret into the frontend service.\n\ndocker compose ps provides a snapshot of the current state of your services, making it easy to see which containers are running, their status, and the ports they are using:\n\n$ docker compose ps\n\n\n\nNAME                IMAGE                COMMAND                  SERVICE             CREATED             STATUS              PORTS\n\nexample-frontend-1  example/webapp       \"nginx -g 'daemon of‚Ä¶\"   frontend            2 minutes ago       Up 2 minutes        0.0.0.0:443->8043/tcp\n\nexample-backend-1   example/database     \"docker-entrypoint.s‚Ä¶\"   backend             2 minutes ago       Up 2 minutes\nWhat's next\nTry the Quickstart guide\nExplore some sample applications\nFamiliarize yourself with the Compose Specification\n\nEdit this page\n\nRequest changes\n\nTable of contents\nThe Compose file\nCLI\nKey commands\nIllustrative example\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995940,
    "timestamp": "2026-02-07T06:37:09.778Z",
    "title": "History and development | Docker Docs",
    "url": "https://docs.docker.com/compose/intro/history/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nHow Compose works\nWhy use Compose?\nHistory and development\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nIntroduction to Compose\n/\nHistory and development\nHistory and development of Docker Compose\nCopy as Markdown\n\nThis page provides:\n\nA brief history of the development of the Docker Compose CLI\nA clear explanation of the major versions and file formats that make up Compose v1, v2, and v5\nThe main differences between Compose v1, v2, and v5\nIntroduction\n\nThe diagram above highlights the key differences between Docker Compose v1, v2, and v5. Today, the supported Docker Compose CLI versions are Compose v2 and Compose v5, both of which are defined by the Compose Specification.\n\nThe diagram provides a high-level comparison of file formats, command-line syntax, and supported top-level elements. This is covered in more detail in the following sections.\n\nDocker Compose CLI versioning\n\nCompose v1 was first released in 2014. It was written in Python and invoked with docker-compose. Typically, Compose v1 projects include a top-level version element in the compose.yaml file, with values ranging from 2.0 to 3.8, which refer to the specific file formats.\n\nCompose v2, announced in 2020, is written in Go and is invoked with docker compose. Unlike v1, Compose v2 ignores the version top-level element in the compose.yaml file and relies entirely on the Compose Specification to interpret the file.\n\nCompose v5, released in 2025, is functionally identical to Compose v2. Its primary distinction is the introduction of an official Go SDK. This SDK provides a comprehensive API that lets you integrate Compose functionality directly into your applications, allowing you to load, validate, and manage multi-container environments without relying on the Compose CLI. To avoid confusion with the legacy Compose file formats labeled ‚Äúv2‚Äù and ‚Äúv3,‚Äù the versioning was advanced directly to v5.\n\nCompose file format versioning\n\nThe Docker Compose CLIs are defined by specific file formats.\n\nThree major versions of the Compose file format for Compose v1 were released:\n\nCompose file format 1 with Compose 1.0.0 in 2014\nCompose file format 2.x with Compose 1.6.0 in 2016\nCompose file format 3.x with Compose 1.10.0 in 2017\n\nCompose file format 1 is substantially different to all the following formats as it lacks a top-level services key. Its usage is historical and files written in this format don't run with Compose v2.\n\nCompose file format 2.x and 3.x are very similar to each other, but the latter introduced many new options targeted at Swarm deployments.\n\nTo address confusion around Compose CLI versioning, Compose file format versioning, and feature parity depending on whether Swarm mode was in use, file format 2.x and 3.x were merged into the Compose Specification.\n\nCompose v2 and v5 uses the Compose Specification for project definition. Unlike the prior file formats, the Compose Specification is rolling and makes the version top-level element optional. Compose v2 and v5 also makes use of optional specifications - Deploy, Develop, and Build.\n\nTo make migration easier, Compose v2 and v5 has backwards compatibility for certain elements that have been deprecated or changed between Compose file format 2.x/3.x and the Compose Specification.\n\nWhat's next?\nHow Compose works\nCompose Specification reference\n\nEdit this page\n\nRequest changes\n\nTable of contents\nIntroduction\nDocker Compose CLI versioning\nCompose file format versioning\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995943,
    "timestamp": "2026-02-07T06:37:09.788Z",
    "title": "Install | Docker Docs",
    "url": "https://docs.docker.com/compose/install/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nPlugin\nStandalone (Legacy)\nUninstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nInstall\nOverview of installing Docker Compose\nCopy as Markdown\n\nThis page summarizes the different ways you can install Docker Compose, depending on your platform and needs.\n\nInstallation scenarios\nDocker Desktop (Recommended)\n\nThe easiest and recommended way to get Docker Compose is to install Docker Desktop.\n\nDocker Desktop includes Docker Compose along with Docker Engine and Docker CLI which are Compose prerequisites.\n\nDocker Desktop is available for:\n\nLinux\nMac\nWindows\nTip\n\nIf you have already installed Docker Desktop, you can check which version of Compose you have by selecting About Docker Desktop from the Docker menu  .\n\nPlugin (Linux only)\nImportant\n\nThis method is only available on Linux.\n\nIf you already have Docker Engine and Docker CLI installed, you can install the Docker Compose plugin from the command line, by either:\n\nUsing Docker's repository\nDownloading and installing manually\nStandalone (Legacy)\nWarning\n\nThis install scenario is not recommended and is only supported for backward compatibility purposes.\n\nYou can install the Docker Compose standalone on Linux or on Windows Server.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nInstallation scenarios\nDocker Desktop (Recommended)\nPlugin (Linux only)\nStandalone (Legacy)\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995946,
    "timestamp": "2026-02-07T06:37:09.791Z",
    "title": "Plugin | Docker Docs",
    "url": "https://docs.docker.com/compose/install/linux/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nPlugin\nStandalone (Legacy)\nUninstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nInstall\n/\nPlugin\nInstall the Docker Compose plugin\nCopy as Markdown\n\nThis page contains instructions on how to install the Docker Compose plugin on Linux from the command line.\n\nTo install the Docker Compose plugin on Linux, you can either:\n\nSet up Docker's repository on your Linux system.\nInstall manually.\nNote\n\nThese instructions assume you already have Docker Engine and Docker CLI installed and now want to install the Docker Compose plugin.\n\nInstall using the repository\n\nSet up the repository. Find distribution-specific instructions in:\n\nUbuntu | CentOS | Debian | Raspberry Pi OS | Fedora | RHEL.\n\nUpdate the package index, and install the latest version of Docker Compose:\n\nFor Ubuntu and Debian, run:\n\n$ sudo apt-get update\n\n$ sudo apt-get install docker-compose-plugin\n\n\nFor RPM-based distributions, run:\n\n$ sudo yum update\n\n$ sudo yum install docker-compose-plugin\n\n\nVerify that Docker Compose is installed correctly by checking the version.\n\n$ docker compose version\n\nUpdate Docker Compose\n\nTo update the Docker Compose plugin, run the following commands:\n\nFor Ubuntu and Debian, run:\n\n$ sudo apt-get update\n\n$ sudo apt-get install docker-compose-plugin\n\n\nFor RPM-based distributions, run:\n\n$ sudo yum update\n\n$ sudo yum install docker-compose-plugin\n\nInstall the plugin manually\nWarning\n\nManual installations don‚Äôt auto-update. For ease of maintenance, use the Docker repository method.\n\nTo download and install the Docker Compose CLI plugin, run:\n\n$ DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n\n$ mkdir -p $DOCKER_CONFIG/cli-plugins\n\n$ curl -SL https://github.com/docker/compose/releases/download/v5.0.1/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose\n\n\nThis command downloads and installs the latest release of Docker Compose for the active user under $HOME directory.\n\nTo install:\n\nDocker Compose for all users on your system, replace ~/.docker/cli-plugins with /usr/local/lib/docker/cli-plugins.\nA different version of Compose, substitute v5.0.1 with the version of Compose you want to use.\nFor a different architecture, substitute x86_64 with the architecture you want.\n\nApply executable permissions to the binary:\n\n$ chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n\n\nor, if you chose to install Compose for all users:\n\n$ sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose\n\n\nTest the installation.\n\n$ docker compose version\n\nWhat's next?\nUnderstand how Compose works\nTry the Quickstart guide\n\nEdit this page\n\nRequest changes\n\nTable of contents\nInstall using the repository\nUpdate Docker Compose\nInstall the plugin manually\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995949,
    "timestamp": "2026-02-07T06:37:09.794Z",
    "title": "Standalone (Legacy) | Docker Docs",
    "url": "https://docs.docker.com/compose/install/standalone/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nPlugin\nStandalone (Legacy)\nUninstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nInstall\n/\nStandalone (Legacy)\nInstall the Docker Compose standalone (Legacy)\nCopy as Markdown\nWarning\n\nThis install scenario is not recommended and is only supported for backward compatibility purposes.\n\nThis page contains instructions on how to install Docker Compose standalone on Linux or Windows Server, from the command line.\n\nWarning\n\nThe Docker Compose standalone uses the -compose syntax instead of the current standard syntax compose.\nFor example, you must type docker-compose up when using Docker Compose standalone, instead of docker compose up. Use it only for backward compatibility.\n\nOn Linux\n\nTo download and install the Docker Compose standalone, run:\n\n$ curl -SL https://github.com/docker/compose/releases/download/v5.0.1/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose\n\n\nApply executable permissions to the standalone binary in the target path for the installation.\n\n$ chmod +x /usr/local/bin/docker-compose\n\n\nTest and execute Docker Compose commands using docker-compose.\n\nTip\n\nIf the command docker-compose fails after installation, check your path. You can also create a symbolic link to /usr/bin or any other directory in your path. For example:\n\n$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n\nOn Windows Server\n\nFollow these instructions if you are running the Docker daemon directly on Microsoft Windows Server and want to install Docker Compose.\n\nRun PowerShell as an administrator. In order to proceed with the installation, select Yes when asked if you want this app to make changes to your device.\n\nOptional. Ensure TLS1.2 is enabled. GitHub requires TLS1.2 for secure connections. If you‚Äôre using an older version of Windows Server, for example 2016, or suspect that TLS1.2 is not enabled, run the following command in PowerShell:\n\n[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\n\nDownload the latest release of Docker Compose (v5.0.1). Run the following command:\n\n Start-BitsTransfer -Source \"https://github.com/docker/compose/releases/download/v5.0.1/docker-compose-windows-x86_64.exe\" -Destination $Env:ProgramFiles\\Docker\\docker-compose.exe\n\nTo install a different version of Docker Compose, substitute v5.0.1 with the version of Compose you want to use.\n\nNote\n\nOn Windows Server 2019 you can add the Compose executable to $Env:ProgramFiles\\Docker. Because this directory is registered in the system PATH, you can run the docker-compose --version command on the subsequent step with no additional configuration.\n\nTest the installation.\n\n$ docker-compose.exe version\n\nDocker Compose version v5.0.1\n\nWhat's next?\nUnderstand how Compose works\nTry the Quickstart guide\n\nEdit this page\n\nRequest changes\n\nTable of contents\nOn Linux\nOn Windows Server\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995952,
    "timestamp": "2026-02-07T06:37:09.802Z",
    "title": "Uninstall | Docker Docs",
    "url": "https://docs.docker.com/compose/install/uninstall/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nPlugin\nStandalone (Legacy)\nUninstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nInstall\n/\nUninstall\nUninstall Docker Compose\nCopy as Markdown\n\nHow you uninstall Docker Compose depends on how it was installed. This guide covers uninstallation instructions for:\n\nDocker Compose installed via Docker Desktop\nDocker Compose installed as a CLI plugin\nUninstalling Docker Compose with Docker Desktop\n\nIf you want to uninstall Docker Compose and you have installed Docker Desktop, see Uninstall Docker Desktop.\n\nWarning\n\nUnless you have other Docker instances installed on that specific environment, uninstalling Docker Desktop removes all Docker components, including Docker Engine, Docker CLI, and Docker Compose.\n\nUninstalling the Docker Compose CLI plugin\n\nIf you installed Docker Compose via a package manager, run:\n\nOn Ubuntu or Debian:\n\n$ sudo apt-get remove docker-compose-plugin\n\n\nOn RPM-based distributions:\n\n$ sudo yum remove docker-compose-plugin\n\nManually installed\n\nIf you installed Docker Compose manually (using curl), remove it by deleting the binary:\n\n$ rm $DOCKER_CONFIG/cli-plugins/docker-compose\n\nRemove for all users\n\nIf installed for all users, remove it from the system directory:\n\n$ rm /usr/local/lib/docker/cli-plugins/docker-compose\n\nNote\n\nIf you get a Permission denied error using either of the previous methods, you do not have the permissions needed to remove Docker Compose. To force the removal, prepend sudo to either of the previous instructions and run it again.\n\nInspect the location of the Compose CLI plugin\n\nTo check where Compose is installed, use:\n\n$ docker info --format '{{range .ClientInfo.Plugins}}{{if eq .Name \"compose\"}}{{.Path}}{{end}}{{end}}'\n\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUninstalling Docker Compose with Docker Desktop\nUninstalling the Docker Compose CLI plugin\nManually installed\nRemove for all users\nInspect the location of the Compose CLI plugin\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995955,
    "timestamp": "2026-02-07T06:37:09.808Z",
    "title": "Quickstart | Docker Docs",
    "url": "https://docs.docker.com/compose/gettingstarted/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nQuickstart\nDocker Compose Quickstart\nCopy as Markdown\n\nThis tutorial aims to introduce fundamental concepts of Docker Compose by guiding you through the development of a basic Python web application.\n\nUsing the Flask framework, the application features a hit counter in Redis, providing a practical example of how Docker Compose can be applied in web development scenarios.\n\nThe concepts demonstrated here should be understandable even if you're not familiar with Python.\n\nThis is a non-normative example that demonstrates core Compose functionality.\n\nPrerequisites\n\nMake sure you have:\n\nInstalled the latest version of Docker Compose\nA basic understanding of Docker concepts and how Docker works\nStep 1: Set up\n\nCreate a directory for the project:\n\n$ mkdir composetest\n\n$ cd composetest\n\n\nCreate a file called app.py in your project directory and paste the following code in:\n\nimport time\n\n\n\nimport redis\n\nfrom flask import Flask\n\n\n\napp = Flask(__name__)\n\ncache = redis.Redis(host='redis', port=6379)\n\n\n\ndef get_hit_count():\n\n    retries = 5\n\n    while True:\n\n        try:\n\n            return cache.incr('hits')\n\n        except redis.exceptions.ConnectionError as exc:\n\n            if retries == 0:\n\n                raise exc\n\n            retries -= 1\n\n            time.sleep(0.5)\n\n\n\n@app.route('/')\n\ndef hello():\n\n    count = get_hit_count()\n\n    return f'Hello World! I have been seen {count} times.\\n'\n\nIn this example, redis is the hostname of the redis container on the application's network and the default port, 6379 is used.\n\nNote\n\nNote the way the get_hit_count function is written. This basic retry loop attempts the request multiple times if the Redis service is not available. This is useful at startup while the application comes online, but also makes the application more resilient if the Redis service needs to be restarted anytime during the app's lifetime. In a cluster, this also helps handling momentary connection drops between nodes.\n\nCreate another file called requirements.txt in your project directory and paste the following code in:\n\nflask\n\nredis\n\nCreate a Dockerfile and paste the following code in:\n\n# syntax=docker/dockerfile:1\n\nFROM python:3.10-alpine\n\nWORKDIR /code\n\nENV FLASK_APP=app.py\n\nENV FLASK_RUN_HOST=0.0.0.0\n\nRUN apk add --no-cache gcc musl-dev linux-headers\n\nCOPY requirements.txt requirements.txt\n\nRUN pip install -r requirements.txt\n\nEXPOSE 5000\n\nCOPY . .\n\nCMD [\"flask\", \"run\", \"--debug\"]\nUnderstand the Dockerfile\nImportant\n\nCheck that the Dockerfile has no file extension like .txt. Some editors may append this file extension automatically which results in an error when you run the application.\n\nFor more information on how to write Dockerfiles, see the Dockerfile reference.\n\nStep 2: Define services in a Compose file\n\nCompose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file.\n\nCreate a file called compose.yaml in your project directory and paste the following:\n\nservices:\n\n  web:\n\n    build: .\n\n    ports:\n\n      - \"8000:5000\"\n\n  redis:\n\n    image: \"redis:alpine\"\n\nThis Compose file defines two services: web and redis.\n\nThe web service uses an image that's built from the Dockerfile in the current directory. It then binds the container and the host machine to the exposed port, 8000. This example service uses the default port for the Flask web server, 5000.\n\nThe redis service uses a public Redis image pulled from the Docker Hub registry.\n\nFor more information on the compose.yaml file, see How Compose works.\n\nStep 3: Build and run your app with Compose\n\nWith a single command, you create and start all the services from your configuration file.\n\nFrom your project directory, start up your application by running docker compose up.\n\n$ docker compose up\n\n\n\nCreating network \"composetest_default\" with the default driver\n\nCreating composetest_web_1 ...\n\nCreating composetest_redis_1 ...\n\nCreating composetest_web_1\n\nCreating composetest_redis_1 ... done\n\nAttaching to composetest_web_1, composetest_redis_1\n\nweb_1    |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n\nredis_1  | 1:C 17 Aug 22:11:10.480 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n\nredis_1  | 1:C 17 Aug 22:11:10.480 # Redis version=4.0.1, bits=64, commit=00000000, modified=0, pid=1, just started\n\nredis_1  | 1:C 17 Aug 22:11:10.480 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n\nweb_1    |  * Restarting with stat\n\nredis_1  | 1:M 17 Aug 22:11:10.483 * Running mode=standalone, port=6379.\n\nredis_1  | 1:M 17 Aug 22:11:10.483 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n\nweb_1    |  * Debugger is active!\n\nredis_1  | 1:M 17 Aug 22:11:10.483 # Server initialized\n\nredis_1  | 1:M 17 Aug 22:11:10.483 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n\nweb_1    |  * Debugger PIN: 330-787-903\n\nredis_1  | 1:M 17 Aug 22:11:10.483 * Ready to accept connections\n\n\nCompose pulls a Redis image, builds an image for your code, and starts the services you defined. In this case, the code is statically copied into the image at build time.\n\nEnter http://localhost:8000/ in a browser to see the application running.\n\nIf this doesn't resolve, you can also try http://127.0.0.1:8000.\n\nYou should see a message in your browser saying:\n\nHello World! I have been seen 1 times.\n\nRefresh the page.\n\nThe number should increment.\n\nHello World! I have been seen 2 times.\n\nSwitch to another terminal window, and type docker image ls to list local images.\n\nListing images at this point should return redis and web.\n\n$ docker image ls\n\n\n\nREPOSITORY        TAG           IMAGE ID      CREATED        SIZE\n\ncomposetest_web   latest        e2c21aa48cc1  4 minutes ago  93.8MB\n\npython            3.4-alpine    84e6077c7ab6  7 days ago     82.5MB\n\nredis             alpine        9d8fa9aa0e5b  3 weeks ago    27.5MB\n\n\nYou can inspect images with docker inspect <tag or id>.\n\nStop the application, either by running docker compose down from within your project directory in the second terminal, or by hitting CTRL+C in the original terminal where you started the app.\n\nStep 4: Edit the Compose file to use Compose Watch\n\nEdit the compose.yaml file in your project directory to use watch so you can preview your running Compose services which are automatically updated as you edit and save your code:\n\nservices:\n\n  web:\n\n    build: .\n\n    ports:\n\n      - \"8000:5000\"\n\n    develop:\n\n      watch:\n\n        - action: sync\n\n          path: .\n\n          target: /code\n\n  redis:\n\n    image: \"redis:alpine\"\n\nWhenever a file is changed, Compose syncs the file to the corresponding location under /code inside the container. Once copied, the bundler updates the running application without a restart.\n\nFor more information on how Compose Watch works, see Use Compose Watch. Alternatively, see Manage data in containers for other options.\n\nNote\n\nFor this example to work, the --debug option is added to the Dockerfile. The --debug option in Flask enables automatic code reload, making it possible to work on the backend API without the need to restart or rebuild the container. After changing the .py file, subsequent API calls will use the new code, but the browser UI will not automatically refresh in this small example. Most frontend development servers include native live reload support that works with Compose.\n\nStep 5: Re-build and run the app with Compose\n\nFrom your project directory, type docker compose watch or docker compose up --watch to build and launch the app and start the file watch mode.\n\n$ docker compose watch\n\n[+] Running 2/2\n\n ‚úî Container docs-redis-1 Created                                                                                                                                                                                                        0.0s\n\n ‚úî Container docs-web-1    Recreated                                                                                                                                                                                                      0.1s\n\nAttaching to redis-1, web-1\n\n         ‚¶ø watch enabled\n\n...\n\n\nCheck the Hello World message in a web browser again, and refresh to see the count increment.\n\nStep 6: Update the application\n\nTo see Compose Watch in action:\n\nChange the greeting in app.py and save it. For example, change the Hello World! message to Hello from Docker!:\n\nreturn f'Hello from Docker! I have been seen {count} times.\\n'\n\nRefresh the app in your browser. The greeting should be updated, and the counter should still be incrementing.\n\nOnce you're done, run docker compose down.\n\nStep 7: Split up your services\n\nUsing multiple Compose files lets you customize a Compose application for different environments or workflows. This is useful for large applications that may use dozens of containers, with ownership distributed across multiple teams.\n\nIn your project folder, create a new Compose file called infra.yaml.\n\nCut the Redis service from your compose.yaml file and paste it into your new infra.yaml file. Make sure you add the services top-level attribute at the top of your file. Your infra.yaml file should now look like this:\n\nservices:\n\n  redis:\n\n    image: \"redis:alpine\"\n\nIn your compose.yaml file, add the include top-level attribute along with the path to the infra.yaml file.\n\ninclude:\n\n   - infra.yaml\n\nservices:\n\n  web:\n\n    build: .\n\n    ports:\n\n      - \"8000:5000\"\n\n    develop:\n\n      watch:\n\n        - action: sync\n\n          path: .\n\n          target: /code\n\nRun docker compose up to build the app with the updated Compose files, and run it. You should see the Hello world message in your browser.\n\nThis is a simplified example, but it demonstrates the basic principle of include and how it can make it easier to modularize complex applications into sub-Compose files. For more information on include and working with multiple Compose files, see Working with multiple Compose files.\n\nStep 8: Experiment with some other commands\n\nIf you want to run your services in the background, you can pass the -d flag (for \"detached\" mode) to docker compose up and use docker compose ps to see what is currently running:\n\n$ docker compose up -d\n\n\n\nStarting composetest_redis_1...\n\nStarting composetest_web_1...\n\n\n\n$ docker compose ps\n\n\n\n       Name                      Command               State           Ports         \n\n-------------------------------------------------------------------------------------\n\ncomposetest_redis_1   docker-entrypoint.sh redis ...   Up      6379/tcp              \n\ncomposetest_web_1     flask run                        Up      0.0.0.0:8000->5000/tcp\n\n\nRun docker compose --help to see other available commands.\n\nIf you started Compose with docker compose up -d, stop your services once you've finished with them:\n\n$ docker compose stop\n\n\nYou can bring everything down, removing the containers entirely, with the docker compose down command.\n\nWhere to go next\nTry the Sample apps with Compose\nExplore the full list of Compose commands\nExplore the Compose file reference\nCheck out the Learning Docker Compose video on LinkedIn Learning\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nStep 1: Set up\nStep 2: Define services in a Compose file\nStep 3: Build and run your app with Compose\nStep 4: Edit the Compose file to use Compose Watch\nStep 5: Re-build and run the app with Compose\nStep 6: Update the application\nStep 7: Split up your services\nStep 8: Experiment with some other commands\nWhere to go next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995958,
    "timestamp": "2026-02-07T06:37:09.811Z",
    "title": "Specify a project name | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/project-name/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nSpecify a project name\nSpecify a project name\nCopy as Markdown\n\nBy default, Compose assigns the project name based on the name of the directory that contains the Compose file. You can override this with several methods.\n\nThis page offers examples of scenarios where custom project names can be helpful, outlines the various methods to set a project name, and provides the order of precedence for each approach.\n\nNote\n\nThe default project directory is the base directory of the Compose file. A custom value can also be set for it using the --project-directory command line option.\n\nExample use cases\n\nCompose uses a project name to isolate environments from each other. There are multiple contexts where a project name is useful:\n\nOn a development host: Create multiple copies of a single environment, useful for running stable copies for each feature branch of a project.\nOn a CI server: Prevent interference between builds by setting the project name to a unique build number.\nOn a shared or development host: Avoid interference between different projects that might share the same service names.\nSet a project name\n\nProject names must contain only lowercase letters, decimal digits, dashes, and underscores, and must begin with a lowercase letter or decimal digit. If the base name of the project directory or current directory violates this constraint, alternative mechanisms are available.\n\nThe precedence order for each method, from highest to lowest, is as follows:\n\nThe -p command line flag.\nThe COMPOSE_PROJECT_NAME environment variable.\nThe top-level name: attribute in your Compose file. Or the last name: if you specify multiple Compose files in the command line with the -f flag.\nThe base name of the project directory containing your Compose file. Or the base name of the first Compose file if you specify multiple Compose files in the command line with the -f flag.\nThe base name of the current directory if no Compose file is specified.\nWhat's next?\nRead up on working with multiple Compose files.\nExplore some sample apps.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExample use cases\nSet a project name\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995961,
    "timestamp": "2026-02-07T06:37:09.814Z",
    "title": "Use lifecycle hooks | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/lifecycle/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse lifecycle hooks\nUsing lifecycle hooks with Compose\nCopy as Markdown\nRequires:\nDocker Compose 2.30.0 and later\nServices lifecycle hooks\n\nWhen Docker Compose runs a container, it uses two elements, ENTRYPOINT and COMMAND, to manage what happens when the container starts and stops.\n\nHowever, it can sometimes be easier to handle these tasks separately with lifecycle hooks - commands that run right after the container starts or just before it stops.\n\nLifecycle hooks are particularly useful because they can have special privileges (like running as the root user), even when the container itself runs with lower privileges for security. This means that certain tasks requiring higher permissions can be done without compromising the overall security of the container.\n\nPost-start hooks\n\nPost-start hooks are commands that run after the container has started, but there's no set time for when exactly they will execute. The hook execution timing is not assured during the execution of the container's entrypoint.\n\nIn the example provided:\n\nThe hook is used to change the ownership of a volume to a non-root user (because volumes are created with root ownership by default).\nAfter the container starts, the chown command changes the ownership of the /data directory to user 1001.\nservices:\n\n  app:\n\n    image: backend\n\n    user: 1001\n\n    volumes:\n\n      - data:/data    \n\n    post_start:\n\n      - command: chown -R /data 1001:1001\n\n        user: root\n\n\n\nvolumes:\n\n  data: {} # a Docker volume is created with root ownership\nPre-stop hooks\n\nPre-stop hooks are commands that run before the container is stopped by a specific command (like docker compose down or stopping it manually with Ctrl+C). These hooks won't run if the container stops by itself or gets killed suddenly.\n\nIn the following example, before the container stops, the ./data_flush.sh script is run to perform any necessary cleanup.\n\nservices:\n\n  app:\n\n    image: backend\n\n    pre_stop:\n\n      - command: ./data_flush.sh\nReference information\npost_start\npre_stop\n\nEdit this page\n\nRequest changes\n\nTable of contents\nServices lifecycle hooks\nPost-start hooks\nPre-stop hooks\nReference information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995964,
    "timestamp": "2026-02-07T06:37:09.826Z",
    "title": "Use service profiles | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/profiles/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse service profiles\nUsing profiles with Compose\nCopy as Markdown\n\nProfiles help you adjust your Compose application for different environments or use cases by selectively activating services. Services can be assigned to one or more profiles; unassigned services start/stop by default, while assigned ones only start/stop when their profile is active. This setup means specific services, like those for debugging or development, to be included in a single compose.yml file and activated only as needed.\n\nAssigning profiles to services\n\nServices are associated with profiles through the profiles attribute which takes an array of profile names:\n\nservices:\n\n  frontend:\n\n    image: frontend\n\n    profiles: [frontend]\n\n\n\n  phpmyadmin:\n\n    image: phpmyadmin\n\n    depends_on: [db]\n\n    profiles: [debug]\n\n\n\n  backend:\n\n    image: backend\n\n\n\n  db:\n\n    image: mysql\n\nHere the services frontend and phpmyadmin are assigned to the profiles frontend and debug respectively and as such are only started when their respective profiles are enabled.\n\nServices without a profiles attribute are always enabled. In this case running docker compose up would only start backend and db.\n\nValid profiles names follow the regex format of [a-zA-Z0-9][a-zA-Z0-9_.-]+.\n\nTip\n\nThe core services of your application shouldn't be assigned profiles so they are always enabled and automatically started.\n\nStart specific profiles\n\nTo start a specific profile supply the --profile command-line option or use the COMPOSE_PROFILES environment variable:\n\n$ docker compose --profile debug up\n\n$ COMPOSE_PROFILES=debug docker compose up\n\n\nBoth commands start the services with the debug profile enabled. In the previous compose.yaml file, this starts the services db, backend and phpmyadmin.\n\nStart multiple profiles\n\nYou can also enable multiple profiles, e.g. with docker compose --profile frontend --profile debug up the profiles frontend and debug will be enabled.\n\nMultiple profiles can be specified by passing multiple --profile flags or a comma-separated list for the COMPOSE_PROFILES environment variable:\n\n$ docker compose --profile frontend --profile debug up\n\n$ COMPOSE_PROFILES=frontend,debug docker compose up\n\n\nIf you want to enable all profiles at the same time, you can run docker compose --profile \"*\".\n\nAuto-starting profiles and dependency resolution\n\nWhen you explicitly target a service on the command line that has one or more profiles assigned, you do not need to enable the profile manually as Compose runs that service regardless of whether its profile is activated. This is useful for running one-off services or debugging tools.\n\nOnly the targeted service (and any of its declared dependencies via depends_on) is started. Other services that share the same profile will not be started unless:\n\nThey are also explicitly targeted, or\nThe profile is explicitly enabled using --profile or COMPOSE_PROFILES.\n\nWhen a service with assigned profiles is explicitly targeted on the command line its profiles are started automatically so you don't need to start them manually. This can be used for one-off services and debugging tools. As an example consider the following configuration:\n\nservices:\n\n  backend:\n\n    image: backend\n\n\n\n  db:\n\n    image: mysql\n\n\n\n  db-migrations:\n\n    image: backend\n\n    command: myapp migrate\n\n    depends_on:\n\n      - db\n\n    profiles:\n\n      - tools\n# Only start backend and db (no profiles involved)\n\n$ docker compose up -d\n\n\n\n# Run the db-migrations service without manually enabling the 'tools' profile\n\n$ docker compose run db-migrations\n\nIn this example, db-migrations runs even though it is assigned to the tools profile, because it was explicitly targeted. The db service is also started automatically because it is listed in depends_on.\n\nIf the targeted service has dependencies that are also gated behind a profile, you must ensure those dependencies are either:\n\nIn the same profile\nStarted separately\nNot assigned to any profile so are always enabled\nStop application and services with specific profiles\n\nAs with starting specific profiles, you can use the --profile command-line option or use the COMPOSE_PROFILES environment variable:\n\n$ docker compose --profile debug down\n\n$ COMPOSE_PROFILES=debug docker compose down\n\n\nBoth commands stop and remove services with the debug profile and services without a profile. In the following compose.yaml file, this stops the services db, backend and phpmyadmin.\n\nservices:\n\n  frontend:\n\n    image: frontend\n\n    profiles: [frontend]\n\n\n\n  phpmyadmin:\n\n    image: phpmyadmin\n\n    depends_on: [db]\n\n    profiles: [debug]\n\n\n\n  backend:\n\n    image: backend\n\n\n\n  db:\n\n    image: mysql\n\nif you only want to stop the phpmyadmin service, you can run\n\n$ docker compose down phpmyadmin\n\n\nor\n\n$ docker compose stop phpmyadmin\n\nNote\n\nRunning docker compose down only stops backend and db.\n\nReference information\n\nprofiles\n\nEdit this page\n\nRequest changes\n\nTable of contents\nAssigning profiles to services\nStart specific profiles\nStart multiple profiles\nAuto-starting profiles and dependency resolution\nStop application and services with specific profiles\nReference information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995967,
    "timestamp": "2026-02-07T06:37:09.832Z",
    "title": "Control startup order | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/startup-order/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nControl startup order\nControl startup and shutdown order in Compose\nCopy as Markdown\n\nYou can control the order of service startup and shutdown with the depends_on attribute. Compose always starts and stops containers in dependency order, where dependencies are determined by depends_on, links, volumes_from, and network_mode: \"service:...\".\n\nFor example, if your application needs to access a database and both services are started with docker compose up, there is a chance this will fail since the application service might start before the database service and won't find a database able to handle its SQL statements.\n\nControl startup\n\nOn startup, Compose does not wait until a container is \"ready\", only until it's running. This can cause issues if, for example, you have a relational database system that needs to start its own services before being able to handle incoming connections.\n\nThe solution for detecting the ready state of a service is to use the condition attribute with one of the following options:\n\nservice_started\nservice_healthy. This specifies that a dependency is expected to be ‚Äúhealthy‚Äù, which is defined with healthcheck, before starting a dependent service.\nservice_completed_successfully. This specifies that a dependency is expected to run to successful completion before starting a dependent service.\nExample\nservices:\n\n  web:\n\n    build: .\n\n    depends_on:\n\n      db:\n\n        condition: service_healthy\n\n        restart: true\n\n      redis:\n\n        condition: service_started\n\n  redis:\n\n    image: redis\n\n  db:\n\n    image: postgres:18\n\n    healthcheck:\n\n      test: [\"CMD-SHELL\", \"pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}\"]\n\n      interval: 10s\n\n      retries: 5\n\n      start_period: 30s\n\n      timeout: 10s\n\nCompose creates services in dependency order. db and redis are created before web.\n\nCompose waits for healthchecks to pass on dependencies marked with service_healthy. db is expected to be \"healthy\" (as indicated by healthcheck) before web is created.\n\nrestart: true ensures that if db is updated or restarted due to an explicit Compose operation, for example docker compose restart, the web service is also restarted automatically, ensuring it re-establishes connections or dependencies correctly.\n\nThe healthcheck for the db service uses the pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} command to check if the PostgreSQL database is ready. The service is retried every 10 seconds, up to 5 times.\n\nCompose also removes services in dependency order. web is removed before db and redis.\n\nReference information\ndepends_on\nhealthcheck\n\nEdit this page\n\nRequest changes\n\nTable of contents\nControl startup\nExample\nReference information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995970,
    "timestamp": "2026-02-07T06:37:09.834Z",
    "title": "Use environment variables | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/environment-variables/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nSet environment variables\nEnvironment variables precedence\nPre-defined environment variables\nInterpolation\nBest practices\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse environment variables\nEnvironment variables in Compose\nCopy as Markdown\n\nEnvironment variables and interpolation in Docker Compose help you create reusable, flexible configurations. This makes Dockerized applications easier to manage and deploy across environments.\n\nTip\n\nBefore using environment variables, read through all of the information first to get a full picture of environment variables in Docker Compose.\n\nThis section covers:\n\nHow to set environment variables within your container's environment.\nHow environment variable precedence works within your container's environment.\nPre-defined environment variables.\n\nIt also covers:\n\nHow interpolation can be used to set variables within your Compose file and how it relates to a container's environment.\nSome best practices.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995973,
    "timestamp": "2026-02-07T06:37:09.837Z",
    "title": "Set environment variables | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/environment-variables/set-environment-variables/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nSet environment variables\nEnvironment variables precedence\nPre-defined environment variables\nInterpolation\nBest practices\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse environment variables\n/\nSet environment variables\nSet environment variables within your container's environment\nCopy as Markdown\n\nA container's environment is not set until there's an explicit entry in the service configuration to make this happen. With Compose, there are two ways you can set environment variables in your containers with your Compose file.\n\nTip\n\nDon't use environment variables to pass sensitive information, such as passwords, in to your containers. Use secrets instead.\n\nUse the environment attribute\n\nYou can set environment variables directly in your container's environment with the environment attribute in your compose.yaml.\n\nIt supports both list and mapping syntax:\n\nservices:\n\n  webapp:\n\n    environment:\n\n      DEBUG: \"true\"\n\nis equivalent to\n\nservices:\n\n  webapp:\n\n    environment:\n\n      - DEBUG=true\n\nSee environment attribute for more examples on how to use it.\n\nAdditional information\nYou can choose not to set a value and pass the environment variables from your shell straight through to your containers. It works in the same way as docker run -e VARIABLE ...:\nweb:\n\n  environment:\n\n    - DEBUG\n\nThe value of the DEBUG variable in the container is taken from the value for the same variable in the shell in which Compose is run. Note that in this case no warning is issued if the DEBUG variable in the shell environment is not set.\n\nYou can also take advantage of interpolation. In the following example, the result is similar to the one above but Compose gives you a warning if the DEBUG variable is not set in the shell environment or in an .env file in the project directory.\n\nweb:\n\n  environment:\n\n    - DEBUG=${DEBUG}\nUse the env_file attribute\n\nA container's environment can also be set using .env files along with the env_file attribute.\n\nservices:\n\n  webapp:\n\n    env_file: \"webapp.env\"\n\nUsing an .env file lets you use the same file for use by a plain docker run --env-file ... command, or to share the same .env file within multiple services without the need to duplicate a long environment YAML block.\n\nIt can also help you keep your environment variables separate from your main configuration file, providing a more organized and secure way to manage sensitive information, as you do not need to place your .env file in the root of your project's directory.\n\nThe env_file attribute also lets you use multiple .env files in your Compose application.\n\nThe paths to your .env file, specified in the env_file attribute, are relative to the location of your compose.yaml file.\n\nImportant\n\nInterpolation in .env files is a Docker Compose CLI feature.\n\nIt is not supported when running docker run --env-file ....\n\nAdditional information\nIf multiple files are specified, they are evaluated in order and can override values set in previous files.\nAs of Docker Compose version 2.24.0, you can set your .env file, defined by the env_file attribute, to be optional by using the required field. When required is set to false and the .env file is missing, Compose silently ignores the entry.\nenv_file:\n\n  - path: ./default.env\n\n    required: true # default\n\n  - path: ./override.env\n\n    required: false\nAs of Docker Compose version 2.30.0, you can use an alternative file format for the env_file with the format attribute. For more information, see format.\nValues in your .env file can be overridden from the command line by using docker compose run -e.\nSet environment variables with docker compose run --env\n\nSimilar to docker run --env, you can set environment variables temporarily with docker compose run --env or its short form docker compose run -e:\n\n$ docker compose run -e DEBUG=1 web python console.py\n\nAdditional information\n\nYou can also pass a variable from the shell or your environment files by not giving it a value:\n\n$ docker compose run -e DEBUG web python console.py\n\n\nThe value of the DEBUG variable in the container is taken from the value for the same variable in the shell in which Compose is run or from the environment files.\n\nFurther resources\nUnderstand environment variable precedence.\nSet or change predefined environment variables\nExplore best practices\nUnderstand interpolation\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUse the environment attribute\nAdditional information\nUse the env_file attribute\nAdditional information\nSet environment variables with docker compose run --env\nAdditional information\nFurther resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995976,
    "timestamp": "2026-02-07T06:37:09.847Z",
    "title": "Environment variables precedence | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/environment-variables/envvars-precedence/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nSet environment variables\nEnvironment variables precedence\nPre-defined environment variables\nInterpolation\nBest practices\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse environment variables\n/\nEnvironment variables precedence\nEnvironment variables precedence in Docker Compose\nCopy as Markdown\n\nWhen the same environment variable is set in multiple sources, Docker Compose follows a precedence rule to determine the value for that variable in your container's environment.\n\nThis page explains how Docker Compose determines the final value of an environment variable when it's defined in multiple locations.\n\nThe order of precedence (highest to lowest) is as follows:\n\nSet using docker compose run -e in the CLI.\nSet with either the environment or env_file attribute but with the value interpolated from your shell or an environment file. (either your default .env file, or with the --env-file argument in the CLI).\nSet using just the environment attribute in the Compose file.\nUse of the env_file attribute in the Compose file.\nSet in a container image in the ENV directive. Having any ARG or ENV setting in a Dockerfile evaluates only if there is no Docker Compose entry for environment, env_file or run --env.\nSimple example\n\nIn the following example, a different value for the same environment variable in an .env file and with the environment attribute in the Compose file:\n\n$ cat ./webapp.env\n\nNODE_ENV=test\n\n\n\n$ cat compose.yaml\n\nservices:\n\n  webapp:\n\n    image: 'webapp'\n\n    env_file:\n\n     - ./webapp.env\n\n    environment:\n\n     - NODE_ENV=production\n\n\nThe environment variable defined with the environment attribute takes precedence.\n\n$ docker compose run webapp env | grep NODE_ENV\n\nNODE_ENV=production\n\nAdvanced example\n\nThe following table uses VALUE, an environment variable defining the version for an image, as an example.\n\nHow the table works\n\nEach column represents a context from where you can set a value, or substitute in a value for VALUE.\n\nThe columns Host OS environment and .env file is listed only for illustration purposes. In reality, they don't result in a variable in the container by itself, but in conjunction with either the environment or env_file attribute.\n\nEach row represents a combination of contexts where VALUE is set, substituted, or both. The Result column indicates the final value for VALUE in each scenario.\n\n#\tdocker compose run\tenvironment attribute\tenv_file attribute\tImage ENV\tHost OS environment\t.env file\tResult\n1\t-\t-\t-\t-\tVALUE=1.4\tVALUE=1.3\t-\n2\t-\t-\tVALUE=1.6\tVALUE=1.5\tVALUE=1.4\t-\tVALUE=1.6\n3\t-\tVALUE=1.7\t-\tVALUE=1.5\tVALUE=1.4\t-\tVALUE=1.7\n4\t-\t-\t-\tVALUE=1.5\tVALUE=1.4\tVALUE=1.3\tVALUE=1.5\n5\t--env VALUE=1.8\t-\t-\tVALUE=1.5\tVALUE=1.4\tVALUE=1.3\tVALUE=1.8\n6\t--env VALUE\t-\t-\tVALUE=1.5\tVALUE=1.4\tVALUE=1.3\tVALUE=1.4\n7\t--env VALUE\t-\t-\tVALUE=1.5\t-\tVALUE=1.3\tVALUE=1.3\n8\t-\t-\tVALUE\tVALUE=1.5\tVALUE=1.4\tVALUE=1.3\tVALUE=1.4\n9\t-\t-\tVALUE\tVALUE=1.5\t-\tVALUE=1.3\tVALUE=1.3\n10\t-\tVALUE\t-\tVALUE=1.5\tVALUE=1.4\tVALUE=1.3\tVALUE=1.4\n11\t-\tVALUE\t-\tVALUE=1.5\t-\tVALUE=1.3\tVALUE=1.3\n12\t--env VALUE\tVALUE=1.7\t-\tVALUE=1.5\tVALUE=1.4\tVALUE=1.3\tVALUE=1.4\n13\t--env VALUE=1.8\tVALUE=1.7\t-\tVALUE=1.5\tVALUE=1.4\tVALUE=1.3\tVALUE=1.8\n14\t--env VALUE=1.8\t-\tVALUE=1.6\tVALUE=1.5\tVALUE=1.4\tVALUE=1.3\tVALUE=1.8\n15\t--env VALUE=1.8\tVALUE=1.7\tVALUE=1.6\tVALUE=1.5\tVALUE=1.4\tVALUE=1.3\tVALUE=1.8\nUnderstanding precedence results\n\nResult 1: The local environment takes precedence, but the Compose file is not set to replicate this inside the container, so no such variable is set.\n\nResult 2: The env_file attribute in the Compose file defines an explicit value for VALUE so the container environment is set accordingly.\n\nResult 3: The environment attribute in the Compose file defines an explicit value for VALUE, so the container environment is set accordingly.\n\nResult 4: The image's ENV directive declares the variable VALUE, and since the Compose file is not set to override this value, this variable is defined by image\n\nResult 5: The docker compose run command has the --env flag set with an explicit value, and overrides the value set by the image.\n\nResult 6: The docker compose run command has the --env flag set to replicate the value from the environment. Host OS value takes precedence and is replicated into the container's environment.\n\nResult 7: The docker compose run command has the --env flag set to replicate the value from the environment. Value from .env file is selected to define the container's environment.\n\nResult 8: The env_file attribute in the Compose file is set to replicate VALUE from the local environment. Host OS value takes precedence and is replicated into the container's environment.\n\nResult 9: The env_file attribute in the Compose file is set to replicate VALUE from the local environment. Value from .env file is selected to define the container's environment.\n\nResult 10: The environment attribute in the Compose file is set to replicate VALUE from the local environment. Host OS value takes precedence and is replicated into the container's environment.\n\nResult 11: The environment attribute in the Compose file is set to replicate VALUE from the local environment. Value from .env file is selected to define the container's environment.\n\nResult 12: The --env flag has higher precedence than the environment and env_file attributes and is to set to replicate VALUE from the local environment. Host OS value takes precedence and is replicated into the container's environment.\n\nResults 13 to 15: The --env flag has higher precedence than the environment and env_file attributes and so sets the value.\n\nNext steps\nSet environment variables in Compose\nUse variable interpolation in Compose files\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSimple example\nAdvanced example\nHow the table works\nUnderstanding precedence results\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995979,
    "timestamp": "2026-02-07T06:37:09.849Z",
    "title": "Pre-defined environment variables | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/environment-variables/envvars/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nSet environment variables\nEnvironment variables precedence\nPre-defined environment variables\nInterpolation\nBest practices\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse environment variables\n/\nPre-defined environment variables\nConfigure pre-defined environment variables in Docker Compose\nCopy as Markdown\n\nDocker Compose includes several pre-defined environment variables. It also inherits common Docker CLI environment variables, such as DOCKER_HOST and DOCKER_CONTEXT. See Docker CLI environment variable reference for details.\n\nThis page explains how to set or change the following pre-defined environment variables:\n\nCOMPOSE_PROJECT_NAME\nCOMPOSE_FILE\nCOMPOSE_PROFILES\nCOMPOSE_CONVERT_WINDOWS_PATHS\nCOMPOSE_PATH_SEPARATOR\nCOMPOSE_IGNORE_ORPHANS\nCOMPOSE_REMOVE_ORPHANS\nCOMPOSE_PARALLEL_LIMIT\nCOMPOSE_ANSI\nCOMPOSE_STATUS_STDOUT\nCOMPOSE_ENV_FILES\nCOMPOSE_DISABLE_ENV_FILE\nCOMPOSE_MENU\nCOMPOSE_EXPERIMENTAL\nCOMPOSE_PROGRESS\nMethods to override\nMethod\tDescription\n.env file\tLocated in the working directory.\nShell\tDefined in the host operating system shell.\nCLI\tPassed with --env or -e flag at runtime.\n\nWhen changing or setting any environment variables, be aware of Environment variable precedence.\n\nConfiguration details\nProject and file configuration\nCOMPOSE_PROJECT_NAME\n\nSets the project name. This value is prepended along with the service name to the container's name on startup.\n\nFor example, if your project name is myapp and it includes two services db and web, then Compose starts containers named myapp-db-1 and myapp-web-1 respectively.\n\nCompose can set the project name in different ways. The level of precedence (from highest to lowest) for each method is as follows:\n\nThe -p command line flag\nCOMPOSE_PROJECT_NAME\nThe top-level name: variable from the config file (or the last name: from a series of config files specified using -f)\nThe basename of the project directory containing the config file (or containing the first config file specified using -f)\nThe basename of the current directory if no config file is specified\n\nProject names must contain only lowercase letters, decimal digits, dashes, and underscores, and must begin with a lowercase letter or decimal digit. If the basename of the project directory or current directory violates this constraint, you must use one of the other mechanisms.\n\nSee also the command-line options overview and using -p to specify a project name.\n\nCOMPOSE_FILE\n\nSpecifies the path to a Compose file. Specifying multiple Compose files is supported.\n\nDefault behavior: If not provided, Compose looks for a file named compose.yaml in the current directory and, if not found, then Compose searches each parent directory recursively until a file by that name is found.\nWhen specifying multiple Compose files, the path separators are, by default, on:\n\nMac and Linux: : (colon)\n\nWindows: ; (semicolon) For example:\n\nCOMPOSE_FILE=compose.yaml:compose.prod.yaml\n\nThe path separator can also be customized using COMPOSE_PATH_SEPARATOR.\n\nSee also the command-line options overview and using -f to specify name and path of one or more Compose files.\n\nCOMPOSE_PROFILES\n\nSpecifies one or more profiles to be enabled when docker compose up is run.\n\nServices with matching profiles are started as well as any services for which no profile has been defined.\n\nFor example, calling docker compose up with COMPOSE_PROFILES=frontend selects services with the frontend profile as well as any services without a profile specified.\n\nIf specifying multiple profiles, use a comma as a separator.\n\nThe following example enables all services matching both the frontend and debug profiles and services without a profile.\n\nCOMPOSE_PROFILES=frontend,debug\n\n\nSee also Using profiles with Compose and the --profile command-line option.\n\nCOMPOSE_PATH_SEPARATOR\n\nSpecifies a different path separator for items listed in COMPOSE_FILE.\n\nDefaults to:\nOn macOS and Linux to :\nOn Windows to;\nCOMPOSE_ENV_FILES\n\nSpecifies which environment files Compose should use if --env-file isn't used.\n\nWhen using multiple environment files, use a comma as a separator. For example:\n\nCOMPOSE_ENV_FILES=.env.envfile1,.env.envfile2\n\n\nIf COMPOSE_ENV_FILES is not set, and you don't provide --env-file in the CLI, Docker Compose uses the default behavior, which is to look for an .env file in the project directory.\n\nCOMPOSE_DISABLE_ENV_FILE\n\nLets you disable the use of the default .env file.\n\nSupported values:\ntrue or 1, Compose ignores the .env file\nfalse or 0, Compose looks for an .env file in the project directory\nDefaults to: 0\nEnvironment handling and container lifecycle\nCOMPOSE_CONVERT_WINDOWS_PATHS\n\nWhen enabled, Compose performs path conversion from Windows-style to Unix-style in volume definitions.\n\nSupported values:\ntrue or 1, to enable\nfalse or 0, to disable\nDefaults to: 0\nCOMPOSE_IGNORE_ORPHANS\n\nWhen enabled, Compose doesn't try to detect orphaned containers for the project.\n\nSupported values:\ntrue or 1, to enable\nfalse or 0, to disable\nDefaults to: 0\nCOMPOSE_REMOVE_ORPHANS\n\nWhen enabled, Compose automatically removes orphaned containers when updating a service or stack. Orphaned containers are those that were created by a previous configuration but are no longer defined in the current compose.yaml file.\n\nSupported values:\ntrue or 1, to enable automatic removal of orphaned containers\nfalse or 0, to disable automatic removal. Compose displays a warning about orphaned containers instead.\nDefaults to: 0\nCOMPOSE_PARALLEL_LIMIT\n\nSpecifies the maximum level of parallelism for concurrent engine calls.\n\nOutput\nCOMPOSE_ANSI\n\nSpecifies when to print ANSI control characters.\n\nSupported values:\nauto, Compose detects if TTY mode can be used. Otherwise, use plain text mode\nnever, use plain text mode\nalways or 0, use TTY mode\nDefaults to: auto\nCOMPOSE_STATUS_STDOUT\n\nWhen enabled, Compose writes its internal status and progress messages to stdout instead of stderr. The default value is false to clearly separate the output streams between Compose messages and your container's logs.\n\nSupported values:\ntrue or 1, to enable\nfalse or 0, to disable\nDefaults to: 0\nCOMPOSE_PROGRESS\nRequires:\nDocker Compose 2.36.0 and later\n\nDefines the type of progress output, if --progress isn't used.\n\nSupported values are auto, tty, plain, json, and quiet. Default is auto.\n\nUser experience\nCOMPOSE_MENU\nRequires:\nDocker Compose 2.26.0 and later\n\nWhen enabled, Compose displays a navigation menu where you can choose to open the Compose stack in Docker Desktop, switch on watch mode, or use Docker Debug.\n\nSupported values:\ntrue or 1, to enable\nfalse or 0, to disable\nDefaults to: 1 if you obtained Docker Compose through Docker Desktop, otherwise the default is 0\nCOMPOSE_EXPERIMENTAL\nRequires:\nDocker Compose 2.26.0 and later\n\nThis is an opt-out variable. When turned off it deactivates the experimental features.\n\nSupported values:\ntrue or 1, to enable\nfalse or 0, to disable\nDefaults to: 1\nUnsupported in Compose V2\n\nThe following environment variables have no effect in Compose V2.\n\nCOMPOSE_API_VERSION By default the API version is negotiated with the server. Use DOCKER_API_VERSION.\nSee the Docker CLI environment variable reference page.\nCOMPOSE_HTTP_TIMEOUT\nCOMPOSE_TLS_VERSION\nCOMPOSE_FORCE_WINDOWS_HOST\nCOMPOSE_INTERACTIVE_NO_CLI\nCOMPOSE_DOCKER_CLI_BUILD Use DOCKER_BUILDKIT to select between BuildKit and the classic builder. If DOCKER_BUILDKIT=0 then docker compose build uses the classic builder to build images.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nMethods to override\nConfiguration details\nProject and file configuration\nEnvironment handling and container lifecycle\nOutput\nUser experience\nUnsupported in Compose V2\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995982,
    "timestamp": "2026-02-07T06:37:09.854Z",
    "title": "Interpolation | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/environment-variables/variable-interpolation/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nSet environment variables\nEnvironment variables precedence\nPre-defined environment variables\nInterpolation\nBest practices\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse environment variables\n/\nInterpolation\nSet, use, and manage variables in a Compose file with interpolation\nCopy as Markdown\n\nA Compose file can use variables to offer more flexibility. If you want to quickly switch between image tags to test multiple versions, or want to adjust a volume source to your local environment, you don't need to edit the Compose file each time, you can just set variables that insert values into your Compose file at runtime.\n\nInterpolation can also be used to insert values into your Compose file at runtime, which is then used to pass variables into your container's environment\n\nBelow is a simple example:\n\n$ cat .env\n\nTAG=v1.5\n\n$ cat compose.yaml\n\nservices:\n\n  web:\n\n    image: \"webapp:${TAG}\"\n\n\nWhen you run docker compose up, the web service defined in the Compose file interpolates in the image webapp:v1.5 which was set in the .env file. You can verify this with the config command, which prints your resolved application config to the terminal:\n\n$ docker compose config\n\nservices:\n\n  web:\n\n    image: 'webapp:v1.5'\n\nInterpolation syntax\n\nInterpolation is applied for unquoted and double-quoted values. Both braced (${VAR}) and unbraced ($VAR) expressions are supported.\n\nFor braced expressions, the following formats are supported:\n\nDirect substitution\n${VAR} -> value of VAR\nDefault value\n${VAR:-default} -> value of VAR if set and non-empty, otherwise default\n${VAR-default} -> value of VAR if set, otherwise default\nRequired value\n${VAR:?error} -> value of VAR if set and non-empty, otherwise exit with error\n${VAR?error} -> value of VAR if set, otherwise exit with error\nAlternative value\n${VAR:+replacement} -> replacement if VAR is set and non-empty, otherwise empty\n${VAR+replacement} -> replacement if VAR is set, otherwise empty\n\nFor more information, see Interpolation in the Compose Specification.\n\nWays to set variables with interpolation\n\nDocker Compose can interpolate variables into your Compose file from multiple sources.\n\nNote that when the same variable is declared by multiple sources, precedence applies:\n\nVariables from your shell environment\nIf --env-file is not set, variables set by an .env file in local working directory (PWD)\nVariables from a file set by --env-file or an .env file in project directory\n\nYou can check variables and values used by Compose to interpolate the Compose model by running docker compose config --environment.\n\n.env file\n\nAn .env file in Docker Compose is a text file used to define variables that should be made available for interpolation when running docker compose up. This file typically contains key-value pairs of variables, and it lets you centralize and manage configuration in one place. The .env file is useful if you have multiple variables you need to store.\n\nThe .env file is the default method for setting variables. The .env file should be placed at the root of the project directory next to your compose.yaml file. For more information on formatting an environment file, see Syntax for environment files.\n\nBasic example:\n\n$ cat .env\n\n## define COMPOSE_DEBUG based on DEV_MODE, defaults to false\n\nCOMPOSE_DEBUG=${DEV_MODE:-false}\n\n\n\n$ cat compose.yaml \n\n  services:\n\n    webapp:\n\n      image: my-webapp-image\n\n      environment:\n\n        - DEBUG=${COMPOSE_DEBUG}\n\n\n\n$ DEV_MODE=true docker compose config\n\nservices:\n\n  webapp:\n\n    environment:\n\n      DEBUG: \"true\"\n\nAdditional information\n\nIf you define a variable in your .env file, you can reference it directly in your compose.yaml with the environment attribute. For example, if your .env file contains the environment variable DEBUG=1 and your compose.yaml file looks like this:\n\n services:\n\n   webapp:\n\n     image: my-webapp-image\n\n     environment:\n\n       - DEBUG=${DEBUG}\n\nDocker Compose replaces ${DEBUG} with the value from the .env file\n\nImportant\n\nBe aware of Environment variables precedence when using variables in an .env file that as environment variables in your container's environment.\n\nYou can place your .env file in a location other than the root of your project's directory, and then use the --env-file option in the CLI so Compose can navigate to it.\n\nYour .env file can be overridden by another .env if it is substituted with --env-file.\n\nImportant\n\nSubstitution from .env files is a Docker Compose CLI feature.\n\nIt is not supported by Swarm when running docker stack deploy.\n\n.env file syntax\n\nThe following syntax rules apply to environment files:\n\nLines beginning with # are processed as comments and ignored.\n\nBlank lines are ignored.\n\nUnquoted and double-quoted (\") values have interpolation applied.\n\nEach line represents a key-value pair. Values can optionally be quoted.\n\nDelimiter separating key and value can be either = or :.\n\nSpaces before and after value are ignored.\n\nVAR=VAL -> VAL\nVAR=\"VAL\" -> VAL\nVAR='VAL' -> VAL\nVAR: VAL -> VAL\nVAR = VAL -> VAL\n\nInline comments for unquoted values must be preceded with a space.\n\nVAR=VAL # comment -> VAL\nVAR=VAL# not a comment -> VAL# not a comment\n\nInline comments for quoted values must follow the closing quote.\n\nVAR=\"VAL # not a comment\" -> VAL # not a comment\nVAR=\"VAL\" # comment -> VAL\n\nSingle-quoted (') values are used literally.\n\nVAR='$OTHER' -> $OTHER\nVAR='${OTHER}' -> ${OTHER}\n\nQuotes can be escaped with \\.\n\nVAR='Let\\'s go!' -> Let's go!\nVAR=\"{\\\"hello\\\": \\\"json\\\"}\" -> {\"hello\": \"json\"}\n\nCommon shell escape sequences including \\n, \\r, \\t, and \\\\ are supported in double-quoted values.\n\nVAR=\"some\\tvalue\" -> some value\nVAR='some\\tvalue' -> some\\tvalue\nVAR=some\\tvalue -> some\\tvalue\n\nSingle-quoted values can span multiple lines. Example:\n\nKEY='SOME\n\nVALUE'\n\nIf you then run docker compose config, you'll see:\n\nenvironment:\n\n  KEY: |-\n\n    SOME\n\n    VALUE\nSubstitute with --env-file\n\nYou can set default values for multiple environment variables, in an .env file and then pass the file as an argument in the CLI.\n\nThe advantage of this method is that you can store the file anywhere and name it appropriately, for example, This file path is relative to the current working directory where the Docker Compose command is executed. Passing the file path is done using the --env-file option:\n\n$ docker compose --env-file ./config/.env.dev up\n\nAdditional information\nThis method is useful if you want to temporarily override an .env file that is already referenced in your compose.yaml file. For example you may have different .env files for production ( .env.prod) and testing (.env.test). In the following example, there are two environment files, .env and .env.dev. Both have different values set for TAG.\n$ cat .env\n\nTAG=v1.5\n\n$ cat ./config/.env.dev\n\nTAG=v1.6\n\n$ cat compose.yaml\n\nservices:\n\n  web:\n\n    image: \"webapp:${TAG}\"\n\nIf the --env-file is not used in the command line, the .env file is loaded by default:\n$ docker compose config\n\nservices:\n\n  web:\n\n    image: 'webapp:v1.5'\n\nPassing the --env-file argument overrides the default file path:\n$ docker compose --env-file ./config/.env.dev config\n\nservices:\n\n  web:\n\n    image: 'webapp:v1.6'\n\nWhen an invalid file path is being passed as an --env-file argument, Compose returns an error:\n$ docker compose --env-file ./doesnotexist/.env.dev  config\n\nERROR: Couldn't find env file: /home/user/./doesnotexist/.env.dev\n\nYou can use multiple --env-file options to specify multiple environment files, and Docker Compose reads them in order. Later files can override variables from earlier files.\n$ docker compose --env-file .env --env-file .env.override up\n\nYou can override specific environment variables from the command line when starting containers.\n$ docker compose --env-file .env.dev up -e DATABASE_URL=mysql://new_user:new_password@new_db:3306/new_database\n\nlocal .env file versus <project directory> .env file\n\nAn .env file can also be used to declare pre-defined environment variables used to control Compose behavior and files to be loaded.\n\nWhen executed without an explicit --env-file flag, Compose searches for an .env file in your working directory (PWD) and loads values both for self-configuration and interpolation. If the values in this file define the COMPOSE_FILE pre-defined variable, which results in a project directory being set to another folder, Compose will load a second .env file, if present. This second .env file has a lower precedence.\n\nThis mechanism makes it possible to invoke an existing Compose project with a custom set of variables as overrides, without the need to pass environment variables by the command line.\n\n$ cat .env\n\nCOMPOSE_FILE=../compose.yaml\n\nPOSTGRES_VERSION=9.3\n\n\n\n$ cat ../compose.yaml \n\nservices:\n\n  db:\n\n    image: \"postgres:${POSTGRES_VERSION}\"\n\n$ cat ../.env\n\nPOSTGRES_VERSION=9.2\n\n\n\n$ docker compose config\n\nservices:\n\n  db:\n\n    image: \"postgres:9.3\"\n\nSubstitute from the shell\n\nYou can use existing environment variables from your host machine or from the shell environment where you execute docker compose commands. This lets you dynamically inject values into your Docker Compose configuration at runtime. For example, suppose the shell contains POSTGRES_VERSION=9.3 and you supply the following configuration:\n\ndb:\n\n  image: \"postgres:${POSTGRES_VERSION}\"\n\nWhen you run docker compose up with this configuration, Compose looks for the POSTGRES_VERSION environment variable in the shell and substitutes its value in. For this example, Compose resolves the image to postgres:9.3 before running the configuration.\n\nIf an environment variable is not set, Compose substitutes with an empty string. In the previous example, if POSTGRES_VERSION is not set, the value for the image option is postgres:.\n\nNote\n\npostgres: is not a valid image reference. Docker expects either a reference without a tag, like postgres which defaults to the latest image, or with a tag such as postgres:15.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nInterpolation syntax\nWays to set variables with interpolation\n.env file\nSubstitute with --env-file\nlocal .env file versus <project directory> .env file\nSubstitute from the shell\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995985,
    "timestamp": "2026-02-07T06:37:09.864Z",
    "title": "Best practices | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/environment-variables/best-practices/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nSet environment variables\nEnvironment variables precedence\nPre-defined environment variables\nInterpolation\nBest practices\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse environment variables\n/\nBest practices\nBest practices for working with environment variables in Docker Compose\nCopy as Markdown\nHandle sensitive information securely\n\nBe cautious about including sensitive data in environment variables. Consider using Secrets for managing sensitive information.\n\nUnderstand environment variable precedence\n\nBe aware of how Docker Compose handles the precedence of environment variables from different sources (.env files, shell variables, Dockerfiles).\n\nUse specific environment files\n\nConsider how your application adapts to different environments. For example development, testing, production, and use different .env files as needed.\n\nKnow interpolation\n\nUnderstand how interpolation works within compose files for dynamic configurations.\n\nCommand line overrides\n\nBe aware that you can override environment variables from the command line when starting containers. This is useful for testing or when you have temporary changes.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nBest practices\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995988,
    "timestamp": "2026-02-07T06:37:09.866Z",
    "title": "Build dependent images | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/dependent-images/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nBuild dependent images\nBuild dependent images\nCopy as Markdown\nRequires:\nDocker Compose 2.22.0 and later\n\nTo reduce push/pull time and image weight, a common practice for Compose applications is to have services share base layers as much as possible. You typically select the same operating system base image for all services. But you can also get one step further by sharing image layers when your images share the same system packages. The challenge to address is then to avoid repeating the exact same Dockerfile instruction in all services.\n\nFor illustration, this page assumes you want all your services to be built with an alpine base image and install the system package openssl.\n\nMulti-stage Dockerfile\n\nThe recommended approach is to group the shared declaration in a single Dockerfile, and use multi-stage features so that service images are built from this shared declaration.\n\nDockerfile:\n\nFROM alpine as base\n\nRUN /bin/sh -c apk add --update --no-cache openssl\n\n\n\nFROM base as service_a\n\n# build service a\n\n...\n\n\n\nFROM base as service_b\n\n# build service b\n\n...\n\nCompose file:\n\nservices:\n\n  a:\n\n     build:\n\n       target: service_a\n\n  b:\n\n     build:\n\n       target: service_b\nUse another service's image as the base image\n\nA popular pattern is to reuse a service image as a base image in another service. As Compose does not parse the Dockerfile, it can't automatically detect this dependency between services to correctly order the build execution.\n\na.Dockerfile:\n\nFROM alpine\n\nRUN /bin/sh -c apk add --update --no-cache openssl\n\nb.Dockerfile:\n\nFROM service_a\n\n# build service b\n\nCompose file:\n\nservices:\n\n  a:\n\n     image: service_a \n\n     build:\n\n       dockerfile: a.Dockerfile\n\n  b:\n\n     image: service_b\n\n     build:\n\n       dockerfile: b.Dockerfile\n\nLegacy Docker Compose v1 used to build images sequentially, which made this pattern usable out of the box. Compose v2 uses BuildKit to optimise builds and build images in parallel and requires an explicit declaration.\n\nThe recommended approach is to declare the dependent base image as an additional build context:\n\nCompose file:\n\nservices:\n\n  a:\n\n     image: service_a\n\n     build: \n\n       dockerfile: a.Dockerfile\n\n  b:\n\n     image: service_b\n\n     build:\n\n       dockerfile: b.Dockerfile\n\n       additional_contexts:\n\n         # `FROM service_a` will be resolved as a dependency on service \"a\" which has to be built first\n\n         service_a: \"service:a\"\n\nWith the additional_contexts attribute, you can refer to an image built by another service without needing to explicitly name it:\n\nb.Dockerfile:\n\n\n\nFROM base_image  \n\n# `base_image` doesn't resolve to an actual image. This is used to point to a named additional context\n\n\n\n# build service b\n\nCompose file:\n\nservices:\n\n  a:\n\n     build: \n\n       dockerfile: a.Dockerfile\n\n       # built image will be tagged <project_name>_a\n\n  b:\n\n     build:\n\n       dockerfile: b.Dockerfile\n\n       additional_contexts:\n\n         # `FROM base_image` will be resolved as a dependency on service \"a\" which has to be built first\n\n         base_image: \"service:a\"\nBuild with Bake\n\nUsing Bake let you pass the complete build definition for all services and to orchestrate build execution in the most efficient way.\n\nTo enable this feature, run Compose with the COMPOSE_BAKE=true variable set in your environment.\n\n$ COMPOSE_BAKE=true docker compose build\n\n[+] Building 0.0s (0/1)                                                         \n\n => [internal] load local bake definitions                                 0.0s\n\n...\n\n[+] Building 2/2 manifest list sha256:4bd2e88a262a02ddef525c381a5bdb08c83  0.0s\n\n ‚úî service_b  Built                                                        0.7s \n\n ‚úî service_a  Built    \n\n\nBake can also be selected as the default builder by editing your $HOME/.docker/config.json config file:\n\n{\n\n  ...\n\n  \"plugins\": {\n\n    \"compose\": {\n\n      \"build\": \"bake\"\n\n    }\n\n  }\n\n  ...\n\n}\nAdditional resources\nDocker Compose build reference\nLearn about multi-stage Dockerfiles\n\nEdit this page\n\nRequest changes\n\nTable of contents\nMulti-stage Dockerfile\nUse another service's image as the base image\nBuild with Bake\nAdditional resources\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995991,
    "timestamp": "2026-02-07T06:37:09.871Z",
    "title": "Use Compose Watch | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/file-watch/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse Compose Watch\nUse Compose Watch\nCopy as Markdown\nRequires:\nDocker Compose 2.22.0 and later\n\nThe watch attribute automatically updates and previews your running Compose services as you edit and save your code. For many projects, this enables a hands-off development workflow once Compose is running, as services automatically update themselves when you save your work.\n\nwatch adheres to the following file path rules:\n\nAll paths are relative to the project directory, apart from ignore file patterns\nDirectories are watched recursively\nGlob patterns aren't supported\nRules from .dockerignore apply\nUse ignore option to define additional paths to be ignored (same syntax)\nTemporary/backup files for common IDEs (Vim, Emacs, JetBrains, & more) are ignored automatically\n.git directories are ignored automatically\n\nYou don't need to switch on watch for all services in a Compose project. In some instances, only part of the project, for example the Javascript frontend, might be suitable for automatic updates.\n\nCompose Watch is designed to work with services built from local source code using the build attribute. It doesn't track changes for services that rely on pre-built images specified by the image attribute.\n\nCompose Watch versus bind mounts\n\nCompose supports sharing a host directory inside service containers. Watch mode does not replace this functionality but exists as a companion specifically suited to developing in containers.\n\nMore importantly, watch allows for greater granularity than is practical with a bind mount. Watch rules let you ignore specific files or entire directories within the watched tree.\n\nFor example, in a JavaScript project, ignoring the node_modules/ directory has two benefits:\n\nPerformance. File trees with many small files can cause a high I/O load in some configurations\nMulti-platform. Compiled artifacts cannot be shared if the host OS or architecture is different from the container\n\nFor example, in a Node.js project, it's not recommended to sync the node_modules/ directory. Even though JavaScript is interpreted, npm packages can contain native code that is not portable across platforms.\n\nConfiguration\n\nThe watch attribute defines a list of rules that control automatic service updates based on local file changes.\n\nEach rule requires a path pattern and action to take when a modification is detected. There are two possible actions for watch and depending on the action, additional fields might be accepted or required.\n\nWatch mode can be used with many different languages and frameworks. The specific paths and rules will vary from project to project, but the concepts remain the same.\n\nPrerequisites\n\nIn order to work properly, watch relies on common executables. Make sure your service image contains the following binaries:\n\nstat\nmkdir\nrmdir\n\nwatch also requires that the container's USER can write to the target path so it can update files. A common pattern is for initial content to be copied into the container using the COPY instruction in a Dockerfile. To ensure such files are owned by the configured user, use the COPY --chown flag:\n\n# Run as a non-privileged user\n\nFROM node:18\n\nRUN useradd -ms /bin/sh -u 1001 app\n\nUSER app\n\n\n\n# Install dependencies\n\nWORKDIR /app\n\nCOPY package.json package-lock.json ./\n\nRUN npm install\n\n\n\n# Copy source files into application directory\n\nCOPY --chown=app:app . /app\naction\nSync\n\nIf action is set to sync, Compose makes sure any changes made to files on your host automatically match with the corresponding files within the service container.\n\nsync is ideal for frameworks that support \"Hot Reload\" or equivalent functionality.\n\nMore generally, sync rules can be used in place of bind mounts for many development use cases.\n\nRebuild\n\nIf action is set to rebuild, Compose automatically builds a new image with BuildKit and replaces the running service container.\n\nThe behavior is the same as running docker compose up --build <svc>.\n\nRebuild is ideal for compiled languages or as a fallback for modifications to particular files that require a full image rebuild (e.g. package.json).\n\nSync + Restart\n\nIf action is set to sync+restart, Compose synchronizes your changes with the service containers and restarts them.\n\nsync+restart is ideal when the config file changes, and you don't need to rebuild the image but just restart the main process of the service containers. It will work well when you update a database configuration or your nginx.conf file, for example.\n\nTip\n\nOptimize your Dockerfile for speedy incremental rebuilds with image layer caching and multi-stage builds.\n\npath and target\n\nThe target field controls how the path is mapped into the container.\n\nFor path: ./app/html and a change to ./app/html/index.html:\n\ntarget: /app/html -> /app/html/index.html\ntarget: /app/static -> /app/static/index.html\ntarget: /assets -> /assets/index.html\nignore\n\nThe ignore patterns are relative to the path defined in the current watch action, not to the project directory. In the following Example 1, the ignore path would be relative to the ./web directory specified in the path attribute.\n\ninitial_sync\n\nWhen using a sync+x action, the initial_sync attribute tells Compose to ensure files that are part of the defined path are up to date before starting a new watch session.\n\nExample 1\n\nThis minimal example targets a Node.js application with the following structure:\n\nmyproject/\n\n‚îú‚îÄ‚îÄ web/\n\n‚îÇ   ‚îú‚îÄ‚îÄ App.jsx\n\n‚îÇ   ‚îú‚îÄ‚îÄ index.js\n\n‚îÇ   ‚îî‚îÄ‚îÄ node_modules/\n\n‚îú‚îÄ‚îÄ Dockerfile\n\n‚îú‚îÄ‚îÄ compose.yaml\n\n‚îî‚îÄ‚îÄ package.json\nservices:\n\n  web:\n\n    build: .\n\n    command: npm start\n\n    develop:\n\n      watch:\n\n        - action: sync\n\n          path: ./web\n\n          target: /src/web\n\n          initial_sync: true\n\n          ignore:\n\n            - node_modules/\n\n        - action: rebuild\n\n          path: package.json\n\nIn this example, when running docker compose up --watch, a container for the web service is launched using an image built from the Dockerfile in the project's root. The web service runs npm start for its command, which then launches a development version of the application with Hot Module Reload enabled in the bundler (Webpack, Vite, Turbopack, etc).\n\nAfter the service is up, the watch mode starts monitoring the target directories and files. Then, whenever a source file in the web/ directory is changed, Compose syncs the file to the corresponding location under /src/web inside the container. For example, ./web/App.jsx is copied to /src/web/App.jsx.\n\nOnce copied, the bundler updates the running application without a restart.\n\nAnd in this case, the ignore rule would apply to myproject/web/node_modules/, not myproject/node_modules/.\n\nUnlike source code files, adding a new dependency can‚Äôt be done on-the-fly, so whenever package.json is changed, Compose rebuilds the image and recreates the web service container.\n\nThis pattern can be followed for many languages and frameworks, such as Python with Flask: Python source files can be synced while a change to requirements.txt should trigger a rebuild.\n\nExample 2\n\nAdapting the previous example to demonstrate sync+restart:\n\nservices:\n\n  web:\n\n    build: .\n\n    command: npm start\n\n    develop:\n\n      watch:\n\n        - action: sync\n\n          path: ./web\n\n          target: /app/web\n\n          ignore:\n\n            - node_modules/\n\n        - action: sync+restart\n\n          path: ./proxy/nginx.conf\n\n          target: /etc/nginx/conf.d/default.conf\n\n\n\n  backend:\n\n    build:\n\n      context: backend\n\n      target: builder\n\nThis setup demonstrates how to use the sync+restart action in Docker Compose to efficiently develop and test a Node.js application with a frontend web server and backend service. The configuration ensures that changes to the application code and configuration files are quickly synchronized and applied, with the web service restarting as needed to reflect the changes.\n\nUse watch\nAdd watch sections to one or more services in compose.yaml.\nRun docker compose up --watch to build and launch a Compose project and start the file watch mode.\nEdit service source files using your preferred IDE or editor.\nNote\n\nWatch can also be used with the dedicated docker compose watch command if you don't want to get the application logs mixed with the (re)build logs and filesystem sync events.\n\nTip\n\nCheck out dockersamples/avatars, or local setup for Docker docs for a demonstration of Compose watch.\n\nReference\nCompose Develop Specification\n\nEdit this page\n\nRequest changes\n\nTable of contents\nCompose Watch versus bind mounts\nConfiguration\nPrerequisites\naction\npath and target\nignore\ninitial_sync\nExample 1\nExample 2\nUse watch\nReference\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995994,
    "timestamp": "2026-02-07T06:37:09.884Z",
    "title": "Secrets in Compose | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/use-secrets/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nSecrets in Compose\nManage secrets securely in Docker Compose\nCopy as Markdown\n\nA secret is any piece of data, such as a password, certificate, or API key, that shouldn‚Äôt be transmitted over a network or stored unencrypted in a Dockerfile or in your application‚Äôs source code.\n\nDocker Compose provides a way for you to use secrets without having to use environment variables to store information. If you‚Äôre injecting passwords and API keys as environment variables, you risk unintentional information exposure. Services can only access secrets when explicitly granted by a secrets attribute within the services top-level element.\n\nEnvironment variables are often available to all processes, and it can be difficult to track access. They can also be printed in logs when debugging errors without your knowledge. Using secrets mitigates these risks.\n\nUse secrets\n\nSecrets are mounted as a file in /run/secrets/<secret_name> inside the container.\n\nGetting a secret into a container is a two-step process. First, define the secret using the top-level secrets element in your Compose file. Next, update your service definitions to reference the secrets they require with the secrets attribute. Compose grants access to secrets on a per-service basis.\n\nUnlike the other methods, this permits granular access control within a service container via standard filesystem permissions.\n\nExamples\nSingle-service secret injection\n\nIn the following example, the frontend service is given access to the my_secret secret. In the container, /run/secrets/my_secret is set to the contents of the file ./my_secret.txt.\n\nservices:\n\n  myapp:\n\n    image: myapp:latest\n\n    secrets:\n\n      - my_secret\n\nsecrets:\n\n  my_secret:\n\n    file: ./my_secret.txt\nMulti-service secret sharing and password management\nservices:\n\n   db:\n\n     image: mysql:latest\n\n     volumes:\n\n       - db_data:/var/lib/mysql\n\n     environment:\n\n       MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password\n\n       MYSQL_DATABASE: wordpress\n\n       MYSQL_USER: wordpress\n\n       MYSQL_PASSWORD_FILE: /run/secrets/db_password\n\n     secrets:\n\n       - db_root_password\n\n       - db_password\n\n\n\n   wordpress:\n\n     depends_on:\n\n       - db\n\n     image: wordpress:latest\n\n     ports:\n\n       - \"8000:80\"\n\n     environment:\n\n       WORDPRESS_DB_HOST: db:3306\n\n       WORDPRESS_DB_USER: wordpress\n\n       WORDPRESS_DB_PASSWORD_FILE: /run/secrets/db_password\n\n     secrets:\n\n       - db_password\n\n\n\n\n\nsecrets:\n\n   db_password:\n\n     file: db_password.txt\n\n   db_root_password:\n\n     file: db_root_password.txt\n\n\n\nvolumes:\n\n    db_data:\n\nIn the advanced example above:\n\nThe secrets attribute under each service defines the secrets you want to inject into the specific container.\nThe top-level secrets section defines the variables db_password and db_root_password and provides the file that populates their values.\nThe deployment of each container means Docker creates a bind mount under /run/secrets/<secret_name> with their specific values.\nNote\n\nThe _FILE environment variables demonstrated here are a convention used by some images, including Docker Official Images like mysql and postgres.\n\nBuild secrets\n\nIn the following example, the npm_token secret is made available at build time. Its value is taken from the NPM_TOKEN environment variable.\n\nservices:\n\n  myapp:\n\n    build:\n\n      secrets:\n\n        - npm_token\n\n      context: .\n\n\n\nsecrets:\n\n  npm_token:\n\n    environment: NPM_TOKEN\nResources\nSecrets top-level element\nSecrets attribute for services top-level element\nBuild secrets\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUse secrets\nExamples\nSingle-service secret injection\nMulti-service secret sharing and password management\nBuild secrets\nResources\nSecrets\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258995997,
    "timestamp": "2026-02-07T06:37:09.884Z",
    "title": "Networking | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/networking/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nNetworking\nNetworking in Compose\nCopy as Markdown\n\nBy default Compose sets up a single network for your app. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by the service's name.\n\nNote\n\nYour app's network is given a name based on the \"project name\", which is based on the name of the directory it lives in. You can override the project name with either the --project-name flag or the COMPOSE_PROJECT_NAME environment variable.\n\nFor example, suppose your app is in a directory called myapp, and your compose.yaml looks like this:\n\nservices:\n\n  web:\n\n    build: .\n\n    ports:\n\n      - \"8000:8000\"\n\n  db:\n\n    image: postgres:18\n\n    ports:\n\n      - \"8001:5432\"\n\nWhen you run docker compose up, the following happens:\n\nA network called myapp_default is created.\nA container is created using web's configuration. It joins the network myapp_default under the name web.\nA container is created using db's configuration. It joins the network myapp_default under the name db.\n\nEach container can now look up the service name web or db and get back the appropriate container's IP address. For example, web's application code could connect to the URL postgres://db:5432 and start using the Postgres database.\n\nIt is important to note the distinction between HOST_PORT and CONTAINER_PORT. In the above example, for db, the HOST_PORT is 8001 and the container port is 5432 (postgres default). Networked service-to-service communication uses the CONTAINER_PORT. When HOST_PORT is defined, the service is accessible outside the swarm as well.\n\nWithin the web container, your connection string to db would look like postgres://db:5432, and from the host machine, the connection string would look like postgres://{DOCKER_IP}:8001 for example postgres://localhost:8001 if your container is running locally.\n\nUpdate containers on the network\n\nIf you make a configuration change to a service and run docker compose up to update it, the old container is removed and the new one joins the network under a different IP address but the same name. Running containers can look up that name and connect to the new address, but the old address stops working.\n\nIf any containers have connections open to the old container, they are closed. It is a container's responsibility to detect this condition, look up the name again and reconnect.\n\nTip\n\nReference containers by name, not IP, whenever possible. Otherwise you‚Äôll need to constantly update the IP address you use.\n\nLink containers\n\nLinks allow you to define extra aliases by which a service is reachable from another service. They are not required to enable services to communicate. By default, any service can reach any other service at that service's name. In the following example, db is reachable from web at the hostnames db and database:\n\nservices:\n\n  web:\n\n    build: .\n\n    links:\n\n      - \"db:database\"\n\n  db:\n\n    image: postgres:18\n\nSee the links reference for more information.\n\nMulti-host networking\n\nWhen deploying a Compose application on a Docker Engine with Swarm mode enabled, you can make use of the built-in overlay driver to enable multi-host communication.\n\nOverlay networks are always created as attachable. You can optionally set the attachable property to false.\n\nConsult the Swarm mode section to see how to set up a Swarm cluster, and the overlay network driver documentation to learn about multi-host overlay networks.\n\nSpecify custom networks\n\nInstead of just using the default app network, you can specify your own networks with the top-level networks key. This lets you create more complex topologies and specify custom network drivers and options. You can also use it to connect services to externally-created networks which aren't managed by Compose.\n\nEach service can specify what networks to connect to with the service-level networks key, which is a list of names referencing entries under the top-level networks key.\n\nThe following example shows a Compose file which defines two custom networks. The proxy service is isolated from the db service, because they do not share a network in common. Only app can talk to both.\n\nservices:\n\n  proxy:\n\n    build: ./proxy\n\n    networks:\n\n      - frontend\n\n  app:\n\n    build: ./app\n\n    networks:\n\n      - frontend\n\n      - backend\n\n  db:\n\n    image: postgres:18\n\n    networks:\n\n      - backend\n\n\n\nnetworks:\n\n  frontend:\n\n    # Specify driver options\n\n    driver: bridge\n\n    driver_opts:\n\n      com.docker.network.bridge.host_binding_ipv4: \"127.0.0.1\"\n\n  backend:\n\n    # Use a custom driver\n\n    driver: custom-driver\n\nNetworks can be configured with static IP addresses by setting the ipv4_address and/or ipv6_address for each attached network.\n\nNetworks can also be given a custom name:\n\nservices:\n\n  # ...\n\nnetworks:\n\n  frontend:\n\n    name: custom_frontend\n\n    driver: custom-driver-1\nConfigure the default network\n\nInstead of, or as well as, specifying your own networks, you can also change the settings of the app-wide default network by defining an entry under networks named default:\n\nservices:\n\n  web:\n\n    build: .\n\n    ports:\n\n      - \"8000:8000\"\n\n  db:\n\n    image: postgres:18\n\n\n\nnetworks:\n\n  default:\n\n    # Use a custom driver\n\n    driver: custom-driver-1\nUse an existing network\n\nIf you've manually created a bridge network outside of Compose using the docker network create command, you can connect your Compose services to it by marking the network as external.\n\nIf you want your containers to join a pre-existing network, use the external option\n\nservices:\n\n  # ...\n\nnetworks:\n\n  network1:\n\n    name: my-pre-existing-network\n\n    external: true\n\nInstead of attempting to create a network called [projectname]_default, Compose looks for a network called my-pre-existing-network and connects your app's containers to it.\n\nFurther reference information\n\nFor full details of the network configuration options available, see the following references:\n\nTop-level networks element\nService-level networks attribute\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUpdate containers on the network\nLink containers\nMulti-host networking\nSpecify custom networks\nConfigure the default network\nUse an existing network\nFurther reference information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996000,
    "timestamp": "2026-02-07T06:37:09.886Z",
    "title": "Use multiple Compose files | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/multiple-compose-files/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nMerge\nExtend\nInclude\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse multiple Compose files\nUse multiple Compose files\nCopy as Markdown\n\nThis section contains information on the ways you can work with multiple Compose files.\n\nUsing multiple Compose files lets you customize a Compose application for different environments or workflows. This is useful for large applications that may use dozens of containers, with ownership distributed across multiple teams. For example, if your organization or team uses a monorepo, each team may have their own ‚Äúlocal‚Äù Compose file to run a subset of the application. They then need to rely on other teams to provide a reference Compose file that defines the expected way to run their own subset. Complexity moves from the code in to the infrastructure and the configuration file.\n\nThe quickest way to work with multiple Compose files is to merge Compose files using the -f flag in the command line to list out your desired Compose files. However, merging rules means this can soon get quite complicated.\n\nDocker Compose provides two other options to manage this complexity when working with multiple Compose files. Depending on your project's needs, you can:\n\nExtend a Compose file by referring to another Compose file and selecting the bits you want to use in your own application, with the ability to override some attributes.\nInclude other Compose files directly in your Compose file.\n\nEdit this page\n\nRequest changes\n\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996003,
    "timestamp": "2026-02-07T06:37:09.896Z",
    "title": "Merge | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/multiple-compose-files/merge/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nMerge\nExtend\nInclude\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse multiple Compose files\n/\nMerge\nMerge Compose files\nCopy as Markdown\n\nDocker Compose lets you merge and override a set of Compose files together to create a composite Compose file.\n\nBy default, Compose reads two files, a compose.yaml and an optional compose.override.yaml file. By convention, the compose.yaml contains your base configuration. The override file can contain configuration overrides for existing services or entirely new services.\n\nIf a service is defined in both files, Compose merges the configurations using the rules described below and in the Compose Specification.\n\nHow to merge multiple Compose files\n\nTo use multiple override files, or an override file with a different name, you can either use the pre-defined COMPOSE_FILE environment variable, or use the -f option to specify the list of files.\n\nCompose merges files in the order they're specified on the command line. Subsequent files may merge, override, or add to their predecessors.\n\nFor example:\n\n$ docker compose -f compose.yaml -f compose.admin.yaml run backup_db\n\n\nThe compose.yaml file might specify a webapp service.\n\nwebapp:\n\n  image: examples/web\n\n  ports:\n\n    - \"8000:8000\"\n\n  volumes:\n\n    - \"/data\"\n\nThe compose.admin.yaml may also specify this same service:\n\nwebapp:\n\n  environment:\n\n    - DEBUG=1\n\nAny matching fields override the previous file. New values, add to the webapp service configuration:\n\nwebapp:\n\n  image: examples/web\n\n  ports:\n\n    - \"8000:8000\"\n\n  volumes:\n\n    - \"/data\"\n\n  environment:\n\n    - DEBUG=1\nMerging rules\n\nPaths are evaluated relative to the base file. When you use multiple Compose files, you must make sure all paths in the files are relative to the base Compose file (the first Compose file specified with -f). This is required because override files need not be valid Compose files. Override files can contain small fragments of configuration. Tracking which fragment of a service is relative to which path is difficult and confusing, so to keep paths easier to understand, all paths must be defined relative to the base file.\n\nTip\n\nYou can use docker compose config to review your merged configuration and avoid path-related issues.\n\nCompose copies configurations from the original service over to the local one. If a configuration option is defined in both the original service and the local service, the local value replaces or extends the original value.\n\nFor single-value options like image, command or mem_limit, the new value replaces the old value.\n\noriginal service:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    command: python app.py\n\nlocal service:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    command: python otherapp.py\n\nresult:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    command: python otherapp.py\n\nFor the multi-value options ports, expose, external_links, dns, dns_search, and tmpfs, Compose concatenates both sets of values:\n\noriginal service:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    expose:\n\n      - \"3000\"\n\nlocal service:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    expose:\n\n      - \"4000\"\n\n      - \"5000\"\n\nresult:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    expose:\n\n      - \"3000\"\n\n      - \"4000\"\n\n      - \"5000\"\n\nIn the case of environment, labels, volumes, and devices, Compose \"merges\" entries together with locally defined values taking precedence. For environment and labels, the environment variable or label name determines which value is used:\n\noriginal service:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    environment:\n\n      - FOO=original\n\n      - BAR=original\n\nlocal service:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    environment:\n\n      - BAR=local\n\n      - BAZ=local\n\nresult:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    environment:\n\n      - FOO=original\n\n      - BAR=local\n\n      - BAZ=local\n\nEntries for volumes and devices are merged using the mount path in the container:\n\noriginal service:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    volumes:\n\n      - ./original:/foo\n\n      - ./original:/bar\n\nlocal service:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    volumes:\n\n      - ./local:/bar\n\n      - ./local:/baz\n\nresult:\n\nservices:\n\n  myservice:\n\n    # ...\n\n    volumes:\n\n      - ./original:/foo\n\n      - ./local:/bar\n\n      - ./local:/baz\n\nFor more merging rules, see Merge and override in the Compose Specification.\n\nAdditional information\n\nUsing -f is optional. If not provided, Compose searches the working directory and its parent directories for a compose.yaml and a compose.override.yaml file. You must supply at least the compose.yaml file. If both files exist on the same directory level, Compose combines them into a single configuration.\n\nYou can use a -f with - (dash) as the filename to read the configuration from stdin. For example:\n\n$ docker compose -f - <<EOF\n\n  webapp:\n\n    image: examples/web\n\n    ports:\n\n     - \"8000:8000\"\n\n    volumes:\n\n     - \"/data\"\n\n    environment:\n\n     - DEBUG=1\n\n  EOF\n\n\nWhen stdin is used, all paths in the configuration are relative to the current working directory.\n\nYou can use the -f flag to specify a path to a Compose file that is not located in the current directory, either from the command line or by setting up a COMPOSE_FILE environment variable in your shell or in an environment file.\n\nFor example, if you are running the Compose Rails sample, and have a compose.yaml file in a directory called sandbox/rails. You can use a command like docker compose pull to get the postgres image for the db service from anywhere by using the -f flag as follows: docker compose -f ~/sandbox/rails/compose.yaml pull db\n\nHere's the full example:\n\n$ docker compose -f ~/sandbox/rails/compose.yaml pull db\n\nPulling db (postgres:18)...\n\n18: Pulling from library/postgres\n\nef0380f84d05: Pull complete\n\n50cf91dc1db8: Pull complete\n\nd3add4cd115c: Pull complete\n\n467830d8a616: Pull complete\n\n089b9db7dc57: Pull complete\n\n6fba0a36935c: Pull complete\n\n81ef0e73c953: Pull complete\n\n338a6c4894dc: Pull complete\n\n15853f32f67c: Pull complete\n\n044c83d92898: Pull complete\n\n17301519f133: Pull complete\n\ndcca70822752: Pull complete\n\ncecf11b8ccf3: Pull complete\n\nDigest: sha256:1364924c753d5ff7e2260cd34dc4ba05ebd40ee8193391220be0f9901d4e1651\n\nStatus: Downloaded newer image for postgres:18\n\nExample\n\nA common use case for multiple files is changing a development Compose app for a production-like environment (which may be production, staging or CI). To support these differences, you can split your Compose configuration into a few different files:\n\nStart with a base file that defines the canonical configuration for the services.\n\ncompose.yaml\n\nservices:\n\n  web:\n\n    image: example/my_web_app:latest\n\n    depends_on:\n\n      - db\n\n      - cache\n\n\n\n  db:\n\n    image: postgres:18\n\n\n\n  cache:\n\n    image: redis:latest\n\nIn this example the development configuration exposes some ports to the host, mounts our code as a volume, and builds the web image.\n\ncompose.override.yaml\n\nservices:\n\n  web:\n\n    build: .\n\n    volumes:\n\n      - '.:/code'\n\n    ports:\n\n      - 8883:80\n\n    environment:\n\n      DEBUG: 'true'\n\n\n\n  db:\n\n    command: '-d'\n\n    ports:\n\n     - 5432:5432\n\n\n\n  cache:\n\n    ports:\n\n      - 6379:6379\n\nWhen you run docker compose up it reads the overrides automatically.\n\nTo use this Compose app in a production environment, another override file is created, which might be stored in a different git repository or managed by a different team.\n\ncompose.prod.yaml\n\nservices:\n\n  web:\n\n    ports:\n\n      - 80:80\n\n    environment:\n\n      PRODUCTION: 'true'\n\n\n\n  cache:\n\n    environment:\n\n      TTL: '500'\n\nTo deploy with this production Compose file you can run\n\n$ docker compose -f compose.yaml -f compose.prod.yaml up -d\n\n\nThis deploys all three services using the configuration in compose.yaml and compose.prod.yaml but not the dev configuration in compose.override.yaml.\n\nFor more information, see Using Compose in production.\n\nLimitations\n\nDocker Compose supports relative paths for the many resources to be included in the application model: build context for service images, location of file defining environment variables, path to a local directory used in a bind-mounted volume. With such a constraint, code organization in a monorepo can become hard as a natural choice would be to have dedicated folders per team or component, but then the Compose files relative paths become irrelevant.\n\nReference information\nMerge rules\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow to merge multiple Compose files\nMerging rules\nAdditional information\nExample\nLimitations\nReference information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996006,
    "timestamp": "2026-02-07T06:37:09.896Z",
    "title": "Extend | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/multiple-compose-files/extends/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nMerge\nExtend\nInclude\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse multiple Compose files\n/\nExtend\nExtend your Compose file\nCopy as Markdown\n\nDocker Compose's extends attribute lets you share common configurations among different files, or even different projects entirely.\n\nExtending services is useful if you have several services that reuse a common set of configuration options. With extends you can define a common set of service options in one place and refer to it from anywhere. You can refer to another Compose file and select a service you want to also use in your own application, with the ability to override some attributes for your own needs.\n\nImportant\n\nWhen you use multiple Compose files, you must make sure all paths in the files are relative to the base Compose file (i.e. the Compose file in your main-project folder). This is required because extend files need not be valid Compose files. Extend files can contain small fragments of configuration. Tracking which fragment of a service is relative to which path is difficult and confusing, so to keep paths easier to understand, all paths must be defined relative to the base file.\n\nHow the extends attribute works\nExtending services from another file\n\nTake the following example:\n\nservices:\n\n  web:\n\n    extends:\n\n      file: common-services.yml\n\n      service: webapp\n\nThis instructs Compose to re-use only the properties of the webapp service defined in the common-services.yml file. The webapp service itself is not part of the final project.\n\nIf common-services.yml looks like this:\n\nservices:\n\n  webapp:\n\n    build: .\n\n    ports:\n\n      - \"8000:8000\"\n\n    volumes:\n\n      - \"/data\"\n\nYou get exactly the same result as if you wrote compose.yaml with the same build, ports, and volumes configuration values defined directly under web.\n\nTo include the service webapp in the final project when extending services from another file, you need to explicitly include both services in your current Compose file. For example (this is for illustrative purposes only):\n\nservices:\n\n  web:\n\n    build: alpine\n\n    command: echo\n\n    extends:\n\n      file: common-services.yml\n\n      service: webapp\n\n  webapp:\n\n    extends:\n\n      file: common-services.yml\n\n      service: webapp\n\nAlternatively, you can use include.\n\nExtending services within the same file\n\nIf you define services in the same Compose file and extend one service from another, both the original service and the extended service will be part of your final configuration. For example:\n\nservices:\n\n  web:\n\n    build: alpine\n\n    extends: webapp\n\n  webapp:\n\n    environment:\n\n      - DEBUG=1\nExtending services within the same file and from another file\n\nYou can go further and define, or re-define, configuration locally in compose.yaml:\n\nservices:\n\n  web:\n\n    extends:\n\n      file: common-services.yml\n\n      service: webapp\n\n    environment:\n\n      - DEBUG=1\n\n    cpu_shares: 5\n\n\n\n  important_web:\n\n    extends: web\n\n    cpu_shares: 10\nAdditional example\n\nExtending an individual service is useful when you have multiple services that have a common configuration. The example below is a Compose app with two services, a web application and a queue worker. Both services use the same codebase and share many configuration options.\n\nThe common.yaml file defines the common configuration:\n\nservices:\n\n  app:\n\n    build: .\n\n    environment:\n\n      CONFIG_FILE_PATH: /code/config\n\n      API_KEY: xxxyyy\n\n    cpu_shares: 5\n\nThe compose.yaml defines the concrete services which use the common configuration:\n\nservices:\n\n  webapp:\n\n    extends:\n\n      file: common.yaml\n\n      service: app\n\n    command: /code/run_web_app\n\n    ports:\n\n      - 8080:8080\n\n    depends_on:\n\n      - queue\n\n      - db\n\n\n\n  queue_worker:\n\n    extends:\n\n      file: common.yaml\n\n      service: app\n\n    command: /code/run_worker\n\n    depends_on:\n\n      - queue\nRelative paths\n\nWhen using extends with a file attribute which points to another folder, relative paths declared by the service being extended are converted so they still point to the same file when used by the extending service. This is illustrated in the following example:\n\nBase Compose file:\n\nservices:\n\n  webapp:\n\n    image: example\n\n    extends:\n\n      file: ../commons/compose.yaml\n\n      service: base\n\nThe commons/compose.yaml file:\n\nservices:\n\n  base:\n\n    env_file: ./container.env\n\nThe resulting service refers to the original container.env file within the commons directory. This can be confirmed with docker compose config which inspects the actual model:\n\nservices:\n\n  webapp:\n\n    image: example\n\n    env_file: \n\n      - ../commons/container.env\nReference information\nextends\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow the extends attribute works\nExtending services from another file\nExtending services within the same file\nExtending services within the same file and from another file\nAdditional example\nRelative paths\nReference information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996007,
    "timestamp": "2026-02-07T06:37:09.898Z",
    "title": "Include | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/multiple-compose-files/include/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nMerge\nExtend\nInclude\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse multiple Compose files\n/\nInclude\nInclude\nCopy as Markdown\nRequires:\nDocker Compose 2.20.3 and later\n\nWith include, you can incorporate a separate compose.yaml file directly in your current compose.yaml file. This makes it easy to modularize complex applications into sub-Compose files, which in turn enables application configurations to be made simpler and more explicit.\n\nThe include top-level element helps to reflect the engineering team responsible for the code directly in the config file's organization. It also solves the relative path problem that extends and merge present.\n\nEach path listed in the include section loads as an individual Compose application model, with its own project directory, in order to resolve relative paths.\n\nOnce the included Compose application loads, all resources are copied into the current Compose application model.\n\nNote\n\ninclude applies recursively so an included Compose file which declares its own include section, causes those files to also be included.\n\nExample\ninclude:\n\n  - my-compose-include.yaml  #with serviceB declared\n\nservices:\n\n  serviceA:\n\n    build: .\n\n    depends_on:\n\n      - serviceB #use serviceB directly as if it was declared in this Compose file\n\nmy-compose-include.yaml manages serviceB which details some replicas, web UI to inspect data, isolated networks, volumes for data persistence, etc. The application relying on serviceB doesn‚Äôt need to know about the infrastructure details, and consumes the Compose file as a building block it can rely on.\n\nThis means the team managing serviceB can refactor its own database component to introduce additional services without impacting any dependent teams. It also means that the dependent teams don't need to include additional flags on each Compose command they run.\n\ninclude:\n\n  - oci://docker.io/username/my-compose-app:latest # use a Compose file stored as an OCI artifact\n\nservices:\n\n  serviceA:\n\n    build: .\n\n    depends_on:\n\n      - serviceB \n\ninclude allows you to reference Compose files from remote sources, such as OCI artifacts or Git repositories.\nHere serviceB is defined in a Compose file stored on Docker Hub.\n\nUsing overrides with included Compose files\n\nCompose reports an error if any resource from include conflicts with resources from the included Compose file. This rule prevents unexpected conflicts with resources defined by the included compose file author. However, there may be some circumstances where you might want to customize the included model. This can be achieved by adding an override file to the include directive:\n\ninclude:\n\n  - path : \n\n      - third-party/compose.yaml\n\n      - override.yaml  # local override for third-party model\n\nThe main limitation with this approach is that you need to maintain a dedicated override file per include. For complex projects with multiple includes this would result in many Compose files.\n\nThe other option is to use a compose.override.yaml file. While conflicts will be rejected from the file using include when same resource is declared, a global Compose override file can override the resulting merged model, as demonstrated in following example:\n\nMain compose.yaml file:\n\ninclude:\n\n  - team-1/compose.yaml # declare service-1\n\n  - team-2/compose.yaml # declare service-2\n\nOverride compose.override.yaml file:\n\nservices:\n\n  service-1:\n\n    # override included service-1 to enable debugger port\n\n    ports:\n\n      - 2345:2345\n\n\n\n  service-2:\n\n    # override included service-2 to use local data folder containing test data\n\n    volumes:\n\n      - ./data:/data\n\nCombined together, this allows you to benefit from third-party reusable components, and adjust the Compose model for your needs.\n\nReference information\n\ninclude top-level element\n\nEdit this page\n\nRequest changes\n\nTable of contents\nExample\nUsing overrides with included Compose files\nReference information\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996015,
    "timestamp": "2026-02-07T06:37:09.906Z",
    "title": "Use Compose in production | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/production/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse Compose in production\nUse Compose in production\nCopy as Markdown\n\nWhen you define your app with Compose in development, you can use this definition to run your application in different environments such as CI, staging, and production.\n\nThe easiest way to deploy an application is to run it on a single server, similar to how you would run your development environment. If you want to scale up your application, you can run Compose apps on a Swarm cluster.\n\nModify your Compose file for production\n\nYou may need to make changes to your app configuration to make it ready for production. These changes might include:\n\nRemoving any volume bindings for application code, so that code stays inside the container and can't be changed from outside\nBinding to different ports on the host\nSetting environment variables differently, such as reducing the verbosity of logging, or to specify settings for external services such as an email server\nSpecifying a restart policy like restart: alwaysto avoid downtime\nAdding extra services such as a log aggregator\n\nFor this reason, consider defining an additional Compose file, for example compose.production.yaml, with production-specific configuration details. This configuration file only needs to include the changes you want to make from the original Compose file. The additional Compose file is then applied over the original compose.yaml to create a new configuration.\n\nOnce you have a second configuration file, you can use it with the -f option:\n\n$ docker compose -f compose.yaml -f compose.production.yaml up -d\n\n\nSee Using multiple compose files for a more complete example, and other options.\n\nDeploying changes\n\nWhen you make changes to your app code, remember to rebuild your image and recreate your app's containers. To redeploy a service called web, use:\n\n$ docker compose build web\n\n$ docker compose up --no-deps -d web\n\n\nThis first command rebuilds the image for web and then stops, destroys, and recreates just the web service. The --no-deps flag prevents Compose from also recreating any services that web depends on.\n\nRunning Compose on a single server\n\nYou can use Compose to deploy an app to a remote Docker host by setting the DOCKER_HOST, DOCKER_TLS_VERIFY, and DOCKER_CERT_PATH environment variables appropriately. For more information, see pre-defined environment variables.\n\nOnce you've set up your environment variables, all the normal docker compose commands work with no further configuration.\n\nNext steps\nUsing multiple Compose files\n\nEdit this page\n\nRequest changes\n\nTable of contents\nModify your Compose file for production\nDeploying changes\nRunning Compose on a single server\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996018,
    "timestamp": "2026-02-07T06:37:09.906Z",
    "title": "OCI artifact applications | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/oci-artifact/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nOCI artifact applications\nPackage and deploy Docker Compose applications as OCI artifacts\nCopy as Markdown\nRequires:\nDocker Compose 2.34.0 and later\n\nDocker Compose supports working with OCI artifacts, allowing you to package and distribute your Compose applications through container registries. This means you can store your Compose files alongside your container images, making it easier to version, share, and deploy your multi-container applications.\n\nPublish your Compose application as an OCI artifact\n\nTo distribute your Compose application as an OCI artifact, you can use the docker compose publish command, to publish it to an OCI-compliant registry. This allows others to then deploy your application directly from the registry.\n\nThe publish function supports most of the composition capabilities of Compose, like overrides, extends or include, with some limitations.\n\nGeneral steps\n\nNavigate to your Compose application's directory.\nEnsure you're in the directory containing your compose.yml file or that you are specifying your Compose file with the -f flag.\n\nIn your terminal, sign in to your Docker account so you're authenticated with Docker Hub.\n\n$ docker login\n\n\nUse the docker compose publish command to push your application as an OCI artifact:\n\n$ docker compose publish username/my-compose-app:latest\n\n\nIf you have multiple Compose files, run:\n\n$ docker compose -f compose-base.yml -f compose-production.yml publish username/my-compose-app:latest\n\nAdvanced publishing options\n\nWhen publishing, you can pass additional options:\n\n--oci-version: Specify the OCI version (default is automatically determined).\n--resolve-image-digests: Pin image tags to digests.\n--with-env: Include environment variables in the published OCI artifact.\n\nCompose checks to make sure there isn't any sensitive data in your configuration and displays your environment variables to confirm you want to publish them.\n\n...\n\nyou are about to publish sensitive data within your OCI artifact.\n\nplease double check that you are not leaking sensitive data\n\nAWS Client ID\n\n\"services.serviceA.environment.AWS_ACCESS_KEY_ID\": xxxxxxxxxx\n\nAWS Secret Key\n\n\"services.serviceA.environment.AWS_SECRET_ACCESS_KEY\": aws\"xxxx/xxxx+xxxx+\"\n\nGithub authentication\n\n\"GITHUB_TOKEN\": ghp_xxxxxxxxxx\n\nJSON Web Token\n\n\"\": xxxxxxx.xxxxxxxx.xxxxxxxx\n\nPrivate Key\n\n\"\": -----BEGIN DSA PRIVATE KEY-----\n\nxxxxx\n\n-----END DSA PRIVATE KEY-----\n\nAre you ok to publish these sensitive data? [y/N]:y\n\n\n\nyou are about to publish environment variables within your OCI artifact.\n\nplease double check that you are not leaking sensitive data\n\nService/Config  serviceA\n\nFOO=bar\n\nService/Config  serviceB\n\nFOO=bar\n\nQUIX=\n\nBAR=baz\n\nAre you ok to publish these environment variables? [y/N]: \n\nIf you decline, the publish process stops without sending anything to the registry.\n\nLimitations\n\nThere are limitations to publishing Compose applications as OCI artifacts. You can't publish a Compose configuration:\n\nWith service(s) containing bind mounts\nWith service(s) containing only a build section\nThat includes local files with the include attribute. To publish successfully, ensure that any included local files are also published. You can then use include to reference these files as remote include is supported.\nStart an OCI artifact application\n\nTo start a Docker Compose application that uses an OCI artifact, you can use the -f (or --file) flag followed by the OCI artifact reference. This allows you to specify a Compose file stored as an OCI artifact in a registry.\n\nThe oci:// prefix indicates that the Compose file should be pulled from an OCI-compliant registry rather than loaded from the local filesystem.\n\n$ docker compose -f oci://docker.io/username/my-compose-app:latest up\n\n\nTo then run the Compose application, use the docker compose up command with the -f flag pointing to your OCI artifact:\n\n$ docker compose -f oci://docker.io/username/my-compose-app:latest up\n\nTroubleshooting\n\nWhen you run an application from an OCI artifact, Compose may display warning messages that require you to confirm the following so as to limit the risk of running a malicious application:\n\nA list of the interpolation variables used along with their values\nA list of all environment variables used by the application\nIf your OCI artifact application is using another remote resources, for example via include.\n$ REGISTRY=myregistry.com docker compose -f oci://docker.io/username/my-compose-app:latest up\n\n\n\nFound the following variables in configuration:\n\nVARIABLE     VALUE                SOURCE        REQUIRED    DEFAULT\n\nREGISTRY     myregistry.com      command-line   yes         \n\nTAG          v1.0                environment    no          latest\n\nDOCKERFILE   Dockerfile          default        no          Dockerfile\n\nAPI_KEY      <unset>             none           no          \n\n\n\nDo you want to proceed with these variables? [Y/n]:y\n\n\n\nWarning: This Compose project includes files from remote sources:\n\n- oci://registry.example.com/stack:latest\n\nRemote includes could potentially be malicious. Make sure you trust the source.\n\nDo you want to continue? [y/N]: \n\nIf you agree to start the application, Compose displays the directory where all the resources from the OCI artifact have been downloaded:\n\n...\n\nDo you want to continue? [y/N]: y\n\n\n\nYour compose stack \"oci://registry.example.com/stack:latest\" is stored in \"~/Library/Caches/docker-compose/964e715660d6f6c3b384e05e7338613795f7dcd3613890cfa57e3540353b9d6d\"\n\nThe docker compose publish command supports non-interactive execution, letting you skip the confirmation prompt by including the -y (or --yes) flag:\n\n$ docker compose publish -y username/my-compose-app:latest\n\nNext steps\nLearn about OCI artifacts in Docker Hub\nCompose publish command\nUnderstand include\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPublish your Compose application as an OCI artifact\nGeneral steps\nAdvanced publishing options\nLimitations\nStart an OCI artifact application\nTroubleshooting\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996012,
    "timestamp": "2026-02-07T06:37:09.908Z",
    "title": "Enable GPU support | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/gpu-support/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nEnable GPU support\nRun Docker Compose services with GPU access\nCopy as Markdown\n\nCompose services can define GPU device reservations if the Docker host contains such devices and the Docker Daemon is set accordingly. For this, make sure you install the prerequisites if you haven't already done so.\n\nThe examples in the following sections focus specifically on providing service containers access to GPU devices with Docker Compose.\n\nEnabling GPU access to service containers\n\nGPUs are referenced in a compose.yaml file using the device attribute from the Compose Deploy specification, within your services that need them.\n\nThis provides more granular control over a GPU reservation as custom values can be set for the following device properties:\n\ncapabilities. This value is specified as a list of strings. For example, capabilities: [gpu]. You must set this field in the Compose file. Otherwise, it returns an error on service deployment.\ncount. Specified as an integer or the value all, represents the number of GPU devices that should be reserved (providing the host holds that number of GPUs). If count is set to all or not specified, all GPUs available on the host are used by default.\ndevice_ids. This value, specified as a list of strings, represents GPU device IDs from the host. You can find the device ID in the output of nvidia-smi on the host. If no device_ids are set, all GPUs available on the host are used by default.\ndriver. Specified as a string, for example driver: 'nvidia'\noptions. Key-value pairs representing driver specific options.\nImportant\n\nYou must set the capabilities field. Otherwise, it returns an error on service deployment.\n\nNote\n\ncount and device_ids are mutually exclusive. You must only define one field at a time.\n\nFor more information on these properties, see the Compose Deploy Specification.\n\nExample of a Compose file for running a service with access to 1 GPU device\nservices:\n\n  test:\n\n    image: nvidia/cuda:12.9.0-base-ubuntu22.04\n\n    command: nvidia-smi\n\n    deploy:\n\n      resources:\n\n        reservations:\n\n          devices:\n\n            - driver: nvidia\n\n              count: 1\n\n              capabilities: [gpu]\n\nRun with Docker Compose:\n\n$ docker compose up\n\nCreating network \"gpu_default\" with the default driver\n\nCreating gpu_test_1 ... done\n\nAttaching to gpu_test_1    \n\ntest_1  | +-----------------------------------------------------------------------------+\n\ntest_1  | | NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.1     |\n\ntest_1  | |-------------------------------+----------------------+----------------------+\n\ntest_1  | | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n\ntest_1  | | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n\ntest_1  | |                               |                      |               MIG M. |\n\ntest_1  | |===============================+======================+======================|\n\ntest_1  | |   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n\ntest_1  | | N/A   23C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n\ntest_1  | |                               |                      |                  N/A |\n\ntest_1  | +-------------------------------+----------------------+----------------------+\n\ntest_1  |                                                                                \n\ntest_1  | +-----------------------------------------------------------------------------+\n\ntest_1  | | Processes:                                                                  |\n\ntest_1  | |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n\ntest_1  | |        ID   ID                                                   Usage      |\n\ntest_1  | |=============================================================================|\n\ntest_1  | |  No running processes found                                                 |\n\ntest_1  | +-----------------------------------------------------------------------------+\n\ngpu_test_1 exited with code 0\n\n\nOn machines hosting multiple GPUs, the device_ids field can be set to target specific GPU devices and count can be used to limit the number of GPU devices assigned to a service container.\n\nYou can use count or device_ids in each of your service definitions. An error is returned if you try to combine both, specify an invalid device ID, or use a value of count that‚Äôs higher than the number of GPUs in your system.\n\n$ nvidia-smi   \n\n+-----------------------------------------------------------------------------+\n\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n\n|-------------------------------+----------------------+----------------------+\n\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n\n|                               |                      |               MIG M. |\n\n|===============================+======================+======================|\n\n|   0  Tesla T4            On   | 00000000:00:1B.0 Off |                    0 |\n\n| N/A   72C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n\n|                               |                      |                  N/A |\n\n+-------------------------------+----------------------+----------------------+\n\n|   1  Tesla T4            On   | 00000000:00:1C.0 Off |                    0 |\n\n| N/A   67C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n\n|                               |                      |                  N/A |\n\n+-------------------------------+----------------------+----------------------+\n\n|   2  Tesla T4            On   | 00000000:00:1D.0 Off |                    0 |\n\n| N/A   74C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n\n|                               |                      |                  N/A |\n\n+-------------------------------+----------------------+----------------------+\n\n|   3  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n\n| N/A   62C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n\n|                               |                      |                  N/A |\n\n+-------------------------------+----------------------+----------------------+\n\nAccess specific devices\n\nTo allow access only to GPU-0 and GPU-3 devices:\n\nservices:\n\n  test:\n\n    image: tensorflow/tensorflow:latest-gpu\n\n    command: python -c \"import tensorflow as tf;tf.test.gpu_device_name()\"\n\n    deploy:\n\n      resources:\n\n        reservations:\n\n          devices:\n\n          - driver: nvidia\n\n            device_ids: ['0', '3']\n\n            capabilities: [gpu]\n\nEdit this page\n\nRequest changes\n\nTable of contents\nEnabling GPU access to service containers\nExample of a Compose file for running a service with access to 1 GPU device\nAccess specific devices\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996024,
    "timestamp": "2026-02-07T06:37:09.928Z",
    "title": "Compose Bridge | Docker Docs",
    "url": "https://docs.docker.com/compose/bridge/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nUsage\nCustomize\nUse Model Runner\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nCompose Bridge\nOverview of Compose Bridge\nCopy as Markdown\nRequires:\nDocker Desktop 4.43.0 and later\n\nCompose Bridge converts your Docker Compose configuration into platform-specific deployment formats such as Kubernetes manifests. By default, it generates:\n\nKubernetes manifests\nA Kustomize overlay\n\nThese outputs are ready for deployment on Docker Desktop with Kubernetes enabled.\n\nCompose Bridge helps you bridge the gap between Compose and Kubernetes, making it easier to adopt Kubernetes while keeping the simplicity and efficiency of Compose.\n\nIt's a flexible tool that lets you either take advantage of the default transformation or create a custom transformation to suit specific project needs and requirements.\n\nHow it works\n\nCompose Bridge uses transformations to convert a Compose model into another form.\n\nA transformation is packaged as a Docker image that receives the fully resolved Compose model as /in/compose.yaml and can produce any target format file under /out.\n\nCompose Bridge provides its own transformation for Kubernetes using Go templates, so that it is easy to extend for customization by replacing or appending your own templates.\n\nFor more detailed information on how these transformations work and how you can customize them for your projects, see Customize.\n\nCompose Bridge also supports applications that use LLMs via Docker Model Runner.\n\nFor more details, see Use Model Runner.\n\nWhat's next?\nUse Compose Bridge\nExplore how you can customize Compose Bridge\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow it works\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996027,
    "timestamp": "2026-02-07T06:37:09.929Z",
    "title": "Usage | Docker Docs",
    "url": "https://docs.docker.com/compose/bridge/usage/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nUsage\nCustomize\nUse Model Runner\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nCompose Bridge\n/\nUsage\nUse the default Compose Bridge transformation\nCopy as Markdown\nRequires:\nDocker Desktop 4.43.0 and later\n\nCompose Bridge includes a built-in transformation that automatically converts your Compose configuration into a set of Kubernetes manifests.\n\nBased on your compose.yaml file, it produces:\n\nA Namespace so all your resources are isolated and don't conflict with resources from other deployments.\nA ConfigMap with an entry for each and every config resource in your Compose application.\nDeployments for application services. This ensures that the specified number of instances of your application are maintained in the Kubernetes cluster.\nServices for ports exposed by your services, used for service-to-service communication.\nServices for ports published by your services, with type LoadBalancer so that Docker Desktop will also expose the same port on the host.\nNetwork policies to replicate the networking topology defined in your compose.yaml file.\nPersistentVolumeClaims for your volumes, using hostpath storage class so that Docker Desktop manages volume creation.\nSecrets with your secret encoded. This is designed for local use in a testing environment.\n\nIt also supplies a Kustomize overlay dedicated to Docker Desktop with:\n\nLoadbalancer for services which need to expose ports on host.\nA PersistentVolumeClaim to use the Docker Desktop storage provisioner desktop-storage-provisioner to handle volume provisioning more effectively.\nA Kustomization.yaml file to link all the resources together.\n\nIf your Compose file defines a models section for a service, Compose Bridge automatically configures your deployment so your service can locate and use its models via Docker Model Runner.\n\nFor each declared model, the transformation injects two environment variables:\n\n<MODELNAME>_URL: The endpoint for Docker Model Runner serving that model\n<MODELNAME>_MODEL: The model‚Äôs name or identifier\n\nYou can optionally customize these variable names using endpoint_var and model_var.\n\nThe default transformation generates two different overlays - one for Docker Desktop whilst using a local instance of Docker Model Runner, and a model-runner overlay with all the relevant Kubernetes resources to deploy Docker Model Runner in a pod.\n\nEnvironment\tEndpoint\nDocker Desktop\thttp://host.docker.internal:12434/engines/v1/\nKubernetes\thttp://model-runner/engines/v1/\n\nFor more details, see Use Model Runner.\n\nUse the default Compose Bridge transformation\n\nTo convert your Compose file using the default transformation:\n\n$ docker compose bridge convert\n\n\nCompose looks for a compose.yaml file inside the current directory and generates Kubernetes manifests.\n\nExample output:\n\n$ docker compose -f compose.yaml bridge convert\n\nKubernetes resource backend-deployment.yaml created\n\nKubernetes resource frontend-deployment.yaml created\n\nKubernetes resource backend-expose.yaml created\n\nKubernetes resource frontend-expose.yaml created\n\nKubernetes resource 0-my-project-namespace.yaml created\n\nKubernetes resource default-network-policy.yaml created\n\nKubernetes resource backend-service.yaml created\n\nKubernetes resource frontend-service.yaml created\n\nKubernetes resource kustomization.yaml created\n\nKubernetes resource backend-deployment.yaml created\n\nKubernetes resource frontend-deployment.yaml created\n\nKubernetes resource backend-service.yaml created\n\nKubernetes resource frontend-service.yaml created\n\nKubernetes resource kustomization.yaml created\n\nKubernetes resource model-runner-configmap.yaml created\n\nKubernetes resource model-runner-deployment.yaml created\n\nKubernetes resource model-runner-service.yaml created\n\nKubernetes resource model-runner-volume-claim.yaml created\n\nKubernetes resource kustomization.yaml created\n\n\nAll generated files are stored in the /out directory in your project.\n\nDeploy the generated manifests\nImportant\n\nBefore you deploy your Compose Bridge transformations, make sure you have enabled Kubernetes in Docker Desktop.\n\nOnce the manifests are generated, deploy them to your local Kubernetes cluster:\n\n$ kubectl apply -k out/overlays/desktop/\n\nTip\n\nYou can convert and deploy your Compose project to a Kubernetes cluster from the Compose file viewer.\n\nMake sure you are signed in to your Docker account, navigate to your container in the Containers view, and in the top-right corner select View configurations and then Convert and Deploy to Kubernetes.\n\nAdditional commands\n\nConvert a compose.yaml file located in another directory:\n\n$ docker compose -f <path-to-file>/compose.yaml bridge convert\n\n\nTo see all available flags, run:\n\n$ docker compose bridge convert --help\n\nWhat's next?\nExplore how you can customize Compose Bridge\nUse Model Runner.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nUse the default Compose Bridge transformation\nDeploy the generated manifests\nAdditional commands\nWhat's next?\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996019,
    "timestamp": "2026-02-07T06:37:09.929Z",
    "title": "Use provider services | Docker Docs",
    "url": "https://docs.docker.com/compose/how-tos/provider-services/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nSpecify a project name\nUse lifecycle hooks\nUse service profiles\nControl startup order\nUse environment variables\nBuild dependent images\nUse Compose Watch\nSecrets in Compose\nNetworking\nUse multiple Compose files\nEnable GPU support\nUse Compose in production\nOCI artifact applications\nUse provider services\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nHow-tos\n/\nUse provider services\nUse provider services\nCopy as Markdown\nRequires:\nDocker Compose 2.36.0 and later\n\nDocker Compose supports provider services, which allow integration with services whose lifecycles are managed by third-party components rather than by Compose itself.\nThis feature enables you to define and utilize platform-specific services without the need for manual setup or direct lifecycle management.\n\nWhat are provider services?\n\nProvider services are a special type of service in Compose that represents platform capabilities rather than containers. They allow you to declare dependencies on specific platform features that your application needs.\n\nWhen you define a provider service in your Compose file, Compose works with the platform to provision and configure the requested capability, making it available to your application services.\n\nUsing provider services\n\nTo use a provider service in your Compose file, you need to:\n\nDefine a service with the provider attribute\nSpecify the type of provider you want to use\nConfigure any provider-specific options\nDeclare dependencies from your application services to the provider service\n\nHere's a basic example:\n\nservices:\n\n  database:\n\n    provider:\n\n      type: awesomecloud\n\n      options:\n\n        type: mysql\n\n        foo: bar  \n\n  app:\n\n    image: myapp \n\n    depends_on:\n\n       - database\n\nNotice the dedicated provider attribute in the database service. This attribute specifies that the service is managed by a provider and lets you define options specific to that provider type.\n\nThe depends_on attribute in the app service specifies that it depends on the database service. This means that the database service will be started before the app service, allowing the provider information to be injected into the app service.\n\nHow it works\n\nDuring the docker compose up command execution, Compose identifies services relying on providers and works with them to provision the requested capabilities. The provider then populates Compose model with information about how to access the provisioned resource.\n\nThis information is passed to services that declare a dependency on the provider service, typically through environment variables. The naming convention for these variables is:\n\n<PROVIDER_SERVICE_NAME>_<VARIABLE_NAME>\n\nFor example, if your provider service is named database, your application service might receive environment variables like:\n\nDATABASE_URL with the URL to access the provisioned resource\nDATABASE_TOKEN with an authentication token\nOther provider-specific variables\n\nYour application can then use these environment variables to interact with the provisioned resource.\n\nProvider types\n\nThe type field in a provider service references the name of either:\n\nA Docker CLI plugin (e.g., docker-model)\nA binary available in the user's PATH\nA path to the binary or script to execute\n\nWhen Compose encounters a provider service, it looks for a plugin or binary with the specified name to handle the provisioning of the requested capability.\n\nFor example, if you specify type: model, Compose will look for a Docker CLI plugin named docker-model or a binary named model in the PATH.\n\nservices:\n\n  ai-runner:\n\n    provider:\n\n      type: model  # Looks for docker-model plugin or model binary\n\n      options:\n\n        model: ai/example-model\n\nThe plugin or binary is responsible for:\n\nInterpreting the options provided in the provider service\nProvisioning the requested capability\nReturning information about how to access the provisioned resource\n\nThis information is then passed to dependent services as environment variables.\n\nTip\n\nIf you're working with AI models in Compose, use the models top-level element instead.\n\nBenefits of using provider services\n\nUsing provider services in your Compose applications offers several benefits:\n\nSimplified configuration: You don't need to manually configure and manage platform capabilities\nDeclarative approach: You can declare all your application's dependencies in one place\nConsistent workflow: You use the same Compose commands to manage your entire application, including platform capabilities\nCreating your own provider\n\nIf you want to create your own provider to extend Compose with custom capabilities, you can implement a Compose plugin that registers provider types.\n\nFor detailed information on how to create and implement your own provider, refer to the Compose Extensions documentation.\nThis guide explains the extension mechanism that allows you to add new provider types to Compose.\n\nReference\nDocker Model Runner documentation\nCompose Extensions documentation\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat are provider services?\nUsing provider services\nHow it works\nProvider types\nBenefits of using provider services\nCreating your own provider\nReference\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996033,
    "timestamp": "2026-02-07T06:37:09.943Z",
    "title": "Use Model Runner | Docker Docs",
    "url": "https://docs.docker.com/compose/bridge/use-model-runner/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nUsage\nCustomize\nUse Model Runner\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nCompose Bridge\n/\nUse Model Runner\nUse Docker Model Runner with Compose Bridge\nCopy as Markdown\n\nCompose Bridge supports model-aware deployments. It can deploy and configure Docker Model Runner, a lightweight service that hosts and serves machine LLMs.\n\nThis reduces manual setup for LLM-enabled services and keeps deployments consistent across Docker Desktop and Kubernetes environments.\n\nIf you have a models top-level element in your compose.yaml file, Compose Bridge:\n\nAutomatically injects environment variables for each model‚Äôs endpoint and name.\nConfigures model endpoints differently for Docker Desktop vs Kubernetes.\nOptionally deploys Docker Model Runner in Kubernetes when enabled in Helm values\nConfigure model runner settings\n\nWhen deploying using generated Helm Charts, you can control the model runner configuration through Helm values.\n\n# Model Runner settings\n\nmodelRunner:\n\n    # Set to false for Docker Desktop (uses host instance)\n\n    # Set to true for standalone Kubernetes clusters\n\n    enabled: false\n\n    # Endpoint used when enabled=false (Docker Desktop)\n\n    hostEndpoint: \"http://host.docker.internal:12434/engines/v1/\"\n\n    # Deployment settings when enabled=true\n\n    image: \"docker/model-runner:latest\"\n\n    imagePullPolicy: \"IfNotPresent\"\n\n    # GPU support\n\n    gpu:\n\n        enabled: false\n\n        vendor: \"nvidia\" # nvidia or amd\n\n        count: 1\n\n    # Node scheduling (uncomment and customize as needed)\n\n    # nodeSelector:\n\n    #   accelerator: nvidia-tesla-t4\n\n    # tolerations: []\n\n    # affinity: {}\n\n\n\n    # Security context\n\n    securityContext:\n\n        allowPrivilegeEscalation: false\n\n    # Environment variables (uncomment and add as needed)\n\n    # env:\n\n    #   DMR_ORIGINS: \"http://localhost:31246\"\n\n    resources:\n\n        limits:\n\n            cpu: \"1000m\"\n\n            memory: \"2Gi\"\n\n        requests:\n\n            cpu: \"100m\"\n\n            memory: \"256Mi\"\n\n    # Storage for models\n\n    storage:\n\n        size: \"100Gi\"\n\n        storageClass: \"\" # Empty uses default storage class\n\n    # Models to pre-pull\n\n    models:\n\n        - ai/qwen2.5:latest\n\n        - ai/mxbai-embed-large\nDeploying a model runner\nDocker Desktop\n\nWhen modelRunner.enabled is false, Compose Bridge configures your workloads to connect to Docker Model Runner running on the host:\n\nhttp://host.docker.internal:12434/engines/v1/\n\nThe endpoint is automatically injected into your service containers.\n\nKubernetes\n\nWhen modelRunner.enabled is true, Compose Bridge uses the generated manifests to deploy Docker Model Runner in your cluster, including:\n\nDeployment: Runs the docker-model-runner container\nService: Exposes port 80 (maps to container port 12434)\nPersistentVolumeClaim: Stores model files\n\nThe modelRunner.enabled setting also determines the number of replicas for the model-runner-deployment:\n\nWhen true, the deployment replica count is set to 1, and Docker Model Runner is deployed in the Kubernetes cluster.\nWhen false, the replica count is 0, and no Docker Model Runner resources are deployed.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nConfigure model runner settings\nDeploying a model runner\nDocker Desktop\nKubernetes\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996030,
    "timestamp": "2026-02-07T06:37:09.943Z",
    "title": "Customize | Docker Docs",
    "url": "https://docs.docker.com/compose/bridge/customize/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nUsage\nCustomize\nUse Model Runner\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nCompose Bridge\n/\nCustomize\nCustomize Compose Bridge\nCopy as Markdown\nRequires:\nDocker Desktop 4.43.0 and later\n\nYou can customize how Compose Bridge converts your Docker Compose files into platform-specific formats.\n\nThis page explains how Compose Bridge uses templating to generate Kubernetes manifests and how you can customize these templates for your specific requirements and needs, or how you can build your own transformation.\n\nHow it works\n\nCompose bridge uses transformations to let you convert a Compose model into another form.\n\nA transformation is packaged as a Docker image that receives the fully-resolved Compose model as /in/compose.yaml and can produce any target format file under /out.\n\nCompose Bridge includes a default Kubernetes transformation using Go templates, which you can customize by replacing or extending templates.\n\nTemplate syntax\n\nCompose Bridge makes use of templates to transform a Compose configuration file into Kubernetes manifests. Templates are plain text files that use the Go templating syntax. This enables the insertion of logic and data, making the templates dynamic and adaptable according to the Compose model.\n\nWhen a template is executed, it must produce a YAML file which is the standard format for Kubernetes manifests. Multiple files can be generated as long as they are separated by ---\n\nEach YAML output file begins with custom header notation, for example:\n\n#! manifest.yaml\n\nIn the following example, a template iterates over services defined in a compose.yaml file. For each service, a dedicated Kubernetes manifest file is generated, named according to the service and containing specified configurations.\n\n{{ range $name, $service := .services }}\n\n---\n\n#! {{ $name }}-manifest.yaml\n\n# Generated code, do not edit\n\nkey: value\n\n## ...\n\n{{ end }}\nInput model\n\nYou can generate the input model by running docker compose config.\n\nThis canonical YAML output serves as the input for Compose Bridge transformations. Within the templates, data from the compose.yaml is accessed using dot notation, allowing you to navigate through nested data structures. For example, to access the deployment mode of a service, you would use service.deploy.mode:\n\n# iterate over a yaml sequence\n\n{{ range $name, $service := .services }}\n\n # access a nested attribute using dot notation\n\n {{ if eq $service.deploy.mode \"global\" }}\n\nkind: DaemonSet\n\n {{¬†end }}\n\n{{¬†end }}\n\nYou can check the Compose Specification JSON schema for a full overview of the Compose model. This schema outlines all possible configurations and their data types in the Compose model.\n\nHelper functions\n\nAs part of the Go templating syntax, Compose Bridge offers a set of YAML helper functions designed to manipulate data within the templates efficiently:\n\nFunction\tDescription\nseconds\tConverts a duration into an integer (seconds).\nuppercase\tConverts a string to uppercase.\ntitle\tCapitalizes the first letter of each word.\nsafe\tConverts a string into a safe identifier (replaces non-lowercase characters with -).\ntruncate\tRemoves the first N elements from a list.\njoin\tJoins list elements into a single string with a separator.\nbase64\tEncodes a string as base64 (used for Kubernetes secrets).\nmap\tMaps values using ‚Äúvalue -> newValue‚Äù syntax.\nindent\tIndents string content by N spaces.\nhelmValue\tOutputs a Helm-style template value.\n\nIn the following example, the template checks if a healthcheck interval is specified for a service, applies the seconds function to convert this interval into seconds and assigns the value to the periodSeconds attribute.\n\n{{ if $service.healthcheck.interval }}\n\n            periodSeconds: {{ $service.healthcheck.interval | seconds }}{{ end }}\n\n{{ end }}\nCustomize the default templates\n\nAs Kubernetes is a versatile platform, there are many ways to map Compose concepts into Kubernetes resource definitions. Compose Bridge lets you customize the transformation to match your own infrastructure decisions and preferences, with varying level of flexibility and effort.\n\nModify the default templates\n\nYou can extract templates used by the default transformation docker/compose-bridge-kubernetes:\n\n$ docker compose bridge transformations create --from docker/compose-bridge-kubernetes my-template\n\n\nThe templates are extracted into a directory named after your template name, in this case my-template. It includes:\n\nA Dockerfile that lets you create your own image to distribute your template\nA directory containing the templating files\n\nEdit, add, or remove templates as needed.\n\nYou can then use the generated Dockerfile to package your changes into a new transformation image, which you can then use with Compose Bridge:\n\n$ docker build --tag mycompany/transform --push .\n\n\nUse your transformation as a replacement:\n\n$ docker compose bridge convert --transformations mycompany/transform \n\nModel Runner templates\n\nThe default transformation also includes templates for applications that use LLMs:\n\nmodel-runner-deployment.tmpl\nmodel-runner-service.tmpl\nmodel-runner-pvc.tmpl\n/overlays/model-runner/kustomization.yaml\n/overlays/desktop/deployment.tmpl\n\nThese templates can be extended or replaced to change how Docker Model Runner is deployed or configured.\n\nFor more details, see Use Model Runner.\n\nAdd your own templates\n\nFor resources that are not managed by Compose Bridge's default transformation, you can build your own templates.\n\nThe compose.yaml model may not offer all the configuration attributes required to populate the target manifest. If this is the case, you can then rely on Compose custom extensions to better describe the application, and offer an agnostic transformation.\n\nFor example, if you add x-virtual-host metadata to service definitions in the compose.yaml file, you can use the following custom attribute to produce Ingress rules:\n\n{{ $project := .name }}\n\n#! {{ $name }}-ingress.yaml\n\n# Generated code, do not edit\n\napiVersion: networking.k8s.io/v1\n\nkind: Ingress\n\nmetadata:\n\n  name: virtual-host-ingress\n\n  namespace: {{ $project }}\n\nspec:\n\n  rules:  \n\n{{ range $name, $service := .services }}\n\n{{ range index $service \"x-virtual-host\" }}\n\n  - host: ${{ . }}\n\n    http:\n\n      paths:\n\n      - path: \"/\"\n\n        backend:\n\n          service:\n\n            name: ${{ name }}\n\n            port:\n\n              number: 80  \n\n{{ end }}\n\n{{ end }}\n\nOnce packaged into a Docker image, you can use this custom template when transforming Compose models into Kubernetes in addition to other transformations:\n\n$ docker compose bridge convert \\\n\n    --transformation docker/compose-bridge-kubernetes \\\n\n    --transformation mycompany/transform \n\nBuild your own transformation\n\nWhile Compose Bridge templates make it easy to customize with minimal changes, you may want to make significant changes, or rely on an existing conversion tool.\n\nA Compose Bridge transformation is a Docker image that is designed to get a Compose model from /in/compose.yaml and produce platform manifests under /out. This simple contract makes it easy to bundle an alternate transformation using Kompose:\n\nFROM alpine\n\n\n\n# Get kompose from github release page\n\nRUN apk add --no-cache curl\n\nARG VERSION=1.32.0\n\nRUN ARCH=$(uname -m | sed 's/armv7l/arm/g' | sed 's/aarch64/arm64/g' | sed 's/x86_64/amd64/g') && \\\n\n    curl -fsL \\\n\n    \"https://github.com/kubernetes/kompose/releases/download/v${VERSION}/kompose-linux-${ARCH}\" \\\n\n    -o /usr/bin/kompose\n\nRUN chmod +x /usr/bin/kompose\n\n\n\nCMD [\"/usr/bin/kompose\", \"convert\", \"-f\", \"/in/compose.yaml\", \"--out\", \"/out\"]\n\nThis Dockerfile bundles Kompose and defines the command to run this tool according to the Compose Bridge transformation contract.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow it works\nTemplate syntax\nInput model\nHelper functions\nCustomize the default templates\nModify the default templates\nAdd your own templates\nBuild your own transformation\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996034,
    "timestamp": "2026-02-07T06:37:09.943Z",
    "title": "Compose SDK | Docker Docs",
    "url": "https://docs.docker.com/compose/compose-sdk/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nCompose SDK\nUsing the Compose SDK\nCopy as Markdown\nRequires:\nDocker Compose 5.0.0 and later\n\nThe docker/compose package can be used as a Go library by third-party applications to programmatically manage containerized applications defined in Compose files. This SDK provides a comprehensive API that lets you integrate Compose functionality directly into your applications, allowing you to load, validate, and manage multi-container environments without relying on the Compose CLI.\n\nWhether you need to orchestrate containers as part of a deployment pipeline, build custom management tools, or embed container orchestration into your application, the Compose SDK offers the same powerful capabilities that drive the Docker Compose command-line tool.\n\nSet up the SDK\n\nTo get started, create an SDK instance using the NewComposeService() function, which initializes a service with the necessary configuration to interact with the Docker daemon and manage Compose projects. This service instance provides methods for all core Compose operations including creating, starting, stopping, and removing containers, as well as loading and validating Compose files. The service handles the underlying Docker API interactions and resource management, allowing you to focus on your application logic.\n\nRequirements\n\nBefore using the SDK, make sure you're using a compatible version of the Docker CLI.\n\nrequire (\n\n    github.com/docker/cli v28.5.2+incompatible\n\n)\n\nDocker CLI version 29.0.0 and later depends on the new github.com/moby/moby module, whereas Docker Compose v5 currently depends on github.com/docker/docker. This means you need to pin docker/cli v28.5.2+incompatible to ensure compatibility and avoid build errors.\n\nExample usage\n\nHere's a basic example demonstrating how to load a Compose project and start the services:\n\npackage main\n\n\n\nimport (\n\n    \"context\"\n\n    \"log\"\n\n\n\n\t\"github.com/docker/cli/cli/command\"\n\n\t\"github.com/docker/cli/cli/flags\"\n\n    \"github.com/docker/compose/v5/pkg/api\"\n\n    \"github.com/docker/compose/v5/pkg/compose\"\n\n)\n\n\n\nfunc main() {\n\n    ctx := context.Background()\n\n\n\n\tdockerCLI, err := command.NewDockerCli()\n\n\tif err != nil {\n\n\t\tlog.Fatalf(\"Failed to create docker CLI: %v\", err)\n\n\t}\n\n\terr = dockerCLI.Initialize(&flags.ClientOptions{})\n\n\tif err != nil {\n\n\t\tlog.Fatalf(\"Failed to initialize docker CLI: %v\", err)\n\n\t}\n\n\t\n\n    // Create a new Compose service instance\n\n    service, err := compose.NewComposeService(dockerCLI)\n\n    if err != nil {\n\n        log.Fatalf(\"Failed to create compose service: %v\", err)\n\n    }\n\n\n\n    // Load the Compose project from a compose file\n\n    project, err := service.LoadProject(ctx, api.ProjectLoadOptions{\n\n        ConfigPaths: []string{\"compose.yaml\"},\n\n        ProjectName: \"my-app\",\n\n    })\n\n    if err != nil {\n\n        log.Fatalf(\"Failed to load project: %v\", err)\n\n    }\n\n\n\n    // Start the services defined in the Compose file\n\n    err = service.Up(ctx, project, api.UpOptions{\n\n        Create: api.CreateOptions{},\n\n        Start:  api.StartOptions{},\n\n    })\n\n    if err != nil {\n\n        log.Fatalf(\"Failed to start services: %v\", err)\n\n    }\n\n\n\n    log.Printf(\"Successfully started project: %s\", project.Name)\n\n}\n\nThis example demonstrates the core workflow - creating a service instance, loading a project from a Compose file, and starting the services. The SDK provides many additional operations for managing the lifecycle of your containerized application.\n\nCustomizing the SDK\n\nThe NewComposeService() function accepts optional compose.Option parameters to customize the SDK behavior. These options allow you to configure I/O streams, concurrency limits, dry-run mode, and other advanced features.\n\n    // Create a custom output buffer to capture logs\n\n    var outputBuffer bytes.Buffer\n\n\n\n    // Create a compose service with custom options\n\n    service, err := compose.NewComposeService(dockerCLI,\n\n        compose.WithOutputStream(&outputBuffer),          // Redirect output to custom writer\n\n        compose.WithErrorStream(os.Stderr),               // Use stderr for errors\n\n        compose.WithMaxConcurrency(4),                    // Limit concurrent operations\n\n        compose.WithPrompt(compose.AlwaysOkPrompt()),     // Auto-confirm all prompts\n\n    )\nAvailable options\nWithOutputStream(io.Writer): Redirect standard output to a custom writer\nWithErrorStream(io.Writer): Redirect error output to a custom writer\nWithInputStream(io.Reader): Provide a custom input stream for interactive prompts\nWithStreams(out, err, in): Set all I/O streams at once\nWithMaxConcurrency(int): Limit the number of concurrent operations against the Docker API\nWithPrompt(Prompt): Customize user confirmation behavior (use AlwaysOkPrompt() for non-interactive mode)\nWithDryRun: Run operations in dry-run mode without actually applying changes\nWithContextInfo(api.ContextInfo): Set custom Docker context information\nWithProxyConfig(map[string]string): Configure HTTP proxy settings for builds\nWithEventProcessor(progress.EventProcessor): Receive progress events and operation notifications\n\nThese options provide fine-grained control over the SDK's behavior, making it suitable for various integration scenarios including CLI tools, web services, automation scripts, and testing environments.\n\nTracking operations with EventProcessor\n\nThe EventProcessor interface allows you to monitor Compose operations in real-time by receiving events about changes applied to Docker resources such as images, containers, volumes, and networks. This is particularly useful for building user interfaces, logging systems, or monitoring tools that need to track the progress of Compose operations.\n\nUnderstanding EventProcessor\n\nA Compose operation, such as up, down, build, performs a series of changes to Docker resources. The EventProcessor receives notifications about these changes through three key methods:\n\nStart(ctx, operation): Called when a Compose operation begins, for example up\nOn(events...): Called with progress events for individual resource changes, for example, container starting, image being pulled\nDone(operation, success): Called when the operation completes, indicating success or failure\n\nEach event contains information about the resource being modified, its current status, and progress indicators when applicable (such as download progress for image pulls).\n\nEvent status types\n\nEvents report resource changes with the following status types:\n\nWorking: Operation is in progress, for example, creating, starting, pulling\nDone: Operation completed successfully\nWarning: Operation completed with warnings\nError: Operation failed\n\nCommon status text values include: Creating, Created, Starting, Started, Running, Stopping, Stopped, Removing, Removed, Building, Built, Pulling, Pulled, and more.\n\nBuilt-in EventProcessor implementations\n\nThe SDK provides three ready-to-use EventProcessor implementations:\n\nprogress.NewTTYWriter(io.Writer): Renders an interactive terminal UI with progress bars and task lists (similar to the Docker Compose CLI output)\nprogress.NewPlainWriter(io.Writer): Outputs simple text-based progress messages suitable for non-interactive environments or log files\nprogress.NewJSONWriter(): Render events as JSON objects\nprogress.NewQuietWriter(): (Default) Silently processes events without producing any output\n\nUsing EventProcessor, a custom UI can be plugged into docker/compose.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSet up the SDK\nRequirements\nExample usage\nCustomizing the SDK\nAvailable options\nTracking operations with EventProcessor\nUnderstanding EventProcessor\nEvent status types\nBuilt-in EventProcessor implementations\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996042,
    "timestamp": "2026-02-07T06:37:09.959Z",
    "title": "Give feedback | Docker Docs",
    "url": "https://docs.docker.com/compose/support-and-feedback/feedback/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nFAQs\nGive feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nSupport and feedback\n/\nGive feedback\nGive feedback\nCopy as Markdown\n\nThere are many ways you can provide feedback on Docker Compose.\n\nReport bugs or problems on GitHub\n\nTo report bugs or problems, visit Docker Compose on GitHub\n\nFeedback via Community Slack channels\n\nYou can also provide feedback through the #docker-compose Docker Community Slack channel.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nReport bugs or problems on GitHub\nFeedback via Community Slack channels\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996039,
    "timestamp": "2026-02-07T06:37:09.959Z",
    "title": "FAQs | Docker Docs",
    "url": "https://docs.docker.com/compose/support-and-feedback/faq/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nIntroduction to Compose\nInstall\nQuickstart\nHow-tos\nCompose Bridge\nCompose SDK New\nSupport and feedback\nFAQs\nGive feedback\nRelease notes\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nDocker Compose\n/\nSupport and feedback\n/\nFAQs\nFrequently asked questions about Docker Compose\nCopy as Markdown\nWhat is the difference between docker compose and docker-compose\n\nVersion one of the Docker Compose command-line binary was first released in 2014. It was written in Python, and is invoked with docker-compose. Typically, Compose v1 projects include a top-level version element in the compose.yaml file, with values ranging from 2.0 to 3.8, which refer to the specific file formats.\n\nVersion two of the Docker Compose command-line binary was announced in 2020, is written in Go, and is invoked with docker compose. Compose v2 ignores the version top-level element in the compose.yaml file.\n\nFor further information, see History and development of Compose.\n\nWhat's the difference between up, run, and start?\n\nTypically, you want docker compose up. Use up to start or restart all the services defined in a compose.yaml. In the default \"attached\" mode, you see all the logs from all the containers. In \"detached\" mode (-d), Compose exits after starting the containers, but the containers continue to run in the background.\n\nThe docker compose run command is for running \"one-off\" or \"adhoc\" tasks. It requires the service name you want to run and only starts containers for services that the running service depends on. Use run to run tests or perform an administrative task such as removing or adding data to a data volume container. The run command acts like docker run -ti in that it opens an interactive terminal to the container and returns an exit status matching the exit status of the process in the container.\n\nThe docker compose start command is useful only to restart containers that were previously created but were stopped. It never creates new containers.\n\nWhy do my services take 10 seconds to recreate or stop?\n\nThe docker compose stop command attempts to stop a container by sending a SIGTERM. It then waits for a default timeout of 10 seconds. After the timeout, a SIGKILL is sent to the container to forcefully kill it. If you are waiting for this timeout, it means that your containers aren't shutting down when they receive the SIGTERM signal.\n\nThere has already been a lot written about this problem of processes handling signals in containers.\n\nTo fix this problem, try the following:\n\nMake sure you're using the exec form of CMD and ENTRYPOINT in your Dockerfile.\n\nFor example use [\"program\", \"arg1\", \"arg2\"] not \"program arg1 arg2\". Using the string form causes Docker to run your process using bash which doesn't handle signals properly. Compose always uses the JSON form, so don't worry if you override the command or entrypoint in your Compose file.\n\nIf you are able, modify the application that you're running to add an explicit signal handler for SIGTERM.\n\nSet the stop_signal to a signal which the application knows how to handle:\n\nservices:\n\n  web:\n\n    build: .\n\n    stop_signal: SIGINT\n\nIf you can't modify the application, wrap the application in a lightweight init system (like s6) or a signal proxy (like dumb-init or tini). Either of these wrappers takes care of handling SIGTERM properly.\n\nHow do I run multiple copies of a Compose file on the same host?\n\nCompose uses the project name to create unique identifiers for all of a project's containers and other resources. To run multiple copies of a project, set a custom project name using the -p command line option or the COMPOSE_PROJECT_NAME environment variable.\n\nCan I use JSON instead of YAML for my Compose file?\n\nYes. YAML is a superset of JSON so any JSON file should be valid YAML. To use a JSON file with Compose, specify the filename to use, for example:\n\n$ docker compose -f compose.json up\n\nShould I include my code with COPY/ADD or a volume?\n\nYou can add your code to the image using COPY or ADD directive in a Dockerfile. This is useful if you need to relocate your code along with the Docker image, for example when you're sending code to another environment (production, CI, etc).\n\nUse a volume if you want to make changes to your code and see them reflected immediately, for example when you're developing code and your server supports hot code reloading or live-reload.\n\nThere may be cases where you want to use both. You can have the image include the code using a COPY, and use a volume in your Compose file to include the code from the host during development. The volume overrides the directory contents of the image.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat is the difference between docker compose and docker-compose\nWhat's the difference between up, run, and start?\nWhy do my services take 10 seconds to recreate or stop?\nHow do I run multiple copies of a Compose file on the same host?\nCan I use JSON instead of YAML for my Compose file?\nShould I include my code with COPY/ADD or a volume?\nFAQ\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996045,
    "timestamp": "2026-02-07T06:37:09.965Z",
    "title": "Releases ¬∑ docker/compose",
    "url": "https://github.com/docker/compose/releases",
    "text": "Skip to content\ndocker\ncompose\nType / to search\nRepository navigation\nCode\nIssues\n37\n¬†(37)\nPull requests\n14\n¬†(14)\nAgents\nActions\nSecurity\n1\n¬†(1)\nInsights\nReleases: docker/compose\nReleases\nTags\nv5.0.2\n github-actions\n v5.0.2\n c428a77\nCompare\nv5.0.2 Latest\nWhat's Changed\nüêõ Fixes\nFixed progress UI to adapt to terminal width by @ndeloof in #13519\nRemoved warning when no explicit build has been requested. by @ndeloof in #13493\nRestored runtime_flags support in models by @ilopezluna in #13460\nAdded service name completion to down command by @bmo-at in #13470\nFixed tilde in --env-file paths expanded to user home directory by @tensorworkerr in #13510\nHandle healthcheck.disable: true by @stavros-k in #13494\nFixed shutdown and error handling for large file change batches in watch by @amyssnippet in #13525\nüîß Internal\nAdded unit test for upOptions.OnExit method by @htoyoda18 in #13489\nclean up temporary compose files after conversion by @htoyoda18 in #13483\nFixed typo in isSwarmEnabled method name by @htoyoda18 in #13481\nFixed incorrect usage of errgroup.WithContext by @htoyoda18 in #13480\nFixed timeout initialization when waitTimeout is zero by @htoyoda18 in #13471\nExtracted API version constants to dedicated file by @htoyoda18 in #13503\nReplace tabbed indentation in sdk.md by @pkqk in #13505\nImproved attach error handling and cleanup by @htoyoda18 in #13488\nModernize tests by @dgageot in #13531\nset fsnotify build tag when building for OSX by @ndeloof in #13532\n‚öôÔ∏è Dependencies\nbuild(deps): bump github.com/klauspost/compress to v1.18.2 by @thaJeztah in #13499\ngo.mod: remove exclude rules by @thaJeztah in #13498\nbuild(deps): bump github.com/containerd/containerd/v2 to v2.2.1 by @thaJeztah in #13497\nbuild(deps): bump golang.org/x/sys from 0.39.0 to 0.40.0 by @dependabot[bot] in #13502\nbuild(deps): bump google.golang.org/grpc from 1.77.0 to 1.78.0 by @dependabot[bot] in #13475\nbuild(deps): bump github.com/go-viper/mapstructure/v2 from 2.4.0 to 2.5.0 by @dependabot[bot] in #13506\nbuild(deps): bump github.com/sirupsen/logrus v1.9.4 by @thaJeztah in #13518\nBump compose to v2.10.1 by @ndeloof in #13528\nNew Contributors\n@bmo-at made their first contribution in #13470\n@pkqk made their first contribution in #13505\n@tensorworkerr made their first contribution in #13510\n@stavros-k made their first contribution in #13494\n@amyssnippet made their first contribution in #13525\n@dgageot made their first contribution in #13531\n\nFull Changelog: v5.0.1...v5.0.2\n\nContributors\npkqk, ndeloof, and 9 other contributors\nAssets\n47\nchecksums.txt\nsha256:bcf6e292cbfb67fb89d06282373de03ab53bdf837ad58636d37c94af2fbcb016\n3.34 KB\ndocker-compose-darwin-aarch64\nsha256:72c7e684a1944cfb1765358d7ea7822cd209f25208ca235b914e761d9abb224e\n28.8 MB\ndocker-compose-darwin-aarch64.provenance.json\nsha256:134b4c6bd2d7666e53a3e57688f2e965de175ad3f649e314089a2c3d3cb76b7b\n60.9 KB\ndocker-compose-darwin-aarch64.sbom.json\nsha256:bef833ab9316d4e0fb10c38abe467a8ee2d2fbd82844f42a3dfacd5a98f3b17f\n273 KB\ndocker-compose-darwin-aarch64.sha256\nsha256:72dfc92ff4b574bb8ec0b49164d73a7f8e805e595e5a39a3f60e31ab72e3f6f9\n96 Bytes\ndocker-compose-darwin-x86_64\nsha256:766d6f9b305d89c3b8fe88cb6fb207fd7f531fbd63982b6136d058c1f98767bd\n30.2 MB\ndocker-compose-darwin-x86_64.provenance.json\nsha256:f289cf19fc76f48e320abe7cc74e7d11c50424eb7c2c41fb36129c848efff727\n60.9 KB\ndocker-compose-darwin-x86_64.sbom.json\nsha256:7f271f3ecf9463fb3e928d12669791f647eb4b37f4f9ae174ddfd4dfe145b9bd\n273 KB\ndocker-compose-darwin-x86_64.sha256\nsha256:7825bcdf3393d23231de1edcfbc3d539a095ffd5c08659cc4e0c4bfa16189850\n95 Bytes\ndocker-compose-linux-aarch64\nsha256:ac7810e0cd56a5b58576688196fafa843e07e8241fb91018a736d549ea20a3f3\n28.6 MB\nSource code\n(zip)\nSource code\n(tar.gz)\nShow all 47 assets\nüëç\n8\nüòÑ\n3\nüéâ\n7\n‚ù§Ô∏è\n4\nüöÄ\n3\nüëÄ\n3\n13 people reacted\nv5.0.1\n github-actions\n v5.0.1\n c89b8a2\nCompare\nv5.0.1\nWhat's Changed\nüêõ Fixes\nRestored support for COMPOSE_COMPATIBILITY by @ndeloof in #13424\nFixed grammatical errors and improve clarity in code. by @xiaolinny in #13429\nFixed broken run --quiet. by @ndeloof in #13430\nFixed SDK example by @ndeloof in #13416\nAdded a check buildx version is set before comparing it. by @yangfeiyu20102011 in #13415\nFixed grammar: pluralize 'service' and remove apostrophes in lets. by @rashmivagha in #13423\nFixed progress UI not restoring terminal once operation completes. by @ndeloof in #13439\nFixed status alignment in progress UI. by @ndeloof in #13438\nRestored image layer download progress details on pull. by @ndeloof in #13445\nAdded 'configured' event at the end of model configuration phase. by @glours in #13446\nIntroduced a build tag to select watcher implementation. by @ndeloof in #13452\nRemoved mention for v2 on README. by @alexislefebvre in #13451\nFixed missing error handling in setEnvWithDotEnv. by @htoyoda18 in #13450\nAdopted morikuni/aec library over raw ANSI sequences. by @ndeloof in #13440\nPrevented incorrect progress metrics to break compose display. by @ndeloof in #13457\nRestored support for BUILDKIT_PROGRESS. by @ndeloof in #13455\nAdded check model plugin is successfully loaded. by @ndeloof in #13464\nAdded a warning when no service has been selected to build. by @ndeloof in #13467\n‚öôÔ∏è Dependencies\nDrop Go min patch version by @austinvazquez in #13418\nbump golang 1.24.11 by @austinvazquez in #13417\nbump osxcross by @ndeloof in #13425\nbump golang.org/x/sys from 0.38.0 to 0.39.0 by @dependabot[bot] in #13433\nbump github.com/docker/cli-docs-tool from 0.10.0 to 0.11.0 by @dependabot[bot] in #13437\nbump golang.org/x/sync from 0.18.0 to 0.19.0 by @dependabot[bot] in #13434\nbump tags.cncf.io/container-device-interface from 1.0.1 to 1.1.0 by @dependabot[bot] in #13441\nbump github.com/moby/buildkit from 0.26.2 to 0.26.3 by @dependabot[bot] in #13462\nNew Contributors\n@yangfeiyu20102011 made their first contribution in #13415\n@xiaolinny made their first contribution in #13429\n@alexislefebvre made their first contribution in #13451\n@htoyoda18 made their first contribution in #13450\n\nFull Changelog: v5.0.0...v5.0.1\n\nContributors\nndeloof, glours, and 7 other contributors\nAssets\n47\nüëç\n11\nüòÑ\n3\nüéâ\n2\n‚ù§Ô∏è\n5\nüöÄ\n1\nüëÄ\n1\n16 people reacted\nv5.0.0 \"Mont Blanc\"\n github-actions\n v5.0.0\n 13d70b1\nCompare\nv5.0.0 \"Mont Blanc\"\n\nMajor changes in this release:\n\nCompose can now officially be used as a SDK to be integrated into third-party softwares\nInternal builder has been removed, build is delegated to Docker Bake (same as docker build command)\nWhy \"v5\" ?\n\nWe decided to skip 3.0.0 for next major release after docker Compose v2 to prevent (more) confusion with the obsolete docker-compose file versions 2.x and 3.x inherited from Docker Compose v1. We also skipped 4.0.0 to have a clear separation with this legacy.\n\nWhat's Changed\n‚ú® Improvements\ndrop support for internal buildkit builder by @ndeloof in #13056\nSetup Compose service using functional parameters by @ndeloof in #13312\nIntroduce abstractions to support SDK usage without requiring Docker CLI by @glours in #13313\nconfigure Compose service with io.Reader and io.Writer by @glours in #13322\nMake progress Writer a CLI component by @ndeloof in #13316\nSDK docs by @ndeloof in #13350\ndocument support for OCI and Git remote resources by @ndeloof in #13362\nrun hooks on restart by @ndeloof in #13321\nfix(run): Ensure images exist only for the target service in run command by @idsulik in #13325\nAdd load project to api definition by @glours in #13329\nintroduce --insecure-registry, reserved for testing purpose by @ndeloof in #13355\nMove progress package to cmd as a command line component by @ndeloof in #13357\nAdded support for build.no_cache_filter by @ndeloof in #13377\nadd --wait option to start command by @ndeloof in #13409\nüêõ Fixes\nfix OCI compose override support by @ndeloof in #13311\nFix help output for \"exec --no-tty\" option by @tonyo in #13314\nfix typo in error message by @stasadev in #13328\ndisable progress UI when build is ran with --print by @ndeloof in #13351\nadd (restore) support for detach keys by @ndeloof in #13369\nimages command should display image Created time or N/A if not available by @ndeloof in #13376\nFix support for port range by @ndeloof in #13381\nskip includes preparing publish by @ndeloof in #13400\nfeat(model): ignore runtime flags in model configuration by @ilopezluna in #13404\nüîß Internal\nNext release will be major version v5.x by @ndeloof in #13375\nTest to check writeComposeFile detects invalid OCI artifact by @ndeloof in #13309\nCode Cleanup by @ndeloof in #13315\nfix various linting issues by @thaJeztah in #13326\nMaking the American/British spellings consistent to the error messages by @JLesDev in #13366\ngha: test against docker v29, v28, and rename checks to use stable, oldstable by @thaJeztah in #13380\nrefactor: replace Split in loops with more efficient SplitSeq by @vicerace in #13393\nrefactor: use strings.Builder to improve performance by @liuyueyangxmu in #13398\nFix grammar: change 'adopted' to 'adopt' in Docker Swarm note by @rashmivagha in #13407\ngolangci-lint: use gci formatter instead of goimports by @thaJeztah in #13406\nrefactor: replace interface{} with any for clarity and modernization by @zjumathcode in #13411\n‚öôÔ∏è Dependencies\nBump compose go to v2.10.0 by @ndeloof in #13410\nbuild(deps): bump github.com/containerd/platforms from 1.0.0-rc.1 to 1.0.0-rc.2 by @dependabot[bot] in #13353\nbuild(deps): bump github.com/docker/docker from 28.5.1+incompatible to 28.5.2+incompatible by @dependabot[bot] in #13361\nbuild(deps): bump github.com/containerd/containerd/v2 from 2.1.4 to 2.2.0 by @dependabot[bot] in #13358\nbuild(deps): bump github.com/docker/cli from 28.5.1+incompatible to 28.5.2+incompatible by @dependabot[bot] in #13360\nbuild(deps): bump golang.org/x/sys from 0.37.0 to 0.38.0 by @dependabot[bot] in #13367\nbuild(deps): bump golang.org/x/sync from 0.17.0 to 0.18.0 by @dependabot[bot] in #13368\nDockerfile: update golangci-lint to v2.6.2 by @thaJeztah in #13370\nbuild(deps): bump buildx v0.30.0, buildkit v0.26.0, otel v1.38.0, otel/contrib v0.63.0 by @thaJeztah in #13379\nbump dependencies by @ndeloof in #13392\nbuild(deps): bump github.com/hashicorp/go-version from 1.7.0 to 1.8.0 by @dependabot[bot] in #13403\nbuild(deps): bump golang.org/x/crypto v0.45.0 by @thaJeztah in #13405\nNew Contributors\n@tonyo made their first contribution in #13314\n@stasadev made their first contribution in #13328\n@JLesDev made their first contribution in #13366\n@vicerace made their first contribution in #13393\n@liuyueyangxmu made their first contribution in #13398\n@ilopezluna made their first contribution in #13404\n@rashmivagha made their first contribution in #13407\n@zjumathcode made their first contribution in #13411\n\nFull Changelog: v2.40.2...v5.0.0\n\nContributors\nndeloof, glours, and 11 other contributors\nAssets\n47\nüëç\n34\nüòÑ\n2\nüéâ\n23\n‚ù§Ô∏è\n8\nüöÄ\n8\nüëÄ\n3\n60 people reacted\nv5.0.0-rc.2\n github-actions\n v5.0.0-rc.2\n d7e5f20\nCompare\nv5.0.0-rc.2 Pre-release\nWhat's Changed\nNext release will be major version v5.x by @ndeloof in #13375\nimages command should display image Created time or N/A if not available by @ndeloof in #13376\n\nFull Changelog: v5.0.0-rc.1...v5.0.0-rc.2\n\nContributors\nndeloof\nAssets\n47\nüëç\n8\n8 people reacted\nv5.0.0-rc.1\n github-actions\n v5.0.0-rc.1\n f0dce1b\nCompare\nv5.0.0-rc.1 Pre-release\nWhat's Changed\n\nThis is the first (candidate) release for next major Docker Compose release.\nTo avoid confusion with the \"v2\" and \"v3\" legacy compose file formats, we decided to directly jump to version 5.x\n\nMajor changes in this release:\n\nCompose can now officially be used as a SDK to be integrated into third-party softwares\nInternal builder has been removed, build is delegated to Docker Bake (same as docker build command)\n‚ú® Improvements\nDocument use as a SDK by @ndeloof in #13350\nSetup Compose service using functional parameters by @ndeloof in #13312\ndrop support for internal buildkit builder by @ndeloof in #13056\ndocument support for OCI and Git remote resources by @ndeloof in #13362\nintroduce --insecure-registry, reserved for testing purpose by @ndeloof in #13355\nadd (restore) support for detach keys by @ndeloof in #13369\nüêõ Fixes\nfix(run): Ensure images exist only for the target service in run command by @idsulik in * fix typo in error message by @stasadev in #13328\n#13325\nüîß Internal\nfix various linting issues by @thaJeztah in #13326\n‚öôÔ∏è Dependencies\nbump compose-go to version v2.9.1 by @glours in #13332\nbuild(deps): bump github.com/containerd/platforms from 1.0.0-rc.1 to 1.0.0-rc.2 by @dependabot[bot] in #13353\nbuild(deps): bump github.com/moby/buildkit from 0.25.1 to 0.25.2 by @dependabot[bot] in #13359\nbuild(deps): bump github.com/docker/docker from 28.5.1+incompatible to 28.5.2+incompatible by @dependabot[bot] in #13361\nbuild(deps): bump github.com/containerd/containerd/v2 from 2.1.4 to 2.2.0 by @dependabot[bot] in #13358\nbuild(deps): bump github.com/docker/cli from 28.5.1+incompatible to 28.5.2+incompatible by @dependabot[bot] in #13360\nbuild(deps): bump golang.org/x/sys from 0.37.0 to 0.38.0 by @dependabot[bot] in #13367\nbuild(deps): bump golang.org/x/sync from 0.17.0 to 0.18.0 by @dependabot[bot] in #13368\nNew Contributors\n@tonyo made their first contribution in #13314\n@stasadev made their first contribution in #13328\n@JLesDev made their first contribution in #13366\n\nFull Changelog: v2.40.2...v5.0.0-rc.1\n\nContributors\nndeloof, glours, and 6 other contributors\nAssets\n47\nüëç\n4\n‚ù§Ô∏è\n1\nüöÄ\n10\nüëÄ\n2\n14 people reacted\nv2.40.3\n github-actions\n v2.40.3\n 49b1c1e\nCompare\nv2.40.3\nWhat's Changed\nüêõ Fixes\nFix OCI compose override support by @ndeloof #13311\nFix help output for \"exec --no-tty\" option by @tonyo #13314\nPrompt default implementation to prevent a panic by @ndeloof #13317\nRun hooks on restart by @ndeloof #13321\nFix(run): Ensure images exist only for the target service in run command by @idsulik #13325\nFix(git): Fix path traversal vulnerability in git remote loader by @idsulik #13331\nüîß Internal\nTest to check writeComposeFile detects invalid OCI artifact by @ndeloof #13309\nCode Cleanup by @ndeloof #13315\n‚öôÔ∏è Dependencies\nBump compose-go to version v2.9.1 by @glours #13332\n\nFull Changelog: v2.40.2...v2.40.3\n\nContributors\nndeloof, glours, and 2 other contributors\nAssets\n47\nüëç\n8\nüéâ\n5\n‚ù§Ô∏è\n4\nüöÄ\n1\n16 people reacted\nv2.40.2\n github-actions\n v2.40.2\n 6007d4c\nCompare\nv2.40.2\nWhat's Changed\nüêõ Fixes\nCompose can't create a tar with adequate uid:gid ownership by @ndeloof in #13299\nTest digest or canonical reference, not only tag, when checking if an image is already present by @glours in #13302\nüîß Internal\nFail build if minimal required version of buildx isn't installed by @ndeloof in #13295\nremove unused code to only rely on api.Service by @ndeloof in #13300\nIntroduce WithPrompt to configure compose backend to use a plugable UI component for user interaction by @ndeloof in #13308\n\nFull Changelog: v2.40.1...v2.40.2\n\nContributors\nndeloof and glours\nAssets\n47\nüëç\n3\nüéâ\n7\n10 people reacted\nv2.40.1\n github-actions\n v2.40.1\n be8c7e6\nCompare\nv2.40.1\nWhat's Changed\nüêõ Fixes\nWrite error to watcher error channel if Start() fails by @Trolldemorted in #13263\nFix: set PWD only if not set by @kianelbo in #13268\nbake only interpolates ${*} by @ndeloof in #13270\nFix: make \"publish\" push all compose files addressed in \"extends\" statements when using \"profiles\". by @ogoulpeau-ledger in #13277\nSupport Ctrl+Z to run compose in background by @ndeloof in #13289\nFix race-condition bug in publish command by @paul-kinexon in #13291\nSet secret/config uid:gid to match container's USER by @ndeloof in #13288\nFix failure to delegate build with bake by @ndeloof in #13275\nMake CTRL+Z a no-op operation on Windows by @glours in #13293\nüîß Internal\npkg/compose: align classic builder implementation with docker/cli by @thaJeztah in #13278\npkg/compose: build with bake: drop support for buildx v0.16 and lower by @thaJeztah in #13280\nUse fixed version of compose bridge transformer images by @glours in #13284\n‚öôÔ∏è Dependencies\nBuild(deps): bump github.com/docker/docker from 28.5.0+incompatible to 28.5.1+incompatible by @dependabot[bot] in #13274\nBuild(deps): bump github.com/docker/cli from 28.5.0+incompatible to 28.5.1+incompatible by @dependabot[bot] in #13273\nBuild(deps): bump golang.org/x/sys from 0.36.0 to 0.37.0 by @dependabot[bot] in #13272\nBuild(deps): bump docker/buildx v0.29.1, moby/buildkit v0.25.1 by @thaJeztah in #13279\nBump golang to version 1.24.9 by @glours in #13285\nNew Contributors\n@Trolldemorted made their first contribution in #13263\n@ogoulpeau-ledger made their first contribution in #13277\n@paul-kinexon made their first contribution in #13291\n\nFull Changelog: v2.40.0...v2.40.1\n\nContributors\nndeloof, glours, and 6 other contributors\nAssets\n47\nüëç\n3\nüéâ\n4\n7 people reacted\nv2.40.0\n github-actions\n v2.40.0\n d9423f6\nCompare\nv2.40.0\nWhat's Changed\n‚ú® Improvements\npublish Compose application as compose.yaml + images by @ndeloof in #13257\nüêõ Fixes\nresolve secrets based on env var before executing bake by @ndeloof in #13237\npass bake secrets by env by @ndeloof in #13249\nescape $ in bake.json as interpolation already has been managed by cpmpose by @ndeloof in #13259\nüîß Internal\npkg/compose: remove uses of deprecated mitchellh/mapstructure module by @thaJeztah in #13239\npkg/watch: remove unused IsWindowsShortReadError by @thaJeztah in #13052\npkg/compose: build: remove permissions warning on Windows by @thaJeztah in #13236\npluginMain: remove uses of DockerCLI.Apply by @thaJeztah in #13240\nuse containerd registry client by @ndeloof in #13245\nprovider services: use '--project-name=' notation by @glours in #13250\ngha: update test-matrix: remove docker 26.x by @thaJeztah in #13254\npkg/compose: explicitly map AuthConfig fields instead of a direct cast by @thaJeztah in #13253\ncmd/compose: fix minor linting issues by @thaJeztah in #13252\nuse containerd client for OCI operations by @ndeloof in #13251\n‚öôÔ∏è Dependencies\nbuild(deps): bump github.com/docker/docker, docker/cli v28.5.0-rc.1 by @thaJeztah in #13241\nbuild(deps): bump github.com/docker/docker from 28.5.0-rc.1+incompatible to 28.5.0+incompatible by @dependabot[bot] in #13260\nbuild(deps): bump github.com/docker/cli from 28.5.0-rc.1+incompatible to 28.5.0+incompatible by @dependabot[bot] in #13261\n\nFull Changelog: v2.39.4...v2.40.0\n\nContributors\nndeloof, glours, and 2 other contributors\nAssets\n47\nüëç\n6\nüéâ\n10\n14 people reacted\nv2.39.4\n github-actions\n v2.39.4\n a32dc3d\nCompare\nv2.39.4\nWhat's Changed\n‚ú® Improvements\nAdd support of develop.watch.initial_sync attribute by @glours in #13232\nüêõ Fixes\nVolume ls command can run without a project by @ndeloof in #13221\nFix support for build with bake when target docker endpoint requires TLS by @ndeloof in #13231\nDisable Tty if run command started from a piped command by @glours in #13233\nüîß Internal\nTest: Set stop_signal to SIGTERM by @ricardobranco777 in #13214\n‚öôÔ∏è Dependencies\nBump compose-go to version v2.9.0 by @glours in #13234\nNew Contributors\n@ricardobranco777 made their first contribution in #13214\n\nFull Changelog: v2.39.3...v2.39.4\n\nContributors\nndeloof, glours, and ricardobranco777\nAssets\n47\nüëç\n4\nüòÑ\n1\nüéâ\n8\nüöÄ\n1\n12 people reacted\nPrevious 1 2 3 4 5 ‚Ä¶ 27 28 Next\nFooter\n¬© 2026 GitHub,¬†Inc.\nFooter navigation\nTerms\nPrivacy\nSecurity\nStatus\nCommunity\nDocs\nContact\nManage cookies\nDo not share my personal information"
  },
  {
    "id": 258996046,
    "timestamp": "2026-02-07T06:37:09.970Z",
    "title": "Testcontainers | Docker Docs",
    "url": "https://docs.docker.com/testcontainers/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\nTestcontainers\nTestcontainers\nCopy as Markdown\n\nTestcontainers is a set of open source libraries that provides easy and lightweight APIs for bootstrapping local development and test dependencies with real services wrapped in Docker containers. Using Testcontainers, you can write tests that depend on the same services you use in production without mocks or in-memory services.\n\nWhat is Testcontainers?\n\nLearn about what Testcontainers does and its key benefits\n\nThe Testcontainers workflow\n\nUnderstand the Testcontainers workflow\n\nQuickstart\nSupported languages\n\nTestcontainers provide support for the most popular languages, and Docker sponsors the development of the following Testcontainers implementations:\n\nGo\nJava\n\nThe rest are community-driven and maintained by independent contributors.\n\nPrerequisites\n\nTestcontainers requires a Docker-API compatible container runtime. During development, Testcontainers is actively tested against recent versions of Docker on Linux, as well as against Docker Desktop on Mac and Windows. These Docker environments are automatically detected and used by Testcontainers without any additional configuration being necessary.\n\nIt is possible to configure Testcontainers to work for other Docker setups, such as a remote Docker host or Docker alternatives. However, these are not actively tested in the main development workflow, so not all Testcontainers features might be available and additional manual configuration might be necessary.\n\nIf you have further questions about configuration details for your setup or whether it supports running Testcontainers-based tests, contact the Testcontainers team and other users from the Testcontainers community on Slack.\n\nTestcontainers for Go\n\nA Go package that makes it simple to create and clean up container-based dependencies for automated integration/smoke tests.\n\nTestcontainers for Java\n\nA Java library that supports JUnit tests, providing lightweight, throwaway instances of anything that can run in a Docker container.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nQuickstart\nSupported languages\nPrerequisites\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996049,
    "timestamp": "2026-02-07T06:37:09.971Z",
    "title": "cagent | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\ncagent\nCopy as Markdown\nAvailability:\nExperimental \n\ncagent is an open source tool for building teams of specialized AI agents. Instead of prompting one generalist model, you define agents with specific roles and instructions that collaborate to solve problems. Run these agent teams from your terminal using any LLM provider.\n\nWhy agent teams\n\nOne agent handling complex work means constant context-switching. Split the work across focused agents instead - each handles what it's best at. cagent manages the coordination.\n\nHere's a two-agent team that debugs problems:\n\nagents:\n\n  root:\n\n    model: openai/gpt-5-mini # Change to the model that you want to use\n\n    description: Bug investigator\n\n    instruction: |\n\n      Analyze error messages, stack traces, and code to find bug root causes.\n\n      Explain what's wrong and why it's happening.\n\n      Delegate fix implementation to the fixer agent.\n\n    sub_agents: [fixer]\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: mcp\n\n        ref: docker:duckduckgo\n\n\n\n  fixer:\n\n    model: anthropic/claude-sonnet-4-5 # Change to the model that you want to use\n\n    description: Fix implementer\n\n    instruction: |\n\n      Write fixes for bugs diagnosed by the investigator.\n\n      Make minimal, targeted changes and add tests to prevent regression.\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\nThe root agent investigates and explains the problem. When it understands the issue, it hands off to fixer for implementation. Each agent stays focused on its specialty.\n\nInstallation\n\ncagent is included in Docker Desktop 4.49 and later.\n\nFor Docker Engine users or custom installations:\n\nHomebrew: brew install cagent\nWinget: winget install Docker.Cagent\nPre-built binaries: GitHub releases\nFrom source: See the cagent repository\nGet started\n\nTry the bug analyzer team:\n\nSet your API key for the model provider you want to use:\n\n$ export ANTHROPIC_API_KEY=<your_key>  # For Claude models\n\n$ export OPENAI_API_KEY=<your_key>     # For OpenAI models\n\n$ export GOOGLE_API_KEY=<your_key>     # For Gemini models\n\n\nSave the example configuration as debugger.yaml.\n\nRun your agent team:\n\n$ cagent run debugger.yaml\n\n\nYou'll see a prompt where you can describe bugs or paste error messages. The investigator analyzes the problem, then hands off to the fixer for implementation.\n\nHow it works\n\nYou interact with the root agent, which can delegate work to sub-agents you define. Each agent:\n\nUses its own model and parameters\nHas its own context (agents don't share knowledge)\nCan access built-in tools like todo lists, memory, and task delegation\nCan use external tools via MCP servers\n\nThe root agent delegates tasks to agents listed under sub_agents. Sub-agents can have their own sub-agents for deeper hierarchies.\n\nConfiguration options\n\nAgent configurations are YAML files. A basic structure looks like this:\n\nagents:\n\n  root:\n\n    model: claude-sonnet-4-0\n\n    description: Brief role summary\n\n    instruction: |\n\n      Detailed instructions for this agent...\n\n    sub_agents: [helper]\n\n\n\n  helper:\n\n    model: gpt-5-mini\n\n    description: Specialist agent role\n\n    instruction: |\n\n      Instructions for the helper agent...\n\nYou can also configure model settings (like context limits), tools (including MCP servers), and more. See the configuration reference for complete details.\n\nShare agent teams\n\nAgent configurations are packaged as OCI artifacts. Push and pull them like container images:\n\n$ cagent push ./debugger.yaml myusername/debugger\n\n$ cagent pull myusername/debugger\n\n\nUse Docker Hub or any OCI-compatible registry. Pushing creates the repository if it doesn't exist yet.\n\nWhat's next\nFollow the tutorial to build your first coding agent\nLearn best practices for building effective agents\nIntegrate cagent with your editor or use agents as tools in MCP clients\nBrowse example agent configurations in the cagent repository\nUse cagent new to generate agent teams with AI\nConnect agents to external tools via the Docker MCP Gateway\nRead the full configuration reference\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhy agent teams\nInstallation\nGet started\nHow it works\nConfiguration options\nShare agent teams\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996054,
    "timestamp": "2026-02-07T06:37:09.977Z",
    "title": "Model providers | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/model-providers/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nModel providers\nModel providers\nCopy as Markdown\n\nTo run cagent, you need a model provider. You can either use a cloud provider with an API key or run models locally with Docker Model Runner.\n\nThis guide covers cloud providers. For the local alternative, see Local models with Docker Model Runner.\n\nSupported providers\n\ncagent supports these cloud model providers:\n\nAnthropic - Claude models\nOpenAI - GPT models\nGoogle - Gemini models\nAnthropic\n\nAnthropic provides the Claude family of models, including Claude Sonnet and Claude Opus.\n\nTo get an API key:\n\nGo to console.anthropic.com.\nSign up or sign in to your account.\nNavigate to the API Keys section.\nCreate a new API key.\nCopy the key.\n\nSet your API key as an environment variable:\n\n$ export ANTHROPIC_API_KEY=your_key_here\n\n\nUse Anthropic models in your agent configuration:\n\nagents:\n\n  root:\n\n    model: anthropic/claude-sonnet-4-5\n\n    instruction: You are a helpful coding assistant\n\nAvailable models include:\n\nanthropic/claude-sonnet-4-5\nanthropic/claude-opus-4-5\nanthropic/claude-haiku-4-5\nOpenAI\n\nOpenAI provides the GPT family of models, including GPT-5 and GPT-5 mini.\n\nTo get an API key:\n\nGo to platform.openai.com/api-keys.\nSign up or sign in to your account.\nNavigate to the API Keys section.\nCreate a new API key.\nCopy the key.\n\nSet your API key as an environment variable:\n\n$ export OPENAI_API_KEY=your_key_here\n\n\nUse OpenAI models in your agent configuration:\n\nagents:\n\n  root:\n\n    model: openai/gpt-5\n\n    instruction: You are a helpful coding assistant\n\nAvailable models include:\n\nopenai/gpt-5\nopenai/gpt-5-mini\nGoogle Gemini\n\nGoogle provides the Gemini family of models.\n\nTo get an API key:\n\nGo to aistudio.google.com/apikey.\nSign in with your Google account.\nCreate an API key.\nCopy the key.\n\nSet your API key as an environment variable:\n\n$ export GOOGLE_API_KEY=your_key_here\n\n\nUse Gemini models in your agent configuration:\n\nagents:\n\n  root:\n\n    model: google/gemini-2.5-flash\n\n    instruction: You are a helpful coding assistant\n\nAvailable models include:\n\ngoogle/gemini-2.5-flash\ngoogle/gemini-2.5-pro\nOpenAI-compatible providers\n\nYou can use the openai provider type to connect to any model or provider that implements the OpenAI API specification. This includes services like Azure OpenAI, local inference servers, and other compatible endpoints.\n\nConfigure an OpenAI-compatible provider by specifying the base URL:\n\nagents:\n\n  root:\n\n    model: openai/your-model-name\n\n    instruction: You are a helpful coding assistant\n\n    provider:\n\n      base_url: https://your-provider.example.com/v1\n\nBy default, cagent uses the OPENAI_API_KEY environment variable for authentication. If your provider uses a different variable, specify it with token_key:\n\nagents:\n\n  root:\n\n    model: openai/your-model-name\n\n    instruction: You are a helpful coding assistant\n\n    provider:\n\n      base_url: https://your-provider.example.com/v1\n\n      token_key: YOUR_PROVIDER_API_KEY\nWhat's next\nFollow the tutorial to build your first agent\nLearn about local models with Docker Model Runner as an alternative to cloud providers\nReview the configuration reference for advanced model settings\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSupported providers\nAnthropic\nOpenAI\nGoogle Gemini\nOpenAI-compatible providers\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996057,
    "timestamp": "2026-02-07T06:37:09.981Z",
    "title": "Local models | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/local-models/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nLocal models\nLocal models with Docker Model Runner\nCopy as Markdown\n\nDocker Model Runner lets you run AI models locally on your machine. No API keys, no recurring costs, and your data stays private.\n\nWhy use local models\n\nDocker Model Runner lets you run models locally without API keys or recurring costs. Your data stays on your machine, and you can work offline once models are downloaded. This is an alternative to cloud model providers.\n\nPrerequisites\n\nYou need Docker Model Runner installed and running:\n\nDocker Desktop (macOS/Windows) - Enable Docker Model Runner in Settings > AI > Enable Docker Model Runner. See Get started with DMR for detailed instructions.\nDocker Engine (Linux) - Install with sudo apt-get install docker-model-plugin or sudo dnf install docker-model-plugin. See Get started with DMR.\n\nVerify Docker Model Runner is available:\n\n$ docker model version\n\n\nIf the command returns version information, you're ready to use local models.\n\nUsing models with DMR\n\nDocker Model Runner can run any compatible model. Models can come from:\n\nDocker Hub repositories (docker.io/namespace/model-name)\nYour own OCI artifacts packaged and pushed to any registry\nHuggingFace models directly (hf.co/org/model-name)\nThe Docker Model catalog in Docker Desktop\n\nTo see models available to the local Docker catalog, run:\n\n$ docker model list --openai\n\n\nTo use a model, reference it in your configuration. DMR automatically pulls models on first use if they're not already local.\n\nConfiguration\n\nConfigure your agent to use Docker Model Runner with the dmr provider:\n\nagents:\n\n  root:\n\n    model: dmr/ai/qwen3\n\n    instruction: You are a helpful assistant\n\n    toolsets:\n\n      - type: filesystem\n\nWhen you first run your agent, cagent prompts you to pull the model if it's not already available locally:\n\n$ cagent run agent.yaml\n\nModel not found locally. Do you want to pull it now? ([y]es/[n]o)\n\nHow it works\n\nWhen you configure an agent to use DMR, cagent automatically connects to your local Docker Model Runner and routes inference requests to it. If a model isn't available locally, cagent prompts you to pull it on first use. No API keys or authentication are required.\n\nAdvanced configuration\n\nFor more control over model behavior, define a model configuration:\n\nmodels:\n\n  local-qwen:\n\n    provider: dmr\n\n    model: ai/qwen3:14B\n\n    temperature: 0.7\n\n    max_tokens: 8192\n\n\n\nagents:\n\n  root:\n\n    model: local-qwen\n\n    instruction: You are a helpful coding assistant\nFaster inference with speculative decoding\n\nSpeed up model responses using speculative decoding with a smaller draft model:\n\nmodels:\n\n  fast-qwen:\n\n    provider: dmr\n\n    model: ai/qwen3:14B\n\n    provider_opts:\n\n      speculative_draft_model: ai/qwen3:0.6B-Q4_K_M\n\n      speculative_num_tokens: 16\n\n      speculative_acceptance_rate: 0.8\n\nThe draft model generates token candidates, and the main model validates them. This can significantly improve throughput for longer responses.\n\nRuntime flags\n\nPass engine-specific flags to optimize performance:\n\nmodels:\n\n  optimized-qwen:\n\n    provider: dmr\n\n    model: ai/qwen3\n\n    provider_opts:\n\n      runtime_flags: [\"--ngl=33\", \"--threads=8\"]\n\nCommon flags:\n\n--ngl - Number of GPU layers\n--threads - CPU thread count\n--repeat-penalty - Repetition penalty\nUsing DMR for RAG\n\nDocker Model Runner supports both embeddings and reranking for RAG workflows.\n\nEmbedding with DMR\n\nUse local embeddings for indexing your knowledge base:\n\nrag:\n\n  codebase:\n\n    docs: [./src]\n\n    strategies:\n\n      - type: chunked-embeddings\n\n        embedding_model: dmr/ai/embeddinggemma\n\n        database: ./code.db\nReranking with DMR\n\nDMR provides native reranking for improved RAG results:\n\nmodels:\n\n  reranker:\n\n    provider: dmr\n\n    model: hf.co/ggml-org/qwen3-reranker-0.6b-q8_0-gguf\n\n\n\nrag:\n\n  docs:\n\n    docs: [./documentation]\n\n    strategies:\n\n      - type: chunked-embeddings\n\n        embedding_model: dmr/ai/embeddinggemma\n\n        limit: 20\n\n    results:\n\n      reranking:\n\n        model: reranker\n\n        threshold: 0.5\n\n      limit: 5\n\nNative DMR reranking is the fastest option for reranking RAG results.\n\nTroubleshooting\n\nIf cagent can't find Docker Model Runner:\n\nVerify Docker Model Runner status:\n\n$ docker model status\n\n\nCheck available models:\n\n$ docker model list\n\n\nCheck model logs for errors:\n\n$ docker model logs\n\n\nEnsure Docker Desktop has Model Runner enabled in settings (macOS/Windows)\n\nWhat's next\nFollow the tutorial to build your first agent with local models\nLearn about RAG to give your agents access to codebases and documentation\nSee the configuration reference for all DMR options\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhy use local models\nPrerequisites\nUsing models with DMR\nConfiguration\nHow it works\nAdvanced configuration\nFaster inference with speculative decoding\nRuntime flags\nUsing DMR for RAG\nEmbedding with DMR\nReranking with DMR\nTroubleshooting\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996060,
    "timestamp": "2026-02-07T06:37:09.982Z",
    "title": "Building a coding agent | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/tutorial/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nBuilding a coding agent\nBuilding a coding agent\nCopy as Markdown\n\nThis tutorial teaches you how to build a coding agent that can help with software development tasks. You'll start with a basic agent and progressively add capabilities until you have a production-ready assistant that can read code, make changes, run tests, and even look up documentation.\n\nBy the end, you'll understand how to structure agent instructions, configure tools, and compose multiple agents for complex workflows.\n\nWhat you'll build\n\nA coding agent that can:\n\nRead and modify files in your project\nRun commands like tests and linters\nFollow a structured development workflow\nLook up documentation when needed\nTrack progress through multi-step tasks\nWhat you'll learn\nHow to configure cagent agents in YAML\nHow to give agents access to tools (filesystem, shell, etc.)\nHow to write effective agent instructions\nHow to compose multiple agents for specialized tasks\nHow to adapt agents for your own projects\nPrerequisites\n\nBefore starting, you need:\n\ncagent installed - See the installation guide\nAPI key configured - Set ANTHROPIC_API_KEY or OPENAI_API_KEY in your environment. Get keys from Anthropic or OpenAI\nA project to work with - Any codebase where you want agent assistance\nCreating your first agent\n\nA cagent agent is defined in a YAML configuration file. The minimal agent needs just a model and instructions that define its purpose.\n\nCreate a file named agents.yml:\n\nagents:\n\n  root:\n\n    model: openai/gpt-5\n\n    description: A basic coding assistant\n\n    instruction: |\n\n      You are a helpful coding assistant.\n\n      Help me write and understand code.\n\nRun your agent:\n\n$ cagent run agents.yml\n\n\nTry asking it: \"How do I read a file in Python?\"\n\nThe agent can answer coding questions, but it can't see your files or run commands yet. To make it useful for real development work, it needs access to tools.\n\nAdding tools\n\nA coding agent needs to interact with your project files and run commands. You enable these capabilities by adding toolsets.\n\nUpdate agents.yml to add filesystem and shell access:\n\nagents:\n\n  root:\n\n    model: openai/gpt-5\n\n    description: A coding assistant with filesystem access\n\n    instruction: |\n\n      You are a helpful coding assistant.\n\n      You can read and write files to help me develop software.\n\n      Always check if code works before finishing a task.\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\nRun the updated agent and try: \"Read the README.md file and summarize it.\"\n\nYour agent can now:\n\nRead and write files in the current directory\nExecute shell commands\nExplore your project structure\nNote\n\ndirectory. The agent will request permission if it needs to access other directories.\n\nThe agent can now interact with your code, but its behavior is still generic. Next, you'll teach it how to work effectively.\n\nStructuring agent instructions\n\nGeneric instructions produce generic results. For production use, you want your agent to follow a specific workflow and understand your project's conventions.\n\nUpdate your agent with structured instructions. This example shows a Go development agent, but you can adapt the pattern for any language:\n\nagents:\n\n  root:\n\n    model: anthropic/claude-sonnet-4-5\n\n    description: Expert Go developer\n\n    instruction: |\n\n      Your goal is to help with code-related tasks by examining, modifying,\n\n      and validating code changes.\n\n\n\n      TASK\n\n          # Workflow:\n\n          # 1. Analyze: Understand requirements and identify relevant code.\n\n          # 2. Examine: Search for files, analyze structure and dependencies.\n\n          # 3. Modify: Make changes following best practices.\n\n          # 4. Validate: Run linters/tests. If issues found, return to Modify.\n\n      </TASK>\n\n\n\n      Constraints:\n\n      - Be thorough in examination before making changes\n\n      - Always validate changes before considering the task complete\n\n      - Write code to files, don't show it in chat\n\n\n\n      ## Development Workflow\n\n      - `go build ./...` - Build the application\n\n      - `go test ./...` - Run tests\n\n      - `golangci-lint run` - Check code quality\n\n\n\n    add_date: true\n\n    add_environment_info: true\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\n      - type: todo\n\nTry asking: \"Add error handling to the parseConfig function in main.go\"\n\nThe structured instructions give your agent:\n\nA clear workflow to follow (analyze, examine, modify, validate)\nProject-specific commands to run\nConstraints that prevent common mistakes\nContext about the environment (add_date and add_environment_info)\n\nThe todo toolset helps the agent track progress through multi-step tasks. When you ask for complex changes, the agent will break down the work and update its progress as it goes.\n\nComposing multiple agents\n\nComplex tasks often benefit from specialized agents. You can add sub-agents that handle specific responsibilities, like researching documentation while your main agent stays focused on coding.\n\nAdd a librarian agent that can search for documentation:\n\nagents:\n\n  root:\n\n    model: anthropic/claude-sonnet-4-5\n\n    description: Expert Go developer\n\n    instruction: |\n\n      Your goal is to help with code-related tasks by examining, modifying,\n\n      and validating code changes.\n\n\n\n      When you need to look up documentation or research how something works,\n\n      ask the librarian agent.\n\n\n\n      (rest of instructions from previous section...)\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\n      - type: todo\n\n    sub_agents:\n\n      - librarian\n\n\n\n  librarian:\n\n    model: anthropic/claude-haiku-4-5\n\n    description: Documentation researcher\n\n    instruction: |\n\n      You are the librarian. Your job is to find relevant documentation,\n\n      articles, or resources to help the developer agent.\n\n\n\n      Search the internet and fetch web pages as needed.\n\n    toolsets:\n\n      - type: mcp\n\n        ref: docker:duckduckgo\n\n      - type: fetch\n\nTry asking: \"How do I use context.Context in Go? Then add it to my server code.\"\n\nYour main agent will delegate the research to the librarian, then use that information to modify your code. This keeps the main agent's context focused on the coding task while still having access to up-to-date documentation.\n\nUsing a smaller, faster model (Haiku) for the librarian saves costs since documentation lookup doesn't need the same reasoning depth as code changes.\n\nAdapting for your project\n\nNow that you understand the core concepts, adapt the agent for your specific project:\n\nUpdate the development commands\n\nReplace the Go commands with your project's workflow:\n\n## Development Workflow\n\n- `npm test` - Run tests\n\n- `npm run lint` - Check code quality\n\n- `npm run build` - Build the application\nAdd project-specific constraints\n\nIf your agent keeps making the same mistakes, add explicit constraints:\n\nConstraints:\n\n  - Always run tests before considering a task complete\n\n  - Follow the existing code style in src/ directories\n\n  - Never modify files in the generated/ directory\n\n  - Use TypeScript strict mode for new files\nChoose the right models\n\nFor coding tasks, use reasoning-focused models:\n\nanthropic/claude-sonnet-4-5 - Strong reasoning, good for complex code\nopenai/gpt-5 - Fast, good general coding ability\n\nFor auxiliary tasks like documentation lookup, smaller models work well:\n\nanthropic/claude-haiku-4-5 - Fast and cost-effective\nopenai/gpt-5-mini - Good for simple tasks\nIterate based on usage\n\nThe best way to improve your agent is to use it. When you notice issues:\n\nAdd specific instructions to prevent the problem\nUpdate constraints to guide behavior\nAdd relevant commands to the development workflow\nConsider adding specialized sub-agents for complex areas\nWhat you learned\n\nYou now know how to:\n\nCreate a basic cagent configuration\nAdd tools to enable agent capabilities\nWrite structured instructions for consistent behavior\nCompose multiple agents for specialized tasks\nAdapt agents for different programming languages and workflows\nNext steps\nLearn best practices for handling large outputs, structuring agent teams, and optimizing performance\nIntegrate cagent with your editor or use agents as tools in MCP clients\nReview the Configuration reference for all available options\nExplore the Tools reference to see what capabilities you can enable\nCheck out example configurations for different use cases\nSee the full golang_developer.yaml that the Docker team uses to develop cagent\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat you'll build\nWhat you'll learn\nPrerequisites\nCreating your first agent\nAdding tools\nStructuring agent instructions\nComposing multiple agents\nAdapting for your project\nUpdate the development commands\nAdd project-specific constraints\nChoose the right models\nIterate based on usage\nWhat you learned\nNext steps\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996063,
    "timestamp": "2026-02-07T06:37:09.987Z",
    "title": "Best practices | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/best-practices/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nBest practices\nBest practices\nCopy as Markdown\n\nPatterns you learn from building and running cagent agents. These aren't features or configuration options - they're approaches that work well in practice.\n\nHandling large command outputs\n\nShell commands that produce large output can overflow your agent's context window. Validation tools, test suites, and build logs often generate thousands of lines. If you capture this output directly, it consumes all available context and the agent fails.\n\nThe solution: redirect output to a file, then read the file. The Read tool automatically truncates large files to 2000 lines, and your agent can navigate through it if needed.\n\nDon't do this:\n\nreviewer:\n\n  instruction: |\n\n    Run validation: `docker buildx bake validate`\n\n    Check the output for errors.\n\n  toolsets:\n\n    - type: shell\n\nThe validation output goes directly into context. If it's large, the agent fails with a context overflow error.\n\nDo this:\n\nreviewer:\n\n  instruction: |\n\n    Run validation and save output:\n\n    `docker buildx bake validate > validation.log 2>&1`\n\n\n\n    Read validation.log to check for errors.\n\n    The file can be large - read the first 2000 lines.\n\n    Errors usually appear at the beginning.\n\n  toolsets:\n\n    - type: filesystem\n\n    - type: shell\n\nThe output goes to a file, not context. The agent reads what it needs using the filesystem toolset.\n\nStructuring agent teams\n\nA single agent handling multiple responsibilities makes instructions complex and behavior unpredictable. Breaking work across specialized agents produces better results.\n\nThe coordinator pattern works well: a root agent understands the overall task and delegates to specialists. Each specialist focuses on one thing.\n\nExample: Documentation writing team\n\nagents:\n\n  root:\n\n    description: Technical writing coordinator\n\n    instruction: |\n\n      Coordinate documentation work:\n\n      1. Delegate to writer for content creation\n\n      2. Delegate to editor for formatting polish\n\n      3. Delegate to reviewer for validation\n\n      4. Loop back through editor if reviewer finds issues\n\n    sub_agents: [writer, editor, reviewer]\n\n    toolsets: [filesystem, todo]\n\n\n\n  writer:\n\n    description: Creates and edits documentation content\n\n    instruction: |\n\n      Write clear, practical documentation.\n\n      Focus on content quality - the editor handles formatting.\n\n    toolsets: [filesystem, think]\n\n\n\n  editor:\n\n    description: Polishes formatting and style\n\n    instruction: |\n\n      Fix formatting issues, wrap lines, run prettier.\n\n      Remove AI-isms and polish style.\n\n      Don't change meaning or add content.\n\n    toolsets: [filesystem, shell]\n\n\n\n  reviewer:\n\n    description: Runs validation tools\n\n    instruction: |\n\n      Run validation suite, report failures.\n\n    toolsets: [filesystem, shell]\n\nEach agent has clear responsibilities. The writer doesn't worry about line wrapping. The editor doesn't generate content. The reviewer just runs tools.\n\nThis example uses sub_agents where root delegates discrete tasks and gets results back. The root agent maintains control and coordinates all work. For different coordination patterns where agents should transfer control to each other, see the handoffs mechanism in the configuration reference.\n\nWhen to use teams:\n\nMultiple distinct steps in your workflow\nDifferent skills required (writing ‚Üî editing ‚Üî testing)\nOne step might need to retry based on later feedback\n\nWhen to use a single agent:\n\nSimple, focused tasks\nAll work happens in one step\nAdding coordination overhead doesn't help\nOptimizing RAG performance\n\nRAG indexing takes time when you have many files. A configuration that indexes your entire codebase might take minutes to start. Optimize for what your agent actually needs.\n\nNarrow the scope:\n\nDon't index everything. Index what's relevant for the agent's work.\n\n# Too broad - indexes entire codebase\n\nrag:\n\n  codebase:\n\n    docs: [./]\n\n\n\n# Better - indexes only relevant directories\n\nrag:\n\n  codebase:\n\n    docs: [./src/api, ./docs, ./examples]\n\nIf your agent only works with API code, don't index tests, vendor directories, or generated files.\n\nIncrease batching and concurrency:\n\nProcess more chunks per API call and make parallel requests.\n\nstrategies:\n\n  - type: chunked-embeddings\n\n    embedding_model: openai/text-embedding-3-small\n\n    batch_size: 50 # More chunks per API call\n\n    max_embedding_concurrency: 10 # Parallel requests\n\n    chunking:\n\n      size: 2000 # Larger chunks = fewer total chunks\n\n      overlap: 150\n\nThis reduces both API calls and indexing time.\n\nConsider BM25 for fast local search:\n\nIf you need exact term matching (function names, error messages, identifiers), BM25 is fast and runs locally without API calls.\n\nstrategies:\n\n  - type: bm25\n\n    database: ./bm25.db\n\n    chunking:\n\n      size: 1500\n\nCombine with embeddings using hybrid retrieval when you need both semantic understanding and exact matching.\n\nPreserving document scope\n\nWhen building agents that update documentation, a common problem: the agent transforms minimal guides into tutorials. It adds prerequisites, troubleshooting, best practices, examples, and detailed explanations to everything.\n\nThese additions might individually be good, but they change the document's character. A focused 90-line how-to becomes a 200-line reference.\n\nBuild this into instructions:\n\nwriter:\n\n  instruction: |\n\n    When updating documentation:\n\n\n\n    1. Understand the current document's scope and length\n\n    2. Match that character - don't transform minimal guides into tutorials\n\n    3. Add only what's genuinely missing\n\n    4. Value brevity - not every topic needs comprehensive coverage\n\n\n\n    Good additions fill gaps. Bad additions change the document's character.\n\n    When in doubt, add less rather than more.\n\nTell your agents explicitly to preserve the existing document's scope. Without this guidance, they default to being comprehensive.\n\nModel selection\n\nChoose models based on the agent's role and complexity.\n\nUse larger models (Sonnet, GPT-5) for:\n\nComplex reasoning and planning\nWriting and editing content\nCoordinating multiple agents\nTasks requiring judgment and creativity\n\nUse smaller models (Haiku, GPT-5 Mini) for:\n\nRunning validation tools\nSimple structured tasks\nReading logs and reporting errors\nHigh-volume, low-complexity work\n\nExample from the documentation writing team:\n\nagents:\n\n  root:\n\n    model: anthropic/claude-sonnet-4-5 # Complex coordination\n\n  writer:\n\n    model: anthropic/claude-sonnet-4-5 # Creative content work\n\n  editor:\n\n    model: anthropic/claude-sonnet-4-5 # Judgment about style\n\n  reviewer:\n\n    model: anthropic/claude-haiku-4-5 # Just runs validation\n\nThe reviewer uses Haiku because it runs commands and checks for errors. No complex reasoning needed, and Haiku is faster and cheaper.\n\nWhat's next\nReview configuration reference for all available options\nCheck toolsets reference to understand what tools agents can use\nSee example configurations for complete working agents\nRead the RAG guide for detailed retrieval optimization\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHandling large command outputs\nStructuring agent teams\nOptimizing RAG performance\nPreserving document scope\nModel selection\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996066,
    "timestamp": "2026-02-07T06:37:09.994Z",
    "title": "Sharing agents | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/sharing-agents/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nSharing agents\nSharing agents\nCopy as Markdown\n\nPush your agent to a registry and share it by name. Your teammates reference agentcatalog/security-expert instead of copying YAML files around or asking you where your agent configuration lives.\n\nWhen you update the agent in the registry, everyone gets the new version the next time they pull or restart their client.\n\nPrerequisites\n\nTo push agents to a registry, authenticate first:\n\n$ docker login\n\n\nFor other registries, use their authentication method.\n\nPublishing agents\n\nPush your agent configuration to a registry:\n\n$ cagent push ./agent.yml myusername/agent-name\n\n\nPush creates the repository if it doesn't exist yet. Use Docker Hub or any OCI-compatible registry.\n\nTag specific versions:\n\n$ cagent push ./agent.yml myusername/agent-name:v1.0.0\n\n$ cagent push ./agent.yml myusername/agent-name:latest\n\nUsing published agents\n\nPull an agent to inspect it locally:\n\n$ cagent pull agentcatalog/pirate\n\n\nThis saves the configuration as a local YAML file.\n\nRun agents directly from the registry:\n\n$ cagent run agentcatalog/pirate\n\n\nOr reference it directly in integrations:\n\nEditor integration (ACP)\n\nUse registry references in ACP configurations so your editor always uses the latest version:\n\n{\n\n  \"agent_servers\": {\n\n    \"myagent\": {\n\n      \"command\": \"cagent\",\n\n      \"args\": [\"acp\", \"agentcatalog/pirate\"]\n\n    }\n\n  }\n\n}\nMCP client integration\n\nAgents can be exposed as tools in MCP clients:\n\n{\n\n  \"mcpServers\": {\n\n    \"myagent\": {\n\n      \"command\": \"/usr/local/bin/cagent\",\n\n      \"args\": [\"mcp\", \"agentcatalog/pirate\"]\n\n    }\n\n  }\n\n}\nWhat's next\nSet up ACP integration with shared agents\nConfigure MCP integration with shared agents\nBrowse the agent catalog for examples\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nPublishing agents\nUsing published agents\nEditor integration (ACP)\nMCP client integration\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996067,
    "timestamp": "2026-02-07T06:37:09.994Z",
    "title": "Integrations | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/integrations/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nA2A\nACP\nMCP\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nIntegrations\nIntegrations\nCopy as Markdown\n\ncagent agents can integrate with different environments depending on how you want to use them. Each integration type serves a specific purpose.\n\nIntegration types\nACP - Editor integration\n\nRun cagent agents directly in your editor (Neovim, Zed). The agent sees your editor's file context and can read and modify files through the editor's interface. Use ACP when you want an AI coding assistant embedded in your editor.\n\nSee ACP integration for setup instructions.\n\nMCP - Tool integration\n\nExpose cagent agents as tools in MCP clients like Claude Desktop or Claude Code. Your agents appear in the client's tool list, and the client can call them when needed. Use MCP when you want Claude Desktop (or another MCP client) to have access to your specialized agents.\n\nSee MCP integration for setup instructions.\n\nA2A - Agent-to-agent communication\n\nRun cagent agents as HTTP servers that other agents or systems can call using the Agent-to-Agent protocol. Your agent becomes a service that other systems can discover and invoke over the network. Use A2A when you want to build multi-agent systems or expose your agent as an HTTP service.\n\nSee A2A integration for setup instructions.\n\nChoosing the right integration\nFeature\tACP\tMCP\tA2A\nUse case\tEditor integration\tAgents as tools\tAgent-to-agent calls\nTransport\tstdio\tstdio/SSE\tHTTP\nDiscovery\tEditor plugin\tServer manifest\tAgent card\nBest for\tCode editing\tTool integration\tMulti-agent systems\nCommunication\tEditor calls agent\tClient calls tools\tBetween agents\n\nChoose ACP if you want your agent embedded in your editor while you code. Choose MCP if you want Claude Desktop (or another MCP client) to be able to call your specialized agents as tools. Choose A2A if you're building multi-agent systems where agents need to call each other over HTTP.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nIntegration types\nACP - Editor integration\nMCP - Tool integration\nA2A - Agent-to-agent communication\nChoosing the right integration\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996072,
    "timestamp": "2026-02-07T06:37:10.000Z",
    "title": "A2A | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/integrations/a2a/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nA2A\nACP\nMCP\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nIntegrations\n/\nA2A\nA2A mode\nCopy as Markdown\n\nA2A mode runs your cagent agent as an HTTP server that other systems can call using the Agent-to-Agent protocol. This lets you expose your agent as a service that other agents or applications can discover and invoke over the network.\n\nUse A2A when you want to make your agent callable by other systems over HTTP. For editor integration, see ACP integration. For using agents as tools in MCP clients, see MCP integration.\n\nPrerequisites\n\nBefore starting an A2A server, you need:\n\ncagent installed - See the installation guide\nAgent configuration - A YAML file defining your agent. See the tutorial\nAPI keys configured - If using cloud model providers (see Model providers)\nStarting an A2A server\n\nBasic usage:\n\n$ cagent a2a ./agent.yaml\n\n\nYour agent is now accessible via HTTP. Other A2A systems can discover your agent's capabilities and call it.\n\nCustom port:\n\n$ cagent a2a ./agent.yaml --port 8080\n\n\nSpecific agent in a team:\n\n$ cagent a2a ./agent.yaml --agent engineer\n\n\nFrom OCI registry:\n\n$ cagent a2a agentcatalog/pirate --port 9000\n\nHTTP endpoints\n\nWhen you start an A2A server, it exposes two HTTP endpoints:\n\nAgent card: /.well-known/agent-card\n\nThe agent card describes your agent's capabilities:\n\n$ curl http://localhost:8080/.well-known/agent-card\n\n{\n\n  \"name\": \"agent\",\n\n  \"description\": \"A helpful coding assistant\",\n\n  \"skills\": [\n\n    {\n\n      \"id\": \"agent_root\",\n\n      \"name\": \"root\",\n\n      \"description\": \"A helpful coding assistant\",\n\n      \"tags\": [\"llm\", \"cagent\"]\n\n    }\n\n  ],\n\n  \"preferredTransport\": \"jsonrpc\",\n\n  \"url\": \"http://localhost:8080/invoke\",\n\n  \"capabilities\": {\n\n    \"streaming\": true\n\n  },\n\n  \"version\": \"0.1.0\"\n\n}\nInvoke endpoint: /invoke\n\nCall your agent by sending a JSON-RPC request:\n\n$ curl -X POST http://localhost:8080/invoke \\\n\n  -H \"Content-Type: application/json\" \\\n\n  -d '{\n\n    \"jsonrpc\": \"2.0\",\n\n    \"id\": \"req-1\",\n\n    \"method\": \"message/send\",\n\n    \"params\": {\n\n      \"message\": {\n\n        \"role\": \"user\",\n\n        \"parts\": [\n\n          {\n\n            \"kind\": \"text\",\n\n            \"text\": \"What is 2+2?\"\n\n          }\n\n        ]\n\n      }\n\n    }\n\n  }'\n\n\nThe response includes the agent's reply:\n\n{\n\n  \"jsonrpc\": \"2.0\",\n\n  \"id\": \"req-1\",\n\n  \"result\": {\n\n    \"artifacts\": [\n\n      {\n\n        \"parts\": [\n\n          {\n\n            \"kind\": \"text\",\n\n            \"text\": \"2+2 equals 4.\"\n\n          }\n\n        ]\n\n      }\n\n    ]\n\n  }\n\n}\nExample: Multi-agent workflow\n\nHere's a concrete scenario where A2A is useful. You have two agents:\n\nA general-purpose agent that interacts with users\nA specialized code review agent with access to your codebase\n\nRun the specialist as an A2A server:\n\n$ cagent a2a ./code-reviewer.yaml --port 8080\n\nListening on 127.0.0.1:8080\n\n\nConfigure your main agent to call it:\n\nagents:\n\n  root:\n\n    model: anthropic/claude-sonnet-4-5\n\n    instruction: You are a helpful assistant\n\n    toolsets:\n\n      - type: a2a\n\n        url: http://localhost:8080\n\n        name: code-reviewer\n\nNow when users ask the main agent about code quality, it can delegate to the specialist. The main agent sees code-reviewer as a tool it can call, and the specialist has access to the codebase tools it needs.\n\nCalling other A2A agents\n\nYour cagent agents can call remote A2A agents as tools. Configure the A2A toolset with the remote agent's URL:\n\nagents:\n\n  root:\n\n    toolsets:\n\n      - type: a2a\n\n        url: http://localhost:8080\n\n        name: specialist\n\nThe url specifies where the remote agent is running, and name is an optional identifier for the tool. Your agent can now delegate tasks to the remote specialist agent.\n\nIf the remote agent requires authentication or custom headers:\n\nagents:\n\n  root:\n\n    toolsets:\n\n      - type: a2a\n\n        url: http://localhost:8080\n\n        name: specialist\n\n        remote:\n\n          headers:\n\n            Authorization: Bearer token123\n\n            X-Custom-Header: value\nWhat's next\nReview the CLI reference for all cagent a2a options\nLearn about MCP mode to expose agents as tools in MCP clients\nLearn about ACP mode for editor integration\nShare your agents with OCI registries\n\nEdit this page\n\nRequest changes\n\nTable of contents\nPrerequisites\nStarting an A2A server\nHTTP endpoints\nAgent card: /.well-known/agent-card\nInvoke endpoint: /invoke\nExample: Multi-agent workflow\nCalling other A2A agents\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996078,
    "timestamp": "2026-02-07T06:37:10.013Z",
    "title": "MCP | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/integrations/mcp/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nA2A\nACP\nMCP\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nIntegrations\n/\nMCP\nMCP mode\nCopy as Markdown\n\nWhen you run cagent in MCP mode, your agents show up as tools in Claude Desktop and other MCP clients. Instead of switching to a terminal to run your security agent, you ask Claude to use it and Claude calls it for you.\n\nThis guide covers setup for Claude Desktop and Claude Code. If you want agents embedded in your editor instead, see ACP integration.\n\nHow it works\n\nYou configure Claude Desktop (or another MCP client) to connect to cagent. Your agents appear in Claude's tool list. When you ask Claude to use one, it calls that agent through the MCP protocol.\n\nSay you have a security agent configured. Ask Claude Desktop \"Use the security agent to audit this authentication code\" and Claude calls it. The agent runs with its configured tools (filesystem, shell, whatever you gave it), then returns results to Claude.\n\nIf your configuration has multiple agents, each one becomes a separate tool. A config with root, designer, and engineer agents gives Claude three tools to choose from. Claude might call the engineer directly or use the root coordinator‚Äîdepends on your agent descriptions and what you ask for.\n\nMCP Gateway\n\nDocker provides an MCP Gateway that gives cagent agents access to a catalog of pre-configured MCP servers. Instead of configuring individual MCP servers, agents can use the gateway to access tools like web search, database queries, and more.\n\nConfigure MCP toolset with gateway reference:\n\nagents:\n\n  root:\n\n    toolsets:\n\n      - type: mcp\n\n        ref: docker:duckduckgo # Uses Docker MCP Gateway\n\nThe docker: prefix tells cagent to use the MCP Gateway for this server. See the MCP Gateway documentation for available servers and configuration options.\n\nYou can also use the MCP Toolkit to explore and manage MCP servers interactively.\n\nPrerequisites\n\nBefore configuring MCP integration, you need:\n\ncagent installed - See the installation guide\nAgent configuration - A YAML file defining your agent. See the tutorial or example configurations\nMCP client - Claude Desktop, Claude Code, or another MCP-compatible application\nAPI keys - Environment variables for any model providers your agents use (ANTHROPIC_API_KEY, OPENAI_API_KEY, etc.)\nMCP client configuration\n\nYour MCP client needs to know how to start cagent and communicate with it. This typically involves adding cagent as an MCP server in your client's configuration.\n\nClaude Desktop\n\nAdd cagent to your Claude Desktop MCP settings file:\n\nmacOS: ~/Library/Application Support/Claude/claude_desktop_config.json\nWindows: %APPDATA%\\Claude\\claude_desktop_config.json\n\nExample configuration:\n\n{\n\n  \"mcpServers\": {\n\n    \"myagent\": {\n\n      \"command\": \"/usr/local/bin/cagent\",\n\n      \"args\": [\n\n        \"mcp\",\n\n        \"/path/to/agent.yml\",\n\n        \"--working-dir\",\n\n        \"/Users/yourname/projects\"\n\n      ],\n\n      \"env\": {\n\n        \"ANTHROPIC_API_KEY\": \"your_anthropic_key_here\",\n\n        \"OPENAI_API_KEY\": \"your_openai_key_here\"\n\n      }\n\n    }\n\n  }\n\n}\n\nConfiguration breakdown:\n\ncommand: Full path to your cagent binary (use which cagent to find it)\nargs: MCP command arguments:\nmcp: The subcommand to run cagent in MCP mode\ndockereng/myagent: Your agent configuration (local file path or OCI reference)\n--working-dir: Optional working directory for agent execution\nenv: Environment variables your agents need:\nModel provider API keys (ANTHROPIC_API_KEY, OPENAI_API_KEY, etc.)\nAny other environment variables your agents reference\n\nAfter updating the configuration, restart Claude Desktop. Your agents will appear as available tools.\n\nClaude Code\n\nAdd cagent as an MCP server using the claude mcp add command:\n\n$ claude mcp add --transport stdio myagent \\\n\n  --env OPENAI_API_KEY=$OPENAI_API_KEY \\\n\n  --env ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \\\n\n  -- cagent mcp /path/to/agent.yml --working-dir $(pwd)\n\n\nCommand breakdown:\n\nclaude mcp add: Claude Code command to register an MCP server\n--transport stdio: Use stdio transport (standard for local MCP servers)\nmyagent: Name for this MCP server in Claude Code\n--env: Pass environment variables (repeat for each variable)\n--: Separates Claude Code options from the MCP server command\ncagent mcp /path/to/agent.yml: The cagent MCP command with the path to your agent configuration\n--working-dir $(pwd): Set the working directory for agent execution\n\nAfter adding the server, your agents will be available as tools in Claude Code sessions.\n\nOther MCP clients\n\nFor other MCP-compatible clients, you need to:\n\nStart cagent with cagent mcp /path/to/agent.yml --working-dir /project/path\nConfigure the client to communicate with cagent over stdio\nPass required environment variables (API keys, etc.)\n\nConsult your MCP client's documentation for specific configuration steps.\n\nAgent references\n\nYou can specify your agent configuration as a local file path or OCI registry reference:\n\n# Local file path\n\n$ cagent mcp ./agent.yml\n\n\n\n# OCI registry reference\n\n$ cagent mcp agentcatalog/pirate\n\n$ cagent mcp dockereng/myagent:v1.0.0\n\n\nUse the same syntax in MCP client configurations:\n\n{\n\n  \"mcpServers\": {\n\n    \"myagent\": {\n\n      \"command\": \"/usr/local/bin/cagent\",\n\n      \"args\": [\"mcp\", \"agentcatalog/pirate\"]\n\n    }\n\n  }\n\n}\n\nRegistry references let your team use the same agent configuration without managing local files. See Sharing agents for details.\n\nDesigning agents for MCP\n\nMCP clients see each of your agents as a separate tool and can call any of them directly. This changes how you should think about agent design compared to running agents with cagent run.\n\nWrite good descriptions\n\nThe description field tells the MCP client what the agent does. This is how the client decides when to call it. \"Analyzes code for security vulnerabilities and compliance issues\" is specific. \"A helpful security agent\" doesn't say what it actually does.\n\nagents:\n\n  security_auditor:\n\n    description: Analyzes code for security vulnerabilities and compliance issues\n\n    # Not: \"A helpful security agent\"\nMCP clients call agents directly\n\nThe MCP client can call any of your agents, not just root. If you have root, designer, and engineer agents, the client might call the engineer directly instead of going through root. Design each agent to work on its own:\n\nagents:\n\n  engineer:\n\n    description: Implements features and writes production code\n\n    instruction: |\n\n      You implement code based on requirements provided.\n\n      You can work independently without a coordinator.\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\nIf an agent needs others to work properly, say so in the description: \"Coordinates design and engineering agents to implement complete features.\"\n\nTest each agent on its own\n\nMCP clients call agents individually, so test them that way:\n\n$ cagent run agent.yml --agent engineer\n\n\nMake sure the agent works without going through root first. Check that it has the right tools and that its instructions make sense when it's called directly.\n\nTesting your setup\n\nVerify your MCP integration works:\n\nRestart your MCP client after configuration changes\nCheck that cagent agents appear as available tools\nInvoke an agent with a simple test prompt\nVerify the agent can access its configured tools (filesystem, shell, etc.)\n\nIf agents don't appear or fail to execute, check:\n\ncagent binary path is correct and executable\nAgent configuration file exists and is valid\nAll required API keys are set in environment variables\nWorking directory path exists and has appropriate permissions\nMCP client logs for connection or execution errors\nCommon workflows\nCall specialist agents\n\nYou have a security agent that knows your compliance rules and common vulnerabilities. In Claude Desktop, paste some authentication code and ask \"Use the security agent to review this.\" The agent checks the code and reports what it finds. You stay in Claude's interface the whole time.\n\nWork with agent teams\n\nYour configuration has a coordinator that delegates to designer and engineer agents. Ask Claude Code \"Use the coordinator to implement a login form\" and the coordinator hands off UI work to the designer and code to the engineer. You get a complete implementation without running cagent run yourself.\n\nRun domain-specific tools\n\nYou built an infrastructure agent with custom deployment scripts and monitoring queries. Ask any MCP client \"Use the infra agent to check production status\" and it runs your tools and returns results. Your deployment knowledge is now available wherever you use MCP clients.\n\nShare agents\n\nYour team keeps agents in an OCI registry. Everyone adds agentcatalog/security-expert to their MCP client config. When you update the agent, they get the new version on their next restart. No YAML files to pass around.\n\nWhat's next\nUse the MCP Gateway to give your agents access to pre-configured MCP servers\nExplore MCP servers interactively with the MCP Toolkit\nReview the configuration reference for advanced agent setup\nExplore the toolsets reference to learn what tools agents can use\nAdd RAG for codebase search to your agent\nCheck the CLI reference for all cagent mcp options\nBrowse example configurations for different agent types\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow it works\nMCP Gateway\nPrerequisites\nMCP client configuration\nClaude Desktop\nClaude Code\nOther MCP clients\nAgent references\nDesigning agents for MCP\nWrite good descriptions\nMCP clients call agents directly\nTest each agent on its own\nTesting your setup\nCommon workflows\nCall specialist agents\nWork with agent teams\nRun domain-specific tools\nShare agents\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996075,
    "timestamp": "2026-02-07T06:37:10.014Z",
    "title": "ACP | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/integrations/acp/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nA2A\nACP\nMCP\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nIntegrations\n/\nACP\nACP integration\nCopy as Markdown\n\nRun cagent agents directly in your editor using the Agent Client Protocol (ACP). Your agent gets access to your editor's filesystem context and can read and modify files as you work. The editor handles file operations while cagent provides the AI capabilities.\n\nThis guide shows you how to configure Neovim, or Zed to run cagent agents. If you're looking to expose cagent agents as tools to MCP clients like Claude Desktop or Claude Code, see MCP integration instead.\n\nHow it works\n\nWhen you run cagent with ACP, it becomes part of your editor's environment. You select code, highlight a function, or reference a file - the agent sees what you see. No copying file paths or switching to a terminal.\n\nAsk \"explain this function\" and the agent reads the file you're viewing. Ask it to \"add error handling\" and it edits the code right in your editor. The agent works with your editor's view of the project, not some external file system it has to navigate.\n\nThe difference from running cagent in a terminal: file operations go through your editor instead of the agent directly accessing your filesystem. When the agent needs to read or write a file, it requests it from your editor. This keeps the agent's view of your code synchronized with yours - same working directory, same files, same state.\n\nPrerequisites\n\nBefore configuring your editor, you need:\n\ncagent installed - See the installation guide\nAgent configuration - A YAML file defining your agent. See the tutorial or example configurations\nEditor with ACP support - Neovim, Intellij, Zed, etc.\n\nYour agents will use model provider API keys from your shell environment (ANTHROPIC_API_KEY, OPENAI_API_KEY, etc.). Make sure these are set before launching your editor.\n\nEditor configuration\nZed\n\nZed has built-in ACP support.\n\nAdd cagent to your agent servers in settings.json:\n\n{\n\n  \"agent_servers\": {\n\n    \"my-cagent-team\": {\n\n      \"command\": \"cagent\",\n\n      \"args\": [\"acp\", \"agent.yml\"]\n\n    }\n\n  }\n\n}\n\nReplace:\n\nmy-cagent-team with the name you want to use for the agent\nagent.yml with the path to your agent configuration file.\n\nIf you have multiple agent files that you like to run separately, you can create multiple entries under agent_servers for each agent.\n\nStart a new external agent thread. Select your agent in the drop-down list.\n\nNeovim\n\nUse the CodeCompanion plugin, which has native support for cagent through a built-in adapter:\n\nInstall CodeCompanion through your plugin manager.\n\nExtend the cagent adapter in your CodeCompanion config:\n\nrequire(\"codecompanion\").setup({\n\n  adapters = {\n\n    acp = {\n\n      cagent = function()\n\n        return require(\"codecompanion.adapters\").extend(\"cagent\", {\n\n          commands = {\n\n            default = {\n\n              \"cagent\",\n\n              \"acp\",\n\n              \"agent.yml\",\n\n            },\n\n          },\n\n        })\n\n      end,\n\n    },\n\n  },\n\n})\n\nReplace agent.yml with the path to your agent configuration file. If you have multiple agent files that you like to run separately, you can create multiple commands for each agent.\n\nRestart Neovim and launch CodeCompanion:\n\n:CodeCompanion\n\nSwitch to the cagent adapter (keymap ga in the CodeCompanion buffer, by default).\n\nSee the CodeCompanion ACP documentation for more information about ACP support in CodeCompanion. Note that terminal operations are not supported, so toolsets like shell or script_shell are not usable through CodeCompanion.\n\nAgent references\n\nYou can specify your agent configuration as a local file path or OCI registry reference:\n\n# Local file path\n\n$ cagent acp ./agent.yml\n\n\n\n# OCI registry reference\n\n$ cagent acp agentcatalog/pirate\n\n$ cagent acp dockereng/myagent:v1.0.0\n\n\nUse the same syntax in your editor configuration:\n\n{\n\n  \"agent_servers\": {\n\n    \"myagent\": {\n\n      \"command\": \"cagent\",\n\n      \"args\": [\"acp\", \"agentcatalog/pirate\"]\n\n    }\n\n  }\n\n}\n\nRegistry references enable team sharing, version management, and clean configuration without local file paths. See Sharing agents for details on using OCI registries.\n\nTesting your setup\n\nVerify your configuration works:\n\nStart the cagent ACP server using your editor's configured method\nSend a test prompt through your editor's interface\nCheck that the agent responds\nVerify filesystem operations work by asking the agent to read a file\n\nIf the agent starts but can't access files or perform other actions, check:\n\nWorking directory in your editor is set correctly to your project root\nAgent configuration file path is absolute or relative to working directory\nYour editor or plugin properly implements ACP protocol features\nWhat's next\nReview the configuration reference for advanced agent setup\nExplore the toolsets reference to learn what tools are available\nAdd RAG for codebase search to your agent\nCheck the CLI reference for all cagent acp options\nBrowse example configurations for inspiration\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow it works\nPrerequisites\nEditor configuration\nZed\nNeovim\nAgent references\nTesting your setup\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996081,
    "timestamp": "2026-02-07T06:37:10.018Z",
    "title": "Configuration file | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/reference/config/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nConfiguration file\nToolsets\nCLI\nExamples\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nReference\n/\nConfiguration file\nConfiguration file reference\nCopy as Markdown\n\nThis reference documents the YAML configuration file format for cagent agents. It covers file structure, agent parameters, model configuration, toolset setup, and RAG sources.\n\nFor detailed documentation of each toolset's capabilities and specific options, see the Toolsets reference.\n\nFile structure\n\nA configuration file has four top-level sections:\n\nagents: # Required - agent definitions\n\n  root:\n\n    model: anthropic/claude-sonnet-4-5\n\n    description: What this agent does\n\n    instruction: How it should behave\n\n\n\nmodels: # Optional - model configurations\n\n  custom_model:\n\n    provider: openai\n\n    model: gpt-5\n\n\n\nrag: # Optional - RAG sources\n\n  docs:\n\n    docs: [./documents]\n\n    strategies: [...]\n\n\n\nmetadata: # Optional - author, license, readme\n\n  author: Your Name\nAgents\nProperty\tType\tDescription\tRequired\nmodel\tstring\tModel reference or name\tYes\ndescription\tstring\tBrief description of agent's purpose\tNo\ninstruction\tstring\tDetailed behavior instructions\tYes\nsub_agents\tarray\tAgent names for task delegation\tNo\nhandoffs\tarray\tAgent names for conversation handoff\tNo\ntoolsets\tarray\tAvailable tools\tNo\nwelcome_message\tstring\tMessage displayed on start\tNo\nadd_date\tboolean\tInclude current date in context\tNo\nadd_environment_info\tboolean\tInclude working directory, OS, Git info\tNo\nadd_prompt_files\tarray\tPrompt file paths to include\tNo\nmax_iterations\tinteger\tMaximum tool call loops (unlimited if not set)\tNo\nnum_history_items\tinteger\tConversation history limit\tNo\ncode_mode_tools\tboolean\tEnable Code Mode for tools\tNo\ncommands\tobject\tNamed prompts accessible via /command_name\tNo\nstructured_output\tobject\tJSON schema for structured responses\tNo\nrag\tarray\tRAG source names\tNo\nTask delegation versus conversation handoff\n\nAgents support two different delegation mechanisms. Choose based on whether you need task results or conversation control.\n\nSub_agents: Hierarchical task delegation\n\nUse sub_agents for hierarchical task delegation. The parent agent assigns a specific task to a child agent using the transfer_task tool. The child executes in its own context and returns results. The parent maintains control and can delegate to multiple agents in sequence.\n\nThis works well for structured workflows where you need to combine results from specialists, or when tasks have clear boundaries. Each delegated task runs independently and reports back to the parent.\n\nExample:\n\nagents:\n\n  root:\n\n    sub_agents: [researcher, analyst]\n\n    instruction: |\n\n      Delegate research to researcher.\n\n      Delegate analysis to analyst.\n\n      Combine results and present findings.\n\nRoot calls: transfer_task(agent=\"researcher\", task=\"Find pricing data\"). The researcher completes the task and returns results to root.\n\nHandoffs: Conversation transfer\n\nUse handoffs to transfer conversation control to a different agent. When an agent uses the handoff tool, the new agent takes over completely. The original agent steps back until someone hands back to it.\n\nThis works well when different agents should own different parts of an ongoing conversation, or when specialists need to collaborate as peers without a coordinator managing every step.\n\nExample:\n\nagents:\n\n  generalist:\n\n    handoffs: [database_expert, security_expert]\n\n    instruction: |\n\n      Help with general development questions.\n\n      If the conversation moves to database optimization,\n\n      hand off to database_expert.\n\n      If security concerns arise, hand off to security_expert.\n\n\n\n  database_expert:\n\n    handoffs: [generalist, security_expert]\n\n    instruction: Handle database design and optimization.\n\n\n\n  security_expert:\n\n    handoffs: [generalist, database_expert]\n\n    instruction: Review code for security vulnerabilities.\n\nWhen the user asks about query performance, generalist executes: handoff(agent=\"database_expert\"). The database expert now owns the conversation and can continue working with the user directly, or hand off to security_expert if the discussion shifts to SQL injection concerns.\n\nCommands\n\nNamed prompts users invoke with /command_name. Supports JavaScript template literals with ${env.VARIABLE} for environment variables:\n\ncommands:\n\n  greet: \"Say hello to ${env.USER}\"\n\n  analyze: \"Analyze ${env.PROJECT_NAME || 'demo'}\"\n\nRun with: cagent run config.yaml /greet\n\nStructured output\n\nConstrain responses to a JSON schema (OpenAI and Gemini only):\n\nstructured_output:\n\n  name: code_analysis\n\n  strict: true\n\n  schema:\n\n    type: object\n\n    properties:\n\n      issues:\n\n        type: array\n\n        items: { ... }\n\n    required: [issues]\nModels\nProperty\tType\tDescription\tRequired\nprovider\tstring\topenai, anthropic, google, dmr\tYes\nmodel\tstring\tModel name\tYes\ntemperature\tfloat\tRandomness (0.0-2.0)\tNo\nmax_tokens\tinteger\tMaximum response length\tNo\ntop_p\tfloat\tNucleus sampling (0.0-1.0)\tNo\nfrequency_penalty\tfloat\tRepetition penalty (-2.0 to 2.0, OpenAI only)\tNo\npresence_penalty\tfloat\tTopic penalty (-2.0 to 2.0, OpenAI only)\tNo\nbase_url\tstring\tCustom API endpoint\tNo\nparallel_tool_calls\tboolean\tEnable parallel tool execution (default: true)\tNo\ntoken_key\tstring\tAuthentication token key\tNo\ntrack_usage\tboolean\tTrack token usage\tNo\nthinking_budget\tmixed\tReasoning effort (provider-specific)\tNo\nprovider_opts\tobject\tProvider-specific options\tNo\nAlloy models\n\nUse multiple models in rotation by separating names with commas:\n\nmodel: anthropic/claude-sonnet-4-5,openai/gpt-5\nThinking budget\n\nControls reasoning depth. Configuration varies by provider:\n\nOpenAI: String values - minimal, low, medium, high\nAnthropic: Integer token budget (1024-32768, must be less than max_tokens)\nSet provider_opts.interleaved_thinking: true for tool use during reasoning\nGemini: Integer token budget (0 to disable, -1 for dynamic, max 24576)\nGemini 2.5 Pro: 128-32768, cannot disable (minimum 128)\n# OpenAI\n\nthinking_budget: low\n\n\n\n# Anthropic\n\nthinking_budget: 8192\n\nprovider_opts:\n\n  interleaved_thinking: true\n\n\n\n# Gemini\n\nthinking_budget: 8192    # Fixed\n\nthinking_budget: -1      # Dynamic\n\nthinking_budget: 0       # Disabled\nDocker Model Runner (DMR)\n\nRun local models. If base_url is omitted, cagent auto-discovers via Docker Model plugin.\n\nprovider: dmr\n\nmodel: ai/qwen3\n\nmax_tokens: 8192\n\nbase_url: http://localhost:12434/engines/llama.cpp/v1 # Optional\n\nPass llama.cpp options via provider_opts.runtime_flags (array, string, or multiline):\n\nprovider_opts:\n\n  runtime_flags: [\"--ngl=33\", \"--threads=8\"]\n\n  # or: runtime_flags: \"--ngl=33 --threads=8\"\n\nModel config fields auto-map to runtime flags:\n\ntemperature ‚Üí --temp\ntop_p ‚Üí --top-p\nmax_tokens ‚Üí --context-size\n\nExplicit runtime_flags override auto-mapped flags.\n\nSpeculative decoding for faster inference:\n\nprovider_opts:\n\n  speculative_draft_model: ai/qwen3:0.6B-F16\n\n  speculative_num_tokens: 16\n\n  speculative_acceptance_rate: 0.8\nTools\n\nConfigure tools in the toolsets array. Three types: built-in, MCP (local/remote), and Docker Gateway.\n\nNote\n\ndocumentation of each toolset's capabilities, available tools, and specific configuration options, see the Toolsets reference.\n\nAll toolsets support common properties like tools (whitelist), defer (deferred loading), toon (output compression), env (environment variables), and instruction (usage guidance). See the Toolsets reference for details on these properties and what each toolset does.\n\nBuilt-in tools\ntoolsets:\n\n  - type: filesystem\n\n  - type: shell\n\n  - type: think\n\n  - type: todo\n\n    shared: true\n\n  - type: memory\n\n    path: ./memory.db\nMCP tools\n\nLocal process:\n\n- type: mcp\n\n  command: npx\n\n  args:\n\n    [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n\n  tools: [\"read_file\", \"write_file\"] # Optional: limit to specific tools\n\n  env:\n\n    NODE_OPTIONS: \"--max-old-space-size=8192\"\n\nRemote server:\n\n- type: mcp\n\n  remote:\n\n    url: https://mcp-server.example.com\n\n    transport_type: sse\n\n    headers:\n\n      Authorization: Bearer token\nDocker MCP Gateway\n\nContainerized tools from Docker MCP Catalog:\n\n- type: mcp\n\n  ref: docker:duckduckgo\nRAG\n\nRetrieval-augmented generation for document knowledge bases. Define sources at the top level, reference in agents.\n\nrag:\n\n  docs:\n\n    docs: [./documents, ./README.md]\n\n    strategies:\n\n      - type: chunked-embeddings\n\n        embedding_model: openai/text-embedding-3-small\n\n        vector_dimensions: 1536\n\n        database: ./embeddings.db\n\n\n\nagents:\n\n  root:\n\n    rag: [docs]\nRetrieval strategies\n\nAll strategies support chunking configuration. Chunk size and overlap are measured in characters (Unicode code points), not tokens.\n\nChunked-embeddings\n\nDirect semantic search using vector embeddings. Best for understanding intent, synonyms, and paraphrasing.\n\nField\tType\tDefault\nembedding_model\tstring\t-\ndatabase\tstring\t-\nvector_dimensions\tinteger\t-\nsimilarity_metric\tstring\tcosine\nthreshold\tfloat\t0.5\nlimit\tinteger\t5\nchunking.size\tinteger\t1000\nchunking.overlap\tinteger\t75\nchunking.respect_word_boundaries\tboolean\ttrue\nchunking.code_aware\tboolean\tfalse\n- type: chunked-embeddings\n\n  embedding_model: openai/text-embedding-3-small\n\n  vector_dimensions: 1536\n\n  database: ./vector.db\n\n  similarity_metric: cosine_similarity\n\n  threshold: 0.5\n\n  limit: 10\n\n  chunking:\n\n    size: 1000\n\n    overlap: 100\nSemantic-embeddings\n\nLLM-enhanced semantic search. Uses a language model to generate rich semantic summaries of each chunk before embedding, capturing deeper meaning.\n\nField\tType\tDefault\nembedding_model\tstring\t-\nchat_model\tstring\t-\ndatabase\tstring\t-\nvector_dimensions\tinteger\t-\nsimilarity_metric\tstring\tcosine\nthreshold\tfloat\t0.5\nlimit\tinteger\t5\nast_context\tboolean\tfalse\nsemantic_prompt\tstring\t-\nchunking.size\tinteger\t1000\nchunking.overlap\tinteger\t75\nchunking.respect_word_boundaries\tboolean\ttrue\nchunking.code_aware\tboolean\tfalse\n- type: semantic-embeddings\n\n  embedding_model: openai/text-embedding-3-small\n\n  vector_dimensions: 1536\n\n  chat_model: openai/gpt-5-mini\n\n  database: ./semantic.db\n\n  threshold: 0.3\n\n  limit: 10\n\n  chunking:\n\n    size: 1000\n\n    overlap: 100\nBM25\n\nKeyword-based search using BM25 algorithm. Best for exact terms, technical jargon, and code identifiers.\n\nField\tType\tDefault\ndatabase\tstring\t-\nk1\tfloat\t1.5\nb\tfloat\t0.75\nthreshold\tfloat\t0.0\nlimit\tinteger\t5\nchunking.size\tinteger\t1000\nchunking.overlap\tinteger\t75\nchunking.respect_word_boundaries\tboolean\ttrue\nchunking.code_aware\tboolean\tfalse\n- type: bm25\n\n  database: ./bm25.db\n\n  k1: 1.5\n\n  b: 0.75\n\n  threshold: 0.3\n\n  limit: 10\n\n  chunking:\n\n    size: 1000\n\n    overlap: 100\nHybrid retrieval\n\nCombine multiple strategies with fusion:\n\nstrategies:\n\n  - type: chunked-embeddings\n\n    embedding_model: openai/text-embedding-3-small\n\n    vector_dimensions: 1536\n\n    database: ./vector.db\n\n    limit: 20\n\n  - type: bm25\n\n    database: ./bm25.db\n\n    limit: 15\n\n\n\nresults:\n\n  fusion:\n\n    strategy: rrf # Options: rrf, weighted, max\n\n    k: 60 # RRF smoothing parameter\n\n  deduplicate: true\n\n  limit: 5\n\nFusion strategies:\n\nrrf: Reciprocal Rank Fusion (recommended, rank-based, no normalization needed)\nweighted: Weighted combination (fusion.weights: {chunked-embeddings: 0.7, bm25: 0.3})\nmax: Maximum score across strategies\nReranking\n\nRe-score results with a specialized model for improved relevance:\n\nresults:\n\n  reranking:\n\n    model: openai/gpt-5-mini\n\n    top_k: 10 # Only rerank top K (0 = all)\n\n    threshold: 0.3 # Minimum score after reranking\n\n    criteria: | # Optional domain-specific guidance\n\n      Prioritize official docs over blog posts\n\n  limit: 5\n\nDMR native reranking:\n\nmodels:\n\n  reranker:\n\n    provider: dmr\n\n    model: hf.co/ggml-org/qwen3-reranker-0.6b-q8_0-gguf\n\n\n\nresults:\n\n  reranking:\n\n    model: reranker\nCode-aware chunking\n\nFor source code, use AST-based chunking. With semantic-embeddings, you can include AST metadata in the LLM prompts:\n\n- type: semantic-embeddings\n\n  embedding_model: openai/text-embedding-3-small\n\n  vector_dimensions: 1536\n\n  chat_model: openai/gpt-5-mini\n\n  database: ./code.db\n\n  ast_context: true # Include AST metadata in semantic prompts\n\n  chunking:\n\n    size: 2000\n\n    code_aware: true # Enable AST-based chunking\nRAG properties\n\nTop-level RAG source:\n\nField\tType\tDescription\ndocs\t[]string\tDocument paths (supports glob patterns, respects .gitignore)\ntool\tobject\tCustomize RAG tool name/description/instruction\nstrategies\t[]object\tRetrieval strategies (see above for strategy-specific fields)\nresults\tobject\tPost-processing (fusion, reranking, limits)\n\nResults:\n\nField\tType\tDefault\nlimit\tinteger\t15\ndeduplicate\tboolean\ttrue\ninclude_score\tboolean\tfalse\nfusion.strategy\tstring\t-\nfusion.k\tinteger\t60\nfusion.weights\tobject\t-\nreranking.model\tstring\t-\nreranking.top_k\tinteger\t0\nreranking.threshold\tfloat\t0.5\nreranking.criteria\tstring\t\"\"\nreturn_full_content\tboolean\tfalse\nMetadata\n\nDocumentation and sharing information:\n\nProperty\tType\tDescription\nauthor\tstring\tAuthor name\nlicense\tstring\tLicense (e.g., MIT, Apache-2.0)\nreadme\tstring\tUsage documentation\nmetadata:\n\n  author: Your Name\n\n  license: MIT\n\n  readme: |\n\n    Description and usage instructions\nExample configuration\n\nComplete configuration demonstrating key features:\n\nagents:\n\n  root:\n\n    model: claude\n\n    description: Technical lead\n\n    instruction: Coordinate development tasks and delegate to specialists\n\n    sub_agents: [developer, reviewer]\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: mcp\n\n        ref: docker:duckduckgo\n\n    rag: [readmes]\n\n    commands:\n\n      status: \"Check project status\"\n\n\n\n  developer:\n\n    model: gpt\n\n    description: Software developer\n\n    instruction: Write clean, maintainable code\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\n\n\n  reviewer:\n\n    model: claude\n\n    description: Code reviewer\n\n    instruction: Review for quality and security\n\n    toolsets:\n\n      - type: filesystem\n\n\n\nmodels:\n\n  gpt:\n\n    provider: openai\n\n    model: gpt-5\n\n\n\n  claude:\n\n    provider: anthropic\n\n    model: claude-sonnet-4-5\n\n    max_tokens: 64000\n\n\n\nrag:\n\n  readmes:\n\n    docs: [\"**/README.md\"]\n\n    strategies:\n\n      - type: chunked-embeddings\n\n        embedding_model: openai/text-embedding-3-small\n\n        vector_dimensions: 1536\n\n        database: ./embeddings.db\n\n        limit: 10\n\n      - type: bm25\n\n        database: ./bm25.db\n\n        limit: 10\n\n    results:\n\n      fusion:\n\n        strategy: rrf\n\n        k: 60\n\n      limit: 5\nWhat's next\nRead the Toolsets reference for detailed toolset documentation\nReview the CLI reference for command-line options\nBrowse example configurations\nLearn about sharing agents\n\nEdit this page\n\nRequest changes\n\nTable of contents\nFile structure\nAgents\nTask delegation versus conversation handoff\nCommands\nStructured output\nModels\nAlloy models\nThinking budget\nDocker Model Runner (DMR)\nTools\nBuilt-in tools\nMCP tools\nDocker MCP Gateway\nRAG\nRetrieval strategies\nHybrid retrieval\nReranking\nCode-aware chunking\nRAG properties\nMetadata\nExample configuration\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996087,
    "timestamp": "2026-02-07T06:37:10.027Z",
    "title": "CLI | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/reference/cli/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nConfiguration file\nToolsets\nCLI\nExamples\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nReference\n/\nCLI\nCLI reference\nCopy as Markdown\n\nCommand-line interface for running, managing, and deploying AI agents.\n\nFor agent configuration file syntax, see the Configuration file reference. For toolset capabilities, see the Toolsets reference.\n\nSynopsis\n$ cagent [command] [flags]\n\nGlobal flags\n\nWork with all commands:\n\nFlag\tType\tDefault\tDescription\n-d, --debug\tboolean\tfalse\tEnable debug logging\n-o, --otel\tboolean\tfalse\tEnable OpenTelemetry\n--log-file\tstring\t-\tDebug log file path\n\nDebug logs write to ~/.cagent/cagent.debug.log by default. Override with --log-file.\n\nRuntime flags\n\nWork with most commands. Supported commands link to this section.\n\nFlag\tType\tDefault\tDescription\n--models-gateway\tstring\t-\tModels gateway address\n--env-from-file\tarray\t-\tLoad environment variables from file\n--code-mode-tools\tboolean\tfalse\tEnable JavaScript tool orchestration\n--working-dir\tstring\t-\tWorking directory for the session\n\nSet --models-gateway via CAGENT_MODELS_GATEWAY environment variable.\n\nCommands\na2a\n\nExpose agent via the Agent2Agent (A2A) protocol. Allows other A2A-compatible systems to discover and interact with your agent. Auto-selects an available port if not specified.\n\n$ cagent a2a agent-file|registry-ref\n\nNote\n\nA2A support is currently experimental and needs further work. Tool calls are handled internally and not exposed as separate ADK events. Some ADK features are not yet integrated.\n\nArguments:\n\nagent-file|registry-ref - Path to YAML or OCI registry reference (required)\n\nFlags:\n\nFlag\tType\tDefault\tDescription\n-a, --agent\tstring\troot\tAgent name\n--port\tinteger\t0\tPort (0 = random)\n\nSupports runtime flags.\n\nExamples:\n\n$ cagent a2a ./agent.yaml --port 8080\n\n$ cagent a2a agentcatalog/pirate --port 9000\n\nacp\n\nStart agent as ACP (Agent Client Protocol) server on stdio for editor integration. See ACP integration for setup guides.\n\n$ cagent acp agent-file|registry-ref\n\n\nArguments:\n\nagent-file|registry-ref - Path to YAML or OCI registry reference (required)\n\nSupports runtime flags.\n\nalias add\n\nCreate alias for agent.\n\n$ cagent alias add name target\n\n\nArguments:\n\nname - Alias name (required)\ntarget - Path to YAML or registry reference (required)\n\nExamples:\n\n$ cagent alias add dev ./dev-agent.yaml\n\n$ cagent alias add prod docker.io/user/prod-agent:latest\n\n$ cagent alias add default ./agent.yaml\n\n\nSetting alias name to \"default\" lets you run cagent run without arguments.\n\nalias list\n\nList all aliases.\n\n$ cagent alias list\n\n$ cagent alias ls\n\nalias remove\n\nRemove alias.\n\n$ cagent alias remove name\n\n$ cagent alias rm name\n\n\nArguments:\n\nname - Alias name (required)\napi\n\nHTTP API server.\n\n$ cagent api agent-file|agents-dir\n\n\nArguments:\n\nagent-file|agents-dir - Path to YAML or directory with agents (required)\n\nFlags:\n\nFlag\tType\tDefault\tDescription\n-l, --listen\tstring\t:8080\tListen address\n-s, --session-db\tstring\tsession.db\tSession database path\n--pull-interval\tinteger\t0\tAuto-pull OCI ref every N minutes\n\nSupports runtime flags.\n\nExamples:\n\n$ cagent api ./agent.yaml\n\n$ cagent api ./agents/ --listen :9000\n\n$ cagent api docker.io/user/agent --pull-interval 10\n\n\nThe --pull-interval flag works only with OCI references. Automatically pulls and reloads at the specified interval.\n\nbuild\n\nBuild Docker image for agent.\n\n$ cagent build agent-file|registry-ref [image-name]\n\n\nArguments:\n\nagent-file|registry-ref - Path to YAML or OCI registry reference (required)\nimage-name - Docker image name (optional)\n\nFlags:\n\nFlag\tType\tDefault\tDescription\n--dry-run\tboolean\tfalse\tPrint Dockerfile only\n--push\tboolean\tfalse\tPush image after build\n--no-cache\tboolean\tfalse\tBuild without cache\n--pull\tboolean\tfalse\tPull all referenced images\n\nExample:\n\n$ cagent build ./agent.yaml myagent:latest\n\n$ cagent build ./agent.yaml --dry-run\n\ncatalog list\n\nList catalog agents.\n\n$ cagent catalog list [org]\n\n\nArguments:\n\norg - Organization name (optional, default: agentcatalog)\n\nQueries Docker Hub for agent repositories.\n\ndebug config\n\nShow resolved agent configuration.\n\n$ cagent debug config agent-file|registry-ref\n\n\nArguments:\n\nagent-file|registry-ref - Path to YAML or OCI registry reference (required)\n\nSupports runtime flags.\n\nShows canonical configuration in YAML after all processing and defaults.\n\ndebug toolsets\n\nList agent tools.\n\n$ cagent debug toolsets agent-file|registry-ref\n\n\nArguments:\n\nagent-file|registry-ref - Path to YAML or OCI registry reference (required)\n\nSupports runtime flags.\n\nLists all tools for each agent in the configuration.\n\neval\n\nRun evaluation tests.\n\n$ cagent eval agent-file|registry-ref [eval-dir]\n\n\nArguments:\n\nagent-file|registry-ref - Path to YAML or OCI registry reference (required)\neval-dir - Evaluation files directory (optional, default: ./evals)\n\nSupports runtime flags.\n\nexec\n\nSingle message execution without TUI.\n\n$ cagent exec agent-file|registry-ref [message|-]\n\n\nArguments:\n\nagent-file|registry-ref - Path to YAML or OCI registry reference (required)\nmessage - Prompt, or - for stdin (optional)\n\nSame flags as run.\n\nSupports runtime flags.\n\nExamples:\n\n$ cagent exec ./agent.yaml\n\n$ cagent exec ./agent.yaml \"Check for security issues\"\n\n$ echo \"Instructions\" | cagent exec ./agent.yaml -\n\nfeedback\n\nSubmit feedback.\n\n$ cagent feedback\n\n\nShows link to submit feedback.\n\nmcp\n\nMCP (Model Context Protocol) server on stdio. Exposes agents as tools to MCP clients. See MCP integration for setup guides.\n\n$ cagent mcp agent-file|registry-ref\n\n\nArguments:\n\nagent-file|registry-ref - Path to YAML or OCI registry reference (required)\n\nSupports runtime flags.\n\nExamples:\n\n$ cagent mcp ./agent.yaml\n\n$ cagent mcp docker.io/user/agent:latest\n\nnew\n\nCreate agent configuration interactively.\n\n$ cagent new [message...]\n\n\nFlags:\n\nFlag\tType\tDefault\tDescription\n--model\tstring\t-\tModel as provider/model\n--max-iterations\tinteger\t0\tMaximum agentic loop iterations\n\nSupports runtime flags.\n\nOpens interactive TUI to configure and generate agent YAML.\n\npull\n\nPull agent from OCI registry.\n\n$ cagent pull registry-ref\n\n\nArguments:\n\nregistry-ref - OCI registry reference (required)\n\nFlags:\n\nFlag\tType\tDefault\tDescription\n--force\tboolean\tfalse\tPull even if already exists\n\nExample:\n\n$ cagent pull docker.io/user/agent:latest\n\n\nSaves to local YAML file.\n\npush\n\nPush agent to OCI registry.\n\n$ cagent push agent-file registry-ref\n\n\nArguments:\n\nagent-file - Path to local YAML (required)\nregistry-ref - OCI reference like docker.io/user/agent:latest (required)\n\nExample:\n\n$ cagent push ./agent.yaml docker.io/myuser/myagent:latest\n\nrun\n\nInteractive terminal UI for agent sessions.\n\n$ cagent run [agent-file|registry-ref] [message|-]\n\n\nArguments:\n\nagent-file|registry-ref - Path to YAML or OCI registry reference (optional)\nmessage - Initial prompt, or - for stdin (optional)\n\nFlags:\n\nFlag\tType\tDefault\tDescription\n-a, --agent\tstring\troot\tAgent name\n--yolo\tboolean\tfalse\tAuto-approve all tool calls\n--attach\tstring\t-\tAttach image file\n--model\tarray\t-\tOverride model (repeatable)\n--dry-run\tboolean\tfalse\tInitialize without executing\n--remote\tstring\t-\tRemote runtime address\n\nSupports runtime flags.\n\nExamples:\n\n$ cagent run ./agent.yaml\n\n$ cagent run ./agent.yaml \"Analyze this codebase\"\n\n$ cagent run ./agent.yaml --agent researcher\n\n$ echo \"Instructions\" | cagent run ./agent.yaml -\n\n$ cagent run\n\n\nRunning without arguments uses the default agent or a \"default\" alias if configured.\n\nShows interactive TUI in a terminal. Falls back to exec mode otherwise.\n\nInteractive commands\n\nTUI slash commands:\n\nCommand\tDescription\n/exit\tExit\n/reset\tClear history\n/eval\tSave conversation for evaluation\n/compact\tCompact conversation\n/yolo\tToggle auto-approval\nversion\n\nPrint version information.\n\n$ cagent version\n\n\nShows cagent version and commit hash.\n\nEnvironment variables\nVariable\tDescription\nCAGENT_MODELS_GATEWAY\tModels gateway address\nTELEMETRY_ENABLED\tTelemetry control (set false)\nCAGENT_HIDE_TELEMETRY_BANNER\tHide telemetry banner (set 1)\nOTEL_EXPORTER_OTLP_ENDPOINT\tOpenTelemetry endpoint\nModel overrides\n\nOverride models specified in your configuration file using the --model flag.\n\nFormat: [agent=]provider/model\n\nWithout an agent name, the model applies to all agents. With an agent name, it applies only to that specific agent.\n\nApply to all agents:\n\n$ cagent run ./agent.yaml --model gpt-5\n\n$ cagent run ./agent.yaml --model anthropic/claude-sonnet-4-5\n\n\nApply to specific agents only:\n\n$ cagent run ./agent.yaml --model researcher=gpt-5\n\n$ cagent run ./agent.yaml --model \"agent1=gpt-5,agent2=claude-sonnet-4-5\"\n\n\nProviders: openai, anthropic, google, dmr\n\nOmit provider for automatic selection based on model name.\n\nEdit this page\n\nRequest changes\n\nTable of contents\nSynopsis\nGlobal flags\nRuntime flags\nCommands\na2a\nacp\nalias add\nalias list\nalias remove\napi\nbuild\ncatalog list\ndebug config\ndebug toolsets\neval\nexec\nfeedback\nmcp\nnew\npull\npush\nrun\nversion\nEnvironment variables\nModel overrides\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996084,
    "timestamp": "2026-02-07T06:37:10.028Z",
    "title": "Toolsets | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/reference/toolsets/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nConfiguration file\nToolsets\nCLI\nExamples\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nReference\n/\nToolsets\nToolsets reference\nCopy as Markdown\n\nThis reference documents the toolsets available in cagent and what each one does. Tools give agents the ability to take action‚Äîinteracting with files, executing commands, accessing external resources, and managing state.\n\nFor configuration file syntax and how to set up toolsets in your agent YAML, see the Configuration file reference.\n\nHow agents use tools\n\nWhen you configure toolsets for an agent, those tools become available in the agent's context. The agent can invoke tools by name with appropriate parameters based on the task at hand.\n\nTool invocation flow:\n\nAgent analyzes the task and determines which tool to use\nAgent constructs tool parameters based on requirements\ncagent executes the tool and returns results\nAgent processes results and decides next steps\n\nAgents can call multiple tools in sequence or make decisions based on tool results. Tool selection is automatic based on the agent's understanding of the task and available capabilities.\n\nTool types\n\ncagent supports three types of toolsets:\n\nBuilt-in toolsets\nCore functionality built directly into cagent (filesystem, shell, memory, etc.). These provide essential capabilities for file operations, command execution, and state management. MCP toolsets\nTools provided by Model Context Protocol servers, either local processes (stdio) or remote servers (HTTP/SSE). MCP enables access to a wide ecosystem of standardized tools. Custom toolsets\nShell scripts wrapped as tools with typed parameters (script_shell). This lets you define domain-specific tools for your use case.\nConfiguration\n\nToolsets are configured in your agent's YAML file under the toolsets array:\n\nagents:\n\n  my_agent:\n\n    model: anthropic/claude-sonnet-4-5\n\n    description: A helpful coding assistant\n\n    toolsets:\n\n      # Built-in toolset\n\n      - type: filesystem\n\n\n\n      # Built-in toolset with configuration\n\n      - type: memory\n\n        path: ./memories.db\n\n\n\n      # Local MCP server (stdio)\n\n      - type: mcp\n\n        command: npx\n\n        args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/dir\"]\n\n\n\n      # Remote MCP server (SSE)\n\n      - type: mcp\n\n        remote:\n\n          url: https://mcp.example.com/sse\n\n          transport_type: sse\n\n          headers:\n\n            Authorization: Bearer ${API_TOKEN}\n\n\n\n      # Custom shell tools\n\n      - type: script_shell\n\n        tools:\n\n          build:\n\n            cmd: npm run build\n\n            description: Build the project\nCommon configuration options\n\nAll toolset types support these optional properties:\n\nProperty\tType\tDescription\ninstruction\tstring\tAdditional instructions for using the toolset\ntools\tarray\tSpecific tool names to enable (defaults to all)\nenv\tobject\tEnvironment variables for the toolset\ntoon\tstring\tComma-delimited regex patterns matching tool names whose JSON outputs should be compressed. Reduces token usage by simplifying/compressing JSON responses from matched tools using automatic encoding. Example: \"search.*,list.*\"\ndefer\tboolean or array\tControl which tools load into initial context. Set to true to defer all tools, or array of tool names to defer specific tools. Deferred tools don't consume context until explicitly loaded via search_tool/add_tool.\nTool selection\n\nBy default, agents have access to all tools from their configured toolsets. You can restrict this using the tools option:\n\ntoolsets:\n\n  - type: filesystem\n\n    tools: [read_file, write_file, list_directory]\n\nThis is useful for:\n\nLimiting agent capabilities for security\nReducing context size for smaller models\nCreating specialized agents with focused tool access\nDeferred loading\n\nDeferred loading keeps tools out of the initial context window, loading them only when explicitly requested. This is useful for large toolsets where most tools won't be used, significantly reducing context consumption.\n\nDefer all tools in a toolset:\n\ntoolsets:\n\n  - type: mcp\n\n    command: npx\n\n    args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path\"]\n\n    defer: true # All tools load on-demand\n\nOr defer specific tools while loading others immediately:\n\ntoolsets:\n\n  - type: mcp\n\n    command: npx\n\n    args: [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path\"]\n\n    defer: [search_files, list_directory] # Only these are deferred\n\nAgents can discover deferred tools via search_tool and load them into context via add_tool when needed. Best for toolsets with dozens of tools where only a few are typically used.\n\nOutput compression\n\nThe toon property compresses JSON outputs from matched tools to reduce token usage. When a tool's output is JSON, it's automatically compressed using efficient encoding before being returned to the agent:\n\ntoolsets:\n\n  - type: mcp\n\n    command: npx\n\n    args: [\"-y\", \"@modelcontextprotocol/server-github\"]\n\n    toon: \"search.*,list.*\" # Compress outputs from search/list tools\n\nUseful for tools that return large JSON responses (API results, file listings, search results). The compression is transparent to the agent but can significantly reduce context consumption for verbose tool outputs.\n\nPer-agent tool configuration\n\nDifferent agents can have different toolsets:\n\nagents:\n\n  coordinator:\n\n    model: anthropic/claude-sonnet-4-5\n\n    sub_agents: [code_writer, code_reviewer]\n\n    toolsets:\n\n      - type: filesystem\n\n        tools: [read_file]\n\n\n\n  code_writer:\n\n    model: openai/gpt-5-mini\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\n\n\n  code_reviewer:\n\n    model: anthropic/claude-sonnet-4-5\n\n    toolsets:\n\n      - type: filesystem\n\n        tools: [read_file, read_multiple_files]\n\nThis allows specialized agents with focused capabilities, security boundaries, and optimized performance.\n\nBuilt-in tools reference\nFilesystem\n\nThe filesystem toolset gives your agent the ability to work with files and directories. Your agent can read files to understand context, write new files, make targeted edits to existing files, search for content, and explore directory structures. Essential for code analysis, documentation updates, configuration management, and any agent that needs to understand or modify project files.\n\nAccess is restricted to the current working directory by default. Agents can request access to additional directories at runtime, which requires your approval.\n\nConfiguration\ntoolsets:\n\n  - type: filesystem\n\n\n\n  # Optional: restrict to specific tools\n\n  - type: filesystem\n\n    tools: [read_file, write_file, edit_file]\nShell\n\nThe shell toolset lets your agent execute commands in your system's shell environment. Use this for agents that need to run builds, execute tests, manage processes, interact with CLI tools, or perform system operations. The agent can run commands in the foreground or background.\n\nCommands execute in the current working directory and inherit environment variables from the cagent process. This toolset is powerful but should be used with appropriate security considerations.\n\nConfiguration\ntoolsets:\n\n  - type: shell\nThink\n\nThe think toolset provides your agent with a reasoning scratchpad. The agent can record thoughts and reasoning steps without taking actions or modifying data. Particularly useful for complex tasks where the agent needs to plan multiple steps, verify requirements, or maintain context across a long conversation.\n\nAgents use this to break down problems, list applicable rules, verify they have all needed information, and document their reasoning process before acting.\n\nConfiguration\ntoolsets:\n\n  - type: think\nTodo\n\nThe todo toolset gives your agent task-tracking capabilities for managing multi-step operations. Your agent can break down complex work into discrete tasks, track progress through each step, and ensure nothing is missed before completing a request. Especially valuable for agents handling complex workflows with multiple dependencies.\n\nThe shared option allows todos to persist across different agents in a multi-agent system, enabling coordination.\n\nConfiguration\ntoolsets:\n\n  - type: todo\n\n\n\n  # Optional: share todos across agents\n\n  - type: todo\n\n    shared: true\nMemory\n\nThe memory toolset allows your agent to store and retrieve information across conversations and sessions. Your agent can remember user preferences, project context, previous decisions, and other information that should persist. Useful for agents that interact with users over time or need to maintain state about a project or environment.\n\nMemories are stored in a local database file and persist across cagent sessions.\n\nConfiguration\ntoolsets:\n\n  - type: memory\n\n\n\n  # Optional: specify database location\n\n  - type: memory\n\n    path: ./agent-memories.db\nFetch\n\nThe fetch toolset enables your agent to retrieve content from HTTP/HTTPS URLs. Your agent can fetch documentation, API responses, web pages, or any content accessible via HTTP GET requests. Useful for agents that need to access external resources, check API documentation, or retrieve web content.\n\nThe agent can specify custom HTTP headers when needed for authentication or other purposes.\n\nConfiguration\ntoolsets:\n\n  - type: fetch\nAPI\n\nThe api toolset lets you define custom tools that call HTTP APIs. Similar to script_shell but for web services, this allows you to expose REST APIs, webhooks, or any HTTP endpoint as a tool your agent can use. The agent sees these as typed tools with automatic parameter validation.\n\nUse this to integrate with external services, call internal APIs, trigger webhooks, or interact with any HTTP-based system.\n\nConfiguration\n\nEach API tool is defined with an api_config containing the endpoint, HTTP method, and optional typed parameters:\n\ntoolsets:\n\n  - type: api\n\n    api_config:\n\n      name: search_docs\n\n      endpoint: https://api.example.com/search\n\n      method: GET\n\n      instruction: Search the documentation database\n\n      headers:\n\n        Authorization: Bearer ${API_TOKEN}\n\n      args:\n\n        query:\n\n          type: string\n\n          description: Search query\n\n        limit:\n\n          type: number\n\n          description: Maximum results\n\n      required: [query]\n\n\n\n  - type: api\n\n    api_config:\n\n      name: create_ticket\n\n      endpoint: https://api.example.com/tickets\n\n      method: POST\n\n      instruction: Create a support ticket\n\n      args:\n\n        title:\n\n          type: string\n\n          description: Ticket title\n\n        description:\n\n          type: string\n\n          description: Ticket description\n\n      required: [title, description]\n\nFor GET requests, parameters are interpolated into the endpoint URL. For POST requests, parameters are sent as JSON in the request body.\n\nSupported argument types: string, number, boolean, array, object.\n\nScript Shell\n\nThe script_shell toolset lets you define custom tools by wrapping shell commands with typed parameters. This allows you to expose domain-specific operations to your agent as first-class tools. The agent sees these custom tools just like built-in tools, with parameter validation and type checking handled automatically.\n\nUse this to create tools for deployment scripts, build commands, test runners, or any operation specific to your project or workflow.\n\nConfiguration\n\nEach custom tool is defined with a command, description, and optional typed parameters:\n\ntoolsets:\n\n  - type: script_shell\n\n    tools:\n\n      deploy:\n\n        cmd: ./deploy.sh\n\n        description: Deploy the application to an environment\n\n        args:\n\n          environment:\n\n            type: string\n\n            description: Target environment (dev, staging, prod)\n\n          version:\n\n            type: string\n\n            description: Version to deploy\n\n        required: [environment]\n\n\n\n      run_tests:\n\n        cmd: npm test\n\n        description: Run the test suite\n\n        args:\n\n          filter:\n\n            type: string\n\n            description: Test name filter pattern\n\nSupported argument types: string, number, boolean, array, object.\n\nTools\n\nThe tools you define become available to your agent. In the previous example, the agent would have access to deploy and run_tests tools.\n\nAutomatic tools\n\nSome tools are automatically added to agents based on their configuration. You don't configure these explicitly‚Äîthey appear when needed.\n\ntransfer_task\n\nAutomatically available when your agent has sub_agents configured. Allows the agent to delegate tasks to sub-agents and receive results back.\n\nhandoff\n\nAutomatically available when your agent has handoffs configured. Allows the agent to transfer the entire conversation to a different agent.\n\nWhat's next\nRead the Configuration file reference for YAML file structure\nReview the CLI reference for running agents\nExplore MCP servers for extended capabilities\nBrowse example configurations\n\nEdit this page\n\nRequest changes\n\nTable of contents\nHow agents use tools\nTool types\nConfiguration\nCommon configuration options\nTool selection\nDeferred loading\nOutput compression\nPer-agent tool configuration\nBuilt-in tools reference\nFilesystem\nShell\nThink\nTodo\nMemory\nFetch\nAPI\nScript Shell\nAutomatic tools\ntransfer_task\nhandoff\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996090,
    "timestamp": "2026-02-07T06:37:10.030Z",
    "title": "Examples | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/reference/examples/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nConfiguration file\nToolsets\nCLI\nExamples\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nReference\n/\nExamples\nExamples\nCopy as Markdown\n\nGet inspiration from the following agent examples. See more examples in the cagent GitHub repository.\n\nDevelopment team\nShow more\n#!/usr/bin/env cagent run\n\n\n\nmodels:\n\n  model:\n\n    provider: anthropic\n\n    model: claude-sonnet-4-0\n\n    max_tokens: 64000\n\n\n\nagents:\n\n  root:\n\n    model: model\n\n    description: Product Manager - Leads the development team and coordinates iterations\n\n    instruction: |\n\n      You are the Product Manager leading a development team consisting of a designer, frontend engineer, full stack engineer, and QA tester.\n\n      \n\n      Your responsibilities:\n\n      - Break down user requirements into small, manageable iterations\n\n      - Each iteration should deliver one complete feature end-to-end\n\n      - Ensure each iteration is small enough to be completed quickly but substantial enough to provide value\n\n      - Coordinate between team members to ensure smooth workflow\n\n      - Define clear acceptance criteria for each feature\n\n      - Prioritize features based on user value and technical dependencies\n\n      \n\n      IMPORTANT ITERATION PRINCIPLES:\n\n      - Start with the most basic, core functionality first\n\n      - Each iteration must result in working, testable code\n\n      - Features should be incrementally built upon previous iterations\n\n      - Don't try to build everything at once - focus on one feature at a time\n\n      - Ensure proper handoffs between designer ‚Üí frontend ‚Üí fullstack ‚Üí QA\n\n      \n\n      Workflow for each iteration:\n\n      1. Define the feature and acceptance criteria\n\n      2. Have designer create UI mockups/wireframes\n\n      3. Have frontend engineer implement the UI\n\n      4. Have fullstack engineer build backend and integrate\n\n      5. Have QA test the complete feature and report issues\n\n      6. Address any issues before moving to next iteration\n\n      \n\n      Always start by understanding what the user wants to build, then break it down into logical, small iterations.\n\n\n\n      Always make sure to ask the right agent to do the right task using the appropriate toolset. don't try to do everything yourself.\n\n\n\n      Always read and write all decisions and important information to a .md file called dev-team.md in the .dev-team directory.\n\n      Make sure to append to the file and edit what is not needed anymore. Consult this file to understand the current state of the project and the team.\n\n      This file might include references to other files that should all be placed inside the .dev-team folder. Don't write anything but code outside of this directory.\n\n\n\n    sub_agents: [designer, awesome_engineer]\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: think\n\n      - type: todo\n\n      - type: memory\n\n        path: dev_memory.db\n\n      - type: mcp\n\n        ref: docker:context7\n\n\n\n  designer:\n\n    model: model\n\n    description: UI/UX Designer - Creates user interface designs and wireframes\n\n    instruction: |\n\n      You are a UI/UX Designer working on a development team. Your role is to create user-friendly, intuitive designs for each feature iteration.\n\n      \n\n      Your responsibilities:\n\n      - Create wireframes and mockups for each feature\n\n      - Design responsive layouts that work on different screen sizes\n\n      - Ensure consistent design patterns across the application\n\n      - Consider user experience and accessibility\n\n      - Provide detailed design specifications for the frontend engineer\n\n      - Use modern design principles and best practices\n\n      \n\n      For each feature you design:\n\n      1. Create a clear wireframe showing layout and components\n\n      2. Specify colors, fonts, spacing, and styling details\n\n      3. Define user interactions and hover states\n\n      4. Consider mobile responsiveness\n\n      5. Provide clear handoff documentation for the frontend engineer\n\n      \n\n      Keep designs simple and focused on the specific feature being built in the current iteration.\n\n      Build upon previous designs to maintain consistency across the application.\n\n\n\n      Always read and write all decisions and important information to a .md file called dev-team.md in the .dev-team directory.\n\n      Make sure to append to the file and edit what is not needed anymore. Consult this file to understand the current state of the project and the team. \n\n      This file might include references to other files that should all be placed inside the .dev-team folder. Don't write anything but code outside of this directory.\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: think\n\n      - type: memory\n\n        path: dev_memory.db\n\n      - type: mcp\n\n        ref: docker:context7\n\n\n\n  awesome_engineer:\n\n    model: model\n\n    description: Awesome Engineer - Implements user interfaces based on designs\n\n    instruction: |\n\n      You are an Awesome Engineer responsible for implementing user interfaces based on the designer's specifications.\n\n      \n\n      Your responsibilities:\n\n      - Convert design mockups into responsive, interactive web interfaces\n\n      - Write clean, maintainable HTML, CSS, and JavaScript\n\n      - Ensure cross-browser compatibility and mobile responsiveness\n\n      - Implement proper accessibility features\n\n      - Create reusable components and maintain code consistency\n\n      - Integrate with backend APIs provided by the fullstack engineer\n\n      \n\n      Technical guidelines:\n\n      - Use modern frontend frameworks/libraries (React, Vue, or vanilla JS as appropriate)\n\n      - Write semantic HTML with proper structure\n\n      - Use CSS best practices (flexbox, grid, responsive design)\n\n      - Implement proper error handling for API calls\n\n      - Follow accessibility guidelines (WCAG)\n\n      - Write clean, commented code that's easy to maintain\n\n      \n\n      For each iteration:\n\n      1. Review the design specifications carefully\n\n      2. Break down the UI into logical components\n\n      3. Implement the interface with proper styling\n\n      4. Test the UI functionality before handoff\n\n      5. Document any deviations from the design and rationale\n\n      \n\n      Focus on creating a working, polished UI for the specific feature in the current iteration.\n\n\n\n      You are also a Full Stack Engineer responsible for building backend systems, APIs, and integrating them with the frontend.\n\n      \n\n      Your responsibilities:\n\n      - Design and implement backend APIs and services\n\n      - Set up databases and data models\n\n      - Handle authentication, authorization, and security\n\n      - Integrate frontend with backend systems\n\n      - Ensure proper error handling and logging\n\n      - Write tests for backend functionality\n\n      - Deploy and maintain the application infrastructure\n\n      \n\n      Technical guidelines:\n\n      - Choose appropriate technology stack based on requirements\n\n      - Design RESTful APIs with proper HTTP methods and status codes\n\n      - Implement proper data validation and sanitization\n\n      - Use appropriate database design patterns\n\n      - Follow security best practices\n\n      - Write comprehensive error handling\n\n      - Include proper logging and monitoring\n\n      - Write unit and integration tests\n\n      \n\n      For each iteration:\n\n      1. Design the backend architecture for the feature\n\n      2. Implement necessary APIs and database changes\n\n      3. Test backend functionality thoroughly\n\n      4. Integrate with the frontend implementation\n\n      5. Ensure end-to-end functionality works correctly\n\n      6. Document API endpoints and usage\n\n      \n\n      Focus on building robust, scalable backend systems that support the current iteration's feature.\n\n      Ensure seamless integration with the frontend implementation.\n\n\n\n      Always read and write all decisions and important information to a .md file called dev-team.md in the .dev-team directory.\n\n      Make sure to append to the file and edit what is not needed anymore. Consult this file to understand the current state of the project and the team. \n\n      This file might include references to other files that should all be placed inside the .dev-team folder. Don't write anything but code outside of this directory.\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\n      - type: think\n\n      - type: memory\n\n        path: dev_memory.db\n\n      - type: mcp\n\n        ref: docker:context7\nGo developer\nShow more\n#!/usr/bin/env cagent run\n\n\n\nmodels:\n\n  claude:\n\n    provider: anthropic\n\n    model: claude-opus-4-6\n\n  haiku:\n\n    provider: anthropic\n\n    model: claude-haiku-4-5\n\n\n\nagents:\n\n  root:\n\n    model: claude\n\n    description: Expert Golang Developer specialized in implementing features and improving code quality.\n\n    instruction: |\n\n      **Goal:**\n\n      Help with Go code-related tasks by examining, modifying, and validating code changes.\n\n\n\n      TASK\n\n          **Workflow:**\n\n          1. **Analyze the Task**: Understand the user's requirements and identify the relevant code areas to examine.\n\n\n\n          2. **Code Examination**: \n\n          - Search for relevant code files and functions\n\n          - Analyze code structure and dependencies\n\n          - Identify potential areas for modification\n\n\n\n          3. **Code Modification**:\n\n          - Make necessary code changes\n\n          - Ensure changes follow best practices\n\n          - Maintain code style consistency\n\n\n\n          4. **Validation Loop**:\n\n          - Run linters and tests to check code quality\n\n          - Verify changes meet requirements\n\n          - If issues found, return to step 3\n\n          - Continue until all requirements are met\n\n\n\n          5. **Summary**:\n\n          - Very concisely summarize the changes made (not in a file)\n\n          - For trivial tasks, answer the question without extra information\n\n      </TASK>\n\n\n\n      **Details:**\n\n       - Be thorough in code examination before making changes\n\n       - Always validate changes before considering the task complete\n\n       - Follow Go best practices\n\n       - Maintain or improve code quality\n\n       - Be proactive in identifying potential issues\n\n       - Only ask for clarification if necessary, try your best to use all the tools to get the info you need\n\n\n\n       **Tools:**\n\n        - When needed and possible, call multiple tools concurrently. It's faster and cheaper.\n\n\n\n    add_date: true\n\n    add_environment_info: true\n\n    add_prompt_files:\n\n      - AGENTS.md\n\n    sub_agents:\n\n      - librarian\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\n      - type: todo\n\n      - type: mcp\n\n        command: gopls\n\n        args: [\"mcp\"]\n\n    commands:\n\n      fix-lint:\n\n        description: \"Fix the lint issues\"\n\n        instruction: |\n\n          Fix the lint issues (if any).\n\n\n\n          Here the result of the linting command:\n\n          $ task lint\n\n          ${shell({cmd: \"task lint\"})}\n\n\n\n          $go_diagnostics\n\n          ${go_diagnostics()}\n\n\n\n          $go_vulncheck\n\n          ${go_vulncheck()}\n\n      remove-comments-tests: \"Remove useless comments in test files (*_test.go)\"\n\n      commit:\n\n        description: \"Commit local changes\"\n\n        instruction: |\n\n            Based on the below changes: create a single commit with an appropriate message.\n\n\n\n            - Current git status: !shell(cmd=\"git status\")\n\n            - Current git diff (staged and unstaged changes): !shell(cmd=\"git diff HEAD\")\n\n            - Current branch: !shell(cmd=\"git branch --show-current\")\n\n      simplify: \"Look at the local changes and try to simplify the code and architecture but don't remove any feature. I just want the code to be easier to read and maintain.\"\n\n      init: |\n\n        Create an AGENTS.md file for this project by inspecting the codebase. The AGENTS.md should help AI coding agents understand how to work with this project effectively.\n\n\n\n        Analyze the project structure and include:\n\n        1. **Development Commands**: Build, test, lint, and run commands (check Makefile, Taskfile, package.json, Cargo.toml, etc.)\n\n        2. **Architecture Overview**: Key packages/modules, their responsibilities, and how they interact\n\n        3. **Code Style and Conventions**: Patterns used, error handling approaches, naming conventions\n\n        4. **Testing Guidelines**: How to run tests, test patterns used, any special testing setup\n\n        5. **Configuration**: Important config files and environment variables\n\n        6. **Common Development Patterns**: Frequently used patterns specific to this codebase\n\n        7. **Key Files Reference**: Quick reference table of important files and their purposes\n\n\n\n        Focus on information that would help an AI agent navigate and modify the codebase correctly. Be concise but comprehensive.\n\n      security-review: |\n\n        Perform a security review of the local changes in this Git repository.\n\n\n\n        **Workflow:**\n\n        1. **Identify Changes**: Run `git diff` to see uncommitted changes, and `git diff HEAD~1` or `git log --oneline -5` to understand recent commits if needed.\n\n\n\n        2. **Security Analysis**: Review the changes for common security issues:\n\n           - **Input Validation**: Check for missing or inadequate input validation\n\n           - **SQL Injection**: Look for raw SQL queries or improper use of query builders\n\n           - **Command Injection**: Identify unsafe use of exec, shell commands, or system calls\n\n           - **Path Traversal**: Check for unsafe file path handling\n\n           - **Sensitive Data Exposure**: Look for hardcoded secrets, API keys, or credentials\n\n           - **Authentication/Authorization**: Review any auth-related changes\n\n           - **Error Handling**: Check for information leakage in error messages\n\n           - **Dependency Security**: Note any new dependencies that should be vetted\n\n           - **Race Conditions**: Identify potential concurrency issues in Go code\n\n           - **Unsafe Pointer Usage**: Check for unsafe package usage\n\n\n\n        3. **Go-Specific Checks**:\n\n           - Run `go_vulncheck` to check for known vulnerabilities\n\n           - Review use of `unsafe` package\n\n           - Check for proper context cancellation and timeout handling\n\n           - Verify proper error wrapping and handling\n\n\n\n        4. **Report**: Provide a structured security review with:\n\n           - **Summary**: Overall security posture of the changes\n\n           - **Findings**: List of identified issues with severity (Critical/High/Medium/Low/Info)\n\n           - **Recommendations**: Specific suggestions to improve security\n\n           - **Tips**: General security best practices relevant to the changes\n\n\n\n  planner:\n\n    model: claude\n\n    instruction: |\n\n      You are a planning agent responsible for gathering user requirements and creating a development plan.\n\n      Always ask clarifying questions to ensure you fully understand the user's needs before creating the plan.\n\n      Once you have a clear understanding, analyze the existing code and create a detailed development plan in a markdown file. Do not write any code yourself.\n\n      Once the plan is created, you will delegate tasks to the root agent. Make sure to provide the file name of the plan when delegating. Write the plan in the current directory.\n\n    toolsets:\n\n      - type: filesystem\n\n    sub_agents:\n\n      - root\n\n\n\n  reviewer:\n\n    model: google/gemini-3-pro-preview\n\n    instruction: |\n\n      Give me feedback about the local changes. Don't be too picky, think about code quality, security, duplication, idiomatic Go,\n\n      performance, maintainability, and best practices.\n\n      Provide suggestions for improvements and point out any potential issues.\n\n      Don't be too verbose, keep your review concise and to the point.\n\n    add_prompt_files:\n\n      - AGENTS.md\n\n    sub_agents:\n\n      - librarian\n\n    toolsets:\n\n      - type: filesystem\n\n      - type: shell\n\n      - type: mcp\n\n        command: gopls\n\n        args: [\"mcp\"]\n\n\n\n  librarian:\n\n    model: haiku\n\n    description: Documentation librarian. Can search the Web and look for relevant documentation to help the golang developer agent.\n\n    instruction: |\n\n      You are the librarian, your job is to look for relevant documentation to help the golang developer agent.\n\n      When given a query, search the internet for relevant documentation, articles, or resources that can assist in completing the task.\n\n      Use context7 for searching documentation and brave for general web searches.\n\n      A good source of information available to agents is https://deepwiki.com/.\n\n    toolsets:\n\n      - type: mcp\n\n        ref: docker:context7\n\n      - type: mcp\n\n        ref: docker:brave\n\n      - type: fetch\n\n\n\npermissions:\n\n  allow:\n\n    - go_diagnostics\n\n    - go_file_context\n\n    - go_package_api\n\n    - go_symbol_references\n\n    - go_vulncheck\n\n    - go_workspace\n\n    - shell:cmd=gh --version\n\n    - shell:cmd=gh pr view *\n\n    - shell:cmd=gh pr diff *\n\n    - shell:cmd=git remote -v\n\n    - shell:cmd=ls *\n\n    - shell:cmd=cat *\n\n    - shell:cmd=head *\n\n    - shell:cmd=tail *\n\n    - shell:cmd=wc *\n\n    - shell:cmd=find *\n\n    - shell:cmd=grep *\n\n    - shell:cmd=pwd\n\n    - shell:cmd=echo *\n\n    - shell:cmd=which *\n\n    - shell:cmd=type *\n\n    - shell:cmd=file *\n\n    - shell:cmd=stat *\n\n    - shell:cmd=git status*\n\n    - shell:cmd=git log*\n\n    - shell:cmd=git diff*\n\n    - shell:cmd=git show*\n\n    - shell:cmd=git branch*\n\n    - shell:cmd=git remote -v*\n\n    - shell:cmd=git commit *\n\n    - shell:cmd=go test*\n\n    - shell:cmd=go build*\nTechnical blog writer\nShow more\n#!/usr/bin/env cagent run\n\n\n\nagents:\n\n  root:\n\n    model: anthropic\n\n    description: Writes technical blog posts\n\n    instruction: |\n\n      You are the leader of a team of AI agents for a technical blog writing workflow.\n\n\n\n      Here are the members in your team:\n\n      <team_members>\n\n      - web_search_agent: Searches the web\n\n      - writer: Writes a 750-word technical blog post based on the chosen prompt\n\n      </team_members>\n\n\n\n      WORKFLOW\n\n        1. Call the `web_search_agent` agent to search for the web to get important information about the task that is asked\n\n\n\n        3. Call the `writer` agent to write a 750-word technical blog post based on the research done by the web_search_agent\n\n      </WORKFLOW>\n\n\n\n      - Use the transfer_to_agent tool to call the right agent at the right time to complete the workflow.\n\n      - DO NOT transfer to multiple members at once\n\n      - ONLY CALL ONE AGENT AT A TIME\n\n      - When using the `transfer_to_agent` tool, make exactly one call and wait for the result before making another. Do not batch or parallelize tool calls.\n\n    sub_agents:\n\n      - web_search_agent\n\n      - writer\n\n    toolsets:\n\n      - type: think\n\n\n\n  web_search_agent:\n\n    model: anthropic\n\n    add_date: true\n\n    description: Search the web for the information\n\n    instruction: |\n\n      Search the web for the information\n\n\n\n      Always include sources\n\n    toolsets:\n\n      - type: mcp\n\n        ref: docker:duckduckgo\n\n\n\n  writer:\n\n    model: anthropic\n\n    description: Writes a 750-word technical blog post based on the chosen prompt.\n\n    instruction: |\n\n      You are an agent that receives a single technical writing prompt and generates a detailed, informative, and well-structured technical blog post.\n\n\n\n      - Ensure the content is technically accurate and includes relevant code examples, diagrams, or technical explanations where appropriate.\n\n      - Structure the blog post with clear sections, including an introduction, main content, and conclusion.\n\n      - Use technical terminology appropriately and explain complex concepts clearly.\n\n      - Include practical examples and real-world applications where relevant.\n\n      - Make sure the content is engaging for a technical audience while maintaining professional standards.\n\n\n\n      Constraints:\n\n      - DO NOT use lists\n\n\n\nmodels:\n\n  anthropic:\n\n    provider: anthropic\n\n    model: claude-3-7-sonnet-latest\n\nEdit this page\n\nRequest changes\n\nTable of contents\nDevelopment team\nGo developer\nTechnical blog writer\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996096,
    "timestamp": "2026-02-07T06:37:10.037Z",
    "title": "Evals | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/evals/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nEvals\nEvals\nCopy as Markdown\n\nEvaluations (evals) help you track how your agent's behavior changes over time. When you save a conversation as an eval, you can replay it later to see if the agent responds differently. Evals measure consistency, not correctness - they tell you if behavior changed, not whether it's right or wrong.\n\nWhat are evals\n\nAn eval is a saved conversation you can replay. When you run evals, cagent replays the user messages and compares the new responses against the original saved conversation. High scores mean the agent behaved similarly; low scores mean behavior changed.\n\nWhat you do with that information depends on why you saved the conversation. You might save successful conversations to catch regressions, or save failure cases to document known issues and track whether they improve.\n\nCommon workflows\n\nHow you use evals depends on what you're trying to accomplish:\n\nRegression testing: Save conversations where your agent performs well. When you make changes later (upgrade models, update prompts, refactor code), run the evals. High scores mean behavior stayed consistent, which is usually what you want. Low scores mean something changed - examine the new behavior to see if it's still correct.\n\nTracking improvements: Save conversations where your agent struggles or fails. As you make improvements, run these evals to see how behavior evolves. Low scores indicate the agent now behaves differently, which might mean you fixed the issue. You'll need to manually verify the new behavior is actually better.\n\nDocumenting edge cases: Save interesting or unusual conversations regardless of quality. Use them to understand how your agent handles edge cases and whether that behavior changes over time.\n\nEvals measure whether behavior changed. You determine if that change is good or bad.\n\nCreating an eval\n\nSave a conversation from an interactive session:\n\n$ cagent run ./agent.yaml\n\n\nHave a conversation with your agent, then save it as an eval:\n\n> /eval test-case-name\n\nEval saved to evals/test-case-name.json\n\n\nThe conversation is saved to the evals/ directory in your current working directory. You can organize eval files in subdirectories if needed.\n\nRunning evals\n\nRun all evals in the default directory:\n\n$ cagent eval ./agent.yaml\n\n\nUse a custom eval directory:\n\n$ cagent eval ./agent.yaml ./my-evals\n\n\nRun evals against an agent from a registry:\n\n$ cagent eval agentcatalog/myagent\n\n\nExample output:\n\n$ cagent eval ./agent.yaml\n\n--- 0\n\nFirst message: tell me something interesting about kil\n\nEval file: c7e556c5-dae5-4898-a38c-73cc8e0e6abe\n\nTool trajectory score: 1.000000\n\nRouge-1 score: 0.447368\n\nCost: 0.00\n\nOutput tokens: 177\n\nUnderstanding results\n\nFor each eval, cagent shows:\n\nFirst message - The initial user message from the saved conversation\nEval file - The UUID of the eval file being run\nTool trajectory score - How similarly the agent used tools (0-1 scale, higher is better)\nROUGE-1 score - Text similarity between responses (0-1 scale, higher is better)\nCost - The cost for this eval run\nOutput tokens - Number of tokens generated\n\nHigher scores mean the agent behaved more similarly to the original recorded conversation. A score of 1.0 means identical behavior.\n\nWhat the scores mean\n\nTool trajectory score measures whether the agent called the same tools in the same order as the original conversation. Lower scores might indicate the agent found a different approach to solve the problem, which isn't necessarily wrong but worth investigating.\n\nRouge-1 score measures how similar the response text is to the original. This is a heuristic measure - different wording might still be correct, so use this as a signal rather than absolute truth.\n\nInterpreting your results\n\nScores close to 1.0 mean your changes maintained consistent behavior - the agent is using the same approach and producing similar responses. This is generally good; your changes didn't break existing functionality.\n\nLower scores mean behavior changed compared to the saved conversation. This could be a regression where the agent now performs worse, or it could be an improvement where the agent found a better approach.\n\nWhen scores drop, examine the actual behavior to determine if it's better or worse. The eval files are stored as JSON in your evals directory - open the file to see the original conversation. Then test your modified agent with the same input to compare responses. If the new response is better, save a new conversation to replace the eval. If it's worse, you found a regression.\n\nThe scores guide you to what changed. Your judgment determines if the change is good or bad.\n\nWhen to use evals\n\nEvals help you track behavior changes over time. They're useful for catching regressions when you upgrade models or dependencies, documenting known failure cases you want to fix, and understanding how edge cases evolve as you iterate.\n\nEvals aren't appropriate for determining which agent configuration works best - they measure similarity to a saved conversation, not correctness. Use manual testing to evaluate different configurations and decide which works better.\n\nSave conversations worth tracking. Build a collection of important workflows, interesting edge cases, and known issues. Run your evals when making changes to see what shifted.\n\nWhat's next\nCheck the CLI reference for all cagent eval options\nLearn best practices for building effective agents\nReview example configurations for different agent types\n\nEdit this page\n\nRequest changes\n\nTable of contents\nWhat are evals\nCommon workflows\nCreating an eval\nRunning evals\nUnderstanding results\nWhat the scores mean\nInterpreting your results\nWhen to use evals\nWhat's next\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  },
  {
    "id": 258996093,
    "timestamp": "2026-02-07T06:37:10.038Z",
    "title": "RAG | Docker Docs",
    "url": "https://docs.docker.com/ai/cagent/rag/",
    "text": "Get started\nGuides\nManuals\nReference\nAsk¬†AI\nOPEN SOURCE\nDocker Engine\nDocker Build\nDocker Compose\nTestcontainers\ncagent Experimental\nModel providers\nLocal models\nBuilding a coding agent\nBest practices\nSharing agents\nIntegrations\nReference\nRAG\nEvals\nAI\nMCP Catalog and Toolkit Beta\nDocker Sandboxes Experimental\nModel Runner\nAsk Gordon Beta\nAI and Docker Compose\nPRODUCTS\nDocker Hardened Images New\nDocker Desktop\nDocker Offload Early Access\nDocker Build Cloud\nDocker Hub\nDocker Scout\nDocker Extensions\nTestcontainers Cloud\nDeprecated products and features\nRelease lifecycle\nPLATFORM\nSupport\nBilling\nDocker accounts\nSecurity\nSubscription\nRelease notes\nENTERPRISE\nAdministration\nDeploy Docker Desktop\nSecurity\nTroubleshoot\nHome\n/\nManuals\n/\ncagent\n/\nRAG\nRAG\nCopy as Markdown\n\nWhen you configure a RAG source in cagent, your agent automatically gains a search tool for that knowledge base. The agent decides when to search, retrieves only relevant information, and uses it to answer questions or complete tasks - all without you manually managing what goes in the prompt.\n\nThis guide explains how cagent's RAG system works, when to use it, and how to configure it effectively for your content.\n\nNote\n\nRAG is an advanced feature that requires configuration and tuning. The defaults work well for getting started, but tailoring the configuration to your specific content and use case significantly improves results.\n\nThe problem: too much context\n\nYour agent can work with your entire codebase, but it can't fit everything in its context window. Even with 200K token limits, medium-sized projects are too large. Finding relevant code buried in hundreds of files wastes context.\n\nFilesystem tools help agents read files, but the agent has to guess which files to read. It can't search by meaning, only by filename. Ask \"find the retry logic\" and the agent reads files hoping to stumble on the right code.\n\nGrep finds exact text matches but misses related concepts. Searching \"authentication\" won't find code using \"auth\" or \"login.\" You either get hundreds of matches or zero, and grep doesn't understand code structure - it just matches strings anywhere they appear.\n\nRAG indexes your content ahead of time and enables semantic search. The agent searches pre-indexed content by meaning, not exact words. It retrieves only relevant chunks that respect code structure. No wasted context on exploration.\n\nHow RAG works in cagent\n\nConfigure a RAG source in your cagent config:\n\nrag:\n\n  codebase:\n\n    docs: [./src, ./pkg]\n\n    strategies:\n\n      - type: chunked-embeddings\n\n        embedding_model: openai/text-embedding-3-small\n\n        vector_dimensions: 1536\n\n        database: ./code.db\n\n\n\nagents:\n\n  root:\n\n    model: openai/gpt-5\n\n    instruction: You are a coding assistant. Search the codebase when needed.\n\n    rag: [codebase]\n\nWhen you reference rag: [codebase], cagent:\n\nAt startup - Indexes your documents (first run only, blocks until complete)\nDuring conversation - Gives the agent a search tool\nWhen the agent searches - Retrieves relevant chunks and adds them to context\nOn file changes - Automatically re-indexes modified files\n\nThe agent decides when to search based on the conversation. You don't manage what goes in context - the agent does.\n\nThe indexing process\n\nOn first run, cagent:\n\nReads files from configured paths\nRespects .gitignore patterns (can be disabled)\nSplits documents into chunks\nCreates searchable representations using your chosen strategy\nStores everything in a local database\n\nSubsequent runs reuse the index. If files change, cagent detects this and re-indexes only what changed, keeping your knowledge base up to date without manual intervention.\n\nRetrieval strategies\n\nDifferent content requires different retrieval approaches. cagent supports three strategies, each optimized for different use cases. The defaults work well, but understanding the trade-offs helps you choose the right approach.\n\nSemantic search (chunked-embeddings)\n\nConverts text to vectors that represent meaning, enabling search by concept rather than exact words:\n\nstrategies:\n\n  - type: chunked-embeddings\n\n    embedding_model: openai/text-embedding-3-small\n\n    vector_dimensions: 1536\n\n    database: ./docs.db\n\n    chunking:\n\n      size: 1000\n\n      overlap: 100\n\nDuring indexing, documents are split into chunks and each chunk is converted to a 1536-dimensional vector by the embedding model. These vectors are essentially coordinates in a high-dimensional space where similar concepts are positioned close together.\n\nWhen you search for \"how do I authenticate users?\", your query becomes a vector and the database finds chunks with nearby vectors using cosine similarity (measuring the angle between vectors). The embedding model learned that \"authentication,\" \"auth,\" and \"login\" are related concepts, so searching for one finds the others.\n\nExample: The query \"how do I authenticate users?\" finds both \"User authentication requires a valid API token\" and \"Token-based auth validates requests\" despite different wording. It won't find \"The authentication tests are failing\" because that's a different meaning despite containing the word.\n\nThis works well for documentation where users ask questions using different terminology than your docs. The downside is it may miss exact technical terms and sometimes you want literal matches, not semantic ones. Requires embedding API calls during indexing.\n\nKeyword search (BM25)\n\nStatistical algorithm that matches and ranks by term frequency and rarity:\n\nstrategies:\n\n  - type: bm25\n\n    database: ./bm25.db\n\n    k1: 1.5\n\n    b: 0.75\n\n    chunking:\n\n      size: 1000\n\n      overlap: 100\n\nDuring indexing, documents are tokenized and the algorithm calculates how often each term appears (term frequency) and how rare it is across all documents (inverse document frequency). The scoring index is stored in a local SQLite database.\n\nWhen you search for \"HandleRequest function\", the algorithm finds chunks containing these exact terms and scores them based on term frequency, term rarity, and document length. Finding \"HandleRequest\" is scored as more significant than finding common words like \"function\". Think of it as grep with statistical ranking.\n\nExample: Searching \"HandleRequest function\" finds func HandleRequest(w http.ResponseWriter, r *http.Request) and \"The HandleRequest function processes incoming requests\", but not \"process HTTP requests\" despite that being semantically similar.\n\nThe k1 parameter (default 1.5) controls how much repeated terms matter - higher values emphasize repetition more. The b parameter (default 0.75) controls length normalization - higher values penalize longer documents more.\n\nThis is fast, local (no API costs), and predictable for finding function names, class names, API endpoints, and any identifier that appears verbatim. The trade-off is zero understanding of meaning - \"RetryHandler\" and \"retry logic\" won't match despite being related. Essential complement to semantic search.\n\nLLM-enhanced semantic search (semantic-embeddings)\n\nGenerates semantic summaries with an LLM before embedding, enabling search by what code does rather than what it's called:\n\nstrategies:\n\n  - type: semantic-embeddings\n\n    embedding_model: openai/text-embedding-3-small\n\n    chat_model: openai/gpt-5-mini\n\n    vector_dimensions: 1536\n\n    database: ./code.db\n\n    ast_context: true\n\n    chunking:\n\n      size: 1000\n\n      code_aware: true\n\nDuring indexing, code is split using AST structure (functions stay intact), then the chat_model generates a semantic summary of each chunk. The summary gets embedded, not the raw code. When you search, your query matches against these summaries, but the original code is returned.\n\nThis solves a problem with regular embeddings: raw code embeddings are dominated by variable names and implementation details. A function called processData that implements retry logic won't semantically match \"retry\". But when the LLM summarizes it first, the summary explicitly mentions \"retry logic,\" making it findable.\n\nExample: Consider this code:\n\nfunc (c *Client) Do(req *Request) (*Response, error) {\n\n    for i := 0; i < 3; i++ {\n\n        resp, err := c.attempt(req)\n\n        if err == nil { return resp, nil }\n\n        time.Sleep(time.Duration(1<<i) * time.Second)\n\n    }\n\n    return nil, errors.New(\"max retries exceeded\")\n\n}\n\nThe LLM summary is: \"Implements exponential backoff retry logic for HTTP requests, attempting up to 3 times with delays of 1s, 2s, 4s before failing.\"\n\nSearching \"retry logic exponential backoff\" now finds this code, despite the code never using those words. The ast_context: true option includes AST metadata in prompts for better understanding. The code_aware: true chunking prevents splitting functions mid-implementation.\n\nThis approach excels at finding code by behavior in large codebases with inconsistent naming. The trade-off is significantly slower indexing (LLM call per chunk) and higher API costs (both chat and embedding models). Often overkill for well-documented code or simple projects.\n\nCombining strategies with hybrid retrieval\n\nEach strategy has strengths and weaknesses. Combining them captures both semantic understanding and exact term matching:\n\nrag:\n\n  knowledge:\n\n    docs: [./documentation, ./src]\n\n    strategies:\n\n      - type: chunked-embeddings\n\n        embedding_model: openai/text-embedding-3-small\n\n        vector_dimensions: 1536\n\n        database: ./vector.db\n\n        limit: 20\n\n\n\n      - type: bm25\n\n        database: ./bm25.db\n\n        limit: 15\n\n\n\n    results:\n\n      fusion:\n\n        strategy: rrf\n\n        k: 60\n\n      deduplicate: true\n\n      limit: 5\nHow fusion works\n\nBoth strategies run in parallel, each returning its top candidates (20 and 15 in this example). Fusion combines results using rank-based scoring, removes duplicates, and returns the top 5 final results. Your agent gets results that work for both semantic queries (\"how do I...\") and exact term searches (\"find configure_auth function\").\n\nFusion strategies\n\nRRF (Reciprocal Rank Fusion) is recommended. It combines results based on rank rather than absolute scores, which works reliably when strategies use different scoring scales. No tuning required.\n\nFor weighted fusion, you give more importance to one strategy:\n\nfusion:\n\n  strategy: weighted\n\n  weights:\n\n    chunked-embeddings: 0.7\n\n    bm25: 0.3\n\nThis requires tuning for your content. Use it when you know one approach works better for your use case.\n\nMax score fusion takes the highest score across strategies:\n\nfusion:\n\n  strategy: max\n\nThis only works if strategies use comparable scoring scales. Simple but less sophisticated than RRF.\n\nImproving retrieval quality\nReranking results\n\nInitial retrieval optimizes for speed. Reranking rescores results with a more sophisticated model for better relevance:\n\nresults:\n\n  reranking:\n\n    model: openai/gpt-5-mini\n\n    threshold: 0.3\n\n    criteria: |\n\n      When scoring relevance, prioritize:\n\n      - Official documentation over community content\n\n      - Recent information over outdated material\n\n      - Practical examples over theoretical explanations\n\n      - Code implementations over design discussions\n\n  limit: 5\n\nThe criteria field is powerful - use it to encode domain knowledge about what makes results relevant for your specific use case. The more specific your criteria, the better the reranking.\n\nTrade-off: Significantly better results but adds latency and API costs (LLM call for scoring each result).\n\nChunking configuration\n\nHow you split documents dramatically affects retrieval quality. Tailor chunking to your content type. Chunk size is measured in characters (Unicode code points), not tokens.\n\nFor documentation and prose, use moderate chunks with overlap:\n\nchunking:\n\n  size: 1000\n\n  overlap: 100\n\n  respect_word_boundaries: true\n\nOverlap preserves context at chunk boundaries. Respecting word boundaries prevents cutting words in half.\n\nFor code, use larger chunks with AST-based splitting:\n\nchunking:\n\n  size: 2000\n\n  code_aware: true\n\nThis keeps functions intact. The code_aware setting uses tree-sitter to respect code structure.\n\nNote\n\nCurrently only Go is supported; support for additional languages is planned.\n\nFor short, focused content like API references:\n\nchunking:\n\n  size: 500\n\n  overlap: 50\n\nBrief sections need less overlap since they're naturally self-contained.\n\nExperiment with these values. If retrieval misses context, increase chunk size or overlap. If results are too broad, decrease chunk size.\n\nMaking decisions about RAG\nWhen to use RAG\n\nUse RAG when:\n\nYour content is too large for the context window\nYou want targeted retrieval, not everything at once\nContent changes and needs to stay current\nAgent needs to search across many files\n\nDon't use RAG when:\n\nContent is small enough to include in agent instructions\nInformation rarely changes (consider prompt engineering instead)\nYou need real-time data (RAG uses pre-indexed snapshots)\nContent is already in a searchable format the agent can query directly\nChoosing retrieval strategies\n\nUse semantic search (chunked-embeddings) for user-facing documentation, content with varied terminology, and conceptual searches where users phrase questions differently than your docs.\n\nUse keyword search (BM25) for code identifiers, function names, API endpoints, error messages, and any content where exact term matching matters. Essential for technical jargon and proper nouns.\n\nUse LLM-enhanced semantic (semantic-embeddings) for code search by functionality, finding implementations by behavior rather than name, or complex technical content requiring deep understanding. Choose this when accuracy matters more than indexing speed.\n\nUse hybrid (multiple strategies) for general-purpose search across mixed content, when you're unsure which approach works best, or for production systems where quality matters most. Maximum coverage at the cost of complexity.\n\nTuning for your project\n\nStart with defaults, then adjust based on results.\n\nIf retrieval misses relevant content:\n\nIncrease limit in strategies to retrieve more candidates\nAdjust threshold to be less strict\nIncrease chunk size to capture more context\nAdd more retrieval strategies\n\nIf retrieval returns irrelevant content:\n\nDecrease limit to fewer candidates\nIncrease threshold to be more strict\nAdd reranking with specific criteria\nDecrease chunk size for more focused results\n\nIf indexing is too slow:\n\nIncrease batch_size for fewer API calls\nIncrease max_embedding_concurrency for parallelism\nConsider BM25 instead of embeddings (local, no API)\nUse smaller embedding models\n\nIf results lack context:\n\nIncrease chunk overlap\nIncrease chunk size\nUse return_full_content: true to return entire documents\nAdd neighboring chunks to results\nFurther reading\nConfiguration reference - Complete RAG options and parameters\nRAG examples - Working configurations for different scenarios\nTools reference - How RAG search tools work in agent workflows\n\nEdit this page\n\nRequest changes\n\nTable of contents\nThe problem: too much context\nHow RAG works in cagent\nThe indexing process\nRetrieval strategies\nSemantic search (chunked-embeddings)\nKeyword search (BM25)\nLLM-enhanced semantic search (semantic-embeddings)\nCombining strategies with hybrid retrieval\nHow fusion works\nFusion strategies\nImproving retrieval quality\nReranking results\nChunking configuration\nMaking decisions about RAG\nWhen to use RAG\nChoosing retrieval strategies\nTuning for your project\nFurther reading\nProduct offerings\nPricing\nAbout us\nllms.txt\nCookies Settings\n|\nTerms of Service\n|\nStatus\n|\nLegal\nCopyright ¬© 2013-2026 Docker Inc. All rights reserved."
  }
]
