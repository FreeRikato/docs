[
  {
    "id": 60494345,
    "timestamp": "2026-02-25T08:09:37.299Z",
    "title": "LangChain overview - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/overview",
    "text": "Error 500\nError loading page\n\nAn unexpected error occurred. Please contact support to get help."
  },
  {
    "id": 60494346,
    "timestamp": "2026-02-25T08:09:37.307Z",
    "title": "Install LangChain - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/install",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nGet started\nInstall LangChain\nCopy page\nTo install the LangChain package:\npip\nuv\nCopy\npip install -U langchain\n# Requires Python 3.10+\n\nLangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages.\npip\nuv\nCopy\n# Installing the OpenAI integration\npip install -U langchain-openai\n\n# Installing the Anthropic integration\npip install -U langchain-anthropic\n\nSee the Integrations tab for a full list of available integrations.\nNow that you have LangChain installed, you can get started by following the Quickstart guide.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494347,
    "timestamp": "2026-02-25T08:09:37.307Z",
    "title": "Quickstart - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/quickstart",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nRequirements\nBuild a basic agent\nBuild a real-world agent\nGet started\nQuickstart\nCopy page\nThis quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.\nLangChain Docs MCP server\nIf youâ€™re using an AI coding assistant or IDE (e.g. Claude Code or Cursor), you should install the LangChain Docs MCP server to get the most out of it. This ensures your agent has access to up-to-date LangChain documentation and examples.\nï¿½\nRequirements\nFor these examples, you will need to:\nInstall the LangChain package\nSet up a Claude (Anthropic) account and get an API key\nSet the ANTHROPIC_API_KEY environment variable in your terminal\nAlthough these examples use Claude, you can use any supported model by changing the model name in the code and setting up the appropriate API key.\nï¿½\nBuild a basic agent\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.\nCopy\nfrom langchain.agents import create_agent\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    system_prompt=\"You are a helpful assistant\",\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n\nTo learn how to trace your agent with LangSmith, see the LangSmith documentation.\nï¿½\nBuild a real-world agent\nNext, build a practical weather forecasting agent that demonstrates key production concepts:\nDetailed system prompts for better agent behavior\nCreate tools that integrate with external data\nModel configuration for consistent responses\nStructured output for predictable results\nConversational memory for chat-like interactions\nCreate and run the agent to test the fully functional agent\nLetâ€™s walk through each step:\n1\n\nDefine the system prompt\n\nThe system prompt defines your agentâ€™s role and behavior. Keep it specific and actionable:\nCopy\nSYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n\nYou have access to two tools:\n\n- get_weather_for_location: use this to get the weather for a specific location\n- get_user_location: use this to get the user's location\n\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n\n2\n\nCreate tools\n\nTools let a model interact with external systems by calling functions you define. Tools can depend on runtime context and also interact with agent memory.\nNotice below how the get_user_location tool uses runtime context:\nCopy\nfrom dataclasses import dataclass\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather_for_location(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\n@dataclass\nclass Context:\n    \"\"\"Custom runtime context schema.\"\"\"\n    user_id: str\n\n@tool\ndef get_user_location(runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Retrieve user information based on user ID.\"\"\"\n    user_id = runtime.context.user_id\n    return \"Florida\" if user_id == \"1\" else \"SF\"\n\nTools should be well-documented: their name, description, and argument names become part of the modelâ€™s prompt. LangChainâ€™s @tool decorator adds metadata and enables runtime injection with the ToolRuntime parameter.\n3\n\nConfigure your model\n\nSet up your language model with the right parameters for your use case:\nCopy\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    temperature=0.5,\n    timeout=10,\n    max_tokens=1000\n)\n\nDepending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.\n4\n\nDefine response format\n\nOptionally, define a structured response format if you need the agent responses to match a specific schema.\nCopy\nfrom dataclasses import dataclass\n\n# We use a dataclass here, but Pydantic models are also supported.\n@dataclass\nclass ResponseFormat:\n    \"\"\"Response schema for the agent.\"\"\"\n    # A punny response (always required)\n    punny_response: str\n    # Any interesting information about the weather if available\n    weather_conditions: str | None = None\n\n5\n\nAdd memory\n\nAdd memory to your agent to maintain state across interactions. This allows the agent to remember previous conversations and context.\nCopy\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n\nIn production, use a persistent checkpointer that saves message history to a database. See Add and manage memory for more details.\n6\n\nCreate and run the agent\n\nNow assemble your agent with all the components and run it!\nCopy\nfrom langchain.agents.structured_output import ToolStrategy\n\nagent = create_agent(\n    model=model,\n    system_prompt=SYSTEM_PROMPT,\n    tools=[get_user_location, get_weather_for_location],\n    context_schema=Context,\n    response_format=ToolStrategy(ResponseFormat),\n    checkpointer=checkpointer\n)\n\n# `thread_id` is a unique identifier for a given conversation.\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n    config=config,\n    context=Context(user_id=\"1\")\n)\n\nprint(response['structured_response'])\n# ResponseFormat(\n#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n#     weather_conditions=\"It's always sunny in Florida!\"\n# )\n\n\n# Note that we can continue the conversation using the same `thread_id`.\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n    config=config,\n    context=Context(user_id=\"1\")\n)\n\nprint(response['structured_response'])\n# ResponseFormat(\n#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n#     weather_conditions=None\n# )\n\n\nShow Full example code\n\nTo learn how to trace your agent with LangSmith, see the LangSmith documentation.\nCongratulations! You now have an AI agent that can:\nUnderstand context and remember conversations\nUse multiple tools intelligently\nProvide structured responses in a consistent format\nHandle user-specific information through context\nMaintain conversation state across interactions\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494349,
    "timestamp": "2026-02-25T08:09:37.320Z",
    "title": "Philosophy - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/philosophy",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nGet started\nPhilosophy\nCopy page\n\nLangChain exists to be the easiest place to start building with LLMs, while also being flexible and production-ready.\n\nLangChain is driven by a few core beliefs:\nLarge Language Models (LLMs) are great, powerful new technology.\nLLMs are even better when you combine them with external sources of data.\nLLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.\nIt is still very early on in that transformation.\nWhile itâ€™s easy to build a prototype of those agentic applications, itâ€™s still really hard to build agents that are reliable enough to put into production.\nWith LangChain, we have two core focuses:\n1\n\nWe want to enable developers to build with the best models.\n\nDifferent providers expose different APIs, with different model parameters and different message formats. Standardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.\n2\n\nWe want to make it easy to use models to orchestrate more complex flows that interact with other data and computation.\n\nModels should be used for more than just text generation - they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define tools that LLMs can use dynamically, as well as help with parsing of and access to unstructured data.\nï¿½\nHistory\nGiven the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:\nï¿½\n2022-10-24\nv0.0.1\nA month before ChatGPT, LangChain was launched as a Python package. It consisted of two main components:\nLLM abstractions\nâ€œChainsâ€, or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step.\nThe name LangChain comes from â€œLanguageâ€ (like Language models) and â€œChainsâ€.\nï¿½\n2022-12\nThe first general purpose agents were added to LangChain.\nThese general purpose agents were based on the ReAct paper (ReAct standing for Reasoning and Acting). They used LLMs to generate JSON that represented tool calls, and then parsed that JSON to determine what tools to call.\nï¿½\n2023-01\nOpenAI releases a â€˜Chat Completionâ€™ API.\nPreviously, models took in strings and returned a string. In the ChatCompletions API, they evolved to take in a list of messages and return a message. Other model providers followed suit, and LangChain updated to work with lists of messages.\nï¿½\n2023-01\nLangChain releases a JavaScript version.\nLLMs and agents will change how applications are built and JavaScript is the language of application developers.\nï¿½\n2023-02\nLangChain Inc. was formed as a company around the open source LangChain project.\nThe main goal was to â€œmake intelligent agents ubiquitousâ€. The team recognized that while LangChain was a key part (LangChain made it simple to get started with LLMs), there was also a need for other components.\nï¿½\n2023-03\nOpenAI releases â€˜function callingâ€™ in their API.\nThis allowed the API to explicitly generate payloads that represented tool calls. Other model providers followed suit, and LangChain was updated to use this as the preferred method for tool calling (rather than parsing JSON).\nï¿½\n2023-06\nLangSmith is released as closed source platform by LangChain Inc., providing observability and evals.\nThe main issue with building agents is getting them to be reliable, and LangSmith, which provides observability and evals, was built to solve that need. LangChain was updated to integrate seamlessly with LangSmith.\nï¿½\n2024-01\nv0.1.0\nLangChain releases 0.1.0, its first non-0.0.x.\nThe industry matured from prototypes to production, and as such, LangChain increased its focus on stability.\nï¿½\n2024-02\nLangGraph is released as an open-source library.\nThe original LangChain had two focuses: LLM abstractions, and high-level interfaces for getting started with common applications; however, it was missing a low-level orchestration layer that allowed developers to control the exact flow of their agent. Enter: LangGraph.\nWhen building LangGraph, we learned from lessons when building LangChain and added functionality we discovered was needed: streaming, durable execution, short-term memory, human-in-the-loop, and more.\nï¿½\n2024-06\nLangChain has over 700 integrations.\nIntegrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or langchain-community.\nï¿½\n2024-10\nLangGraph becomes the preferred way to build any AI application that is more than a single LLM call.\nAs developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.\nï¿½\n2025-04\nModel APIs become more multimodal.\nModels started to accept files, images, videos, and more. We updated the langchain-core message format accordingly to allow developers to specify these multimodal inputs in a standard way.\nï¿½\n2025-10-20\nv1.0.0\nLangChain releases 1.0 with two major changes:\nComplete revamp of all chains and agents in langchain. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain.\nFor users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the langchain-classic package.\nA standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494348,
    "timestamp": "2026-02-25T08:09:37.334Z",
    "title": "Changelog - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/releases/changelog",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nReference\nLangChain SDK\nLangGraph SDK\nIntegrations\nDeep Agents\nErrors\nReference\nReleases\nVersioning\nChangelog\nReleases\nMigration guides\nPolicies\nRelease policy\nSecurity\nFilters\nlangchain\nintegrations\ndeepagents\nlanggraph\nReleases\nChangelog\nCopy page\n\nLog of updates and improvements to our Python packages\n\nSubscribe: Our changelog includes an RSS feed that can integrate with Slack, email, Discord bots like Readybot or RSS Feeds to Discord Bot, and other subscription tools.\nï¿½\nFeb 10, 2026\ndeepagents\nï¿½\ndeepagents v0.4\nNew integration packages for pluggable sandboxes: langchain-modal, langchain-daytona, and langchain-runloop. See sandboxes guide and example data analysis tutorial.\nChanges to conversation history summarization:\nSummarization now happens in the model node via wrap_model_call events. Due to this we retain the full message history in the graph state.\nMore accurate token counting.\nSummarization will now automatically trigger if a chat model raises a ContextOverflowError (defined in langchain-core). Currently langchain-anthropic and langchain-openai support this.\nWe now default to the Responses API for model strings prefixed with \"openai:\".\nNote: to disable data retention with the Responses API, use\nCopy\nfrom langchain.chat_models import init_chat_model\n\nagent = create_deep_agent(\n    model=init_chat_model(\n        \"openai:...\",\n        use_responses_api=True,\n        store=False,\n        include=[\"reasoning.encrypted_content\"],\n    )\n)\n\nï¿½\nDec 15, 2025\nlangchain\nintegrations\nï¿½\nlangchain v1.2.0\ncreate_agent: Simplified support for provider-specific tool parameters and definitions via a new extras attribute on tools. Examples:\nProvider-specific configuration such as Anthropicâ€™s programmatic tool calling and tool search.\nBuilt-in tools that are executed client-side, as supported by Anthropic, OpenAI, and other providers.\nSupport for strict schema-adherence in agent response_format (see ProviderStrategy docs).\nï¿½\nDec 8, 2025\nlangchain\nintegrations\nï¿½\nlangchain-google-genai v4.0.0\nWeâ€™ve re-written the Google GenAI integration to use Googleâ€™s consolidated Generative AI SDK, which provides access to the Gemini API and Vertex AI Platform under the same interface. This includes minimal breaking changes as well as deprecated packages in langchain-google-vertexai.\nSee the full release notes and migration guide for details.\nï¿½\nNov 25, 2025\nlangchain\nï¿½\nlangchain v1.1.0\nModel profiles: Chat models now expose supported features and capabilities through a .profile attribute. These data are derived from models.dev, an open source project providing model capability data.\nSummarization middleware: Updated to support flexible trigger points using model profiles for context-aware summarization.\nStructured output: ProviderStrategy support (native structured output) can now be inferred from model profiles.\nSystemMessage for create_agent: Support for passing SystemMessage instances directly to create_agentâ€™s system_prompt parameter, enabling advanced features like cache control and structured content blocks.\nModel retry middleware: New middleware for automatically retrying failed model calls with configurable exponential backoff.\nContent moderation middleware: OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results.\nï¿½\nOct 20, 2025\nlangchain\nlanggraph\nï¿½\nv1.0.0\nï¿½\nlangchain\nRelease notes\nMigration guide\nï¿½\nlanggraph\nRelease notes\nMigration guide\nIf you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content and API reference.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494350,
    "timestamp": "2026-02-25T08:09:37.335Z",
    "title": "Agents - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/agents",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCore components\nModel\nStatic model\nDynamic model\nTools\nStatic tools\nDynamic tools\nTool error handling\nTool use in the ReAct loop\nSystem prompt\nDynamic system prompt\nName\nInvocation\nAdvanced concepts\nStructured output\nToolStrategy\nProviderStrategy\nMemory\nDefining state via middleware\nDefining state via state_schema\nStreaming\nMiddleware\nCore components\nAgents\nCopy page\nAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\ncreate_agent provides a production-ready agent implementation.\nAn LLM Agent runs tools in a loop to achieve a goal. An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\naction\n\nobservation\n\nfinish\n\ninput\n\nmodel\n\ntools\n\noutput\n\ncreate_agent builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.\nLearn more about the Graph API.\nï¿½\nCore components\nï¿½\nModel\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\nï¿½\nStatic model\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\nTo initialize a static model from a model identifier string:\nCopy\nfrom langchain.agents import create_agent\n\nagent = create_agent(\"openai:gpt-5\", tools=tools)\n\nModel identifier strings support automatic inference (e.g., \"gpt-5\" will be inferred as \"openai:gpt-5\"). Refer to the reference to see a full list of model identifier string mappings.\nFor more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI. See Chat models for other available chat model classes.\nCopy\nfrom langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-5\",\n    temperature=0.1,\n    max_tokens=1000,\n    timeout=30\n    # ... (other params)\n)\nagent = create_agent(model, tools=tools)\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like temperature, max_tokens, timeouts, base_url, and other provider-specific settings. Refer to the reference to see available params and methods on your model.\nï¿½\nDynamic model\nDynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\nTo use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:\nCopy\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n\n\nbasic_model = ChatOpenAI(model=\"gpt-4.1-mini\")\nadvanced_model = ChatOpenAI(model=\"gpt-4.1\")\n\n@wrap_model_call\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n    \"\"\"Choose model based on conversation complexity.\"\"\"\n    message_count = len(request.state[\"messages\"])\n\n    if message_count > 10:\n        # Use an advanced model for longer conversations\n        model = advanced_model\n    else:\n        model = basic_model\n\n    return handler(request.override(model=model))\n\nagent = create_agent(\n    model=basic_model,  # Default model\n    tools=tools,\n    middleware=[dynamic_model_selection]\n)\n\nPre-bound models (models with bind_tools already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.\nFor model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.\nï¿½\nTools\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\nMultiple tool calls in sequence (triggered by a single prompt)\nParallel tool calls when appropriate\nDynamic tool selection based on previous results\nTool retry logic and error handling\nState persistence across tool calls\nFor more information, see Tools.\nï¿½\nStatic tools\nStatic tools are defined when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\nTo define an agent with static tools, pass a list of the tools to the agent.\nTools can be specified as plain Python functions or coroutines.\nThe tool decorator can be used to customize tool names, descriptions, argument schemas, and other properties.\nCopy\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather information for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72Â°F\"\n\nagent = create_agent(model, tools=[search, get_weather])\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\nï¿½\nDynamic tools\nWith dynamic tools, the set of tools available to the agent is modified at runtime rather than defined all upfront. Not every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection enables adapting the available toolset based on authentication state, user permissions, feature flags, or conversation stage.\nThere are two approaches depending on whether tools are known ahead of time:\nFiltering pre-registered tools\nRuntime tool registration\nWhen all possible tools are known at agent creation time, you can pre-register them and dynamically filter which ones are exposed to the model based on state, permissions, or context.\nState\nStore\nRuntime Context\nEnable advanced tools only after certain conversation milestones:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n@wrap_model_call\ndef state_based_tools(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Filter tools based on conversation State.\"\"\"\n    # Read from State: check if user has authenticated\n    state = request.state\n    is_authenticated = state.get(\"authenticated\", False)\n    message_count = len(state[\"messages\"])\n\n    # Only enable sensitive tools after authentication\n    if not is_authenticated:\n        tools = [t for t in request.tools if t.name.startswith(\"public_\")]\n        request = request.override(tools=tools)\n    elif message_count < 5:\n        # Limit tools early in conversation\n        tools = [t for t in request.tools if t.name != \"advanced_search\"]\n        request = request.override(tools=tools)\n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[public_search, private_search, advanced_search],\n    middleware=[state_based_tools]\n)\n\nThis approach is best when:\nAll possible tools are known at compile/startup time\nYou want to filter based on permissions, feature flags, or conversation state\nTools are static but their availability is dynamic\nSee Dynamically selecting tools for more examples.\nTo learn more about tools, see Tools.\nï¿½\nTool error handling\nTo customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain.messages import ToolMessage\n\n\n@wrap_tool_call\ndef handle_tool_errors(request, handler):\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n    try:\n        return handler(request)\n    except Exception as e:\n        # Return a custom error message to the model\n        return ToolMessage(\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n            tool_call_id=request.tool_call[\"id\"]\n        )\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search, get_weather],\n    middleware=[handle_tool_errors]\n)\n\nThe agent will return a ToolMessage with the custom error message when a tool fails:\nCopy\n[\n    ...\n    ToolMessage(\n        content=\"Tool error: Please check your input and try again. (division by zero)\",\n        tool_call_id=\"...\"\n    ),\n    ...\n]\n\nï¿½\nTool use in the ReAct loop\nAgents follow the ReAct (â€œReasoning + Actingâ€) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\n\nExample of ReAct loop\n\nï¿½\nSystem prompt\nYou can shape how your agent approaches tasks by providing a prompt. The system_prompt parameter can be provided as a string:\nCopy\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\n)\n\nWhen no system_prompt is provided, the agent will infer its task from the messages directly.\nThe system_prompt parameter accepts either a str or a SystemMessage. Using a SystemMessage gives you more control over the prompt structure, which is useful for provider-specific features like Anthropicâ€™s prompt caching:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.messages import SystemMessage, HumanMessage\n\nliterary_agent = create_agent(\n    model=\"anthropic:claude-sonnet-4-5\",\n    system_prompt=SystemMessage(\n        content=[\n            {\n                \"type\": \"text\",\n                \"text\": \"You are an AI assistant tasked with analyzing literary works.\",\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"<the entire contents of 'Pride and Prejudice'>\",\n                \"cache_control\": {\"type\": \"ephemeral\"}\n            }\n        ]\n    )\n)\n\nresult = literary_agent.invoke(\n    {\"messages\": [HumanMessage(\"Analyze the major themes in 'Pride and Prejudice'.\")]}\n)\n\nThe cache_control field with {\"type\": \"ephemeral\"} tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.\nï¿½\nDynamic system prompt\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.\nThe @dynamic_prompt decorator creates middleware that generates system prompts based on the model request:\nCopy\nfrom typing import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass Context(TypedDict):\n    user_role: str\n\n@dynamic_prompt\ndef user_role_prompt(request: ModelRequest) -> str:\n    \"\"\"Generate system prompt based on user role.\"\"\"\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\n    base_prompt = \"You are a helpful assistant.\"\n\n    if user_role == \"expert\":\n        return f\"{base_prompt} Provide detailed technical responses.\"\n    elif user_role == \"beginner\":\n        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n\n    return base_prompt\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[web_search],\n    middleware=[user_role_prompt],\n    context_schema=Context\n)\n\n# The system prompt will be set dynamically based on context\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]},\n    context={\"user_role\": \"expert\"}\n)\n\nFor more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.\nï¿½\nName\nSet an optional name for the agent. This is used as the node identifier when adding the agent as a subgraph in multi-agent systems:\nCopy\nagent = create_agent(\n    model,\n    tools,\n    name=\"research_assistant\"\n)\n\nPrefer snake_case for agent names (e.g., research_assistant instead of Research Assistant). Some model providers reject names containing spaces or special characters with errors. Using alphanumeric characters, underscores, and hyphens only ensures compatibility across all providers. The same applies to tool names.\nï¿½\nInvocation\nYou can invoke an agent by passing an update to its State. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:\nCopy\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]}\n)\n\nFor streaming steps and / or tokens from the agent, refer to the streaming guide.\nOtherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as stream and invoke.\nUse LangSmith to trace, debug, and evaluate your agents.\nï¿½\nAdvanced concepts\nï¿½\nStructured output\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the response_format parameter.\nï¿½\nToolStrategy\nToolStrategy uses artificial tool calling to generate structured output. This works with any model that supports tool calling. ToolStrategy should be used when provider-native structured output (via ProviderStrategy) is not available or reliable.\nCopy\nfrom pydantic import BaseModel\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str\n\nagent = create_agent(\n    model=\"gpt-4.1-mini\",\n    tools=[search_tool],\n    response_format=ToolStrategy(ContactInfo)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n\nï¿½\nProviderStrategy\nProviderStrategy uses the model providerâ€™s native structured output generation. This is more reliable but only works with providers that support native structured output:\nCopy\nfrom langchain.agents.structured_output import ProviderStrategy\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    response_format=ProviderStrategy(ContactInfo)\n)\n\nAs of langchain 1.0, simply passing a schema (e.g., response_format=ContactInfo) will default to ProviderStrategy if the model supports native structured output. It will fall back to ToolStrategy otherwise.\nTo learn about structured output, see Structured output.\nï¿½\nMemory\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\nInformation stored in the state can be thought of as the short-term memory of the agent:\nCustom state schemas must extend AgentState as a TypedDict.\nThere are two ways to define custom state:\nVia middleware (preferred)\nVia state_schema on create_agent\nï¿½\nDefining state via middleware\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.\nCopy\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\nfrom typing import Any\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nclass CustomMiddleware(AgentMiddleware):\n    state_schema = CustomState\n    tools = [tool1, tool2]\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        ...\n\nagent = create_agent(\n    model,\n    tools=tools,\n    middleware=[CustomMiddleware()]\n)\n\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n\nï¿½\nDefining state via state_schema\nUse the state_schema parameter as a shortcut to define custom state that is only used in tools.\nCopy\nfrom langchain.agents import AgentState\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nagent = create_agent(\n    model,\n    tools=[tool1, tool2],\n    state_schema=CustomState\n)\n# The agent can now track additional state beyond messages\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n\nAs of langchain 1.0, custom state schemas must be TypedDict types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.\nDefining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\nstate_schema is still supported for backwards compatibility on create_agent.\nTo learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.\nï¿½\nStreaming\nWeâ€™ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\nCopy\nfrom langchain.messages import AIMessage, HumanMessage\n\nfor chunk in agent.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n}, stream_mode=\"values\"):\n    # Each chunk contains the full state at that point\n    latest_message = chunk[\"messages\"][-1]\n    if latest_message.content:\n        if isinstance(latest_message, HumanMessage):\n            print(f\"User: {latest_message.content}\")\n        elif isinstance(latest_message, AIMessage):\n            print(f\"Agent: {latest_message.content}\")\n    elif latest_message.tool_calls:\n        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")\n\nFor more details on streaming, see Streaming.\nï¿½\nMiddleware\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\nProcess state before the model is called (e.g., message trimming, context injection)\nModify or validate the modelâ€™s response (e.g., guardrails, content filtering)\nHandle tool execution errors with custom logic\nImplement dynamic model selection based on state or context\nAdd custom logging, monitoring, or analytics\nMiddleware integrates seamlessly into the agentâ€™s execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.\nFor comprehensive middleware documentation including decorators like @before_model, @after_model, and @wrap_tool_call, see Middleware.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494351,
    "timestamp": "2026-02-25T08:09:37.343Z",
    "title": "Models - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/models",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nBasic usage\nInitialize a model\nSupported models\nKey methods\nParameters\nInvocation\nInvoke\nStream\nBatch\nTool calling\nStructured output\nAdvanced topics\nModel profiles\nMultimodal\nReasoning\nLocal models\nPrompt caching\nServer-side tool use\nRate limiting\nBase URL or proxy\nLog probabilities\nToken usage\nInvocation config\nConfigurable models\nCore components\nModels\nCopy page\nLLMs are powerful AI tools that can interpret and generate text like humans. Theyâ€™re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.\nIn addition to text generation, many models support:\n Tool calling - calling external tools (like databases queries or API calls) and use results in their responses.\n Structured output - where the modelâ€™s response is constrained to follow a defined format.\n Multimodality - process and return data other than text, such as images, audio, and video.\n Reasoning - models perform multi-step reasoning to arrive at a conclusion.\nModels are the reasoning engine of agents. They drive the agentâ€™s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.\nThe quality and capabilities of the model you choose directly impact your agentâ€™s baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.\nLangChainâ€™s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.\nFor provider-specific integration information and capabilities, see the providerâ€™s chat model page.\nï¿½\nBasic usage\nModels can be utilized in two ways:\nWith agents - Models can be dynamically specified when creating an agent.\nStandalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.\nThe same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\nï¿½\nInitialize a model\nThe easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice (examples below):\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\nðŸ‘‰ Read the OpenAI chat model integration docs\nCopy\npip install -U \"langchain[openai]\"\n\ninit_chat_model\nModel Class\nCopy\nimport os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-4.1\")\n\nCopy\nresponse = model.invoke(\"Why do parrots talk?\")\n\nSee init_chat_model for more detail, including information on how to pass model parameters.\nï¿½\nSupported models\nLangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page.\nï¿½\nKey methods\nInvoke\nThe model takes messages as input and outputs messages after generating a complete response.\nStream\nInvoke the model, but stream the output as it is generated in real-time.\nBatch\nSend multiple requests to a model in a batch for more efficient processing.\nIn addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the integrations page for details.\nï¿½\nParameters\nA chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:\nï¿½\nmodel\nstringrequired\nThe name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the â€™:â€™ format, for example, â€˜openai:o1â€™.\nï¿½\napi_key\nstring\nThe key required for authenticating with the modelâ€™s provider. This is usually issued when you sign up for access to the model. Often accessed by setting an environment variable.\nï¿½\ntemperature\nnumber\nControls the randomness of the modelâ€™s output. A higher number makes responses more creative; lower ones make them more deterministic.\nï¿½\nmax_tokens\nnumber\nLimits the total number of tokens in the response, effectively controlling how long the output can be.\nï¿½\ntimeout\nnumber\nThe maximum time (in seconds) to wait for a response from the model before canceling the request.\nï¿½\nmax_retries\nnumber\nThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\nUsing init_chat_model, pass these parameters as inline **kwargs:\nInitialize using model parameters\nCopy\nmodel = init_chat_model(\n    \"claude-sonnet-4-5-20250929\",\n    # Kwargs passed to the model:\n    temperature=0.7,\n    timeout=30,\n    max_tokens=1000,\n)\n\nEach chat model integration may have additional params used to control provider-specific functionality.\nFor example, ChatOpenAI has use_responses_api to dictate whether to use the OpenAI Responses or Completions API.\nTo find all the parameters supported by a given chat model, head to the chat model integrations page.\nï¿½\nInvocation\nA chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.\nï¿½\nInvoke\nThe most straightforward way to call a model is to use invoke() with a single message or a list of messages.\nSingle message\nCopy\nresponse = model.invoke(\"Why do parrots have colorful feathers?\")\nprint(response)\n\nA list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.\nSee the messages guide for more detail on roles, types, and content.\nDictionary format\nCopy\nconversation = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\n    {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"},\n    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\n]\n\nresponse = model.invoke(conversation)\nprint(response)  # AIMessage(\"J'adore crÃ©er des applications.\")\n\nMessage objects\nCopy\nfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nconversation = [\n    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n    HumanMessage(\"Translate: I love programming.\"),\n    AIMessage(\"J'adore la programmation.\"),\n    HumanMessage(\"Translate: I love building applications.\")\n]\n\nresponse = model.invoke(conversation)\nprint(response)  # AIMessage(\"J'adore crÃ©er des applications.\")\n\nIf the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with â€œChatâ€, e.g., ChatOpenAI(/oss/integrations/chat/openai).\nï¿½\nStream\nMost models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.\nCalling stream() returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:\nBasic text streaming\nStream tool calls, reasoning, and other content\nCopy\nfor chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n    print(chunk.text, end=\"|\", flush=True)\n\nAs opposed to invoke(), which returns a single AIMessage after the model has finished generating its full response, stream() returns multiple AIMessageChunk objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:\nConstruct an AIMessage\nCopy\nfull = None  # None | AIMessageChunk\nfor chunk in model.stream(\"What color is the sky?\"):\n    full = chunk if full is None else full + chunk\n    print(full.text)\n\n# The\n# The sky\n# The sky is\n# The sky is typically\n# The sky is typically blue\n# ...\n\nprint(full.content_blocks)\n# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]\n\nThe resulting message can be treated the same as a message that was generated with invoke() â€“ for example, it can be aggregated into a message history and passed back to the model as conversational context.\nStreaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isnâ€™t streaming-capable would be one that needs to store the entire output in memory before it can be processed.\n\nAdvanced streaming topics\n\nï¿½\nBatch\nBatching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:\nBatch\nCopy\nresponses = model.batch([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n])\nfor response in responses:\n    print(response)\n\nThis section describes a chat model method batch(), which parallelizes model calls client-side.\nIt is distinct from batch APIs supported by inference providers, such as OpenAI or Anthropic.\nBy default, batch() will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with batch_as_completed():\nYield batch responses upon completion\nCopy\nfor response in model.batch_as_completed([\n    \"Why do parrots have colorful feathers?\",\n    \"How do airplanes fly?\",\n    \"What is quantum computing?\"\n]):\n    print(response)\n\nWhen using batch_as_completed(), results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.\nWhen processing a large number of inputs using batch() or batch_as_completed(), you may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary.\nBatch with max concurrency\nCopy\nmodel.batch(\n    list_of_inputs,\n    config={\n        'max_concurrency': 5,  # Limit to 5 parallel calls\n    }\n)\n\nSee the RunnableConfig reference for a full list of supported attributes.\nFor more details on batching, see the reference.\nï¿½\nTool calling\nModels can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:\nA schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)\nA function or coroutine to execute.\nYou may hear the term â€œfunction callingâ€. We use this interchangeably with â€œtool callingâ€.\nHereâ€™s the basic tool calling flow between a user and a model:\nTools\nModel\nUser\nTools\nModel\nUser\npar\n[Parallel Tool Calls]\npar\n[Tool Execution]\n\"What's the weather in SF and NYC?\"\nAnalyze request & decide tools needed\nget_weather(\"San Francisco\")\nget_weather(\"New York\")\nSF weather data\nNYC weather data\nProcess results & generate response\n\"SF: 72Â°F sunny, NYC: 68Â°F cloudy\"\nTo make tools that you have defined available for use by a model, you must bind them using bind_tools. In subsequent invocations, the model can choose to call any of the bound tools as needed.\nSome model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. ChatOpenAI, ChatAnthropic). Check the respective provider reference for details.\nSee the tools guide for details and other options for creating tools.\nBinding user tools\nCopy\nfrom langchain.tools import tool\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    return f\"It's sunny in {location}.\"\n\n\nmodel_with_tools = model.bind_tools([get_weather])  \n\nresponse = model_with_tools.invoke(\"What's the weather like in Boston?\")\nfor tool_call in response.tool_calls:\n    # View tool calls made by the model\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n\nWhen binding user-defined tools, the modelâ€™s response includes a request to execute a tool. When using a model separately from an agent, it is up to you to execute the requested tool and return the result back to the model for use in subsequent reasoning. When using an agent, the agent loop will handle the tool execution loop for you.\nBelow, we show some common ways you can use tool calling.\n\nTool execution loop\n\nForcing tool calls\n\nParallel tool calls\n\nStreaming tool calls\n\nï¿½\nStructured output\nModels can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.\nTo learn about structured output, see Structured output.\nPydantic\nTypedDict\nJSON Schema\nPydantic models provide the richest feature set with field validation, descriptions, and nested structures.\nCopy\nfrom pydantic import BaseModel, Field\n\nclass Movie(BaseModel):\n    \"\"\"A movie with details.\"\"\"\n    title: str = Field(..., description=\"The title of the movie\")\n    year: int = Field(..., description=\"The year the movie was released\")\n    director: str = Field(..., description=\"The director of the movie\")\n    rating: float = Field(..., description=\"The movie's rating out of 10\")\n\nmodel_with_structure = model.with_structured_output(Movie)\nresponse = model_with_structure.invoke(\"Provide details about the movie Inception\")\nprint(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)\n\nKey considerations for structured output\nMethod parameter: Some providers support different methods for structured output:\n'json_schema': Uses dedicated structured output features offered by the provider.\n'function_calling': Derives structured output by forcing a tool call that follows the given schema.\n'json_mode': A precursor to 'json_schema' offered by some providers. Generates valid JSON, but the schema must be described in the prompt.\nInclude raw: Set include_raw=True to get both the parsed output and the raw AI message.\nValidation: Pydantic models provide automatic validation. TypedDict and JSON Schema require manual validation.\nSee your providerâ€™s integration page for supported methods and configuration options.\n\nExample: Message output alongside parsed structure\n\nExample: Nested structures\n\nï¿½\nAdvanced topics\nï¿½\nModel profiles\nModel profiles require langchain>=1.1.\nLangChain chat models can expose a dictionary of supported features and capabilities through a .profile attribute:\nCopy\nmodel.profile\n# {\n#   \"max_input_tokens\": 400000,\n#   \"image_inputs\": True,\n#   \"reasoning_output\": True,\n#   \"tool_calling\": True,\n#   ...\n# }\n\nRefer to the full set of fields in the API reference.\nMuch of the model profile data is powered by the models.dev project, an open source initiative that provides model capability data. These data are augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.\nModel profile data allow applications to work around model capabilities dynamically. For example:\nSummarization middleware can trigger summarization based on a modelâ€™s context window size.\nStructured output strategies in create_agent can be inferred automatically (e.g., by checking support for native structured output features).\nModel inputs can be gated based on supported modalities and maximum input tokens.\n\nUpdating or overwriting profile data\n\nModel profiles are a beta feature. The format of a profile is subject to change.\nï¿½\nMultimodal\nCertain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks.\nAll LangChain chat models with underlying multimodal capabilities support:\nData in the cross-provider standard format (see our messages guide)\nOpenAI chat completions format\nAny format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)\nSee the multimodal section of the messages guide for details.\nSome models can return multimodal data as part of their response. If invoked to do so, the resulting AIMessage will have content blocks with multimodal types.\nMultimodal output\nCopy\nresponse = model.invoke(\"Create a picture of a cat\")\nprint(response.content_blocks)\n# [\n#     {\"type\": \"text\", \"text\": \"Here's a picture of a cat\"},\n#     {\"type\": \"image\", \"base64\": \"...\", \"mime_type\": \"image/jpeg\"},\n# ]\n\nSee the integrations page for details on specific providers.\nï¿½\nReasoning\nMany models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.\nIf supported by the underlying model, you can surface this reasoning process to better understand how the model arrived at its final answer.\nStream reasoning output\nComplete reasoning output\nCopy\nfor chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n    reasoning_steps = [r for r in chunk.content_blocks if r[\"type\"] == \"reasoning\"]\n    print(reasoning_steps if reasoning_steps else chunk.text)\n\nDepending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical â€œtiersâ€ of reasoning (e.g., 'low' or 'high') or integer token budgets.\nFor details, see the integrations page or reference for your respective chat model.\nï¿½\nLocal models\nLangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.\nOllama is one of the easiest ways to run chat and embedding models locally.\nï¿½\nPrompt caching\nMany providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be implicit or explicit:\nImplicit prompt caching: providers will automatically pass on cost savings if a request hits a cache. Examples: OpenAI and Gemini.\nExplicit caching: providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:\nChatOpenAI (via prompt_cache_key)\nAnthropicâ€™s AnthropicPromptCachingMiddleware\nGemini.\nAWS Bedrock\nPrompt caching is often only engaged above a minimum input token threshold. See provider pages for details.\nCache usage will be reflected in the usage metadata of the model response.\nï¿½\nServer-side tool use\nSome providers support server-side tool-calling loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.\nIf a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the content blocks of the response will return the server-side tool calls and results in a provider-agnostic format:\nInvoke with server-side tool use\nCopy\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-4.1-mini\")\n\ntool = {\"type\": \"web_search\"}\nmodel_with_tools = model.bind_tools([tool])\n\nresponse = model_with_tools.invoke(\"What was a positive news story from today?\")\nprint(response.content_blocks)\n\nResult\nCopy\n[\n    {\n        \"type\": \"server_tool_call\",\n        \"name\": \"web_search\",\n        \"args\": {\n            \"query\": \"positive news stories today\",\n            \"type\": \"search\"\n        },\n        \"id\": \"ws_abc123\"\n    },\n    {\n        \"type\": \"server_tool_result\",\n        \"tool_call_id\": \"ws_abc123\",\n        \"status\": \"success\"\n    },\n    {\n        \"type\": \"text\",\n        \"text\": \"Here are some positive news stories from today...\",\n        \"annotations\": [\n            {\n                \"end_index\": 410,\n                \"start_index\": 337,\n                \"title\": \"article title\",\n                \"type\": \"citation\",\n                \"url\": \"...\"\n            }\n        ]\n    }\n]\n\nSee all 29 lines\nThis represents a single conversational turn; there are no associated ToolMessage objects that need to be passed in as in client-side tool-calling.\nSee the integration page for your given provider for available tools and usage details.\nï¿½\nRate limiting\nMany chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\nTo help manage rate limits, chat model integrations accept a rate_limiter parameter that can be provided during initialization to control the rate at which requests are made.\n\nInitialize and use a rate limiter\n\nï¿½\nBase URL or proxy\nFor many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.\n\nBase URL\n\nProxy configuration\n\nï¿½\nLog probabilities\nCertain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the logprobs parameter when initializing the model:\nCopy\nmodel = init_chat_model(\n    model=\"gpt-4.1\",\n    model_provider=\"openai\"\n).bind(logprobs=True)\n\nresponse = model.invoke(\"Why do parrots talk?\")\nprint(response.response_metadata[\"logprobs\"])\n\nï¿½\nToken usage\nA number of model providers return token usage information as part of the invocation response. When available, this information will be included on the AIMessage objects produced by the corresponding model. For more details, see the messages guide.\nSome provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the streaming usage metadata section of the integration guide for details.\nYou can track aggregate token counts across models in an application using either a callback or context manager, as shown below:\nCallback handler\nContext manager\nCopy\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.callbacks import UsageMetadataCallbackHandler\n\nmodel_1 = init_chat_model(model=\"gpt-4.1-mini\")\nmodel_2 = init_chat_model(model=\"claude-haiku-4-5-20251001\")\n\ncallback = UsageMetadataCallbackHandler()\nresult_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\nresult_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\nprint(callback.usage_metadata)\n\nCopy\n{\n    'gpt-4.1-mini-2025-04-14': {\n        'input_tokens': 8,\n        'output_tokens': 10,\n        'total_tokens': 18,\n        'input_token_details': {'audio': 0, 'cache_read': 0},\n        'output_token_details': {'audio': 0, 'reasoning': 0}\n    },\n    'claude-haiku-4-5-20251001': {\n        'input_tokens': 8,\n        'output_tokens': 21,\n        'total_tokens': 29,\n        'input_token_details': {'cache_read': 0, 'cache_creation': 0}\n    }\n}\n\nï¿½\nInvocation config\nWhen invoking a model, you can pass additional configuration through the config parameter using a RunnableConfig dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking.\nCommon configuration options include:\nInvocation with config\nCopy\nresponse = model.invoke(\n    \"Tell me a joke\",\n    config={\n        \"run_name\": \"joke_generation\",      # Custom name for this run\n        \"tags\": [\"humor\", \"demo\"],          # Tags for categorization\n        \"metadata\": {\"user_id\": \"123\"},     # Custom metadata\n        \"callbacks\": [my_callback_handler], # Callback handlers\n    }\n)\n\nThese configuration values are particularly useful when:\nDebugging with LangSmith tracing\nImplementing custom logging or monitoring\nControlling resource usage in production\nTracking invocations across complex pipelines\n\nKey configuration attributes\n\nSee full RunnableConfig reference for all supported attributes.\nï¿½\nConfigurable models\nYou can also create a runtime-configurable model by specifying configurable_fields. If you donâ€™t specify a model value, then 'model' and 'model_provider' will be configurable by default.\nCopy\nfrom langchain.chat_models import init_chat_model\n\nconfigurable_model = init_chat_model(temperature=0)\n\nconfigurable_model.invoke(\n    \"what's your name\",\n    config={\"configurable\": {\"model\": \"gpt-5-nano\"}},  # Run with GPT-5-Nano\n)\nconfigurable_model.invoke(\n    \"what's your name\",\n    config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}},  # Run with Claude\n)\n\n\nConfigurable model with default values\n\nUsing a configurable model declaratively\n\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494352,
    "timestamp": "2026-02-25T08:09:37.347Z",
    "title": "Messages - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/messages",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nBasic usage\nText prompts\nMessage prompts\nDictionary format\nMessage types\nSystem message\nHuman message\nText content\nMessage metadata\nAI message\nTool calls\nToken usage\nStreaming and chunks\nTool message\nMessage content\nStandard content blocks\nMultimodal\nContent block reference\nUse with chat models\nCore components\nMessages\nCopy page\nMessages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\nMessages are objects that contain:\n Role - Identifies the message type (e.g. system, user)\n Content - Represents the actual content of the message (like text, images, audio, documents, etc.)\n Metadata - Optional fields such as response information, message IDs, and token usage\nLangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.\nï¿½\nBasic usage\nThe simplest way to use messages is to create message objects and pass them to a model when invoking.\nCopy\nfrom langchain.chat_models import init_chat_model\nfrom langchain.messages import HumanMessage, AIMessage, SystemMessage\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nsystem_msg = SystemMessage(\"You are a helpful assistant.\")\nhuman_msg = HumanMessage(\"Hello, how are you?\")\n\n# Use with chat models\nmessages = [system_msg, human_msg]\nresponse = model.invoke(messages)  # Returns AIMessage\n\nï¿½\nText prompts\nText prompts are strings - ideal for straightforward generation tasks where you donâ€™t need to retain conversation history.\nCopy\nresponse = model.invoke(\"Write a haiku about spring\")\n\nUse text prompts when:\nYou have a single, standalone request\nYou donâ€™t need conversation history\nYou want minimal code complexity\nï¿½\nMessage prompts\nAlternatively, you can pass in a list of messages to the model by providing a list of message objects.\nCopy\nfrom langchain.messages import SystemMessage, HumanMessage, AIMessage\n\nmessages = [\n    SystemMessage(\"You are a poetry expert\"),\n    HumanMessage(\"Write a haiku about spring\"),\n    AIMessage(\"Cherry blossoms bloom...\")\n]\nresponse = model.invoke(messages)\n\nUse message prompts when:\nManaging multi-turn conversations\nWorking with multimodal content (images, audio, files)\nIncluding system instructions\nï¿½\nDictionary format\nYou can also specify messages directly in OpenAI chat completions format.\nCopy\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a poetry expert\"},\n    {\"role\": \"user\", \"content\": \"Write a haiku about spring\"},\n    {\"role\": \"assistant\", \"content\": \"Cherry blossoms bloom...\"}\n]\nresponse = model.invoke(messages)\n\nï¿½\nMessage types\n System message - Tells the model how to behave and provide context for interactions\n Human message - Represents user input and interactions with the model\n AI message - Responses generated by the model, including text content, tool calls, and metadata\n Tool message - Represents the outputs of tool calls\nï¿½\nSystem message\nA SystemMessage represent an initial set of instructions that primes the modelâ€™s behavior. You can use a system message to set the tone, define the modelâ€™s role, and establish guidelines for responses.\nBasic instructions\nCopy\nsystem_msg = SystemMessage(\"You are a helpful coding assistant.\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n\nDetailed persona\nCopy\nfrom langchain.messages import SystemMessage, HumanMessage\n\nsystem_msg = SystemMessage(\"\"\"\nYou are a senior Python developer with expertise in web frameworks.\nAlways provide code examples and explain your reasoning.\nBe concise but thorough in your explanations.\n\"\"\")\n\nmessages = [\n    system_msg,\n    HumanMessage(\"How do I create a REST API?\")\n]\nresponse = model.invoke(messages)\n\nï¿½\nHuman message\nA HumanMessage represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content.\nï¿½\nText content\nMessage object\nString shortcut\nCopy\nresponse = model.invoke([\n  HumanMessage(\"What is machine learning?\")\n])\n\nï¿½\nMessage metadata\nAdd metadata\nCopy\nhuman_msg = HumanMessage(\n    content=\"Hello!\",\n    name=\"alice\",  # Optional: identify different users\n    id=\"msg_123\",  # Optional: unique identifier for tracing\n)\n\nThe name field behavior varies by provider â€“ some use it for user identification, others ignore it. To check, refer to the model providerâ€™s reference.\nï¿½\nAI message\nAn AIMessage represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\nCopy\nresponse = model.invoke(\"Explain AI\")\nprint(type(response))  # <class 'langchain.messages.AIMessage'>\n\nAIMessage objects are returned by the model when calling it, which contains all of the associated metadata in the response.\nProviders weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new AIMessage object and insert it into the message history as if it came from the model.\nCopy\nfrom langchain.messages import AIMessage, SystemMessage, HumanMessage\n\n# Create an AI message manually (e.g., for conversation history)\nai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n\n# Add to conversation history\nmessages = [\n    SystemMessage(\"You are a helpful assistant\"),\n    HumanMessage(\"Can you help me?\"),\n    ai_msg,  # Insert as if it came from the model\n    HumanMessage(\"Great! What's 2+2?\")\n]\n\nresponse = model.invoke(messages)\n\n\nAttributes\n\nï¿½\nTool calls\nWhen models make tool calls, theyâ€™re included in the AIMessage:\nCopy\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\ndef get_weather(location: str) -> str:\n    \"\"\"Get the weather at a location.\"\"\"\n    ...\n\nmodel_with_tools = model.bind_tools([get_weather])\nresponse = model_with_tools.invoke(\"What's the weather in Paris?\")\n\nfor tool_call in response.tool_calls:\n    print(f\"Tool: {tool_call['name']}\")\n    print(f\"Args: {tool_call['args']}\")\n    print(f\"ID: {tool_call['id']}\")\n\nOther structured data, such as reasoning or citations, can also appear in message content.\nï¿½\nToken usage\nAn AIMessage can hold token counts and other usage metadata in its usage_metadata field:\nCopy\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\n\nresponse = model.invoke(\"Hello!\")\nresponse.usage_metadata\n\nCopy\n{'input_tokens': 8,\n 'output_tokens': 304,\n 'total_tokens': 312,\n 'input_token_details': {'audio': 0, 'cache_read': 0},\n 'output_token_details': {'audio': 0, 'reasoning': 256}}\n\nSee UsageMetadata for details.\nï¿½\nStreaming and chunks\nDuring streaming, youâ€™ll receive AIMessageChunk objects that can be combined into a full message object:\nCopy\nchunks = []\nfull_message = None\nfor chunk in model.stream(\"Hi\"):\n    chunks.append(chunk)\n    print(chunk.text)\n    full_message = chunk if full_message is None else full_message + chunk\n\nLearn more:\nStreaming tokens from chat models\nStreaming tokens and/or steps from agents\nï¿½\nTool message\nFor models that support tool calling, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.\nTools can generate ToolMessage objects directly. Below, we show a simple example. Read more in the tools guide.\nCopy\nfrom langchain.messages import AIMessage\nfrom langchain.messages import ToolMessage\n\n# After a model makes a tool call\n# (Here, we demonstrate manually creating the messages for brevity)\nai_message = AIMessage(\n    content=[],\n    tool_calls=[{\n        \"name\": \"get_weather\",\n        \"args\": {\"location\": \"San Francisco\"},\n        \"id\": \"call_123\"\n    }]\n)\n\n# Execute tool and create result message\nweather_result = \"Sunny, 72Â°F\"\ntool_message = ToolMessage(\n    content=weather_result,\n    tool_call_id=\"call_123\"  # Must match the call ID\n)\n\n# Continue conversation\nmessages = [\n    HumanMessage(\"What's the weather in San Francisco?\"),\n    ai_message,  # Model's tool call\n    tool_message,  # Tool execution result\n]\nresponse = model.invoke(messages)  # Model processes the result\n\n\nAttributes\n\nThe artifact field stores supplementary data that wonâ€™t be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the modelâ€™s context.\n\nExample: Using artifact for retrieval metadata\n\nï¿½\nMessage content\nYou can think of a messageâ€™s content as the payload of data that gets sent to the model. Messages have a content attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as multimodal content and other data.\nSeparately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See content blocks below.\nLangChain chat models accept message content in the content attribute.\nThis may contain either:\nA string\nA list of content blocks in a provider-native format\nA list of LangChainâ€™s standard content blocks\nSee below for an example using multimodal inputs:\nCopy\nfrom langchain.messages import HumanMessage\n\n# String content\nhuman_message = HumanMessage(\"Hello, how are you?\")\n\n# Provider-native format (e.g., OpenAI)\nhuman_message = HumanMessage(content=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n])\n\n# List of standard content blocks\nhuman_message = HumanMessage(content_blocks=[\n    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n])\n\nSpecifying content_blocks when initializing a message will still populate message content, but provides a type-safe interface for doing so.\nï¿½\nStandard content blocks\nLangChain provides a standard representation for message content that works across providers.\nMessage objects implement a content_blocks property that will lazily parse the content attribute into a standard, type-safe representation. For example, messages generated from ChatAnthropic or ChatOpenAI will include thinking or reasoning blocks in the format of the respective provider, but can be lazily parsed into a consistent ReasoningContentBlock representation:\nAnthropic\nOpenAI\nCopy\nfrom langchain.messages import AIMessage\n\nmessage = AIMessage(\n    content=[\n        {\"type\": \"thinking\", \"thinking\": \"...\", \"signature\": \"WaUjzkyp...\"},\n        {\"type\": \"text\", \"text\": \"...\"},\n    ],\n    response_metadata={\"model_provider\": \"anthropic\"}\n)\nmessage.content_blocks\n\nCopy\n[{'type': 'reasoning',\n  'reasoning': '...',\n  'extras': {'signature': 'WaUjzkyp...'}},\n {'type': 'text', 'text': '...'}]\n\nSee the integrations guides to get started with the inference provider of your choice.\nSerializing standard content\nIf an application outside of LangChain needs access to the standard content block representation, you can opt-in to storing content blocks in message content.\nTo do this, you can set the LC_OUTPUT_VERSION environment variable to v1. Or, initialize any chat model with output_version=\"v1\":\nCopy\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\", output_version=\"v1\")\n\nï¿½\nMultimodal\nMultimodality refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. LangChain includes standard types for these data that can be used across providers.\nChat models can accept multimodal data as input and generate it as output. Below we show short examples of input messages featuring multimodal data.\nExtra keys can be included top-level in the content block or nested in \"extras\": {\"key\": value}.\nOpenAI and AWS Bedrock Converse, for example, require a filename for PDFs. See the provider page for your chosen model for specifics.\nImage input\nPDF document input\nAudio input\nVideo input\nCopy\n# From URL\nmessage = {\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n        {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n    ]\n}\n\n# From base64 data\nmessage = {\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n        {\n            \"type\": \"image\",\n            \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n            \"mime_type\": \"image/jpeg\",\n        },\n    ]\n}\n\n# From provider-managed File ID\nmessage = {\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n        {\"type\": \"image\", \"file_id\": \"file-abc123\"},\n    ]\n}\n\nNot all models support all file types. Check the model providerâ€™s reference for supported formats and size limits.\nï¿½\nContent block reference\nContent blocks are represented (either when creating a message or accessing the content_blocks property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:\n\nCore\n\nMultimodal\n\nTool Calling\n\nServer-Side Tool Execution\n\nProvider-Specific Blocks\n\nView the canonical type definitions in the API reference.\nContent blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.\nContent blocks are not a replacement for the content property, but rather a new property that can be used to access the content of a message in a standardized format.\nï¿½\nUse with chat models\nChat models accept a sequence of message objects as input and return an AIMessage as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.\nRefer to the below guides to learn more:\nBuilt-in features for persisting and managing conversation histories\nStrategies for managing context windows, including trimming and summarizing messages\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494354,
    "timestamp": "2026-02-25T08:09:37.355Z",
    "title": "Short-term memory - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/short-term-memory",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nUsage\nIn production\nCustomizing agent memory\nCommon patterns\nTrim messages\nDelete messages\nSummarize messages\nAccess memory\nTools\nRead short-term memory in a tool\nWrite short-term memory from tools\nPrompt\nBefore model\nAfter model\nCore components\nShort-term memory\nCopy page\nï¿½\nOverview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\nShort term memory lets your application remember previous interactions within a single thread or conversation.\nA thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\nConversation history is the most common form of short-term memory. Long conversations pose a challenge to todayâ€™s LLMs; a full history may not fit inside an LLMâ€™s context window, resulting in an context loss or errors.\nEven if your model supports the full context length, most LLMs still perform poorly over long contexts. They get â€œdistractedâ€ by stale or off-topic content, all while suffering from slower response times and higher costs.\nChat models accept context using messages, which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or â€œforgetâ€ stale information.\nï¿½\nUsage\nTo add short-term memory (thread-level persistence) to an agent, you need to specify a checkpointer when creating an agent.\nLangChainâ€™s agent manages short-term memory as a part of your agentâ€™s state.\nBy storing these in the graphâ€™s state, the agent can access the full context for a given conversation while maintaining separation between different threads.\nState is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.\nShort-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\nCopy\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver  \n\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=[get_user_info],\n    checkpointer=InMemorySaver(),  \n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n    {\"configurable\": {\"thread_id\": \"1\"}},  \n)\n\nï¿½\nIn production\nIn production, use a checkpointer backed by a database:\nCopy\npip install langgraph-checkpoint-postgres\n\nCopy\nfrom langchain.agents import create_agent\n\nfrom langgraph.checkpoint.postgres import PostgresSaver  \n\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    checkpointer.setup() # auto create tables in PostgresSql\n    agent = create_agent(\n        \"gpt-5\",\n        tools=[get_user_info],\n        checkpointer=checkpointer,  \n    )\n\nFor more checkpointer options including SQLite, Postgres, and Azure Cosmos DB, see the list of checkpointer libraries in the Persistence documentation.\nï¿½\nCustomizing agent memory\nBy default, agents use AgentState to manage short term memory, specifically the conversation history via a messages key.\nYou can extend AgentState to add additional fields. Custom state schemas are passed to create_agent using the state_schema parameter.\nCopy\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass CustomAgentState(AgentState):  \n    user_id: str\n    preferences: dict\n\nagent = create_agent(\n    \"gpt-5\",\n    tools=[get_user_info],\n    state_schema=CustomAgentState,  \n    checkpointer=InMemorySaver(),\n)\n\n# Custom state can be passed in invoke\nresult = agent.invoke(\n    {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"user_id\": \"user_123\",  \n        \"preferences\": {\"theme\": \"dark\"}  \n    },\n    {\"configurable\": {\"thread_id\": \"1\"}})\n\nï¿½\nCommon patterns\nWith short-term memory enabled, long conversations can exceed the LLMâ€™s context window. Common solutions are:\nTrim messages\nRemove first or last N messages (before calling LLM)\nDelete messages\nDelete messages from LangGraph state permanently\nSummarize messages\nSummarize earlier messages in the history and replace them with a summary\nCustom strategies\nCustom strategies (e.g., message filtering, etc.)\nThis allows the agent to keep track of the conversation without exceeding the LLMâ€™s context window.\nï¿½\nTrim messages\nMost LLMs have a maximum supported context window (denominated in tokens).\nOne way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If youâ€™re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens) to use for handling the boundary.\nTo trim message history in an agent, use the @before_model middleware decorator:\nCopy\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\nfrom typing import Any\n\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\nagent = create_agent(\n    your_model_here,\n    tools=your_tools_here,\n    middleware=[trim_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n\nï¿½\nDelete messages\nYou can delete messages from the graph state to manage the message history.\nThis is useful when you want to remove specific messages or clear the entire message history.\nTo delete messages from the graph state, you can use the RemoveMessage.\nFor RemoveMessage to work, you need to use a state key with add_messages reducer.\nThe default AgentState provides this.\nTo remove specific messages:\nCopy\nfrom langchain.messages import RemoveMessage  \n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n\nTo remove all messages:\nCopy\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\n\ndef delete_messages(state):\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  \n\nWhen deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider youâ€™re using. For example:\nSome providers expect message history to start with a user message\nMost providers require assistant messages with tool calls to be followed by corresponding tool result messages.\nCopy\nfrom langchain.messages import RemoveMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import after_model\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.runtime import Runtime\nfrom langchain_core.runnables import RunnableConfig\n\n\n@after_model\ndef delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:\n    \"\"\"Remove old messages to keep conversation manageable.\"\"\"\n    messages = state[\"messages\"]\n    if len(messages) > 2:\n        # remove the earliest two messages\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n    return None\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[],\n    system_prompt=\"Please be concise and to the point.\",\n    middleware=[delete_old_messages],\n    checkpointer=InMemorySaver(),\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n    config,\n    stream_mode=\"values\",\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n    config,\n    stream_mode=\"values\",\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n\nCopy\n[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n[('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]\n\nï¿½\nSummarize messages\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\nTo summarize message history in an agent, use the built-in SummarizationMiddleware:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_core.runnables import RunnableConfig\n\n\ncheckpointer = InMemorySaver()\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 20)\n        )\n    ],\n    checkpointer=checkpointer,\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob!\n\"\"\"\n\nSee SummarizationMiddleware for more configuration options.\nï¿½\nAccess memory\nYou can access and modify the short-term memory (state) of an agent in several ways:\nï¿½\nTools\nï¿½\nRead short-term memory in a tool\nAccess short term memory (state) in a tool using the runtime parameter (typed as ToolRuntime).\nThe runtime parameter is hidden from the tool signature (so the model doesnâ€™t see it), but the tool can access the state through it.\nCopy\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.tools import tool, ToolRuntime\n\n\nclass CustomState(AgentState):\n    user_id: str\n\n@tool\ndef get_user_info(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Look up user info.\"\"\"\n    user_id = runtime.state[\"user_id\"]\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_user_info],\n    state_schema=CustomState,\n)\n\nresult = agent.invoke({\n    \"messages\": \"look up user information\",\n    \"user_id\": \"user_123\"\n})\nprint(result[\"messages\"][-1].content)\n# > User is John Smith.\n\nï¿½\nWrite short-term memory from tools\nTo modify the agentâ€™s short-term memory (state) during execution, you can return state updates directly from the tools.\nThis is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\nCopy\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.messages import ToolMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langgraph.types import Command\nfrom pydantic import BaseModel\n\n\nclass CustomState(AgentState):  \n    user_name: str\n\nclass CustomContext(BaseModel):\n    user_id: str\n\n@tool\ndef update_user_info(\n    runtime: ToolRuntime[CustomContext, CustomState],\n) -> Command:\n    \"\"\"Look up and update user info.\"\"\"\n    user_id = runtime.context.user_id\n    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n    return Command(update={  \n        \"user_name\": name,\n        # update the message history\n        \"messages\": [\n            ToolMessage(\n                \"Successfully looked up user information\",\n                tool_call_id=runtime.tool_call_id\n            )\n        ]\n    })\n\n@tool\ndef greet(\n    runtime: ToolRuntime[CustomContext, CustomState]\n) -> str | Command:\n    \"\"\"Use this to greet the user once you found their info.\"\"\"\n    user_name = runtime.state.get(\"user_name\", None)\n    if user_name is None:\n       return Command(update={\n            \"messages\": [\n                ToolMessage(\n                    \"Please call the 'update_user_info' tool it will get and update the user's name.\",\n                    tool_call_id=runtime.tool_call_id\n                )\n            ]\n        })\n    return f\"Hello {user_name}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[update_user_info, greet],\n    state_schema=CustomState, \n    context_schema=CustomContext,\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\n    context=CustomContext(user_id=\"user_123\"),\n)\n\nï¿½\nPrompt\nAccess short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.\nCopy\nfrom langchain.agents import create_agent\nfrom typing import TypedDict\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass CustomContext(TypedDict):\n    user_name: str\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a city.\"\"\"\n    return f\"The weather in {city} is always sunny!\"\n\n\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context[\"user_name\"]\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n    middleware=[dynamic_system_prompt],\n    context_schema=CustomContext,\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    context=CustomContext(user_name=\"John Smith\"),\n)\nfor msg in result[\"messages\"]:\n    msg.pretty_print()\n\n\nOutput\nCopy\n================================ Human Message =================================\n\nWhat is the weather in SF?\n================================== Ai Message ==================================\nTool Calls:\n  get_weather (call_WFQlOGn4b2yoJrv7cih342FG)\n Call ID: call_WFQlOGn4b2yoJrv7cih342FG\n  Args:\n    city: San Francisco\n================================= Tool Message =================================\nName: get_weather\n\nThe weather in San Francisco is always sunny!\n================================== Ai Message ==================================\n\nHi John Smith, the weather in San Francisco is always sunny!\n\nï¿½\nBefore model\nAccess short term memory (state) in @before_model middleware to process messages before model calls.\n\n__start__\n\nbefore_model\n\nmodel\n\ntools\n\n__end__\n\nCopy\nfrom langchain.messages import RemoveMessage\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import before_model\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\n\n@before_model\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n    messages = state[\"messages\"]\n\n    if len(messages) <= 3:\n        return None  # No changes needed\n\n    first_msg = messages[0]\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n    new_messages = [first_msg] + recent_messages\n\n    return {\n        \"messages\": [\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n            *new_messages\n        ]\n    }\n\n\nagent = create_agent(\n    \"gpt-5-nano\",\n    tools=[],\n    middleware=[trim_messages],\n    checkpointer=InMemorySaver()\n)\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n\"\"\"\n================================== Ai Message ==================================\n\nYour name is Bob. You told me that earlier.\nIf you'd like me to call you a nickname or use a different name, just say the word.\n\"\"\"\n\nï¿½\nAfter model\nAccess short term memory (state) in @after_model middleware to process messages after model calls.\n\n__start__\n\nmodel\n\nafter_model\n\ntools\n\n__end__\n\nCopy\nfrom langchain.messages import RemoveMessage\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import after_model\nfrom langgraph.runtime import Runtime\n\n\n@after_model\ndef validate_response(state: AgentState, runtime: Runtime) -> dict | None:\n    \"\"\"Remove messages containing sensitive words.\"\"\"\n    STOP_WORDS = [\"password\", \"secret\"]\n    last_message = state[\"messages\"][-1]\n    if any(word in last_message.content for word in STOP_WORDS):\n        return {\"messages\": [RemoveMessage(id=last_message.id)]}\n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[],\n    middleware=[validate_response],\n    checkpointer=InMemorySaver(),\n)\n\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494355,
    "timestamp": "2026-02-25T08:09:37.362Z",
    "title": "Overview - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/streaming/overview",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nOverview\nFrontend\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nSupported stream modes\nAgent progress\nLLM tokens\nCustom updates\nStream multiple modes\nCommon patterns\nStreaming tool calls\nAccessing completed messages\nStreaming with human-in-the-loop\nStreaming from sub-agents\nDisable streaming\nRelated\nCore components\nStreaming\nOverview\nCopy page\n\nStream real-time updates from agent runs\n\nLangChain implements a streaming system to surface real-time updates.\nStreaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\nï¿½\nOverview\nLangChainâ€™s streaming system lets you surface live feedback from agent runs to your application.\nWhatâ€™s possible with LangChain streaming:\n Stream agent progress â€” get state updates after each agent step.\n Stream LLM tokens â€” stream language model tokens as theyâ€™re generated.\n Stream custom updates â€” emit user-defined signals (e.g., \"Fetched 10/100 records\").\n Stream multiple modes â€” choose from updates (agent progress), messages (LLM tokens + metadata), or custom (arbitrary user data).\nSee the common patterns section below for additional end-to-end examples.\nï¿½\nSupported stream modes\nPass one or more of the following stream modes as a list to the stream or astream methods:\nMode\tDescription\nupdates\tStreams state updates after each agent step. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.\nmessages\tStreams tuples of (token, metadata) from any graph nodes where an LLM is invoked.\ncustom\tStreams custom data from inside your graph nodes using the stream writer.\nï¿½\nAgent progress\nTo stream agent progress, use the stream or astream methods with stream_mode=\"updates\". This emits an event after every agent step.\nFor example, if you have an agent that calls a tool once, you should see the following updates:\nLLM node: AIMessage with tool call requests\nTool node: ToolMessage with execution result\nLLM node: Final AI response\nStreaming agent progress\nCopy\nfrom langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor chunk in agent.stream(  \n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"updates\",\n):\n    for step, data in chunk.items():\n        print(f\"step: {step}\")\n        print(f\"content: {data['messages'][-1].content_blocks}\")\n\nOutput\nCopy\nstep: model\ncontent: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]\n\nstep: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\nstep: model\ncontent: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}]\n\nï¿½\nLLM tokens\nTo stream tokens as they are produced by the LLM, use stream_mode=\"messages\". Below you can see the output of the agent streaming tool calls and the final response.\nStreaming LLM tokens\nCopy\nfrom langchain.agents import create_agent\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\nfor token, metadata in agent.stream(  \n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"messages\",\n):\n    print(f\"node: {metadata['langgraph_node']}\")\n    print(f\"content: {token.content_blocks}\")\n    print(\"\\n\")\n\nOutput\nCopy\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\":\"', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}]\n\n\nnode: model\ncontent: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '\"}', 'index': 0}]\n\n\nnode: model\ncontent: []\n\n\nnode: tools\ncontent: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n\n\nnode: model\ncontent: []\n\n\nnode: model\ncontent: [{'type': 'text', 'text': 'Here'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ''s'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' what'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' I'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' got'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ':'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' \"'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': \"It's\"}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' always'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' sunny'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' in'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' San'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': ' Francisco'}]\n\n\nnode: model\ncontent: [{'type': 'text', 'text': '!\"\\n\\n'}]\n\nSee all 94 lines\nï¿½\nCustom updates\nTo stream updates from tools as they are executed, you can use get_stream_writer.\nStreaming custom updates\nCopy\nfrom langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer  \n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()  \n    # stream any arbitrary data\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n)\n\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=\"custom\"\n):\n    print(chunk)\n\nOutput\nCopy\nLooking up data for city: San Francisco\nAcquired data for city: San Francisco\n\nIf you add get_stream_writer inside your tool, you wonâ€™t be able to invoke the tool outside of a LangGraph execution context.\nï¿½\nStream multiple modes\nYou can specify multiple streaming modes by passing stream mode as a list: stream_mode=[\"updates\", \"custom\"].\nThe streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode.\nStreaming multiple modes\nCopy\nfrom langchain.agents import create_agent\nfrom langgraph.config import get_stream_writer\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = get_stream_writer()\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[get_weather],\n)\n\nfor stream_mode, chunk in agent.stream(  \n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n    stream_mode=[\"updates\", \"custom\"]\n):\n    print(f\"stream_mode: {stream_mode}\")\n    print(f\"content: {chunk}\")\n    print(\"\\n\")\n\nOutput\nCopy\nstream_mode: updates\ncontent: {'model': {'messages': [AIMessage(content='', response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 132, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_KTNQIftMrl9vgNwEfAJMVu7r', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 280, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}}\n\n\nstream_mode: custom\ncontent: Looking up data for city: San Francisco\n\n\nstream_mode: custom\ncontent: Acquired data for city: San Francisco\n\n\nstream_mode: updates\ncontent: {'tools': {'messages': [ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', tool_call_id='call_KTNQIftMrl9vgNwEfAJMVu7r')]}}\n\n\nstream_mode: updates\ncontent: {'model': {'messages': [AIMessage(content='San Francisco weather: It's always sunny in San Francisco!\\n\\n', response_metadata={'token_usage': {'completion_tokens': 764, 'prompt_tokens': 168, 'total_tokens': 932, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage_metadata={'input_tokens': 168, 'output_tokens': 764, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}}\n\nï¿½\nCommon patterns\nBelow are examples showing common use cases for streaming.\nï¿½\nStreaming tool calls\nYou may want to stream both:\nPartial JSON as tool calls are generated\nThe completed, parsed tool calls that are executed\nSpecifying stream_mode=\"messages\" will stream incremental message chunks generated by all LLM calls in the agent. To access the completed messages with parsed tool calls:\nIf those messages are tracked in the state (as in the model node of create_agent), use stream_mode=[\"messages\", \"updates\"] to access completed messages through state updates (demonstrated below).\nIf those messages are not tracked in the state, use custom updates or aggregate the chunks during the streaming loop (next section).\nRefer to the section below on streaming from sub-agents if your agent includes multiple LLMs.\nCopy\nfrom typing import Any\n\nfrom langchain.agents import create_agent\nfrom langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\n\nagent = create_agent(\"openai:gpt-5.2\", tools=[get_weather])\n\n\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n    # N.B. all content is available through token.content_blocks\n\n\ndef _render_completed_message(message: AnyMessage) -> None:\n    if isinstance(message, AIMessage) and message.tool_calls:\n        print(f\"Tool calls: {message.tool_calls}\")\n    if isinstance(message, ToolMessage):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\"],  \n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)  \n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):  # `source` captures node name\n                _render_completed_message(update[\"messages\"][-1])  \n\nOutput\nCopy\n[{'name': 'get_weather', 'args': '', 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'type': 'tool_call'}]\nTool response: [{'type': 'text', 'text': \"It's always sunny in Boston!\"}]\nThe| weather| in| Boston| is| **|sun|ny|**|.|\n\nSee all 9 lines\nï¿½\nAccessing completed messages\nIf completed messages are tracked in an agentâ€™s state, you can use stream_mode=[\"messages\", \"updates\"] as demonstrated above to access completed messages during streaming.\nIn some cases, completed messages are not reflected in state updates. If you have access to the agent internals, you can use custom updates to access these messages during streaming. Otherwise, you can aggregate message chunks in the streaming loop (see below).\nConsider the below example, where we incorporate a stream writer into a simplified guardrail middleware. This middleware demonstrates tool calling to generate a structured â€œsafe / unsafeâ€ evaluation (one could also use structured outputs for this):\nCopy\nfrom typing import Any, Literal\n\nfrom langchain.agents.middleware import after_agent, AgentState\nfrom langgraph.runtime import Runtime\nfrom langchain.messages import AIMessage\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.config import get_stream_writer  \nfrom pydantic import BaseModel\n\n\nclass ResponseSafety(BaseModel):\n    \"\"\"Evaluate a response as safe or unsafe.\"\"\"\n    evaluation: Literal[\"safe\", \"unsafe\"]\n\n\nsafety_model = init_chat_model(\"openai:gpt-5.2\")\n\n@after_agent(can_jump_to=[\"end\"])\ndef safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\n    stream_writer = get_stream_writer()  \n    # Get the model response\n    if not state[\"messages\"]:\n        return None\n\n    last_message = state[\"messages\"][-1]\n    if not isinstance(last_message, AIMessage):\n        return None\n\n    # Use another model to evaluate safety\n    model_with_tools = safety_model.bind_tools([ResponseSafety], tool_choice=\"any\")\n    result = model_with_tools.invoke(\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"Evaluate this AI response as generally safe or unsafe.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"AI response: {last_message.text}\"\n            }\n        ]\n    )\n    stream_writer(result)  \n\n    tool_call = result.tool_calls[0]\n    if tool_call[\"args\"][\"evaluation\"] == \"unsafe\":\n        last_message.content = \"I cannot provide that response. Please rephrase your request.\"\n\n    return None\n\nWe can then incorporate this middleware into our agent and include its custom stream events:\nCopy\nfrom typing import Any\n\nfrom langchain.agents import create_agent\nfrom langchain.messages import AIMessageChunk, AIMessage, AnyMessage\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\n\nagent = create_agent(\n    model=\"openai:gpt-5.2\",\n    tools=[get_weather],\n    middleware=[safety_guardrail],  \n)\n\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n\n\ndef _render_completed_message(message: AnyMessage) -> None:\n    if isinstance(message, AIMessage) and message.tool_calls:\n        print(f\"Tool calls: {message.tool_calls}\")\n    if isinstance(message, ToolMessage):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\", \"custom\"],  \n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n    if stream_mode == \"custom\":  \n        # access completed message in stream\n        print(f\"Tool calls: {data.tool_calls}\")  \n\nOutput\nCopy\n[{'name': 'get_weather', 'args': '', 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'type': 'tool_call'}]\nTool response: [{'type': 'text', 'text': \"It's always sunny in Boston!\"}]\nThe| weather| in| **|Boston|**| is| **|sun|ny|**|.|[{'name': 'ResponseSafety', 'args': '', 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'evaluation', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'safe', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'ResponseSafety', 'args': {'evaluation': 'safe'}, 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'type': 'tool_call'}]\n\nSee all 15 lines\nAlternatively, if you arenâ€™t able to add custom events to the stream, you can aggregate message chunks within the streaming loop:\nCopy\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\nfull_message = None\nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\"],\n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)\n            full_message = token if full_message is None else full_message + token  \n            if token.chunk_position == \"last\":  \n                if full_message.tool_calls:  \n                    print(f\"Tool calls: {full_message.tool_calls}\")  \n                full_message = None\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source == \"tools\":\n                _render_completed_message(update[\"messages\"][-1])\n\nï¿½\nStreaming with human-in-the-loop\nTo handle human-in-the-loop interrupts, we build on the above example:\nWe configure the agent with human-in-the-loop middleware and a checkpointer\nWe collect interrupts generated during the \"updates\" stream mode\nWe respond to those interrupts with a command\nCopy\nfrom typing import Any\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import Command, Interrupt\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\n\ncheckpointer = InMemorySaver()\n\nagent = create_agent(\n    \"openai:gpt-5.2\",\n    tools=[get_weather],\n    middleware=[  \n        HumanInTheLoopMiddleware(interrupt_on={\"get_weather\": True}),  \n    ],  \n    checkpointer=checkpointer,  \n)\n\n\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n\n\ndef _render_completed_message(message: AnyMessage) -> None:\n    if isinstance(message, AIMessage) and message.tool_calls:\n        print(f\"Tool calls: {message.tool_calls}\")\n    if isinstance(message, ToolMessage):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ndef _render_interrupt(interrupt: Interrupt) -> None:  \n    interrupts = interrupt.value  \n    for request in interrupts[\"action_requests\"]:  \n        print(request[\"description\"])  \n\n\ninput_message = {\n    \"role\": \"user\",\n    \"content\": (\n        \"Can you look up the weather in Boston and San Francisco?\"\n    ),\n}\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}  \ninterrupts = []  \nfor stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    config=config,  \n    stream_mode=[\"messages\", \"updates\"],\n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n            if source == \"__interrupt__\":  \n                interrupts.extend(update)  \n                _render_interrupt(update[0])  \n\nOutput\nCopy\n[{'name': 'get_weather', 'args': '', 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"ci', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'ty\": ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"Bosto', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'n\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': 'get_weather', 'args': '', 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"ci', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'ty\": ', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"San F', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'ranc', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'isco\"', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '}', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'type': 'tool_call'}, {'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'type': 'tool_call'}]\nTool execution requires approval\n\nTool: get_weather\nArgs: {'city': 'Boston'}\nTool execution requires approval\n\nTool: get_weather\nArgs: {'city': 'San Francisco'}\n\nSee all 21 lines\nWe next collect a decision for each interrupt. Importantly, the order of decisions must match the order of actions we collected.\nTo illustrate, we will edit one tool call and accept the other:\nCopy\ndef _get_interrupt_decisions(interrupt: Interrupt) -> list[dict]:\n    return [\n        {\n            \"type\": \"edit\",\n            \"edited_action\": {\n                \"name\": \"get_weather\",\n                \"args\": {\"city\": \"Boston, U.K.\"},\n            },\n        }\n        if \"boston\" in request[\"description\"].lower()\n        else {\"type\": \"approve\"}\n        for request in interrupt.value[\"action_requests\"]\n    ]\n\ndecisions = {}\nfor interrupt in interrupts:\n    decisions[interrupt.id] = {\n        \"decisions\": _get_interrupt_decisions(interrupt)\n    }\n\ndecisions\n\nOutput\nCopy\n{\n    'a96c40474e429d661b5b32a8d86f0f3e': {\n        'decisions': [\n            {\n                'type': 'edit',\n                 'edited_action': {\n                     'name': 'get_weather',\n                     'args': {'city': 'Boston, U.K.'}\n                 }\n            },\n            {'type': 'approve'},\n        ]\n    }\n}\n\nWe can then resume by passing a command into the same streaming loop:\nCopy\ninterrupts = []\nfor stream_mode, data in agent.stream(\n    Command(resume=decisions),  \n    config=config,\n    stream_mode=[\"messages\", \"updates\"],\n):\n    # Streaming loop is unchanged\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if isinstance(token, AIMessageChunk):\n            _render_message_chunk(token)\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n            if source == \"__interrupt__\":\n                interrupts.extend(update)\n                _render_interrupt(update[0])\n\nOutput\nCopy\nTool response: [{'type': 'text', 'text': \"It's always sunny in Boston, U.K.!\"}]\nTool response: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}]\n-| **|Boston|**|:| It|'s| always| sunny| in| Boston|,| U|.K|.|\n|-| **|San| Francisco|**|:| It|'s| always| sunny| in| San| Francisco|!|\n\nï¿½\nStreaming from sub-agents\nWhen there are multiple LLMs at any point in an agent, itâ€™s often necessary to disambiguate the source of messages as they are generated.\nTo do this, pass a name to each agent when creating it. This name is then available in metadata via the lc_agent_name key when streaming in \"messages\" mode.\nBelow, we update the streaming tool calls example:\nWe replace our tool with a call_weather_agent tool that invokes an agent internally\nWe add a name to each agent\nWe specify subgraphs=True when creating the stream\nOur stream processing is identical to before, but we add logic to keep track of what agent is active using create_agentâ€™s name parameter\nWhen you set a name on an agent, that name is also attached to any AIMessages generated by that agent.\nFirst we construct the agent:\nCopy\nfrom typing import Any\n\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\nfrom langchain.messages import AIMessage, AnyMessage\n\n\ndef get_weather(city: str) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n\n    return f\"It's always sunny in {city}!\"\n\n\nweather_model = init_chat_model(\"openai:gpt-5.2\")\nweather_agent = create_agent(\n    model=weather_model,\n    tools=[get_weather],\n    name=\"weather_agent\",  \n)\n\n\ndef call_weather_agent(query: str) -> str:\n    \"\"\"Query the weather agent.\"\"\"\n    result = weather_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return result[\"messages\"][-1].text\n\n\nsupervisor_model = init_chat_model(\"openai:gpt-5.2\")\nagent = create_agent(\n    model=supervisor_model,\n    tools=[call_weather_agent],\n    name=\"supervisor\",  \n)\n\nNext, we add logic to the streaming loop to report which agent is emitting tokens:\nCopy\ndef _render_message_chunk(token: AIMessageChunk) -> None:\n    if token.text:\n        print(token.text, end=\"|\")\n    if token.tool_call_chunks:\n        print(token.tool_call_chunks)\n\n\ndef _render_completed_message(message: AnyMessage) -> None:\n    if isinstance(message, AIMessage) and message.tool_calls:\n        print(f\"Tool calls: {message.tool_calls}\")\n    if isinstance(message, ToolMessage):\n        print(f\"Tool response: {message.content_blocks}\")\n\n\ninput_message = {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}\ncurrent_agent = None\nfor _, stream_mode, data in agent.stream(\n    {\"messages\": [input_message]},\n    stream_mode=[\"messages\", \"updates\"],\n    subgraphs=True,  \n):\n    if stream_mode == \"messages\":\n        token, metadata = data\n        if agent_name := metadata.get(\"lc_agent_name\"):  \n            if agent_name != current_agent:  \n                print(f\"ðŸ¤– {agent_name}: \")  \n                current_agent = agent_name  \n        if isinstance(token, AIMessage):\n            _render_message_chunk(token)\n    if stream_mode == \"updates\":\n        for source, update in data.items():\n            if source in (\"model\", \"tools\"):\n                _render_completed_message(update[\"messages\"][-1])\n\nOutput\nCopy\nðŸ¤– supervisor:\n[{'name': 'call_weather_agent', 'args': '', 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'query', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' weather', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' right', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' now', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': \" today's\", 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': ' forecast', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'call_weather_agent', 'args': {'query': \"Boston weather right now and today's forecast\"}, 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'type': 'tool_call'}]\nðŸ¤– weather_agent:\n[{'name': 'get_weather', 'args': '', 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n[{'name': None, 'args': '\"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\nTool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'type': 'tool_call'}]\nTool response: [{'type': 'text', 'text': \"It's always sunny in Boston!\"}]\nBoston| weather| right| now|:| **|Sunny|**|.\n\n|Today|'s| forecast| for| Boston|:| **|Sunny| all| day|**|.|Tool response: [{'type': 'text', 'text': 'Boston weather right now: **Sunny**.\\n\\nToday's forecast for Boston: **Sunny all day**.'}]\nðŸ¤– supervisor:\nBoston| weather| right| now|:| **|Sunny|**|.\n\n|Today|'s| forecast| for| Boston|:| **|Sunny| all| day|**|.|\n\nSee all 30 lines\nï¿½\nDisable streaming\nIn some applications you might need to disable streaming of individual tokens for a given model. This is useful when:\nWorking with multi-agent systems to control which agents stream their output\nMixing models that support streaming with those that do not\nDeploying to LangSmith and wanting to prevent certain model outputs from being streamed to the client\nSet streaming=False when initializing the model.\nCopy\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-4.1\",\n    streaming=False\n)\n\nWhen deploying to LangSmith, set streaming=False on any models whose output you donâ€™t want streamed to the client. This is configured in your graph code before deployment.\nNot all chat model integrations support the streaming parameter. If your model doesnâ€™t support it, use disable_streaming=True instead. This parameter is available on all chat models via the base class.\nSee the LangGraph streaming guide for more details.\nï¿½\nRelated\nFrontend streaming â€” Build React UIs with useStream for real-time agent interactions\nStreaming with chat models â€” Stream tokens directly from a chat model without using an agent or graph\nStreaming with human-in-the-loop â€” Stream agent progress while handling interrupts for human review\nLangGraph streaming â€” Advanced streaming options including values, debug modes, and subgraph streaming\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494353,
    "timestamp": "2026-02-25T08:09:37.364Z",
    "title": "Tools - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/tools",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nCreate tools\nBasic tool definition\nCustomize tool properties\nCustom tool name\nCustom tool description\nAdvanced schema definition\nReserved argument names\nAccess context\nShort-term memory (State)\nAccess state\nUpdate state\nContext\nLong-term memory (Store)\nStream writer\nToolNode\nBasic usage\nError handling\nRoute with tools_condition\nState injection\nPrebuilt tools\nServer-side tool use\nCore components\nTools\nCopy page\nTools extend what agents can doâ€”letting them fetch real-time data, execute code, query external databases, and take actions in the world.\nUnder the hood, tools are callable functions with well-defined inputs and outputs that get passed to a chat model. The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.\nFor details on how models handle tool calls, see Tool calling.\nï¿½\nCreate tools\nï¿½\nBasic tool definition\nThe simplest way to create a tool is with the @tool decorator. By default, the functionâ€™s docstring becomes the toolâ€™s description that helps the model understand when to use it:\nCopy\nfrom langchain.tools import tool\n\n@tool\ndef search_database(query: str, limit: int = 10) -> str:\n    \"\"\"Search the customer database for records matching the query.\n\n    Args:\n        query: Search terms to look for\n        limit: Maximum number of results to return\n    \"\"\"\n    return f\"Found {limit} results for '{query}'\"\n\nType hints are required as they define the toolâ€™s input schema. The docstring should be informative and concise to help the model understand the toolâ€™s purpose.\nServer-side tool use: Some chat models feature built-in tools (web search, code interpreters) that are executed server-side. See Server-side tool use for details.\nPrefer snake_case for tool names (e.g., web_search instead of Web Search). Some model providers have issues with or reject names containing spaces or special characters with errors. Sticking to alphanumeric characters, underscores, and hyphens helps to improve compatibility across providers.\nï¿½\nCustomize tool properties\nï¿½\nCustom tool name\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:\nCopy\n@tool(\"web_search\")  # Custom name\ndef search(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nprint(search.name)  # web_search\n\nï¿½\nCustom tool description\nOverride the auto-generated tool description for clearer model guidance:\nCopy\n@tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\")\ndef calc(expression: str) -> str:\n    \"\"\"Evaluate mathematical expressions.\"\"\"\n    return str(eval(expression))\n\nï¿½\nAdvanced schema definition\nDefine complex inputs with Pydantic models or JSON schemas:\nPydantic model\nJSON Schema\nCopy\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\nclass WeatherInput(BaseModel):\n    \"\"\"Input for weather queries.\"\"\"\n    location: str = Field(description=\"City name or coordinates\")\n    units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n        default=\"celsius\",\n        description=\"Temperature unit preference\"\n    )\n    include_forecast: bool = Field(\n        default=False,\n        description=\"Include 5-day forecast\"\n    )\n\n@tool(args_schema=WeatherInput)\ndef get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n    \"\"\"Get current weather and optional forecast.\"\"\"\n    temp = 22 if units == \"celsius\" else 72\n    result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n    if include_forecast:\n        result += \"\\nNext 5 days: Sunny\"\n    return result\n\nï¿½\nReserved argument names\nThe following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.\nParameter name\tPurpose\nconfig\tReserved for passing RunnableConfig to tools internally\nruntime\tReserved for ToolRuntime parameter (accessing state, context, store)\nTo access runtime information, use the ToolRuntime parameter instead of naming your own arguments config or runtime.\nï¿½\nAccess context\nTools are most powerful when they can access runtime information like conversation history, user data, and persistent memory. This section covers how to access and update this information from within your tools.\nTools can access runtime information through the ToolRuntime parameter, which provides:\nComponent\tDescription\tUse case\nState\tShort-term memory - mutable data that exists for the current conversation (messages, counters, custom fields)\tAccess conversation history, track tool call counts\nContext\tImmutable configuration passed at invocation time (user IDs, session info)\tPersonalize responses based on user identity\nStore\tLong-term memory - persistent data that survives across conversations\tSave user preferences, maintain knowledge base\nStream Writer\tEmit real-time updates during tool execution\tShow progress for long-running operations\nConfig\tRunnableConfig for the execution\tAccess callbacks, tags, and metadata\nTool Call ID\tUnique identifier for the current tool invocation\tCorrelate tool calls for logs and model invocations\n\nâš¡ Enhanced Tool Capabilities\n\nðŸ“Š Available Resources\n\nðŸ”§ Tool Runtime Context\n\nTool Call\n\nToolRuntime\n\nState Access\n\nContext Access\n\nStore Access\n\nStream Writer\n\nMessages\n\nCustom State\n\nUser ID\n\nSession Info\n\nLong-term Memory\n\nUser Preferences\n\nContext-Aware Tools\n\nStateful Tools\n\nMemory-Enabled Tools\n\nStreaming Tools\n\nï¿½\nShort-term memory (State)\nState represents short-term memory that exists for the duration of a conversation. It includes the message history and any custom fields you define in your graph state.\nAdd runtime: ToolRuntime to your tool signature to access state. This parameter is automatically injected and hidden from the LLM - it wonâ€™t appear in the toolâ€™s schema.\nï¿½\nAccess state\nTools can access the current conversation state using runtime.state:\nCopy\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.messages import HumanMessage\n\n@tool\ndef get_last_user_message(runtime: ToolRuntime) -> str:\n    \"\"\"Get the most recent message from the user.\"\"\"\n    messages = runtime.state[\"messages\"]\n\n    # Find the last human message\n    for message in reversed(messages):\n        if isinstance(message, HumanMessage):\n            return message.content\n\n    return \"No user messages found\"\n\n# Access custom state fields\n@tool\ndef get_user_preference(\n    pref_name: str,\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Get a user preference value.\"\"\"\n    preferences = runtime.state.get(\"user_preferences\", {})\n    return preferences.get(pref_name, \"Not set\")\n\nThe runtime parameter is hidden from the model. For the example above, the model only sees pref_name in the tool schema.\nï¿½\nUpdate state\nUse Command to update the agentâ€™s state. This is useful for tools that need to update custom state fields:\nCopy\nfrom langgraph.types import Command\nfrom langchain.tools import tool\n\n@tool\ndef set_user_name(new_name: str) -> Command:\n    \"\"\"Set the user's name in the conversation state.\"\"\"\n    return Command(update={\"user_name\": new_name})\n\nWhen tools update state variables, consider defining a reducer for those fields. Since LLMs can call multiple tools in parallel, a reducer determines how to resolve conflicts when the same state field is updated by concurrent tool calls.\nï¿½\nContext\nContext provides immutable configuration data that is passed at invocation time. Use it for user IDs, session details, or application-specific settings that shouldnâ€™t change during a conversation.\nAccess context through runtime.context:\nCopy\nfrom dataclasses import dataclass\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\nUSER_DATABASE = {\n    \"user123\": {\n        \"name\": \"Alice Johnson\",\n        \"account_type\": \"Premium\",\n        \"balance\": 5000,\n        \"email\": \"alice@example.com\"\n    },\n    \"user456\": {\n        \"name\": \"Bob Smith\",\n        \"account_type\": \"Standard\",\n        \"balance\": 1200,\n        \"email\": \"bob@example.com\"\n    }\n}\n\n@dataclass\nclass UserContext:\n    user_id: str\n\n@tool\ndef get_account_info(runtime: ToolRuntime[UserContext]) -> str:\n    \"\"\"Get the current user's account information.\"\"\"\n    user_id = runtime.context.user_id\n\n    if user_id in USER_DATABASE:\n        user = USER_DATABASE[user_id]\n        return f\"Account holder: {user['name']}\\nType: {user['account_type']}\\nBalance: ${user['balance']}\"\n    return \"User not found\"\n\nmodel = ChatOpenAI(model=\"gpt-4.1\")\nagent = create_agent(\n    model,\n    tools=[get_account_info],\n    context_schema=UserContext,\n    system_prompt=\"You are a financial assistant.\"\n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]},\n    context=UserContext(user_id=\"user123\")\n)\n\nï¿½\nLong-term memory (Store)\nThe BaseStore provides persistent storage that survives across conversations. Unlike state (short-term memory), data saved to the store remains available in future sessions.\nAccess the store through runtime.store. The store uses a namespace/key pattern to organize data:\nFor production deployments, use a persistent store implementation like PostgresStore instead of InMemoryStore. See the memory documentation for setup details.\nCopy\nfrom typing import Any\nfrom langgraph.store.memory import InMemoryStore\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\n\n\n# Access memory\n@tool\ndef get_user_info(user_id: str, runtime: ToolRuntime) -> str:\n    \"\"\"Look up user info.\"\"\"\n    store = runtime.store\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\n# Update memory\n@tool\ndef save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:\n    \"\"\"Save user info.\"\"\"\n    store = runtime.store\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nstore = InMemoryStore()\nagent = create_agent(\n    model,\n    tools=[get_user_info, save_user_info],\n    store=store\n)\n\n# First session: save user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}]\n})\n\n# Second session: get user info\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}]\n})\n# Here is the user info for user with ID \"abc123\":\n# - Name: Foo\n# - Age: 25\n# - Email: foo@langchain.dev\n\nSee all 42 lines\nï¿½\nStream writer\nStream real-time updates from tools during execution. This is useful for providing progress feedback to users during long-running operations.\nUse runtime.stream_writer to emit custom updates:\nCopy\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef get_weather(city: str, runtime: ToolRuntime) -> str:\n    \"\"\"Get weather for a given city.\"\"\"\n    writer = runtime.stream_writer\n\n    # Stream custom updates as the tool executes\n    writer(f\"Looking up data for city: {city}\")\n    writer(f\"Acquired data for city: {city}\")\n\n    return f\"It's always sunny in {city}!\"\n\nIf you use runtime.stream_writer inside your tool, the tool must be invoked within a LangGraph execution context. See Streaming for more details.\nï¿½\nToolNode\nToolNode is a prebuilt node that executes tools in LangGraph workflows. It handles parallel tool execution, error handling, and state injection automatically.\nFor custom workflows where you need fine-grained control over tool execution patterns, use ToolNode instead of create_agent. Itâ€™s the building block that powers agent tool execution.\nï¿½\nBasic usage\nCopy\nfrom langchain.tools import tool\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n@tool\ndef calculator(expression: str) -> str:\n    \"\"\"Evaluate a math expression.\"\"\"\n    return str(eval(expression))\n\n# Create the ToolNode with your tools\ntool_node = ToolNode([search, calculator])\n\n# Use in a graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"tools\", tool_node)\n# ... add other nodes and edges\n\nï¿½\nError handling\nConfigure how tool errors are handled. See the ToolNode API reference for all options.\nCopy\nfrom langgraph.prebuilt import ToolNode\n\n# Default: catch invocation errors, re-raise execution errors\ntool_node = ToolNode(tools)\n\n# Catch all errors and return error message to LLM\ntool_node = ToolNode(tools, handle_tool_errors=True)\n\n# Custom error message\ntool_node = ToolNode(tools, handle_tool_errors=\"Something went wrong, please try again.\")\n\n# Custom error handler\ndef handle_error(e: ValueError) -> str:\n    return f\"Invalid input: {e}\"\n\ntool_node = ToolNode(tools, handle_tool_errors=handle_error)\n\n# Only catch specific exception types\ntool_node = ToolNode(tools, handle_tool_errors=(ValueError, TypeError))\n\nï¿½\nRoute with tools_condition\nUse tools_condition for conditional routing based on whether the LLM made tool calls:\nCopy\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"llm\", call_llm)\nbuilder.add_node(\"tools\", ToolNode(tools))\n\nbuilder.add_edge(START, \"llm\")\nbuilder.add_conditional_edges(\"llm\", tools_condition)  # Routes to \"tools\" or END\nbuilder.add_edge(\"tools\", \"llm\")\n\ngraph = builder.compile()\n\nï¿½\nState injection\nTools can access the current graph state through ToolRuntime:\nCopy\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.prebuilt import ToolNode\n\n@tool\ndef get_message_count(runtime: ToolRuntime) -> str:\n    \"\"\"Get the number of messages in the conversation.\"\"\"\n    messages = runtime.state[\"messages\"]\n    return f\"There are {len(messages)} messages.\"\n\ntool_node = ToolNode([get_message_count])\n\nFor more details on accessing state, context, and long-term memory from tools, see Access context.\nï¿½\nPrebuilt tools\nLangChain provides a large collection of prebuilt tools and toolkits for common tasks like web search, code interpretation, database access, and more. These ready-to-use tools can be directly integrated into your agents without writing custom code.\nSee the tools and toolkits integration page for a complete list of available tools organized by category.\nï¿½\nServer-side tool use\nSome chat models feature built-in tools that are executed server-side by the model provider. These include capabilities like web search and code interpreters that donâ€™t require you to define or host the tool logic.\nRefer to the individual chat model integration pages and the tool calling documentation for details on enabling and using these built-in tools.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494356,
    "timestamp": "2026-02-25T08:09:37.370Z",
    "title": "Frontend - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/streaming/frontend",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nOverview\nFrontend\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nInstallation\nBasic usage\nThread management\nResume after page refresh\nOptimistic updates\nOptimistic thread creation\nCached thread display\nBranching\nType-safe streaming\nWith createAgent\nWith StateGraph\nWith Annotation types\nAdvanced type configuration\nRendering tool calls\nCustom streaming events\nEvent handling\nAvailable callbacks\nMulti-agent streaming\nHuman-in-the-loop\nReasoning models\nCustom state types\nCustom transport\nRelated\nCore components\nStreaming\nFrontend\nCopy page\n\nBuild generative UIs with real-time streaming from LangChain agents, LangGraph graphs, and custom APIs\n\nThe useStream React hook provides seamless integration with LangGraph streaming capabilities. It handles all the complexities of streaming, state management, and branching logic, letting you focus on building great generative UI experiences.\nKey features:\n Messages streaming â€” Handle a stream of message chunks to form a complete message\n Automatic state management â€” for messages, interrupts, loading states, and errors\n Conversation branching â€” Create alternate conversation paths from any point in the chat history\n UI-agnostic design â€” Bring your own components and styling\nï¿½\nInstallation\nInstall the LangGraph SDK to use the useStream hook in your React application:\nï¿½\nBasic usage\nThe useStream hook connects to any LangGraph graph, whether thatâ€™s running on from your own endpoint, or deployed using LangSmith deployments.\nCopy\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\nfunction Chat() {\n  const stream = useStream({\n    assistantId: \"agent\",\n    // Local development\n    apiUrl: \"http://localhost:2024\",\n    // Production deployment (LangSmith hosted)\n    // apiUrl: \"https://your-deployment.us.langgraph.app\"\n  });\n\n  const handleSubmit = (message: string) => {\n    stream.submit({\n      messages: [\n        { content: message, type: \"human\" }\n      ],\n    });\n  };\n\n  return (\n    <div>\n      {stream.messages.map((message, idx) => (\n        <div key={message.id ?? idx}>\n          {message.type}: {message.content}\n        </div>\n      ))}\n\n      {stream.isLoading && <div>Loading...</div>}\n      {stream.error && <div>Error: {stream.error.message}</div>}\n    </div>\n  );\n}\n\nLearn how to deploy your agents to LangSmith for production-ready hosting with built-in observability, authentication, and scaling.\n\n`useStream` parameters\n\n`useStream` return values\n\nï¿½\nThread management\nKeep track of conversations with built-in thread management. You can access the current thread ID and get notified when new threads are created:\nCopy\nimport { useState } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\nfunction Chat() {\n  const [threadId, setThreadId] = useState<string | null>(null);\n\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId: threadId,\n    onThreadId: setThreadId,\n  });\n\n  // threadId is updated when a new thread is created\n  // Store it in URL params or localStorage for persistence\n}\n\nWe recommend storing the threadId to let users resume conversations after page refreshes.\nï¿½\nResume after page refresh\nThe useStream hook can automatically resume an ongoing run upon mounting by setting reconnectOnMount: true. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost.\nCopy\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  reconnectOnMount: true,\n});\n\nBy default the ID of the created run is stored in window.sessionStorage, which can be swapped by passing a custom storage function:\nCopy\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n  reconnectOnMount: () => window.localStorage,\n});\n\nFor manual control over the resumption process, use the run callbacks to persist metadata and joinStream to resume:\nCopy\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport { useEffect, useRef } from \"react\";\n\nfunction Chat({ threadId }: { threadId: string | null }) {\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId,\n    onCreated: (run) => {\n      // Persist run ID when stream starts\n      window.sessionStorage.setItem(`resume:${run.thread_id}`, run.run_id);\n    },\n    onFinish: (_, run) => {\n      // Clean up when stream completes\n      window.sessionStorage.removeItem(`resume:${run?.thread_id}`);\n    },\n  });\n\n  // Resume stream on mount if there's a stored run ID\n  const joinedThreadId = useRef<string | null>(null);\n  useEffect(() => {\n    if (!threadId) return;\n    const runId = window.sessionStorage.getItem(`resume:${threadId}`);\n    if (runId && joinedThreadId.current !== threadId) {\n      stream.joinStream(runId);\n      joinedThreadId.current = threadId;\n    }\n  }, [threadId]);\n\n  const handleSubmit = (text: string) => {\n    // Use streamResumable to ensure events aren't lost\n    stream.submit(\n      { messages: [{ type: \"human\", content: text }] },\n      { streamResumable: true }\n    );\n  };\n}\n\nTry the session persistence example\nSee a complete implementation of stream resumption with reconnectOnMount and thread persistence in the session-persistence example.\nï¿½\nOptimistic updates\nYou can optimistically update the client state before performing a network request, providing immediate feedback to the user:\nCopy\nconst stream = useStream({\n  apiUrl: \"http://localhost:2024\",\n  assistantId: \"agent\",\n});\n\nconst handleSubmit = (text: string) => {\n  const newMessage = { type: \"human\" as const, content: text };\n\n  stream.submit(\n    { messages: [newMessage] },\n    {\n      optimisticValues(prev) {\n        const prevMessages = prev.messages ?? [];\n        return { ...prev, messages: [...prevMessages, newMessage] };\n      },\n    }\n  );\n};\n\nï¿½\nOptimistic thread creation\nUse the threadId option in submit to enable optimistic UI patterns where you need to know the thread ID before the thread is created:\nCopy\nimport { useState } from \"react\";\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\n\nfunction Chat() {\n  const [threadId, setThreadId] = useState<string | null>(null);\n  const [optimisticThreadId] = useState(() => crypto.randomUUID());\n\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId,\n    onThreadId: setThreadId,\n  });\n\n  const handleSubmit = (text: string) => {\n    // Navigate immediately without waiting for thread creation\n    window.history.pushState({}, \"\", `/threads/${optimisticThreadId}`);\n\n    // Create thread with the predetermined ID\n    stream.submit(\n      { messages: [{ type: \"human\", content: text }] },\n      { threadId: optimisticThreadId }\n    );\n  };\n}\n\nï¿½\nCached thread display\nUse the initialValues option to display cached thread data immediately while the history is being loaded from the server:\nCopy\nfunction Chat({ threadId, cachedData }) {\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n    threadId,\n    initialValues: cachedData?.values,\n  });\n\n  // Shows cached messages instantly, then updates when server responds\n}\n\nï¿½\nBranching\nCreate alternate conversation paths by editing previous messages or regenerating AI responses. Use getMessagesMetadata() to access checkpoint information for branching:\nChat.tsx\nBranchSwitcher.tsx\nCopy\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport { BranchSwitcher } from \"./BranchSwitcher\";\n\nfunction Chat() {\n  const stream = useStream({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"agent\",\n  });\n\n  return (\n    <div>\n      {stream.messages.map((message) => {\n        const meta = stream.getMessagesMetadata(message);\n        const parentCheckpoint = meta?.firstSeenState?.parent_checkpoint;\n\n        return (\n          <div key={message.id}>\n            <div>{message.content as string}</div>\n\n            {/* Edit human messages */}\n            {message.type === \"human\" && (\n              <button\n                onClick={() => {\n                  const newContent = prompt(\"Edit message:\", message.content as string);\n                  if (newContent) {\n                    stream.submit(\n                      { messages: [{ type: \"human\", content: newContent }] },\n                      { checkpoint: parentCheckpoint }\n                    );\n                  }\n                }}\n              >\n                Edit\n              </button>\n            )}\n\n            {/* Regenerate AI messages */}\n            {message.type === \"ai\" && (\n              <button\n                onClick={() => stream.submit(undefined, { checkpoint: parentCheckpoint })}\n              >\n                Regenerate\n              </button>\n            )}\n\n            {/* Switch between branches */}\n            <BranchSwitcher\n              branch={meta?.branch}\n              branchOptions={meta?.branchOptions}\n              onSelect={(branch) => stream.setBranch(branch)}\n            />\n          </div>\n        );\n      })}\n    </div>\n  );\n}\n\nFor advanced use cases, use the experimental_branchTree property to get the tree representation of the thread for non-message based graphs.\nTry the branching example\nSee a complete implementation of conversation branching with edit, regenerate, and branch switching in the branching-chat example.\nï¿½\nType-safe streaming\nThe useStream hook supports full type inference when used with agents created via @[createAgent] or graphs created with StateGraph. Pass typeof agent or typeof graph as the type parameter to automatically infer tool call types.\nï¿½\nWith createAgent\nWhen using @[createAgent], tool call types are automatically inferred from the tools you register to your agent:\nagent.py\nChat.tsx\ntypes.ts\nCopy\nfrom langchain import create_agent, tool\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get weather for a location.\"\"\"\n    return f\"Weather in {location}: Sunny, 72Â°F\"\n\nagent = create_agent(\n    model=\"openai:gpt-4.1-mini\",\n    tools=[get_weather],\n)\n\nï¿½\nWith StateGraph\nFor custom StateGraph applications, the state types are inferred from the graphâ€™s annotation:\ngraph.py\nChat.tsx\ntypes.ts\nCopy\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langchain_openai import ChatOpenAI\nfrom typing import TypedDict, Annotated\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\")\n\nasync def agent(state: State) -> dict:\n    response = await model.ainvoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"agent\", agent)\nworkflow.add_edge(START, \"agent\")\nworkflow.add_edge(\"agent\", END)\n\ngraph = workflow.compile()\n\nï¿½\nWith Annotation types\nIf youâ€™re using LangGraph.js, you can reuse your graphâ€™s annotation types. Make sure to only import types to avoid importing the entire LangGraph.js runtime:\nï¿½\nAdvanced type configuration\nYou can specify additional type parameters for interrupts, custom events, and configurable options:\nï¿½\nRendering tool calls\nUse getToolCalls to extract and render tool calls from AI messages. Tool calls include the call details, result (if completed), and state.\nagent.py\nChat.tsx\nToolCallCard.tsx\nWeatherCard.tsx\ntypes.ts\nutils.ts\nCopy\nfrom langchain import create_agent, tool\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    return f'{{\"status\": \"success\", \"content\": \"Weather in {location}: Sunny, 72Â°F\"}}'\n\nagent = create_agent(\n    model=\"openai:gpt-4.1-mini\",\n    tools=[get_weather],\n)\n\nTry the tool calling example\nSee a complete implementation of tool call rendering with weather, calculator, and note-taking tools in the tool-calling-agent example.\nï¿½\nCustom streaming events\nStream custom data from your agent using the writer in your tools or nodes. Handle these events in the UI with the onCustomEvent callback.\nagent.py\nChat.tsx\ntypes.ts\nCopy\nimport asyncio\nimport time\nfrom langchain import create_agent, tool\nfrom langchain.types import ToolRuntime\n\n@tool\nasync def analyze_data(data_source: str, *, config: ToolRuntime) -> str:\n    \"\"\"Analyze data with progress updates.\"\"\"\n    steps = [\"Connecting...\", \"Fetching...\", \"Processing...\", \"Done!\"]\n\n    for i, step in enumerate(steps):\n        # Emit progress events during execution\n        if config.writer:\n            config.writer({\n                \"type\": \"progress\",\n                \"id\": f\"analysis-{int(time.time() * 1000)}\",\n                \"message\": step,\n                \"progress\": ((i + 1) / len(steps)) * 100,\n            })\n        await asyncio.sleep(0.5)\n\n    return '{\"result\": \"Analysis complete\"}'\n\nagent = create_agent(\n    model=\"openai:gpt-4.1-mini\",\n    tools=[analyze_data],\n)\n\nTry the custom streaming example\nSee a complete implementation of custom events with progress bars, status badges, and file operation cards in the custom-streaming example.\nï¿½\nEvent handling\nThe useStream hook provides callback options that give you access to different types of streaming events. You donâ€™t need to explicitly configure stream modesâ€”just pass callbacks for the event types you want to handle:\nï¿½\nAvailable callbacks\nCallback\tDescription\tStream mode\nonUpdateEvent\tCalled when a state update is received after each graph step\tupdates\nonCustomEvent\tCalled when a custom event is received from your graph\tcustom\nonMetadataEvent\tCalled with run and thread metadata\tmetadata\nonError\tCalled when an error occurs\t-\nonFinish\tCalled when the stream completes\t-\nï¿½\nMulti-agent streaming\nWhen working with multi-agent systems or graphs with multiple nodes, use message metadata to identify which node generated each message. This is particularly useful when multiple LLMs run in parallel and you want to display their outputs with distinct visual styling.\nagent.py\nChat.tsx\ntypes.ts\nCopy\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, START, END, Send\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langchain.messages import BaseMessage, AIMessage\nfrom typing import TypedDict, Annotated\nimport operator\n\n# Use different model instances for variety\nanalytical_model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.3)\ncreative_model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.9)\npractical_model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.5)\n\nclass State(TypedDict):\n    messages: Annotated[list[BaseMessage], operator.add]\n    topic: str\n    analytical_research: str\n    creative_research: str\n    practical_research: str\n\ndef fan_out_to_researchers(state: State) -> list[Send]:\n    return [\n        Send(\"researcher_analytical\", state),\n        Send(\"researcher_creative\", state),\n        Send(\"researcher_practical\", state),\n    ]\n\ndef dispatcher(state: State) -> dict:\n    last_message = state[\"messages\"][-1] if state[\"messages\"] else None\n    topic = last_message.content if last_message else \"\"\n    return {\"topic\": topic}\n\nasync def researcher_analytical(state: State) -> dict:\n    response = await analytical_model.ainvoke([\n        {\"role\": \"system\", \"content\": \"You are an analytical research expert.\"},\n        {\"role\": \"user\", \"content\": f\"Research: {state['topic']}\"},\n    ])\n    return {\n        \"analytical_research\": response.content,\n        \"messages\": [AIMessage(content=response.content, name=\"researcher_analytical\")],\n    }\n\n# Similar nodes for creative and practical researchers...\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"dispatcher\", dispatcher)\nworkflow.add_node(\"researcher_analytical\", researcher_analytical)\nworkflow.add_node(\"researcher_creative\", researcher_creative)\nworkflow.add_node(\"researcher_practical\", researcher_practical)\nworkflow.add_edge(START, \"dispatcher\")\nworkflow.add_conditional_edges(\"dispatcher\", fan_out_to_researchers)\nworkflow.add_edge(\"researcher_analytical\", END)\nworkflow.add_edge(\"researcher_creative\", END)\nworkflow.add_edge(\"researcher_practical\", END)\n\nagent: CompiledStateGraph = workflow.compile()\n\nTry the parallel research example\nSee a complete implementation of multi-agent streaming with three parallel researchers and distinct visual styling in the parallel-research example.\nï¿½\nHuman-in-the-loop\nHandle interrupts when the agent requires human approval for tool execution. Learn more in the How to handle interrupts guide.\nagent.py\nChat.tsx\ntypes.ts\nCopy\nfrom langchain import create_agent, tool, human_in_the_loop_middleware\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\")\n\n@tool\ndef send_email(to: str, subject: str, body: str) -> dict:\n    \"\"\"Send an email. Requires human approval.\"\"\"\n    return {\n        \"status\": \"success\",\n        \"content\": f'Email sent to {to} with subject \"{subject}\"',\n    }\n\n@tool\ndef delete_file(path: str) -> dict:\n    \"\"\"Delete a file. Requires human approval.\"\"\"\n    return {\"status\": \"success\", \"content\": f'File \"{path}\" deleted'}\n\n@tool\ndef read_file(path: str) -> dict:\n    \"\"\"Read file contents. No approval needed.\"\"\"\n    return {\"status\": \"success\", \"content\": f\"Contents of {path}...\"}\n\nagent = create_agent(\n    model=model,\n    tools=[send_email, delete_file, read_file],\n    middleware=[\n        human_in_the_loop_middleware(\n            interrupt_on={\n                \"send_email\": {\n                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n                    \"description\": \"ðŸ“§ Review email before sending\",\n                },\n                \"delete_file\": {\n                    \"allowed_decisions\": [\"approve\", \"reject\"],\n                    \"description\": \"ðŸ—‘ï¸ Confirm file deletion\",\n                },\n                \"read_file\": False,  # Safe - auto-approved\n            }\n        ),\n    ],\n    checkpointer=MemorySaver(),\n)\n\nTry the human-in-the-loop example\nSee a complete implementation of approval workflows with approve, reject, and edit actions in the human-in-the-loop example.\nï¿½\nReasoning models\nExtended reasoning/thinking support is currently experimental. The streaming interface for reasoning tokens varies by provider (OpenAI vs. Anthropic) and may change as abstractions are developed.\nWhen using models with extended reasoning capabilities (like OpenAIâ€™s reasoning models or Anthropicâ€™s extended thinking), the thinking process is embedded in the message content. Youâ€™ll need to extract and display it separately.\nagent.py\nChat.tsx\ntypes.ts\nutils.ts\nCopy\nfrom langchain import create_agent\nfrom langchain_openai import ChatOpenAI\n\n# Use a reasoning-capable model\n# For OpenAI: o1, o1-mini, o1-preview\n# For Anthropic: claude-sonnet-4-20250514 with extended thinking enabled\nmodel = ChatOpenAI(model=\"o1-mini\")\n\nagent = create_agent(\n    model=model,\n    tools=[],  # Reasoning models work best for complex reasoning tasks\n)\n\nTry the reasoning example\nSee a complete implementation of reasoning token display with OpenAI and Anthropic models in the reasoning-agent example.\nï¿½\nCustom state types\nFor custom LangGraph applications, embed your tool call types in your stateâ€™s messages property.\nï¿½\nCustom transport\nFor custom API endpoints or non-standard deployments, use the transport option with FetchStreamTransport to connect to any streaming API.\nï¿½\nRelated\nStreaming overview â€” Server-side streaming with LangChain agents\nuseStream API Reference â€” Full API documentation\nAgent Chat UI â€” Pre-built chat interface for LangGraph agents\nHuman-in-the-loop â€” Configuring interrupts for human review\nMulti-agent systems â€” Building agents with multiple LLMs\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494357,
    "timestamp": "2026-02-25T08:09:37.374Z",
    "title": "Structured output - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/structured-output",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nResponse format\nProvider strategy\nTool calling strategy\nCustom tool message content\nError handling\nMultiple structured outputs error\nSchema validation error\nError handling strategies\nCore components\nStructured output\nCopy page\nStructured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, Pydantic models, or dataclasses that your application can use directly.\nThis page covers structured output with agents using create_agent. To use structured output directly on a model (outside of agents), see Models - Structured output.\nLangChainâ€™s create_agent handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, itâ€™s captured, validated, and returned in the 'structured_response' key of the agentâ€™s state.\nCopy\ndef create_agent(\n    ...\n    response_format: Union[\n        ToolStrategy[StructuredResponseT],\n        ProviderStrategy[StructuredResponseT],\n        type[StructuredResponseT],\n        None,\n    ]\n\nï¿½\nResponse format\nUse response_format to control how the agent returns structured data:\nToolStrategy[StructuredResponseT]: Uses tool calling for structured output\nProviderStrategy[StructuredResponseT]: Uses provider-native structured output\ntype[StructuredResponseT]: Schema type - automatically selects best strategy based on model capabilities\nNone: Structured output not explicitly requested\nWhen a schema type is provided directly, LangChain automatically chooses:\nProviderStrategy if the model and provider chosen supports native structured output (e.g. OpenAI, Anthropic (Claude), or xAI (Grok)).\nToolStrategy for all other models.\nSupport for native structured output features is read dynamically from the modelâ€™s profile data if using langchain>=1.1. If data are not available, use another condition or specify manually:\nCopy\ncustom_profile = {\n    \"structured_output\": True,\n    # ...\n}\nmodel = init_chat_model(\"...\", profile=custom_profile)\n\nIf tools are specified, the model must support simultaneous use of tools and structured output.\nThe structured response is returned in the structured_response key of the agentâ€™s final state.\nï¿½\nProvider strategy\nSome model providers support structured output natively through their APIs (e.g. OpenAI, xAI (Grok), Gemini, Anthropic (Claude)). This is the most reliable method when available.\nTo use this strategy, configure a ProviderStrategy:\nCopy\nclass ProviderStrategy(Generic[SchemaT]):\n    schema: type[SchemaT]\n    strict: bool | None = None\n\nThe strict param requires langchain>=1.2.\nï¿½\nschema\nrequired\nThe schema defining the structured output format. Supports:\nPydantic models: BaseModel subclasses with field validation. Returns validated Pydantic instance.\nDataclasses: Python dataclasses with type annotations. Returns dict.\nTypedDict: Typed dictionary classes. Returns dict.\nJSON Schema: Dictionary with JSON schema specification. Returns dict.\nï¿½\nstrict\nOptional boolean parameter to enable strict schema adherence. Supported by some providers (e.g., OpenAI and xAI). Defaults to None (disabled).\nLangChain automatically uses ProviderStrategy when you pass a schema type directly to create_agent.response_format and the model supports native structured output:\nPydantic Model\nDataclass\nTypedDict\nJSON Schema\nCopy\nfrom pydantic import BaseModel, Field\nfrom langchain.agents import create_agent\n\n\nclass ContactInfo(BaseModel):\n    \"\"\"Contact information for a person.\"\"\"\n    name: str = Field(description=\"The name of the person\")\n    email: str = Field(description=\"The email address of the person\")\n    phone: str = Field(description=\"The phone number of the person\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    response_format=ContactInfo  # Auto-selects ProviderStrategy\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n})\n\nprint(result[\"structured_response\"])\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n\nProvider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.\nIf the provider natively supports structured output for your model choice, it is functionally equivalent to write response_format=ProductReview instead of response_format=ProviderStrategy(ProductReview).\nIn either case, if structured output is not supported, the agent will fall back to a tool calling strategy.\nï¿½\nTool calling strategy\nFor models that donâ€™t support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling (most modern models).\nTo use this strategy, configure a ToolStrategy:\nCopy\nclass ToolStrategy(Generic[SchemaT]):\n    schema: type[SchemaT]\n    tool_message_content: str | None\n    handle_errors: Union[\n        bool,\n        str,\n        type[Exception],\n        tuple[type[Exception], ...],\n        Callable[[Exception], str],\n    ]\n\nï¿½\nschema\nrequired\nThe schema defining the structured output format. Supports:\nPydantic models: BaseModel subclasses with field validation. Returns validated Pydantic instance.\nDataclasses: Python dataclasses with type annotations. Returns dict.\nTypedDict: Typed dictionary classes. Returns dict.\nJSON Schema: Dictionary with JSON schema specification. Returns dict.\nUnion types: Multiple schema options. The model will choose the most appropriate schema based on the context.\nï¿½\ntool_message_content\nCustom content for the tool message returned when structured output is generated. If not provided, defaults to a message showing the structured response data.\nï¿½\nhandle_errors\nError handling strategy for structured output validation failures. Defaults to True.\nTrue: Catch all errors with default error template\nstr: Catch all errors with this custom message\ntype[Exception]: Only catch this exception type with default message\ntuple[type[Exception], ...]: Only catch these exception types with default message\nCallable[[Exception], str]: Custom function that returns error message\nFalse: No retry, let exceptions propagate\nPydantic Model\nDataclass\nTypedDict\nJSON Schema\nUnion Types\nCopy\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ProductReview(BaseModel):\n    \"\"\"Analysis of a product review.\"\"\"\n    rating: int | None = Field(description=\"The rating of the product\", ge=1, le=5)\n    sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\")\n    key_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=tools,\n    response_format=ToolStrategy(ProductReview)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n})\nresult[\"structured_response\"]\n# ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])\n\nï¿½\nCustom tool message content\nThe tool_message_content parameter allows you to customize the message that appears in the conversation history when structured output is generated:\nCopy\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass MeetingAction(BaseModel):\n    \"\"\"Action items extracted from a meeting transcript.\"\"\"\n    task: str = Field(description=\"The specific task to be completed\")\n    assignee: str = Field(description=\"Person responsible for the task\")\n    priority: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Priority level\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(\n        schema=MeetingAction,\n        tool_message_content=\"Action item captured and added to meeting notes!\"\n    )\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"From our meeting: Sarah needs to update the project timeline as soon as possible\"}]\n})\n\nCopy\n================================ Human Message =================================\n\nFrom our meeting: Sarah needs to update the project timeline as soon as possible\n================================== Ai Message ==================================\nTool Calls:\n  MeetingAction (call_1)\n Call ID: call_1\n  Args:\n    task: Update the project timeline\n    assignee: Sarah\n    priority: high\n================================= Tool Message =================================\nName: MeetingAction\n\nAction item captured and added to meeting notes!\n\nWithout tool_message_content, our final ToolMessage would be:\nCopy\n================================= Tool Message =================================\nName: MeetingAction\n\nReturning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}\n\nï¿½\nError handling\nModels can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.\nï¿½\nMultiple structured outputs error\nWhen a model incorrectly calls multiple structured output tools, the agent provides error feedback in a ToolMessage and prompts the model to retry:\nCopy\nfrom pydantic import BaseModel, Field\nfrom typing import Union\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str = Field(description=\"Person's name\")\n    email: str = Field(description=\"Email address\")\n\nclass EventDetails(BaseModel):\n    event_name: str = Field(description=\"Name of the event\")\n    date: str = Field(description=\"Event date\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(Union[ContactInfo, EventDetails])  # Default: handle_errors=True\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}]\n})\n\nCopy\n================================ Human Message =================================\n\nExtract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\nNone\n================================== Ai Message ==================================\nTool Calls:\n  ContactInfo (call_1)\n Call ID: call_1\n  Args:\n    name: John Doe\n    email: john@email.com\n  EventDetails (call_2)\n Call ID: call_2\n  Args:\n    event_name: Tech Conference\n    date: March 15th\n================================= Tool Message =================================\nName: ContactInfo\n\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.\n================================= Tool Message =================================\nName: EventDetails\n\nError: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\n  ContactInfo (call_3)\n Call ID: call_3\n  Args:\n    name: John Doe\n    email: john@email.com\n================================= Tool Message =================================\nName: ContactInfo\n\nReturning structured response: {'name': 'John Doe', 'email': 'john@email.com'}\n\nï¿½\nSchema validation error\nWhen structured output doesnâ€™t match the expected schema, the agent provides specific error feedback:\nCopy\nfrom pydantic import BaseModel, Field\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ProductRating(BaseModel):\n    rating: int | None = Field(description=\"Rating from 1-5\", ge=1, le=5)\n    comment: str = Field(description=\"Review comment\")\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(ProductRating),  # Default: handle_errors=True\n    system_prompt=\"You are a helpful assistant that parses product reviews. Do not make any field or value up.\"\n)\n\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Parse this: Amazing product, 10/10!\"}]\n})\n\nCopy\n================================ Human Message =================================\n\nParse this: Amazing product, 10/10!\n================================== Ai Message ==================================\nTool Calls:\n  ProductRating (call_1)\n Call ID: call_1\n  Args:\n    rating: 10\n    comment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\n\nError: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating.rating\n  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\n Please fix your mistakes.\n================================== Ai Message ==================================\nTool Calls:\n  ProductRating (call_2)\n Call ID: call_2\n  Args:\n    rating: 5\n    comment: Amazing product\n================================= Tool Message =================================\nName: ProductRating\n\nReturning structured response: {'rating': 5, 'comment': 'Amazing product'}\n\nï¿½\nError handling strategies\nYou can customize how errors are handled using the handle_errors parameter:\nCustom error message:\nCopy\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=\"Please provide a valid rating between 1-5 and include a comment.\"\n)\n\nIf handle_errors is a string, the agent will always prompt the model to re-try with a fixed tool message:\nCopy\n================================= Tool Message =================================\nName: ProductRating\n\nPlease provide a valid rating between 1-5 and include a comment.\n\nHandle specific exceptions only:\nCopy\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=ValueError  # Only retry on ValueError, raise others\n)\n\nIf handle_errors is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.\nHandle multiple exception types:\nCopy\nToolStrategy(\n    schema=ProductRating,\n    handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError\n)\n\nIf handle_errors is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised.\nCustom error handler function:\nCopy\n\nfrom langchain.agents.structured_output import StructuredOutputValidationError\nfrom langchain.agents.structured_output import MultipleStructuredOutputsError\n\ndef custom_error_handler(error: Exception) -> str:\n    if isinstance(error, StructuredOutputValidationError):\n        return \"There was an issue with the format. Try again.\"\n    elif isinstance(error, MultipleStructuredOutputsError):\n        return \"Multiple structured outputs were returned. Pick the most relevant one.\"\n    else:\n        return f\"Error: {str(error)}\"\n\n\nagent = create_agent(\n    model=\"gpt-5\",\n    tools=[],\n    response_format=ToolStrategy(\n                        schema=Union[ContactInfo, EventDetails],\n                        handle_errors=custom_error_handler\n                    )  # Default: handle_errors=True\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}]\n})\n\nfor msg in result['messages']:\n    # If message is actually a ToolMessage object (not a dict), check its class name\n    if type(msg).__name__ == \"ToolMessage\":\n        print(msg.content)\n    # If message is a dictionary or you want a fallback\n    elif isinstance(msg, dict) and msg.get('tool_call_id'):\n        print(msg['content'])\n\n\nOn StructuredOutputValidationError:\nCopy\n================================= Tool Message =================================\nName: ToolStrategy\n\nThere was an issue with the format. Try again.\n\nOn MultipleStructuredOutputsError:\nCopy\n================================= Tool Message =================================\nName: ToolStrategy\n\nMultiple structured outputs were returned. Pick the most relevant one.\n\nOn other errors:\nCopy\n================================= Tool Message =================================\nName: ToolStrategy\n\nError: <error message>\n\nNo error handling:\nCopy\nresponse_format = ToolStrategy(\n    schema=ProductRating,\n    handle_errors=False  # All errors raised\n)\n\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494358,
    "timestamp": "2026-02-25T08:09:37.377Z",
    "title": "Overview - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/middleware/overview",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nThe agent loop\nAdditional resources\nMiddleware\nOverview\nCopy page\n\nControl and customize agent execution at every step\n\nMiddleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:\nTracking agent behavior with logging, analytics, and debugging.\nTransforming prompts, tool selection, and output formatting.\nAdding retries, fallbacks, and early termination logic.\nApplying rate limits, guardrails, and PII detection.\nAdd middleware by passing them to create_agent:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[...],\n    middleware=[\n        SummarizationMiddleware(...),\n        HumanInTheLoopMiddleware(...)\n    ],\n)\n\nï¿½\nThe agent loop\nThe core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\nMiddleware exposes hooks before and after each of those steps:\nï¿½\nAdditional resources\nBuilt-in middleware\nExplore built-in middleware for common use cases.\nCustom middleware\nBuild your own middleware with hooks and decorators.\nMiddleware API reference\nComplete API reference for middleware.\nTesting agents\nTest your agents with LangSmith.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494359,
    "timestamp": "2026-02-25T08:09:37.384Z",
    "title": "Prebuilt middleware - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/middleware/built-in",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nProvider-agnostic middleware\nSummarization\nHuman-in-the-loop\nModel call limit\nTool call limit\nModel fallback\nPII detection\nCustom PII types\nTo-do list\nLLM tool selector\nTool retry\nModel retry\nLLM tool emulator\nContext editing\nShell tool\nFile search\nFilesystem middleware\nShort-term vs. long-term filesystem\nSubagent\nProvider-specific middleware\nMiddleware\nPrebuilt middleware\nCopy page\n\nPrebuilt middleware for common agent use cases\n\nLangChain and Deep Agents provide prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\nï¿½\nProvider-agnostic middleware\nThe following middleware work with any LLM provider:\nMiddleware\tDescription\nSummarization\tAutomatically summarize conversation history when approaching token limits.\nHuman-in-the-loop\tPause execution for human approval of tool calls.\nModel call limit\tLimit the number of model calls to prevent excessive costs.\nTool call limit\tControl tool execution by limiting call counts.\nModel fallback\tAutomatically fallback to alternative models when primary fails.\nPII detection\tDetect and handle Personally Identifiable Information (PII).\nTo-do list\tEquip agents with task planning and tracking capabilities.\nLLM tool selector\tUse an LLM to select relevant tools before calling main model.\nTool retry\tAutomatically retry failed tool calls with exponential backoff.\nModel retry\tAutomatically retry failed model calls with exponential backoff.\nLLM tool emulator\tEmulate tool execution using an LLM for testing purposes.\nContext editing\tManage conversation context by trimming or clearing tool uses.\nShell tool\tExpose a persistent shell session to agents for command execution.\nFile search\tProvide Glob and Grep search tools over filesystem files.\nFilesystem\tProvide agents with a filesystem for storing context and long-term memories.\nSubagent\tAdd the ability to spawn subagents.\nï¿½\nSummarization\nAutomatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following:\nLong-running conversations that exceed context windows.\nMulti-turn dialogues with extensive history.\nApplications where preserving full conversation context matters.\nAPI reference: SummarizationMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[your_weather_tool, your_calculator_tool],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 20),\n        ),\n    ],\n)\n\n\nConfiguration options\n\nFull example\n\nï¿½\nHuman-in-the-loop\nPause agent execution for human approval, editing, or rejection of tool calls before they execute. Human-in-the-loop is useful for the following:\nHigh-stakes operations requiring human approval (e.g. database writes, financial transactions).\nCompliance workflows where human oversight is mandatory.\nLong-running conversations where human feedback guides the agent.\nAPI reference: HumanInTheLoopMiddleware\nHuman-in-the-loop middleware requires a checkpointer to maintain state across interruptions.\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\ndef read_email_tool(email_id: str) -> str:\n    \"\"\"Mock function to read an email by its ID.\"\"\"\n    return f\"Email content for ID: {email_id}\"\n\ndef send_email_tool(recipient: str, subject: str, body: str) -> str:\n    \"\"\"Mock function to send an email.\"\"\"\n    return f\"Email sent to {recipient} with subject '{subject}'\"\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[your_read_email_tool, your_send_email_tool],\n    checkpointer=InMemorySaver(),\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                \"your_send_email_tool\": {\n                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n                },\n                \"your_read_email_tool\": False,\n            }\n        ),\n    ],\n)\n\nFor complete examples, configuration options, and integration patterns, see the Human-in-the-loop documentation.\nWatch this video guide demonstrating Human-in-the-loop middleware behavior.\nï¿½\nModel call limit\nLimit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following:\nPreventing runaway agents from making too many API calls.\nEnforcing cost controls on production deployments.\nTesting agent behavior within specific call budgets.\nAPI reference: ModelCallLimitMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelCallLimitMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    checkpointer=InMemorySaver(),  # Required for thread limiting\n    tools=[],\n    middleware=[\n        ModelCallLimitMiddleware(\n            thread_limit=10,\n            run_limit=5,\n            exit_behavior=\"end\",\n        ),\n    ],\n)\n\nWatch this video guide demonstrating Model Call Limit middleware behavior.\n\nConfiguration options\n\nï¿½\nTool call limit\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following:\nPreventing excessive calls to expensive external APIs.\nLimiting web searches or database queries.\nEnforcing rate limits on specific tool usage.\nProtecting against runaway agent loops.\nAPI reference: ToolCallLimitMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolCallLimitMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        # Global limit\n        ToolCallLimitMiddleware(thread_limit=20, run_limit=10),\n        # Tool-specific limit\n        ToolCallLimitMiddleware(\n            tool_name=\"search\",\n            thread_limit=5,\n            run_limit=3,\n        ),\n    ],\n)\n\nWatch this video guide demonstrating Tool Call Limit middleware behavior.\n\nConfiguration options\n\nFull example\n\nï¿½\nModel fallback\nAutomatically fallback to alternative models when the primary model fails. Model fallback is useful for the following:\nBuilding resilient agents that handle model outages.\nCost optimization by falling back to cheaper models.\nProvider redundancy across OpenAI, Anthropic, etc.\nAPI reference: ModelFallbackMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelFallbackMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        ModelFallbackMiddleware(\n            \"gpt-4.1-mini\",\n            \"claude-3-5-sonnet-20241022\",\n        ),\n    ],\n)\n\nWatch this video guide demonstrating Model Fallback middleware behavior.\n\nConfiguration options\n\nï¿½\nPII detection\nDetect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following:\nHealthcare and financial applications with compliance requirements.\nCustomer service agents that need to sanitize logs.\nAny application handling sensitive user data.\nAPI reference: PIIMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n    ],\n)\n\nï¿½\nCustom PII types\nYou can create custom PII types by providing a detector parameter. This allows you to detect patterns specific to your use case beyond the built-in types.\nThree ways to create custom detectors:\nRegex pattern string - Simple pattern matching\nCustom function - Complex detection logic with validation\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\nimport re\n\n\n# Method 1: Regex pattern string\nagent1 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"api_key\",\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\n            strategy=\"block\",\n        ),\n    ],\n)\n\n# Method 2: Compiled regex pattern\nagent2 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"phone_number\",\n            detector=re.compile(r\"\\+?\\d{1,3}[\\s.-]?\\d{3,4}[\\s.-]?\\d{4}\"),\n            strategy=\"mask\",\n        ),\n    ],\n)\n\n# Method 3: Custom detector function\ndef detect_ssn(content: str) -> list[dict[str, str | int]]:\n    \"\"\"Detect SSN with validation.\n\n    Returns a list of dictionaries with 'text', 'start', and 'end' keys.\n    \"\"\"\n    import re\n    matches = []\n    pattern = r\"\\d{3}-\\d{2}-\\d{4}\"\n    for match in re.finditer(pattern, content):\n        ssn = match.group(0)\n        # Validate: first 3 digits shouldn't be 000, 666, or 900-999\n        first_three = int(ssn[:3])\n        if first_three not in [0, 666] and not (900 <= first_three <= 999):\n            matches.append({\n                \"text\": ssn,\n                \"start\": match.start(),\n                \"end\": match.end(),\n            })\n    return matches\n\nagent3 = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        PIIMiddleware(\n            \"ssn\",\n            detector=detect_ssn,\n            strategy=\"hash\",\n        ),\n    ],\n)\n\nCustom detector function signature:\nThe detector function must accept a string (content) and return matches:\nReturns a list of dictionaries with text, start, and end keys:\nCopy\ndef detector(content: str) -> list[dict[str, str | int]]:\n    return [\n        {\"text\": \"matched_text\", \"start\": 0, \"end\": 12},\n        # ... more matches\n    ]\n\nFor custom detectors:\nUse regex strings for simple patterns\nUse RegExp objects when you need flags (e.g., case-insensitive matching)\nUse custom functions when you need validation logic beyond pattern matching\nCustom functions give you full control over detection logic and can implement complex validation rules\n\nConfiguration options\n\nï¿½\nTo-do list\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:\nComplex multi-step tasks requiring coordination across multiple tools.\nLong-running operations where progress visibility is important.\nThis middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning.\nAPI reference: TodoListMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import TodoListMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[read_file, write_file, run_tests],\n    middleware=[TodoListMiddleware()],\n)\n\nWatch this video guide demonstrating To-do List middleware behavior.\n\nConfiguration options\n\nï¿½\nLLM tool selector\nUse an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following:\nAgents with many tools (10+) where most arenâ€™t relevant per query.\nReducing token usage by filtering irrelevant tools.\nImproving model focus and accuracy.\nThis middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines the available tool names and descriptions. Model providers often add this structured output information to the system prompt behind the scenes.\nAPI reference: LLMToolSelectorMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolSelectorMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[tool1, tool2, tool3, tool4, tool5, ...],\n    middleware=[\n        LLMToolSelectorMiddleware(\n            model=\"gpt-4.1-mini\",\n            max_tools=3,\n            always_include=[\"search\"],\n        ),\n    ],\n)\n\n\nConfiguration options\n\nï¿½\nTool retry\nAutomatically retry failed tool calls with configurable exponential backoff. Tool retry is useful for the following:\nHandling transient failures in external API calls.\nImproving reliability of network-dependent tools.\nBuilding resilient agents that gracefully handle temporary errors.\nAPI reference: ToolRetryMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ToolRetryMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        ToolRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n        ),\n    ],\n)\n\n\nConfiguration options\n\nFull example\n\nï¿½\nModel retry\nAutomatically retry failed model calls with configurable exponential backoff. Model retry is useful for the following:\nHandling transient failures in model API calls.\nImproving reliability of network-dependent model requests.\nBuilding resilient agents that gracefully handle temporary model errors.\nAPI reference: ModelRetryMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ModelRetryMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, database_tool],\n    middleware=[\n        ModelRetryMiddleware(\n            max_retries=3,\n            backoff_factor=2.0,\n            initial_delay=1.0,\n        ),\n    ],\n)\n\n\nConfiguration options\n\nFull example\n\nï¿½\nLLM tool emulator\nEmulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses. LLM tool emulators are useful for the following:\nTesting agent behavior without executing real tools.\nDeveloping agents when external tools are unavailable or expensive.\nPrototyping agent workflows before implementing actual tools.\nAPI reference: LLMToolEmulator\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import LLMToolEmulator\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[get_weather, search_database, send_email],\n    middleware=[\n        LLMToolEmulator(),  # Emulate all tools\n    ],\n)\n\n\nConfiguration options\n\nFull example\n\nï¿½\nContext editing\nManage conversation context by clearing older tool call outputs when token limits are reached, while preserving recent results. This helps keep context windows manageable in long conversations with many tool calls. Context editing is useful for the following:\nLong conversations with many tool calls that exceed token limits\nReducing token costs by removing older tool outputs that are no longer relevant\nMaintaining only the most recent N tool results in context\nAPI reference: ContextEditingMiddleware, ClearToolUsesEdit\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        ContextEditingMiddleware(\n            edits=[\n                ClearToolUsesEdit(\n                    trigger=100000,\n                    keep=3,\n                ),\n            ],\n        ),\n    ],\n)\n\n\nConfiguration options\n\nFull example\n\nï¿½\nShell tool\nExpose a persistent shell session to agents for command execution. Shell tool middleware is useful for the following:\nAgents that need to execute system commands\nDevelopment and deployment automation tasks\nTesting and validation workflows\nFile system operations and script execution\nSecurity consideration: Use appropriate execution policies (HostExecutionPolicy, DockerExecutionPolicy, or CodexSandboxExecutionPolicy) to match your deploymentâ€™s security requirements.\nLimitation: Persistent shell sessions do not currently work with interrupts (human-in-the-loop). We anticipate adding support for this in the future.\nAPI reference: ShellToolMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    ShellToolMiddleware,\n    HostExecutionPolicy,\n)\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool],\n    middleware=[\n        ShellToolMiddleware(\n            workspace_root=\"/workspace\",\n            execution_policy=HostExecutionPolicy(),\n        ),\n    ],\n)\n\n\nConfiguration options\n\nFull example\n\nï¿½\nFile search\nProvide Glob and Grep search tools over a filesystem. File search middleware is useful for the following:\nCode exploration and analysis\nFinding files by name patterns\nSearching code content with regex\nLarge codebases where file discovery is needed\nAPI reference: FilesystemFileSearchMiddleware\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import FilesystemFileSearchMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[],\n    middleware=[\n        FilesystemFileSearchMiddleware(\n            root_path=\"/workspace\",\n            use_ripgrep=True,\n        ),\n    ],\n)\n\n\nConfiguration options\n\nFull example\n\nï¿½\nFilesystem middleware\nContext engineering is a main challenge in building effective agents. This is particularly difficult when using tools that return variable-length results (for example, web_search and RAG), as long tool results can quickly fill your context window.\nFilesystemMiddleware from deep agents provides four tools for interacting with both short-term and long-term memory:\nls: List the files in the filesystem\nread_file: Read an entire file or a certain number of lines from a file\nwrite_file: Write a new file to the filesystem\nedit_file: Edit an existing file in the filesystem\nCopy\nfrom langchain.agents import create_agent\nfrom deepagents.middleware.filesystem import FilesystemMiddleware\n\n# FilesystemMiddleware is included by default in create_deep_agent\n# You can customize it if building a custom agent\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    middleware=[\n        FilesystemMiddleware(\n            backend=None,  # Optional: custom backend (defaults to StateBackend)\n            system_prompt=\"Write to the filesystem when...\",  # Optional custom addition to the system prompt\n            custom_tool_descriptions={\n                \"ls\": \"Use the ls tool when...\",\n                \"read_file\": \"Use the read_file tool to...\"\n            }  # Optional: Custom descriptions for filesystem tools\n        ),\n    ],\n)\n\nï¿½\nShort-term vs. long-term filesystem\nBy default, these tools write to a local â€œfilesystemâ€ in your graph state. To enable persistent storage across threads, configure a CompositeBackend that routes specific paths (like /memories/) to a StoreBackend.\nCopy\nfrom langchain.agents import create_agent\nfrom deepagents.middleware import FilesystemMiddleware\nfrom deepagents.backends import CompositeBackend, StateBackend, StoreBackend\nfrom langgraph.store.memory import InMemoryStore\n\nstore = InMemoryStore()\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    store=store,\n    middleware=[\n        FilesystemMiddleware(\n            backend=lambda rt: CompositeBackend(\n                default=StateBackend(rt),\n                routes={\"/memories/\": StoreBackend(rt)}\n            ),\n            custom_tool_descriptions={\n                \"ls\": \"Use the ls tool when...\",\n                \"read_file\": \"Use the read_file tool to...\"\n            }  # Optional: Custom descriptions for filesystem tools\n        ),\n    ],\n)\n\nWhen you configure a CompositeBackend with a StoreBackend for /memories/, any files prefixed with /memories/ are saved to persistent storage and survive across different threads. Files without this prefix remain in ephemeral state storage.\nï¿½\nSubagent\nHanding off tasks to subagents isolates context, keeping the main (supervisor) agentâ€™s context window clean while still going deep on a task.\nThe subagents middleware from deep agents allows you to supply subagents through a task tool.\nCopy\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\nfrom deepagents.middleware.subagents import SubAgentMiddleware\n\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather in a city.\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    middleware=[\n        SubAgentMiddleware(\n            default_model=\"claude-sonnet-4-5-20250929\",\n            default_tools=[],\n            subagents=[\n                {\n                    \"name\": \"weather\",\n                    \"description\": \"This subagent can get weather in cities.\",\n                    \"system_prompt\": \"Use the get_weather tool to get the weather in a city.\",\n                    \"tools\": [get_weather],\n                    \"model\": \"gpt-4.1\",\n                    \"middleware\": [],\n                }\n            ],\n        )\n    ],\n)\n\nA subagent is defined with a name, description, system prompt, and tools. You can also provide a subagent with a custom model, or with additional middleware. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.\nFor more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.\nCopy\nfrom langchain.agents import create_agent\nfrom deepagents.middleware.subagents import SubAgentMiddleware\nfrom deepagents import CompiledSubAgent\nfrom langgraph.graph import StateGraph\n\n# Create a custom LangGraph graph\ndef create_weather_graph():\n    workflow = StateGraph(...)\n    # Build your custom graph\n    return workflow.compile()\n\nweather_graph = create_weather_graph()\n\n# Wrap it in a CompiledSubAgent\nweather_subagent = CompiledSubAgent(\n    name=\"weather\",\n    description=\"This subagent can get weather in cities.\",\n    runnable=weather_graph\n)\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    middleware=[\n        SubAgentMiddleware(\n            default_model=\"claude-sonnet-4-5-20250929\",\n            default_tools=[],\n            subagents=[weather_subagent],\n        )\n    ],\n)\n\nIn addition to any user-defined subagents, the main agent has access to a general-purpose subagent at all times. This subagent has the same instructions as the main agent and all the tools it has access to. The primary purpose of the general-purpose subagent is context isolationâ€”the main agent can delegate a complex task to this subagent and get a concise answer back without bloat from intermediate tool calls.\nï¿½\nProvider-specific middleware\nThese middleware are optimized for specific LLM providers. See each providerâ€™s documentation for full details and examples.\nAnthropic\nPrompt caching, bash tool, text editor, memory, and file search middleware for Claude models.\nOpenAI\nContent moderation middleware for OpenAI models.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494360,
    "timestamp": "2026-02-25T08:09:37.385Z",
    "title": "Custom middleware - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/middleware/custom",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nHooks\nNode-style hooks\nWrap-style hooks\nCreate middleware\nDecorator-based middleware\nClass-based middleware\nCustom state schema\nExecution order\nAgent jumps\nBest practices\nExamples\nDynamic model selection\nTool call monitoring\nDynamically selecting tools\nWorking with system messages\nAdditional resources\nMiddleware\nCustom middleware\nCopy page\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\nï¿½\nHooks\nMiddleware provides two styles of hooks to intercept agent execution:\nNode-style hooks\nRun sequentially at specific execution points.\nWrap-style hooks\nRun around each model or tool call.\nï¿½\nNode-style hooks\nRun sequentially at specific execution points. Use for logging, validation, and state updates.\nAvailable hooks:\nbefore_agent - Before agent starts (once per invocation)\nbefore_model - Before each model call\nafter_model - After each model response\nafter_agent - After agent completes (once per invocation)\nExample:\nDecorator\nClass\nCopy\nfrom langchain.agents.middleware import before_model, after_model, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\n\n@before_model(can_jump_to=[\"end\"])\ndef check_message_limit(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    if len(state[\"messages\"]) >= 50:\n        return {\n            \"messages\": [AIMessage(\"Conversation limit reached.\")],\n            \"jump_to\": \"end\"\n        }\n    return None\n\n@after_model\ndef log_response(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    print(f\"Model returned: {state['messages'][-1].content}\")\n    return None\n\nï¿½\nWrap-style hooks\nIntercept execution and control when the handler is called. Use for retries, caching, and transformation.\nYou decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\nAvailable hooks:\nwrap_model_call - Around each model call\nwrap_tool_call - Around each tool call\nExample:\nDecorator\nClass\nCopy\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n\n@wrap_model_call\ndef retry_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    for attempt in range(3):\n        try:\n            return handler(request)\n        except Exception as e:\n            if attempt == 2:\n                raise\n            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n\nï¿½\nCreate middleware\nYou can create middleware in two ways:\nDecorator-based middleware\nQuick and simple for single-hook middleware. Use decorators to wrap individual functions.\nClass-based middleware\nMore powerful for complex middleware with multiple hooks or configuration.\nï¿½\nDecorator-based middleware\nQuick and simple for single-hook middleware. Use decorators to wrap individual functions.\nAvailable decorators:\nNode-style:\n@before_agent - Runs before agent starts (once per invocation)\n@before_model - Runs before each model call\n@after_model - Runs after each model response\n@after_agent - Runs after agent completes (once per invocation)\nWrap-style:\n@wrap_model_call - Wraps each model call with custom logic\n@wrap_tool_call - Wraps each tool call with custom logic\nConvenience:\n@dynamic_prompt - Generates dynamic system prompts\nExample:\nCopy\nfrom langchain.agents.middleware import (\n    before_model,\n    wrap_model_call,\n    AgentState,\n    ModelRequest,\n    ModelResponse,\n)\nfrom langchain.agents import create_agent\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\n\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    print(f\"About to call model with {len(state['messages'])} messages\")\n    return None\n\n@wrap_model_call\ndef retry_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    for attempt in range(3):\n        try:\n            return handler(request)\n        except Exception as e:\n            if attempt == 2:\n                raise\n            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    middleware=[log_before_model, retry_model],\n    tools=[...],\n)\n\nWhen to use decorators:\nSingle hook needed\nNo complex configuration\nQuick prototyping\nï¿½\nClass-based middleware\nMore powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.\nExample:\nCopy\nfrom langchain.agents.middleware import (\n    AgentMiddleware,\n    AgentState,\n    ModelRequest,\n    ModelResponse,\n)\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n\nclass LoggingMiddleware(AgentMiddleware):\n    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"About to call model with {len(state['messages'])} messages\")\n        return None\n\n    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        print(f\"Model returned: {state['messages'][-1].content}\")\n        return None\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    middleware=[LoggingMiddleware()],\n    tools=[...],\n)\n\nWhen to use classes:\nDefining both sync and async implementations for the same hook\nMultiple hooks needed in a single middleware\nComplex configuration required (e.g., configurable thresholds, custom models)\nReuse across projects with init-time configuration\nï¿½\nCustom state schema\nMiddleware can extend the agentâ€™s state with custom properties. This enables middleware to:\nTrack state across execution: Maintain counters, flags, or other values that persist throughout the agentâ€™s execution lifecycle\nShare data between hooks: Pass information from before_model to after_model or between different middleware instances\nImplement cross-cutting concerns: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic\nMake conditional decisions: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically\nDecorator\nClass\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langchain.agents.middleware import AgentState, before_model, after_model\nfrom typing_extensions import NotRequired\nfrom typing import Any\nfrom langgraph.runtime import Runtime\n\n\nclass CustomState(AgentState):\n    model_call_count: NotRequired[int]\n    user_id: NotRequired[str]\n\n\n@before_model(state_schema=CustomState, can_jump_to=[\"end\"])\ndef check_call_limit(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n    count = state.get(\"model_call_count\", 0)\n    if count > 10:\n        return {\"jump_to\": \"end\"}\n    return None\n\n\n@after_model(state_schema=CustomState)\ndef increment_counter(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\n    return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    middleware=[check_call_limit, increment_counter],\n    tools=[],\n)\n\n# Invoke with custom state\nresult = agent.invoke({\n    \"messages\": [HumanMessage(\"Hello\")],\n    \"model_call_count\": 0,\n    \"user_id\": \"user-123\",\n})\n\nï¿½\nExecution order\nWhen using multiple middleware, understand how they execute:\nCopy\nagent = create_agent(\n    model=\"gpt-4.1\",\n    middleware=[middleware1, middleware2, middleware3],\n    tools=[...],\n)\n\n\nExecution flow\n\nKey rules:\nbefore_* hooks: First to last\nafter_* hooks: Last to first (reverse)\nwrap_* hooks: Nested (first middleware wraps all others)\nï¿½\nAgent jumps\nTo exit early from middleware, return a dictionary with jump_to:\nAvailable jump targets:\n'end': Jump to the end of the agent execution (or the first after_agent hook)\n'tools': Jump to the tools node\n'model': Jump to the model node (or the first before_model hook)\nDecorator\nClass\nCopy\nfrom langchain.agents.middleware import after_model, hook_config, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n\n\n@after_model\n@hook_config(can_jump_to=[\"end\"])\ndef check_for_blocked(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n    last_message = state[\"messages\"][-1]\n    if \"BLOCKED\" in last_message.content:\n        return {\n            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n            \"jump_to\": \"end\"\n        }\n    return None\n\nï¿½\nBest practices\nKeep middleware focused - each should do one thing well\nHandle errors gracefully - donâ€™t let middleware errors crash the agent\nUse appropriate hook types:\nNode-style for sequential logic (logging, validation)\nWrap-style for control flow (retry, fallback, caching)\nClearly document any custom state properties\nUnit test middleware independently before integrating\nConsider execution order - place critical middleware first in the list\nUse built-in middleware when possible\nï¿½\nExamples\nï¿½\nDynamic model selection\nDecorator\nClass\nCopy\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.chat_models import init_chat_model\nfrom typing import Callable\n\n\ncomplex_model = init_chat_model(\"gpt-4.1\")\nsimple_model = init_chat_model(\"gpt-4.1-mini\")\n\n@wrap_model_call\ndef dynamic_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    # Use different model based on conversation length\n    if len(request.messages) > 10:\n        model = complex_model\n    else:\n        model = simple_model\n    return handler(request.override(model=model))\n\nï¿½\nTool call monitoring\nDecorator\nClass\nCopy\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain.tools.tool_node import ToolCallRequest\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\nfrom typing import Callable\n\n\n@wrap_tool_call\ndef monitor_tool(\n    request: ToolCallRequest,\n    handler: Callable[[ToolCallRequest], ToolMessage | Command],\n) -> ToolMessage | Command:\n    print(f\"Executing tool: {request.tool_call['name']}\")\n    print(f\"Arguments: {request.tool_call['args']}\")\n    try:\n        result = handler(request)\n        print(f\"Tool completed successfully\")\n        return result\n    except Exception as e:\n        print(f\"Tool failed: {e}\")\n        raise\n\nï¿½\nDynamically selecting tools\nSelect relevant tools at runtime to improve performance and accuracy. This section covers filtering pre-registered tools. For registering tools that are discovered at runtime (e.g., from MCP servers), see Runtime tool registration.\nBenefits:\nShorter prompts - Reduce complexity by exposing only relevant tools\nBetter accuracy - Models choose correctly from fewer options\nPermission control - Dynamically filter tools based on user access\nDecorator\nClass\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n\n@wrap_model_call\ndef select_tools(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n    # Select a small, relevant subset of tools based on state/context\n    relevant_tools = select_relevant_tools(request.state, request.runtime)\n    return handler(request.override(tools=relevant_tools))\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=all_tools,  # All available tools need to be registered upfront\n    middleware=[select_tools],\n)\n\nï¿½\nWorking with system messages\nModify system messages in middleware using the system_message field on ModelRequest. The system_message field contains a SystemMessage object (even if the agent was created with a string system_prompt).\nExample: Adding context to system message\nDecorator\nClass\nCopy\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n\n\n@wrap_model_call\ndef add_context(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    # Always work with content blocks\n    new_content = list(request.system_message.content_blocks) + [\n        {\"type\": \"text\", \"text\": \"Additional context.\"}\n    ]\n    new_system_message = SystemMessage(content=new_content)\n    return handler(request.override(system_message=new_system_message))\n\nExample: Working with cache control (Anthropic)\nWhen working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:\nDecorator\nClass\nCopy\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n\n\n@wrap_model_call\ndef add_cached_context(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    # Always work with content blocks\n    new_content = list(request.system_message.content_blocks) + [\n        {\n            \"type\": \"text\",\n            \"text\": \"Here is a large document to analyze:\\n\\n<document>...</document>\",\n            # content up until this point is cached\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ]\n\n    new_system_message = SystemMessage(content=new_content)\n    return handler(request.override(system_message=new_system_message))\n\nNotes:\nModelRequest.system_message is always a SystemMessage object, even if the agent was created with system_prompt=\"string\"\nUse SystemMessage.content_blocks to access content as a list of blocks, regardless of whether the original content was a string or list\nWhen modifying system messages, use content_blocks and append new blocks to preserve existing structure\nYou can pass SystemMessage objects directly to create_agentâ€™s system_prompt parameter for advanced use cases like cache control\nï¿½\nAdditional resources\nMiddleware API reference\nBuilt-in middleware\nTesting agents\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494361,
    "timestamp": "2026-02-25T08:09:37.390Z",
    "title": "Guardrails - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/guardrails",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nBuilt-in guardrails\nPII detection\nHuman-in-the-loop\nCustom guardrails\nBefore agent guardrails\nAfter agent guardrails\nCombine multiple guardrails\nAdditional resources\nAdvanced usage\nGuardrails\nCopy page\n\nImplement safety checks and content filtering for your agents\n\nGuardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agentâ€™s execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.\nCommon use cases include:\nPreventing PII leakage\nDetecting and blocking prompt injection attacks\nBlocking inappropriate or harmful content\nEnforcing business rules and compliance requirements\nValidating output quality and accuracy\nYou can implement guardrails using middleware to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.\nGuardrails can be implemented using two complementary approaches:\nDeterministic guardrails\nUse rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.\nModel-based guardrails\nUse LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.\nLangChain provides both built-in guardrails (e.g., PII detection, human-in-the-loop) and a flexible middleware system for building custom guardrails using either approach.\nï¿½\nBuilt-in guardrails\nï¿½\nPII detection\nLangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.\nPII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.\nThe PII middleware supports multiple strategies for handling detected PII:\nStrategy\tDescription\tExample\nredact\tReplace with [REDACTED_{PII_TYPE}]\t[REDACTED_EMAIL]\nmask\tPartially obscure (e.g., last 4 digits)\t****-****-****-1234\nhash\tReplace with deterministic hash\ta8f5f167...\nblock\tRaise exception when detected\tError thrown\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[customer_service_tool, email_tool],\n    middleware=[\n        # Redact emails in user input before sending to model\n        PIIMiddleware(\n            \"email\",\n            strategy=\"redact\",\n            apply_to_input=True,\n        ),\n        # Mask credit cards in user input\n        PIIMiddleware(\n            \"credit_card\",\n            strategy=\"mask\",\n            apply_to_input=True,\n        ),\n        # Block API keys - raise error if detected\n        PIIMiddleware(\n            \"api_key\",\n            detector=r\"sk-[a-zA-Z0-9]{32}\",\n            strategy=\"block\",\n            apply_to_input=True,\n        ),\n    ],\n)\n\n# When user provides PII, it will be handled according to the strategy\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"My email is john.doe@example.com and card is 5105-1051-0510-5100\"}]\n})\n\n\nBuilt-in PII types and configuration\n\nSee the middleware documentation for complete details on PII detection capabilities.\nï¿½\nHuman-in-the-loop\nLangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.\nHuman-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import Command\n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, send_email_tool, delete_database_tool],\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                # Require approval for sensitive operations\n                \"send_email\": True,\n                \"delete_database\": True,\n                # Auto-approve safe operations\n                \"search\": False,\n            }\n        ),\n    ],\n    # Persist the state across interrupts\n    checkpointer=InMemorySaver(),\n)\n\n# Human-in-the-loop requires a thread ID for persistence\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\n\n# Agent will pause and wait for approval before executing sensitive tools\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]},\n    config=config\n)\n\nresult = agent.invoke(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n    config=config  # Same thread ID to resume the paused conversation\n)\n\nSee the human-in-the-loop documentation for complete details on implementing approval workflows.\nï¿½\nCustom guardrails\nFor more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.\nï¿½\nBefore agent guardrails\nUse â€œbefore agentâ€ hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.\nClass syntax\nDecorator syntax\nCopy\nfrom typing import Any\n\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langgraph.runtime import Runtime\n\nclass ContentFilterMiddleware(AgentMiddleware):\n    \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\"\n\n    def __init__(self, banned_keywords: list[str]):\n        super().__init__()\n        self.banned_keywords = [kw.lower() for kw in banned_keywords]\n\n    @hook_config(can_jump_to=[\"end\"])\n    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        # Get the first user message\n        if not state[\"messages\"]:\n            return None\n\n        first_message = state[\"messages\"][0]\n        if first_message.type != \"human\":\n            return None\n\n        content = first_message.content.lower()\n\n        # Check for banned keywords\n        for keyword in self.banned_keywords:\n            if keyword in content:\n                # Block execution before any processing\n                return {\n                    \"messages\": [{\n                        \"role\": \"assistant\",\n                        \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\"\n                    }],\n                    \"jump_to\": \"end\"\n                }\n\n        return None\n\n# Use the custom guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, calculator_tool],\n    middleware=[\n        ContentFilterMiddleware(\n            banned_keywords=[\"hack\", \"exploit\", \"malware\"]\n        ),\n    ],\n)\n\n# This request will be blocked before any processing\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n})\n\nï¿½\nAfter agent guardrails\nUse â€œafter agentâ€ hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.\nClass syntax\nDecorator syntax\nCopy\nfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langgraph.runtime import Runtime\nfrom langchain.messages import AIMessage\nfrom langchain.chat_models import init_chat_model\nfrom typing import Any\n\nclass SafetyGuardrailMiddleware(AgentMiddleware):\n    \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.safety_model = init_chat_model(\"gpt-4.1-mini\")\n\n    @hook_config(can_jump_to=[\"end\"])\n    def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n        # Get the final AI response\n        if not state[\"messages\"]:\n            return None\n\n        last_message = state[\"messages\"][-1]\n        if not isinstance(last_message, AIMessage):\n            return None\n\n        # Use a model to evaluate safety\n        safety_prompt = f\"\"\"Evaluate if this response is safe and appropriate.\n        Respond with only 'SAFE' or 'UNSAFE'.\n\n        Response: {last_message.content}\"\"\"\n\n        result = self.safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\n\n        if \"UNSAFE\" in result.content:\n            last_message.content = \"I cannot provide that response. Please rephrase your request.\"\n\n        return None\n\n# Use the safety guardrail\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, calculator_tool],\n    middleware=[SafetyGuardrailMiddleware()],\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}]\n})\n\nï¿½\nCombine multiple guardrails\nYou can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[search_tool, send_email_tool],\n    middleware=[\n        # Layer 1: Deterministic input filter (before agent)\n        ContentFilterMiddleware(banned_keywords=[\"hack\", \"exploit\"]),\n\n        # Layer 2: PII protection (before and after model)\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_output=True),\n\n        # Layer 3: Human approval for sensitive tools\n        HumanInTheLoopMiddleware(interrupt_on={\"send_email\": True}),\n\n        # Layer 4: Model-based safety check (after agent)\n        SafetyGuardrailMiddleware(),\n    ],\n)\n\nï¿½\nAdditional resources\nMiddleware documentation - Complete guide to custom middleware\nMiddleware API reference - Complete guide to custom middleware\nHuman-in-the-loop - Add human review for sensitive operations\nTesting agents - Strategies for testing safety mechanisms\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494362,
    "timestamp": "2026-02-25T08:09:37.397Z",
    "title": "Runtime - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/runtime",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nAccess\nInside tools\nInside middleware\nAdvanced usage\nRuntime\nCopy page\nï¿½\nOverview\nLangChainâ€™s create_agent runs on LangGraphâ€™s runtime under the hood.\nLangGraph exposes a Runtime object with the following information:\nContext: static information like user id, db connections, or other dependencies for an agent invocation\nStore: a BaseStore instance used for long-term memory\nStream writer: an object used for streaming information via the \"custom\" stream mode\nRuntime context provides dependency injection for your tools and middleware. Instead of hardcoding values or using global state, you can inject runtime dependencies (like database connections, user IDs, or configuration) when invoking your agent. This makes your tools more testable, reusable, and flexible.\nYou can access the runtime information within tools and middleware.\nï¿½\nAccess\nWhen creating an agent with create_agent, you can specify a context_schema to define the structure of the context stored in the agent Runtime.\nWhen invoking the agent, pass the context argument with the relevant configuration for the run:\nCopy\nfrom dataclasses import dataclass\n\nfrom langchain.agents import create_agent\n\n\n@dataclass\nclass Context:\n    user_name: str\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    context_schema=Context  \n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")  \n)\n\nï¿½\nInside tools\nYou can access the runtime information inside tools to:\nAccess the context\nRead or write long-term memory\nWrite to the custom stream (ex, tool progress / updates)\nUse the ToolRuntime parameter to access the Runtime object inside a tool.\nCopy\nfrom dataclasses import dataclass\nfrom langchain.tools import tool, ToolRuntime  \n\n@dataclass\nclass Context:\n    user_id: str\n\n@tool\ndef fetch_user_email_preferences(runtime: ToolRuntime[Context]) -> str:  \n    \"\"\"Fetch the user's email preferences from the store.\"\"\"\n    user_id = runtime.context.user_id  \n\n    preferences: str = \"The user prefers you to write a brief and polite email.\"\n    if runtime.store:  \n        if memory := runtime.store.get((\"users\",), user_id):  \n            preferences = memory.value[\"preferences\"]\n\n    return preferences\n\nï¿½\nInside middleware\nYou can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.\nUse the Runtime parameter to access the Runtime object inside node-style hooks. For wrap-style hooks, the Runtime object is available inside the ModelRequest parameter.\nCopy\nfrom dataclasses import dataclass\n\nfrom langchain.messages import AnyMessage\nfrom langchain.agents import create_agent, AgentState\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest, before_model, after_model\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass Context:\n    user_name: str\n\n# Dynamic prompts\n@dynamic_prompt\ndef dynamic_system_prompt(request: ModelRequest) -> str:\n    user_name = request.runtime.context.user_name  \n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return system_prompt\n\n# Before model hook\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  \n    print(f\"Processing request for user: {runtime.context.user_name}\")  \n    return None\n\n# After model hook\n@after_model\ndef log_after_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  \n    print(f\"Completed request for user: {runtime.context.user_name}\")  \n    return None\n\nagent = create_agent(\n    model=\"gpt-5-nano\",\n    tools=[...],\n    middleware=[dynamic_system_prompt, log_before_model, log_after_model],  \n    context_schema=Context\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n    context=Context(user_name=\"John Smith\")\n)\n\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494363,
    "timestamp": "2026-02-25T08:09:37.397Z",
    "title": "Context engineering in agents - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/context-engineering",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nWhy do agents fail?\nThe agent loop\nWhat you can control\nData sources\nHow it works\nModel context\nSystem Prompt\nMessages\nTools\nDefining tools\nSelecting tools\nModel\nResponse format\nDefining formats\nSelecting formats\nTool context\nReads\nWrites\nLife-cycle context\nExample: Summarization\nBest practices\nRelated resources\nAdvanced usage\nContext engineering in agents\nCopy page\nï¿½\nOverview\nThe hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.\nï¿½\nWhy do agents fail?\nWhen agents fail, itâ€™s usually because the LLM call inside the agent took the wrong action / didnâ€™t do what we expected. LLMs fail for one of two reasons:\nThe underlying LLM is not capable enough\nThe â€œrightâ€ context was not passed to the LLM\nMore often than not - itâ€™s actually the second reason that causes agents to not be reliable.\nContext engineering is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of â€œrightâ€ context is the number one blocker for more reliable agents, and LangChainâ€™s agent abstractions are uniquely designed to facilitate context engineering.\nNew to context engineering? Start with the conceptual overview to understand the different types of context and when to use them.\nï¿½\nThe agent loop\nA typical agent loop consists of two main steps:\nModel call - calls the LLM with a prompt and available tools, returns either a response or a request to execute tools\nTool execution - executes the tools that the LLM requested, returns tool results\nThis loop continues until the LLM decides to finish.\nï¿½\nWhat you can control\nTo build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.\nContext Type\tWhat You Control\tTransient or Persistent\nModel Context\tWhat goes into model calls (instructions, message history, tools, response format)\tTransient\nTool Context\tWhat tools can access and produce (reads/writes to state, store, runtime context)\tPersistent\nLife-cycle Context\tWhat happens between model and tool calls (summarization, guardrails, logging, etc.)\tPersistent\nTransient context\nWhat the LLM sees for a single call. You can modify messages, tools, or prompts without changing whatâ€™s saved in state.\nPersistent context\nWhat gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.\nï¿½\nData sources\nThroughout this process, your agent accesses (reads / writes) different sources of data:\nData Source\tAlso Known As\tScope\tExamples\nRuntime Context\tStatic configuration\tConversation-scoped\tUser ID, API keys, database connections, permissions, environment settings\nState\tShort-term memory\tConversation-scoped\tCurrent messages, uploaded files, authentication status, tool results\nStore\tLong-term memory\tCross-conversation\tUser preferences, extracted insights, memories, historical data\nï¿½\nHow it works\nLangChain middleware is the mechanism under the hood that makes context engineering practical for developers using LangChain.\nMiddleware allows you to hook into any step in the agent lifecycle and:\nUpdate context\nJump to a different step in the agent lifecycle\nThroughout this guide, youâ€™ll see frequent use of the middleware API as a means to the context engineering end.\nï¿½\nModel context\nControl what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.\nSystem Prompt\nBase instructions from the developer to the LLM.\nMessages\nThe full list of messages (conversation history) sent to the LLM.\nTools\nUtilities the agent has access to to take actions.\nModel\nThe actual model (including configuration) to be called.\nResponse Format\nSchema specification for the modelâ€™s final response.\nAll of these types of model context can draw from state (short-term memory), store (long-term memory), or runtime context (static configuration).\nï¿½\nSystem Prompt\nThe system prompt sets the LLMâ€™s behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.\nState\nStore\nRuntime Context\nAccess message count or conversation context from state:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n@dynamic_prompt\ndef state_aware_prompt(request: ModelRequest) -> str:\n    # request.messages is a shortcut for request.state[\"messages\"]\n    message_count = len(request.messages)\n\n    base = \"You are a helpful assistant.\"\n\n    if message_count > 10:\n        base += \"\\nThis is a long conversation - be extra concise.\"\n\n    return base\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[...],\n    middleware=[state_aware_prompt]\n)\n\nï¿½\nMessages\nMessages make up the prompt that is sent to the LLM. Itâ€™s critical to manage the content of messages to ensure that the LLM has the right information to respond well.\nState\nStore\nRuntime Context\nInject uploaded file context from State when relevant to current query:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n@wrap_model_call\ndef inject_file_context(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Inject context about files user has uploaded this session.\"\"\"\n    # Read from State: get uploaded files metadata\n    uploaded_files = request.state.get(\"uploaded_files\", [])  \n\n    if uploaded_files:\n        # Build context about available files\n        file_descriptions = []\n        for file in uploaded_files:\n            file_descriptions.append(\n                f\"- {file['name']} ({file['type']}): {file['summary']}\"\n            )\n\n        file_context = f\"\"\"Files you have access to in this conversation:\n{chr(10).join(file_descriptions)}\n\nReference these files when answering questions.\"\"\"\n\n        # Inject file context before recent messages\n        messages = [  \n            *request.messages,\n            {\"role\": \"user\", \"content\": file_context},\n        ]\n        request = request.override(messages=messages)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[...],\n    middleware=[inject_file_context]\n)\n\nTransient vs Persistent Message Updates:\nThe examples above use wrap_model_call to make transient updates - modifying what messages are sent to the model for a single call without changing whatâ€™s saved in state.\nFor persistent updates that modify state (like the summarization example in Life-cycle Context), use life-cycle hooks like before_model or after_model to permanently update the conversation history. See the middleware documentation for more details.\nï¿½\nTools\nTools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\nï¿½\nDefining tools\nEach tool needs a clear name, description, argument names, and argument descriptions. These arenâ€™t just metadataâ€”they guide the modelâ€™s reasoning about when and how to use the tool.\nCopy\nfrom langchain.tools import tool\n\n@tool(parse_docstring=True)\ndef search_orders(\n    user_id: str,\n    status: str,\n    limit: int = 10\n) -> str:\n    \"\"\"Search for user orders by status.\n\n    Use this when the user asks about order history or wants to check\n    order status. Always filter by the provided status.\n\n    Args:\n        user_id: Unique identifier for the user\n        status: Order status: 'pending', 'shipped', or 'delivered'\n        limit: Maximum number of results to return\n    \"\"\"\n    # Implementation here\n    pass\n\nï¿½\nSelecting tools\nNot every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.\nState\nStore\nRuntime Context\nEnable advanced tools only after certain conversation milestones:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n@wrap_model_call\ndef state_based_tools(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Filter tools based on conversation State.\"\"\"\n    # Read from State: check if user has authenticated\n    state = request.state  \n    is_authenticated = state.get(\"authenticated\", False)  \n    message_count = len(state[\"messages\"])\n\n    # Only enable sensitive tools after authentication\n    if not is_authenticated:\n        tools = [t for t in request.tools if t.name.startswith(\"public_\")]\n        request = request.override(tools=tools)  \n    elif message_count < 5:\n        # Limit tools early in conversation\n        tools = [t for t in request.tools if t.name != \"advanced_search\"]\n        request = request.override(tools=tools)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[public_search, private_search, advanced_search],\n    middleware=[state_based_tools]\n)\n\nSee Dynamic tools for both filtering pre-registered tools and registering tools at runtime (e.g., from MCP servers).\nï¿½\nModel\nDifferent models have different strengths, costs, and context windows. Select the right model for the task at hand, which might change during an agent run.\nState\nStore\nRuntime Context\nUse different models based on conversation length from State:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.chat_models import init_chat_model\nfrom typing import Callable\n\n# Initialize models once outside the middleware\nlarge_model = init_chat_model(\"claude-sonnet-4-5-20250929\")\nstandard_model = init_chat_model(\"gpt-4.1\")\nefficient_model = init_chat_model(\"gpt-4.1-mini\")\n\n@wrap_model_call\ndef state_based_model(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Select model based on State conversation length.\"\"\"\n    # request.messages is a shortcut for request.state[\"messages\"]\n    message_count = len(request.messages)  \n\n    if message_count > 20:\n        # Long conversation - use model with larger context window\n        model = large_model\n    elif message_count > 10:\n        # Medium conversation\n        model = standard_model\n    else:\n        # Short conversation - use efficient model\n        model = efficient_model\n\n    request = request.override(model=model)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4.1-mini\",\n    tools=[...],\n    middleware=[state_based_model]\n)\n\nSee Dynamic model for more examples.\nï¿½\nResponse format\nStructured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isnâ€™t sufficient.\nHow it works: When you provide a schema as the response format, the modelâ€™s final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.\nï¿½\nDefining formats\nSchema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.\nCopy\nfrom pydantic import BaseModel, Field\n\nclass CustomerSupportTicket(BaseModel):\n    \"\"\"Structured ticket information extracted from customer message.\"\"\"\n\n    category: str = Field(\n        description=\"Issue category: 'billing', 'technical', 'account', or 'product'\"\n    )\n    priority: str = Field(\n        description=\"Urgency level: 'low', 'medium', 'high', or 'critical'\"\n    )\n    summary: str = Field(\n        description=\"One-sentence summary of the customer's issue\"\n    )\n    customer_sentiment: str = Field(\n        description=\"Customer's emotional tone: 'frustrated', 'neutral', or 'satisfied'\"\n    )\n\nï¿½\nSelecting formats\nDynamic response format selection adapts schemas based on user preferences, conversation stage, or roleâ€”returning simple formats early and detailed formats as complexity increases.\nState\nStore\nRuntime Context\nConfigure structured output based on conversation state:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom pydantic import BaseModel, Field\nfrom typing import Callable\n\nclass SimpleResponse(BaseModel):\n    \"\"\"Simple response for early conversation.\"\"\"\n    answer: str = Field(description=\"A brief answer\")\n\nclass DetailedResponse(BaseModel):\n    \"\"\"Detailed response for established conversation.\"\"\"\n    answer: str = Field(description=\"A detailed answer\")\n    reasoning: str = Field(description=\"Explanation of reasoning\")\n    confidence: float = Field(description=\"Confidence score 0-1\")\n\n@wrap_model_call\ndef state_based_output(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse]\n) -> ModelResponse:\n    \"\"\"Select output format based on State.\"\"\"\n    # request.messages is a shortcut for request.state[\"messages\"]\n    message_count = len(request.messages)  \n\n    if message_count < 3:\n        # Early conversation - use simple format\n        request = request.override(response_format=SimpleResponse)  \n    else:\n        # Established conversation - use detailed format\n        request = request.override(response_format=DetailedResponse)  \n\n    return handler(request)\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[...],\n    middleware=[state_based_output]\n)\n\nï¿½\nTool context\nTools are special in that they both read and write context.\nIn the most basic case, when a tool executes, it receives the LLMâ€™s request parameters and returns a tool message back. The tool does its work and produces a result.\nTools can also fetch important information for the model that allows it to perform and complete tasks.\nï¿½\nReads\nMost real-world tools need more than just the LLMâ€™s parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.\nState\nStore\nRuntime Context\nRead from State to check current session information:\nCopy\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.agents import create_agent\n\n@tool\ndef check_authentication(\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Check if user is authenticated.\"\"\"\n    # Read from State: check current auth status\n    current_state = runtime.state\n    is_authenticated = current_state.get(\"authenticated\", False)\n\n    if is_authenticated:\n        return \"User is authenticated\"\n    else:\n        return \"User is not authenticated\"\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[check_authentication]\n)\n\nï¿½\nWrites\nTool results can be used to help an agent complete a given task. Tools can both return results directly to the model and update the memory of the agent to make important context available to future steps.\nState\nStore\nWrite to State to track session-specific information using Command:\nCopy\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.agents import create_agent\nfrom langgraph.types import Command\n\n@tool\ndef authenticate_user(\n    password: str,\n    runtime: ToolRuntime\n) -> Command:\n    \"\"\"Authenticate user and update State.\"\"\"\n    # Perform authentication (simplified)\n    if password == \"correct\":\n        # Write to State: mark as authenticated using Command\n        return Command(\n            update={\"authenticated\": True},\n        )\n    else:\n        return Command(update={\"authenticated\": False})\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[authenticate_user]\n)\n\nSee Tools for comprehensive examples of accessing state, store, and runtime context in tools.\nï¿½\nLife-cycle context\nControl what happens between the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.\nAs youâ€™ve seen in Model Context and Tool Context, middleware is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:\nUpdate context - Modify state and store to persist changes, update conversation history, or save insights\nJump in the lifecycle - Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)\nï¿½\nExample: Summarization\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context, summarization persistently updates state - permanently replacing old messages with a summary thatâ€™s saved for all future turns.\nLangChain offers built-in middleware for this:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[...],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4.1-mini\",\n            trigger={\"tokens\": 4000},\n            keep={\"messages\": 20},\n        ),\n    ],\n)\n\nWhen the conversation exceeds the token limit, SummarizationMiddleware automatically:\nSummarizes older messages using a separate LLM call\nReplaces them with a summary message in State (permanently)\nKeeps recent messages intact for context\nThe summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.\nFor a complete list of built-in middleware, available hooks, and how to create custom middleware, see the Middleware documentation.\nï¿½\nBest practices\nStart simple - Begin with static prompts and tools, add dynamics only when needed\nTest incrementally - Add one context engineering feature at a time\nMonitor performance - Track model calls, token usage, and latency\nUse built-in middleware - Leverage SummarizationMiddleware, LLMToolSelectorMiddleware, etc.\nDocument your context strategy - Make it clear what context is being passed and why\nUnderstand transient vs persistent: Model context changes are transient (per-call), while life-cycle context changes persist to state\nï¿½\nRelated resources\nContext conceptual overview - Understand context types and when to use them\nMiddleware - Complete middleware guide\nTools - Tool creation and context access\nMemory - Short-term and long-term memory patterns\nAgents - Core agent concepts\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494364,
    "timestamp": "2026-02-25T08:09:37.402Z",
    "title": "Model Context Protocol (MCP) - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/mcp",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nQuickstart\nCustom servers\nTransports\nHTTP\nPassing headers\nAuthentication\nstdio\nStateful sessions\nCore features\nTools\nLoading tools\nStructured content\nMultimodal tool content\nResources\nLoading resources\nPrompts\nLoading prompts\nAdvanced features\nTool interceptors\nAccessing runtime context\nState updates and commands\nCustom interceptors\nProgress notifications\nLogging\nElicitation\nServer setup\nClient setup\nResponse actions\nAdditional resources\nAdvanced usage\nModel Context Protocol (MCP)\nCopy page\nModel Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the langchain-mcp-adapters library.\nï¿½\nQuickstart\nInstall the langchain-mcp-adapters library:\npip\nuv\nCopy\npip install langchain-mcp-adapters\n\nlangchain-mcp-adapters enables agents to use tools defined across one or more MCP servers.\nMultiServerMCPClient is stateless by default. Each tool invocation creates a fresh MCP ClientSession, executes the tool, and then cleans up. See the stateful sessions section for more details.\nAccessing multiple MCP servers\nCopy\nimport asyncio\nfrom langchain_mcp_adapters.client import MultiServerMCPClient  \nfrom langchain.agents import create_agent\n\nasync def main():\n    client = MultiServerMCPClient(  \n        {\n            \"math\": {\n                \"transport\": \"stdio\",  # Local subprocess communication\n                \"command\": \"python\",\n                # Absolute path to your math_server.py file\n                \"args\": [\"/path/to/math_server.py\"],\n            },\n            \"weather\": {\n                \"transport\": \"http\",  # HTTP-based remote server\n                # Ensure you start your weather server on port 8000\n                \"url\": \"http://localhost:8000/mcp\",\n            }\n        }\n    )\n\n    tools = await client.get_tools()  \n    agent = create_agent(\n        \"claude-sonnet-4-5-20250929\",\n        tools  \n    )\n    math_response = await agent.ainvoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n    )\n    weather_response = await agent.ainvoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n    )\n    print(math_response)\n    print(weather_response)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nï¿½\nCustom servers\nTo create a custom MCP server, use the FastMCP library:\npip\nuv\nCopy\npip install fastmcp\n\nTo test your agent with MCP tool servers, use the following examples:\nMath server (stdio transport)\nWeather server (streamable HTTP transport)\nCopy\nfrom fastmcp import FastMCP\n\nmcp = FastMCP(\"Math\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n\nï¿½\nTransports\nMCP supports different transport mechanisms for client-server communication.\nï¿½\nHTTP\nThe http transport (also referred to as streamable-http) uses HTTP requests for client-server communication. See the MCP HTTP transport specification for more details.\nCopy\nclient = MultiServerMCPClient(\n    {\n        \"weather\": {\n            \"transport\": \"http\",\n            \"url\": \"http://localhost:8000/mcp\",\n        }\n    }\n)\n\nï¿½\nPassing headers\nWhen connecting to MCP servers over HTTP, you can include custom headers (e.g., for authentication or tracing) using the headers field in the connection configuration. This is supported for sse (deprecated by MCP spec) and streamable_http transports.\nPassing headers with MultiServerMCPClient\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient(\n    {\n        \"weather\": {\n            \"transport\": \"http\",\n            \"url\": \"http://localhost:8000/mcp\",\n            \"headers\": {  \n                \"Authorization\": \"Bearer YOUR_TOKEN\",  \n                \"X-Custom-Header\": \"custom-value\"\n            },  \n        }\n    }\n)\ntools = await client.get_tools()\nagent = create_agent(\"openai:gpt-4.1\", tools)\nresponse = await agent.ainvoke({\"messages\": \"what is the weather in nyc?\"})\n\nï¿½\nAuthentication\nThe langchain-mcp-adapters library uses the official MCP SDK under the hood, which allows you to provide a custom authentication mechanism by implementing the httpx.Auth interface.\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\nclient = MultiServerMCPClient(\n    {\n        \"weather\": {\n            \"transport\": \"http\",\n            \"url\": \"http://localhost:8000/mcp\",\n            \"auth\": auth, \n        }\n    }\n)\n\nExample custom auth implementation\nBuilt-in OAuth flow\nï¿½\nstdio\nClient launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\nUnlike HTTP transports, stdio connections are inherently statefulâ€”the subprocess persists for the lifetime of the client connection. However, when using MultiServerMCPClient without explicit session management, each tool call still creates a new session. See stateful sessions for managing persistent connections.\nCopy\nclient = MultiServerMCPClient(\n    {\n        \"math\": {\n            \"transport\": \"stdio\",\n            \"command\": \"python\",\n            \"args\": [\"/path/to/math_server.py\"],\n        }\n    }\n)\n\nï¿½\nStateful sessions\nBy default, MultiServerMCPClient is statelessâ€”each tool invocation creates a fresh MCP session, executes the tool, and then cleans up.\nIf you need to control the lifecycle of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent ClientSession using client.session().\nUsing MCP ClientSession for stateful tool usage\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.tools import load_mcp_tools\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient({...})\n\n# Create a session explicitly\nasync with client.session(\"server_name\") as session:  \n    # Pass the session to load tools, resources, or prompts\n    tools = await load_mcp_tools(session)  \n    agent = create_agent(\n        \"anthropic:claude-3-7-sonnet-latest\",\n        tools\n    )\n\nï¿½\nCore features\nï¿½\nTools\nTools allow MCP servers to expose executable functions that LLMs can invoke to perform actionsâ€”such as querying databases, calling APIs, or interacting with external systems. LangChain converts MCP tools into LangChain tools, making them directly usable in any LangChain agent or workflow.\nï¿½\nLoading tools\nUse client.get_tools() to retrieve tools from MCP servers and pass them to your agent:\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient({...})\ntools = await client.get_tools()  \nagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)\n\nï¿½\nStructured content\nMCP tools can return structured content alongside the human-readable text response. This is useful when a tool needs to return machine-parseable data (like JSON) in addition to text that gets shown to the model.\nWhen an MCP tool returns structuredContent, the adapter wraps it in an MCPToolArtifact and returns it as the toolâ€™s artifact. You can access this using the artifact field on the ToolMessage. You can also use interceptors to process or transform structured content automatically.\nExtracting structured content from artifact\nAfter invoking your agent, you can access the structured content from tool messages in the response:\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\nfrom langchain.messages import ToolMessage\n\nclient = MultiServerMCPClient({...})\ntools = await client.get_tools()\nagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)\n\nresult = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Get data from the server\"}]}\n)\n\n# Extract structured content from tool messages\nfor message in result[\"messages\"]:\n    if isinstance(message, ToolMessage) and message.artifact:\n        structured_content = message.artifact[\"structured_content\"]\n\nAppending structured content via interceptor\nIf you want structured content to be visible in the conversation history (visible to the model), you can use an interceptor to automatically append structured content to the tool result:\nCopy\nimport json\n\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest\nfrom mcp.types import TextContent\n\nasync def append_structured_content(request: MCPToolCallRequest, handler):\n    \"\"\"Append structured content from artifact to tool message.\"\"\"\n    result = await handler(request)\n    if result.structuredContent:\n        result.content += [\n            TextContent(type=\"text\", text=json.dumps(result.structuredContent)),\n        ]\n    return result\n\nclient = MultiServerMCPClient({...}, tool_interceptors=[append_structured_content])\n\nï¿½\nMultimodal tool content\nMCP tools can return multimodal content (images, text, etc.) in their responses. When an MCP server returns content with multiple parts (e.g., text and images), the adapter converts them to LangChainâ€™s standard content blocks. You can access the standardized representation via the content_blocks property on the ToolMessage:\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.agents import create_agent\n\nclient = MultiServerMCPClient({...})\ntools = await client.get_tools()\nagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)\n\nresult = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Take a screenshot of the current page\"}]}\n)\n\n# Access multimodal content from tool messages\nfor message in result[\"messages\"]:\n    if message.type == \"tool\":\n        # Raw content in provider-native format\n        print(f\"Raw content: {message.content}\")\n\n        # Standardized content blocks  #\n        for block in message.content_blocks:  \n            if block[\"type\"] == \"text\":  \n                print(f\"Text: {block['text']}\")  \n            elif block[\"type\"] == \"image\":  \n                print(f\"Image URL: {block.get('url')}\")  \n                print(f\"Image base64: {block.get('base64', '')[:50]}...\")  \n\nThis allows you to handle multimodal tool responses in a provider-agnostic way, regardless of how the underlying MCP server formats its content.\nï¿½\nResources\nResources allow MCP servers to expose dataâ€”such as files, database records, or API responsesâ€”that can be read by clients. LangChain converts MCP resources into Blob objects, which provide a unified interface for handling both text and binary content.\nï¿½\nLoading resources\nUse client.get_resources() to load resources from an MCP server:\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\nclient = MultiServerMCPClient({...})\n\n# Load all resources from a server\nblobs = await client.get_resources(\"server_name\")  \n\n# Or load specific resources by URI\nblobs = await client.get_resources(\"server_name\", uris=[\"file:///path/to/file.txt\"])  \n\nfor blob in blobs:\n    print(f\"URI: {blob.metadata['uri']}, MIME type: {blob.mimetype}\")\n    print(blob.as_string())  # For text content\n\nYou can also use load_mcp_resources directly with a session for more control:\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.resources import load_mcp_resources\n\nclient = MultiServerMCPClient({...})\n\nasync with client.session(\"server_name\") as session:\n    # Load all resources\n    blobs = await load_mcp_resources(session)\n\n    # Or load specific resources by URI\n    blobs = await load_mcp_resources(session, uris=[\"file:///path/to/file.txt\"])\n\nï¿½\nPrompts\nPrompts allow MCP servers to expose reusable prompt templates that can be retrieved and used by clients. LangChain converts MCP prompts into messages, making them easy to integrate into chat-based workflows.\nï¿½\nLoading prompts\nUse client.get_prompt() to load a prompt from an MCP server:\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\n\nclient = MultiServerMCPClient({...})\n\n# Load a prompt by name\nmessages = await client.get_prompt(\"server_name\", \"summarize\")  \n\n# Load a prompt with arguments\nmessages = await client.get_prompt(  \n    \"server_name\",  \n    \"code_review\",  \n    arguments={\"language\": \"python\", \"focus\": \"security\"}  \n)  \n\n# Use the messages in your workflow\nfor message in messages:\n    print(f\"{message.type}: {message.content}\")\n\nYou can also use load_mcp_prompt directly with a session for more control:\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.prompts import load_mcp_prompt\n\nclient = MultiServerMCPClient({...})\n\nasync with client.session(\"server_name\") as session:\n    # Load a prompt by name\n    messages = await load_mcp_prompt(session, \"summarize\")\n\n    # Load a prompt with arguments\n    messages = await load_mcp_prompt(\n        session,\n        \"code_review\",\n        arguments={\"language\": \"python\", \"focus\": \"security\"}\n    )\n\nï¿½\nAdvanced features\nï¿½\nTool interceptors\nMCP servers run as separate processesâ€”they canâ€™t access LangGraph runtime information like the store, context, or agent state. Interceptors bridge this gap by giving you access to this runtime context during MCP tool execution.\nInterceptors also provide middleware-like control over tool calls: you can modify requests, implement retries, add headers dynamically, or short-circuit execution entirely.\nSection\tDescription\nAccessing runtime context\tRead user IDs, API keys, store data, and agent state\nState updates and commands\tUpdate agent state or control graph flow with Command\nWriting interceptors\tPatterns for modifying requests, composing interceptors, and error handling\nï¿½\nAccessing runtime context\nWhen MCP tools are used within a LangChain agent (via create_agent), interceptors receive access to the ToolRuntime context. This provides access to the tool call ID, state, config, and storeâ€”enabling powerful patterns for accessing user data, persisting information, and controlling agent behavior.\nRuntime context\nStore\nState\nTool call ID\nAccess user-specific configuration like user IDs, API keys, or permissions that are passed at invocation time:\nInject user context into MCP tool calls\nCopy\nfrom dataclasses import dataclass\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest\nfrom langchain.agents import create_agent\n\n@dataclass\nclass Context:\n    user_id: str\n    api_key: str\n\nasync def inject_user_context(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Inject user credentials into MCP tool calls.\"\"\"\n    runtime = request.runtime\n    user_id = runtime.context.user_id  \n    api_key = runtime.context.api_key  \n\n    # Add user context to tool arguments\n    modified_request = request.override(\n        args={**request.args, \"user_id\": user_id}\n    )\n    return await handler(modified_request)\n\nclient = MultiServerMCPClient(\n    {...},\n    tool_interceptors=[inject_user_context],\n)\ntools = await client.get_tools()\nagent = create_agent(\"gpt-4.1\", tools, context_schema=Context)\n\n# Invoke with user context\nresult = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]},\n    context={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"}\n)\n\nFor more context engineering patterns, see Context engineering and Tools.\nï¿½\nState updates and commands\nInterceptors can return Command objects to update agent state or control graph execution flow. This is useful for tracking task progress, switching between agents, or ending execution early.\nMark task complete and switch agents\nCopy\nfrom langchain.agents import AgentState, create_agent\nfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\nasync def handle_task_completion(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Mark task complete and hand off to summary agent.\"\"\"\n    result = await handler(request)\n\n    if request.name == \"submit_order\":\n        return Command(\n            update={\n                \"messages\": [result] if isinstance(result, ToolMessage) else [],\n                \"task_status\": \"completed\",  \n            },\n            goto=\"summary_agent\",  \n        )\n\n    return result\n\nUse Command with goto=\"__end__\" to end execution early:\nEnd agent run on completion\nCopy\nasync def end_on_success(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"End agent run when task is marked complete.\"\"\"\n    result = await handler(request)\n\n    if request.name == \"mark_complete\":\n        return Command(\n            update={\"messages\": [result], \"status\": \"done\"},\n            goto=\"__end__\",  \n        )\n\n    return result\n\nï¿½\nCustom interceptors\nInterceptors are async functions that wrap tool execution, enabling request/response modification, retry logic, and other cross-cutting concerns. They follow an â€œonionâ€ pattern where the first interceptor in the list is the outermost layer.\nBasic pattern\nAn interceptor is an async function that receives a request and a handler. You can modify the request before calling the handler, modify the response after, or skip the handler entirely.\nBasic interceptor pattern\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest\n\nasync def logging_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Log tool calls before and after execution.\"\"\"\n    print(f\"Calling tool: {request.name} with args: {request.args}\")\n    result = await handler(request)\n    print(f\"Tool {request.name} returned: {result}\")\n    return result\n\nclient = MultiServerMCPClient(\n    {\"math\": {\"transport\": \"stdio\", \"command\": \"python\", \"args\": [\"/path/to/server.py\"]}},\n    tool_interceptors=[logging_interceptor],  \n)\n\nModifying requests\nUse request.override() to create a modified request. This follows an immutable pattern, leaving the original request unchanged.\nModifying tool arguments\nCopy\nasync def double_args_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Double all numeric arguments before execution.\"\"\"\n    modified_args = {k: v * 2 for k, v in request.args.items()}\n    modified_request = request.override(args=modified_args)  \n    return await handler(modified_request)\n\n# Original call: add(a=2, b=3) becomes add(a=4, b=6)\n\nModifying headers at runtime\nInterceptors can modify HTTP headers dynamically based on the request context:\nDynamic header modification\nCopy\nasync def auth_header_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Add authentication headers based on the tool being called.\"\"\"\n    token = get_token_for_tool(request.name)\n    modified_request = request.override(\n        headers={\"Authorization\": f\"Bearer {token}\"}  \n    )\n    return await handler(modified_request)\n\nComposing interceptors\nMultiple interceptors compose in â€œonionâ€ order â€” the first interceptor in the list is the outermost layer:\nComposing multiple interceptors\nCopy\nasync def outer_interceptor(request, handler):\n    print(\"outer: before\")\n    result = await handler(request)\n    print(\"outer: after\")\n    return result\n\nasync def inner_interceptor(request, handler):\n    print(\"inner: before\")\n    result = await handler(request)\n    print(\"inner: after\")\n    return result\n\nclient = MultiServerMCPClient(\n    {...},\n    tool_interceptors=[outer_interceptor, inner_interceptor],  \n)\n\n# Execution order:\n# outer: before -> inner: before -> tool execution -> inner: after -> outer: after\n\nError handling\nUse interceptors to catch tool execution errors and implement retry logic:\nRetry on error\nCopy\nimport asyncio\n\nasync def retry_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n    max_retries: int = 3,\n    delay: float = 1.0,\n):\n    \"\"\"Retry failed tool calls with exponential backoff.\"\"\"\n    last_error = None\n    for attempt in range(max_retries):\n        try:\n            return await handler(request)\n        except Exception as e:\n            last_error = e\n            if attempt < max_retries - 1:\n                wait_time = delay * (2 ** attempt)  # Exponential backoff\n                print(f\"Tool {request.name} failed (attempt {attempt + 1}), retrying in {wait_time}s...\")\n                await asyncio.sleep(wait_time)\n    raise last_error\n\nclient = MultiServerMCPClient(\n    {...},\n    tool_interceptors=[retry_interceptor],  \n)\n\nYou can also catch specific error types and return fallback values:\nError handling with fallback\nCopy\nasync def fallback_interceptor(\n    request: MCPToolCallRequest,\n    handler,\n):\n    \"\"\"Return a fallback value if tool execution fails.\"\"\"\n    try:\n        return await handler(request)\n    except TimeoutError:\n        return f\"Tool {request.name} timed out. Please try again later.\"\n    except ConnectionError:\n        return f\"Could not connect to {request.name} service. Using cached data.\"\n\nï¿½\nProgress notifications\nSubscribe to progress updates for long-running tool executions:\nProgress callback\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.callbacks import Callbacks, CallbackContext\n\nasync def on_progress(\n    progress: float,\n    total: float | None,\n    message: str | None,\n    context: CallbackContext,\n):\n    \"\"\"Handle progress updates from MCP servers.\"\"\"\n    percent = (progress / total * 100) if total else progress\n    tool_info = f\" ({context.tool_name})\" if context.tool_name else \"\"\n    print(f\"[{context.server_name}{tool_info}] Progress: {percent:.1f}% - {message}\")\n\nclient = MultiServerMCPClient(\n    {...},\n    callbacks=Callbacks(on_progress=on_progress),  \n)\n\nThe CallbackContext provides:\nserver_name: Name of the MCP server\ntool_name: Name of the tool being executed (available during tool calls)\nï¿½\nLogging\nThe MCP protocol supports logging notifications from servers. Use the Callbacks class to subscribe to these events.\nLogging callback\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.callbacks import Callbacks, CallbackContext\nfrom mcp.types import LoggingMessageNotificationParams\n\nasync def on_logging_message(\n    params: LoggingMessageNotificationParams,\n    context: CallbackContext,\n):\n    \"\"\"Handle log messages from MCP servers.\"\"\"\n    print(f\"[{context.server_name}] {params.level}: {params.data}\")\n\nclient = MultiServerMCPClient(\n    {...},\n    callbacks=Callbacks(on_logging_message=on_logging_message),  \n)\n\nï¿½\nElicitation\nElicitation allows MCP servers to request additional input from users during tool execution. Instead of requiring all inputs upfront, servers can interactively ask for information as needed.\nï¿½\nServer setup\nDefine a tool that uses ctx.elicit() to request user input with a schema:\nMCP server with elicitation\nCopy\nfrom pydantic import BaseModel\nfrom mcp.server.fastmcp import Context, FastMCP\n\nserver = FastMCP(\"Profile\")\n\nclass UserDetails(BaseModel):\n    email: str\n    age: int\n\n@server.tool()\nasync def create_profile(name: str, ctx: Context) -> str:\n    \"\"\"Create a user profile, requesting details via elicitation.\"\"\"\n    result = await ctx.elicit(  \n        message=f\"Please provide details for {name}'s profile:\",  \n        schema=UserDetails,  \n    )  \n    if result.action == \"accept\" and result.data:\n        return f\"Created profile for {name}: email={result.data.email}, age={result.data.age}\"\n    if result.action == \"decline\":\n        return f\"User declined. Created minimal profile for {name}.\"\n    return \"Profile creation cancelled.\"\n\nif __name__ == \"__main__\":\n    server.run(transport=\"http\")\n\nï¿½\nClient setup\nHandle elicitation requests by providing a callback to MultiServerMCPClient:\nHandling elicitation requests\nCopy\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain_mcp_adapters.callbacks import Callbacks, CallbackContext\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import ElicitRequestParams, ElicitResult\n\nasync def on_elicitation(\n    mcp_context: RequestContext,\n    params: ElicitRequestParams,\n    context: CallbackContext,\n) -> ElicitResult:\n    \"\"\"Handle elicitation requests from MCP servers.\"\"\"\n    # In a real application, you would prompt the user for input\n    # based on params.message and params.requestedSchema\n    return ElicitResult(  \n        action=\"accept\",  \n        content={\"email\": \"user@example.com\", \"age\": 25},  \n    )  \n\nclient = MultiServerMCPClient(\n    {\n        \"profile\": {\n            \"url\": \"http://localhost:8000/mcp\",\n            \"transport\": \"http\",\n        }\n    },\n    callbacks=Callbacks(on_elicitation=on_elicitation),  \n)\n\nï¿½\nResponse actions\nThe elicitation callback can return one of three actions:\nAction\tDescription\naccept\tUser provided valid input. Include the data in the content field.\ndecline\tUser chose not to provide the requested information.\ncancel\tUser cancelled the operation entirely.\nResponse action examples\nCopy\n# Accept with data\nElicitResult(action=\"accept\", content={\"email\": \"user@example.com\", \"age\": 25})\n\n# Decline (user doesn't want to provide info)\nElicitResult(action=\"decline\")\n\n# Cancel (abort the operation)\nElicitResult(action=\"cancel\")\n\nï¿½\nAdditional resources\nMCP documentation\nMCP Transport documentation\nlangchain-mcp-adapters\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494366,
    "timestamp": "2026-02-25T08:09:37.411Z",
    "title": "Multi-agent - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nWhy multi-agent?\nPatterns\nChoosing a pattern\nVisual overview\nPerformance comparison\nOne-shot request\nRepeat request\nMulti-domain\nSummary\nAdvanced usage\nMulti-agent\nMulti-agent\nCopy page\nMulti-agent systems coordinate specialized components to tackle complex workflows. However, not every complex task requires this approach â€” a single agent with the right (sometimes dynamic) tools and prompt can often achieve similar results.\nï¿½\nWhy multi-agent?\nWhen developers say they need â€œmulti-agent,â€ theyâ€™re usually looking for one or more of these capabilities:\n Context management: Provide specialized knowledge without overwhelming the modelâ€™s context window. If context were infinite and latency zero, you could dump all knowledge into a single prompt â€” but since itâ€™s not, you need patterns to selectively surface relevant information.\n Distributed development: Allow different teams to develop and maintain capabilities independently, composing them into a larger system with clear boundaries.\n Parallelization: Spawn specialized workers for subtasks and execute them concurrently for faster results.\nMulti-agent patterns are particularly valuable when a single agent has too many tools and makes poor decisions about which to use, when tasks require specialized knowledge with extensive context (long prompts and domain-specific tools), or when you need to enforce sequential constraints that unlock capabilities only after certain conditions are met.\nAt the center of multi-agent design is context engineeringâ€”deciding what information each agent sees. The quality of your system depends on ensuring each agent has access to the right data for its task.\nï¿½\nPatterns\nHere are the main patterns for building multi-agent systems, each suited to different use cases:\nPattern\tHow it works\nSubagents\tA main agent coordinates subagents as tools. All routing passes through the main agent, which decides when and how to invoke each subagent.\nHandoffs\tBehavior changes dynamically based on state. Tool calls update a state variable that triggers routing or configuration changes, switching agents or adjusting the current agentâ€™s tools and prompt.\nSkills\tSpecialized prompts and knowledge loaded on-demand. A single agent stays in control while loading context from skills as needed.\nRouter\tA routing step classifies input and directs it to one or more specialized agents. Results are synthesized into a combined response.\nCustom workflow\tBuild bespoke execution flows with LangGraph, mixing deterministic logic and agentic behavior. Embed other patterns as nodes in your workflow.\nï¿½\nChoosing a pattern\nUse this table to match your requirements to the right pattern:\nPattern\tDistributed development\tParallelization\tMulti-hop\tDirect user interaction\nSubagents\tâ­â­â­â­â­\tâ­â­â­â­â­\tâ­â­â­â­â­\tâ­\nHandoffs\tâ€”\tâ€”\tâ­â­â­â­â­\tâ­â­â­â­â­\nSkills\tâ­â­â­â­â­\tâ­â­â­\tâ­â­â­â­â­\tâ­â­â­â­â­\nRouter\tâ­â­â­\tâ­â­â­â­â­\tâ€”\tâ­â­â­\nDistributed development: Can different teams maintain components independently?\nParallelization: Can multiple agents execute concurrently?\nMulti-hop: Does the pattern support calling multiple subagents in series?\nDirect user interaction: Can subagents converse directly with the user?\nYou can mix patterns! For example, a subagents architecture can invoke tools that invoke custom workflows or router agents. Subagents can even use the skills pattern to load context on-demand. The possibilities are endless!\nï¿½\nVisual overview\nSubagents\nHandoffs\nSkills\nRouter\nA main agent coordinates subagents as tools. All routing passes through the main agent.\nï¿½\nPerformance comparison\nDifferent patterns have different performance characteristics. Understanding these tradeoffs helps you choose the right pattern for your latency and cost requirements.\nKey metrics:\nModel calls: Number of LLM invocations. More calls = higher latency (especially if sequential) and higher per-request API costs.\nTokens processed: Total context window usage across all calls. More tokens = higher processing costs and potential context limits.\nï¿½\nOne-shot request\nUser: â€œBuy coffeeâ€\nA specialized coffee agent/skill can call a buy_coffee tool.\nPattern\tModel calls\tBest fit\nSubagents\t4\t\nHandoffs\t3\tâœ…\nSkills\t3\tâœ…\nRouter\t3\tâœ…\nSubagents\nHandoffs\nSkills\nRouter\n4 model calls:\nKey insight: Handoffs, Skills, and Router are most efficient for single tasks (3 calls each). Subagents adds one extra call because results flow back through the main agentâ€”this overhead provides centralized control.\nï¿½\nRepeat request\nTurn 1: â€œBuy coffeeâ€ Turn 2: â€œBuy coffee againâ€\nThe user repeats the same request in the same conversation.\nPattern\tTurn 2 calls\tTotal (both turns)\tBest fit\nSubagents\t4\t8\t\nHandoffs\t2\t5\tâœ…\nSkills\t2\t5\tâœ…\nRouter\t3\t6\t\nSubagents\nHandoffs\nSkills\nRouter\n4 calls again â†’ 8 total\nSubagents are stateless by designâ€”each invocation follows the same flow\nThe main agent maintains conversation context, but subagents start fresh each time\nThis provides strong context isolation but repeats the full flow\nKey insight: Stateful patterns (Handoffs, Skills) save 40-50% of calls on repeat requests. Subagents maintain consistent cost per requestâ€”this stateless design provides strong context isolation but at the cost of repeated model calls.\nï¿½\nMulti-domain\nUser: â€œCompare Python, JavaScript, and Rust for web developmentâ€\nEach language agent/skill contains ~2000 tokens of documentation. All patterns can make parallel tool calls.\nPattern\tModel calls\tTotal tokens\tBest fit\nSubagents\t5\t~9K\tâœ…\nHandoffs\t7+\t~14K+\t\nSkills\t3\t~15K\t\nRouter\t5\t~9K\tâœ…\nSubagents\nHandoffs\nSkills\nRouter\n5 calls, ~9K tokens\nEach subagent works in isolation with only its relevant context. Total: 9K tokens.\nKey insight: For multi-domain tasks, patterns with parallel execution (Subagents, Router) are most efficient. Skills has fewer calls but high token usage due to context accumulation. Handoffs is inefficient hereâ€”it must execute sequentially and canâ€™t leverage parallel tool calling for consulting multiple domains simultaneously.\nï¿½\nSummary\nHereâ€™s how patterns compare across all three scenarios:\nPattern\tOne-shot\tRepeat request\tMulti-domain\nSubagents\t4 calls\t8 calls (4+4)\t5 calls, 9K tokens\nHandoffs\t3 calls\t5 calls (3+2)\t7+ calls, 14K+ tokens\nSkills\t3 calls\t5 calls (3+2)\t3 calls, 15K tokens\nRouter\t3 calls\t6 calls (3+3)\t5 calls, 9K tokens\nChoosing a pattern:\nOptimize for\tSubagents\tHandoffs\tSkills\tRouter\nSingle requests\t\tâœ…\tâœ…\tâœ…\nRepeat requests\t\tâœ…\tâœ…\t\nParallel execution\tâœ…\t\t\tâœ…\nLarge-context domains\tâœ…\t\t\tâœ…\nSimple, focused tasks\t\t\tâœ…\t\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494365,
    "timestamp": "2026-02-25T08:09:37.411Z",
    "title": "Human-in-the-loop - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/human-in-the-loop",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nInterrupt decision types\nConfiguring interrupts\nResponding to interrupts\nDecision types\nStreaming with human-in-the-loop\nExecution lifecycle\nCustom HITL logic\nAdvanced usage\nHuman-in-the-loop\nCopy page\nThe Human-in-the-Loop (HITL) middleware lets you add human oversight to agent tool calls. When a model proposes an action that might require review â€” for example, writing to a file or executing SQL â€” the middleware can pause execution and wait for a decision.\nIt does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an interrupt that halts execution. The graph state is saved using LangGraphâ€™s persistence layer, so execution can pause safely and resume later.\nA human decision then determines what happens next: the action can be approved as-is (approve), modified before running (edit), or rejected with feedback (reject).\nï¿½\nInterrupt decision types\nThe middleware defines three built-in ways a human can respond to an interrupt:\nDecision Type\tDescription\tExample Use Case\nâœ… approve\tThe action is approved as-is and executed without changes.\tSend an email draft exactly as written\nâœï¸ edit\tThe tool call is executed with modifications.\tChange the recipient before sending an email\nâŒ reject\tThe tool call is rejected, with an explanation added to the conversation.\tReject an email draft and explain how to rewrite it\nThe available decision types for each tool depend on the policy you configure in interrupt_on. When multiple tool calls are paused at the same time, each action requires a separate decision. Decisions must be provided in the same order as the actions appear in the interrupt request.\nWhen editing tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\nï¿½\nConfiguring interrupts\nTo use HITL, add the middleware to the agentâ€™s middleware list when creating the agent.\nYou configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware \nfrom langgraph.checkpoint.memory import InMemorySaver \n\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[write_file_tool, execute_sql_tool, read_data_tool],\n    middleware=[\n        HumanInTheLoopMiddleware( \n            interrupt_on={\n                \"write_file\": True,  # All decisions (approve, edit, reject) allowed\n                \"execute_sql\": {\"allowed_decisions\": [\"approve\", \"reject\"]},  # No editing allowed\n                # Safe operation, no approval needed\n                \"read_data\": False,\n            },\n            # Prefix for interrupt messages - combined with tool name and args to form the full message\n            # e.g., \"Tool execution pending approval: execute_sql with query='DELETE FROM...'\"\n            # Individual tools can override this by specifying a \"description\" in their interrupt config\n            description_prefix=\"Tool execution pending approval\",\n        ),\n    ],\n    # Human-in-the-loop requires checkpointing to handle interrupts.\n    # In production, use a persistent checkpointer like AsyncPostgresSaver.\n    checkpointer=InMemorySaver(),  \n)\n\nYou must configure a checkpointer to persist the graph state across interrupts. In production, use a persistent checkpointer like AsyncPostgresSaver. For testing or prototyping, use InMemorySaver.\nWhen invoking the agent, pass a config that includes the thread ID to associate execution with a conversation thread. See the LangGraph interrupts documentation for details.\n\nConfiguration options\n\nï¿½\nResponding to interrupts\nWhen you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in interrupt_on. In that case, the invocation result will include an __interrupt__ field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.\nCopy\nfrom langgraph.types import Command\n\n# Human-in-the-loop leverages LangGraph's persistence layer.\n# You must provide a thread ID to associate the execution with a conversation thread,\n# so the conversation can be paused and resumed (as is needed for human review).\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}} \n# Run the graph until the interrupt is hit.\nresult = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Delete old records from the database\",\n            }\n        ]\n    },\n    config=config \n)\n\n# The interrupt contains the full HITL request with action_requests and review_configs\nprint(result['__interrupt__'])\n# > [\n# >    Interrupt(\n# >       value={\n# >          'action_requests': [\n# >             {\n# >                'name': 'execute_sql',\n# >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \\'30 days\\';'},\n# >                'description': 'Tool execution pending approval\\n\\nTool: execute_sql\\nArgs: {...}'\n# >             }\n# >          ],\n# >          'review_configs': [\n# >             {\n# >                'action_name': 'execute_sql',\n# >                'allowed_decisions': ['approve', 'reject']\n# >             }\n# >          ]\n# >       }\n# >    )\n# > ]\n\n\n# Resume with approval decision\nagent.invoke(\n    Command( \n        resume={\"decisions\": [{\"type\": \"approve\"}]}  # or \"reject\"\n    ), \n    config=config # Same thread ID to resume the paused conversation\n)\n\nï¿½\nDecision types\nâœ… approve\nâœï¸ edit\nâŒ reject\nUse approve to approve the tool call as-is and execute it without changes.\nCopy\nagent.invoke(\n    Command(\n        # Decisions are provided as a list, one per action under review.\n        # The order of decisions must match the order of actions\n        # listed in the `__interrupt__` request.\n        resume={\n            \"decisions\": [\n                {\n                    \"type\": \"approve\",\n                }\n            ]\n        }\n    ),\n    config=config  # Same thread ID to resume the paused conversation\n)\n\nï¿½\nStreaming with human-in-the-loop\nYou can use stream() instead of invoke() to get real-time updates while the agent runs and handles interrupts. Use stream_mode=['updates', 'messages'] to stream both agent progress and LLM tokens.\nCopy\nfrom langgraph.types import Command\n\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\n\n# Stream agent progress and LLM tokens until interrupt\nfor mode, chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Delete old records from the database\"}]},\n    config=config,\n    stream_mode=[\"updates\", \"messages\"],  \n):\n    if mode == \"messages\":\n        # LLM token\n        token, metadata = chunk\n        if token.content:\n            print(token.content, end=\"\", flush=True)\n    elif mode == \"updates\":\n        # Check for interrupt\n        if \"__interrupt__\" in chunk:\n            print(f\"\\n\\nInterrupt: {chunk['__interrupt__']}\")\n\n# Resume with streaming after human decision\nfor mode, chunk in agent.stream(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n    config=config,\n    stream_mode=[\"updates\", \"messages\"],\n):\n    if mode == \"messages\":\n        token, metadata = chunk\n        if token.content:\n            print(token.content, end=\"\", flush=True)\n\nSee the Streaming guide for more details on stream modes.\nï¿½\nExecution lifecycle\nThe middleware defines an after_model hook that runs after the model generates a response but before any tool calls are executed:\nThe agent invokes the model to generate a response.\nThe middleware inspects the response for tool calls.\nIf any calls require human input, the middleware builds a HITLRequest with action_requests and review_configs and calls interrupt.\nThe agent waits for human decisions.\nBased on the HITLResponse decisions, the middleware executes approved or edited calls, synthesizes ToolMessageâ€™s for rejected calls, and resumes execution.\nï¿½\nCustom HITL logic\nFor more specialized workflows, you can build custom HITL logic directly using the interrupt primitive and middleware abstraction.\nReview the execution lifecycle above to understand how to integrate interrupts into the agentâ€™s operation.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494367,
    "timestamp": "2026-02-25T08:09:37.412Z",
    "title": "Subagents - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent/subagents",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nDesign decisions\nSync vs. async\nSynchronous (default)\nAsynchronous\nTool patterns\nTool per agent\nSingle dispatch tool\nContext engineering\nSubagent specs\nSystem prompt enumeration\nEnum constraint on dispatch tool\nTool-based discovery\nSubagent inputs\nSubagent outputs\nCheckpointing and state inspection\nAdvanced usage\nMulti-agent\nSubagents\nCopy page\nIn the subagents architecture, a central main agent (often referred to as a supervisor) coordinates subagents by calling them as tools. The main agent decides which subagent to invoke, what input to provide, and how to combine results. Subagents are statelessâ€”they donâ€™t remember past interactions, with all conversation memory maintained by the main agent. This provides context isolation: each subagent invocation works in a clean context window, preventing context bloat in the main conversation.\n\nUser\n\nMain Agent\n\nSubagent A\n\nSubagent B\n\nSubagent C\n\nUser response\n\nï¿½\nKey characteristics\nCentralized control: All routing passes through the main agent\nNo direct user interaction: Subagents return results to the main agent, not the user (though you can use interrupts within a subagent to allow user interaction)\nSubagents via tools: Subagents are invoked via tools\nParallel execution: The main agent can invoke multiple subagents in a single turn\nSupervisor vs. Router: A supervisor agent (this pattern) is different from a router. The supervisor is a full agent that maintains conversation context and dynamically decides which subagents to call across multiple turns. A router is typically a single classification step that dispatches to agents without maintaining ongoing conversation state.\nï¿½\nWhen to use\nUse the subagents pattern when you have multiple distinct domains (e.g., calendar, email, CRM, database), subagents donâ€™t need to converse directly with users, or you want centralized workflow control. For simpler cases with just a few tools, use a single agent.\nNeed user interaction within a subagent? While subagents typically return results to the main agent rather than conversing directly with users, you can use interrupts within a subagent to pause execution and gather user input. This is useful when a subagent needs clarification or approval before proceeding. The main agent remains the orchestrator, but the subagent can collect information from the user mid-task.\nï¿½\nBasic implementation\nThe core mechanism wraps a subagent as a tool that the main agent can call:\nCopy\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n# Create a subagent\nsubagent = create_agent(model=\"anthropic:claude-sonnet-4-20250514\", tools=[...])\n\n# Wrap it as a tool\n@tool(\"research\", description=\"Research a topic and return findings\")\ndef call_research_agent(query: str):\n    result = subagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n    return result[\"messages\"][-1].content\n\n# Main agent with subagent as a tool\nmain_agent = create_agent(model=\"anthropic:claude-sonnet-4-20250514\", tools=[call_research_agent])\n\nTutorial: Build a personal assistant with subagents\nLearn how to build a personal assistant using the subagents pattern, where a central main agent (supervisor) coordinates specialized worker agents.\nLearn more\nï¿½\nDesign decisions\nWhen implementing the subagents pattern, youâ€™ll make several key design choices. This table summarizes the optionsâ€”each is covered in detail in the sections below.\nDecision\tOptions\nSync vs. async\tSync (blocking) vs. async (background)\nTool patterns\tTool per agent vs. single dispatch tool\nSubagent specs\tSystem prompt vs. enum constraint vs. tool-based discovery (single dispatch tool only)\nSubagent inputs\tQuery only vs. full context\nSubagent outputs\tSubagent result vs full conversation history\nï¿½\nSync vs. async\nSubagent execution can be synchronous (blocking) or asynchronous (background). Your choice depends on whether the main agent needs the result to continue.\nMode\tMain agent behavior\tBest for\tTradeoff\nSync\tWaits for subagent to complete\tMain agent needs result to continue\tSimple, but blocks the conversation\nAsync\tContinues while subagent runs in background\tIndependent tasks, user shouldnâ€™t wait\tResponsive, but more complex\nNot to be confused with Pythonâ€™s async/await. Here, â€œasyncâ€ means the main agent kicks off a background job (typically in a separate process or service) and continues without blocking.\nï¿½\nSynchronous (default)\nBy default, subagent calls are synchronousâ€”the main agent waits for each subagent to complete before continuing. Use sync when the main agentâ€™s next action depends on the subagentâ€™s result.\nResearch Subagent\nMain Agent\nUser\nResearch Subagent\nMain Agent\nUser\nWaiting for result...\n\"What's the weather in Tokyo?\"\nresearch(\"Tokyo weather\")\n\"Currently 72Â°F, sunny\"\n\"It's 72Â°F and sunny in Tokyo\"\nWhen to use sync:\nMain agent needs the subagentâ€™s result to formulate its response\nTasks have order dependencies (e.g., fetch data â†’ analyze â†’ respond)\nSubagent failures should block the main agentâ€™s response\nTradeoffs:\nSimple implementationâ€”just call and wait\nUser sees no response until all subagents complete\nLong-running tasks freeze the conversation\nï¿½\nAsynchronous\nUse asynchronous execution when the subagentâ€™s work is independentâ€”the main agent doesnâ€™t need the result to continue conversing with the user. The main agent kicks off a background job and remains responsive.\nContract Reviewer\nJob System\nMain Agent\nUser\nContract Reviewer\nJob System\nMain Agent\nUser\nReviewing 150+ pages...\nReview completes\n\"Review this M&A contract\"\nrun_agent(\"legal_reviewer\", task)\nStart agent\njob_id: \"job_123\"\n\"Started review (job_123)\"\n\"What's the status?\"\ncheck_status(job_id)\n\"running\"\n\"Still reviewing contract...\"\n\"Is it done yet?\"\ncheck_status(job_id)\n\"completed\"\nget_result(job_id)\nContract analysis\n\"Review complete: [findings]\"\nWhen to use async:\nSubagent work is independent of the main conversation flow\nUsers should be able to continue chatting while work happens\nYou want to run multiple independent tasks in parallel\nThree-tool pattern:\nStart job: Kicks off the background task, returns a job ID\nCheck status: Returns current state (pending, running, completed, failed)\nGet result: Retrieves the completed result\nHandling job completion: When a job finishes, your application needs to notify the user. One approach: surface a notification that, when clicked, sends a HumanMessage like â€œCheck job_123 and summarize the results.â€\nï¿½\nTool patterns\nThere are two main ways to expose subagents as tools:\nPattern\tBest for\tTrade-off\nTool per agent\tFine-grained control over each subagentâ€™s input/output\tMore setup, but more customization\nSingle dispatch tool\tMany agents, distributed teams, convention over configuration\tSimpler composition, less per-agent customization\nï¿½\nTool per agent\n\nUser\n\nMain Agent\n\nSubagent A\n\nSubagent B\n\nSubagent C\n\nUser response\n\nThe key idea is wrapping subagents as tools that the main agent can call:\nCopy\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n# Create a sub-agent\nsubagent = create_agent(model=\"...\", tools=[...])  \n\n# Wrap it as a tool  #\n@tool(\"subagent_name\", description=\"subagent_description\")  \ndef call_subagent(query: str):  \n    result = subagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n    return result[\"messages\"][-1].content\n\n# Main agent with subagent as a tool  #\nmain_agent = create_agent(model=\"...\", tools=[call_subagent])  \n\nThe main agent invokes the subagent tool when it decides the task matches the subagentâ€™s description, receives the result, and continues orchestration. See Context engineering for fine-grained control.\nï¿½\nSingle dispatch tool\nAn alternative approach uses a single parameterized tool to invoke ephemeral sub-agents for independent tasks. Unlike the tool per agent approach where each sub-agent is wrapped as a separate tool, this uses a convention-based approach with a single task tool: the task description is passed as a human message to the sub-agent, and the sub-agentâ€™s final message is returned as the tool result.\nUse this approach when you want to distribute agent development across multiple teams, need to isolate complex tasks into separate context windows, need a scalable way to add new agents without modifying the coordinator, or prefer convention over customization. This approach trades flexibility in context engineering for simplicity in agent composition and strong context isolation.\n\nresearch\n\nwriter\n\nreviewer\n\nUser\n\nMain Agent\n\ntask\nagent_name, description\n\nResearch Agent\n\nWriter Agent\n\nReviewer Agent\n\nUser response\n\nKey characteristics:\nSingle task tool: One parameterized tool that can invoke any registered sub-agent by name\nConvention-based invocation: Agent selected by name, task passed as human message, final message returned as tool result\nTeam distribution: Different teams can develop and deploy agents independently\nAgent discovery: Sub-agents can be discovered via system prompt (listing available agents) or through progressive disclosure (loading agent information on-demand via tools)\nAn interesting aspect of this approach is that sub-agents may have the exact same capabilities as the main agent. In such cases, invoking a sub-agent is really about context isolation as the primary reasonâ€”allowing complex, multi-step tasks to run in isolated context windows without bloating the main agentâ€™s conversation history. The sub-agent completes its work autonomously and returns only a concise summary, keeping the main thread focused and efficient.\n\nAgent registry with task dispatcher\n\nï¿½\nContext engineering\nControl how context flows between the main agent and its subagents:\nCategory\tPurpose\tImpacts\nSubagent specs\tEnsure subagents are invoked when they should be\tMain agent routing decisions\nSubagent inputs\tEnsure subagents can execute well with optimized context\tSubagent performance\nSubagent outputs\tEnsure the supervisor can act on subagent results\tMain agent performance\nSee also our comprehensive guide on context engineering for agents.\nï¿½\nSubagent specs\nThe names and descriptions associated with subagents are the primary way the main agent knows which subagents to invoke. These are prompting leversâ€”choose them carefully.\nName: How the main agent refers to the sub-agent. Keep it clear and action-oriented (e.g., research_agent, code_reviewer).\nDescription: What the main agent knows about the sub-agentâ€™s capabilities. Be specific about what tasks it handles and when to use it.\nFor the single dispatch tool design, you must additionally provide the main agent with information about the subagents it can invoke. You can provide this information in different ways based on the number of agents and whether your registry is static or dynamic:\nMethod\tBest for\tTradeoff\nSystem prompt enumeration\tSmall, static agent lists (< 10 agents)\tSimple, but requires prompt updates when agents change\nEnum constraint\tSmall, static agent lists (< 10 agents)\tType-safe and explicit, but requires code changes when agents change\nTool-based discovery\tLarge or dynamic agent registries\tFlexible and scalable, but adds complexity\nï¿½\nSystem prompt enumeration\nList available agents directly in the main agentâ€™s system prompt. The main agent sees the list of agents and their descriptions as part of its instructions.\nWhen to use:\nYou have a small, fixed set of agents (< 10)\nAgent registry rarely changes\nYou want the simplest implementation\nExample:\nCopy\nmain_agent = create_agent(\n    model=\"...\",\n    tools=[task],\n    system_prompt=(\n        \"You coordinate specialized sub-agents. \"\n        \"Available agents:\\n\"\n        \"- research: Research and fact-finding\\n\"\n        \"- writer: Content creation and editing\\n\"\n        \"- reviewer: Code and document review\\n\"\n        \"Use the task tool to delegate work.\"\n    ),\n)\n\nï¿½\nEnum constraint on dispatch tool\nAdd an enum constraint to the agent_name parameter in your dispatch tool. This provides type safety and makes available agents explicit in the tool schema.\nWhen to use:\nYou have a small, fixed set of agents (< 10)\nYou want type safety and explicit agent names\nYou prefer schema-based validation over prompt-based guidance\nExample:\nCopy\nfrom enum import Enum\n\nclass AgentName(str, Enum):\n    RESEARCH = \"research\"\n    WRITER = \"writer\"\n    REVIEWER = \"reviewer\"\n\n@tool\ndef task(\n    agent_name: AgentName,  # Enum constraint\n    description: str\n) -> str:\n    \"\"\"Launch an ephemeral subagent for a task.\"\"\"\n    # ...\n\nï¿½\nTool-based discovery\nProvide a separate tool (e.g., list_agents or search_agents) that the main agent can call to discover available agents on-demand. This enables progressive disclosure and supports dynamic registries.\nWhen to use:\nYou have many agents (> 10) or a growing registry\nAgent registry changes frequently or is dynamic\nYou want to reduce prompt size and token usage\nDifferent teams manage different agents independently\nExample:\nCopy\n@tool\ndef list_agents(query: str = \"\") -> str:\n    \"\"\"List available subagents, optionally filtered by query.\"\"\"\n    agents = search_agent_registry(query)\n    return format_agent_list(agents)\n\n@tool\ndef task(agent_name: str, description: str) -> str:\n    \"\"\"Launch an ephemeral subagent for a task.\"\"\"\n    # ...\n\nmain_agent = create_agent(\n    model=\"...\",\n    tools=[task, list_agents],\n    system_prompt=\"Use list_agents to discover available subagents, then use task to invoke them.\"\n)\n\nï¿½\nSubagent inputs\nCustomize what context the subagent receives to execute its task. Add input that isnâ€™t practical to capture in a static promptâ€”full message history, prior results, or task metadataâ€”by pulling from the agentâ€™s state.\nSubagent inputs example\nCopy\nfrom langchain.agents import AgentState\nfrom langchain.tools import tool, ToolRuntime\n\nclass CustomState(AgentState):\n    example_state_key: str\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\ndef call_subagent1(query: str, runtime: ToolRuntime[None, CustomState]):\n    # Apply any logic needed to transform the messages into a suitable input\n    subagent_input = some_logic(query, runtime.state[\"messages\"])\n    result = subagent1.invoke({\n        \"messages\": subagent_input,\n        # You could also pass other state keys here as needed.\n        # Make sure to define these in both the main and subagent's\n        # state schemas.\n        \"example_state_key\": runtime.state[\"example_state_key\"]\n    })\n    return result[\"messages\"][-1].content\n\nSee all 21 lines\nï¿½\nSubagent outputs\nCustomize what the main agent receives back so it can make good decisions. Two strategies:\nPrompt the sub-agent: Specify exactly what should be returned. A common failure mode is that the sub-agent performs tool calls or reasoning but doesnâ€™t include results in its final messageâ€”remind it that the supervisor only sees the final output.\nFormat in code: Adjust or enrich the response before returning it. For example, pass specific state keys back in addition to the final text using a Command.\nSubagent outputs example\nCopy\nfrom typing import Annotated\nfrom langchain.agents import AgentState\nfrom langchain.tools import InjectedToolCallId\nfrom langgraph.types import Command\n\n\n@tool(\n    \"subagent1_name\",\n    description=\"subagent1_description\"\n)\ndef call_subagent1(\n    query: str,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n) -> Command:\n    result = subagent1.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": query}]\n    })\n    return Command(update={\n        # Pass back additional state from the subagent\n        \"example_state_key\": result[\"example_state_key\"],\n        \"messages\": [\n            ToolMessage(\n                content=result[\"messages\"][-1].content,\n                tool_call_id=tool_call_id\n            )\n        ]\n    })\n\nSee all 27 lines\nï¿½\nCheckpointing and state inspection\nBy default, subagents use the inherited checkpointer mode â€” each invocation starts with fresh state, supports interrupts, and runs safely in parallel. If you need a subagent to maintain its own persistent conversation history across invocations, compile it with checkpointer=True (continuations mode). See subgraph persistence for a full comparison of modes.\nBecause subagents are called inside tool functions, LangGraph cannot statically discover them. This means get_state with subgraphs will not return subagent state. If you need to read nested graph state (e.g., during an interrupt), invoke the subagent from a node function in a custom graph instead. See subgraph persistence for details on how each mode affects state visibility.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494369,
    "timestamp": "2026-02-25T08:09:37.423Z",
    "title": "Skills - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent/skills",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nExtending the pattern\nAdvanced usage\nMulti-agent\nSkills\nCopy page\nIn the skills architecture, specialized capabilities are packaged as invokable â€œskillsâ€ that augment an agentâ€™s behavior. Skills are primarily prompt-driven specializations that an agent can invoke on-demand. For built-in skill support, see Deep Agents.\nThis pattern is conceptually identical to Agent Skills and llms.txt (introduced by Jeremy Howard), which uses tool calling for progressive disclosure of documentation. The skills pattern applies progressive disclosure to specialized prompts and domain knowledge rather than just documentation pages.\n\nUser\n\nAgent\n\nSkill A\n\nSkill B\n\nSkill C\n\nï¿½\nKey characteristics\nPrompt-driven specialization: Skills are primarily defined by specialized prompts\nProgressive disclosure: Skills become available based on context or user needs\nTeam distribution: Different teams can develop and maintain skills independently\nLightweight composition: Skills are simpler than full sub-agents\nReference awareness: Skills can reference scripts, templates, and other resources\nï¿½\nWhen to use\nUse the skills pattern when you want a single agent with many possible specializations, you donâ€™t need to enforce specific constraints between skills, or different teams need to develop capabilities independently. Common examples include coding assistants (skills for different languages or tasks), knowledge bases (skills for different domains), and creative assistants (skills for different formats).\nï¿½\nBasic implementation\nCopy\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n@tool\ndef load_skill(skill_name: str) -> str:\n    \"\"\"Load a specialized skill prompt.\n\n    Available skills:\n    - write_sql: SQL query writing expert\n    - review_legal_doc: Legal document reviewer\n\n    Returns the skill's prompt and context.\n    \"\"\"\n    # Load skill content from file/database\n    ...\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[load_skill],\n    system_prompt=(\n        \"You are a helpful assistant. \"\n        \"You have access to two skills: \"\n        \"write_sql and review_legal_doc. \"\n        \"Use load_skill to access them.\"\n    ),\n)\n\nFor a complete implementation, see the tutorial below.\nTutorial: Build a SQL assistant with on-demand skills\nLearn how to implement skills with progressive disclosure, where the agent loads specialized prompts and schemas on-demand rather than upfront.\nLearn more\nï¿½\nExtending the pattern\nWhen writing custom implementations, you can extend the basic skills pattern in several ways:\nDynamic tool registration: Combine progressive disclosure with state management to register new tools as skills load. For example, loading a â€œdatabase_adminâ€ skill could both add specialized context and register database-specific tools (backup, restore, migrate). This uses the same tool-and-state mechanisms used across multi-agent patternsâ€”tools updating state to dynamically change agent capabilities.\nHierarchical skills: Skills can define other skills in a tree structure, creating nested specializations. For instance, loading a â€œdata_scienceâ€ skill might make available sub-skills like â€œpandas_expertâ€, â€œvisualizationâ€, and â€œstatistical_analysisâ€. Each sub-skill can be loaded independently as needed, allowing for fine-grained progressive disclosure of domain knowledge. This hierarchical approach helps manage large knowledge bases by organizing capabilities into logical groupings that can be discovered and loaded on-demand.\nReference awareness: While each skill only has one prompt, this prompt can reference the location of other assets and provide information on when the agent should use those assets. When those assets become relevant, the agent will know that those files exist and read them into memory as needed to complete tasks. This also follows the progressive disclosure pattern and limits the information in the context window.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494370,
    "timestamp": "2026-02-25T08:09:37.432Z",
    "title": "Router - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent/router",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nStateless vs. stateful\nStateless\nStateful\nTool wrapper\nFull persistence\nAdvanced usage\nMulti-agent\nRouter\nCopy page\nIn the router architecture, a routing step classifies input and directs it to specialized agents. This is useful when you have distinct verticalsâ€”separate knowledge domains that each require their own agent.\n\nQuery\n\nRouter\n\nAgent A\n\nAgent B\n\nAgent C\n\nSynthesize\n\nCombined answer\n\nï¿½\nKey characteristics\nRouter decomposes the query\nZero or more specialized agents are invoked in parallel\nResults are synthesized into a coherent response\nï¿½\nWhen to use\nUse the router pattern when you have distinct verticals (separate knowledge domains that each require their own agent), need to query multiple sources in parallel, and want to synthesize results into a combined response.\nï¿½\nBasic implementation\nThe router classifies the query and directs it to the appropriate agent(s). Use Command for single-agent routing or Send for parallel fan-out to multiple agents.\nSingle agent\nMultiple agents (parallel)\nUse Command to route to a single specialized agent:\nCopy\nfrom langgraph.types import Command\n\ndef classify_query(query: str) -> str:\n    \"\"\"Use LLM to classify query and determine the appropriate agent.\"\"\"\n    # Classification logic here\n    ...\n\ndef route_query(state: State) -> Command:\n    \"\"\"Route to the appropriate agent based on query classification.\"\"\"\n    active_agent = classify_query(state[\"query\"])\n\n    # Route to the selected agent\n    return Command(goto=active_agent)\n\nFor a complete implementation, see the tutorial below.\nTutorial: Build a multi-source knowledge base with routing\nBuild a router that queries GitHub, Notion, and Slack in parallel, then synthesizes results into a coherent answer. Covers state definition, specialized agents, parallel execution with Send, and result synthesis.\nï¿½\nStateless vs. stateful\nTwo approaches:\nStateless routers address each request independently\nStateful routers maintain conversation history across requests\nï¿½\nStateless\nEach request is routed independentlyâ€”no memory between calls. For multi-turn conversations, see Stateful routers.\nRouter vs. Subagents: Both patterns can dispatch work to multiple agents, but they differ in how routing decisions are made:\nRouter: A dedicated routing step (often a single LLM call or rule-based logic) that classifies the input and dispatches to agents. The router itself typically doesnâ€™t maintain conversation history or perform multi-turn orchestrationâ€”itâ€™s a preprocessing step.\nSubagents: An main supervisor agent dynamically decides which subagents to call as part of an ongoing conversation. The main agent maintains context, can call multiple subagents across turns, and orchestrates complex multi-step workflows.\nUse a router when you have clear input categories and want deterministic or lightweight classification. Use a supervisor when you need flexible, conversation-aware orchestration where the LLM decides what to do next based on evolving context.\nï¿½\nStateful\nFor multi-turn conversations, you need to maintain context across invocations.\nï¿½\nTool wrapper\nThe simplest approach: wrap the stateless router as a tool that a conversational agent can call. The conversational agent handles memory and context; the router stays stateless. This avoids the complexity of managing conversation history across multiple parallel agents.\nCopy\n@tool\ndef search_docs(query: str) -> str:\n    \"\"\"Search across multiple documentation sources.\"\"\"\n    result = workflow.invoke({\"query\": query})  \n    return result[\"final_answer\"]\n\n# Conversational agent uses the router as a tool\nconversational_agent = create_agent(\n    model,\n    tools=[search_docs],\n    prompt=\"You are a helpful assistant. Use search_docs to answer questions.\"\n)\n\nï¿½\nFull persistence\nIf you need the router itself to maintain state, use persistence to store message history. When routing to an agent, fetch previous messages from state and selectively include them in the agentâ€™s contextâ€”this is a lever for context engineering.\nStateful routers require custom history management. If the router switches between agents across turns, conversations may not feel fluid to end users when agents have different tones or prompts. With parallel invocation, youâ€™ll need to maintain history at the router level (inputs and synthesized outputs) and leverage this history in routing logic. Consider the handoffs pattern or subagents pattern insteadâ€”both provide clearer semantics for multi-turn conversations.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494368,
    "timestamp": "2026-02-25T08:09:37.433Z",
    "title": "Handoffs - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent/handoffs",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nImplementation approaches\nSingle agent with middleware\nMultiple agent subgraphs\nContext engineering\nImplementation considerations\nAdvanced usage\nMulti-agent\nHandoffs\nCopy page\nIn the handoffs architecture, behavior changes dynamically based on state. The core mechanism: tools update a state variable (e.g., current_step or active_agent) that persists across turns, and the system reads this variable to adjust behaviorâ€”either applying different configuration (system prompt, tools) or routing to a different agent. This pattern supports both handoffs between distinct agents and dynamic configuration changes within a single agent.\nThe term handoffs was coined by OpenAI for using tool calls (e.g., transfer_to_sales_agent) to transfer control between agents or states.\nWorkflow State\nAgent\nUser\nWorkflow State\nAgent\nUser\nStep: Get warranty status\nTools: record_warranty_status\nStep: Classify issue\nTools: record_issue_type\nStep: Provide resolution\nTools: provide_solution, escalate_to_human\n\"My phone is broken\"\n\"Is your device under warranty?\"\n\"Yes, it's still under warranty\"\nrecord_warranty_status(\"in_warranty\")\n\"Can you describe the issue?\"\n\"The screen is cracked\"\nrecord_issue_type(\"hardware\")\n\"Here's the warranty repair process...\"\nï¿½\nKey characteristics\nState-driven behavior: Behavior changes based on a state variable (e.g., current_step or active_agent)\nTool-based transitions: Tools update the state variable to move between states\nDirect user interaction: Each stateâ€™s configuration handles user messages directly\nPersistent state: State survives across conversation turns\nï¿½\nWhen to use\nUse the handoffs pattern when you need to enforce sequential constraints (unlock capabilities only after preconditions are met), the agent needs to converse directly with the user across different states, or youâ€™re building multi-stage conversational flows. This pattern is particularly valuable for customer support scenarios where you need to collect information in a specific sequence â€” for example, collecting a warranty ID before processing a refund.\nï¿½\nBasic implementation\nThe core mechanism is a tool that returns a Command to update state, triggering a transition to a new step or agent:\nCopy\nfrom langchain.tools import tool\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\n@tool\ndef transfer_to_specialist(runtime) -> Command:\n    \"\"\"Transfer to the specialist agent.\"\"\"\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(  \n                    content=\"Transferred to specialist\",\n                    tool_call_id=runtime.tool_call_id  \n                )\n            ],\n            \"current_step\": \"specialist\"  # Triggers behavior change\n        }\n    )\n\nWhy include a ToolMessage? When an LLM calls a tool, it expects a response. The ToolMessage with matching tool_call_id completes this request-response cycleâ€”without it, the conversation history becomes malformed. This is required whenever your handoff tool updates messages.\nFor a complete implementation, see the tutorial below.\nTutorial: Build customer support with handoffs\nLearn how to build a customer support agent using the handoffs pattern, where a single agent transitions between different configurations.\nLearn more\nï¿½\nImplementation approaches\nThere are two ways to implement handoffs: single agent with middleware (one agent with dynamic configuration) or multiple agent subgraphs (distinct agents as graph nodes).\nï¿½\nSingle agent with middleware\nA single agent changes its behavior based on state. Middleware intercepts each model call and dynamically adjusts the system prompt and available tools. Tools update the state variable to trigger transitions:\nCopy\nfrom langchain.tools import ToolRuntime, tool\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\n@tool\ndef record_warranty_status(\n    status: str,\n    runtime: ToolRuntime[None, SupportState]\n) -> Command:\n    \"\"\"Record warranty status and transition to next step.\"\"\"\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(\n                    content=f\"Warranty status recorded: {status}\",\n                    tool_call_id=runtime.tool_call_id\n                )\n            ],\n            \"warranty_status\": status,\n            \"current_step\": \"specialist\"  # Update state to trigger transition\n        }\n    )\n\n\nComplete example: Customer support with middleware\n\nï¿½\nMultiple agent subgraphs\nMultiple distinct agents exist as separate nodes in a graph. Handoff tools navigate between agent nodes using Command.PARENT to specify which node to execute next.\nSubgraph handoffs require careful context engineering. Unlike single-agent middleware (where message history flows naturally), you must explicitly decide what messages pass between agents. Get this wrong and agents receive malformed conversation history or bloated context. See Context engineering below.\nCopy\nfrom langchain.messages import AIMessage, ToolMessage\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.types import Command\n\n@tool\ndef transfer_to_sales(\n    runtime: ToolRuntime,\n) -> Command:\n    \"\"\"Transfer to the sales agent.\"\"\"\n    last_ai_message = next(  \n        msg for msg in reversed(runtime.state[\"messages\"]) if isinstance(msg, AIMessage)  \n    )  \n    transfer_message = ToolMessage(  \n        content=\"Transferred to sales agent\",  \n        tool_call_id=runtime.tool_call_id,  \n    )  \n    return Command(\n        goto=\"sales_agent\",\n        update={\n            \"active_agent\": \"sales_agent\",\n            \"messages\": [last_ai_message, transfer_message],  \n        },\n        graph=Command.PARENT\n    )\n\n\nComplete example: Sales and support with handoffs\n\nUse single agent with middleware for most handoffs use casesâ€”itâ€™s simpler. Only use multiple agent subgraphs when you need bespoke agent implementations (e.g., a node thatâ€™s itself a complex graph with reflection or retrieval steps).\nï¿½\nContext engineering\nWith subgraph handoffs, you control exactly what messages flow between agents. This precision is essential for maintaining valid conversation history and avoiding context bloat that could confuse downstream agents. For more on this topic, see context engineering.\nHandling context during handoffs\nWhen handing off between agents, you need to ensure the conversation history remains valid. LLMs expect tool calls to be paired with their responses, so when using Command.PARENT to hand off to another agent, you must include both:\nThe AIMessage containing the tool call (the message that triggered the handoff)\nA ToolMessage acknowledging the handoff (the artificial response to that tool call)\nWithout this pairing, the receiving agent will see an incomplete conversation and may produce errors or unexpected behavior.\nThe example below assumes only the handoff tool was called (no parallel tool calls):\nCopy\n@tool\ndef transfer_to_sales(runtime: ToolRuntime) -> Command:\n    # Get the AI message that triggered this handoff\n    last_ai_message = runtime.state[\"messages\"][-1]\n\n    # Create an artificial tool response to complete the pair\n    transfer_message = ToolMessage(\n        content=\"Transferred to sales agent\",\n        tool_call_id=runtime.tool_call_id,\n    )\n\n    return Command(\n        goto=\"sales_agent\",\n        update={\n            \"active_agent\": \"sales_agent\",\n            # Pass only these two messages, not the full subagent history\n            \"messages\": [last_ai_message, transfer_message],\n        },\n        graph=Command.PARENT,\n    )\n\nWhy not pass all subagent messages? While you could include the full subagent conversation in the handoff, this often creates problems. The receiving agent may become confused by irrelevant internal reasoning, and token costs increase unnecessarily. By passing only the handoff pair, you keep the parent graphâ€™s context focused on high-level coordination. If the receiving agent needs additional context, consider summarizing the subagentâ€™s work in the ToolMessage content instead of passing raw message history.\nReturning control to the user\nWhen returning control to the user (ending the agentâ€™s turn), ensure the final message is an AIMessage. This maintains valid conversation history and signals to the user interface that the agent has finished its work.\nï¿½\nImplementation considerations\nAs you design your multi-agent system, consider:\nContext filtering strategy: Will each agent receive full conversation history, filtered portions, or summaries? Different agents may need different context depending on their role.\nTool semantics: Clarify whether handoff tools only update routing state or also perform side effects. For example, should transfer_to_sales() also create a support ticket, or should that be a separate action?\nToken efficiency: Balance context completeness against token costs. Summarization and selective context passing become more important as conversations grow longer.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494372,
    "timestamp": "2026-02-25T08:09:37.446Z",
    "title": "Retrieval - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/retrieval",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nBuilding a knowledge base\nFrom retrieval to RAG\nRetrieval pipeline\nBuilding blocks\nRAG architectures\n2-step RAG\nAgentic RAG\nHybrid RAG\nAdvanced usage\nRetrieval\nCopy page\nLarge Language Models (LLMs) are powerful, but they have two key limitations:\nFinite context â€” they canâ€™t ingest entire corpora at once.\nStatic knowledge â€” their training data is frozen at a point in time.\nRetrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of Retrieval-Augmented Generation (RAG): enhancing an LLMâ€™s answers with context-specific information.\nï¿½\nBuilding a knowledge base\nA knowledge base is a repository of documents or structured data used during retrieval.\nIf you need a custom knowledge base, you can use LangChainâ€™s document loaders and vector stores to build one from your own data.\nIf you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do not need to rebuild it. You can:\nConnect it as a tool for an agent in Agentic RAG.\nQuery it and supply the retrieved content as context to the LLM (2-Step RAG).\nSee the following tutorial to build a searchable knowledge base and minimal RAG workflow:\nTutorial: Semantic search\nLearn how to create a searchable knowledge base from your own data using LangChainâ€™s document loaders, embeddings, and vector stores. In this tutorial, youâ€™ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. Youâ€™ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\nLearn more\nï¿½\nFrom retrieval to RAG\nRetrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they integrate retrieval with generation to produce grounded, context-aware answers.\nThis is the core idea behind Retrieval-Augmented Generation (RAG). The retrieval pipeline becomes a foundation for a broader system that combines search with generation.\nï¿½\nRetrieval pipeline\nA typical retrieval workflow looks like this:\n\nSources\n(Google Drive, Slack, Notion, etc.)\n\nDocument Loaders\n\nDocuments\n\nSplit into chunks\n\nTurn into embeddings\n\nVector Store\n\nUser Query\n\nQuery embedding\n\nRetriever\n\nLLM uses retrieved info\n\nAnswer\n\nEach component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the appâ€™s logic.\nï¿½\nBuilding blocks\nDocument loaders\nIngest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized Document objects.\nLearn more\nText splitters\nBreak large docs into smaller chunks that will be retrievable individually and fit within a modelâ€™s context window.\nLearn more\nEmbedding models\nAn embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\nLearn more\nVector stores\nSpecialized databases for storing and searching embeddings.\nLearn more\nRetrievers\nA retriever is an interface that returns documents given an unstructured query.\nLearn more\nï¿½\nRAG architectures\nRAG can be implemented in multiple ways, depending on your systemâ€™s needs. We outline each type in the sections below.\nArchitecture\tDescription\tControl\tFlexibility\tLatency\tExample Use Case\n2-Step RAG\tRetrieval always happens before generation. Simple and predictable\tâœ… High\tâŒ Low\tâš¡ Fast\tFAQs, documentation bots\nAgentic RAG\tAn LLM-powered agent decides when and how to retrieve during reasoning\tâŒ Low\tâœ… High\tâ³ Variable\tResearch assistants with access to multiple tools\nHybrid\tCombines characteristics of both approaches with validation steps\tâš–ï¸ Medium\tâš–ï¸ Medium\tâ³ Variable\tDomain-specific Q&A with quality validation\nLatency: Latency is generally more predictable in 2-Step RAG, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval stepsâ€”such as API response times, network delays, or database queriesâ€”which can vary based on the tools and infrastructure in use.\nï¿½\n2-step RAG\nIn 2-Step RAG, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.\n\nUser Question\n\nRetrieve Relevant Documents\n\nGenerate Answer\n\nReturn Answer to User\n\nTutorial: Retrieval-Augmented Generation (RAG)\nSee how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation. This tutorial walks through two approaches:\nA RAG agent that runs searches with a flexible toolâ€”great for general-purpose use.\nA 2-step RAG chain that requires just one LLM call per queryâ€”fast and efficient for simpler tasks.\nLearn more\nï¿½\nAgentic RAG\nAgentic Retrieval-Augmented Generation (RAG) combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides when and how to retrieve information during the interaction.\nThe only thing an agent needs to enable RAG behavior is access to one or more tools that can fetch external knowledge â€” such as documentation loaders, web APIs, or database queries.\n\nYes\n\nNo\n\nYes\n\nNo\n\nUser Input / Question\n\nAgent (LLM)\n\nNeed external info?\n\nSearch using tool(s)\n\nEnough to answer?\n\nGenerate final answer\n\nReturn to user\n\nCopy\nimport requests\nfrom langchain.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom langchain.agents import create_agent\n\n\n@tool\ndef fetch_url(url: str) -> str:\n    \"\"\"Fetch text content from a URL\"\"\"\n    response = requests.get(url, timeout=10.0)\n    response.raise_for_status()\n    return response.text\n\nsystem_prompt = \"\"\"\\\nUse fetch_url when you need to fetch information from a web-page; quote relevant snippets.\n\"\"\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[fetch_url], # A tool for retrieval\n    system_prompt=system_prompt,\n)\n\n\nShow Extended example: Agentic RAG for LangGraph's llms.txt\n\nTutorial: Retrieval-Augmented Generation (RAG)\nSee how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation. This tutorial walks through two approaches:\nA RAG agent that runs searches with a flexible toolâ€”great for general-purpose use.\nA 2-step RAG chain that requires just one LLM call per queryâ€”fast and efficient for simpler tasks.\nLearn more\nï¿½\nHybrid RAG\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.\nTypical components include:\nQuery enhancement: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\nRetrieval validation: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\nAnswer validation: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\nThe architecture often supports multiple iterations between these steps:\n\nNo\n\nYes\n\nNo\n\nYes\n\nNo\n\nYes\n\nUser Question\n\nQuery Enhancement\n\nRetrieve Documents\n\nSufficient Info?\n\nRefine Query\n\nGenerate Answer\n\nAnswer Quality OK?\n\nTry Different Approach?\n\nReturn Best Answer\n\nReturn to User\n\nThis architecture is suitable for:\nApplications with ambiguous or underspecified queries\nSystems that require validation or quality control steps\nWorkflows involving multiple sources or iterative refinement\nTutorial: Agentic RAG with Self-Correction\nAn example of Hybrid RAG that combines agentic reasoning with retrieval and self-correction.\nLearn more\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494373,
    "timestamp": "2026-02-25T08:09:37.447Z",
    "title": "Long-term memory - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/long-term-memory",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nOverview\nMemory storage\nRead long-term memory in tools\nWrite long-term memory from tools\nAdvanced usage\nLong-term memory\nCopy page\nï¿½\nOverview\nLangChain agents use LangGraph persistence to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.\nï¿½\nMemory storage\nLangGraph stores long-term memories as JSON documents in a store.\nEach memory is organized under a custom namespace (similar to a folder) and a distinct key (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.\nThis structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\nCopy\nfrom langgraph.store.memory import InMemoryStore\n\n\ndef embed(texts: list[str]) -> list[list[float]]:\n    # Replace with an actual embedding function or LangChain embeddings object\n    return [[1.0, 2.0] * len(texts)]\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore = InMemoryStore(index={\"embed\": embed, \"dims\": 2}) \nuser_id = \"my-user\"\napplication_context = \"chitchat\"\nnamespace = (user_id, application_context) \nstore.put( \n    namespace,\n    \"a-memory\",\n    {\n        \"rules\": [\n            \"User likes short, direct language\",\n            \"User only speaks English & python\",\n        ],\n        \"my-key\": \"my-value\",\n    },\n)\n# get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\") \n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems = store.search( \n    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n)\n\nFor more information about the memory store, see the Persistence guide.\nï¿½\nRead long-term memory in tools\nA tool the agent can use to look up user information\nCopy\nfrom dataclasses import dataclass\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.store.memory import InMemoryStore\n\n\n@dataclass\nclass Context:\n    user_id: str\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore = InMemoryStore() \n\n# Write sample data to the store using the put method\nstore.put( \n    (\"users\",),  # Namespace to group related data together (users namespace for user data)\n    \"user_123\",  # Key within the namespace (user ID as key)\n    {\n        \"name\": \"John Smith\",\n        \"language\": \"English\",\n    }  # Data to store for the given user\n)\n\n@tool\ndef get_user_info(runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Look up user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store \n    user_id = runtime.context.user_id\n    # Retrieve data from store - returns StoreValue object with value and metadata\n    user_info = store.get((\"users\",), user_id) \n    return str(user_info.value) if user_info else \"Unknown user\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_user_info],\n    # Pass store to agent - enables agent to access store when running tools\n    store=store, \n    context_schema=Context\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    context=Context(user_id=\"user_123\") \n)\n\nï¿½\nWrite long-term memory from tools\nExample of a tool that updates user information\nCopy\nfrom dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.store.memory import InMemoryStore\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.\nstore = InMemoryStore() \n\n@dataclass\nclass Context:\n    user_id: str\n\n# TypedDict defines the structure of user information for the LLM\nclass UserInfo(TypedDict):\n    name: str\n\n# Tool that allows agent to update user information (useful for chat applications)\n@tool\ndef save_user_info(user_info: UserInfo, runtime: ToolRuntime[Context]) -> str:\n    \"\"\"Save user info.\"\"\"\n    # Access the store - same as that provided to `create_agent`\n    store = runtime.store \n    user_id = runtime.context.user_id \n    # Store data in the store (namespace, key, data)\n    store.put((\"users\",), user_id, user_info) \n    return \"Successfully saved user info.\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[save_user_info],\n    store=store, \n    context_schema=Context\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n    # user_id passed in context to identify whose information is being updated\n    context=Context(user_id=\"user_123\") \n)\n\n# You can access the store directly to get the value\nstore.get((\"users\",), \"user_123\").value\n\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494371,
    "timestamp": "2026-02-25T08:09:37.464Z",
    "title": "Custom workflow - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent/custom-workflow",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nOverview\nSubagents\nHandoffs\nSkills\nRouter\nCustom workflow\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nKey characteristics\nWhen to use\nBasic implementation\nExample: RAG pipeline\nAdvanced usage\nMulti-agent\nCustom workflow\nCopy page\nIn the custom workflow architecture, you define your own bespoke execution flow using LangGraph. You have complete control over the graph structureâ€”including sequential steps, conditional branches, loops, and parallel execution.\n\npath_a\n\npath_b\n\nInput\n\nConditional\n\nDeterministic step\n\nAgentic step\n\nOutput\n\nï¿½\nKey characteristics\nComplete control over graph structure\nMix deterministic logic with agentic behavior\nSupport for sequential steps, conditional branches, loops, and parallel execution\nEmbed other patterns as nodes in your workflow\nï¿½\nWhen to use\nUse custom workflows when standard patterns (subagents, skills, etc.) donâ€™t fit your requirements, you need to mix deterministic logic with agentic behavior, or your use case requires complex routing or multi-stage processing.\nEach node in your workflow can be a simple function, an LLM call, or an entire agent with tools. You can also compose other architectures within a custom workflowâ€”for example, embedding a multi-agent system as a single node.\nFor a complete example of a custom workflow, see the tutorial below.\nTutorial: Build a multi-source knowledge base with routing\nThe router pattern is an example of a custom workflow. This tutorial walks through building a router that queries GitHub, Notion, and Slack in parallel, then synthesizes results.\nLearn more\nï¿½\nBasic implementation\nThe core insight is that you can call a LangChain agent directly inside any LangGraph node, combining the flexibility of custom workflows with the convenience of pre-built agents:\nCopy\nfrom langchain.agents import create_agent\nfrom langgraph.graph import StateGraph, START, END\n\nagent = create_agent(model=\"openai:gpt-4.1\", tools=[...])\n\ndef agent_node(state: State) -> dict:\n    \"\"\"A LangGraph node that invokes a LangChain agent.\"\"\"\n    result = agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]\n    })\n    return {\"answer\": result[\"messages\"][-1].content}\n\n# Build a simple workflow\nworkflow = (\n    StateGraph(State)\n    .add_node(\"agent\", agent_node)\n    .add_edge(START, \"agent\")\n    .add_edge(\"agent\", END)\n    .compile()\n)\n\nï¿½\nExample: RAG pipeline\nA common use case is combining retrieval with an agent. This example builds a WNBA stats assistant that retrieves from a knowledge base and can fetch live news.\n\nCustom RAG workflow\n\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494375,
    "timestamp": "2026-02-25T08:09:37.465Z",
    "title": "Test - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/test",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nUnit testing\nMocking chat model\nInMemorySaver checkpointer\nIntegration testing\nInstalling AgentEvals\nTrajectory match evaluator\nLLM-as-Judge evaluator\nAsync support\nLangSmith integration\nRecording & replaying HTTP calls\nAgent development\nTest\nCopy page\nAgentic applications let an LLM decide its own next steps to solve a problem. That flexibility is powerful, but the modelâ€™s black-box nature makes it hard to predict how a tweak in one part of your agent will affect the rest. To build production-ready agents, thorough testing is essential.\nThere are a few approaches to testing your agents:\nUnit tests exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.\nIntegration tests test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable.\nAgentic applications tend to lean more on integration because they chain multiple components together and must deal with flakiness due to the nondeterministic nature of LLMs.\nï¿½\nUnit testing\nï¿½\nMocking chat model\nFor logic not requiring API calls, you can use an in-memory stub for mocking responses.\nLangChain provides GenericFakeChatModel for mocking text responses. It accepts an iterator of responses (AIMessages or strings) and returns one per invocation. It supports both regular and streaming usage.\nCopy\nfrom langchain_core.language_models.fake_chat_models import GenericFakeChatModel\n\nmodel = GenericFakeChatModel(messages=iter([\n    AIMessage(content=\"\", tool_calls=[ToolCall(name=\"foo\", args={\"bar\": \"baz\"}, id=\"call_1\")]),\n    \"bar\"\n]))\n\nmodel.invoke(\"hello\")\n# AIMessage(content='', ..., tool_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call_1', 'type': 'tool_call'}])\n\nIf we invoke the model again, it will return the next item in the iterator:\nCopy\nmodel.invoke(\"hello, again!\")\n# AIMessage(content='bar', ...)\n\nï¿½\nInMemorySaver checkpointer\nTo enable persistence during testing, you can use the InMemorySaver checkpointer. This allows you to simulate multiple turns to test state-dependent behavior:\nCopy\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model,\n    tools=[],\n    checkpointer=InMemorySaver()\n)\n\n# First invocation\nagent.invoke(\n    {\"messages\": [HumanMessage(content=\"I live in Sydney, Australia\")]},\n    config={\"configurable\": {\"thread_id\": \"session-1\"}}\n)\n\n# Second invocation: the first message is persisted (Sydney location), so the model returns GMT+10 time\nagent.invoke(\n    {\"messages\": [HumanMessage(content=\"What's my local time?\")]},\n    config={\"configurable\": {\"thread_id\": \"session-1\"}}\n)\n\nï¿½\nIntegration testing\nMany agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChainâ€™s agentevals package provides evaluators specifically designed for testing agent trajectories with live models.\nAgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a trajectory match or by using an LLM judge:\nTrajectory match\nHard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.\nIdeal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesnâ€™t require additional LLM calls.\nLLM-as-judge\nUse a LLM to qualitatively validate your agentâ€™s execution trajectory. The â€œjudgeâ€ LLM reviews the agentâ€™s decisions against a prompt rubric (which can include a reference trajectory).\nMore flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agentâ€™s trajectory without strict tool call or ordering requirements.\nï¿½\nInstalling AgentEvals\nCopy\npip install agentevals\n\nOr, clone the AgentEvals repository directly.\nï¿½\nTrajectory match evaluator\nAgentEvals offers the create_trajectory_match_evaluator function to match your agentâ€™s trajectory against a reference trajectory. There are four modes to choose from:\nMode\tDescription\tUse Case\nstrict\tExact match of messages and tool calls in the same order\tTesting specific sequences (e.g., policy lookup before authorization)\nunordered\tSame tool calls allowed in any order\tVerifying information retrieval when order doesnâ€™t matter\nsubset\tAgent calls only tools from reference (no extras)\tEnsuring agent doesnâ€™t exceed expected scope\nsuperset\tAgent calls at least the reference tools (extras allowed)\tVerifying minimum required actions are taken\n\nStrict match\n\nUnordered match\n\nSubset and superset match\n\nYou can also set the tool_args_match_mode property and/or tool_args_match_overrides to customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the repository for more details.\nï¿½\nLLM-as-Judge evaluator\nYou can also use an LLM to evaluate the agentâ€™s execution path with the create_trajectory_llm_as_judge function. Unlike the trajectory match evaluators, it doesnâ€™t require a reference trajectory, but one can be provided if available.\n\nWithout reference trajectory\n\nWith reference trajectory\n\nFor more configurability over how the LLM evaluates the trajectory, visit the repository.\nï¿½\nAsync support\nAll agentevals evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding async after create_ in the function name.\n\nAsync judge and evaluator example\n\nï¿½\nLangSmith integration\nFor tracking experiments over time, you can log evaluator results to LangSmith, a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools.\nFirst, set up LangSmith by setting the required environment variables:\nCopy\nexport LANGSMITH_API_KEY=\"your_langsmith_api_key\"\nexport LANGSMITH_TRACING=\"true\"\n\nLangSmith offers two main approaches for running evaluations: pytest integration and the evaluate function.\n\nUsing pytest integration\n\nUsing the evaluate function\n\nTo learn more about evaluating your agent, see the LangSmith docs.\nï¿½\nRecording & replaying HTTP calls\nIntegration tests that call real LLM APIs can be slow and expensive, especially when run frequently in CI/CD pipelines. We recommend using a library for recording HTTP requests and responses, then replaying them in subsequent runs without making actual network calls.\nYou can use vcrpy to achieve this. If youâ€™re using pytest, the pytest-recording plugin provides a simple way to enable this with minimal configuration. Request/responses are recorded in cassettes, which are then used to mock the real network calls in subsequent runs.\nSet up your conftest.py file to filter out sensitive information from the cassettes:\nconftest.py\nCopy\nimport pytest\n\n@pytest.fixture(scope=\"session\")\ndef vcr_config():\n    return {\n        \"filter_headers\": [\n            (\"authorization\", \"XXXX\"),\n            (\"x-api-key\", \"XXXX\"),\n            # ... other headers you want to mask\n        ],\n        \"filter_query_parameters\": [\n            (\"api_key\", \"XXXX\"),\n            (\"key\", \"XXXX\"),\n        ],\n    }\n\nThen configure your project to recognise the vcr marker:\npytest.ini\npyproject.toml\nCopy\n[pytest]\nmarkers =\n    vcr: record/replay HTTP via VCR\naddopts = --record-mode=once\n\nThe --record-mode=once option records HTTP interactions on the first run and replays them on subsequent runs.\nNow, simply decorate your tests with the vcr marker:\nCopy\n@pytest.mark.vcr()\ndef test_agent_trajectory():\n    # ...\n\nThe first time you run this test, your agent will make real network calls and pytest will generate a cassette file test_agent_trajectory.yaml in the tests/cassettes directory. Subsequent runs will use that cassette to mock the real network calls, granted the agentâ€™s requests donâ€™t change from the previous run. If they do, the test will fail and youâ€™ll need to delete the cassette and rerun the test to record fresh interactions.\nWhen you modify prompts, add new tools, or change expected trajectories, your saved cassettes will become outdated and your existing tests will fail. You should delete the corresponding cassette files and rerun the tests to record fresh interactions.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494374,
    "timestamp": "2026-02-25T08:09:37.465Z",
    "title": "LangSmith Studio - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/studio",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nPrerequisites\nSet up local Agent server\n1. Install the LangGraph CLI\n2. Prepare your agent\n3. Environment variables\n4. Create a LangGraph config file\n5. Install dependencies\n6. View your agent in Studio\nVideo guide\nAgent development\nLangSmith Studio\nCopy page\nWhen building agents with LangChain locally, itâ€™s helpful to visualize whatâ€™s happening inside your agent, interact with it in real-time, and debug issues as they occur. LangSmith Studio is a free visual interface for developing and testing your LangChain agents from your local machine.\nStudio connects to your locally running agent to show you each step your agent takes: the prompts sent to the model, tool calls and their results, and the final output. You can test different inputs, inspect intermediate states, and iterate on your agentâ€™s behavior without additional code or deployment.\nThis pages describes how to set up Studio with your local LangChain agent.\nï¿½\nPrerequisites\nBefore you begin, ensure you have the following:\nA LangSmith account: Sign up (for free) or log in at smith.langchain.com.\nA LangSmith API key: Follow the Create an API key guide.\nIf you donâ€™t want data traced to LangSmith, set LANGSMITH_TRACING=false in your applicationâ€™s .env file. With tracing disabled, no data leaves your local server.\nï¿½\nSet up local Agent server\nï¿½\n1. Install the LangGraph CLI\nThe LangGraph CLI provides a local development server (also called Agent Server) that connects your agent to Studio.\nCopy\n# Python >= 3.11 is required.\npip install --upgrade \"langgraph-cli[inmem]\"\n\nï¿½\n2. Prepare your agent\nIf you already have a LangChain agent, you can use it directly. This example uses a simple email agent:\nagent.py\nCopy\nfrom langchain.agents import create_agent\n\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email\"\"\"\n    email = {\n        \"to\": to,\n        \"subject\": subject,\n        \"body\": body\n    }\n    # ... email sending logic\n\n    return f\"Email sent to {to}\"\n\nagent = create_agent(\n    \"gpt-4.1\",\n    tools=[send_email],\n    system_prompt=\"You are an email assistant. Always use the send_email tool.\",\n)\n\nï¿½\n3. Environment variables\nStudio requires a LangSmith API key to connect your local agent. Create a .env file in the root of your project and add your API key from LangSmith.\nEnsure your .env file is not committed to version control, such as Git.\n.env\nCopy\nLANGSMITH_API_KEY=lsv2...\n\nï¿½\n4. Create a LangGraph config file\nThe LangGraph CLI uses a configuration file to locate your agent and manage dependencies. Create a langgraph.json file in your appâ€™s directory:\nlanggraph.json\nCopy\n{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"agent\": \"./src/agent.py:agent\"\n  },\n  \"env\": \".env\"\n}\n\nThe create_agent function automatically returns a compiled LangGraph graph, which is what the graphs key expects in the configuration file.\nFor detailed explanations of each key in the JSON object of the configuration file, refer to the LangGraph configuration file reference.\nAt this point, the project structure will look like this:\nCopy\nmy-app/\nâ”œâ”€â”€ src\nâ”‚   â””â”€â”€ agent.py\nâ”œâ”€â”€ .env\nâ””â”€â”€ langgraph.json\n\nï¿½\n5. Install dependencies\nInstall your project dependencies from the root directory:\npip\nuv\nCopy\npip install langchain langchain-openai \n\nï¿½\n6. View your agent in Studio\nStart the development server to connect your agent to Studio:\nCopy\nlanggraph dev\n\nSafari blocks localhost connections to Studio. To work around this, run the above command with --tunnel to access Studio via a secure tunnel.\nOnce the server is running, your agent is accessible both via API at http://127.0.0.1:2024 and through the Studio UI at https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024:\nWith Studio connected to your local agent, you can iterate quickly on your agentâ€™s behavior. Run a test input, inspect the full execution trace including prompts, tool arguments, return values, and token/latency metrics. When something goes wrong, Studio captures exceptions with the surrounding state to help you understand what happened.\nThe development server supports hot-reloadingâ€”make changes to prompts or tool signatures in your code, and Studio reflects them immediately. Re-run conversation threads from any step to test your changes without starting over. This workflow scales from simple single-tool agents to complex multi-node graphs.\nFor more information on how to run Studio, refer to the following guides in the LangSmith docs:\nRun application\nManage assistants\nManage threads\nIterate on prompts\nDebug LangSmith traces\nAdd node to dataset\nï¿½\nVideo guide\nFor more information about local and deployed agents, see Set up local Agent Server and Deploy.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494376,
    "timestamp": "2026-02-25T08:09:37.476Z",
    "title": "Agent Chat UI - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/ui",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nQuick start\nLocal development\nConnect to your agent\nAgent development\nAgent Chat UI\nCopy page\nAgent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI works seamlessly with agents created using create_agent and provides interactive experiences for your agents with minimal setup, whether youâ€™re running locally or in a deployed context (such as LangSmith).\nAgent Chat UI is open source and can be adapted to your application needs.\nYou can use generative UI in the Agent Chat UI. For more information, see Implement generative user interfaces with LangGraph.\nï¿½\nQuick start\nThe fastest way to get started is using the hosted version:\nVisit Agent Chat UI\nConnect your agent by entering your deployment URL or local server address\nStart chatting - the UI will automatically detect and render tool calls and interrupts\nï¿½\nLocal development\nFor customization or local development, you can run Agent Chat UI locally:\nUse npx\nClone repository\nCopy\n# Create a new Agent Chat UI project\nnpx create-agent-chat-app --project-name my-chat-ui\ncd my-chat-ui\n\n# Install dependencies and start\npnpm install\npnpm dev\n\nï¿½\nConnect to your agent\nAgent Chat UI can connect to both local and deployed agents.\nAfter starting Agent Chat UI, youâ€™ll need to configure it to connect to your agent:\nGraph ID: Enter your graph name (find this under graphs in your langgraph.json file)\nDeployment URL: Your Agent serverâ€™s endpoint (e.g., http://localhost:2024 for local development, or your deployed agentâ€™s URL)\nLangSmith API key (optional): Add your LangSmith API key (not required if youâ€™re using a local Agent server)\nOnce configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\nAgent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see Hiding Messages in the Chat.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494377,
    "timestamp": "2026-02-25T08:09:37.478Z",
    "title": "LangSmith Deployment - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/deploy",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nPrerequisites\nDeploy your agent\n1. Create a repository on GitHub\n2. Deploy to LangSmith\n3. Test your application in Studio\n4. Get the API URL for your deployment\n5. Test the API\nDeploy with LangSmith\nLangSmith Deployment\nCopy page\nWhen youâ€™re ready to deploy your LangChain agent to production, LangSmith provides a managed hosting platform designed for agent workloads. Traditional hosting platforms are built for stateless, short-lived web applications, while LangGraph is purpose-built for stateful, long-running agents that require persistent state and background execution. LangSmith handles the infrastructure, scaling, and operational concerns so you can deploy directly from your repository.\nï¿½\nPrerequisites\nBefore you begin, ensure you have the following:\nA GitHub account\nA LangSmith account (free to sign up)\nï¿½\nDeploy your agent\nï¿½\n1. Create a repository on GitHub\nYour applicationâ€™s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide. Then, push your code to the repository.\nï¿½\n2. Deploy to LangSmith\n1\n\nNavigate to LangSmith Deployment\n\nLog in to LangSmith. In the left sidebar, select Deployments.\n2\n\nCreate new deployment\n\nClick the + New Deployment button. A pane will open where you can fill in the required fields.\n3\n\nLink repository\n\nIf you are a first time user or adding a private repository that has not been previously connected, click the Add new account button and follow the instructions to connect your GitHub account.\n4\n\nDeploy repository\n\nSelect your applicationâ€™s repository. Click Submit to deploy. This may take about 15 minutes to complete. You can check the status in the Deployment details view.\nï¿½\n3. Test your application in Studio\nOnce your application is deployed:\nSelect the deployment you just created to view more details.\nClick the Studio button in the top right corner. Studio will open to display your graph.\nï¿½\n4. Get the API URL for your deployment\nIn the Deployment details view in LangGraph, click the API URL to copy it to your clipboard.\nClick the URL to copy it to the clipboard.\nï¿½\n5. Test the API\nYou can now test the API:\nPython\nRest API\nInstall LangGraph Python:\nCopy\npip install langgraph-sdk\n\nSend a message to the agent:\nCopy\nfrom langgraph_sdk import get_sync_client # or get_client for async\n\nclient = get_sync_client(url=\"your-deployment-url\", api_key=\"your-langsmith-api-key\")\n\nfor chunk in client.runs.stream(\n    None,    # Threadless run\n    \"agent\", # Name of agent. Defined in langgraph.json.\n    input={\n        \"messages\": [{\n            \"role\": \"human\",\n            \"content\": \"What is LangGraph?\",\n        }],\n    },\n    stream_mode=\"updates\",\n):\n    print(f\"Receiving new event of type: {chunk.event}...\")\n    print(chunk.data)\n    print(\"\\n\\n\")\n\nLangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the Platform setup overview.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494378,
    "timestamp": "2026-02-25T08:09:37.479Z",
    "title": "LangSmith Observability - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/observability",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nChangelog\nPhilosophy\nCore components\nAgents\nModels\nMessages\nTools\nShort-term memory\nStreaming\nStructured output\nMiddleware\nOverview\nPrebuilt middleware\nCustom middleware\nAdvanced usage\nGuardrails\nRuntime\nContext engineering\nModel Context Protocol (MCP)\nHuman-in-the-loop\nMulti-agent\nRetrieval\nLong-term memory\nAgent development\nLangSmith Studio\nTest\nAgent Chat UI\nDeploy with LangSmith\nDeployment\nObservability\nOn this page\nPrerequisites\nEnable tracing\nQuickstart\nTrace selectively\nLog to a project\nAdd metadata to traces\nDeploy with LangSmith\nLangSmith Observability\nCopy page\nAs you build and run agents with LangChain, you need visibility into how they behave: which tools they call, what prompts they generate, and how they make decisions. LangChain agents built with create_agent automatically support tracing through LangSmith, a platform for capturing, debugging, evaluating, and monitoring LLM application behavior.\nTraces record every step of your agentâ€™s execution, from the initial user input to the final response, including all tool calls, model interactions, and decision points. This execution data helps you debug issues, evaluate performance across different inputs, and monitor usage patterns in production.\nThis guide shows you how to enable tracing for your LangChain agents and use LangSmith to analyze their execution.\nï¿½\nPrerequisites\nBefore you begin, ensure you have the following:\nA LangSmith account: Sign up (for free) or log in at smith.langchain.com.\nA LangSmith API key: Follow the Create an API key guide.\nï¿½\nEnable tracing\nAll LangChain agents automatically support LangSmith tracing. To enable it, set the following environment variables:\nCopy\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=<your-api-key>\n\nï¿½\nQuickstart\nNo extra code is needed to log a trace to LangSmith. Just run your agent code as you normally would:\nCopy\nfrom langchain.agents import create_agent\n\n\ndef send_email(to: str, subject: str, body: str):\n    \"\"\"Send an email to a recipient.\"\"\"\n    # ... email sending logic\n    return f\"Email sent to {to}\"\n\ndef search_web(query: str):\n    \"\"\"Search the web for information.\"\"\"\n    # ... web search logic\n    return f\"Search results for: {query}\"\n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=[send_email, search_web],\n    system_prompt=\"You are a helpful assistant that can send emails and search the web.\"\n)\n\n# Run the agent - all steps will be traced automatically\nresponse = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Search for the latest AI news and email a summary to john@example.com\"}]\n})\n\nBy default, the trace will be logged to the project with the name default. To configure a custom project name, see Log to a project.\nï¿½\nTrace selectively\nYou may opt to trace specific invocations or parts of your application using LangSmithâ€™s tracing_context context manager:\nCopy\nimport langsmith as ls\n\n# This WILL be traced\nwith ls.tracing_context(enabled=True):\n    agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send a test email to alice@example.com\"}]})\n\n# This will NOT be traced (if LANGSMITH_TRACING is not set)\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send another email\"}]})\n\nï¿½\nLog to a project\n\nStatically\n\nDynamically\n\nï¿½\nAdd metadata to traces\nYou can annotate your traces with custom metadata and tags:\nCopy\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]},\n    config={\n        \"tags\": [\"production\", \"email-assistant\", \"v1.0\"],\n        \"metadata\": {\n            \"user_id\": \"user_123\",\n            \"session_id\": \"session_456\",\n            \"environment\": \"production\"\n        }\n    }\n)\n\ntracing_context also accepts tags and metadata for fine-grained control:\nCopy\nwith ls.tracing_context(\n    project_name=\"email-agent-test\",\n    enabled=True,\n    tags=[\"production\", \"email-assistant\", \"v1.0\"],\n    metadata={\"user_id\": \"user_123\", \"session_id\": \"session_456\", \"environment\": \"production\"}):\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]}\n    )\n\nThis custom metadata and tags will be attached to the trace in LangSmith.\nTo learn more about how to use traces to debug, evaluate, and monitor your agents, see the LangSmith documentation.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494413,
    "timestamp": "2026-02-25T08:10:59.755Z",
    "title": "LangChain Python integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/overview",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nLangChain Python integrations\nCopy page\n\nIntegrate with providers using LangChain Python.\n\nLangChain offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more.\nA provider is a third-party service or platform that LangChain integrates with to access AI capabilities like chat models, embeddings, and vector stores. These providers have standalone langchain-provider packages for improved versioning, dependency management, and testing.\nChat models\nEmbedding models\nTools and toolkits\nSandboxes\nTo see a full list of integrations by component type, refer to the categories in the sidebar.\nï¿½\nPopular providers\nProvider\tPackage\tDownloads\tLatest version\tJS/TS support\nOpenAI\tlangchain-openai\t\n\t\n\tâœ…\nGoogle (Vertex AI)\tlangchain-google-vertexai\t\n\t\n\tâœ…\nAnthropic (Claude)\tlangchain-anthropic\t\n\t\n\tâœ…\nAWS\tlangchain-aws\t\n\t\n\tâœ…\nGoogle (GenAI)\tlangchain-google-genai\t\n\t\n\tâœ…\nOllama\tlangchain-ollama\t\n\t\n\tâœ…\nGroq\tlangchain-groq\t\n\t\n\tâœ…\nHuggingface\tlangchain-huggingface\t\n\t\n\tâœ…\nDatabricks\tdatabricks-langchain\t\n\t\n\tâœ…\nChroma\tlangchain-chroma\t\n\t\n\tâœ…\nPostgres\tlangchain-postgres\t\n\t\n\tâœ…\nPinecone\tlangchain-pinecone\t\n\t\n\tâœ…\nMistralAI\tlangchain-mistralai\t\n\t\n\tâœ…\nCohere\tlangchain-cohere\t\n\t\n\tâœ…\nFireworks\tlangchain-fireworks\t\n\t\n\tâœ…\nMongoDB\tlangchain-mongodb\t\n\t\n\tâœ…\nxAI (Grok)\tlangchain-xai\t\n\t\n\tâœ…\nDeepSeek\tlangchain-deepseek\t\n\t\n\tâœ…\nPerplexity\tlangchain-perplexity\t\n\t\n\tâœ…\nAzure AI\tlangchain-azure-ai\t\n\t\n\tâœ…\nNvidia AI Endpoints\tlangchain-nvidia-ai-endpoints\t\n\t\n\tâŒ\nIBM\tlangchain-ibm\t\n\t\n\tâœ…\nTavily\tlangchain-tavily\t\n\t\n\tâœ…\nQdrant\tlangchain-qdrant\t\n\t\n\tâœ…\nMilvus\tlangchain-milvus\t\n\t\n\tâœ…\nElasticsearch\tlangchain-elasticsearch\t\n\t\n\tâœ…\nLiteLLM\tlangchain-litellm\t\n\t\n\tN/A\nDataStax Astra DB\tlangchain-astradb\t\n\t\n\tâœ…\nRedis\tlangchain-redis\t\n\t\n\tâœ…\nTogether\tlangchain-together\t\n\t\n\tâœ…\nMCP Toolbox (Google)\ttoolbox-langchain\t\n\t\n\tâŒ\nOpenRouter\tlangchain-openrouter\t\n\t\n\tâŒ\nGoogle (Community)\tlangchain-google-community\t\n\t\n\tâŒ\nUnstructured\tlangchain-unstructured\t\n\t\n\tâœ…\nNeo4J\tlangchain-neo4j\t\n\t\n\tâœ…\nGraph RAG\tlangchain-graph-retriever\t\n\t\n\tâŒ\nï¿½\nAll providers\nSee all providers or search for a provider using the search field.\nCommunity integrations can be found in langchain-community.\nIf youâ€™d like to contribute an integration, see the contributing guide.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494415,
    "timestamp": "2026-02-25T08:10:59.761Z",
    "title": "OpenAI integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/openai",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nModel interfaces\nTools and toolkits\nRetrievers\nDocument loaders\nOther\nPopular Providers\nOpenAI integrations\nCopy page\n\nIntegrate with OpenAI using LangChain Python.\n\nThis page covers all LangChain integrations with OpenAI\nï¿½\nModel interfaces\nChatOpenAI\nOpenAI chat models.\nGet started\nAzureChatOpenAI\nWrapper for OpenAI chat models hosted on Azure.\nGet started\nOpenAIEmbeddings\nOpenAI embedding models.\nGet started\nAzureOpenAIEmbeddings\nWrapper for OpenAI embedding models hosted on Azure.\nGet started\nï¿½\nTools and toolkits\nDall-E Image Generator\nText-to-image generation using OpenAIâ€™s Dall-E models.\nGet started\nï¿½\nRetrievers\nChatGPTPluginRetriever\nRetrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.\nGet started\nï¿½\nDocument loaders\nChatGPTLoader\nLoad conversations.json from your ChatGPT data export folder.\nGet started\nï¿½\nOther\nOpenAI\n(Legacy) OpenAI text completion models.\nGet started\nAzureOpenAI\nWrapper for (legacy) OpenAI text completion models hosted on Azure.\nGet started\nAdapter\nAdapt LangChain models to OpenAI APIs.\nGet started\nOpenAIModerationChain\nDetect text that could be hateful, violent, etc.\nGet started\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494414,
    "timestamp": "2026-02-25T08:10:59.763Z",
    "title": "All LangChain Python integration providers - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/all_providers",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nAll LangChain Python integration providers\nCopy page\nBrowse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more.\nï¿½\nProviders\nAbso\nCustom AI integration platform for enterprise workflows.\nAcreom\nKnowledge management platform with AI-powered organization.\nActiveLoop DeepLake\nVector database for AI applications with deep learning focus.\nAds4GPTs\nAdvertising platform for GPT applications and AI services.\nAG-UI Protocol\nOpen event-based protocol for connecting LangGraph agents to any frontend.\nAgentQL\nWeb scraping with natural language queries.\nAgentSystems\nGovernance infrastructure for AI systems.\nAI21\nAI21 Labsâ€™ Jurassic models for text generation.\nAIM Tracking\nExperiment tracking and management platform.\nAI/ML API\nUnified API for multiple AI and ML services.\nAI Network\nDecentralized AI computing network platform.\nAirbyte\nData integration platform for ETL and ELT pipelines.\nAirtable\nCloud-based spreadsheet and database platform.\nAlchemy\nBlockchain development platform and APIs.\nAleph Alpha\nEuropean AI companyâ€™s multilingual language models.\nAlibaba Cloud\nAlibabaâ€™s cloud computing and AI services.\nAmpersend\nPayment infrastructure for AI agent services via x402 protocol.\nAnalyticDB\nAlibaba Cloudâ€™s real-time analytics database.\nAnchor Browser\nBrowser automation and web scraping tools.\nAnnoy\nApproximate nearest neighbors search library.\nAnthropic\nClaude models for advanced reasoning and conversation.\nany-llm\nUnified interface for OpenAI, Anthropic, Google, and local models.\nAnyscale\nDistributed computing platform for ML workloads.\nApache Doris\nReal-time analytical database management system.\nApache\nApache Software Foundation tools and libraries.\nApify\nWeb scraping and automation platform.\nApple\nAppleâ€™s machine learning and AI frameworks.\nArangoDB\nMulti-model database with graph capabilities.\nArcee\nDomain-specific language model training platform.\nArcGIS\nGeographic information system platform.\nArgilla\nData labeling and annotation platform for NLP.\nArize\nML observability and performance monitoring.\nArthur Tracking\nAI model monitoring and governance platform.\narXiv\nAcademic paper repository and search platform.\nAscend\nData engineering and pipeline automation platform.\nAsk News\nReal-time news search and analysis API.\nAssemblyAI\nSpeech-to-text and audio intelligence API.\nassistant-ui\nReact framework for building AI chat interfaces with streaming support and LangGraph integration.\nAstraDB\nDataStax Astra DB vector database platform.\nAtlas\nData visualization and exploration platform.\nAwaDB\nVector database for AI and ML applications.\nAWS\nAmazon Web Services cloud platform and AI services.\nAZLyrics\nSong lyrics database and search platform.\nAzure AI\nMicrosoft Azure AI and cognitive services.\nBAAI\nBeijing Academy of AI research and models.\nBagel\nVector database and semantic search platform.\nBagelDB\nMulti-modal AI database and storage system.\nBaichuan\nChinese language model from Baichuan AI.\nBaidu\nBaiduâ€™s AI services and language models.\nBananaDev\nServerless GPU infrastructure for ML models.\nBaseten\nML model deployment and serving platform.\nBeam\nServerless GPU computing platform.\nBeautiful Soup\nHTML and XML parsing library for web scraping.\nBibTeX\nBibliography management and citation format.\nBilibili\nChinese video sharing platform integration.\nBittensor\nDecentralized AI network and incentive protocol.\nBlackboard\nEducational technology and learning management.\nBodo DataFrames\nHigh-performance analytics and data processing.\nBookendAI\nAI-powered reading and research assistant.\nBox\nCloud content management and collaboration.\nBrave Search\nPrivacy-focused search engine API.\nBreebs\nAI knowledge management and retrieval platform.\nBrightdata\nWeb data platform and proxy services.\nBrowserbase\nHeadless browser automation platform.\nBrowserless\nServerless browser automation service.\nByteDance\nByteDanceâ€™s AI models and services.\nCAMB AI\nMultilingual audio and localization services supporting 140+ languages.\nCassandra\nDistributed NoSQL database management system.\nCerebras\nAI compute platform with specialized processors.\nCerebriumAI\nServerless GPU platform for AI applications.\nChaindesk\nNo-code AI chatbot and automation platform.\nChroma\nOpen-source embedding database for AI apps.\nClarifai\nComputer vision and AI model platform.\nClearML Tracking\nML experiment tracking and automation.\ncloro\nThe scraper for SEO and AI SEO.\nCopilotKit\nReact framework with pre-built UI components for AI copilots.\nClickHouse\nFast columnar database for analytics.\nClickUp\nProject management and productivity platform.\nCloudflare\nWeb infrastructure and security services.\nClova\nNaverâ€™s AI assistant and NLP platform.\nCnosDB\nTime series database for IoT and analytics.\nCockroachDB\nDistributed SQL database with native vector support and C-SPANN indexes.\nCognee\nMemory layer for AI applications and agents.\nCogniSwitch\nAI knowledge management and retrieval system.\nCohere\nLanguage AI platform for enterprise applications.\nCollege Confidential\nCollege admissions and education platform.\nComet Tracking\nML experiment tracking and model management.\nConfident\nAI observability and monitoring platform.\nConfluence\nTeam collaboration and documentation platform.\nConnery\nPlugin system for AI agents and applications.\nContext\nContext management for AI applications.\nContextual\nContextual AI and language understanding.\nCouchbase\nNoSQL cloud database platform.\nCoze\nConversational AI platform and chatbot builder.\nCrateDB\nDistributed SQL database for machine data.\nCTransformers\nPython bindings for transformer models in C/C++.\nCTranslate2\nFast inference engine for Transformer models.\nCube\nSemantic layer for building data applications.\nDappier\nReal-time AI data platform and API.\nDashVector\nAlibaba Cloudâ€™s vector database service.\nDatabricks\nUnified analytics platform for big data and ML.\nDatadog\nMonitoring and analytics platform for applications.\nDatadog Logs\nLog management and analysis platform.\nDataForSEO\nSEO and SERP data API platform.\nDataHerald\nNatural language to SQL query platform.\nDaytona\nSecure and elastic infrastructure for running your AI-generated code.\nDedoc\nDocument analysis and structure detection.\nDeepInfra\nServerless inference for deep learning models.\nDeepLake\nVector database for deep learning applications.\nDeepSeek\nAdvanced reasoning and coding AI models.\nDeepSparse\nInference runtime for sparse neural networks.\nDell\nDell Technologies AI and computing solutions.\nDiffbot\nWeb data extraction and knowledge graph.\nDingo\nDistributed vector database system.\nDiscord\nCommunication platform integration and bots.\nDiscord Shikenso\nDiscord analytics and moderation tools.\nDocArray\nData structure for multimodal AI applications.\nDocling\nDocument processing and AI integration.\nDoctran\nDocument transformation and processing.\nDocugami\nDocument AI and semantic processing.\nDocusaurus\nDocumentation website generator and platform.\nDria\nDecentralized knowledge retrieval network.\nDropbox\nCloud storage and file sharing platform.\nDuckDB\nIn-process SQL OLAP database management system.\nDuckDuckGo Search\nPrivacy-focused search engine integration.\nE2B\nCloud development environment platform.\nEdenAI\nUnified API for multiple AI services.\nEgnyte\nCloud content collaboration and AI-powered search platform.\nElasticsearch\nDistributed search and analytics engine.\nElevenLabs\nAI voice synthesis and speech platform.\nEmbedChain\nFramework for creating RAG applications.\nEpsilla\nVector database for AI and ML applications.\nEtherscan\nEthereum blockchain explorer and analytics.\nEverlyAI\nServerless AI inference platform.\nEvernote\nNote-taking and organization platform.\nExa Search\nAI-powered search engine for developers.\nFacebook\nMetaâ€™s social platform integration and APIs.\nFalkorDB\nGraph database with ultra-low latency.\nFauna\nServerless, globally distributed database.\nFeatherless AI\nFast and efficient AI model serving.\nFiddler\nAI observability and monitoring platform.\nFigma\nDesign collaboration and prototyping platform.\nFireCrawl\nWeb scraping and crawling API service.\nFireworks\nFast inference platform for open-source models.\nFlyte\nWorkflow orchestration for ML and data processing.\nFMP Data\nFinancial market data and analytics API.\nForefrontAI\nFine-tuning platform for language models.\nFriendli\nOptimized serving engine for AI models.\nGalaxia\nPrompt-driven engineering assistant.\nGel\nKnowledge extraction and NLP platform.\nGeoPandas\nGeographic data analysis with Python.\nGit\nVersion control system integration.\nGitBook\nDocumentation platform and knowledge base.\nGitHub\nCode hosting and collaboration platform.\nGitLab\nDevOps platform and code repository.\nGOAT\nTool use framework for AI agents.\nGolden\nKnowledge graph and data platform.\nGoogle\nGoogleâ€™s AI services and cloud platform.\nGoogle Serper\nGoogle Search API service.\nGooseAI\nFully managed NLP-as-a-Service platform.\nGPT4All\nOpen-source LLM ecosystem for local deployment.\nGradient\nAI model training and deployment platform.\nDigitalOcean Gradient AI Platform\nSingle endpoint to multiple LLMs via serverless inference.\nGraph RAG\nGraph-based retrieval augmented generation.\nGraphSignal\nAI observability and monitoring platform.\nGreenNode\nSustainable AI computing platform.\nGROBID\nMachine learning library for bibliographic data.\nGroq\nUltra-fast inference with specialized hardware.\nGutenberg\nProject Gutenberg digital library access.\nHacker News\nTech news and discussion platform.\nHazy Research\nMachine learning research and tools.\nHelicone\nLLM observability and monitoring platform.\nHologres\nReal-time interactive analytics service.\nHTML2Text\nHTML to plain text conversion utility.\nHuawei\nHuawei Cloud AI services and models.\nHugging Face\nOpen platform for ML models and datasets.\nHyperBrowser\nWeb automation and scraping platform.\nIBM\nIBM Watson AI and enterprise solutions.\nIEIT Systems\nEnterprise AI and system integration.\niFixit\nRepair guides and technical documentation.\niFlytek\nChinese speech and language AI platform.\nIMSDb\nInternet Movie Script Database access.\nInfinispanVS\nDistributed cache and data grid platform.\nInfinity\nHigh-performance embedding inference server.\nInfino\nObservability and monitoring platform.\nIntel\nIntelâ€™s AI optimization tools and libraries.\nIsaacus\nLegal AI models, apps, and data.\nIUGU\nBrazilian payment processing platform.\nJaguar\nVector database and search platform.\nJavelin AI Gateway\nAI model gateway and management platform.\nJenkins\nAutomation server and CI/CD platform.\nJina\nNeural search framework and cloud platform.\nJohn Snow Labs\nEnterprise NLP and healthcare AI platform.\nJoplin\nOpen-source note taking and organization.\nKDB.AI\nTime-series vector database platform.\nKinetica\nReal-time analytics and database platform.\nKoboldAI\nBrowser-based AI writing assistant.\nKonko\nGenerative AI platform and model hosting.\nKoNLPy\nKorean natural language processing toolkit.\nKuzu\nEmbedded graph database management system.\nLabel Studio\nData labeling and annotation platform.\nLakeFS\nGit-like version control for data lakes.\nLambdaDB\nServerless database for RAG and Agents\nLanceDB\nDeveloper-friendly embedded vector database.\nLangChain Decorators\nSyntactic sugar and utilities for LangChain.\nLangFair\nBias testing framework for language models.\nLangFuse\nLLM engineering platform and observability.\nLantern\nPostgreSQL vector database extension.\nLindorm\nAlibaba Cloudâ€™s multi-model database service.\nLinkUp\nReal-time job market data and search.\nLiteLLM\nUnified interface for 100+ LLM APIs.\nLlamaIndex\nData framework for LLM applications.\nLlamaCPP\nPort of Metaâ€™s LLaMA model in C/C++.\nLlamaEdge\nEdge computing platform for LLaMA models.\nLlamaFile\nSingle-file executable for running LLMs.\nLLMonitor\nObservability platform for LLM applications.\nLocalAI\nSelf-hosted OpenAI-compatible API server.\nLog10\nLLM data management and observability.\nMariaDB\nOpen-source relational database management.\nMaritALK\nBrazilian Portuguese language model.\nMarqo\nEnd-to-end vector search engine.\nMediaWiki Dump\nWikipedia and MediaWiki data processing.\nMeilisearch\nLightning-fast search engine platform.\nMemcached\nDistributed memory caching system.\nMemgraph\nReal-time graph database platform.\nMetal\nManaged vector search and retrieval.\nMicrosoft\nMicrosoft Azure AI and enterprise services.\nMilvus\nOpen-source vector database for AI applications.\nMindsDB\nAI layer for databases and data platforms.\nMinimax\nChinese AI companyâ€™s language models.\nMistralAI\nEfficient open-source language models.\nMLflow\nML lifecycle management platform.\nMLflow Tracking\nExperiment tracking and model registry.\nMLX\nAppleâ€™s machine learning framework.\nModal\nServerless cloud computing for data science.\nModelScope\nAlibabaâ€™s open-source model hub.\nModern Treasury\nPayment operations and treasury management.\nMomento\nServerless cache and vector index.\nMongoDB\nDocument-based NoSQL database platform.\nMongoDB Atlas\nCloud-hosted MongoDB with vector search.\nMotherDuck\nServerless analytics with DuckDB in the cloud.\nMotorhead\nLong-term memory for AI conversations.\nMyScale\nSQL-compatible vector database platform.\nNaver\nNaverâ€™s AI services and language models.\nNebius\nAI cloud platform and infrastructure.\nNeo4j\nNative graph database and analytics platform.\nNetMind\nDecentralized AI computing network.\nNimble\nWeb intelligence and data extraction.\nNLP Cloud\nProduction-ready NLP API platform.\nNomic\nOpen-source embedding models and tools.\nNotion\nAll-in-one workspace and collaboration platform.\nNuclia\nAI-powered search and understanding platform.\nNVIDIA\nNVIDIAâ€™s AI computing platform and models.\nObsidian\nConnected note-taking and knowledge management.\nOceanBase\nDistributed relational database system.\nOCI\nOracle Cloud Infrastructure AI services.\nOctoAI\nEfficient AI compute and model serving.\nOllama\nRun Large Language Models (LLMs) locally.\nOntotext GraphDB\nRDF database and semantic graph platform.\nOpenAI\nGPT models and comprehensive AI platform.\nOpenDataLoader PDF\nSafe, Open, High-Performance â€” PDF for AI\nOpenGradient\nAI model training and fine-tuning platform.\nOpenLLM\nOperating LLMs in production environment.\nOpenRouter\nUnified API for models from multiple providers.\nOpenSearch\nDistributed search and analytics suite.\nOpenWeatherMap\nWeather data and forecasting API.\nOpen Agent Spec\nFramework-agnostic language for portable agent definitions.\nOracle AI Database\nOracleâ€™s AI and machine learning services.\nOutline\nTeam knowledge base and wiki platform.\nOutlines\nStructured generation for language models.\nOxylabs\nWeb scraping and proxy services.\nPandas\nData analysis and manipulation library.\nParallel\nAI-powered web search and content extraction for LLMs.\nPerigon\nReal-time news and media monitoring.\nPermit\nAuthorization and access control platform.\nPerplexity\nAI-powered search and reasoning engine.\nPetals\nDistributed inference for Large Language Models.\nPlainId\nAuthorization and access control platform.\nPG Embedding\nPostgreSQL vector embedding extensions.\npgvector\nVector similarity search for PostgreSQL.\nPinecone\nManaged vector database for ML applications.\nPipelineAI\nML pipeline and model deployment platform.\nPipeshift\nAI-powered content moderation platform.\nPolarisAIDataInsight\nDocument-loaders for various file formats.\nPortkey\nAI gateway and observability platform.\nPredibase\nFine-tuning platform for Large Language Models.\nPredictionGuard\nAI model security and compliance platform.\nPreMAI\nAI platform for model deployment and management.\nPrivy\nWallets and payments for AI agents.\nProlog\nLogic programming language integration.\nPromptLayer\nPrompt engineering and observability platform.\nPsychic\nUniversal API for SaaS integrations.\nPubMed\nBiomedical literature database access.\nPygmalionAI\nConversational AI model platform.\nPyMuPDF4LLM\nPDF processing optimized for LLM ingestion.\nQdrant\nVector similarity search engine.\nRagatouille\nRAG toolkit with ColBERT indexing.\nRank BM25\nBM25 ranking algorithm implementation.\nRay Serve\nScalable model serving framework.\nRebuff\nPrompt injection detection and prevention.\nReddit\nSocial media platform integration and APIs.\nRedis\nIn-memory data structure store and cache.\nRemembrall\nAI memory and context management.\nReplicate\nCloud platform for running ML models.\nRoam\nResearch and note-taking platform.\nRobocorp\nPython automation and RPA platform.\nRockset\nReal-time analytics database platform.\nRunPod\nGPU cloud platform for AI workloads.\nSalesforce\nCRM platform and business automation.\nSambaNova\nAI platform with specialized hardware.\nSAP\nEnterprise software and AI solutions.\nScrapeGraph\nAI-powered web scraping framework.\nScrapeless\nWeb scraping API and proxy service.\nSearchAPI\nReal-time search engine results API.\nSearX\nPrivacy-respecting metasearch engine.\nSemaDB\nVector database for semantic search.\nSerpApi\nGoogle Search results scraping API.\nShale Protocol\nDecentralized AI inference protocol.\nSingleStore\nDistributed database with vector capabilities.\nscikit-learn\nMachine learning library for Python.\nSlack\nBusiness communication and collaboration.\nSnowflake\nCloud data platform and analytics.\nSoniox\nTranscribe multilingual audio using Sonioxâ€™s transcription API.\nspaCy\nIndustrial-strength NLP library.\nSpark\nUnified analytics engine for big data.\nSparkLLM\niFlytekâ€™s multilingual language model.\nSpreedly\nPayment orchestration platform.\nSQLite\nEmbedded relational database engine.\nStackExchange\nQ&A platform network integration.\nStardog\nEnterprise knowledge graph platform.\nStarRocks\nHigh-performance analytical database.\nStochasticAI\nGPU cloud platform for ML acceleration.\nStreamlit\nWeb app framework for data science.\nStripe\nOnline payment processing platform.\nSupabase\nOpen-source Firebase alternative.\nSupadata\nLoad transcripts and metadata from YouTube, TikTok, and more.\nSurrealDB\nMulti-model database for modern applications.\nSymbl.ai Nebula\nConversation intelligence platform.\nTableau\nData visualization and business intelligence.\nTaiga\nProject management platform for agile teams.\nTair\nAlibaba Cloudâ€™s in-memory database.\nTavily\nAI-optimized search API for applications.\nTelegram\nMessaging platform and bot integration.\nTencent\nTencent Cloud AI services and models.\nTensorFlow Datasets\nCollection of ready-to-use datasets.\nTensorLake\nData infrastructure for ML applications.\nTeradata\nAutonomous AI platform with integrated vector search.\nTiDB\nDistributed SQL database platform.\nTigerGraph\nScalable graph database and analytics.\nTigris\nGlobally distributed database platform.\nTilores\nEntity resolution and data matching.\nTimbr\nSemantic layer for data integration and querying.\nTogether\nFast inference for open-source models.\nToMarkdown\nHTML to Markdown conversion utility.\nToolbox LangChain\nExtended toolkit for LangChain applications.\nTranswarp\nBig data platform and analytics suite.\nTrello\nVisual project management and collaboration.\nTrubrics\nLLM evaluation and analytics platform.\nTrueFoundry\nML platform for model deployment.\nTrueLens\nEvaluation framework for LLM applications.\nTwitter\nSocial media platform integration.\nTypesense\nFast and typo-tolerant search engine.\nUnDatasIO\nData extraction and processing platform.\nUnstructured\nDocument processing and data extraction.\nUpstage\nDocument AI and OCR platform.\nUpstash\nServerless data platform for Redis and Kafka.\nUpTrain\nML observability and evaluation platform.\nUSearch\nSingle-file vector search engine.\nValthera\nAI platform for healthcare applications.\nValyu\nAI-powered data analysis platform.\nVDMS\nVisual data management system.\nVearch\nDistributed vector search engine.\nVectara\nNeural search platform with built-in understanding.\nVectorize\nVector database and semantic search.\nVespa\nBig data serving engine for vector search.\nVLite\nSimple vector database for embeddings.\nVoyageAI\nEmbedding models and semantic search.\nWeights & Biases\nML experiment tracking and collaboration.\nWeights & Biases Tracking\nExperiment tracking and model management.\nWeights & Biases Tracing\nLLM tracing and observability.\nWeather\nWeather data and forecasting services.\nWeaviate\nOpen-source vector database with GraphQL.\nWhatsApp\nMessaging platform integration and automation.\nWhyLabs Profiling\nAI observability and data monitoring.\nWikipedia\nWikipedia content access and search.\nWolfram Alpha\nComputational knowledge engine.\nWRITER\nEnterprise models and tools for building, activating, and supervising AI agents.\nXAI\nxAIâ€™s Grok models for conversational AI.\nXata\nServerless database with vector search.\nXinference\nDistributed inference framework for LLMs.\nYahoo\nYahoo services and data integration.\nYandex\nYandex AI services and language models.\nYDB\nYandex Database distributed storage system.\nYeagerAI\nAI agent framework and development platform.\nYellowbrick\nData warehouse and analytics platform.\nYi\n01.AIâ€™s bilingual language models.\nYou\nYou.com search engine and AI platform.\nYouTube\nVideo platform integration and content access.\nZep\nLong-term memory for AI assistants.\nZeusDB\nHigh-performance vector database.\nZhipuAI\nChatGLM and other Chinese language models.\nZilliz\nManaged Milvus vector database service.\nZotero\nReference management and research tool.\nZvec\nLightweight, lightning-fast, in-process vector database.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494416,
    "timestamp": "2026-02-25T08:10:59.769Z",
    "title": "Anthropic (Claude) integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/anthropic",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nModel interfaces\nOther\nPopular Providers\nAnthropic (Claude) integrations\nCopy page\n\nIntegrate with Anthropic (Claude) using LangChain Python.\n\nThis page covers all LangChain integrations with Anthropic, the makers of Claude.\nï¿½\nModel interfaces\nChatAnthropic\nAnthropic chat models.\nGet started\nAnthropic middleware\nAnthropic-specific middleware for Claude models.\nGet started\nï¿½\nOther\nAnthropicLLM\n(Legacy) Anthropic text completion models.\nGet started\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494417,
    "timestamp": "2026-02-25T08:10:59.772Z",
    "title": "Google integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/google",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nGoogle Generative AI\nChat models\nLLMs\nEmbedding models\nGoogle Cloud\nChat models\nLLMs\nEmbedding models\nDocument loaders\nDocument transformers\nVector stores\nRetrievers\nTools\nCallbacks\nEvaluators\nOther Google products\nDocument loaders\nVector stores\nRetrievers\nTools\nMCP\nToolkits\nChat loaders\n3rd party integrations\nSearch\nYouTube\nPopular Providers\nGoogle integrations\nCopy page\n\nIntegrate with Google using LangChain Python.\n\nThis page covers all LangChain integrations with Google Gemini, Google Cloud, and other Google products (such as Google Maps, YouTube, and more).\nUnified SDK & package consolidation\nAs of langchain-google-genai 4.0.0, this package uses the consolidated google-genai SDK and now supports both the Gemini Developer API and Vertex AI backends.\nThe langchain-google-vertexai package remains supported for Vertex AI platform-specific features (Model Garden, Vector Search, evaluation services, etc.).\nRead the full announcement and migration guide.\nNot sure which package to use?\n\nGoogle Generative AI (Gemini API & Vertex AI)\n\nGoogle Cloud (Vertex AI Platform Services)\n\nSee Googleâ€™s guide on migrating from the Gemini API to Vertex AI for more details on the differences.\nï¿½\nGoogle Generative AI\nAccess Google Gemini models via the Gemini Developer API or Vertex AI using the unified langchain-google-genai package.\nï¿½\nChat models\nChatGoogleGenerativeAI\nGoogle Gemini chat models via Gemini Developer API or Vertex AI.\nGet started\nï¿½\nLLMs\nGoogleGenerativeAI\nGemini models using the (legacy) LLM text completion interface.\nGet started\nï¿½\nEmbedding models\nGoogleGenerativeAIEmbeddings\nGemini embedding models via Gemini Developer API or Vertex AI.\nGet started\nï¿½\nGoogle Cloud\nAccess Vertex AI platform-specific services including Model Garden (Llama, Mistral, Anthropic), Vector Search, evaluation services, and specialized vision models.\nFor Gemini models, use ChatGoogleGenerativeAI from langchain-google-genai. The classes below focus on Vertex AI platform services not available in the consolidated SDK.\nï¿½\nChat models\nChatAnthropicVertex\nAnthropic on Vertex AI Model Garden\nGet started\n\nChatVertexAI (deprecated)\n\nVertexModelGardenLlama\n\nVertexModelGardenMistral\n\nGemmaChatLocalHF\n\nGemmaChatLocalKaggle\n\nGemmaChatVertexAIModelGarden\n\nVertexAIImageCaptioningChat\n\nVertexAIImageEditorChat\n\nVertexAIImageGeneratorChat\n\nVertexAIVisualQnAChat\n\nï¿½\nLLMs\n(Legacy) string-in, string-out LLM interface.\nVertexAIModelGarden\nHundreds of OSS models via Vertex AI Model Garden.\nGet started\n\nVertexAI (deprecated)\n\nGemma local from Hugging Face\n\nGemma local from Kaggle\n\nGemma on Vertex AI Model Garden\n\nVertex AI image captioning\n\nï¿½\nEmbedding models\n\nVertexAIEmbeddings (deprecated)\n\nï¿½\nDocument loaders\nAlloyDB for PostgreSQL\nPostgreSQL-compatible database on Google Cloud.\nGet started\nBigQuery\nServerless data warehouse.\nGet started\nBigtable\nKey-value and wide-column store for structured and semi-structured data.\nGet started\nCloud SQL for MySQL\nManaged MySQL database.\nGet started\nCloud SQL for SQL Server\nManaged SQL Server database.\nGet started\nCloud SQL for PostgreSQL\nManaged PostgreSQL database.\nGet started\nCloud Storage (directory)\nLoad documents from a GCS bucket directory.\nGet started\nCloud Storage (file)\nLoad a single document from GCS.\nGet started\nEl Carro for Oracle Workloads\nOracle databases on Kubernetes via El Carro.\nGet started\nFirestore (Native Mode)\nNoSQL document database.\nGet started\nFirestore (Datastore Mode)\nFirestore in Datastore mode.\nGet started\nMemorystore for Redis\nManaged Redis service.\nGet started\nSpanner\nGlobally distributed relational database.\nGet started\nSpeech-to-Text\nTranscribe audio files.\nGet started\n\nCloud Vision loader\n\nï¿½\nDocument transformers\nDocument AI\nExtract structured data from unstructured documents.\nGet started\nGoogle Translate\nTranslate text and HTML via Cloud Translation API.\nGet started\nï¿½\nVector stores\nStore and search vectors using Google Cloud databases and Vertex AI Vector Search.\nAlloyDB for PostgreSQL\nPostgreSQL-compatible vector store on AlloyDB.\nGet started\nBigQuery Vector Search\nSemantic search using GoogleSQL with vector indexes.\nGet started\nMemorystore for Redis\nVector store on Memorystore for Redis.\nGet started\nSpanner\nVector store on Cloud Spanner.\nGet started\nBigtable\nVector store on Cloud Bigtable.\nGet started\nFirestore (Native Mode)\nVector store on Firestore.\nGet started\nCloud SQL for MySQL\nVector store on Cloud SQL for MySQL.\nGet started\nCloud SQL for PostgreSQL\nVector store on Cloud SQL for PostgreSQL.\nGet started\nVertex AI Vector Search\nFormerly known as Vertex AI Matching Engine, provides a low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.\nGet started\nVertex AI Vector Search + Datastore\nVector search with Datastore for document storage.\nGet started\nï¿½\nRetrievers\nVertex AI Search\nGenerative AI powered search via Vertex AI Search.\nGet started\nDocument AI Warehouse\nSearch, store, and manage documents using Document AI Warehouse.\nGet started\nOther retrievers\nCopy\nfrom langchain_google_community import VertexAIMultiTurnSearchRetriever\nfrom langchain_google_community import VertexAISearchRetriever\nfrom langchain_google_community import VertexAISearchSummaryTool\n\nï¿½\nTools\nIntegrate agents with various Google Cloud services.\nText-to-Speech\nSynthesize natural-sounding speech with 100+ voices.\nGet started\nï¿½\nCallbacks\nTrack LLM/Chat model usage.\n\nVertex AI callback handler\n\nGoogle BigQuery\n\nï¿½\nEvaluators\nEvaluate model outputs using Vertex AI.\n\nVertexPairWiseStringEvaluator\n\nVertexStringEvaluator\n\nï¿½\nOther Google products\nIntegrations with various Google services beyond the core Cloud Platform.\nï¿½\nDocument loaders\nGoogle Drive\nLoad files from Google Drive. Currently supports Google Docs.\nGet started\nï¿½\nVector stores\nScaNN (Local Index)\nEfficient local vector similarity search at scale.\nGet started\nï¿½\nRetrievers\nGoogle Drive\nRetrieve documents from Google Drive.\nGet started\nï¿½\nTools\nGoogle Search\nWeb search via Google Custom Search Engine (CSE).\nGet started\nGoogle Drive\nInteract with Google Drive.\nGet started\nGoogle Finance\nQuery financial data.\nGet started\nGoogle Jobs\nQuery job listings.\nGet started\nGoogle Lens\nVisual searches.\nGet started\nGoogle Places\nSearch for places.\nGet started\nGoogle Scholar\nSearch academic papers.\nGet started\nGoogle Trends\nQuery Google Trends data.\nGet started\nï¿½\nMCP\nMCP Toolbox\nConnect to databases including Cloud SQL and AlloyDB.\nGet started\nï¿½\nToolkits\nGmail\nCreate, search, and send emails via the Gmail API.\nGet started\nï¿½\nChat loaders\nGmail\nLoad chat history from Gmail threads.\nGet started\nï¿½\n3rd party integrations\nAccess Google services via unofficial third-party APIs.\nï¿½\nSearch\nSearchApi\nAPI access to Google search results, YouTube, and more.\nGet started\nSerpApi\nAPI access to Google search results.\nGet started\nSerper.dev\nAPI access to Google search results.\nGet started\ncloro\nGoogle Search results with AI Overview support.\nGet started\nï¿½\nYouTube\nSearch tool\nSearch YouTube videos without the official API.\nGet started\nAudio loader\nDownload audio from YouTube videos.\nGet started\nTranscripts loader\nLoad video transcripts.\nGet started\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494418,
    "timestamp": "2026-02-25T08:10:59.772Z",
    "title": "AWS (Amazon) integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/aws",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nChat models\nBedrock chat\nBedrock converse\nLLMs\nBedrock\nAmazon API Gateway\nSageMaker endpoint\nEmbedding models\nBedrock\nSageMaker endpoint\nDocument loaders\nAWS S3 directory and file\nAmazon textract\nAmazon athena\nAWS glue\nVector stores\nAmazon OpenSearch Service\nAmazon DocumentDB vector search\nInstallation and setup\nDeploy DocumentDB on AWS\nAmazon MemoryDB\nRetrievers\nAmazon kendra\nAmazon Bedrock (Knowledge bases)\nTools\nAWS lambda\nAmazon Bedrock AgentCore Browser\nAmazon Bedrock AgentCore Code Interpreter\nGraphs\nAmazon neptune\nAmazon neptune with cypher\nAmazon neptune with SPARQL\nMemory\nAmazon Bedrock AgentCore Memory\nAmazon Bedrock AgentCore Memory Store\nCallbacks\nBedrock token usage\nSageMaker tracking\nChains\nAmazon Comprehend moderation chain\nRuntime\nAmazon Bedrock AgentCore Runtime\nPopular Providers\nAWS (Amazon) integrations\nCopy page\n\nIntegrate with AWS (Amazon) using LangChain Python.\n\nThis page covers all LangChain integrations with the Amazon Web Services (AWS) platform.\nï¿½\nChat models\nï¿½\nBedrock chat\nAmazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you donâ€™t have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.\nSee a usage example.\nCopy\nfrom langchain_aws import ChatBedrock\n\nï¿½\nBedrock converse\nAWS Bedrock maintains a Converse API that provides a unified conversational interface for Bedrock models. This API does not yet support custom models. You can see a list of all models that are supported here.\nWe recommend the Converse API for users who do not need to use custom models. It can be accessed using ChatBedrockConverse.\nSee a usage example.\nCopy\nfrom langchain_aws import ChatBedrockConverse\n\nï¿½\nLLMs\nï¿½\nBedrock\nSee a usage example.\nCopy\nfrom langchain_aws import BedrockLLM\n\nï¿½\nAmazon API Gateway\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the â€œfront doorâ€ for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. API Gateway has no minimum fees or startup costs. You pay for the API calls you receive and the amount of data transferred out and, with the API Gateway tiered pricing model, you can reduce your cost as your API usage scales.\nSee a usage example.\nCopy\nfrom langchain_community.llms import AmazonAPIGateway\n\nï¿½\nSageMaker endpoint\nAmazon SageMaker is a system that can build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows.\nWe use SageMaker to host our model and expose it as the SageMaker Endpoint.\nSee a usage example.\nCopy\nfrom langchain_aws import SagemakerEndpoint\n\nï¿½\nEmbedding models\nï¿½\nBedrock\nSee a usage example.\nCopy\nfrom langchain_aws import BedrockEmbeddings\n\nï¿½\nSageMaker endpoint\nSee a usage example.\nCopy\nfrom langchain_community.embeddings import SagemakerEndpointEmbeddings\nfrom langchain_community.llms.sagemaker_endpoint import ContentHandlerBase\n\nï¿½\nDocument loaders\nï¿½\nAWS S3 directory and file\nAmazon Simple Storage Service (Amazon S3) is an object storage service. AWS S3 Directory AWS S3 Buckets\nSee a usage example for S3DirectoryLoader.\nSee a usage example for S3FileLoader.\nCopy\nfrom langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n\nï¿½\nAmazon textract\nAmazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders import AmazonTextractPDFLoader\n\nï¿½\nAmazon athena\nAmazon Athena is a serverless, interactive analytics service built on open-source frameworks, supporting open-table and file formats.\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders.athena import AthenaLoader\n\nï¿½\nAWS glue\nThe AWS Glue Data Catalog is a centralized metadata repository that allows you to manage, access, and share metadata about your data stored in AWS. It acts as a metadata store for your data assets, enabling various AWS services and your applications to query and connect to the data they need efficiently.\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders.glue_catalog import GlueCatalogLoader\n\nï¿½\nVector stores\nï¿½\nAmazon OpenSearch Service\nAmazon OpenSearch Service performs interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. Amazon OpenSearch Service offers the latest versions of OpenSearch, support for many versions of Elasticsearch, as well as visualization capabilities powered by OpenSearch Dashboards and Kibana.\nWe need to install several python libraries.\npip\nuv\nCopy\npip install boto3 requests requests-aws4auth\n\nSee a usage example.\nCopy\nfrom langchain_community.vectorstores import OpenSearchVectorSearch\n\nï¿½\nAmazon DocumentDB vector search\nAmazon DocumentDB (with MongoDB Compatibility) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud. With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB. Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search.\nï¿½\nInstallation and setup\nSee detail configuration instructions.\nWe need to install the pymongo python package.\npip\nuv\nCopy\npip install pymongo\n\nï¿½\nDeploy DocumentDB on AWS\nAmazon DocumentDB (with MongoDB Compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.\nAWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see Cloud Computing with Amazon Web Services.\nSee a usage example.\nCopy\nfrom langchain_community.vectorstores import DocumentDBVectorSearch\n\nï¿½\nAmazon MemoryDB\nAmazon MemoryDB is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store, enabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today.\nInMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB.\nCopy\nfrom langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore\n\nvds = InMemoryVectorStore.from_documents(\n            chunks,\n            embeddings,\n            redis_url=\"rediss://cluster_endpoint:6379/ssl=True ssl_cert_reqs=none\",\n            vector_schema=vector_schema,\n            index_name=INDEX_NAME,\n        )\n\nSee a usage example.\nï¿½\nRetrievers\nï¿½\nAmazon kendra\nAmazon Kendra is an intelligent search service provided by Amazon Web Services (AWS). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.\nWith Kendra, we can search across a wide range of content types, including documents, FAQs, knowledge bases, manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and contextual meanings to provide highly relevant search results.\nWe need to install the langchain-aws library.\npip\nuv\nCopy\npip install langchain-aws\n\nSee a usage example.\nCopy\nfrom langchain_aws import AmazonKendraRetriever\n\nï¿½\nAmazon Bedrock (Knowledge bases)\nKnowledge bases for Amazon Bedrock is an Amazon Web Services (AWS) offering which lets you quickly build RAG applications by using your private data to customize foundation model response.\nWe need to install the langchain-aws library.\npip\nuv\nCopy\npip install langchain-aws\n\nSee a usage example.\nCopy\nfrom langchain_aws import AmazonKnowledgeBasesRetriever\n\nï¿½\nTools\nï¿½\nAWS lambda\nAmazon AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS). It helps developers to build and run applications and services without provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications.\nWe need to install boto3 python library.\npip\nuv\nCopy\npip install boto3\n\nSee a usage example.\nCopy\nfrom langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n\nï¿½\nAmazon Bedrock AgentCore Browser\nAmazon Bedrock AgentCore Browser enables agents to interact with web pages through a managed Chrome browser for navigation, content extraction, and web automation.\npip\nuv\nCopy\npip install langchain-aws bedrock-agentcore playwright beautifulsoup4\n\nSee a usage example.\nCopy\nfrom langchain_aws.tools import create_browser_toolkit\n\n# Create toolkit\ntoolkit, browser_tools = create_browser_toolkit(region=\"us-west-2\")\n\n# Use with an agent\nagent = create_react_agent(model=llm, tools=browser_tools)\nresult = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Go to example.com and get the heading\"}]},\n    config={\"configurable\": {\"thread_id\": \"session-1\"}}\n)\n\n# Cleanup when done\nawait toolkit.cleanup()\n\nï¿½\nAmazon Bedrock AgentCore Code Interpreter\nAmazon Bedrock AgentCore Code Interpreter enables agents to execute Python, JavaScript, and TypeScript code in secure, managed sandbox environments for calculations, data analysis, and visualizations.\npip\nuv\nCopy\npip install langchain-aws bedrock-agentcore\n\nSee a usage example.\nCopy\nfrom langchain_aws.tools import create_code_interpreter_toolkit\n\n# Create toolkit (async)\ntoolkit, code_tools = await create_code_interpreter_toolkit(region=\"us-west-2\")\n\n# Use with an agent\nagent = create_react_agent(model=llm, tools=code_tools)\nresult = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Calculate factorial of 10\"}]},\n    config={\"configurable\": {\"thread_id\": \"session-1\"}}\n)\n\n# Cleanup when done\nawait toolkit.cleanup()\n\nï¿½\nGraphs\nï¿½\nAmazon neptune\nAmazon Neptune is a high-performance graph analytics and serverless database for superior scalability and availability.\nFor the Cypher and SPARQL integrations below, we need to install the langchain-aws library.\npip\nuv\nCopy\npip install langchain-aws\n\nï¿½\nAmazon neptune with cypher\nSee a usage example.\nCopy\nfrom langchain_aws.graphs import NeptuneGraph\nfrom langchain_aws.graphs import NeptuneAnalyticsGraph\nfrom langchain_aws.chains import create_neptune_opencypher_qa_chain\n\nï¿½\nAmazon neptune with SPARQL\nCopy\nfrom langchain_aws.graphs import NeptuneRdfGraph\nfrom langchain_aws.chains import create_neptune_sparql_qa_chain\n\nï¿½\nMemory\nï¿½\nAmazon Bedrock AgentCore Memory\nAmazon Bedrock AgentCore Memory provides managed persistence for LangGraph agents, enabling conversation history and state management across sessions with automatic scaling and high availability.\npip\nuv\nCopy\npip install langgraph-checkpoint-aws\n\nCopy\nfrom langgraph_checkpoint_aws import AgentCoreMemorySaver\n\n# Create checkpointer\ncheckpointer = AgentCoreMemorySaver(\n    memory_id=\"your-memory-id\",\n    region_name=\"us-west-2\"\n)\n\n# Use with LangGraph\ngraph = workflow.compile(checkpointer=checkpointer)\n\n# Invoke with thread_id and actor_id for conversation persistence\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"user-123\",\n        \"actor_id\": \"my-agent\"  # Required for AgentCore\n    }\n}\nresult = graph.invoke({\"messages\": []}, config)\n\nKey features:\nManaged infrastructure with no database setup required\nAutomatic scaling and high availability\nMulti-agent support via actor_id isolation\nEncryption at rest and in transit\nï¿½\nAmazon Bedrock AgentCore Memory Store\nAmazon Bedrock AgentCore Memory Store provides long-term memory with semantic search capabilities for LangGraph agents, enabling storage and retrieval of user preferences, facts, and extracted memories across sessions.\nCopy\nfrom langgraph_checkpoint_aws import AgentCoreMemoryStore\n\n# Initialize store for long-term memories\nstore = AgentCoreMemoryStore(memory_id=\"your-memory-id\", region_name=\"us-west-2\")\n\n# Use in a pre-model hook to save and retrieve memories\ndef pre_model_hook(state, config, *, store):\n    actor_id = config[\"configurable\"][\"actor_id\"]\n    thread_id = config[\"configurable\"][\"thread_id\"]\n    namespace = (actor_id, thread_id)\n\n    # Save a message\n    store.put(namespace, str(uuid.uuid4()), {\"message\": msg})\n\n    # Search for relevant memories\n    results = store.search((\"preferences\", actor_id), query=\"user preferences\", limit=5)\n    return {\"model_input_messages\": state[\"messages\"]}\n\nï¿½\nCallbacks\nï¿½\nBedrock token usage\nCopy\nfrom langchain_community.callbacks.bedrock_anthropic_callback import BedrockAnthropicTokenUsageCallbackHandler\n\nï¿½\nSageMaker tracking\nAmazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models.\nAmazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.\nWe need to install several python libraries.\npip\nuv\nCopy\npip install google-search-results sagemaker\n\nSee a usage example.\nCopy\nfrom langchain_community.callbacks import SageMakerCallbackHandler\n\nï¿½\nChains\nï¿½\nAmazon Comprehend moderation chain\nAmazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text.\nWe need to install the boto3 and nltk libraries.\npip\nuv\nCopy\npip install boto3 nltk\n\nSee a usage example.\nCopy\nfrom langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain\n\nï¿½\nRuntime\nï¿½\nAmazon Bedrock AgentCore Runtime\nAmazon Bedrock AgentCore Runtime provides managed, serverless execution for LangGraph agents with built-in observability, automatic scaling, and seamless integration with other AgentCore services.\npip\nuv\nCopy\npip install bedrock-agentcore\n\nCopy\nfrom bedrock_agentcore.runtime import BedrockAgentCoreApp\n\napp = BedrockAgentCoreApp()\n\n@app.entrypoint\ndef agent_invocation(payload, context):\n    result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": payload[\"prompt\"]}]})\n    return {\"result\": result[\"messages\"][-1].content}\n\napp.run()\n\nDeploy using the AgentCore CLI:\nCopy\n# Configure your agent\nagentcore configure\n\n# Deploy to AgentCore Runtime\nagentcore launch -e your_agent.py\n\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494419,
    "timestamp": "2026-02-25T08:10:59.779Z",
    "title": "Hugging Face integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/huggingface",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nChat models\nChatHuggingFace\nLLMs\nHuggingFaceEndpoint\nHuggingFacePipeline\nEmbedding models\nHuggingFaceEmbeddings\nHuggingFaceEndpointEmbeddings\nHuggingFaceInferenceAPIEmbeddings\nHuggingFaceInstructEmbeddings\nHuggingFaceBgeEmbeddings\nDocument loaders\nHugging Face dataset\nHugging Face model loader\nImage captions\nTools\nHugging Face hub tools\nHugging Face Text-to-Speech model inference.\nPopular Providers\nHugging Face integrations\nCopy page\n\nIntegrate with Hugging Face using LangChain Python.\n\nThis page covers all LangChain integrations with Hugging Face Hub and libraries like transformers, sentence transformers, and datasets.\nï¿½\nChat models\nï¿½\nChatHuggingFace\nWe can use the Hugging Face LLM classes or directly use the ChatHuggingFace class.\nSee a usage example.\nCopy\nfrom langchain_huggingface import ChatHuggingFace\n\nï¿½\nLLMs\nï¿½\nHuggingFaceEndpoint\nWe can use the HuggingFaceEndpoint class to run open source models via serverless Inference Providers or via dedicated Inference Endpoints.\nSee a usage example.\nCopy\nfrom langchain_huggingface import HuggingFaceEndpoint\n\nï¿½\nHuggingFacePipeline\nWe can use the HuggingFacePipeline class to run open source models locally.\nSee a usage example.\nCopy\nfrom langchain_huggingface import HuggingFacePipeline\n\nï¿½\nEmbedding models\nï¿½\nHuggingFaceEmbeddings\nWe can use the HuggingFaceEmbeddings class to run open source embedding models locally.\nSee a usage example.\nCopy\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nï¿½\nHuggingFaceEndpointEmbeddings\nWe can use the HuggingFaceEndpointEmbeddings class to run open source embedding models via a dedicated Inference Endpoint.\nSee a usage example.\nCopy\nfrom langchain_huggingface import HuggingFaceEndpointEmbeddings\n\nï¿½\nHuggingFaceInferenceAPIEmbeddings\nWe can use the HuggingFaceInferenceAPIEmbeddings class to run open source embedding models via Inference Providers.\nSee a usage example.\nCopy\nfrom langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n\nï¿½\nHuggingFaceInstructEmbeddings\nWe can use the HuggingFaceInstructEmbeddings class to run open source embedding models locally.\nSee a usage example.\nCopy\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings\n\nï¿½\nHuggingFaceBgeEmbeddings\nBGE models on the HuggingFace are one of the best open-source embedding models. BGE model is created by the Beijing Academy of Artificial Intelligence (BAAI). BAAI is a private non-profit organization engaged in AI research and development.\nSee a usage example.\nCopy\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\n\nï¿½\nDocument loaders\nï¿½\nHugging Face dataset\nHugging Face Hub is home to over 75,000 datasets in more than 100 languages that can be used for a broad range of tasks across NLP, Computer Vision, and Audio. They used for a diverse range of tasks such as translation, automatic speech recognition, and image classification.\nWe need to install datasets python package.\npip\nuv\nCopy\npip install datasets\n\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader\n\nï¿½\nHugging Face model loader\nLoad model information from Hugging Face Hub, including README content. This loader interfaces with the Hugging Face Models API to fetch and load model metadata and README files. The API allows you to search and filter models based on specific criteria such as model tags, authors, and more.\nCopy\nfrom langchain_community.document_loaders import HuggingFaceModelLoader\n\nï¿½\nImage captions\nIt uses the Hugging Face models to generate image captions.\nWe need to install several python packages.\npip\nuv\nCopy\npip install transformers pillow\n\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders import ImageCaptionLoader\n\nï¿½\nTools\nï¿½\nHugging Face hub tools\nHugging Face Tools support text I/O and are loaded using the load_huggingface_tool function.\nWe need to install several python packages.\npip\nuv\nCopy\npip install transformers huggingface_hub\n\nSee a usage example.\nCopy\nfrom langchain_community.agent_toolkits.load_tools import load_huggingface_tool\n\nï¿½\nHugging Face Text-to-Speech model inference.\nIt is a wrapper around OpenAI Text-to-Speech API.\nCopy\nfrom langchain_community.tools.audio import HuggingFaceTextToSpeechModelInference\n\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494421,
    "timestamp": "2026-02-25T08:10:59.784Z",
    "title": "Ollama integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/ollama",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nModel interfaces\nOther\nPopular Providers\nOllama integrations\nCopy page\n\nIntegrate with Ollama using LangChain Python.\n\nThis page covers all LangChain integrations with Ollama.\nOllama allows you to run open-source models (like gpt-oss) locally.\nFor a complete list of supported models and variants, see the Ollama model library.\nï¿½\nModel interfaces\nChatOllama\nOllama chat models.\nGet started\nOllamaEmbeddings\nOllama embedding models.\nGet started\nï¿½\nOther\nOllamaLLM\n(Legacy) Ollama text completion models.\nGet started\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494420,
    "timestamp": "2026-02-25T08:10:59.786Z",
    "title": "Microsoft integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/microsoft",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nChat models\nAzure OpenAI\nAzure AI\nAzure ML chat online endpoint\nLLMs\nAzure ML\nAzure OpenAI\nEmbedding models\nAzure OpenAI\nAzure AI\nDocument loaders\nAzure AI data\nAzure AI document intelligence\nAzure Blob Storage\nMicrosoft OneDrive\nMicrosoft OneDrive file\nMicrosoft word\nMicrosoft excel\nMicrosoft SharePoint\nMicrosoft PowerPoint\nMicrosoft OneNote\nPlaywright URL loader\nMemory\nAzure cosmos DB chat message history\nVector stores\nAzure cosmos DB\nAzure cosmos DB for MongoDB (vCore)\nAzure cosmos DB NoSQL\nAzure Database for PostgreSQL\nAzure SQL Database\nAzure AI search\nRetrievers\nAzure AI search\nVector store\nAzure Database for PostgreSQL\nTools\nAzure Container apps dynamic sessions\nBing search\nToolkits\nAzure AI services\nAzure AI services individual tools\nAzure cognitive services\nAzure AI services individual tools\nMicrosoft office 365 email and calendar\nOffice 365 individual tools\nMicrosoft Azure PowerBI\nPowerBI individual tools\nPlayWright browser toolkit\nPlayWright browser individual tools\nGraphs\nAzure cosmos DB for apache gremlin\nUtilities\nBing search API\nMore\nMicrosoft Presidio\nPopular Providers\nMicrosoft integrations\nCopy page\n\nIntegrate with Microsoft using LangChain Python.\n\nThis page covers all LangChain integrations with Microsoft Azure and other Microsoft products.\nï¿½\nChat models\nMicrosoft offers three main options for accessing chat models through Azure:\nAzure OpenAI - Provides access to OpenAIâ€™s powerful models like o3, 4.1, and other models through Microsoft Azureâ€™s secure enterprise platform.\nAzure AI - Offers access to a variety of models from different providers including Anthropic, DeepSeek, Cohere, Phi and Mistral through a unified API.\nAzure ML - Allows deployment and management of your own custom models or fine-tuned open-source models with Azure Machine Learning.\nï¿½\nAzure OpenAI\nMicrosoft Azure, often referred to as Azure is a cloud computing platform run by Microsoft, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.\nAzure OpenAI is an Azure service with powerful language models from OpenAI including the GPT-3, Codex and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation.\npip\nuv\nCopy\npip install langchain-openai\n\nSet the environment variables to get access to the Azure OpenAI service.\nCopy\nimport os\n\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://<your-endpoint.openai.azure.com/\"\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"your AzureOpenAI key\"\n\nSee a usage example\nCopy\nfrom langchain_openai import AzureChatOpenAI\n\nï¿½\nAzure AI\nAzure AI Foundry provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the AzureAIChatCompletionsModel class.\npip\nuv\nCopy\npip install -U langchain-azure-ai\n\nConfigure your API key and Endpoint.\nCopy\nexport AZURE_AI_CREDENTIAL=your-api-key\nexport AZURE_AI_ENDPOINT=your-endpoint\n\nCopy\nfrom langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n\nllm = AzureAIChatCompletionsModel(\n    model_name=\"gpt-4.1\",\n    api_version=\"2024-05-01-preview\",\n)\n\nSee a usage example\nï¿½\nAzure ML chat online endpoint\nSee the documentation here for accessing chat models hosted with Azure Machine Learning.\nï¿½\nLLMs\nï¿½\nAzure ML\nSee a usage example.\nCopy\nfrom langchain_community.llms.azureml_endpoint import AzureMLOnlineEndpoint\n\nï¿½\nAzure OpenAI\nSee a usage example.\nCopy\nfrom langchain_openai import AzureOpenAI\n\nï¿½\nEmbedding models\nMicrosoft offers two main options for accessing embedding models through Azure:\nï¿½\nAzure OpenAI\nSee a usage example\nCopy\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nï¿½\nAzure AI\npip\nuv\nCopy\npip install -U langchain-azure-ai\n\nConfigure your API key and Endpoint.\nCopy\nexport AZURE_AI_CREDENTIAL=your-api-key\nexport AZURE_AI_ENDPOINT=your-endpoint\n\nCopy\nfrom langchain_azure_ai.embeddings import AzureAIEmbeddingsModel\n\nembed_model = AzureAIEmbeddingsModel(\n    model_name=\"text-embedding-ada-002\"\n)\n\nï¿½\nDocument loaders\nï¿½\nAzure AI data\nAzure AI Foundry (formerly Azure AI Studio provides the capability to upload data assets to cloud storage and register existing data assets from the following sources:\nMicrosoft OneLake\nAzure Blob Storage\nAzure Data Lake gen 2\nFirst, you need to install several python packages.\npip\nuv\nCopy\npip install azureml-fsspec, azure-ai-generative\n\nSee a usage example.\nCopy\nfrom langchain.document_loaders import AzureAIDataLoader\n\nï¿½\nAzure AI document intelligence\nAzure AI Document Intelligence (formerly known as Azure Form Recognizer) is machine-learning based service that extracts texts (including handwriting), tables, document structures, and key-value-pairs from digital or scanned PDFs, images, Office and HTML files. Document Intelligence supports PDF, JPEG/JPG, PNG, BMP, TIFF, HEIF, DOCX, XLSX, PPTX and HTML.\nFirst, you need to install a python package.\npip\nuv\nCopy\npip install azure-ai-documentintelligence\n\nSee a usage example.\nCopy\nfrom langchain.document_loaders import AzureAIDocumentIntelligenceLoader\n\nï¿½\nAzure Blob Storage\nAzure Blob Storage is Microsoftâ€™s object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesnâ€™t adhere to a particular data model or definition, such as text or binary data.\nAzure Blob Storage is designed for:\nServing images or documents directly to a browser.\nStoring files for distributed access.\nStreaming video and audio.\nWriting to log files.\nStoring data for backup and restore, disaster recovery, and archiving.\nStoring data for analysis by an on-premises or Azure-hosted service.\npip\nuv\nCopy\npip install langchain-azure-storage\n\nSee usage examples for the Azure Blob Storage Loader.\nCopy\nfrom langchain_azure_storage.document_loaders import AzureBlobStorageLoader\n\nï¿½\nMicrosoft OneDrive\nMicrosoft OneDrive (formerly SkyDrive) is a file-hosting service operated by Microsoft.\nFirst, you need to install a python package.\npip\nuv\nCopy\npip install o365\n\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders import OneDriveLoader\n\nï¿½\nMicrosoft OneDrive file\nMicrosoft OneDrive (formerly SkyDrive) is a file-hosting service operated by Microsoft.\nFirst, you need to install a python package.\npip\nuv\nCopy\npip install o365\n\nCopy\nfrom langchain_community.document_loaders import OneDriveFileLoader\n\nï¿½\nMicrosoft word\nMicrosoft Word is a word processor developed by Microsoft.\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders import UnstructuredWordDocumentLoader\n\nï¿½\nMicrosoft excel\nMicrosoft Excel is a spreadsheet editor developed by Microsoft for Windows, macOS, Android, iOS and iPadOS. It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software.\nThe UnstructuredExcelLoader is used to load Microsoft Excel files. The loader works with both .xlsx and .xls files. The page content will be the raw text of the Excel file. If you use the loader in \"elements\" mode, an HTML representation of the Excel file will be available in the document metadata under the text_as_html key.\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders import UnstructuredExcelLoader\n\nï¿½\nMicrosoft SharePoint\nMicrosoft SharePoint is a website-based collaboration system that uses workflow applications, â€œlistâ€ databases, and other web parts and security features to empower business teams to work together developed by Microsoft.\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders.sharepoint import SharePointLoader\n\nï¿½\nMicrosoft PowerPoint\nMicrosoft PowerPoint is a presentation program by Microsoft.\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders import UnstructuredPowerPointLoader\n\nï¿½\nMicrosoft OneNote\nFirst, letâ€™s install dependencies:\npip\nuv\nCopy\npip install bs4 msal\n\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders.onenote import OneNoteLoader\n\nï¿½\nPlaywright URL loader\nPlaywright is an open-source automation tool developed by Microsoft that allows you to programmatically control and automate web browsers. It is designed for end-to-end testing, scraping, and automating tasks across various web browsers such as Chromium, Firefox, and WebKit.\nFirst, letâ€™s install dependencies:\npip\nuv\nCopy\npip install playwright unstructured\n\nSee a usage example.\nCopy\nfrom langchain_community.document_loaders.onenote import OneNoteLoader\n\nï¿½\nMemory\nï¿½\nAzure cosmos DB chat message history\nAzure Cosmos DB provides chat message history storage for conversational AI applications, enabling you to persist and retrieve conversation history with low latency and high availability.\npip\nuv\nCopy\npip install langchain-azure-ai\n\nConfigure your Azure Cosmos DB connection:\nCopy\nfrom langchain_azure_ai.chat_message_histories import CosmosDBChatMessageHistory\n\nhistory = CosmosDBChatMessageHistory(\n    cosmos_endpoint=\"https://<your-account>.documents.azure.com:443/\",\n    cosmos_database=\"<your-database>\",\n    cosmos_container=\"<your-container>\",\n    session_id=\"<session-id>\",\n    user_id=\"<user-id>\",\n    credential=\"<your-credential>\"  # or use connection_string\n)\n\nï¿½\nVector stores\nï¿½\nAzure cosmos DB\nAI agents can rely on Azure Cosmos DB as a unified memory system solution, enjoying speed, scale, and simplicity. This service successfully enabled OpenAIâ€™s ChatGPT service to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the worldâ€™s first globally distributed NoSQL, relational, and vector database service that offers a serverless mode.\nBelow are two available Azure Cosmos DB APIs that can provide vector store functionalities.\nï¿½\nAzure cosmos DB for MongoDB (vCore)\nAzure Cosmos DB for MongoDB vCore makes it easy to create a database with full native MongoDB support. You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore accountâ€™s connection string. Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data thatâ€™s stored in Azure Cosmos DB.\nInstallation and setup\nSee detailed configuration instructions.\nWe need to install langchain-azure-ai and pymongo python packages.\npip\nuv\nCopy\npip install langchain-azure-ai pymongo\n\nDeploy Azure cosmos DB on Microsoft Azure\nAzure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.\nWith Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.\nSign Up for free to get started today.\nSee a usage example.\nCopy\nfrom langchain_azure_ai.vectorstores import AzureCosmosDBMongoVCoreVectorSearch\n\nï¿½\nAzure cosmos DB NoSQL\nAzure Cosmos DB for NoSQL now offers vector indexing and search in preview. This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data, but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching, as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the efficiency of vector-based operations.\nInstallation and setup\nSee detail configuration instructions.\nWe need to install langchain-azure-ai and azure-cosmos python packages.\npip\nuv\nCopy\npip install langchain-azure-ai azure-cosmos\n\nDeploy Azure cosmos DB on Microsoft Azure\nAzure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available in every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.\nSign Up for free to get started today.\nSee a usage example.\nCopy\nfrom langchain_azure_ai.vectorstores import AzureCosmosDBNoSqlVectorSearch\n\nï¿½\nAzure Database for PostgreSQL\nAzure Database for PostgreSQL - Flexible Server is a relational database service based on the open-source Postgres database engine. Itâ€™s a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\nSee set up instructions for Azure Database for PostgreSQL.\nSimply use the connection string from your Azure Portal.\nSince Azure Database for PostgreSQL is open-source Postgres, you can use the LangChainâ€™s Postgres support to connect to Azure Database for PostgreSQL.\nï¿½\nAzure SQL Database\nAzure SQL Database is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution. It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity.\nBy leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems.\nInstallation and setup\nSee detail configuration instructions.\nWe need to install the langchain-sqlserver python package.\nCopy\n!pip install langchain-sqlserver==0.1.1\n\nDeploy Azure SQL DB on Microsoft Azure\nSign Up for free to get started today.\nSee a usage example.\nCopy\nfrom langchain_sqlserver import SQLServer_VectorStore\n\nï¿½\nAzure AI search\nAzure AI Search is a cloud search service that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid queries at scale. See here for usage examples.\nCopy\nfrom langchain_community.vectorstores.azuresearch import AzureSearch\n\nï¿½\nRetrievers\nï¿½\nAzure AI search\nAzure AI Search (formerly known as Azure Search or Azure Cognitive Search ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.\nSearch is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, youâ€™ll work with the following capabilities:\nA search engine for full text search over a search index containing user-owned content\nRich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation\nRich query syntax for text search, fuzzy search, autocomplete, geo-search and more\nProgrammability through REST APIs and client libraries in Azure SDKs\nAzure integration at the data layer, machine learning layer, and AI (AI Services)\nSee set up instructions.\nSee a usage example.\nCopy\nfrom langchain_community.retrievers import AzureAISearchRetriever\n\nï¿½\nVector store\nï¿½\nAzure Database for PostgreSQL\nAzure Database for PostgreSQL - Flexible Server is a relational database service based on the open-source Postgres database engine. Itâ€™s a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.\nSee set up instructions for Azure Database for PostgreSQL.\nYou need to enable pgvector extension in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the PGVector in LangChain to connect to Azure Database for PostgreSQL.\nSee a usage example. Simply use the connection string from your Azure Portal.\nï¿½\nTools\nï¿½\nAzure Container apps dynamic sessions\nWe need to get the POOL_MANAGEMENT_ENDPOINT environment variable from the Azure Container Apps service. See the instructions here.\nWe need to install a python package.\npip\nuv\nCopy\npip install langchain-azure-dynamic-sessions\n\nSee a usage example.\nCopy\nfrom langchain_azure_dynamic_sessions import SessionsPythonREPLTool\n\nï¿½\nBing search\nFollow the documentation here to get a detail explanations and instructions of this tool.\nThe environment variable BING_SUBSCRIPTION_KEY and BING_SEARCH_URL are required from Bing Search resource.\nCopy\nfrom langchain_community.tools.bing_search import BingSearchResults\nfrom langchain_community.utilities import BingSearchAPIWrapper\n\napi_wrapper = BingSearchAPIWrapper()\ntool = BingSearchResults(api_wrapper=api_wrapper)\n\nï¿½\nToolkits\nï¿½\nAzure AI services\nWe need to install several python packages.\npip\nuv\nCopy\npip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n\nSee a usage example.\nCopy\nfrom langchain_community.agent_toolkits import azure_ai_services\n\nï¿½\nAzure AI services individual tools\nThe azure_ai_services toolkit includes the following tools:\nImage Analysis: AzureAiServicesImageAnalysisTool\nDocument Intelligence: AzureAiServicesDocumentIntelligenceTool\nSpeech to Text: AzureAiServicesSpeechToTextTool\nText to Speech: AzureAiServicesTextToSpeechTool\nText Analytics for Health: AzureAiServicesTextAnalyticsForHealthTool\nï¿½\nAzure cognitive services\nWe need to install several python packages.\npip\nuv\nCopy\npip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis\n\nSee a usage example.\nCopy\nfrom langchain_community.agent_toolkits import AzureCognitiveServicesToolkit\n\nï¿½\nAzure AI services individual tools\nThe azure_ai_services toolkit includes the tools that queries the Azure Cognitive Services:\nAzureCogsFormRecognizerTool: Form Recognizer API\nAzureCogsImageAnalysisTool: Image Analysis API\nAzureCogsSpeech2TextTool: Speech2Text API\nAzureCogsText2SpeechTool: Text2Speech API\nAzureCogsTextAnalyticsHealthTool: Text Analytics for Health API\nCopy\nfrom langchain_community.tools.azure_cognitive_services import (\n    AzureCogsFormRecognizerTool,\n    AzureCogsImageAnalysisTool,\n    AzureCogsSpeech2TextTool,\n    AzureCogsText2SpeechTool,\n    AzureCogsTextAnalyticsHealthTool,\n)\n\nï¿½\nMicrosoft office 365 email and calendar\nWe need to install O365 python package.\npip\nuv\nCopy\npip install O365\n\nSee a usage example.\nCopy\nfrom langchain_community.agent_toolkits import O365Toolkit\n\nï¿½\nOffice 365 individual tools\nYou can use individual tools from the Office 365 Toolkit:\nO365CreateDraftMessage: creating a draft email in Office 365\nO365SearchEmails: searching email messages in Office 365\nO365SearchEvents: searching calendar events in Office 365\nO365SendEvent: sending calendar events in Office 365\nO365SendMessage: sending an email in Office 365\nCopy\nfrom langchain_community.tools.office365 import O365CreateDraftMessage\nfrom langchain_community.tools.office365 import O365SearchEmails\nfrom langchain_community.tools.office365 import O365SearchEvents\nfrom langchain_community.tools.office365 import O365SendEvent\nfrom langchain_community.tools.office365 import O365SendMessage\n\nï¿½\nMicrosoft Azure PowerBI\nWe need to install azure-identity python package.\npip\nuv\nCopy\npip install azure-identity\n\nSee a usage example.\nCopy\nfrom langchain_community.agent_toolkits import PowerBIToolkit\nfrom langchain_community.utilities.powerbi import PowerBIDataset\n\nï¿½\nPowerBI individual tools\nYou can use individual tools from the Azure PowerBI Toolkit:\nInfoPowerBITool: getting metadata about a PowerBI Dataset\nListPowerBITool: getting tables names\nQueryPowerBITool: querying a PowerBI Dataset\nCopy\nfrom langchain_community.tools.powerbi.tool import InfoPowerBITool\nfrom langchain_community.tools.powerbi.tool import ListPowerBITool\nfrom langchain_community.tools.powerbi.tool import QueryPowerBITool\n\nï¿½\nPlayWright browser toolkit\nPlaywright is an open-source automation tool developed by Microsoft that allows you to programmatically control and automate web browsers. It is designed for end-to-end testing, scraping, and automating tasks across various web browsers such as Chromium, Firefox, and WebKit.\nWe need to install several python packages.\npip\nuv\nCopy\npip install playwright lxml\n\nSee a usage example.\nCopy\nfrom langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n\nï¿½\nPlayWright browser individual tools\nYou can use individual tools from the PlayWright Browser Toolkit.\nCopy\nfrom langchain_community.tools.playwright import ClickTool\nfrom langchain_community.tools.playwright import CurrentWebPageTool\nfrom langchain_community.tools.playwright import ExtractHyperlinksTool\nfrom langchain_community.tools.playwright import ExtractTextTool\nfrom langchain_community.tools.playwright import GetElementsTool\nfrom langchain_community.tools.playwright import NavigateTool\nfrom langchain_community.tools.playwright import NavigateBackTool\n\nï¿½\nGraphs\nï¿½\nAzure cosmos DB for apache gremlin\nWe need to install a python package.\npip\nuv\nCopy\npip install gremlinpython\n\nSee a usage example.\nCopy\nfrom langchain_community.graphs import GremlinGraph\nfrom langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n\nï¿½\nUtilities\nï¿½\nBing search API\nMicrosoft Bing, commonly referred to as Bing or Bing Search, is a web search engine owned and operated by Microsoft.\nSee a usage example.\nCopy\nfrom langchain_community.utilities import BingSearchAPIWrapper\n\nï¿½\nMore\nï¿½\nMicrosoft Presidio\nPresidio (Origin from Latin praesidium â€˜protection, garrisonâ€™) helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.\nFirst, you need to install several python packages and download a SpaCy model.\npip\nuv\nCopy\npip install langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker\npython -m spacy download en_core_web_lg\n\nSee usage examples.\nCopy\nfrom langchain_experimental.data_anonymizer import PresidioAnonymizer, PresidioReversibleAnonymizer\n\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494423,
    "timestamp": "2026-02-25T08:10:59.792Z",
    "title": "Chat model integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/chat",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nIntegrations by component\nChat model integrations\nCopy page\n\nIntegrate with chat models using LangChain Python.\n\nChat models are language models that use a sequence of messages as inputs and return messages as outputs (as opposed to traditional, plaintext LLMs).\nï¿½\nFeatured models\nWhile these LangChain classes support the indicated advanced feature, you may need to refer to provider-specific documentation to learn which hosted models or backends support the feature.\nModel\tTool calling\tStructured output\tMultimodal\nChatOpenAI\tâœ…\tâœ…\tâœ…\nChatAnthropic\tâœ…\tâœ…\tâœ…\nChatVertexAI (deprecated)\tâœ…\tâœ…\tâœ…\nChatGoogleGenerativeAI\tâœ…\tâœ…\tâœ…\nAzureChatOpenAI\tâœ…\tâœ…\tâœ…\nChatGroq\tâœ…\tâœ…\tâŒ\nChatBedrock\tâœ…\tâœ…\tâŒ\nChatAmazonNova\tâœ…\tâŒ\tâœ…\nChatHuggingFace\tâœ…\tâœ…\tâŒ\nChatOllama\tâœ…\tâœ…\tâŒ\nChatWatsonx\tâœ…\tâœ…\tâœ…\nChatXAI\tâœ…\tâœ…\tâŒ\nChatNVIDIA\tâœ…\tâœ…\tâœ…\nChatCohere\tâœ…\tâœ…\tâŒ\nChatMistralAI\tâœ…\tâœ…\tâŒ\nChatTogether\tâœ…\tâœ…\tâŒ\nChatFireworks\tâœ…\tâœ…\tâŒ\nChatLlamaCpp\tâœ…\tâœ…\tâŒ\nChatDeepSeek\tâœ…\tâœ…\tâŒ\nChatDatabricks\tâœ…\tâœ…\tâŒ\nChatPerplexity\tâŒ\tâœ…\tâœ…\nChatOpenRouter\tâœ…\tâœ…\tâœ…\nï¿½\nRouters & proxies\nRouters and proxies give you access to models from multiple providers through a single API and credential. They can simplify billing, let you switch between models without changing integrations, and offer features like automatic fallbacks.\nProvider\tIntegration\tDescription\nOpenRouter\tChatOpenRouter\tUnified access to models from OpenAI, Anthropic, Google, Meta, and more\nï¿½\nChat Completions API\nCertain model providers offer endpoints that are compatible with OpenAIâ€™s Chat Completions API. In such cases, you can use ChatOpenAI with a custom base_url to connect to these endpoints for basic chat functionality.\nChatOpenAI targets official OpenAI API specifications only. Non-standard response fields from third-party providers (e.g., reasoning_content, reasoning, reasoning_details) are not extracted or preserved. Use a provider-specific package when you need access to non-standard features.\nFor instance, OpenRouter has a dedicated LangChain integration. See the ChatOpenRouter guide for setup and usage.\nï¿½\nAll chat models\nAbso\nView guide\nAI21 Labs\nView guide\nAI/ML API\nView guide\nAlibaba Cloud PAI EAS\nView guide\nAmazon Nova\nView guide\nAnthropic\nView guide\nAzureAIChatCompletionsModel\nView guide\nAzure OpenAI\nView guide\nAzure ML Endpoint\nView guide\nBaichuan Chat\nView guide\nBaidu Qianfan\nView guide\nBaseten\nView guide\nAWS Bedrock\nView guide\nCerebras\nView guide\nCloudflareWorkersAI\nView guide\nCohere\nView guide\nContextualAI\nView guide\nCoze Chat\nView guide\nDappier AI\nView guide\nDatabricks\nView guide\nDeepInfra\nView guide\nDeepSeek\nView guide\nEden AI\nView guide\nEverlyAI\nView guide\nFeatherless AI\nView guide\nFireworks\nView guide\nChatFriendli\nView guide\nGoogle Gemini\nView guide\nGoogle Cloud Vertex AI\nView guide\nGPTRouter\nView guide\nDigitalOcean Gradient\nView guide\nGreenNode\nView guide\nGroq\nView guide\nChatHuggingFace\nView guide\nIBM watsonx.ai\nView guide\nJinaChat\nView guide\nKinetica\nView guide\nKonko\nView guide\nLiteLLM\nView guide\nLlama 2 Chat\nView guide\nLlama API\nView guide\nLlamaEdge\nView guide\nLlama.cpp\nView guide\nmaritalk\nView guide\nMiniMax\nView guide\nMistralAI\nView guide\nMLX\nView guide\nModelScope\nView guide\nMoonshot\nView guide\nNaver\nView guide\nNebius\nView guide\nNetmind\nView guide\nNVIDIA AI Endpoints\nView guide\nChatOCIModelDeployment\nView guide\nOCIGenAI\nView guide\nChatOctoAI\nView guide\nOllama\nView guide\nOpenAI\nView guide\nOpenRouter\nView guide\nOutlines\nView guide\nPerplexity\nView guide\nPipeshift\nView guide\nChatPredictionGuard\nView guide\nPremAI\nView guide\nPromptLayer ChatOpenAI\nView guide\nQwen QwQ\nView guide\nQwen\nView guide\nReka\nView guide\nRunPod Chat Model\nView guide\nSambaNova\nView guide\nChatSeekrFlow\nView guide\nSnowflake Cortex\nView guide\nSparkLLM Chat\nView guide\nNebula (Symbl.ai)\nView guide\nTencent Hunyuan\nView guide\nTogether\nView guide\nTongyi Qwen\nView guide\nUpstage\nView guide\nvLLM Chat\nView guide\nVolc Engine Maas\nView guide\nChatWriter\nView guide\nxAI\nView guide\nXinference\nView guide\nYandexGPT\nView guide\nChatYI\nView guide\nYuan2.0\nView guide\nZHIPU AI\nView guide\nIf youâ€™d like to contribute an integration, see Contributing integrations.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494424,
    "timestamp": "2026-02-25T08:10:59.800Z",
    "title": "Tool integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/tools",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nSearch\nCode interpreter\nProductivity\nWeb browsing\nDatabase\nFinance\nIntegration platforms\nAll tools and toolkits\nIntegrations by component\nTool integrations\nCopy page\n\nIntegrate with tools using LangChain Python.\n\nTools are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\nA toolkit is a collection of tools meant to be used together.\nï¿½\nSearch\nThe following table shows tools that execute online searches in some shape or form:\nTool/Toolkit\tFree/Paid\tReturn Data\nBing Search\tPaid\tURL, Snippet, Title\nBrave Search\tFree\tURL, Snippet, Title\ncloro\tPaid\tURL, Snippet, Title, Answer\nDuckDuckgoSearch\tFree\tURL, Snippet, Title\nExa Search\t1000 free searches/month\tURL, Author, Title, Published Date\nGoogle Search\tPaid\tURL, Snippet, Title\nGoogle Serper\tFree\tURL, Snippet, Title, Search Rank, Site Links\nJina Search\t1M Response Tokens Free\tURL, Snippet, Title, Page Content\nMojeek Search\tPaid\tURL, Snippet, Title\nNimble Search\tFree trial available\tURL, Content, Title\nParallel Search\tPaid\tURL, Title, Excerpts\nSearchApi\t100 Free Searches on Sign Up\tURL, Snippet, Title, Search Rank, Site Links, Authors\nSearxNG Search\tFree\tURL, Snippet, Title, Category\nSerpApi\t250 Free Searches/Month\tAnswer\nTavily Search\t1000 free searches/month\tURL, Content, Title, Images, Answer\nApify\tFree tier, pay-per-use (varies by Actor)\tActor output (varies by Actor)\nYou.com Search\tFree for 60 days\tURL, Title, Page Content\nï¿½\nCode interpreter\nThe following table shows tools that can be used as code interpreters:\nTool/Toolkit\tSupported Languages\tSandbox Lifetime\tSupports File Uploads\tReturn Types\tSupports Self-Hosting\nAmazon Bedrock AgentCore Code Interpreter\tPython, JavaScript, TypeScript\tConfigurable (up to 8 hours)\tâœ…\tText, Images, Files\tâŒ\nAzure Container Apps dynamic sessions\tPython\t1 Hour\tâœ…\tText, Images\tâŒ\nBearly Code Interpreter\tPython\tResets on Execution\tâœ…\tText\tâŒ\nRiza Code Interpreter\tPython, JavaScript, PHP, Ruby\tResets on Execution\tâœ…\tText\tâœ…\nï¿½\nProductivity\nThe following table shows tools that can be used to automate tasks in productivity tools:\nTool/Toolkit\tPricing\nGitHub Toolkit\tFree\nGitLab Toolkit\tFree for personal project\nGmail Toolkit\tFree, with limit of 250 quota units per user per second\nInfobip Tool\tFree trial, with variable pricing after\nJira Toolkit\tFree, with rate limits\nOffice365 Toolkit\tFree with Office365, includes rate limits\nSlack Toolkit\tFree\nTwilio Tool\tFree trial, with pay-as-you-go pricing after\nï¿½\nWeb browsing\nThe following table shows tools that can be used to automate tasks in web browsers:\nTool/Toolkit\tPricing\tSupports Interacting with the Browser\nAgentQL Toolkit\tFree trial, with pay-as-you-go and flat rate plans after\tâœ…\nAmazon Bedrock AgentCore Browser\tPay-per-use (AWS)\tâœ…\nHyperbrowser Browser Agent Tools\tFree trial, with flat rate plans and pre-paid credits after\tâœ…\nHyperbrowser Web Scraping Tools\tFree trial, with flat rate plans and pre-paid credits after\tâŒ\nMultiOn Toolkit\t40 free requests/day\tâœ…\nNimble Extract\tFree trial available\tâŒ\nOxylabs Web Scraper API\tFree trial, with flat rate plans and pre-paid credits after\tâŒ\nPlayWright Browser Toolkit\tFree\tâœ…\nRequests Toolkit\tFree\tâŒ\nï¿½\nDatabase\nThe following table shows tools that can be used to automate tasks in databases:\nTool/Toolkit\tAllowed Operations\nCassandra Database Toolkit\tSELECT and schema introspection\nMCP Toolbox\tAny SQL operation\nSQLDatabase Toolkit\tAny SQL operation\nSpark SQL Toolkit\tAny SQL operation\nDrasi Toolkit\tReal-time database change detection\nStardog\tSPARQL SELECT and schema introspection\nï¿½\nFinance\nThe following table shows tools that can be used to execute financial transactions such as payments, purchases, and more:\nTool/Toolkit\tPricing\tCapabilities\nAmpersend\tPaid\tPay for and use remote AI agent services with automatic x402 payment handling.\nGOAT\tFree\tCreate and receive payments, purchase physical goods, make investments, and more.\nPrivy\tFree\tCreate wallets with configurable permissions and execute transactions with speed.\nï¿½\nIntegration platforms\nThe following platforms provide access to multiple tools and services through a unified interface:\nTool/Toolkit\tNumber of Integrations\tPricing\tKey Features\nComposio\t500+\tFree tier available\tOAuth handling, event-driven workflows, multi-user support\nï¿½\nAll tools and toolkits\nADS4GPTs\nView guide\nAgentQL\nView guide\nAINetwork Toolkit\nView guide\nAlpha Vantage\nView guide\nAmpersend\nView guide\nAmadeus Toolkit\nView guide\nAmazon Bedrock AgentCore Browser\nView guide\nAmazon Bedrock AgentCore Code Interpreter\nView guide\nAnchor Browser\nView guide\nApify\nView guide\nArXiv\nView guide\nAskNews\nView guide\nAWS Lambda\nView guide\nAzure AI Services Toolkit\nView guide\nAzure Cognitive Services Toolkit\nView guide\nAzure Container Apps Dynamic Sessions\nView guide\nShell (bash)\nView guide\nBearly Code Interpreter\nView guide\nBing Search\nView guide\nBodo DataFrames\nView guide\ncloro\nView guide\nBrave Search\nView guide\nBrightData Web Scraper API\nView guide\nBrightData SERP\nView guide\nBrightData Unlocker\nView guide\nCAMB AI\nView guide\nCassandra Database Toolkit\nView guide\nCDP\nView guide\nChatGPT Plugins\nView guide\nClickUp Toolkit\nView guide\nCogniswitch Toolkit\nView guide\nCompass DeFi Toolkit\nView guide\nComposio\nView guide\nConnery Toolkit\nView guide\nDall-E Image Generator\nView guide\nDappier\nView guide\nDatabricks Unity Catalog\nView guide\nDataForSEO\nView guide\nDataherald\nView guide\nDaytona Data Analysis\nView guide\nDuckDuckGo Search\nView guide\nDiscord\nView guide\nDrasi\nView guide\nE2B Data Analysis\nView guide\nEden AI\nView guide\nElevenLabs Text2Speech\nView guide\nExa Search\nView guide\nFile System\nView guide\nFinancial Datasets Toolkit\nView guide\nFMP Data\nView guide\nGitHub Toolkit\nView guide\nGitLab Toolkit\nView guide\nGmail Toolkit\nView guide\nGOAT\nView guide\nPrivy\nView guide\nGolden Query\nView guide\nGoogle Books\nView guide\nGoogle Calendar Toolkit\nView guide\nGoogle Cloud Text-to-Speech\nView guide\nGoogle Drive\nView guide\nGoogle Finance\nView guide\nGoogle Imagen\nView guide\nGoogle Jobs\nView guide\nGoogle Lens\nView guide\nGoogle Places\nView guide\nGoogle Scholar\nView guide\nGoogle Search\nView guide\nGoogle Serper\nView guide\nGoogle Trends\nView guide\nGradio\nView guide\nGraphQL\nView guide\nHuggingFace Hub Tools\nView guide\nHuman as a Tool\nView guide\nHyperbrowser Browser Agent Tools\nView guide\nHyperbrowser Web Scraping Tools\nView guide\nIBM watsonx.ai\nView guide\nIBM watsonx.ai (SQL)\nView guide\nIFTTT WebHooks\nView guide\nInfobip\nView guide\nIonic Shopping Tool\nView guide\nJenkins\nView guide\nJina Search\nView guide\nJira Toolkit\nView guide\nJSON Toolkit\nView guide\nLemon Agent\nView guide\nLinkup Search Tool\nView guide\nMemgraph\nView guide\nMemorize\nView guide\nMojeek Search\nView guide\nMultiOn Toolkit\nView guide\nNASA Toolkit\nView guide\nNaver Search\nView guide\nNuclia Understanding\nView guide\nNimble Extract\nView guide\nNimble Search\nView guide\nNVIDIA Riva\nView guide\nOffice365 Toolkit\nView guide\nOpenAPI Toolkit\nView guide\nNatural Language API Toolkits\nView guide\nOpenGradient\nView guide\nOpenWeatherMap\nView guide\nOracle AI Database\nView guide\nOxylabs\nView guide\nPandas Dataframe\nView guide\nPassio NutritionAI\nView guide\nParallel Extract\nView guide\nParallel Search\nView guide\nPermit\nView guide\nPlayWright Browser Toolkit\nView guide\nPolygon IO Toolkit\nView guide\nPowerBI Toolkit\nView guide\nProlog\nView guide\nPubMed\nView guide\nPython REPL\nView guide\nReddit Search\nView guide\nRequests Toolkit\nView guide\nRiza Code Interpreter\nView guide\nRobocorp Toolkit\nView guide\nSalesforce\nView guide\nSceneXplain\nView guide\nScrapeGraph\nView guide\nScrapeless Crawl\nView guide\nScrapeless Scraping API\nView guide\nScrapeless Universal Scraping\nView guide\nSearchApi\nView guide\nSearxNG Search\nView guide\nSemantic Scholar API\nView guide\nSerpApi\nView guide\nSlack Toolkit\nView guide\nSpark SQL Toolkit\nView guide\nSQLDatabase Toolkit\nView guide\nStackExchange\nView guide\nStardog\nView guide\nSteam Toolkit\nView guide\nStripe\nView guide\nTableau\nView guide\nTaiga\nView guide\nTavily Extract\nView guide\nTavily Search\nView guide\nTilores\nView guide\nMCP Toolbox\nView guide\nTwilio\nView guide\nUpstage\nView guide\nValthera\nView guide\nValyuContext\nView guide\nVectara\nView guide\nWikidata\nView guide\nWikipedia\nView guide\nWolfram Alpha\nView guide\nWRITER Tools\nView guide\nYahoo Finance News\nView guide\nYou.com Search\nView guide\nYouTube\nView guide\nZapier Natural Language Actions\nView guide\nZenGuard AI\nView guide\nIf youâ€™d like to contribute an integration, see Contributing integrations.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494425,
    "timestamp": "2026-02-25T08:10:59.806Z",
    "title": "Middleware integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/middleware",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nIntegrations by component\nMiddleware integrations\nCopy page\n\nIntegrate with middleware using LangChain Python.\n\nMiddleware designed for specific providers. Learn more about middleware.\nProvider\tMiddleware available\nAnthropic\tPrompt caching, bash tool, text editor, memory, and file search\nOpenAI\tContent moderation\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494422,
    "timestamp": "2026-02-25T08:10:59.811Z",
    "title": "Groq integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/providers/groq",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nModel interfaces\nPopular Providers\nGroq integrations\nCopy page\n\nIntegrate with Groq using LangChain Python.\n\nThis page makes reference to Groq, an AI hardware and software company. For information on how to use Grok models (provided by xAI), see the xAI provider page.\nï¿½\nModel interfaces\nChatGroq\nInterface to chat models hosted on the Groq platform.\nGet started\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494426,
    "timestamp": "2026-02-25T08:10:59.817Z",
    "title": "Retriever integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/retrievers",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nBring-your-own documents\nExternal index\nAll retrievers\nIntegrations by component\nRetriever integrations\nCopy page\n\nIntegrate with retrievers using LangChain Python.\n\nA retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Retrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra.\nRetrievers accept a string query as input and return a list of Document objects as output.\nNote that all vector stores can be cast to retrievers. Refer to the vector store integration docs for available vector stores. This page lists custom retrievers, implemented via subclassing BaseRetriever.\nï¿½\nBring-your-own documents\nThe below retrievers allow you to index and search a custom corpus of documents.\nRetriever\tSelf-host\tCloud offering\tPackage\nAmazonKnowledgeBasesRetriever\tâŒ\tâœ…\tlangchain-aws\nAzureAISearchRetriever\tâŒ\tâœ…\tlangchain-community\nElasticsearchRetriever\tâœ…\tâœ…\tlangchain-elasticsearch\nVertexAISearchRetriever\tâŒ\tâœ…\tlangchain-google-community\nï¿½\nExternal index\nThe below retrievers will search over an external index (e.g., constructed from Internet data or similar).\nRetriever\tSource\tPackage\nArxivRetriever\tScholarly articles on arxiv.org\tlangchain-community\nTavilySearchAPIRetriever\tInternet search\tlangchain-community\nWikipediaRetriever\tWikipedia articles\tlangchain-community\nï¿½\nAll retrievers\nActiveloop Deep Memory\nView guide\nAmazon Kendra\nView guide\nArcee\nView guide\nArxiv\nView guide\nAskNews\nView guide\nAzure AI Search\nView guide\nBedrock (Knowledge Bases)\nView guide\nBM25\nView guide\nBox\nView guide\nBREEBS (Open Knowledge)\nView guide\nChaindesk\nView guide\nChatGPT plugin\nView guide\nCognee\nView guide\nCohere reranker\nView guide\nCohere RAG\nView guide\nContextual AI Reranker\nView guide\nDappier\nView guide\nDocArray\nView guide\nDria\nView guide\nElasticSearch BM25\nView guide\nElasticsearch\nView guide\nEgnyte\nView guide\nEmbedchain\nView guide\nFlashRank reranker\nView guide\nFleet AI Context\nView guide\nGalaxia\nView guide\nGoogle Drive\nView guide\nGoogle Vertex AI Search\nView guide\nGraph RAG\nView guide\nGreenNode\nView guide\nIBM watsonx.ai\nView guide\nJaguarDB Vector Database\nView guide\nKay.ai\nView guide\nKinetica Vectorstore\nView guide\nkNN\nView guide\nLinkupSearchRetriever\nView guide\nLLMLingua Document Compressor\nView guide\nLOTR (Merger Retriever)\nView guide\nMetal\nView guide\nNanoPQ (Product Quantization)\nView guide\nNebius\nView guide\nneedle\nView guide\nNimble Extract\nView guide\nNimble Search\nView guide\nOutline\nView guide\nPermit\nView guide\nPinecone Hybrid Search\nView guide\nPinecone Rerank\nView guide\nPubMed\nView guide\nQdrant Sparse Vector\nView guide\nRAGatouille\nView guide\nRePhraseQuery\nView guide\nRememberizer\nView guide\nSEC filing\nView guide\nSVM\nView guide\nTavilySearchAPI\nView guide\nTF-IDF\nView guide\nNeuralDB\nView guide\nValyuContext\nView guide\nVectorize\nView guide\nVespa\nView guide\nWikipedia\nView guide\nYou.com\nView guide\nZep Cloud\nView guide\nZep Open Source\nView guide\nZotero\nView guide\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494427,
    "timestamp": "2026-02-25T08:10:59.822Z",
    "title": "Text splitter integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/splitters",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nText structure-based\nLength-based\nDocument structure-based\nIntegrations by component\nText splitter integrations\nCopy page\n\nIntegrate with text splitters using LangChain.\n\npip\nuv\nCopy\npip install -U langchain-text-splitters\n\nText splitters break large docs into smaller chunks that will be retrievable individually and fit within model context window limit.\nThere are several strategies for splitting documents, each with its own advantages.\nFor most use cases, start with the RecursiveCharacterTextSplitter. It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.\nï¿½\nText structure-based\nText is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChainâ€™s RecursiveCharacterTextSplitter implements this concept:\nThe RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\nIf a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\nThis process continues down to the word level if necessary.\nExample usage:\nCopy\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\ntexts = text_splitter.split_text(document)\n\nAvailable text splitters:\nRecursively split text\nï¿½\nLength-based\nAn intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesnâ€™t exceed a specified size limit. Key benefits of length-based splitting:\nStraightforward implementation\nConsistent chunk sizes\nEasily adaptable to different model requirements\nTypes of length-based splitting:\nToken-based: Splits text based on the number of tokens, which is useful when working with language models.\nCharacter-based: Splits text based on the number of characters, which can be more consistent across different types of text.\nExample implementation using LangChainâ€™s CharacterTextSplitter with token-based splitting:\nCopy\nfrom langchain_text_splitters import CharacterTextSplitter\n\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n)\ntexts = text_splitter.split_text(document)\n\nAvailable text splitters:\nSplit by tokens\nSplit by characters\nï¿½\nDocument structure-based\nSome documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, itâ€™s beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:\nPreserves the logical organization of the document\nMaintains context within each chunk\nCan be more effective for downstream tasks like retrieval or summarization\nExamples of structure-based splitting:\nMarkdown: Split based on headers (e.g., #, ##, ###)\nHTML: Split using tags\nJSON: Split by object or array elements\nCode: Split by functions, classes, or logical blocks\nAvailable text splitters:\nSplit Markdown\nSplit JSON\nSplit code\nSplit HTML\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494428,
    "timestamp": "2026-02-25T08:10:59.824Z",
    "title": "Embedding model integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/text_embedding",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nOverview\nHow it works\nSimilarity metrics\nInterface\nTop integrations\nCaching\nAll embedding models\nIntegrations by component\nEmbedding model integrations\nCopy page\n\nIntegrate with embedding models using LangChain Python.\n\nï¿½\nOverview\nThis overview covers text-based embedding models. LangChain does not currently support multimodal embeddings.\nSee top embedding models.\nEmbedding models transform raw textâ€”such as a sentence, paragraph, or tweetâ€”into a fixed-length vector of numbers that captures its semantic meaning. These vectors allow machines to compare and search text based on meaning rather than exact words.\nIn practice, this means that texts with similar ideas are placed close together in the vector space. For example, instead of matching only the phrase â€œmachine learningâ€, embeddings can surface documents that discuss related concepts even when different wording is used.\nï¿½\nHow it works\nVectorization â€” The model encodes each input string as a high-dimensional vector.\nSimilarity scoring â€” Vectors are compared using mathematical metrics to measure how closely related the underlying texts are.\nï¿½\nSimilarity metrics\nSeveral metrics are commonly used to compare embeddings:\nCosine similarity â€” measures the angle between two vectors.\nEuclidean distance â€” measures the straight-line distance between points.\nDot product â€” measures how much one vector projects onto another.\nHereâ€™s an example of computing cosine similarity between two vectors:\nCopy\nimport numpy as np\n\ndef cosine_similarity(vec1, vec2):\n    dot = np.dot(vec1, vec2)\n    return dot / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\nsimilarity = cosine_similarity(query_embedding, document_embedding)\nprint(\"Cosine Similarity:\", similarity)\n\nï¿½\nInterface\nLangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the Embeddings interface.\nTwo main methods are available:\nembed_documents(texts: List[str]) â†’ List[List[float]]: Embeds a list of documents.\nembed_query(text: str) â†’ List[float]: Embeds a single query.\nThe interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.\nï¿½\nTop integrations\nModel\tPackage\nOpenAIEmbeddings\tlangchain-openai\nAzureOpenAIEmbeddings\tlangchain-openai\nGoogleGenerativeAIEmbeddings\tlangchain-google-genai\nOllamaEmbeddings\tlangchain-ollama\nTogetherEmbeddings\tlangchain-together\nFireworksEmbeddings\tlangchain-fireworks\nMistralAIEmbeddings\tlangchain-mistralai\nVoyageAIEmbeddings\tlangchain-voyageai\nCohereEmbeddings\tlangchain-cohere\nNomicEmbeddings\tlangchain-nomic\nFakeEmbeddings\tlangchain-core\nDatabricksEmbeddings\tdatabricks-langchain\nWatsonxEmbeddings\tlangchain-ibm\nNVIDIAEmbeddings\tlangchain-nvidia\nAIMLAPIEmbeddings\tlangchain-aimlapi\nï¿½\nCaching\nEmbeddings can be stored or temporarily cached to avoid needing to recompute them.\nCaching embeddings can be done using a CacheBackedEmbeddings. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.\nThe main supported way to initialize a CacheBackedEmbeddings is from_bytes_store. It takes the following parameters:\nunderlying_embedder: The embedder to use for embedding.\ndocument_embedding_cache: Any ByteStore for caching document embeddings.\nbatch_size: (optional, defaults to None) The number of documents to embed between store updates.\nnamespace: (optional, defaults to \"\") The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).\nquery_embedding_cache: (optional, defaults to None) A ByteStore for caching query embeddings, or True to reuse the same store as document_embedding_cache.\nCopy\nimport time\nfrom langchain_classic.embeddings import CacheBackedEmbeddings  \nfrom langchain_classic.storage import LocalFileStore \nfrom langchain_core.vectorstores import InMemoryVectorStore\n\n# Create your underlying embeddings model\nunderlying_embeddings = ... # e.g., OpenAIEmbeddings(), HuggingFaceEmbeddings(), etc.\n\n# Store persists embeddings to the local filesystem\n# This isn't for production use, but is useful for local\nstore = LocalFileStore(\"./cache/\") \n\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(\n    underlying_embeddings,\n    store,\n    namespace=underlying_embeddings.model\n)\n\n# Example: caching a query embedding\ntic = time.time()\nprint(cached_embedder.embed_query(\"Hello, world!\"))\nprint(f\"First call took: {time.time() - tic:.2f} seconds\")\n\n# Subsequent calls use the cache\ntic = time.time()\nprint(cached_embedder.embed_query(\"Hello, world!\"))\nprint(f\"Second call took: {time.time() - tic:.2f} seconds\")\n\nIn production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see stores integrations for options.\nï¿½\nAll embedding models\nAleph Alpha\nView guide\nAnyscale\nView guide\nAscend\nView guide\nAI/ML API\nView guide\nAwaDB\nView guide\nAzureOpenAI\nView guide\nBaichuan Text Embeddings\nView guide\nBaidu Qianfan\nView guide\nBaseten\nView guide\nBedrock\nView guide\nBGE on Hugging Face\nView guide\nBookend AI\nView guide\nClarifai\nView guide\nCloudflare Workers AI\nView guide\nClova Embeddings\nView guide\nCohere\nView guide\nDashScope\nView guide\nDatabricks\nView guide\nDeepInfra\nView guide\nEDEN AI\nView guide\nElasticsearch\nView guide\nEmbaas\nView guide\nFake Embeddings\nView guide\nFastEmbed by Qdrant\nView guide\nFireworks\nView guide\nGoogle Gemini\nView guide\nGoogle Vertex AI\nView guide\nGPT4All\nView guide\nGradient\nView guide\nGreenNode\nView guide\nHugging Face\nView guide\nIBM watsonx.ai\nView guide\nInfinity\nView guide\nInstruct Embeddings\nView guide\nIPEX-LLM CPU\nView guide\nIPEX-LLM GPU\nView guide\nIsaacus\nView guide\nIntel Extension for Transformers\nView guide\nJina\nView guide\nJohn Snow Labs\nView guide\nLASER\nView guide\nLindorm\nView guide\nLlama.cpp\nView guide\nLLMRails\nView guide\nLocalAI\nView guide\nMiniMax\nView guide\nMistralAI\nView guide\nModel2Vec\nView guide\nModelScope\nView guide\nMosaicML\nView guide\nNaver\nView guide\nNebius\nView guide\nNetmind\nView guide\nNLP Cloud\nView guide\nNomic\nView guide\nNVIDIA NIMs\nView guide\nOracle Cloud Infrastructure\nView guide\nOllama\nView guide\nOpenClip\nView guide\nOpenAI\nView guide\nOpenVINO\nView guide\nOptimum Intel\nView guide\nOracle AI Database\nView guide\nOVHcloud\nView guide\nPinecone Embeddings\nView guide\nPredictionGuard\nView guide\nPremAI\nView guide\nSageMaker\nView guide\nSambaNova\nView guide\nSelf Hosted\nView guide\nSentence Transformers\nView guide\nSolar\nView guide\nSpaCy\nView guide\nSparkLLM\nView guide\nTensorFlow Hub\nView guide\nText Embeddings Inference\nView guide\nTextEmbed\nView guide\nTitan Takeoff\nView guide\nTogether AI\nView guide\nUpstage\nView guide\nVolc Engine\nView guide\nVoyage AI\nView guide\nXinference\nView guide\nYandexGPT\nView guide\nZhipuAI\nView guide\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494431,
    "timestamp": "2026-02-25T08:10:59.833Z",
    "title": "Store integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/stores",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nOverview\nInterface\nBuilt-in stores for local development\nCustom stores\nAll key-value stores\nIntegrations by component\nStore integrations\nCopy page\n\nIntegrate with stores using LangChain Python.\n\nï¿½\nOverview\nLangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching embeddings.\nï¿½\nInterface\nAll BaseStores support the following interface:\nmget(key: Sequence[str]) -> List[Optional[bytes]]: get the contents of multiple keys, returning None if the key does not exist\nmset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None: set the contents of multiple keys\nmdelete(key: Sequence[str]) -> None: delete multiple keys\nyield_keys(prefix: Optional[str] = None) -> Iterator[str]: yield all keys in the store, optionally filtering by a prefix\nBase stores are designed to work multiple key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.\nï¿½\nBuilt-in stores for local development\nInMemoryByteStore\nView guide\nLocalFileStore\nView guide\nï¿½\nCustom stores\nYou can also implement your own custom store by extending the BaseStore class. See the store interface documentation for more details.\nï¿½\nAll key-value stores\nAstraDBByteStore\nView guide\nCassandraByteStore\nView guide\nElasticsearchEmbeddingsCache\nView guide\nRedisStore\nView guide\nUpstashRedisByteStore\nView guide\nBigtableByteStore\nView guide\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494430,
    "timestamp": "2026-02-25T08:10:59.833Z",
    "title": "Document loader integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/document_loaders",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nInterface\nBy category\nWebpages\nPDFs\nCloud providers\nSocial platforms\nMessaging services\nProductivity tools\nCommon file types\nAll document loaders\nIntegrations by component\nDocument loader integrations\nCopy page\n\nIntegrate with document loaders using LangChain Python.\n\nDocument loaders provide a standard interface for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChainâ€™s Document format. This ensures that data can be handled consistently regardless of the source.\nAll document loaders implement the BaseLoader interface.\nï¿½\nInterface\nEach document loader may define its own parameters, but they share a common API:\nload() â€“ Loads all documents at once.\nlazy_load() â€“ Streams documents lazily, useful for large datasets.\nCopy\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\n\nloader = CSVLoader(\n    ...  # Integration-specific parameters here\n)\n\n# Load all documents\ndocuments = loader.load()\n\n# For large datasets, lazily load documents\nfor document in loader.lazy_load():\n    print(document)\n\nï¿½\nBy category\nï¿½\nWebpages\nThe below document loaders allow you to load webpages.\nDocument Loader\tDescription\tPackage/API\nWeb\tUses urllib and BeautifulSoup to load and parse HTML web pages\tPackage\nUnstructured\tUses Unstructured to load and parse web pages\tPackage\nRecursiveURL\tRecursively scrapes all child links from a root URL\tPackage\nSitemap\tScrapes all pages on a given sitemap\tPackage\nSpider\tCrawler and scraper that returns LLM-ready data\tAPI\nFirecrawl\tAPI service that can be deployed locally\tAPI\nApify Dataset\tLoad documents from Apify datasets\tAPI\nDocling\tUses Docling to load and parse web pages\tPackage\nHyperbrowser\tPlatform for running and scaling headless browsers, can be used to scrape/crawl any site\tAPI\nAgentQL\tWeb interaction and structured data extraction from any web page using an AgentQL query or a Natural Language prompt\tAPI\nï¿½\nPDFs\nThe below document loaders allow you to load PDF documents.\nDocument Loader\tDescription\tPackage/API\nPyPDF\tUses pypdf to load and parse PDFs\tPackage\nUnstructured\tUses Unstructuredâ€™s open source library to load PDFs\tPackage\nAmazon Textract\tUses AWS API to load PDFs\tAPI\nMathPix\tUses MathPix to load PDFs\tPackage\nPDFPlumber\tLoad PDF files using PDFPlumber\tPackage\nPyPDFDirectry\tLoad a directory with PDF files\tPackage\nPyPDFium2\tLoad PDF files using PyPDFium2\tPackage\nPyMuPDF\tLoad PDF files using PyMuPDF\tPackage\nPyMuPDF4LLM\tLoad PDF content to Markdown using PyMuPDF4LLM\tPackage\nPDFMiner\tLoad PDF files using PDFMiner\tPackage\nUpstage Document Parse Loader\tLoad PDF files using UpstageDocumentParseLoader\tPackage\nDocling\tLoad PDF files using Docling\tPackage\nUnDatasIO\tLoad PDF files using UnDatasIO\tPackage\nOpenDataLoader PDF\tLoad PDF files using OpenDataLoader PDF\tPackage\nï¿½\nCloud providers\nThe below document loaders allow you to load documents from your favorite cloud providers.\nDocument Loader\tDescription\tPartner Package\tAPI reference\nAWS S3 Directory\tLoad documents from an AWS S3 directory\tâŒ\tS3DirectoryLoader\nAWS S3 File\tLoad documents from an AWS S3 file\tâŒ\tS3FileLoader\nAzure AI Data\tLoad documents from Azure AI services\tâŒ\tAzureAIDataLoader\nAzure Blob Storage\tLoad documents from Azure Blob Storage\tâœ…\tAzureBlobStorageLoader\nDropbox\tLoad documents from Dropbox\tâŒ\tDropboxLoader\nGoogle Cloud Storage Directory\tLoad documents from GCS bucket\tâœ…\tGCSDirectoryLoader\nGoogle Cloud Storage File\tLoad documents from GCS file object\tâœ…\tGCSFileLoader\nGoogle Drive\tLoad documents from Google Drive (Google Docs only)\tâœ…\tGoogleDriveLoader\nHuawei OBS Directory\tLoad documents from Huawei Object Storage Service Directory\tâŒ\tOBSDirectoryLoader\nHuawei OBS File\tLoad documents from Huawei Object Storage Service File\tâŒ\tOBSFileLoader\nMicrosoft OneDrive\tLoad documents from Microsoft OneDrive\tâŒ\tOneDriveLoader\nMicrosoft SharePoint\tLoad documents from Microsoft SharePoint\tâŒ\tSharePointLoader\nTencent COS Directory\tLoad documents from Tencent Cloud Object Storage Directory\tâŒ\tTencentCOSDirectoryLoader\nTencent COS File\tLoad documents from Tencent Cloud Object Storage File\tâŒ\tTencentCOSFileLoader\nï¿½\nSocial platforms\nThe below document loaders allow you to load documents from different social media platforms.\nDocument Loader\tAPI reference\nTwitter\tTwitterTweetLoader\nReddit\tRedditPostsLoader\nï¿½\nMessaging services\nThe below document loaders allow you to load data from different messaging platforms.\nDocument Loader\tAPI reference\nTelegram\tTelegramChatFileLoader\nWhatsApp\tWhatsAppChatLoader\nDiscord\tDiscordChatLoader\nFacebook Chat\tFacebookChatLoader\nMastodon\tMastodonTootsLoader\nï¿½\nProductivity tools\nThe below document loaders allow you to load data from commonly used productivity tools.\nDocument Loader\tAPI reference\nFigma\tFigmaFileLoader\nNotion\tNotionDirectoryLoader\nSlack\tSlackDirectoryLoader\nQuip\tQuipLoader\nTrello\tTrelloLoader\nRoam\tRoamLoader\nGitHub\tGithubFileLoader\nï¿½\nCommon file types\nThe below document loaders allow you to load data from common data formats.\nDocument Loader\tData Type\nCSVLoader\tCSV files\nUnstructured\tMany file types (see https://docs.unstructured.io/platform/supported-file-types)\nJSONLoader\tJSON files\nBSHTMLLoader\tHTML files\nDoclingLoader\tVarious file types (see https://ds4sd.github.io/docling/)\nPolarisAIDataInsightLoader\tVarious file types (see https://datainsight.polarisoffice.com/documentation?docType=doc_extract)\nï¿½\nAll document loaders\nacreom\nView guide\nAgentQLLoader\nView guide\nAirbyteLoader\nView guide\nAirtable\nView guide\nAlibaba Cloud MaxCompute\nView guide\nAmazon Textract\nView guide\nApify Dataset\nView guide\nArxivLoader\nView guide\nAssemblyAI Audio Transcripts\nView guide\nAstraDB\nView guide\nAsync Chromium\nView guide\nAsyncHtml\nView guide\nAthena\nView guide\nAWS S3 Directory\nView guide\nAWS S3 File\nView guide\nAZLyrics\nView guide\nAzure AI Data\nView guide\nAzure Blob Storage\nView guide\nAzure AI Document Intelligence\nView guide\nBibTeX\nView guide\nBiliBili\nView guide\nBlackboard\nView guide\nBlockchain\nView guide\nBox\nView guide\nBrave Search\nView guide\nBrowserbase\nView guide\nBrowserless\nView guide\nBSHTMLLoader\nView guide\nCassandra\nView guide\nChatGPT Data\nView guide\nCollege Confidential\nView guide\nConcurrent Loader\nView guide\nConfluence\nView guide\nCoNLL-U\nView guide\nCopy Paste\nView guide\nCouchbase\nView guide\nCSV\nView guide\nCube Semantic Layer\nView guide\nDatadog Logs\nView guide\nDedoc\nView guide\nDiffbot\nView guide\nDiscord\nView guide\nDocling\nView guide\nDocugami\nView guide\nDocusaurus\nView guide\nDropbox\nView guide\nEmail\nView guide\nEPub\nView guide\nEtherscan\nView guide\nEverNote\nView guide\nFacebook Chat\nView guide\nFauna\nView guide\nFigma\nView guide\nFireCrawl\nView guide\nGeopandas\nView guide\nGit\nView guide\nGitBook\nView guide\nGitHub\nView guide\nGlue Catalog\nView guide\nGoogle AlloyDB for PostgreSQL\nView guide\nGoogle BigQuery\nView guide\nGoogle Bigtable\nView guide\nGoogle Cloud SQL for SQL Server\nView guide\nGoogle Cloud SQL for MySQL\nView guide\nGoogle Cloud SQL for PostgreSQL\nView guide\nGoogle Cloud Storage Directory\nView guide\nGoogle Cloud Storage File\nView guide\nGoogle Firestore in Datastore Mode\nView guide\nGoogle Drive\nView guide\nGoogle El Carro for Oracle Workloads\nView guide\nGoogle Firestore (Native Mode)\nView guide\nGoogle Memorystore for Redis\nView guide\nGoogle Spanner\nView guide\nGoogle Speech-to-Text\nView guide\nGrobid\nView guide\nGutenberg\nView guide\nHacker News\nView guide\nHuawei OBS Directory\nView guide\nHuawei OBS File\nView guide\nHuggingFace Dataset\nView guide\nHyperbrowserLoader\nView guide\niFixit\nView guide\nImages\nView guide\nImage Captions\nView guide\nIMSDb\nView guide\nIugu\nView guide\nJoplin\nView guide\nJSONLoader\nView guide\nJupyter Notebook\nView guide\nKinetica\nView guide\nlakeFS\nView guide\nLangSmith\nView guide\nLarkSuite (FeiShu)\nView guide\nLLM Sherpa\nView guide\nMastodon\nView guide\nMathPixPDFLoader\nView guide\nMediaWiki Dump\nView guide\nMerge Documents Loader\nView guide\nMHTML\nView guide\nMicrosoft Excel\nView guide\nMicrosoft OneDrive\nView guide\nMicrosoft OneNote\nView guide\nMicrosoft PowerPoint\nView guide\nMicrosoft SharePoint\nView guide\nMicrosoft Word\nView guide\nNear Blockchain\nView guide\nModern Treasury\nView guide\nMongoDB\nView guide\nNeedle Document Loader\nView guide\nNews URL\nView guide\nNotion DB\nView guide\nNuclia\nView guide\nObsidian\nView guide\nOpenDataLoader PDF\nView guide\nOpen Document Format (ODT)\nView guide\nOpen City Data\nView guide\nOracle Autonomous Database\nView guide\nOracle AI Database\nView guide\nOrg-mode\nView guide\nOutline Document Loader\nView guide\nPandas DataFrame\nView guide\nPDFMinerLoader\nView guide\nPDFPlumber\nView guide\nPebblo Safe DocumentLoader\nView guide\nPolaris AI DataInsight\nView guide\nPolars DataFrame\nView guide\nDell PowerScale\nView guide\nPsychic\nView guide\nPubMed\nView guide\nPyMuPDFLoader\nView guide\nPyMuPDF4LLM\nView guide\nPyPDFDirectoryLoader\nView guide\nPyPDFium2Loader\nView guide\nPyPDFLoader\nView guide\nPySpark\nView guide\nQuip\nView guide\nReadTheDocs Documentation\nView guide\nRecursive URL\nView guide\nReddit\nView guide\nRoam\nView guide\nRockset\nView guide\nrspace\nView guide\nRSS Feeds\nView guide\nRST\nView guide\nscrapfly\nView guide\nScrapingAnt\nView guide\nSingleStore\nView guide\nSitemap\nView guide\nSlack\nView guide\nSnowflake\nView guide\nSoniox\nView guide\nSource Code\nView guide\nSpider\nView guide\nSpreedly\nView guide\nStripe\nView guide\nSubtitle\nView guide\nSurrealDB\nView guide\nTelegram\nView guide\nTencent COS Directory\nView guide\nTencent COS File\nView guide\nTensorFlow Datasets\nView guide\nTiDB\nView guide\n2Markdown\nView guide\nTOML\nView guide\nTrello\nView guide\nTSV\nView guide\nTwitter\nView guide\nUnDatasIO\nView guide\nUnstructured\nView guide\nUnstructuredMarkdownLoader\nView guide\nUnstructuredPDFLoader\nView guide\nUpstage\nView guide\nURL\nView guide\nVsdx\nView guide\nWeather\nView guide\nWebBaseLoader\nView guide\nWhatsApp Chat\nView guide\nWikipedia\nView guide\nUnstructuredXMLLoader\nView guide\nXorbits Pandas DataFrame\nView guide\nYouTube Audio\nView guide\nYouTube Transcripts\nView guide\nYoutubeLoaderDL\nView guide\nYuque\nView guide\nZeroxPDFLoader\nView guide\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494429,
    "timestamp": "2026-02-25T08:10:59.840Z",
    "title": "Vector store integrations - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/integrations/vectorstores",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLangChain integrations\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nHugging Face\nMicrosoft\nOllama\nGroq\nIntegrations by component\nChat models\nTools and toolkits\nMiddleware\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nOn this page\nOverview\nInterface\nInitialization\nAdding documents\nDeleting documents\nSimilarity search\nSimilarity metrics & indexing\nMetadata filtering\nTop integrations\nAll vector stores\nIntegrations by component\nVector store integrations\nCopy page\n\nIntegrate with vector stores using LangChain Python.\n\nï¿½\nOverview\nA vector stores embedded data and performs similarity search.\n\nðŸ“¤ Query phase (retrieval)\n\nðŸ“¥ Indexing phase (store)\n\nðŸ“„ Documents\n\nðŸ”¢ Embedding model\n\nðŸ”˜ Embedding vectors\n\nVector store\n\nâ“ Query text\n\nðŸ”¢ Embedding model\n\nðŸ”˜ Query vector\n\nðŸ” Similarity search\n\nðŸ“„ Top-k results\n\nï¿½\nInterface\nLangChain provides a unified interface for vector stores, allowing you to:\nadd_documents - Add documents to the store.\ndelete - Remove stored documents by ID.\nsimilarity_search - Query for semantically similar documents.\nThis abstraction lets you switch between different implementations without altering your application logic.\nï¿½\nInitialization\nTo initialize a vector store, provide it with an embedding model:\nCopy\nfrom langchain_core.vectorstores import InMemoryVectorStore\nvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())\n\nï¿½\nAdding documents\nAdd Document objects (holding page_content and optional metadata) like so:\nCopy\nvector_store.add_documents(documents=[doc1, doc2], ids=[\"id1\", \"id2\"])\n\nï¿½\nDeleting documents\nDelete by specifying IDs:\nCopy\nvector_store.delete(ids=[\"id1\"])\n\nï¿½\nSimilarity search\nIssue a semantic query using similarity_search, which returns the closest embedded documents:\nCopy\nsimilar_docs = vector_store.similarity_search(\"your query here\")\n\nMany vector stores support parameters like:\nk â€” number of results to return\nfilter â€” conditional filtering based on metadata\nï¿½\nSimilarity metrics & indexing\nEmbedding similarity may be computed using:\nCosine similarity\nEuclidean distance\nDot product\nEfficient search often employs indexing methods such as HNSW (Hierarchical Navigable Small World), though specifics depend on the vector store.\nï¿½\nMetadata filtering\nFiltering by metadata (e.g., source, date) can refine search results:\nCopy\nvector_store.similarity_search(\n  \"query\",\n  k=3,\n  filter={\"source\": \"tweets\"}\n)\n\nï¿½\nTop integrations\nSelect embedding model:\n\nOpenAI\n\nAzure\n\nGoogle Gemini\n\nGoogle Vertex\n\nAWS\n\nHuggingFace\n\nOllama\n\nCohere\n\nMistral AI\n\nNomic\n\nNVIDIA\n\nVoyage AI\n\nIBM watsonx\n\nFake\n\nxAI\n\nPerplexity\n\nDeepSeek\n\nSelect vector store:\n\nIn-memory\n\nAmazon OpenSearch\n\nAstra DB\n\nAzure Cosmos DB NoSQL\n\nAzure Cosmos DB Mongo vCore\n\nChroma\n\nCockroachDB\n\nFAISS\n\nMilvus\n\nMongoDB\n\nPGVector\n\nPGVectorStore\n\nPinecone\n\nQdrant\n\nOracle AI Database\n\nVectorstore\tDelete by ID\tFiltering\tSearch by Vector\tSearch with score\tAsync\tPasses Standard Tests\tMulti Tenancy\tIDs in add Documents\nAstraDBVectorStore\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\nAzureCosmosDBNoSqlVectorStore\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\tâœ…\tâœ…\nAzureCosmosDBMongoVCoreVectorStore\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\tâœ…\tâœ…\nChroma\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\nClickhouse\tâœ…\tâœ…\tâŒ\tâœ…\tâŒ\tâŒ\tâŒ\tâœ…\nAsyncCockroachDBVectorStore\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\nCouchbaseSearchVectorStore\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\tâœ…\nDatabricksVectorSearch\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâŒ\tâœ…\nElasticsearchStore\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâŒ\tâœ…\nFAISS\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâŒ\tâœ…\nInMemoryVectorStore\tâœ…\tâœ…\tâŒ\tâœ…\tâœ…\tâŒ\tâŒ\tâœ…\nLambdaDB\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\nMilvus\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\nMoorcheh\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\nMongoDBAtlasVectorSearch\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\nopenGauss\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\tâŒ\tâœ…\nPGVector\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâŒ\tâœ…\nPGVectorStore\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\nPineconeVectorStore\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\tâŒ\tâŒ\tâœ…\nQdrantVectorStore\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\tâœ…\nWeaviate\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\tâœ…\nSQLServer\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâŒ\tâŒ\tâœ…\nZeusDB\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\nOracle AI Database\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâœ…\tâŒ\tâœ…\nï¿½\nAll vector stores\nActiveloop Deep Lake\nView guide\nAlibaba Cloud MySQL\nView guide\nAlibaba Cloud OpenSearch\nView guide\nAnalyticDB\nView guide\nAnnoy\nView guide\nApache Doris\nView guide\nApertureDB\nView guide\nAstra DB Vector Store\nView guide\nAtlas\nView guide\nAwaDB\nView guide\nAzure Cosmos DB Mongo vCore\nView guide\nAzure Cosmos DB No SQL\nView guide\nAzure Database for PostgreSQL - Flexible Server\nView guide\nAzure AI Search\nView guide\nBagel\nView guide\nBagelDB\nView guide\nBaidu Cloud ElasticSearch VectorSearch\nView guide\nBaidu VectorDB\nView guide\nApache Cassandra\nView guide\nChroma\nView guide\nClarifai\nView guide\nClickHouse\nView guide\nCockroachDB\nView guide\nCouchbase\nView guide\nDashVector\nView guide\nDatabricks\nView guide\nIBM Db2\nView guide\nDingoDB\nView guide\nDocArray HnswSearch\nView guide\nDocArray InMemorySearch\nView guide\nAmazon Document DB\nView guide\nDuckDB\nView guide\nChina Mobile ECloud ElasticSearch\nView guide\nElasticsearch\nView guide\nEpsilla\nView guide\nFaiss\nView guide\nFaiss (Async)\nView guide\nFalkorDB\nView guide\nGel\nView guide\nGoogle AlloyDB\nView guide\nGoogle BigQuery Vector Search\nView guide\nGoogle Cloud SQL for MySQL\nView guide\nGoogle Cloud SQL for PostgreSQL\nView guide\nFirestore\nView guide\nGoogle Memorystore for Redis\nView guide\nGoogle Spanner\nView guide\nGoogle Bigtable\nView guide\nGoogle Vertex AI Feature Store\nView guide\nGoogle Vertex AI Vector Search\nView guide\nHippo\nView guide\nHologres\nView guide\nJaguar Vector Database\nView guide\nKinetica\nView guide\nLambdaDB\nView guide\nLanceDB\nView guide\nLantern\nView guide\nLindorm\nView guide\nLLMRails\nView guide\nManticoreSearch\nView guide\nMariaDB\nView guide\nMarqo\nView guide\nMeilisearch\nView guide\nAmazon MemoryDB\nView guide\nMilvus\nView guide\nMomento Vector Index\nView guide\nMoorcheh\nView guide\nMongoDB Atlas\nView guide\nMyScale\nView guide\nNeo4j Vector Index\nView guide\nNucliaDB\nView guide\nOceanbase\nView guide\nopenGauss\nView guide\nOpenSearch\nView guide\nOracle AI Database\nView guide\nPathway\nView guide\nPostgres Embedding\nView guide\nPGVecto.rs\nView guide\nPGVector\nView guide\nPGVectorStore\nView guide\nPinecone\nView guide\nPinecone (sparse)\nView guide\nQdrant\nView guide\nRelyt\nView guide\nRockset\nView guide\nSAP HANA Cloud Vector Engine\nView guide\nScaNN\nView guide\nSemaDB\nView guide\nSingleStore\nView guide\nscikit-learn\nView guide\nSQLiteVec\nView guide\nSQLite-VSS\nView guide\nSQLServer\nView guide\nStarRocks\nView guide\nSupabase\nView guide\nSurrealDB\nView guide\nTablestore\nView guide\nTair\nView guide\nTencent Cloud VectorDB\nView guide\nTeradata VectorStore\nView guide\nThirdAI NeuralDB\nView guide\nTiDB Vector\nView guide\nTigris\nView guide\nTileDB\nView guide\nTimescale Vector\nView guide\nTypesense\nView guide\nUpstash Vector\nView guide\nUSearch\nView guide\nVald\nView guide\nVDMS\nView guide\nveDB for MySQL\nView guide\nVearch\nView guide\nVectara\nView guide\nVespa\nView guide\nviking DB\nView guide\nvlite\nView guide\nVolcengine RDS for MySQL\nView guide\nWeaviate\nView guide\nXata\nView guide\nYDB\nView guide\nYellowbrick\nView guide\nZep\nView guide\nZep Cloud\nView guide\nZeusDB\nView guide\nZilliz\nView guide\nZvec\nView guide\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494433,
    "timestamp": "2026-02-25T08:10:59.847Z",
    "title": "Build a data analysis agent - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/deepagents/data-analysis",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nKey concepts\nSetup\nInstallation\nOptional dependencies\nLangSmith\nSet up the backend\nUpload sample data\nImplement custom tools\nRun the agent\nResults\nNext steps\nTutorials\nDeep Agents\nBuild a data analysis agent\nCopy page\n\nBuild an agent that analyzes data files, generates visualizations, and shares results\n\nï¿½\nOverview\nThis guide demonstrates how to build a data analysis agent using a deep agent. Data analysis tasks typically require planning, code execution, and working with artifacts such as scripts, reports, and plotsâ€”capabilities that deep agents are designed to handle.\nThe agent weâ€™ll build will:\nAccept a CSV file for analysis\nPerform exploratory data analysis and generate visualizations\nShare results to a Slack channel\nThe Slack integration is optional. The agent can be modified to save artifacts locally or share results through other channels.\nï¿½\nKey concepts\nThis tutorial covers:\nBackends for sandboxed code execution\nCustom tools for external integrations\nï¿½\nSetup\nï¿½\nInstallation\nInstall the core dependencies:\npip\nCopy\npip install deepagents\n\nï¿½\nOptional dependencies\nFor this tutorial, weâ€™ll use:\nSlack Python SDK for sharing results (token setup)\nA sandbox environment for code execution. See available providers for setup details\npip\nCopy\npip install slack-sdk\n\nThese services are optional, though a sandboxed environment is highly recommended for any production use. You can alternatively use the local shell backend (with important security considerations) or download artifacts directly from the backend.\nï¿½\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\nOr, set them in Python:\nCopy\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n\nï¿½\nSet up the backend\nDeep agents use backends to execute code in sandboxed environments.\nSee available providers for setup details.\nDaytona\nModal\nRunloop\nLocal shell\npip\nuv\nCopy\npip install langchain-daytona\n\nCopy\nfrom daytona import Daytona\n\nfrom langchain_daytona import DaytonaSandbox\n\nsandbox = Daytona().create()\nbackend = DaytonaSandbox(sandbox=sandbox)\n\nVerify the sandbox is ready:\nCopy\nresult = backend.execute(\"echo ready\")\nprint(result)\n# ExecuteResponse(output='ready', exit_code=0, ...)\n\nï¿½\nUpload sample data\nCreate and upload sample sales data to the backend:\nCopy\nimport csv\nimport io\n\n# Create sample sales data\ndata = [\n    [\"Date\", \"Product\", \"Units Sold\", \"Revenue\"],\n    [\"2025-08-01\", \"Widget A\", 10, 250],\n    [\"2025-08-02\", \"Widget B\", 5, 125],\n    [\"2025-08-03\", \"Widget A\", 7, 175],\n    [\"2025-08-04\", \"Widget C\", 3, 90],\n    [\"2025-08-05\", \"Widget B\", 8, 200],\n]\n\n# Convert to CSV bytes\ntext_buf = io.StringIO()\nwriter = csv.writer(text_buf)\nwriter.writerows(data)\ncsv_bytes = text_buf.getvalue().encode(\"utf-8\")\ntext_buf.close()\n\n# Upload to backend\nbackend.upload_files([(\"/home/daytona/data/sales_data.csv\", csv_bytes)])\n\nï¿½\nImplement custom tools\nData analysis tasks might produce artefacts, like reports or plots. The following simple tool downloads them with backend.download_files and then uploads them using the Slack SDK. We could also ask our agent to list the relevant file paths instead of uploading them, so interested parties can obtain them separately as needed.\nCopy\nfrom langchain.tools import tool\nfrom slack_sdk import WebClient\n\n\nslack_token = os.environ[\"SLACK_USER_TOKEN\"]\nslack_client = WebClient(token=slack_token)\n\n\n@tool(parse_docstring=True)\ndef slack_send_message(text: str, file_path: str | None = None) -> str:\n    \"\"\"Send message, optionally including attachments such as images.\n\n    Args:\n        text: (str) text content of the message\n        file_path: (str) file path of attachment in the filesystem.\n    \"\"\"\n    if not file_path:\n        slack_client.chat_postMessage(channel=channel, text=text)\n    else:\n        fp = backend.download_files([file_path])\n        slack_client.files_upload_v2(\n            channel=\"C0123456ABC\",  # specify your own channel here\n            content=fp[0].content,\n            initial_comment=text,\n        )\n\n    return \"Message sent.\"\n\nIt is generally good practice to avoid adding credentials and other secrets to the sandbox. Here we manage the Slack token outside the sandbox in a tool.\nï¿½\nRun the agent\nLetâ€™s instantiate an agent:\nCopy\nimport uuid\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom deepagents import create_deep_agent\n\n\ncheckpointer = InMemorySaver()\n\nagent = create_deep_agent(\n    model=\"anthropic:claude-sonnet-4-5\",\n    tools=[slack_send_message],\n    backend=backend,\n    checkpointer=checkpointer,\n)\n\nthread_id = str(uuid.uuid4())\nconfig={\"configurable\": {\"thread_id\": thread_id}}\n\nWe include:\nA choice of model\nOur custom tool\nThe backend\nA checkpointer to support multi-turn conversations\nLetâ€™s now invoke our agent.\nCopy\ninput_message = {\n    \"role\": \"user\",\n    \"content\": (\n        \"Analyze ./data/sales_data.csv in the current dir and generate a beautiful plot. \"\n        \"When finished, send your analysis and the plot to Slack using the tool.\"\n    ),\n}\nfor step in agent.stream(\n    {\"messages\": [input_message]},\n    config,\n    stream_mode=\"updates\",\n):\n    for _, update in step.items():\n        if update and (messages := update.get(\"messages\")) and isinstance(messages, list):\n            for message in messages:\n                message.pretty_print()\n\nCopy\n================================== Ai Message ==================================\n\n[{'text': \"I'll help you analyze the sales data and create a beautiful plot, then send the results to Slack. Let me start by exploring the data.\", 'type': 'text'}, {'id': 'toolu_01LRot5h6WkhdpDQ1SG6EQGQ', 'input': {'file_path': './data/sales_data.csv'}, 'name': 'read_file', 'type': 'tool_use'}]\nTool Calls:\n  read_file (toolu_01LRot5h6WkhdpDQ1SG6EQGQ)\n Call ID: toolu_01LRot5h6WkhdpDQ1SG6EQGQ\n  Args:\n    file_path: ./data/sales_data.csv\n================================= Tool Message =================================\nName: read_file\n\nError: File '/data/sales_data.csv' not found\n================================== Ai Message ==================================\n\n[{'text': 'Let me check the current directory structure first:', 'type': 'text'}, {'id': 'toolu_018DSeF8Dx714Dh2M5vJ1JuK', 'input': {'command': 'pwd'}, 'name': 'execute', 'type': 'tool_use'}, {'id': 'toolu_01As782aRsVB3mJpKPKNwg9c', 'input': {'path': '/'}, 'name': 'ls', 'type': 'tool_use'}]\nTool Calls:\n  execute (toolu_018DSeF8Dx714Dh2M5vJ1JuK)\n Call ID: toolu_018DSeF8Dx714Dh2M5vJ1JuK\n  Args:\n    command: pwd\n  ls (toolu_01As782aRsVB3mJpKPKNwg9c)\n Call ID: toolu_01As782aRsVB3mJpKPKNwg9c\n  Args:\n    path: /\n================================= Tool Message =================================\nName: execute\n\n/home/daytona\n[Command succeeded with exit code 0]\n================================= Tool Message =================================\nName: ls\n\n['/bin', '/boot', '/dev', '/etc', '/home', '/lib', '/lib64', '/media', '/mnt', '/opt', '/proc', '/root', '/run', '/sbin', '/srv', '/sys', '/tmp', '/usr', '/var', '/.dockerenv']\n================================== Ai Message ==================================\n\n[{'id': 'toolu_01Sv4ut1P2XFsjrzVmQbJLpr', 'input': {'path': '/home/daytona'}, 'name': 'ls', 'type': 'tool_use'}]\nTool Calls:\n  ls (toolu_01Sv4ut1P2XFsjrzVmQbJLpr)\n Call ID: toolu_01Sv4ut1P2XFsjrzVmQbJLpr\n  Args:\n    path: /home/daytona\n================================= Tool Message =================================\nName: ls\n\n['/home/daytona/.bash_logout', '/home/daytona/.bashrc', '/home/daytona/.face', '/home/daytona/.face.icon', '/home/daytona/.profile', '/home/daytona/.daytona', '/home/daytona/data', '/home/daytona/.zshrc']\n================================== Ai Message ==================================\n\n[{'text': \"Great! I can see there's a data directory. Let me check its contents:\", 'type': 'text'}, {'id': 'toolu_01WN6V9RZjGGoDXzuWdEgB1z', 'input': {'path': '/home/daytona/data'}, 'name': 'ls', 'type': 'tool_use'}]\nTool Calls:\n  ls (toolu_01WN6V9RZjGGoDXzuWdEgB1z)\n Call ID: toolu_01WN6V9RZjGGoDXzuWdEgB1z\n  Args:\n    path: /home/daytona/data\n================================= Tool Message =================================\nName: ls\n\n['/home/daytona/data/sales_data.csv']\n================================== Ai Message ==================================\n\n[{'text': 'Perfect! Now let me read the sales data:', 'type': 'text'}, {'id': 'toolu_01RFmneg7mua1G5csH3rH2Zq', 'input': {'file_path': '/home/daytona/data/sales_data.csv'}, 'name': 'read_file', 'type': 'tool_use'}]\nTool Calls:\n  read_file (toolu_01RFmneg7mua1G5csH3rH2Zq)\n Call ID: toolu_01RFmneg7mua1G5csH3rH2Zq\n  Args:\n    file_path: /home/daytona/data/sales_data.csv\n================================= Tool Message =================================\nName: read_file\n\n     1\tDate,Product,Units Sold,Revenue\n     2\t2025-08-01,Widget A,10,250\n     3\t2025-08-02,Widget B,5,125\n     4\t2025-08-03,Widget A,7,175\n     5\t2025-08-04,Widget C,3,90\n     6\t2025-08-05,Widget B,8,200\n================================== Ai Message ==================================\n\n[{'text': \"Excellent! I can see the sales data. Now I'll create a Python script to analyze the data and generate a beautiful plot. Let me create a comprehensive analysis with visualizations:\", 'type': 'text'}, {'id': 'toolu_01JcBjEXgPpCPuaGxn9ALwMg', 'input': {'file_path': '/home/daytona/analyze_sales.py', 'content': 'import pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom datetime import datetime\\n\\n# Set style for beautiful plots\\nsns.set_style(\"whitegrid\")\\nplt.rcParams[\\'figure.figsize\\'] = (14, 10)\\nplt.rcParams[\\'font.size\\'] = 11\\n\\n# Read the data\\ndf = pd.read_csv(\\'/home/daytona/data/sales_data.csv\\')\\ndf[\\'Date\\'] = pd.to_datetime(df[\\'Date\\'])\\n\\n# Analysis\\nprint(\"=\" * 60)\\nprint(\"SALES DATA ANALYSIS REPORT\")\\nprint(\"=\" * 60)\\nprint(\"\\\\n1. DATA OVERVIEW\")\\nprint(\"-\" * 60)\\nprint(f\"Date Range: {df[\\'Date\\'].min().strftime(\\'%Y-%m-%d\\')} to {df[\\'Date\\'].max().strftime(\\'%Y-%m-%d\\')}\")\\nprint(f\"Total Records: {len(df)}\")\\nprint(f\"Products: {\\', \\'.join(df[\\'Product\\'].unique())}\")\\n\\nprint(\"\\\\n2. SUMMARY STATISTICS\")\\nprint(\"-\" * 60)\\nprint(f\"Total Revenue: ${df[\\'Revenue\\'].sum():,.2f}\")\\nprint(f\"Total Units Sold: {df[\\'Units Sold\\'].sum()}\")\\nprint(f\"Average Daily Revenue: ${df[\\'Revenue\\'].mean():.2f}\")\\nprint(f\"Average Units per Sale: {df[\\'Units Sold\\'].mean():.2f}\")\\n\\nprint(\"\\\\n3. PRODUCT PERFORMANCE\")\\nprint(\"-\" * 60)\\nproduct_stats = df.groupby(\\'Product\\').agg({\\n    \\'Revenue\\': [\\'sum\\', \\'mean\\'],\\n    \\'Units Sold\\': [\\'sum\\', \\'mean\\']\\n}).round(2)\\nprint(product_stats)\\n\\nprint(\"\\\\n4. BEST PERFORMING DAY\")\\nprint(\"-\" * 60)\\nbest_day = df.loc[df[\\'Revenue\\'].idxmax()]\\nprint(f\"Date: {best_day[\\'Date\\'].strftime(\\'%Y-%m-%d\\')}\")\\nprint(f\"Product: {best_day[\\'Product\\']}\")\\nprint(f\"Revenue: ${best_day[\\'Revenue\\']:.2f}\")\\nprint(f\"Units Sold: {best_day[\\'Units Sold\\']}\")\\n\\n# Create visualizations\\nfig = plt.figure(figsize=(16, 12))\\nfig.suptitle(\\'Sales Data Analysis Dashboard\\', fontsize=20, fontweight=\\'bold\\', y=0.995)\\n\\n# 1. Revenue by Date\\nax1 = plt.subplot(2, 3, 1)\\ncolors = sns.color_palette(\"husl\", len(df))\\nbars = ax1.bar(df[\\'Date\\'].dt.strftime(\\'%m-%d\\'), df[\\'Revenue\\'], color=colors, edgecolor=\\'black\\', linewidth=1.5)\\nax1.set_title(\\'Daily Revenue\\', fontsize=14, fontweight=\\'bold\\', pad=10)\\nax1.set_xlabel(\\'Date\\', fontsize=12, fontweight=\\'bold\\')\\nax1.set_ylabel(\\'Revenue ($)\\', fontsize=12, fontweight=\\'bold\\')\\nax1.grid(axis=\\'y\\', alpha=0.3)\\nfor bar in bars:\\n    height = bar.get_height()\\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\\n            f\\'${int(height)}\\',\\n            ha=\\'center\\', va=\\'bottom\\', fontsize=9, fontweight=\\'bold\\')\\nplt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\\n\\n# 2. Units Sold by Date\\nax2 = plt.subplot(2, 3, 2)\\nbars = ax2.bar(df[\\'Date\\'].dt.strftime(\\'%m-%d\\'), df[\\'Units Sold\\'], color=colors, edgecolor=\\'black\\', linewidth=1.5)\\nax2.set_title(\\'Daily Units Sold\\', fontsize=14, fontweight=\\'bold\\', pad=10)\\nax2.set_xlabel(\\'Date\\', fontsize=12, fontweight=\\'bold\\')\\nax2.set_ylabel(\\'Units\\', fontsize=12, fontweight=\\'bold\\')\\nax2.grid(axis=\\'y\\', alpha=0.3)\\nfor bar in bars:\\n    height = bar.get_height()\\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\\n            f\\'{int(height)}\\',\\n            ha=\\'center\\', va=\\'bottom\\', fontsize=9, fontweight=\\'bold\\')\\nplt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\\n\\n# 3. Revenue by Product (Pie Chart)\\nax3 = plt.subplot(2, 3, 3)\\nproduct_revenue = df.groupby(\\'Product\\')[\\'Revenue\\'].sum()\\ncolors_pie = sns.color_palette(\"Set2\", len(product_revenue))\\nwedges, texts, autotexts = ax3.pie(product_revenue, labels=product_revenue.index, autopct=\\'%1.1f%%\\',\\n                                     colors=colors_pie, startangle=90, textprops={\\'fontsize\\': 11, \\'fontweight\\': \\'bold\\'},\\n                                     explode=[0.05] * len(product_revenue))\\nax3.set_title(\\'Revenue Distribution by Product\\', fontsize=14, fontweight=\\'bold\\', pad=10)\\nfor autotext in autotexts:\\n    autotext.set_color(\\'white\\')\\n    autotext.set_fontsize(12)\\n\\n# 4. Total Revenue by Product (Bar Chart)\\nax4 = plt.subplot(2, 3, 4)\\nproduct_revenue_sorted = product_revenue.sort_values(ascending=False)\\ncolors_product = sns.color_palette(\"coolwarm\", len(product_revenue_sorted))\\nbars = ax4.barh(product_revenue_sorted.index, product_revenue_sorted.values, color=colors_product, edgecolor=\\'black\\', linewidth=1.5)\\nax4.set_title(\\'Total Revenue by Product\\', fontsize=14, fontweight=\\'bold\\', pad=10)\\nax4.set_xlabel(\\'Revenue ($)\\', fontsize=12, fontweight=\\'bold\\')\\nax4.set_ylabel(\\'Product\\', fontsize=12, fontweight=\\'bold\\')\\nax4.grid(axis=\\'x\\', alpha=0.3)\\nfor i, (product, revenue) in enumerate(product_revenue_sorted.items()):\\n    ax4.text(revenue, i, f\\' ${int(revenue)}\\', va=\\'center\\', fontsize=10, fontweight=\\'bold\\')\\n\\n# 5. Total Units Sold by Product\\nax5 = plt.subplot(2, 3, 5)\\nproduct_units = df.groupby(\\'Product\\')[\\'Units Sold\\'].sum().sort_values(ascending=False)\\ncolors_units = sns.color_palette(\"viridis\", len(product_units))\\nbars = ax5.barh(product_units.index, product_units.values, color=colors_units, edgecolor=\\'black\\', linewidth=1.5)\\nax5.set_title(\\'Total Units Sold by Product\\', fontsize=14, fontweight=\\'bold\\', pad=10)\\nax5.set_xlabel(\\'Units\\', fontsize=12, fontweight=\\'bold\\')\\nax5.set_ylabel(\\'Product\\', fontsize=12, fontweight=\\'bold\\')\\nax5.grid(axis=\\'x\\', alpha=0.3)\\nfor i, (product, units) in enumerate(product_units.items()):\\n    ax5.text(units, i, f\\' {int(units)}\\', va=\\'center\\', fontsize=10, fontweight=\\'bold\\')\\n\\n# 6. Product Distribution\\nax6 = plt.subplot(2, 3, 6)\\nproduct_counts = df[\\'Product\\'].value_counts()\\ncolors_count = sns.color_palette(\"muted\", len(product_counts))\\nwedges, texts, autotexts = ax6.pie(product_counts, labels=product_counts.index, autopct=\\'%1.1f%%\\',\\n                                     colors=colors_count, startangle=45, textprops={\\'fontsize\\': 11, \\'fontweight\\': \\'bold\\'},\\n                                     explode=[0.05] * len(product_counts))\\nax6.set_title(\\'Sales Transactions by Product\\', fontsize=14, fontweight=\\'bold\\', pad=10)\\nfor autotext in autotexts:\\n    autotext.set_color(\\'white\\')\\n    autotext.set_fontsize(12)\\n\\nplt.tight_layout()\\nplt.savefig(\\'/home/daytona/sales_analysis_plot.png\\', dpi=300, bbox_inches=\\'tight\\', facecolor=\\'white\\')\\nprint(\"\\\\n\" + \"=\" * 60)\\nprint(\"Plot saved as: sales_analysis_plot.png\")\\nprint(\"=\" * 60)\\n'}, 'name': 'write_file', 'type': 'tool_use'}]\nTool Calls:\n  write_file (toolu_01JcBjEXgPpCPuaGxn9ALwMg)\n Call ID: toolu_01JcBjEXgPpCPuaGxn9ALwMg\n  Args:\n    file_path: /home/daytona/analyze_sales.py\n    content: import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Set style for beautiful plots\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (14, 10)\nplt.rcParams['font.size'] = 11\n\n# Read the data\ndf = pd.read_csv('/home/daytona/data/sales_data.csv')\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Analysis\nprint(\"=\" * 60)\nprint(\"SALES DATA ANALYSIS REPORT\")\nprint(\"=\" * 60)\nprint(\"\\n1. DATA OVERVIEW\")\nprint(\"-\" * 60)\nprint(f\"Date Range: {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\")\nprint(f\"Total Records: {len(df)}\")\nprint(f\"Products: {', '.join(df['Product'].unique())}\")\n\nprint(\"\\n2. SUMMARY STATISTICS\")\nprint(\"-\" * 60)\nprint(f\"Total Revenue: ${df['Revenue'].sum():,.2f}\")\nprint(f\"Total Units Sold: {df['Units Sold'].sum()}\")\nprint(f\"Average Daily Revenue: ${df['Revenue'].mean():.2f}\")\nprint(f\"Average Units per Sale: {df['Units Sold'].mean():.2f}\")\n\nprint(\"\\n3. PRODUCT PERFORMANCE\")\nprint(\"-\" * 60)\nproduct_stats = df.groupby('Product').agg({\n    'Revenue': ['sum', 'mean'],\n    'Units Sold': ['sum', 'mean']\n}).round(2)\nprint(product_stats)\n\nprint(\"\\n4. BEST PERFORMING DAY\")\nprint(\"-\" * 60)\nbest_day = df.loc[df['Revenue'].idxmax()]\nprint(f\"Date: {best_day['Date'].strftime('%Y-%m-%d')}\")\nprint(f\"Product: {best_day['Product']}\")\nprint(f\"Revenue: ${best_day['Revenue']:.2f}\")\nprint(f\"Units Sold: {best_day['Units Sold']}\")\n\n# Create visualizations\nfig = plt.figure(figsize=(16, 12))\nfig.suptitle('Sales Data Analysis Dashboard', fontsize=20, fontweight='bold', y=0.995)\n\n# 1. Revenue by Date\nax1 = plt.subplot(2, 3, 1)\ncolors = sns.color_palette(\"husl\", len(df))\nbars = ax1.bar(df['Date'].dt.strftime('%m-%d'), df['Revenue'], color=colors, edgecolor='black', linewidth=1.5)\nax1.set_title('Daily Revenue', fontsize=14, fontweight='bold', pad=10)\nax1.set_xlabel('Date', fontsize=12, fontweight='bold')\nax1.set_ylabel('Revenue ($)', fontsize=12, fontweight='bold')\nax1.grid(axis='y', alpha=0.3)\nfor bar in bars:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n            f'${int(height)}',\n            ha='center', va='bottom', fontsize=9, fontweight='bold')\nplt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n\n# 2. Units Sold by Date\nax2 = plt.subplot(2, 3, 2)\nbars = ax2.bar(df['Date'].dt.strftime('%m-%d'), df['Units Sold'], color=colors, edgecolor='black', linewidth=1.5)\nax2.set_title('Daily Units Sold', fontsize=14, fontweight='bold', pad=10)\nax2.set_xlabel('Date', fontsize=12, fontweight='bold')\nax2.set_ylabel('Units', fontsize=12, fontweight='bold')\nax2.grid(axis='y', alpha=0.3)\nfor bar in bars:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n            f'{int(height)}',\n            ha='center', va='bottom', fontsize=9, fontweight='bold')\nplt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n\n# 3. Revenue by Product (Pie Chart)\nax3 = plt.subplot(2, 3, 3)\nproduct_revenue = df.groupby('Product')['Revenue'].sum()\ncolors_pie = sns.color_palette(\"Set2\", len(product_revenue))\nwedges, texts, autotexts = ax3.pie(product_revenue, labels=product_revenue.index, autopct='%1.1f%%',\n                                     colors=colors_pie, startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'},\n                                     explode=[0.05] * len(product_revenue))\nax3.set_title('Revenue Distribution by Product', fontsize=14, fontweight='bold', pad=10)\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(12)\n\n# 4. Total Revenue by Product (Bar Chart)\nax4 = plt.subplot(2, 3, 4)\nproduct_revenue_sorted = product_revenue.sort_values(ascending=False)\ncolors_product = sns.color_palette(\"coolwarm\", len(product_revenue_sorted))\nbars = ax4.barh(product_revenue_sorted.index, product_revenue_sorted.values, color=colors_product, edgecolor='black', linewidth=1.5)\nax4.set_title('Total Revenue by Product', fontsize=14, fontweight='bold', pad=10)\nax4.set_xlabel('Revenue ($)', fontsize=12, fontweight='bold')\nax4.set_ylabel('Product', fontsize=12, fontweight='bold')\nax4.grid(axis='x', alpha=0.3)\nfor i, (product, revenue) in enumerate(product_revenue_sorted.items()):\n    ax4.text(revenue, i, f' ${int(revenue)}', va='center', fontsize=10, fontweight='bold')\n\n# 5. Total Units Sold by Product\nax5 = plt.subplot(2, 3, 5)\nproduct_units = df.groupby('Product')['Units Sold'].sum().sort_values(ascending=False)\ncolors_units = sns.color_palette(\"viridis\", len(product_units))\nbars = ax5.barh(product_units.index, product_units.values, color=colors_units, edgecolor='black', linewidth=1.5)\nax5.set_title('Total Units Sold by Product', fontsize=14, fontweight='bold', pad=10)\nax5.set_xlabel('Units', fontsize=12, fontweight='bold')\nax5.set_ylabel('Product', fontsize=12, fontweight='bold')\nax5.grid(axis='x', alpha=0.3)\nfor i, (product, units) in enumerate(product_units.items()):\n    ax5.text(units, i, f' {int(units)}', va='center', fontsize=10, fontweight='bold')\n\n# 6. Product Distribution\nax6 = plt.subplot(2, 3, 6)\nproduct_counts = df['Product'].value_counts()\ncolors_count = sns.color_palette(\"muted\", len(product_counts))\nwedges, texts, autotexts = ax6.pie(product_counts, labels=product_counts.index, autopct='%1.1f%%',\n                                     colors=colors_count, startangle=45, textprops={'fontsize': 11, 'fontweight': 'bold'},\n                                     explode=[0.05] * len(product_counts))\nax6.set_title('Sales Transactions by Product', fontsize=14, fontweight='bold', pad=10)\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontsize(12)\n\nplt.tight_layout()\nplt.savefig('/home/daytona/sales_analysis_plot.png', dpi=300, bbox_inches='tight', facecolor='white')\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Plot saved as: sales_analysis_plot.png\")\nprint(\"=\" * 60)\n================================= Tool Message =================================\nName: write_file\n\nUpdated file /home/daytona/analyze_sales.py\n================================== Ai Message ==================================\n\n[{'text': 'Now let me run the analysis script:', 'type': 'text'}, {'id': 'toolu_015xkZr21TuzCTEKGzSqgnbp', 'input': {'command': 'cd /home/daytona && python analyze_sales.py'}, 'name': 'execute', 'type': 'tool_use'}]\nTool Calls:\n  execute (toolu_015xkZr21TuzCTEKGzSqgnbp)\n Call ID: toolu_015xkZr21TuzCTEKGzSqgnbp\n  Args:\n    command: cd /home/daytona && python analyze_sales.py\n================================= Tool Message =================================\nName: execute\n\n============================================================\nSALES DATA ANALYSIS REPORT\n============================================================\n\n1. DATA OVERVIEW\n------------------------------------------------------------\nDate Range: 2025-08-01 to 2025-08-05\nTotal Records: 5\nProducts: Widget A, Widget B, Widget C\n\n2. SUMMARY STATISTICS\n------------------------------------------------------------\nTotal Revenue: $840.00\nTotal Units Sold: 33\nAverage Daily Revenue: $168.00\nAverage Units per Sale: 6.60\n\n3. PRODUCT PERFORMANCE\n------------------------------------------------------------\n         Revenue        Units Sold\n             sum   mean        sum mean\nProduct\nWidget A     425  212.5         17  8.5\nWidget B     325  162.5         13  6.5\nWidget C      90   90.0          3  3.0\n\n4. BEST PERFORMING DAY\n------------------------------------------------------------\nDate: 2025-08-01\nProduct: Widget A\nRevenue: $250.00\nUnits Sold: 10\n\n============================================================\nPlot saved as: sales_analysis_plot.png\n============================================================\n[Command succeeded with exit code 0]\n================================== Ai Message ==================================\n\n[{'text': 'Perfect! The analysis is complete and the plot has been generated. Now let me send both the analysis and the plot to Slack:', 'type': 'text'}, {'id': 'toolu_01Qust5a8DECKPZuU9pBBvkz', 'input': {'text': 'ðŸ“Š **SALES DATA ANALYSIS REPORT** ðŸ“Š\\n\\n**1. DATA OVERVIEW**\\nâ€¢ Date Range: August 1-5, 2025 (5 days)\\nâ€¢ Products Analyzed: Widget A, Widget B, Widget C\\n\\n**2. KEY METRICS**\\nðŸ’° Total Revenue: $840.00\\nðŸ“¦ Total Units Sold: 33 units\\nðŸ“ˆ Average Daily Revenue: $168.00\\nðŸ“Š Average Units per Sale: 6.60\\n\\n**3. PRODUCT PERFORMANCE**\\nðŸ† Widget A (Top Performer)\\n   - Total Revenue: $425 (50.6% of total)\\n   - Total Units Sold: 17 (avg 8.5 per transaction)\\n\\nðŸ¥ˆ Widget B\\n   - Total Revenue: $325 (38.7% of total)\\n   - Total Units Sold: 13 (avg 6.5 per transaction)\\n\\nðŸ¥‰ Widget C\\n   - Total Revenue: $90 (10.7% of total)\\n   - Total Units Sold: 3 (avg 3.0 per transaction)\\n\\n**4. BEST PERFORMING DAY**\\nðŸŒŸ August 1, 2025\\n   - Product: Widget A\\n   - Revenue: $250.00\\n   - Units Sold: 10\\n\\n**KEY INSIGHTS:**\\nâœ… Widget A is the clear leader, generating over 50% of total revenue\\nâœ… Widget B shows strong performance with consistent sales\\nâš ï¸ Widget C has potential for improvement - lowest sales and revenue\\nðŸ“Š Daily revenue ranges from $90 to $250\\n\\nSee attached visualization dashboard for detailed charts! ðŸ“ˆ', 'file_path': '/home/daytona/sales_analysis_plot.png'}, 'name': 'slack_send_message', 'type': 'tool_use'}]\nTool Calls:\n  slack_send_message (toolu_01Qust5a8DECKPZuU9pBBvkz)\n Call ID: toolu_01Qust5a8DECKPZuU9pBBvkz\n  Args:\n    text: ðŸ“Š **SALES DATA ANALYSIS REPORT** ðŸ“Š\n\n**1. DATA OVERVIEW**\nâ€¢ Date Range: August 1-5, 2025 (5 days)\nâ€¢ Products Analyzed: Widget A, Widget B, Widget C\n\n**2. KEY METRICS**\nðŸ’° Total Revenue: $840.00\nðŸ“¦ Total Units Sold: 33 units\nðŸ“ˆ Average Daily Revenue: $168.00\nðŸ“Š Average Units per Sale: 6.60\n\n**3. PRODUCT PERFORMANCE**\nðŸ† Widget A (Top Performer)\n   - Total Revenue: $425 (50.6% of total)\n   - Total Units Sold: 17 (avg 8.5 per transaction)\n\nðŸ¥ˆ Widget B\n   - Total Revenue: $325 (38.7% of total)\n   - Total Units Sold: 13 (avg 6.5 per transaction)\n\nðŸ¥‰ Widget C\n   - Total Revenue: $90 (10.7% of total)\n   - Total Units Sold: 3 (avg 3.0 per transaction)\n\n**4. BEST PERFORMING DAY**\nðŸŒŸ August 1, 2025\n   - Product: Widget A\n   - Revenue: $250.00\n   - Units Sold: 10\n\n**KEY INSIGHTS:**\nâœ… Widget A is the clear leader, generating over 50% of total revenue\nâœ… Widget B shows strong performance with consistent sales\nâš ï¸ Widget C has potential for improvement - lowest sales and revenue\nðŸ“Š Daily revenue ranges from $90 to $250\n\nSee attached visualization dashboard for detailed charts! ðŸ“ˆ\n    file_path: /home/daytona/sales_analysis_plot.png\n================================= Tool Message =================================\nName: slack_send_message\n\nMessage sent.\n================================== Ai Message ==================================\n\nExcellent! I've successfully completed the analysis and sent everything to Slack. Here's what I did:\n\n## Summary\n\n**Analysis Completed:**\n- Analyzed the sales data from August 1-5, 2025\n- Processed data for 3 products (Widget A, B, and C)\n- Generated comprehensive statistics and insights\n\n**Key Findings:**\n1. **Total Revenue:** $840 across 5 days\n2. **Best Product:** Widget A ($425 revenue, 50.6% of total)\n3. **Best Day:** August 1st with $250 in revenue\n4. **Total Units Sold:** 33 units\n\n**Visualization Created:**\nA beautiful 6-panel dashboard featuring:\n- Daily revenue trends\n- Daily units sold\n- Revenue distribution by product (pie chart)\n- Total revenue by product (horizontal bar chart)\n- Total units sold by product (horizontal bar chart)\n- Sales transactions distribution (pie chart)\n\n**Delivered to Slack:**\nâœ… Comprehensive analysis report with key metrics and insights\nâœ… High-resolution visualization dashboard (saved as PNG)\n\nThe analysis reveals that Widget A is the top performer, while Widget C shows room for improvement. All results have been sent to your Slack workspace!\n\nSee all 349 lines\nView the full LangSmith trace for this execution.\nï¿½\nResults\nThe agent successfully analyzes the data and shares a comprehensive report with visualizations to Slack:\n\nAgent-generated analysis report and visualization dashboard delivered to Slack\n\nYou can download artifacts directly from the backend without using external tools:\nCopy\nbackend.download_files(list_of_filepaths)\n\nSee provider guides for how to clean up the sandbox once finished.\nï¿½\nNext steps\nNow that youâ€™ve built a data analysis agent, explore these resources to extend its capabilities:\nBackends: Learn about the Deep Agents backend system\nSandboxes: Review backends for sandboxed code execution, including security considerations and advanced configurations\nCustomization: Discover how to customize your agent with different models, tools, prompts, and planning strategies\nCLI: Try the Deep Agents CLI as a terminal coding agent to assist with data analysis and other agentic tasks locally\nSkills: Equip your agent with reusable skills for common workflows\nHuman-in-the-loop: Add interactive approval steps for critical operations in your data analysis workflow\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494434,
    "timestamp": "2026-02-25T08:10:59.853Z",
    "title": "Build a semantic search engine with LangChain - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/knowledge-base",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nSetup\nInstallation\nLangSmith\n1. Documents and document loaders\nLoading documents\nSplitting\n2. Embeddings\n3. Vector stores\n4. Retrievers\nNext steps\nTutorials\nLangChain\nBuild a semantic search engine with LangChain\nCopy page\nï¿½\nOverview\nThis tutorial will familiarize you with LangChainâ€™s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of dataâ€” from (vector) databases and other sources â€” for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG.\nHere we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.\nï¿½\nConcepts\nThis guide focuses on retrieval of text data. We will cover the following concepts:\nDocuments and document loaders;\nText splitters;\nEmbeddings;\nVector stores and retrievers.\nï¿½\nSetup\nï¿½\nInstallation\nThis tutorial requires the langchain-community and pypdf packages:\npip\nconda\nuv\nCopy\npip install langchain-community pypdf\n\nFor more details, see our Installation guide.\nï¿½\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\nOr, if in a notebook, you can set them with:\nCopy\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n\nï¿½\n1. Documents and document loaders\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\npage_content: a string representing the content;\nmetadata: a dict containing arbitrary metadata;\nid: (optional) a string identifier for the document.\nThe metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\nWe can generate sample documents when desired:\nCopy\nfrom langchain_core.documents import Document\n\ndocuments = [\n    Document(\n        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n    Document(\n        page_content=\"Cats are independent pets that often enjoy their own space.\",\n        metadata={\"source\": \"mammal-pets-doc\"},\n    ),\n]\n\nHowever, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.\nï¿½\nLoading documents\nLetâ€™s load a PDF into a sequence of Document objects. Here is a sample PDF â€” a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders.\nCopy\nfrom langchain_community.document_loaders import PyPDFLoader\n\nfile_path = \"../example_data/nke-10k-2023.pdf\"\nloader = PyPDFLoader(file_path)\n\ndocs = loader.load()\n\nprint(len(docs))\n\nCopy\n107\n\nPyPDFLoader loads one Document object per PDF page. For each, we can easily access:\nThe string content of the page;\nMetadata containing the file name and page number.\nCopy\nprint(f\"{docs[0].page_content[:200]}\\n\")\nprint(docs[0].metadata)\n\nCopy\nTable of Contents\nUNITED STATES\nSECURITIES AND EXCHANGE COMMISSION\nWashington, D.C. 20549\nFORM 10-K\n(Mark One)\nâ˜‘ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\nFO\n\n{'source': '../example_data/nke-10k-2023.pdf', 'page': 0}\n\nï¿½\nSplitting\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not â€œwashed outâ€ by surrounding text.\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nWe set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute â€œstart_indexâ€.\nCopy\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=200, add_start_index=True\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(len(all_splits))\n\nCopy\n514\n\nï¿½\n2. Embeddings\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\nLangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Letâ€™s select a model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip install -U \"langchain-openai\"\n\nCopy\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n\nCopy\nvector_1 = embeddings.embed_query(all_splits[0].page_content)\nvector_2 = embeddings.embed_query(all_splits[1].page_content)\n\nassert len(vector_1) == len(vector_2)\nprint(f\"Generated vectors of length {len(vector_1)}\\n\")\nprint(vector_1[:10])\n\nCopy\nGenerated vectors of length 1536\n\n[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]\n\nArmed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\nï¿½\n3. Vector stores\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\nLangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Letâ€™s select a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip install -U \"langchain-core\"\n\nCopy\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nvector_store = InMemoryVectorStore(embeddings)\n\nHaving instantiated our vector store, we can now index the documents.\nCopy\nids = vector_store.add_documents(documents=all_splits)\n\nNote that most vector store implementations will allow you to connect to an existing vector storeâ€” e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.\nOnce weâ€™ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\nSynchronously and asynchronously;\nBy string query and by vector;\nWith and without returning similarity scores;\nBy similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\nThe methods will generally include a list of Document objects in their outputs.\nUsage\nEmbeddings typically represent text as a â€œdenseâ€ vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\nReturn documents based on similarity to a string query:\nCopy\nresults = vector_store.similarity_search(\n    \"How many distribution centers does Nike have in the US?\"\n)\n\nprint(results[0])\n\nCopy\npage_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213\nNIKE Brand in-line stores (including employee-only stores) 74\nConverse stores (including factory stores) 82\nTOTAL 369\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}\n\nAsync query:\nCopy\nresults = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n\nprint(results[0])\n\nCopy\npage_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n\nReturn scores:\nCopy\n# Note that providers implement different scores; the score here\n# is a distance metric that varies inversely with similarity.\n\nresults = vector_store.similarity_search_with_score(\"What was Nike's revenue in 2023?\")\ndoc, score = results[0]\nprint(f\"Score: {score}\\n\")\nprint(doc)\n\nCopy\nScore: 0.23699893057346344\n\npage_content='Table of Contents\nFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\nThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\nFISCAL 2023 COMPARED TO FISCAL 2022\nâ€¢NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\nThe increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\n2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\nâ€¢NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\nincrease was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\nequivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n\nReturn documents based on similarity to an embedded query:\nCopy\nembedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")\n\nresults = vector_store.similarity_search_by_vector(embedding)\nprint(results[0])\n\nCopy\npage_content='Table of Contents\nGROSS MARGIN\nFISCAL 2023 COMPARED TO FISCAL 2022\nFor fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to\n43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:\n*Wholesale equivalent\nThe decrease in gross margin for fiscal 2023 was primarily due to:\nâ€¢Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as\nproduct mix;\nâ€¢Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in\nthe prior period resulting from lower available inventory supply;\nâ€¢Unfavorable changes in net foreign currency exchange rates, including hedges; and\nâ€¢Lower off-price margin, on a wholesale equivalent basis.\nThis was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}\n\nLearn more:\nAPI Reference\nIntegration-specific docs\nï¿½\n4. Retrievers\nLangChain VectorStore objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\nWe can create a simple version of this ourselves, without subclassing Retriever. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method:\nCopy\nfrom typing import List\n\nfrom langchain_core.documents import Document\nfrom langchain_core.runnables import chain\n\n\n@chain\ndef retriever(query: str) -> List[Document]:\n    return vector_store.similarity_search(query, k=1)\n\n\nretriever.batch(\n    [\n        \"How many distribution centers does Nike have in the US?\",\n        \"When was Nike incorporated?\",\n    ],\n)\n\nCopy\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\n\nVectorstores implement an as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:\nCopy\nretriever = vector_store.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 1},\n)\n\nretriever.batch(\n    [\n        \"How many distribution centers does Nike have in the US?\",\n        \"When was Nike incorporated?\",\n    ],\n)\n\nCopy\n[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')],\n [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]]\n\nVectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\". We can use the latter to threshold documents output by the retriever by similarity score.\nRetrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial.\nï¿½\nNext steps\nYouâ€™ve now seen how to build a semantic search engine over a PDF document.\nFor more on document loaders:\nOverview\nAvailable integrations\nFor more on embeddings:\nOverview\nAvailable integrations\nFor more on vector stores:\nOverview\nAvailable integrations\nFor more on RAG, see:\nBuild a Retrieval Augmented Generation (RAG) App\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494436,
    "timestamp": "2026-02-25T08:10:59.866Z",
    "title": "Build a SQL agent - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/sql-agent",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nSetup\nInstallation\nLangSmith\n1. Select an LLM\n2. Configure the database\n3. Add tools for database interactions\n4. Use create_agent\n5. Run the agent\n(Optional) Use Studio\n6. Implement human-in-the-loop review\nNext steps\nTutorials\nLangChain\nBuild a SQL agent\nCopy page\nï¿½\nOverview\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain agents.\nAt a high level, the agent will:\n1\n\nFetch the available tables and schemas from the database\n\n2\n\nDecide which tables are relevant to the question\n\n3\n\nFetch the schemas for the relevant tables\n\n4\n\nGenerate a query based on the question and information from the schemas\n\n5\n\nDouble-check the query for common mistakes using an LLM\n\n6\n\nExecute the query and return the results\n\n7\n\nCorrect mistakes surfaced by the database engine until the query is successful\n\n8\n\nFormulate a response based on the results\n\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agentâ€™s needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\nï¿½\nConcepts\nWe will cover the following concepts:\nTools for reading from SQL databases\nLangChain agents\nHuman-in-the-loop processes\nï¿½\nSetup\nï¿½\nInstallation\npip\nCopy\npip install langchain  langgraph  langchain-community\n\nï¿½\nLangSmith\nSet up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:\nCopy\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\nï¿½\n1. Select an LLM\nSelect a model that supports tool-calling:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\nðŸ‘‰ Read the OpenAI chat model integration docs\nCopy\npip install -U \"langchain[openai]\"\n\ninit_chat_model\nModel Class\nCopy\nimport os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-4.1\")\n\nThe output shown in the examples below used OpenAI.\nï¿½\n2. Configure the database\nYou will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the chinook database, which is a sample database that represents a digital media store.\nFor convenience, we have hosted the database (Chinook.db) on a public GCS bucket.\nCopy\nimport requests, pathlib\n\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\nlocal_path = pathlib.Path(\"Chinook.db\")\n\nif local_path.exists():\n    print(f\"{local_path} already exists, skipping download.\")\nelse:\n    response = requests.get(url)\n    if response.status_code == 200:\n        local_path.write_bytes(response.content)\n        print(f\"File downloaded and saved as {local_path}\")\n    else:\n        print(f\"Failed to download the file. Status code: {response.status_code}\")\n\nWe will use a handy SQL database wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\nCopy\nfrom langchain_community.utilities import SQLDatabase\n\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\nprint(f\"Dialect: {db.dialect}\")\nprint(f\"Available tables: {db.get_usable_table_names()}\")\nprint(f'Sample output: {db.run(\"SELECT * FROM Artist LIMIT 5;\")}')\n\nCopy\nDialect: sqlite\nAvailable tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\nSample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\n\nï¿½\n3. Add tools for database interactions\nUse the SQLDatabase wrapper available in the langchain_community package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\nCopy\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\n\ntoolkit = SQLDatabaseToolkit(db=db, llm=model)\n\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(f\"{tool.name}: {tool.description}\\n\")\n\nCopy\nsql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\n\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\n\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\n\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n\nï¿½\n4. Use create_agent\nUse create_agent to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\nInitialize the agent with a descriptive system prompt to customize its behavior:\nCopy\nsystem_prompt = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\n\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\n\nThen you should query the schema of the most relevant tables.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n\nNow, create an agent with the model, tools, and prompt:\nCopy\nfrom langchain.agents import create_agent\n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n)\n\nï¿½\n5. Run the agent\nRun the agent on a sample query and observe its behavior:\nCopy\nquestion = \"Which genre on average has the longest tracks?\"\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n\nCopy\n================================ Human Message =================================\n\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_list_tables (call_BQsWg8P65apHc8BTJ1NPDvnM)\n Call ID: call_BQsWg8P65apHc8BTJ1NPDvnM\n  Args:\n================================= Tool Message =================================\nName: sql_db_list_tables\n\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_schema (call_i89tjKECFSeERbuACYm4w0cU)\n Call ID: call_i89tjKECFSeERbuACYm4w0cU\n  Args:\n    table_names: Track, Genre\n================================= Tool Message =================================\nName: sql_db_schema\n\n\nCREATE TABLE \"Genre\" (\n\t\"GenreId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(120),\n\tPRIMARY KEY (\"GenreId\")\n)\n\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\n\n\nCREATE TABLE \"Track\" (\n\t\"TrackId\" INTEGER NOT NULL,\n\t\"Name\" NVARCHAR(200) NOT NULL,\n\t\"AlbumId\" INTEGER,\n\t\"MediaTypeId\" INTEGER NOT NULL,\n\t\"GenreId\" INTEGER,\n\t\"Composer\" NVARCHAR(220),\n\t\"Milliseconds\" INTEGER NOT NULL,\n\t\"Bytes\" INTEGER,\n\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\n\tPRIMARY KEY (\"TrackId\"),\n\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\n\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\n\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tU. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\t230619\t3990994\t0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query_checker (call_G64yYm6R6UauiVPCXJZMA49b)\n Call ID: call_G64yYm6R6UauiVPCXJZMA49b\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query_checker\n\nSELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_AnO3SrhD0ODJBxh6dHMwvHwZ)\n Call ID: call_AnO3SrhD0ODJBxh6dHMwvHwZ\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nOn average, the genre with the longest tracks is \"Sci Fi & Fantasy\" with an average track length of approximately 2,911,783 milliseconds. This is followed by \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\"\n\nThe agent correctly wrote a query, checked the query, and ran it to inform its final response.\nYou can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the LangSmith trace.\nï¿½\n(Optional) Use Studio\nStudio provides a â€œclient sideâ€ loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like â€œTell me the scheme of the databaseâ€ or â€œShow me the invoices for the 5 top customersâ€. You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.\n\nRun your agent in Studio\n\nï¿½\n6. Implement human-in-the-loop review\nIt can be prudent to check the agentâ€™s SQL queries before they are executed for any unintended actions or inefficiencies.\nLangChain agents feature support for built-in human-in-the-loop middleware to add oversight to agent tool calls. Letâ€™s configure the agent to pause for human review on calling the sql_db_query tool:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware \nfrom langgraph.checkpoint.memory import InMemorySaver \n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=system_prompt,\n    middleware=[ \n        HumanInTheLoopMiddleware( \n            interrupt_on={\"sql_db_query\": True}, \n            description_prefix=\"Tool execution pending approval\", \n        ), \n    ], \n    checkpointer=InMemorySaver(), \n)\n\nWeâ€™ve added a checkpointer to our agent to allow execution to be paused and resumed. See the human-in-the-loop guide for detalis on this as well as available middleware configurations.\nOn running the agent, it will now pause for review before executing the sql_db_query tool:\nCopy\nquestion = \"Which genre on average has the longest tracks?\"\nconfig = {\"configurable\": {\"thread_id\": \"1\"}} \n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    config, \n    stream_mode=\"values\",\n):\n    if \"__interrupt__\" in step: \n        print(\"INTERRUPTED:\") \n        interrupt = step[\"__interrupt__\"][0] \n        for request in interrupt.value[\"action_requests\"]: \n            print(request[\"description\"]) \n    elif \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    else:\n        pass\n\nCopy\n...\n\nINTERRUPTED:\nTool execution pending approval\n\nTool: sql_db_query\nArgs: {'query': 'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;'}\n\nWe can resume execution, in this case accepting the query, using Command:\nCopy\nfrom langgraph.types import Command \n\nfor step in agent.stream(\n    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), \n    config,\n    stream_mode=\"values\",\n):\n    if \"messages\" in step:\n        step[\"messages\"][-1].pretty_print()\n    elif \"__interrupt__\" in step:\n        print(\"INTERRUPTED:\")\n        interrupt = step[\"__interrupt__\"][0]\n        for request in interrupt.value[\"action_requests\"]:\n            print(request[\"description\"])\n    else:\n        pass\n\nCopy\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)\n Call ID: call_7oz86Epg7lYRqi9rQHbZPS1U\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nThe genre with the longest average track length is \"Sci Fi & Fantasy\" with an average duration of about 2,911,783 milliseconds, followed by \"Science Fiction\" and \"Drama.\"\n\nRefer to the human-in-the-loop guide for details.\nï¿½\nNext steps\nFor deeper customization, check out this tutorial for implementing a SQL agent directly using LangGraph primitives.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494432,
    "timestamp": "2026-02-25T08:10:59.872Z",
    "title": "Learn - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/learn",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nUse cases\nDeep Agents\nLangChain\nLangGraph\nMulti-agent\nConceptual overviews\nAdditional resources\nLearn\nCopy page\n\nTutorials, conceptual guides, and resources to help you get started.\n\nIn the Learn section of the documentation, youâ€™ll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph.\nï¿½\nUse cases\nBelow are tutorials for common use cases, organized by framework.\nï¿½\nDeep Agents\nDeep agents include built-in functionality for managing context, a virtual filesystem, and other common agent requirements.\nData analysis\nBuild a data analysis agent that sends reports to Slack.\nï¿½\nLangChain\nLangChain agent implementations make it easy to get started for simple use cases.\nSemantic Search\nBuild a semantic search engine over a PDF with LangChain components.\nRAG Agent\nCreate a Retrieval Augmented Generation (RAG) agent.\nSQL Agent\nBuild a SQL agent to interact with databases with human-in-the-loop review.\nVoice Agent\nBuild an agent you can speak and listen to.\nï¿½\nLangGraph\nLangChainâ€™s agent implementations use LangGraph primitives. If deeper customization is required, agents can be implemented directly in LangGraph.\nCustom RAG Agent\nBuild a RAG agent using LangGraph primitives for fine-grained control.\nCustom SQL Agent\nImplement a SQL agent directly in LangGraph for maximum flexibility.\nï¿½\nMulti-agent\nThese tutorials demonstrate multi-agent patterns, blending LangChain agents with LangGraph workflows.\nSubagents: Personal assistant\nBuild a personal assistant that delegates to sub-agents.\nHandoffs: Customer support\nBuild a customer support workflow where a single agent transitions between different states.\nRouter: Knowledge base\nBuild a multi-source knowledge base that routes queries to specialized agents.\nSkills: SQL assistant\nBuild an agent that loads specialized skills progressively using on-demand context loading.\nï¿½\nConceptual overviews\nThese guides explain the core concepts and APIs underlying LangChain and LangGraph.\nMemory\nUnderstand persistence of interactions within and across threads.\nContext engineering\nLearn methods for providing AI applications the right information and tools to accomplish a task.\nGraph API\nExplore LangGraphâ€™s declarative graph-building API.\nFunctional API\nBuild agents as a single function.\nï¿½\nAdditional resources\nLangChain Academy\nCourses and exercises to level up your LangChain skills.\nCase Studies\nSee how teams are using LangChain and LangGraph in production.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494437,
    "timestamp": "2026-02-25T08:10:59.881Z",
    "title": "Build a voice agent with LangChain - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/voice-agent",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nWhat are voice agents?\nHow do voice agents work?\n1. STT > Agent > TTS architecture (The â€œSandwichâ€)\n2. Speech-to-Speech architecture (S2S)\nDemo Application overview\nArchitecture\nSetup\n1. Speech-to-text\nKey concepts\nImplementation\n2. LangChain agent\nKey concepts\nImplementation\n3. Text-to-speech\nKey concepts\nImplementation\nLangSmith\nPutting it all together\nTutorials\nLangChain\nBuild a voice agent with LangChain\nCopy page\nï¿½\nOverview\nChat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.\nVoice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.\nï¿½\nWhat are voice agents?\nVoice agents are agents that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.\nTheyâ€™re suited for a variety of use cases, including:\nCustomer support\nPersonal assistants\nHands-free interfaces\nCoaching and training\nï¿½\nHow do voice agents work?\nAt a high level, every voice agent needs to handle three tasks:\nListen - capture audio and transcribe it\nThink - interpret intent, reason, plan\nSpeak - generate audio and stream it back to the user\nThe difference lies in how these steps are sequenced and coupled. In practice, production agents follow one of two main architectures:\nï¿½\n1. STT > Agent > TTS architecture (The â€œSandwichâ€)\nThe Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).\n\nUser Audio\n\nSpeech-to-Text\n\nLangChain Agent\n\nText-to-Speech\n\nAudio Output\n\nPros:\nFull control over each component (swap STT/TTS providers as needed)\nAccess to latest capabilities from modern text-modality models\nTransparent behavior with clear boundaries between components\nCons:\nRequires orchestrating multiple services\nAdditional complexity in managing the pipeline\nConversion from speech to text loses information (e.g., tone, emotion)\nï¿½\n2. Speech-to-Speech architecture (S2S)\nSpeech-to-speech uses a multimodal model that processes audio input and generates audio output natively.\n\nUser Audio\n\nMultimodal Model\n\nAudio Output\n\nPros:\nSimpler architecture with fewer moving parts\nTypically lower latency for simple interactions\nDirect audio processing captures tone and other nuances of speech\nCons:\nLimited model options, greater risk of provider lock-in\nFeatures may lag behind text-modality models\nLess transparency in how audio is processed\nReduced controllability and customization options\nThis guide demonstrates the sandwich architecture to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.\nï¿½\nDemo Application overview\nWeâ€™ll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using AssemblyAI for STT and Cartesia for TTS (although adapters can be built for most providers).\nAn end-to-end reference application is available in the voice-sandwich-demo repository. We will walk through that application here.\nThe demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.\nï¿½\nArchitecture\nThe demo implements a streaming pipeline where each stage processes data asynchronously:\nClient (Browser)\nCaptures microphone audio and encodes it as PCM\nEstablishes WebSocket connection to the backend server\nStreams audio chunks to the server in real-time\nReceives and plays back synthesized speech audio\nServer (Python)\nAccepts WebSocket connections from clients\nOrchestrates the three-step pipeline:\nSpeech-to-text (STT): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events\nAgent: Processes transcripts with LangChain agent, streams response tokens\nText-to-speech (TTS): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks\nReturns synthesized audio to the client for playback\nThe pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.\nï¿½\nSetup\nFor detailed installation instructions and setup, see the repository README.\nï¿½\n1. Speech-to-text\nThe STT stage transforms an incoming audio stream into text transcripts. The implementation uses a producer-consumer pattern to handle audio streaming and transcript reception concurrently.\nï¿½\nKey concepts\nProducer-Consumer Pattern: Audio chunks are sent to the STT service concurrently with receiving transcript events. This allows transcription to begin before all audio has arrived.\nEvent Types:\nstt_chunk: Partial transcripts provided as the STT service processes audio\nstt_output: Final, formatted transcripts that trigger agent processing\nWebSocket Connection: Maintains a persistent connection to AssemblyAIâ€™s real-time STT API, configured for 16kHz PCM audio with automatic turn formatting.\nï¿½\nImplementation\nCopy\nfrom typing import AsyncIterator\nimport asyncio\nfrom assemblyai_stt import AssemblyAISTT\nfrom events import VoiceAgentEvent\n\nasync def stt_stream(\n    audio_stream: AsyncIterator[bytes],\n) -> AsyncIterator[VoiceAgentEvent]:\n    \"\"\"\n    Transform stream: Audio (Bytes) â†’ Voice Events (VoiceAgentEvent)\n\n    Uses a producer-consumer pattern where:\n    - Producer: Reads audio chunks and sends them to AssemblyAI\n    - Consumer: Receives transcription events from AssemblyAI\n    \"\"\"\n    stt = AssemblyAISTT(sample_rate=16000)\n\n    async def send_audio():\n        \"\"\"Background task that pumps audio chunks to AssemblyAI.\"\"\"\n        try:\n            async for audio_chunk in audio_stream:\n                await stt.send_audio(audio_chunk)\n        finally:\n            # Signal completion when audio stream ends\n            await stt.close()\n\n    # Launch audio sending in background\n    send_task = asyncio.create_task(send_audio())\n\n    try:\n        # Receive and yield transcription events as they arrive\n        async for event in stt.receive_events():\n            yield event\n    finally:\n        # Cleanup\n        with contextlib.suppress(asyncio.CancelledError):\n            send_task.cancel()\n            await send_task\n        await stt.close()\n\nThe application implements an AssemblyAI client to manage the WebSocket connection and message parsing. See below for implementations; similar adapters can be constructed for other STT providers.\n\nAssemblyAI Client\n\nï¿½\n2. LangChain agent\nThe agent stage processes text transcripts through a LangChain agent and streams the response tokens. In this case, we stream all text content blocks generated by the agent.\nï¿½\nKey concepts\nStreaming Responses: The agent uses stream_mode=\"messages\" to emit response tokens as theyâ€™re generated, rather than waiting for the complete response. This enables the TTS stage to begin synthesis immediately.\nConversation Memory: A checkpointer maintains conversation state across turns using a unique thread ID. This allows the agent to reference previous exchanges in the conversation.\nï¿½\nImplementation\nCopy\nfrom uuid import uuid4\nfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Define agent tools\ndef add_to_order(item: str, quantity: int) -> str:\n    \"\"\"Add an item to the customer's sandwich order.\"\"\"\n    return f\"Added {quantity} x {item} to the order.\"\n\ndef confirm_order(order_summary: str) -> str:\n    \"\"\"Confirm the final order with the customer.\"\"\"\n    return f\"Order confirmed: {order_summary}. Sending to kitchen.\"\n\n# Create agent with tools and memory\nagent = create_agent(\n    model=\"anthropic:claude-haiku-4-5\",  # Select your model\n    tools=[add_to_order, confirm_order],\n    system_prompt=\"\"\"You are a helpful sandwich shop assistant.\n    Your goal is to take the user's order. Be concise and friendly.\n    Do NOT use emojis, special characters, or markdown.\n    Your responses will be read by a text-to-speech engine.\"\"\",\n    checkpointer=InMemorySaver(),\n)\n\nasync def agent_stream(\n    event_stream: AsyncIterator[VoiceAgentEvent],\n) -> AsyncIterator[VoiceAgentEvent]:\n    \"\"\"\n    Transform stream: Voice Events â†’ Voice Events (with Agent Responses)\n\n    Passes through all upstream events and adds agent_chunk events\n    when processing STT transcripts.\n    \"\"\"\n    # Generate unique thread ID for conversation memory\n    thread_id = str(uuid4())\n\n    async for event in event_stream:\n        # Pass through all upstream events\n        yield event\n\n        # Process final transcripts through the agent\n        if event.type == \"stt_output\":\n            # Stream agent response with conversation context\n            stream = agent.astream(\n                {\"messages\": [HumanMessage(content=event.transcript)]},\n                {\"configurable\": {\"thread_id\": thread_id}},\n                stream_mode=\"messages\",\n            )\n\n            # Yield agent response chunks as they arrive\n            async for message, _ in stream:\n                if message.text:\n                    yield AgentChunkEvent.create(message.text)\n\nï¿½\n3. Text-to-speech\nThe TTS stage synthesizes agent response text into audio and streams it back to the client. Like the STT stage, it uses a producer-consumer pattern to handle concurrent text sending and audio reception.\nï¿½\nKey concepts\nConcurrent Processing: The implementation merges two async streams:\nUpstream processing: Passes through all events and sends agent text chunks to the TTS provider\nAudio reception: Receives synthesized audio chunks from the TTS provider\nStreaming TTS: Some providers (such as Cartesia) begin synthesizing audio as soon as it receives text, enabling audio playback to start before the agent finishes generating its complete response.\nEvent Passthrough: All upstream events flow through unchanged, allowing the client or other observers to track the full pipeline state.\nï¿½\nImplementation\nCopy\nfrom cartesia_tts import CartesiaTTS\nfrom utils import merge_async_iters\n\nasync def tts_stream(\n    event_stream: AsyncIterator[VoiceAgentEvent],\n) -> AsyncIterator[VoiceAgentEvent]:\n    \"\"\"\n    Transform stream: Voice Events â†’ Voice Events (with Audio)\n\n    Merges two concurrent streams:\n    1. process_upstream(): passes through events and sends text to Cartesia\n    2. tts.receive_events(): yields audio chunks from Cartesia\n    \"\"\"\n    tts = CartesiaTTS()\n\n    async def process_upstream() -> AsyncIterator[VoiceAgentEvent]:\n        \"\"\"Process upstream events and send agent text to Cartesia.\"\"\"\n        async for event in event_stream:\n            # Pass through all events\n            yield event\n            # Send agent text to Cartesia for synthesis\n            if event.type == \"agent_chunk\":\n                await tts.send_text(event.text)\n\n    try:\n        # Merge upstream events with TTS audio events\n        # Both streams run concurrently\n        async for event in merge_async_iters(\n            process_upstream(),\n            tts.receive_events()\n        ):\n            yield event\n    finally:\n        await tts.close()\n\nThe application implements an Cartesia client to manage the WebSocket connection and audio streaming. See below for implementations; similar adapters can be constructed for other TTS providers.\n\nCartesia Client\n\nï¿½\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\nOr, set them in Python:\nCopy\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n\nï¿½\nPutting it all together\nThe complete pipeline chains the three stages together:\nCopy\nfrom langchain_core.runnables import RunnableGenerator\n\npipeline = (\n    RunnableGenerator(stt_stream)      # Audio â†’ STT events\n    | RunnableGenerator(agent_stream)  # STT events â†’ Agent events\n    | RunnableGenerator(tts_stream)    # Agent events â†’ TTS audio\n)\n\n# Use in WebSocket endpoint\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n\n    async def websocket_audio_stream():\n        \"\"\"Yield audio bytes from WebSocket.\"\"\"\n        while True:\n            data = await websocket.receive_bytes()\n            yield data\n\n    # Transform audio through pipeline\n    output_stream = pipeline.atransform(websocket_audio_stream())\n\n    # Send TTS audio back to client\n    async for event in output_stream:\n        if event.type == \"tts_chunk\":\n            await websocket.send_bytes(event.audio)\n\nWe use RunnableGenerators to compose each step of the pipeline. This is an abstraction LangChain uses internally to manage streaming across components.\nEach stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.\nFor more on building agents with LangChain, see the Agents guide.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494438,
    "timestamp": "2026-02-25T08:10:59.889Z",
    "title": "Build a personal assistant with subagents - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent/subagents-personal-assistant",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nWhy use a supervisor?\nConcepts\nSetup\nInstallation\nLangSmith\nComponents\n1. Define tools\n2. Create specialized sub-agents\nCreate a calendar agent\nCreate an email agent\n3. Wrap sub-agents as tools\n4. Create the supervisor agent\n5. Use the supervisor\nExample 1: Simple single-domain request\nExample 2: Complex multi-domain request\nComplete working example\nUnderstanding the architecture\n6. Add human-in-the-loop review\n7. Advanced: Control information flow\nPass additional conversational context to sub-agents\nControl what supervisor receives\n8. Key takeaways\nNext steps\nTutorials\nMulti-agent\nBuild a personal assistant with subagents\nCopy page\nï¿½\nOverview\nThe supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.\nIn this tutorial, youâ€™ll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:\nA calendar agent that handles scheduling, availability checking, and event management.\nAn email agent that manages communication, drafts messages, and sends notifications.\nWe will also incorporate human-in-the-loop review to allow users to approve, edit, and reject actions (such as outbound emails) as desired.\nï¿½\nWhy use a supervisor?\nMulti-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).\nï¿½\nConcepts\nWe will cover the following concepts:\nMulti-agent systems\nHuman-in-the-loop review\nï¿½\nSetup\nï¿½\nInstallation\nThis tutorial requires the langchain package:\npip\nconda\nCopy\npip install langchain\n\nFor more details, see our Installation guide.\nï¿½\nLangSmith\nSet up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:\nbash\npython\nCopy\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\nï¿½\nComponents\nWe will need to select a chat model from LangChainâ€™s suite of integrations:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\nðŸ‘‰ Read the OpenAI chat model integration docs\nCopy\npip install -U \"langchain[openai]\"\n\ninit_chat_model\nModel Class\nCopy\nimport os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-4.1\")\n\nï¿½\n1. Define tools\nStart by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, youâ€™ll use stubs to demonstrate the pattern.\nCopy\nfrom langchain.tools import tool\n\n@tool\ndef create_calendar_event(\n    title: str,\n    start_time: str,       # ISO format: \"2024-01-15T14:00:00\"\n    end_time: str,         # ISO format: \"2024-01-15T15:00:00\"\n    attendees: list[str],  # email addresses\n    location: str = \"\"\n) -> str:\n    \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\"\n    # Stub: In practice, this would call Google Calendar API, Outlook API, etc.\n    return f\"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees\"\n\n\n@tool\ndef send_email(\n    to: list[str],  # email addresses\n    subject: str,\n    body: str,\n    cc: list[str] = []\n) -> str:\n    \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\"\n    # Stub: In practice, this would call SendGrid, Gmail API, etc.\n    return f\"Email sent to {', '.join(to)} - Subject: {subject}\"\n\n\n@tool\ndef get_available_time_slots(\n    attendees: list[str],\n    date: str,  # ISO format: \"2024-01-15\"\n    duration_minutes: int\n) -> list[str]:\n    \"\"\"Check calendar availability for given attendees on a specific date.\"\"\"\n    # Stub: In practice, this would query calendar APIs\n    return [\"09:00\", \"14:00\", \"16:00\"]\n\nï¿½\n2. Create specialized sub-agents\nNext, weâ€™ll create specialized sub-agents that handle each domain.\nï¿½\nCreate a calendar agent\nThe calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation.\nCopy\nfrom langchain.agents import create_agent\n\n\nCALENDAR_AGENT_PROMPT = (\n    \"You are a calendar scheduling assistant. \"\n    \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \"\n    \"into proper ISO datetime formats. \"\n    \"Use get_available_time_slots to check availability when needed. \"\n    \"Use create_calendar_event to schedule events. \"\n    \"Always confirm what was scheduled in your final response.\"\n)\n\ncalendar_agent = create_agent(\n    model,\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n)\n\nTest the calendar agent to see how it handles natural language scheduling:\nCopy\nquery = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\"\n\nfor step in calendar_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\nCopy\n================================== Ai Message ==================================\nTool Calls:\n  get_available_time_slots (call_EIeoeIi1hE2VmwZSfHStGmXp)\n Call ID: call_EIeoeIi1hE2VmwZSfHStGmXp\n  Args:\n    attendees: []\n    date: 2024-06-18\n    duration_minutes: 60\n================================= Tool Message =================================\nName: get_available_time_slots\n\n[\"09:00\", \"14:00\", \"16:00\"]\n================================== Ai Message ==================================\nTool Calls:\n  create_calendar_event (call_zgx3iJA66Ut0W8S3NpT93kEB)\n Call ID: call_zgx3iJA66Ut0W8S3NpT93kEB\n  Args:\n    title: Team Meeting\n    start_time: 2024-06-18T14:00:00\n    end_time: 2024-06-18T15:00:00\n    attendees: []\n================================= Tool Message =================================\nName: create_calendar_event\n\nEvent created: Team Meeting from 2024-06-18T14:00:00 to 2024-06-18T15:00:00 with 0 attendees\n================================== Ai Message ==================================\n\nThe team meeting has been scheduled for next Tuesday, June 18th, at 2:00 PM and will last for 1 hour. If you need to add attendees or a location, please let me know!\n\nThe agent parses â€œnext Tuesday at 2pmâ€ into ISO format (â€œ2024-01-16T14:00:00â€), calculates the end time, calls create_calendar_event, and returns a natural language confirmation.\nï¿½\nCreate an email agent\nThe email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.\nCopy\nEMAIL_AGENT_PROMPT = (\n    \"You are an email assistant. \"\n    \"Compose professional emails based on natural language requests. \"\n    \"Extract recipient information and craft appropriate subject lines and body text. \"\n    \"Use send_email to send the message. \"\n    \"Always confirm what was sent in your final response.\"\n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n)\n\nTest the email agent with a natural language request:\nCopy\nquery = \"Send the design team a reminder about reviewing the new mockups\"\n\nfor step in email_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\nCopy\n================================== Ai Message ==================================\nTool Calls:\n  send_email (call_OMl51FziTVY6CRZvzYfjYOZr)\n Call ID: call_OMl51FziTVY6CRZvzYfjYOZr\n  Args:\n    to: ['design-team@example.com']\n    subject: Reminder: Please Review the New Mockups\n    body: Hi Design Team,\n\nThis is a friendly reminder to review the new mockups at your earliest convenience. Your feedback is important to ensure that we stay on track with our project timeline.\n\nPlease let me know if you have any questions or need additional information.\n\nThank you!\n\nBest regards,\n================================= Tool Message =================================\nName: send_email\n\nEmail sent to design-team@example.com - Subject: Reminder: Please Review the New Mockups\n================================== Ai Message ==================================\n\nI've sent a reminder to the design team asking them to review the new mockups. If you need any further communication on this topic, just let me know!\n\nThe agent infers the recipient from the informal request, crafts a professional subject line and body, calls send_email, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task.\nï¿½\n3. Wrap sub-agents as tools\nNow wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like â€œschedule_eventâ€, not low-level tools like â€œcreate_calendar_eventâ€.\nCopy\n@tool\ndef schedule_event(request: str) -> str:\n    \"\"\"Schedule calendar events using natural language.\n\n    Use this when the user wants to create, modify, or check calendar appointments.\n    Handles date/time parsing, availability checking, and event creation.\n\n    Input: Natural language scheduling request (e.g., 'meeting with design team\n    next Tuesday at 2pm')\n    \"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\n\n@tool\ndef manage_email(request: str) -> str:\n    \"\"\"Send emails using natural language.\n\n    Use this when the user wants to send notifications, reminders, or any email\n    communication. Handles recipient extraction, subject generation, and email\n    composition.\n\n    Input: Natural language email request (e.g., 'send them a reminder about\n    the meeting')\n    \"\"\"\n    result = email_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n    return result[\"messages\"][-1].text\n\nThe tool descriptions help the supervisor decide when to use each tool, so make them clear and specific. We return only the sub-agentâ€™s final response, as the supervisor doesnâ€™t need to see intermediate reasoning or tool calls.\nï¿½\n4. Create the supervisor agent\nNow create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.\nCopy\nSUPERVISOR_PROMPT = (\n    \"You are a helpful personal assistant. \"\n    \"You can schedule calendar events and send emails. \"\n    \"Break down user requests into appropriate tool calls and coordinate the results. \"\n    \"When a request involves multiple actions, use multiple tools in sequence.\"\n)\n\nsupervisor_agent = create_agent(\n    model,\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n)\n\nï¿½\n5. Use the supervisor\nNow test your complete system with complex requests that require coordination across multiple domains:\nï¿½\nExample 1: Simple single-domain request\nCopy\nquery = \"Schedule a team standup for tomorrow at 9am\"\n\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\nCopy\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_mXFJJDU8bKZadNUZPaag8Lct)\n Call ID: call_mXFJJDU8bKZadNUZPaag8Lct\n  Args:\n    request: Schedule a team standup for tomorrow at 9am with Alice and Bob.\n================================= Tool Message =================================\nName: schedule_event\n\nThe team standup has been scheduled for tomorrow at 9:00 AM with Alice and Bob. If you need to make any changes or add more details, just let me know!\n================================== Ai Message ==================================\n\nThe team standup with Alice and Bob is scheduled for tomorrow at 9:00 AM. If you need any further arrangements or adjustments, please let me know!\n\nThe supervisor identifies this as a calendar task, calls schedule_event, and the calendar agent handles date parsing and event creation.\nFor full transparency into the information flow, including prompts and responses for each chat model call, check out the LangSmith trace for the above run.\nï¿½\nExample 2: Complex multi-domain request\nCopy\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\n\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n):\n    for update in step.values():\n        for message in update.get(\"messages\", []):\n            message.pretty_print()\n\nCopy\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_YA68mqF0koZItCFPx0kGQfZi)\n Call ID: call_YA68mqF0koZItCFPx0kGQfZi\n  Args:\n    request: meeting with the design team next Tuesday at 2pm for 1 hour\n  manage_email (call_XxqcJBvVIuKuRK794ZIzlLxx)\n Call ID: call_XxqcJBvVIuKuRK794ZIzlLxx\n  Args:\n    request: send the design team an email reminder about reviewing the new mockups\n================================= Tool Message =================================\nName: schedule_event\n\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm. Let me know if you need to add more details or make any changes!\n================================= Tool Message =================================\nName: manage_email\n\nI've sent an email reminder to the design team requesting them to review the new mockups. If you need to include more information or recipients, just let me know!\n================================== Ai Message ==================================\n\nYour meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm.\n\nI've also sent an email reminder to the design team, asking them to review the new mockups.\n\nLet me know if you'd like to add more details to the meeting or include additional information in the email!\n\nThe supervisor recognizes this requires both calendar and email actions, calls schedule_event for the meeting, then calls manage_email for the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\nRefer to the LangSmith trace to see the detailed information flow for the above run, including individual chat model prompts and responses.\nï¿½\nComplete working example\nHereâ€™s everything together in a runnable script:\n\nShow View complete code\n\nï¿½\nUnderstanding the architecture\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.\nThis separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.\nï¿½\n6. Add human-in-the-loop review\nIt can be prudent to incorporate human-in-the-loop review of sensitive actions. LangChain includes built-in middleware to review tool calls, in this case the tools invoked by sub-agents.\nLetâ€™s add human-in-the-loop review to both sub-agents:\nWe configure the create_calendar_event and send_email tools to interrupt, permitting all response types (approve, edit, reject)\nWe add a checkpointer only to the top-level agent. This is required to pause and resume execution.\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware \nfrom langgraph.checkpoint.memory import InMemorySaver \n\n\ncalendar_agent = create_agent(\n    model,\n    tools=[create_calendar_event, get_available_time_slots],\n    system_prompt=CALENDAR_AGENT_PROMPT,\n    middleware=[ \n        HumanInTheLoopMiddleware( \n            interrupt_on={\"create_calendar_event\": True}, \n            description_prefix=\"Calendar event pending approval\", \n        ), \n    ], \n)\n\nemail_agent = create_agent(\n    model,\n    tools=[send_email],\n    system_prompt=EMAIL_AGENT_PROMPT,\n    middleware=[ \n        HumanInTheLoopMiddleware( \n            interrupt_on={\"send_email\": True}, \n            description_prefix=\"Outbound email pending approval\", \n        ), \n    ], \n)\n\nsupervisor_agent = create_agent(\n    model,\n    tools=[schedule_event, manage_email],\n    system_prompt=SUPERVISOR_PROMPT,\n    checkpointer=InMemorySaver(), \n)\n\nLetâ€™s repeat the query. Note that we gather interrupt events into a list to access downstream:\nCopy\nquery = (\n    \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \"\n    \"and send them an email reminder about reviewing the new mockups.\"\n)\n\nconfig = {\"configurable\": {\"thread_id\": \"6\"}}\n\ninterrupts = []\nfor step in supervisor_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    config,\n):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n\nCopy\n================================== Ai Message ==================================\nTool Calls:\n  schedule_event (call_t4Wyn32ohaShpEZKuzZbl83z)\n Call ID: call_t4Wyn32ohaShpEZKuzZbl83z\n  Args:\n    request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.\n  manage_email (call_JWj4vDJ5VMnvkySymhCBm4IR)\n Call ID: call_JWj4vDJ5VMnvkySymhCBm4IR\n  Args:\n    request: Send an email reminder to the design team about reviewing the new mockups before our meeting next Tuesday at 2pm.\n\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\n\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\n\nThis time weâ€™ve interrupted execution. Letâ€™s inspect the interrupt events:\nCopy\nfor interrupt_ in interrupts:\n    for request in interrupt_.value[\"action_requests\"]:\n        print(f\"INTERRUPTED: {interrupt_.id}\")\n        print(f\"{request['description']}\\n\")\n\nCopy\nINTERRUPTED: 4f994c9721682a292af303ec1a46abb7\nCalendar event pending approval\n\nTool: create_calendar_event\nArgs: {'title': 'Meeting with the Design Team', 'start_time': '2024-06-18T14:00:00', 'end_time': '2024-06-18T15:00:00', 'attendees': ['design team']}\n\nINTERRUPTED: 2b56f299be313ad8bc689eff02973f16\nOutbound email pending approval\n\nTool: send_email\nArgs: {'to': ['designteam@example.com'], 'subject': 'Reminder: Review New Mockups Before Meeting Next Tuesday at 2pm', 'body': \"Hello Team,\\n\\nThis is a reminder to review the new mockups ahead of our meeting scheduled for next Tuesday at 2pm. Your feedback and insights will be valuable for our discussion and next steps.\\n\\nPlease ensure you've gone through the designs and are ready to share your thoughts during the meeting.\\n\\nThank you!\\n\\nBest regards,\\n[Your Name]\"}\n\nWe can specify decisions for each interrupt by referring to its ID using a Command. Refer to the human-in-the-loop guide for additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email:\nCopy\nfrom langgraph.types import Command \n\nresume = {}\nfor interrupt_ in interrupts:\n    if interrupt_.id == \"2b56f299be313ad8bc689eff02973f16\":\n        # Edit email\n        edited_action = interrupt_.value[\"action_requests\"][0].copy()\n        edited_action[\"args\"][\"subject\"] = \"Mockups reminder\"\n        resume[interrupt_.id] = {\n            \"decisions\": [{\"type\": \"edit\", \"edited_action\": edited_action}]\n        }\n    else:\n        resume[interrupt_.id] = {\"decisions\": [{\"type\": \"approve\"}]}\n\ninterrupts = []\nfor step in supervisor_agent.stream(\n    Command(resume=resume), \n    config,\n):\n    for update in step.values():\n        if isinstance(update, dict):\n            for message in update.get(\"messages\", []):\n                message.pretty_print()\n        else:\n            interrupt_ = update[0]\n            interrupts.append(interrupt_)\n            print(f\"\\nINTERRUPTED: {interrupt_.id}\")\n\nCopy\n================================= Tool Message =================================\nName: schedule_event\n\nYour meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\n================================= Tool Message =================================\nName: manage_email\n\nYour email reminder to the design team has been sent. Hereâ€™s what was sent:\n\n- Recipient: designteam@example.com\n- Subject: Mockups reminder\n- Body: A reminder to review the new mockups before the meeting next Tuesday at 2pm, with a request for feedback and readiness for discussion.\n\nLet me know if you need any further assistance!\n================================== Ai Message ==================================\n\n- Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.\n- An email reminder has been sent to the design team about reviewing the new mockups before the meeting.\n\nLet me know if you need any further assistance!\n\nThe run proceeds with our input.\nï¿½\n7. Advanced: Control information flow\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.\nï¿½\nPass additional conversational context to sub-agents\nCopy\nfrom langchain.tools import tool, ToolRuntime\n\n@tool\ndef schedule_event(\n    request: str,\n    runtime: ToolRuntime\n) -> str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    # Customize context received by sub-agent\n    original_user_message = next(\n        message for message in runtime.state[\"messages\"]\n        if message.type == \"human\"\n    )\n    prompt = (\n        \"You are assisting with the following user inquiry:\\n\\n\"\n        f\"{original_user_message.text}\\n\\n\"\n        \"You are tasked with the following sub-request:\\n\\n\"\n        f\"{request}\"\n    )\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n    })\n    return result[\"messages\"][-1].text\n\nThis allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like â€œschedule it for the same time tomorrowâ€ (referencing a previous conversation).\nYou can see the full context received by the sub agent in the chat model call of the LangSmith trace.\nï¿½\nControl what supervisor receives\nYou can also customize what information flows back to the supervisor:\nCopy\nimport json\n\n@tool\ndef schedule_event(request: str) -> str:\n    \"\"\"Schedule calendar events using natural language.\"\"\"\n    result = calendar_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": request}]\n    })\n\n    # Option 1: Return just the confirmation message\n    return result[\"messages\"][-1].text\n\n    # Option 2: Return structured data\n    # return json.dumps({\n    #     \"status\": \"success\",\n    #     \"event_id\": \"evt_123\",\n    #     \"summary\": result[\"messages\"][-1].text\n    # })\n\nImportant: Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but donâ€™t include the results in their final response.\nï¿½\n8. Key takeaways\nThe supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.\nWhen to use the supervisor pattern\nUse the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents donâ€™t need to converse directly with users.\nFor simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use handoffs instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.\nï¿½\nNext steps\nLearn about handoffs for agent-to-agent conversations, explore context engineering to fine-tune information flow, read the multi-agent overview to compare different patterns, and use LangSmith to debug and monitor your multi-agent system.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494439,
    "timestamp": "2026-02-25T08:10:59.907Z",
    "title": "Build customer support with handoffs - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent/handoffs-customer-support",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nSetup\nInstallation\nLangSmith\nSelect an LLM\n1. Define custom state\n2. Create tools that manage workflow state\n3. Define step configurations\n4. Create step-based middleware\n5. Create the agent\n6. Test the workflow\n7. Understanding state transitions\nTurn 1: Initial message\nTurn 2: After warranty recorded\nTurn 3: After issue classified\n8. Manage message history\n9. Add flexibility: Go back\nComplete example\nNext steps\nTutorials\nMulti-agent\nBuild customer support with handoffs\nCopy page\nThe state machine pattern describes workflows where an agentâ€™s behavior changes as it moves through different states of a task. This tutorial shows how to implement a state machine by using tool calls to dynamically change a single agentâ€™s configurationâ€”updating its available tools and instructions based on the current state. The state can be determined from multiple sources: the agentâ€™s past actions (tool calls), external state (such as API call results), or even initial user input (for example, by running a classifier to determine user intent).\nIn this tutorial, youâ€™ll build a customer support agent that does the following:\nCollects warranty information before proceeding.\nClassifies issues as hardware or software.\nProvides solutions or escalates to human support.\nMaintains conversation state across multiple turns.\nUnlike the subagents pattern where sub-agents are called as tools, the state machine pattern uses a single agent whose configuration changes based on workflow progress. Each â€œstepâ€ is just a different configuration (system prompt + tools) of the same underlying agent, selected dynamically based on state.\nHereâ€™s the workflow weâ€™ll build:\n\nâœ… Yes\n\nâŒ No\n\nðŸ”© Hardware\n\nðŸ’» Software\n\nðŸ”© Hardware\n\nðŸ’» Software\n\nðŸ’¬ Customer reports\nan issue\n\nIs the device\nunder warranty?\n\nWhat type\nof issue?\n\nWhat type\nof issue?\n\nProvide warranty\nrepair instructions\n\nProvide troubleshooting\nsteps\n\nEscalate to human\nfor paid repair options\n\nâœ… Issue Resolved\n\nï¿½\nSetup\nï¿½\nInstallation\nThis tutorial requires the langchain package:\npip\nuv\nconda\nCopy\npip install langchain\n\nFor more details, see our Installation guide.\nï¿½\nLangSmith\nSet up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:\nbash\npython\nCopy\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\nï¿½\nSelect an LLM\nSelect a chat model from LangChainâ€™s suite of integrations:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\nðŸ‘‰ Read the OpenAI chat model integration docs\nCopy\npip install -U \"langchain[openai]\"\n\ninit_chat_model\nModel Class\nCopy\nimport os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-4.1\")\n\nï¿½\n1. Define custom state\nFirst, define a custom state schema that tracks which step is currently active:\nCopy\nfrom langchain.agents import AgentState\nfrom typing_extensions import NotRequired\nfrom typing import Literal\n\n# Define the possible workflow steps\nSupportStep = Literal[\"warranty_collector\", \"issue_classifier\", \"resolution_specialist\"]  \n\nclass SupportState(AgentState):  \n    \"\"\"State for customer support workflow.\"\"\"\n    current_step: NotRequired[SupportStep]  \n    warranty_status: NotRequired[Literal[\"in_warranty\", \"out_of_warranty\"]]\n    issue_type: NotRequired[Literal[\"hardware\", \"software\"]]\n\nThe current_step field is the core of the state machine pattern - it determines which configuration (prompt + tools) is loaded on each turn.\nï¿½\n2. Create tools that manage workflow state\nCreate tools that update the workflow state. These tools allow the agent to record information and transition to the next step.\nThe key is using Command to update state, including the current_step field:\nCopy\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\n\n@tool\ndef record_warranty_status(\n    status: Literal[\"in_warranty\", \"out_of_warranty\"],\n    runtime: ToolRuntime[None, SupportState],\n) -> Command:  \n    \"\"\"Record the customer's warranty status and transition to issue classification.\"\"\"\n    return Command(  \n        update={  \n            \"messages\": [\n                ToolMessage(\n                    content=f\"Warranty status recorded as: {status}\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"warranty_status\": status,\n            \"current_step\": \"issue_classifier\",  \n        }\n    )\n\n\n@tool\ndef record_issue_type(\n    issue_type: Literal[\"hardware\", \"software\"],\n    runtime: ToolRuntime[None, SupportState],\n) -> Command:  \n    \"\"\"Record the type of issue and transition to resolution specialist.\"\"\"\n    return Command(  \n        update={  \n            \"messages\": [\n                ToolMessage(\n                    content=f\"Issue type recorded as: {issue_type}\",\n                    tool_call_id=runtime.tool_call_id,\n                )\n            ],\n            \"issue_type\": issue_type,\n            \"current_step\": \"resolution_specialist\",  \n        }\n    )\n\n\n@tool\ndef escalate_to_human(reason: str) -> str:\n    \"\"\"Escalate the case to a human support specialist.\"\"\"\n    # In a real system, this would create a ticket, notify staff, etc.\n    return f\"Escalating to human support. Reason: {reason}\"\n\n\n@tool\ndef provide_solution(solution: str) -> str:\n    \"\"\"Provide a solution to the customer's issue.\"\"\"\n    return f\"Solution provided: {solution}\"\n\nNotice how record_warranty_status and record_issue_type return Command objects that update both the data (warranty_status, issue_type) AND the current_step. This is how the state machine works - tools control workflow progression.\nï¿½\n3. Define step configurations\nDefine prompts and tools for each step. First, define the prompts for each step:\n\nView complete prompt definitions\n\nThen map step names to their configurations using a dictionary:\nCopy\n# Step configuration: maps step name to (prompt, tools, required_state)\nSTEP_CONFIG = {\n    \"warranty_collector\": {\n        \"prompt\": WARRANTY_COLLECTOR_PROMPT,\n        \"tools\": [record_warranty_status],\n        \"requires\": [],\n    },\n    \"issue_classifier\": {\n        \"prompt\": ISSUE_CLASSIFIER_PROMPT,\n        \"tools\": [record_issue_type],\n        \"requires\": [\"warranty_status\"],\n    },\n    \"resolution_specialist\": {\n        \"prompt\": RESOLUTION_SPECIALIST_PROMPT,\n        \"tools\": [provide_solution, escalate_to_human],\n        \"requires\": [\"warranty_status\", \"issue_type\"],\n    },\n}\n\nThis dictionary-based configuration makes it easy to:\nSee all steps at a glance\nAdd new steps (just add another entry)\nUnderstand the workflow dependencies (requires field)\nUse prompt templates with state variables (e.g., {warranty_status})\nï¿½\n4. Create step-based middleware\nCreate middleware that reads current_step from state and applies the appropriate configuration. Weâ€™ll use the @wrap_model_call decorator for a clean implementation:\nCopy\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n\n\n@wrap_model_call\ndef apply_step_config(\n    request: ModelRequest,\n    handler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n    \"\"\"Configure agent behavior based on the current step.\"\"\"\n    # Get current step (defaults to warranty_collector for first interaction)\n    current_step = request.state.get(\"current_step\", \"warranty_collector\")  \n\n    # Look up step configuration\n    stage_config = STEP_CONFIG[current_step]  \n\n    # Validate required state exists\n    for key in stage_config[\"requires\"]:\n        if request.state.get(key) is None:\n            raise ValueError(f\"{key} must be set before reaching {current_step}\")\n\n    # Format prompt with state values (supports {warranty_status}, {issue_type}, etc.)\n    system_prompt = stage_config[\"prompt\"].format(**request.state)\n\n    # Inject system prompt and step-specific tools\n    request = request.override(  \n        system_prompt=system_prompt,  \n        tools=stage_config[\"tools\"],  \n    )\n\n    return handler(request)\n\nThis middleware:\nReads current step: Gets current_step from state (defaults to â€œwarranty_collectorâ€).\nLooks up configuration: Finds the matching entry in STEP_CONFIG.\nValidates dependencies: Ensures required state fields exist.\nFormats prompt: Injects state values into the prompt template.\nApplies configuration: Overrides the system prompt and available tools.\nThe request.override() method is key - it allows us to dynamically change the agentâ€™s behavior based on state without creating separate agent instances.\nï¿½\n5. Create the agent\nNow create the agent with the step-based middleware and a checkpointer for state persistence:\nCopy\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Collect all tools from all step configurations\nall_tools = [\n    record_warranty_status,\n    record_issue_type,\n    provide_solution,\n    escalate_to_human,\n]\n\n# Create the agent with step-based configuration\nagent = create_agent(\n    model,\n    tools=all_tools,\n    state_schema=SupportState,  \n    middleware=[apply_step_config],  \n    checkpointer=InMemorySaver(),  \n)\n\nWhy a checkpointer? The checkpointer maintains state across conversation turns. Without it, the current_step state would be lost between user messages, breaking the workflow.\nï¿½\n6. Test the workflow\nTest the complete workflow:\nCopy\nfrom langchain.messages import HumanMessage\nimport uuid\n\n# Configuration for this conversation thread\nthread_id = str(uuid.uuid4())\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\n\n# Turn 1: Initial message - starts with warranty_collector step\nprint(\"=== Turn 1: Warranty Collection ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"Hi, my phone screen is cracked\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\n\n# Turn 2: User responds about warranty\nprint(\"\\n=== Turn 2: Warranty Response ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"Yes, it's still under warranty\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\nprint(f\"Current step: {result.get('current_step')}\")\n\n# Turn 3: User describes the issue\nprint(\"\\n=== Turn 3: Issue Description ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"The screen is physically cracked from dropping it\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\nprint(f\"Current step: {result.get('current_step')}\")\n\n# Turn 4: Resolution\nprint(\"\\n=== Turn 4: Resolution ===\")\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"What should I do?\")]},\n    config\n)\nfor msg in result['messages']:\n    msg.pretty_print()\n\nExpected flow:\nWarranty verification step: Asks about warranty status\nIssue classification step: Asks about the problem, determines itâ€™s hardware\nResolution step: Provides warranty repair instructions\nï¿½\n7. Understanding state transitions\nLetâ€™s trace what happens at each turn:\nï¿½\nTurn 1: Initial message\nCopy\n{\n    \"messages\": [HumanMessage(\"Hi, my phone screen is cracked\")],\n    \"current_step\": \"warranty_collector\"  # Default value\n}\n\nMiddleware applies:\nSystem prompt: WARRANTY_COLLECTOR_PROMPT\nTools: [record_warranty_status]\nï¿½\nTurn 2: After warranty recorded\nTool call: record_warranty_status(\"in_warranty\") returns:\nCopy\nCommand(update={\n    \"warranty_status\": \"in_warranty\",\n    \"current_step\": \"issue_classifier\"  # State transition!\n})\n\nNext turn, middleware applies:\nSystem prompt: ISSUE_CLASSIFIER_PROMPT (formatted with warranty_status=\"in_warranty\")\nTools: [record_issue_type]\nï¿½\nTurn 3: After issue classified\nTool call: record_issue_type(\"hardware\") returns:\nCopy\nCommand(update={\n    \"issue_type\": \"hardware\",\n    \"current_step\": \"resolution_specialist\"  # State transition!\n})\n\nNext turn, middleware applies:\nSystem prompt: RESOLUTION_SPECIALIST_PROMPT (formatted with warranty_status and issue_type)\nTools: [provide_solution, escalate_to_human]\nThe key insight: Tools drive the workflow by updating current_step, and middleware responds by applying the appropriate configuration on the next turn.\nï¿½\n8. Manage message history\nAs the agent progresses through steps, message history grows. Use summarization middleware to compress earlier messages while preserving conversational context:\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware  \nfrom langgraph.checkpoint.memory import InMemorySaver\n\nagent = create_agent(\n    model,\n    tools=all_tools,\n    state_schema=SupportState,\n    middleware=[\n        apply_step_config,\n        SummarizationMiddleware(  \n            model=\"gpt-4.1-mini\",\n            trigger=(\"tokens\", 4000),\n            keep=(\"messages\", 10)\n        )\n    ],\n    checkpointer=InMemorySaver(),\n)\n\nSee the short-term memory guide for other memory management techniques.\nï¿½\n9. Add flexibility: Go back\nSome workflows need to allow users to return to previous steps to correct information (e.g., changing warranty status or issue classification). However, not all transitions make senseâ€”for example, you typically canâ€™t go back once a refund has been processed. For this support workflow, weâ€™ll add tools to return to the warranty verification and issue classification steps.\nIf your workflow requires arbitrary transitions between most steps, consider whether you need a structured workflow at all. This pattern works best when steps follow a clear sequential progression with occasional backwards transitions for corrections.\nAdd â€œgo backâ€ tools to the resolution step:\nCopy\n@tool\ndef go_back_to_warranty() -> Command:  \n    \"\"\"Go back to warranty verification step.\"\"\"\n    return Command(update={\"current_step\": \"warranty_collector\"})  \n\n\n@tool\ndef go_back_to_classification() -> Command:  \n    \"\"\"Go back to issue classification step.\"\"\"\n    return Command(update={\"current_step\": \"issue_classifier\"})  \n\n\n# Update the resolution_specialist configuration to include these tools\nSTEP_CONFIG[\"resolution_specialist\"][\"tools\"].extend([\n    go_back_to_warranty,\n    go_back_to_classification\n])\n\nUpdate the resolution specialistâ€™s prompt to mention these tools:\nCopy\nRESOLUTION_SPECIALIST_PROMPT = \"\"\"You are a customer support agent helping with device issues.\n\nCURRENT STAGE: Resolution\nCUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}\n\nAt this step, you need to:\n1. For SOFTWARE issues: provide troubleshooting steps using provide_solution\n2. For HARDWARE issues:\n   - If IN WARRANTY: explain warranty repair process using provide_solution\n   - If OUT OF WARRANTY: escalate_to_human for paid repair options\n\nIf the customer indicates any information was wrong, use:\n- go_back_to_warranty to correct warranty status\n- go_back_to_classification to correct issue type\n\nBe specific and helpful in your solutions.\"\"\"\n\nNow the agent can handle corrections:\nCopy\nresult = agent.invoke(\n    {\"messages\": [HumanMessage(\"Actually, I made a mistake - my device is out of warranty\")]},\n    config\n)\n# Agent will call go_back_to_warranty and restart the warranty verification step\n\nï¿½\nComplete example\nHereâ€™s everything together in a runnable script:\n\nShow Complete code\n\nï¿½\nNext steps\nLearn about the subagents pattern for centralized orchestration\nExplore middleware for more dynamic behaviors\nRead the multi-agent overview to compare patterns\nUse LangSmith to debug and monitor your multi-agent system\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494440,
    "timestamp": "2026-02-25T08:10:59.915Z",
    "title": "Build a multi-source knowledge base with routing - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent/router-knowledge-base",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nWhy use a router?\nConcepts\nSetup\nInstallation\nLangSmith\nSelect an LLM\n1. Define state\n2. Define tools for each vertical\n3. Create specialized agents\n4. Build the router workflow\n5. Compile the workflow\n6. Use the router\n7. Understanding the architecture\nClassification phase\nParallel execution with send\nResult collection with reducers\nSynthesis phase\n8. Complete working example\n9. Advanced: Stateful routers\nTool wrapper approach\nFull persistence approach\n10. Key takeaways\nNext steps\nTutorials\nMulti-agent\nBuild a multi-source knowledge base with routing\nCopy page\nï¿½\nOverview\nThe router pattern is a multi-agent architecture where a routing step classifies input and directs it to specialized agents, with results synthesized into a combined response. This pattern excels when your organizationâ€™s knowledge lives across distinct verticalsâ€”separate knowledge domains that each require their own agent with specialized tools and prompts.\nIn this tutorial, youâ€™ll build a multi-source knowledge base router that demonstrates these benefits through a realistic enterprise scenario. The system will coordinate three specialists:\nA GitHub agent that searches code, issues, and pull requests.\nA Notion agent that searches internal documentation and wikis.\nA Slack agent that searches relevant threads and discussions.\nWhen a user asks â€œHow do I authenticate API requests?â€, the router decomposes the query into source-specific sub-questions, routes them to the relevant agents in parallel, and synthesizes results into a coherent answer.\n\nQuery\n\nClassify\n\nGitHub agent\n\nNotion agent\n\nSlack agent\n\nSynthesize\n\nCombined answer\n\nï¿½\nWhy use a router?\nThe router pattern provides several advantages:\nParallel execution: Query multiple sources simultaneously, reducing latency compared to sequential approaches.\nSpecialized agents: Each vertical has focused tools and prompts optimized for its domain.\nSelective routing: Not every query needs every sourceâ€”the router intelligently selects relevant verticals.\nTargeted sub-questions: Each agent receives a question tailored to its domain, improving result quality.\nClean synthesis: Results from multiple sources are combined into a single, coherent response.\nï¿½\nConcepts\nWe will cover the following concepts:\nMulti-agent systems\nStateGraph for workflow orchestration\nSend API for parallel execution\nRouter vs. Subagents: The subagents pattern can also route to multiple agents. Use the router pattern when you need specialized preprocessing, custom routing logic, or want explicit control over parallel execution. Use the subagents pattern when you want the LLM to decide which agents to call dynamically.\nï¿½\nSetup\nï¿½\nInstallation\nThis tutorial requires the langchain and langgraph packages:\npip\nuv\nconda\nCopy\npip install langchain langgraph\n\nFor more details, see our Installation guide.\nï¿½\nLangSmith\nSet up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:\nbash\npython\nCopy\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\nï¿½\nSelect an LLM\nSelect a chat model from LangChainâ€™s suite of integrations:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\nðŸ‘‰ Read the OpenAI chat model integration docs\nCopy\npip install -U \"langchain[openai]\"\n\ninit_chat_model\nModel Class\nCopy\nimport os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-4.1\")\n\nï¿½\n1. Define state\nFirst, define the state schemas. We use three types:\nAgentInput: Simple state passed to each subagent (just a query)\nAgentOutput: Result returned by each subagent (source name + result)\nRouterState: Main workflow state tracking the query, classifications, results, and final answer\nCopy\nfrom typing import Annotated, Literal, TypedDict\nimport operator\n\n\nclass AgentInput(TypedDict):\n    \"\"\"Simple input state for each subagent.\"\"\"\n    query: str\n\n\nclass AgentOutput(TypedDict):\n    \"\"\"Output from each subagent.\"\"\"\n    source: str\n    result: str\n\n\nclass Classification(TypedDict):\n    \"\"\"A single routing decision: which agent to call with what query.\"\"\"\n    source: Literal[\"github\", \"notion\", \"slack\"]\n    query: str\n\n\nclass RouterState(TypedDict):\n    query: str\n    classifications: list[Classification]\n    results: Annotated[list[AgentOutput], operator.add]  # Reducer collects parallel results\n    final_answer: str\n\nThe results field uses a reducer (operator.add in Python, a concat function in JS) to collect outputs from parallel agent executions into a single list.\nï¿½\n2. Define tools for each vertical\nCreate tools for each knowledge domain. In a production system, these would call actual APIs. For this tutorial, we use stub implementations that return mock data. We define 7 tools across 3 verticals: GitHub (search code, issues, PRs), Notion (search docs, get page), and Slack (search messages, get thread).\nCopy\nfrom langchain.tools import tool\n\n\n@tool\ndef search_code(query: str, repo: str = \"main\") -> str:\n    \"\"\"Search code in GitHub repositories.\"\"\"\n    return f\"Found code matching '{query}' in {repo}: authentication middleware in src/auth.py\"\n\n\n@tool\ndef search_issues(query: str) -> str:\n    \"\"\"Search GitHub issues and pull requests.\"\"\"\n    return f\"Found 3 issues matching '{query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)\"\n\n\n@tool\ndef search_prs(query: str) -> str:\n    \"\"\"Search pull requests for implementation details.\"\"\"\n    return f\"PR #156 added JWT authentication, PR #178 updated OAuth scopes\"\n\n\n@tool\ndef search_notion(query: str) -> str:\n    \"\"\"Search Notion workspace for documentation.\"\"\"\n    return f\"Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens\"\n\n\n@tool\ndef get_page(page_id: str) -> str:\n    \"\"\"Get a specific Notion page by ID.\"\"\"\n    return f\"Page content: Step-by-step authentication setup instructions\"\n\n\n@tool\ndef search_slack(query: str) -> str:\n    \"\"\"Search Slack messages and threads.\"\"\"\n    return f\"Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'\"\n\n\n@tool\ndef get_thread(thread_id: str) -> str:\n    \"\"\"Get a specific Slack thread.\"\"\"\n    return f\"Thread discusses best practices for API key rotation\"\n\nSee all 43 lines\nï¿½\n3. Create specialized agents\nCreate an agent for each vertical. Each agent has domain-specific tools and a prompt optimized for its knowledge source. All three follow the same patternâ€”only the tools and system prompt differ.\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"openai:gpt-4.1\")\n\ngithub_agent = create_agent(\n    model,\n    tools=[search_code, search_issues, search_prs],\n    system_prompt=(\n        \"You are a GitHub expert. Answer questions about code, \"\n        \"API references, and implementation details by searching \"\n        \"repositories, issues, and pull requests.\"\n    ),\n)\n\nnotion_agent = create_agent(\n    model,\n    tools=[search_notion, get_page],\n    system_prompt=(\n        \"You are a Notion expert. Answer questions about internal \"\n        \"processes, policies, and team documentation by searching \"\n        \"the organization's Notion workspace.\"\n    ),\n)\n\nslack_agent = create_agent(\n    model,\n    tools=[search_slack, get_thread],\n    system_prompt=(\n        \"You are a Slack expert. Answer questions by searching \"\n        \"relevant threads and discussions where team members have \"\n        \"shared knowledge and solutions.\"\n    ),\n)\n\nSee all 34 lines\nï¿½\n4. Build the router workflow\nNow build the router workflow using a StateGraph. The workflow has four main steps:\nClassify: Analyze the query and determine which agents to invoke with what sub-questions\nRoute: Fan out to selected agents in parallel using Send\nQuery agents: Each agent receives a simple AgentInput and returns an AgentOutput\nSynthesize: Combine collected results into a coherent response\nCopy\nfrom pydantic import BaseModel, Field\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Send\n\nrouter_llm = init_chat_model(\"openai:gpt-4.1-mini\")\n\n\n# Define structured output schema for the classifier\nclass ClassificationResult(BaseModel):  \n    \"\"\"Result of classifying a user query into agent-specific sub-questions.\"\"\"\n    classifications: list[Classification] = Field(\n        description=\"List of agents to invoke with their targeted sub-questions\"\n    )\n\n\ndef classify_query(state: RouterState) -> dict:\n    \"\"\"Classify query and determine which agents to invoke.\"\"\"\n    structured_llm = router_llm.with_structured_output(ClassificationResult)  \n\n    result = structured_llm.invoke([\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"Analyze this query and determine which knowledge bases to consult.\nFor each relevant source, generate a targeted sub-question optimized for that source.\n\nAvailable sources:\n- github: Code, API references, implementation details, issues, pull requests\n- notion: Internal documentation, processes, policies, team wikis\n- slack: Team discussions, informal knowledge sharing, recent conversations\n\nReturn ONLY the sources that are relevant to the query. Each source should have\na targeted sub-question optimized for that specific knowledge domain.\n\nExample for \"How do I authenticate API requests?\":\n- github: \"What authentication code exists? Search for auth middleware, JWT handling\"\n- notion: \"What authentication documentation exists? Look for API auth guides\"\n(slack omitted because it's not relevant for this technical question)\"\"\"\n        },\n        {\"role\": \"user\", \"content\": state[\"query\"]}\n    ])\n\n    return {\"classifications\": result.classifications}\n\n\ndef route_to_agents(state: RouterState) -> list[Send]:\n    \"\"\"Fan out to agents based on classifications.\"\"\"\n    return [\n        Send(c[\"source\"], {\"query\": c[\"query\"]})  \n        for c in state[\"classifications\"]\n    ]\n\n\ndef query_github(state: AgentInput) -> dict:\n    \"\"\"Query the GitHub agent.\"\"\"\n    result = github_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]  \n    })\n    return {\"results\": [{\"source\": \"github\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef query_notion(state: AgentInput) -> dict:\n    \"\"\"Query the Notion agent.\"\"\"\n    result = notion_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]  \n    })\n    return {\"results\": [{\"source\": \"notion\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef query_slack(state: AgentInput) -> dict:\n    \"\"\"Query the Slack agent.\"\"\"\n    result = slack_agent.invoke({\n        \"messages\": [{\"role\": \"user\", \"content\": state[\"query\"]}]  \n    })\n    return {\"results\": [{\"source\": \"slack\", \"result\": result[\"messages\"][-1].content}]}\n\n\ndef synthesize_results(state: RouterState) -> dict:\n    \"\"\"Combine results from all agents into a coherent answer.\"\"\"\n    if not state[\"results\"]:\n        return {\"final_answer\": \"No results found from any knowledge source.\"}\n\n    # Format results for synthesis\n    formatted = [\n        f\"**From {r['source'].title()}:**\\n{r['result']}\"\n        for r in state[\"results\"]\n    ]\n\n    synthesis_response = router_llm.invoke([\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"Synthesize these search results to answer the original question: \"{state['query']}\"\n\n- Combine information from multiple sources without redundancy\n- Highlight the most relevant and actionable information\n- Note any discrepancies between sources\n- Keep the response concise and well-organized\"\"\"\n        },\n        {\"role\": \"user\", \"content\": \"\\n\\n\".join(formatted)}\n    ])\n\n    return {\"final_answer\": synthesis_response.content}\n\nï¿½\n5. Compile the workflow\nNow assemble the workflow by connecting nodes with edges. The key is using add_conditional_edges with the routing function to enable parallel execution:\nCopy\nworkflow = (\n    StateGraph(RouterState)\n    .add_node(\"classify\", classify_query)\n    .add_node(\"github\", query_github)\n    .add_node(\"notion\", query_notion)\n    .add_node(\"slack\", query_slack)\n    .add_node(\"synthesize\", synthesize_results)\n    .add_edge(START, \"classify\")\n    .add_conditional_edges(\"classify\", route_to_agents, [\"github\", \"notion\", \"slack\"])\n    .add_edge(\"github\", \"synthesize\")\n    .add_edge(\"notion\", \"synthesize\")\n    .add_edge(\"slack\", \"synthesize\")\n    .add_edge(\"synthesize\", END)\n    .compile()\n)\n\nThe add_conditional_edges call connects the classify node to the agent nodes through the route_to_agents function. When route_to_agents returns multiple Send objects, those nodes execute in parallel.\nï¿½\n6. Use the router\nTest your router with queries that span multiple knowledge domains:\nCopy\nresult = workflow.invoke({\n    \"query\": \"How do I authenticate API requests?\"\n})\n\nprint(\"Original query:\", result[\"query\"])\nprint(\"\\nClassifications:\")\nfor c in result[\"classifications\"]:\n    print(f\"  {c['source']}: {c['query']}\")\nprint(\"\\n\" + \"=\" * 60 + \"\\n\")\nprint(\"Final Answer:\")\nprint(result[\"final_answer\"])\n\nExpected output:\nCopy\nOriginal query: How do I authenticate API requests?\n\nClassifications:\n  github: What authentication code exists? Search for auth middleware, JWT handling\n  notion: What authentication documentation exists? Look for API auth guides\n\n============================================================\n\nFinal Answer:\nTo authenticate API requests, you have several options:\n\n1. **JWT Tokens**: The recommended approach for most use cases.\n   Implementation details are in `src/auth.py` (PR #156).\n\n2. **OAuth2 Flow**: For third-party integrations, follow the OAuth2\n   flow documented in Notion's 'API Authentication Guide'.\n\n3. **API Keys**: For server-to-server communication, use Bearer tokens\n   in the Authorization header.\n\nFor token refresh handling, see issue #203 and PR #178 for the latest\nOAuth scope updates.\n\nThe router analyzed the query, classified it to determine which agents to invoke (GitHub and Notion, but not Slack for this technical question), queried both agents in parallel, and synthesized the results into a coherent answer.\nï¿½\n7. Understanding the architecture\nThe router workflow follows a clear pattern:\nï¿½\nClassification phase\nThe classify_query function uses structured output to analyze the userâ€™s query and determine which agents to invoke. This is where the routing intelligence lives:\nUses a Pydantic model (Python) or Zod schema (JS) to ensure valid output\nReturns a list of Classification objects, each with a source and targeted query\nOnly includes relevant sourcesâ€”irrelevant ones are simply omitted\nThis structured approach is more reliable than free-form JSON parsing and makes the routing logic explicit.\nï¿½\nParallel execution with send\nThe route_to_agents function maps classifications to Send objects. Each Send specifies the target node and the state to pass:\nCopy\n# Classifications: [{\"source\": \"github\", \"query\": \"...\"}, {\"source\": \"notion\", \"query\": \"...\"}]\n# Becomes:\n[Send(\"github\", {\"query\": \"...\"}), Send(\"notion\", {\"query\": \"...\"})]\n# Both agents execute simultaneously, each receiving only the query it needs\n\nEach agent node receives a simple AgentInput with just a query fieldâ€”not the full router state. This keeps the interface clean and explicit.\nï¿½\nResult collection with reducers\nAgent results flow back to the main state via a reducer. Each agent returns:\nCopy\n{\"results\": [{\"source\": \"github\", \"result\": \"...\"}]}\n\nThe reducer (operator.add in Python) concatenates these lists, collecting all parallel results into state[\"results\"].\nï¿½\nSynthesis phase\nAfter all agents complete, the synthesize_results function iterates over the collected results:\nWaits for all parallel branches to complete (LangGraph handles this automatically)\nReferences the original query to ensure the answer addresses what the user asked\nCombines information from all sources without redundancy\nPartial results: In this tutorial, all selected agents must complete before synthesis.\nï¿½\n8. Complete working example\nHereâ€™s everything together in a runnable script:\n\nShow View complete code\n\nï¿½\n9. Advanced: Stateful routers\nThe router weâ€™ve built so far is statelessâ€”each request is handled independently with no memory between calls. For multi-turn conversations, you need a stateful approach.\nï¿½\nTool wrapper approach\nThe simplest way to add conversation memory is to wrap the stateless router as a tool that a conversational agent can call:\nCopy\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\n@tool\ndef search_knowledge_base(query: str) -> str:\n    \"\"\"Search across multiple knowledge sources (GitHub, Notion, Slack).\n\n    Use this to find information about code, documentation, or team discussions.\n    \"\"\"\n    result = workflow.invoke({\"query\": query})\n    return result[\"final_answer\"]\n\n\nconversational_agent = create_agent(\n    model,\n    tools=[search_knowledge_base],\n    system_prompt=(\n        \"You are a helpful assistant that answers questions about our organization. \"\n        \"Use the search_knowledge_base tool to find information across our code, \"\n        \"documentation, and team discussions.\"\n    ),\n    checkpointer=InMemorySaver(),\n)\n\nThis approach keeps the router stateless while the conversational agent handles memory and context. The user can have a multi-turn conversation, and the agent will call the router tool as needed.\nCopy\nconfig = {\"configurable\": {\"thread_id\": \"user-123\"}}\n\nresult = conversational_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"How do I authenticate API requests?\"}]},\n    config\n)\nprint(result[\"messages\"][-1].content)\n\nresult = conversational_agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What about rate limiting for those endpoints?\"}]},\n    config\n)\nprint(result[\"messages\"][-1].content)\n\nThe tool wrapper approach is recommended for most use cases. It provides clean separation: the router handles multi-source querying, while the conversational agent handles context and memory.\nï¿½\nFull persistence approach\nIf you need the router itself to maintain stateâ€”for example, to use previous search results in routing decisionsâ€”use persistence to store message history at the router level.\nStateful routers add complexity. When routing to different agents across turns, conversations may feel inconsistent if agents have different tones or prompts. Consider the handoffs pattern or subagents pattern insteadâ€”both provide clearer semantics for multi-turn conversations with different agents.\nï¿½\n10. Key takeaways\nThe router pattern excels when you have:\nDistinct verticals: Separate knowledge domains that each require specialized tools and prompts\nParallel query needs: Questions that benefit from querying multiple sources simultaneously\nSynthesis requirements: Results from multiple sources need to be combined into a coherent response\nThe pattern has three phases: decompose (analyze the query and generate targeted sub-questions), route (execute queries in parallel), and synthesize (combine results).\nWhen to use the router pattern\nUse the router pattern when you have multiple independent knowledge sources, need low-latency parallel queries, and want explicit control over routing logic.\nFor simpler cases with dynamic tool selection, consider the subagents pattern. For workflows where agents need to converse with users sequentially, consider handoffs.\nï¿½\nNext steps\nLearn about handoffs for agent-to-agent conversations\nExplore the subagents pattern for centralized orchestration\nRead the multi-agent overview to compare different patterns\nUse LangSmith to debug and monitor your router\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494442,
    "timestamp": "2026-02-25T08:10:59.935Z",
    "title": "Frameworks, runtimes, and harnesses - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/concepts/products",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nAgent frameworks (like LangChain)\nWhen to use LangChain\nAgent runtimes (like LangGraph)\nWhen to use LangGraph\nAgent harnesses (like the Deep Agents SDK)\nWhen to use the Deep Agents SDK\nFeature comparison\nLearn more\nConceptual overviews\nFrameworks, runtimes, and harnesses\nCopy page\n\nUnderstand the differences between LangChain, LangGraph, and Deep Agents and when to use each one\n\nLangChain maintains several open source packages to help you build agents. Each serves a different purpose in the agent development stack. Understanding the distinctions between agent frameworks, agent runtimes, and agent harnesses helps you choose the right tool for your needs.\n\tFramework\tRuntime\tHarness\nValue add\t\nAbstractions\nIntegrations\n\t\nDurable execution\nStreaming\nHITL\nPersistence\n\t\nPredefined tools\nPrompts\nSubagents\n\nWhen to use\t\nGetting started quickly\nStandardizing how a team builds\n\t\nLow-level control\nLong running, stateful workflows and agents\n\t\nMore autonomous agents\nAgents faced with complex, non-deterministic tasks\n\nOptions\t\nLangChain\nVercelâ€™s AI SDK\nCrewAI\nOpenAI Agents SDK\nGoogle ADK\nLlamaIndex\n\t\nLangGraph\nTemporal\nInngest\n\t\nDeep Agents SDK\nClaude Agent SDK\nManus\nï¿½\nAgent frameworks (like LangChain)\nAgent frameworks provide abstractions that make it easier to get started when building with LLMs.LangChain is an agent framework that provides abstractions like structured content blocks, the agent loop, and middleware.LangChainâ€™s abstractions are designed to be easy to get started with while still providing the flexibility needed for advanced use cases.While LangChain is built on top of LangGraph, you donâ€™t need to know LangGraph to use LangChain.Other examples of agent frameworks include Vercelâ€™s AI SDK, CrewAI, OpenAI Agents SDK, Google ADK, LlamaIndex, and many more.\nï¿½\nWhen to use LangChain\nUse LangChain when:\nYou want to quickly build agents and autonomous applications.\nYou need standard abstractions for models, tools, and agent loops.\nYou want an easy-to-use framework that still provides flexibility.\nYouâ€™re building straightforward agent applications without complex orchestration needs.\nï¿½\nAgent runtimes (like LangGraph)\nAgent runtimes provide the tooling for running agents in production. Supported tools may include:\nDurable execution: Agents persist through failures and can run for extended periods, resuming from where they left off.\nStreaming: Support for streaming workflows and responses.\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state.\nPersistence: Thread-level and cross-thread persistence for state management.\nLow-level control: Direct control over agent orchestration without high-level abstractions.\nLangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.Agent frameworks are generally higher level and run on agent runtimes. For example, LangChain 1.0 is built on top of LangGraph.Other examples of agent runtimes include Temporal, Inngest, and other durable execution engines.\nï¿½\nWhen to use LangGraph\nUse LangGraph when:\nYou need fine-grained, low-level control over agent orchestration.\nYou need durable execution for long-running, stateful agents.\nYouâ€™re building complex workflows that combine deterministic and agentic steps.\nYou need production-ready infrastructure for agent deployment.\nï¿½\nAgent harnesses (like the Deep Agents SDK)\nAgent harnesses are opinionated agent frameworks with that come batteries included with built-in tools and capabilities that make building sophisticated, long-running agents easier. Supported tools may include:\nPlanning capabilities: Track multiple tasks with a to-do list.\nTask delegation: Delegate work and keep context clean with subagents.\nFile system: Read and write access to files on different pluggable storage backends.\nToken management: Conversation history summarization and large tool result eviction.\nThe Deep Agents SDK builds on top of LangGraph and adds planning capabilities, file systems for context management, the ability to spawn subagents, and more. DeepAgents is designed for complex, multi-step tasks that require planning and decomposition. Example tasks include working with search results, scripts, and other artifacts in state.Other examples of agent harnesses include Claude Agent SDK, Manus, and other coding CLIs.\nï¿½\nWhen to use the Deep Agents SDK\nUse the Deep Agents SDK when:\nYou are building agents that run over long time periods.\nYou are building agents that need to handle complex, multi-step tasks.\nYou want to use predefined tools, such as filesystem operations, bash execution, and automated context engineering.\nYou want to use predefined prompts and subagents.\nï¿½\nFeature comparison\nWhile you can accomplish similar tasks with LangChain, LangGraph, and Deep Agents, the level at which you integrate them differ:\nFeature\tLangChain\tLangGraph\tDeep Agents\nShort-term memory\tShort-term memory\tShort-term memory\tStateBackend\nLong-term memory\tLong-term memory\tLong-term memory\tLong-term memory\nSkills\tMulti-agent skills\t-\tSkills\nSubagents\tMulti-agent subagents\tSubgraphs\tSubagents\nHuman-in-the-loop\tHuman-in-the-loop middleware\tInterrupts\tinterrupt_on parameter\nStreaming\tAgent Streaming\tStreaming\tStreaming\nï¿½\nLearn more\nLangChain overview\nLangGraph overview\nDeep Agents overview\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494435,
    "timestamp": "2026-02-25T08:10:59.949Z",
    "title": "Build a RAG agent with LangChain - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/rag",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nOverview\nConcepts\nPreview\nSetup\nInstallation\nLangSmith\nComponents\n1. Indexing\nLoading documents\nSplitting documents\nStoring documents\n2. Retrieval and generation\nRAG agents\nRAG chains\nNext steps\nTutorials\nLangChain\nBuild a RAG agent with LangChain\nCopy page\nï¿½\nOverview\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\nï¿½\nConcepts\nWe will cover the following concepts:\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\nOnce weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.\nIf your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\nï¿½\nPreview\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\n\nExpand for full code snippet\n\nï¿½\nSetup\nï¿½\nInstallation\nThis tutorial requires these langchain dependencies:\npip\nuv\nCopy\npip install langchain langchain-text-splitters langchain-community bs4\n\nFor more details, see our Installation guide.\nï¿½\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\nCopy\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\nOr, set them in Python:\nCopy\nimport getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n\nï¿½\nComponents\nWe will need to select three components from LangChainâ€™s suite of integrations.\nSelect a chat model:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\nðŸ‘‰ Read the OpenAI chat model integration docs\nCopy\npip install -U \"langchain[openai]\"\n\ninit_chat_model\nModel Class\nCopy\nimport os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-4.1\")\n\nSelect an embeddings model:\nOpenAI\nAzure\nGoogle Gemini\nGoogle Vertex\nAWS\nHuggingFace\nOllama\nCohere\nMistralAI\nNomic\nNVIDIA\nVoyage AI\nIBM watsonx\nFake\nIsaacus\nCopy\npip install -U \"langchain-openai\"\n\nCopy\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n\nSelect a vector store:\nIn-memory\nAmazon OpenSearch\nAstraDB\nChroma\nFAISS\nMilvus\nMongoDB\nPGVector\nPGVectorStore\nPinecone\nQdrant\nCopy\npip install -U \"langchain-core\"\n\nCopy\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nvector_store = InMemoryVectorStore(embeddings)\n\nï¿½\n1. Indexing\nThis section is an abbreviated version of the content in the semantic search tutorial.\nIf your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\nIndexing commonly works as follows:\nLoad: First we need to load our data. This is done with Document Loaders.\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\nï¿½\nLoading documents\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others.\nCopy\nimport bs4\nfrom langchain_community.document_loaders import WebBaseLoader\n\n# Only keep post title, headers, and content from the full HTML.\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs={\"parse_only\": bs4_strainer},\n)\ndocs = loader.load()\n\nassert len(docs) == 1\nprint(f\"Total characters: {len(docs[0].page_content)}\")\n\nCopy\nTotal characters: 43131\n\nCopy\nprint(docs[0].page_content[:500])\n\nCopy\n      LLM Powered Autonomous Agents\n\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview#\nIn\n\nGo deeper\nDocumentLoader: Object that loads data from a source as list of Documents.\nIntegrations: 160+ integrations to choose from.\nBaseLoader: API reference for the base interface.\nï¿½\nSplitting documents\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\nCopy\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # chunk size (characters)\n    chunk_overlap=200,  # chunk overlap (characters)\n    add_start_index=True,  # track index in original document\n)\nall_splits = text_splitter.split_documents(docs)\n\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\n\nCopy\nSplit blog post into 66 sub-documents.\n\nGo deeper\nTextSplitter: Object that splits a list of Document objects into smaller chunks for storage and retrieval.\nIntegrations\nInterface: API reference for the base interface.\nï¿½\nStoring documents\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\nCopy\ndocument_ids = vector_store.add_documents(documents=all_splits)\n\nprint(document_ids[:3])\n\nCopy\n['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\n\nGo deeper\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\nIntegrations: 30+ integrations to choose from.\nInterface: API reference for the base interface.\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\nIntegrations: 40+ integrations to choose from.\nInterface: API reference for the base interface.\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\nï¿½\n2. Retrieval and generation\nRAG applications commonly work as follows:\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\nNow letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\nWe will demonstrate:\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\nï¿½\nRAG agents\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\nCopy\nfrom langchain.tools import tool\n\n@tool(response_format=\"content_and_artifact\")\ndef retrieve_context(query: str):\n    \"\"\"Retrieve information to help answer a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\nRetrieval tools are not limited to a single string query argument, as in the above example. You can force the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:\nCopy\nfrom typing import Literal\n\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\n\nGiven our tool, we can construct the agent:\nCopy\nfrom langchain.agents import create_agent\n\n\ntools = [retrieve_context]\n# If desired, specify custom instructions\nprompt = (\n    \"You have access to a tool that retrieves context from a blog post. \"\n    \"Use the tool to help answer user queries.\"\n)\nagent = create_agent(model, tools, system_prompt=prompt)\n\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\nCopy\nquery = (\n    \"What is the standard method for Task Decomposition?\\n\\n\"\n    \"Once you get the answer, look up common extensions of that method.\"\n)\n\nfor event in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    event[\"messages\"][-1].pretty_print()\n\nCopy\n================================ Human Message =================================\n\nWhat is the standard method for Task Decomposition?\n\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\n  Args:\n    query: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\n  Args:\n    query: common extensions of the standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve_context\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Task decomposition can be done...\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Component One: Planning...\n================================== Ai Message ==================================\n\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\n\nNote that the agent:\nGenerates a query to search for a standard method for task decomposition;\nReceiving the answer, generates a second query to search for common extensions of it;\nHaving received all necessary context, answers the question.\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\nï¿½\nRAG chains\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\nâœ… Benefits\tâš ï¸ Drawbacks\nSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\tTwo inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.\nContextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.\tReduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.\nMultiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\t\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\nIn this approach we no longer call the model in a loop, but instead make a single pass.\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\nCopy\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n@dynamic_prompt\ndef prompt_with_context(request: ModelRequest) -> str:\n    \"\"\"Inject context into state messages.\"\"\"\n    last_query = request.state[\"messages\"][-1].text\n    retrieved_docs = vector_store.similarity_search(last_query)\n\n    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n\n    system_message = (\n        \"You are a helpful assistant. Use the following context in your response:\"\n        f\"\\n\\n{docs_content}\"\n    )\n\n    return system_message\n\n\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\n\nLetâ€™s try this out:\nCopy\nquery = \"What is task decomposition?\"\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n\nCopy\n================================ Human Message =================================\n\nWhat is task decomposition?\n================================== Ai Message ==================================\n\nTask decomposition is...\n\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\n\nReturning source documents\n\nï¿½\nNext steps\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\nStream tokens and other information for responsive user experiences\nAdd conversational memory to support multi-turn interactions\nAdd long-term memory to support memory across conversational threads\nAdd structured responses\nDeploy your application with LangSmith Deployment\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494444,
    "timestamp": "2026-02-25T08:10:59.963Z",
    "title": "Memory overview - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/concepts/memory",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nShort-term memory\nManage short-term memory\nLong-term memory\nSemantic memory\nProfile\nCollection\nEpisodic memory\nProcedural memory\nWriting memories\nIn the hot path\nIn the background\nMemory storage\nLearn more\nConceptual overviews\nMemory overview\nCopy page\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\nThis conceptual guide covers two types of memory, based on their recall scope:\nShort-term memory, or thread-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agentâ€™s state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.\nLong-term memory stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores (reference doc) to let you save and recall long-term memories.\nï¿½\nShort-term memory\nShort-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\nLangGraph manages short-term memory as part of the agentâ€™s state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graphâ€™s state, the bot can access the full context for a given conversation while maintaining separation between different threads.\nï¿½\nManage short-term memory\nConversation history is the most common form of short-term memory, and long conversations pose a challenge to todayâ€™s LLMs. A full history may not fit inside an LLMâ€™s context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get â€œdistractedâ€ by stale or off-topic content, all while suffering from slower response times and higher costs.\nChat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.\nFor more information on common techniques for managing messages, see the Add and manage memory guide.\nï¿½\nLong-term memory\nLong-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is thread-scoped, long-term memory is saved within custom â€œnamespaces.â€\nLong-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:\nWhat is the type of memory? Humans use memories to remember facts (semantic memory), experiences (episodic memory), and rules (procedural memory). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task.\nWhen do you want to update memories? Memory can be updated as part of an agentâ€™s application logic (e.g., â€œon the hot pathâ€). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the section below.\nDifferent applications require various types of memory. Although the analogy isnâ€™t perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper) have even mapped these human memory types to those used in AI agents.\nMemory Type\tWhat is Stored\tHuman Example\tAgent Example\nSemantic\tFacts\tThings I learned in school\tFacts about a user\nEpisodic\tExperiences\tThings I did\tPast agent actions\nProcedural\tInstructions\tInstincts or motor skills\tAgent system prompt\nï¿½\nSemantic memory\nSemantic memory, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.\nSemantic memory is different from â€œsemantic search,â€ which is a technique for finding similar content using â€œmeaningâ€ (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.\nSemantic memories can be managed in different ways:\nï¿½\nProfile\nMemories can be a single, continuously updated â€œprofileâ€ of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs youâ€™ve selected to represent your domain.\nWhen remembering a profile, you will want to make sure that you are updating the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or strict decoding when generating documents to ensure the memory schemas remains valid.\nï¿½\nCollection\nAlternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that youâ€™re less likely to lose information over time. Itâ€™s easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream.\nHowever, this shifts some complexity memory updating. The model must now delete or update existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith) to help you tune the behavior.\nWorking with document collections also shifts complexity to memory search over the list. The Store currently supports both semantic search and filtering by content.\nFinally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.\nRegardless of memory management approach, the central point is that the agent will use the semantic memories to ground its responses, which often leads to more personalized and relevant interactions.\nï¿½\nEpisodic memory\nEpisodic memory, in both humans and AI agents, involves recalling past events or actions. The CoALA paper frames this well: facts can be written to semantic memory, whereas experiences can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task.\nIn practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes itâ€™s easier to â€œshowâ€ than â€œtellâ€ and LLMs learn well from examples. Few-shot learning lets you â€œprogramâ€ your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input.\nNote that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity (using a BM25-like algorithm for keyword based similarity).\nSee this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences.\nï¿½\nProcedural memory\nProcedural memory, in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agentâ€™s prompt that collectively determine the agentâ€™s functionality.\nIn practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts.\nOne effective approach to refining an agentâ€™s instructions is through â€œReflectionâ€ or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions.\nFor example, we built a Tweet generator using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify a priori, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process.\nThe below pseudo-code shows how you might implement this with the LangGraph memory store, using the store to save a prompt, the update_instructions node to get the current prompt (as well as feedback from the conversation with the user captured in state[\"messages\"]), update the prompt, and save the new prompt back to the store. Then, the call_model get the updated prompt from the store and uses it to generate a response.\nCopy\n# Node that *uses* the instructions\ndef call_model(state: State, store: BaseStore):\n    namespace = (\"agent_instructions\", )\n    instructions = store.get(namespace, key=\"agent_a\")[0]\n    # Application logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"])\n    ...\n\n# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nï¿½\nWriting memories\nThere are two primary methods for agents to write memories: â€œin the hot pathâ€ and â€œin the backgroundâ€.\nï¿½\nIn the hot path\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nï¿½\nIn the background\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nï¿½\nMemory storage\nLangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a custom namespace (similar to a folder) and a distinct key (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\nCopy\nfrom langgraph.store.memory import InMemoryStore\n\n\ndef embed(texts: list[str]) -> list[list[float]]:\n    # Replace with an actual embedding function or LangChain embeddings object\n    return [[1.0, 2.0] * len(texts)]\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore = InMemoryStore(index={\"embed\": embed, \"dims\": 2})\nuser_id = \"my-user\"\napplication_context = \"chitchat\"\nnamespace = (user_id, application_context)\nstore.put(\n    namespace,\n    \"a-memory\",\n    {\n        \"rules\": [\n            \"User likes short, direct language\",\n            \"User only speaks English & python\",\n        ],\n        \"my-key\": \"my-value\",\n    },\n)\n# get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\")\n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems = store.search(\n    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n)\n\nFor more information about the memory store, see the Persistence guide.\nï¿½\nLearn more\nContext conceptual overview\nShort-term memory in LangChain\nMemory in LangGraph\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494443,
    "timestamp": "2026-02-25T08:10:59.971Z",
    "title": "Component architecture - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/component-architecture",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nCore component ecosystem\nHow components connect\nComponent categories\nCommon patterns\nRAG (Retrieval-Augmented generation)\nAgent with tools\nMulti-agent system\nLearn more\nConceptual overviews\nComponent architecture\nCopy page\nLangChainâ€™s power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components.\nï¿½\nCore component ecosystem\nThe diagram below shows how LangChainâ€™s major components connect to form complete AI applications:\n\nðŸŽ¯ Orchestration\n\nðŸ¤– Generation\n\nðŸ” Retrieval\n\nðŸ”¢ Embedding & storage\n\nðŸ“¥ Input processing\n\nText input\n\nDocument loaders\n\nText splitters\n\nDocuments\n\nEmbedding models\n\nVectors\n\nVector stores\n\nUser Query\n\nEmbedding models\n\nQuery vector\n\nRetrievers\n\nRelevant context\n\nChat models\n\nTools\n\nTool results\n\nAI response\n\nAgents\n\nMemory\n\nï¿½\nHow components connect\nEach component layer builds on the previous ones:\nInput processing â€“ Transform raw data into structured documents\nEmbedding & storage â€“ Convert text into searchable vector representations\nRetrieval â€“ Find relevant information based on user queries\nGeneration â€“ Use AI models to create responses, optionally with tools\nOrchestration â€“ Coordinate everything through agents and memory systems\nï¿½\nComponent categories\nLangChain organizes components into these main categories:\nCategory\tPurpose\tKey Components\tUse Cases\nModels\tAI reasoning and generation\tChat models, LLMs, Embedding models\tText generation, reasoning, semantic understanding\nTools\tExternal capabilities\tAPIs, databases, etc.\tWeb search, data access, computations\nAgents\tOrchestration and reasoning\tReAct agents, tool calling agents\tNondeterministic workflows, decision making\nMemory\tContext preservation\tMessage history, custom state\tConversations, stateful interactions\nRetrievers\tInformation access\tVector retrievers, web retrievers\tRAG, knowledge base search\nDocument processing\tData ingestion\tLoaders, splitters, transformers\tPDF processing, web scraping\nVector Stores\tSemantic search\tChroma, Pinecone, FAISS\tSimilarity search, embeddings storage\nï¿½\nCommon patterns\nï¿½\nRAG (Retrieval-Augmented generation)\n\nUser question\n\nRetriever\n\nRelevant docs\n\nChat model\n\nInformed response\n\nï¿½\nAgent with tools\n\nYes\n\nNo\n\nUser request\n\nAgent\n\nNeed tool?\n\nCall tool\n\nTool result\n\nFinal answer\n\nï¿½\nMulti-agent system\n\nComplex Task\n\nSupervisor agent\n\nSpecialist agent 1\n\nSpecialist agent 2\n\nResults\n\nCoordinated response\n\nï¿½\nLearn more\nCreating agents\nWorking with tools\nBrowse integrations\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494445,
    "timestamp": "2026-02-25T08:10:59.978Z",
    "title": "Context overview - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/concepts/context",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nStatic runtime context\nDynamic runtime context\nDynamic cross-conversation context\nLearn more\nConceptual overviews\nContext overview\nCopy page\nContext engineering is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:\nBy mutability:\nStatic context: Immutable data that doesnâ€™t change during execution (e.g., user metadata, database connections, tools)\nDynamic context: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\nBy lifetime:\nRuntime context: Data scoped to a single run or invocation\nCross-conversation context: Data that persists across multiple conversations or sessions\nRuntime context refers to local context: data and dependencies your code needs to run. It does not refer to:\nThe LLM context, which is the data passed into the LLMâ€™s prompt.\nThe â€œcontext windowâ€, which is the maximum number of tokens that can be passed to the LLM.\nRuntime context is a form of dependency injection and can be used to optimize the LLM context. It lets to provide dependencies (like database connections, user IDs, or API clients) to your tools and nodes at runtime rather than hardcoding them. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.\nLangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:\nContext type\tDescription\tMutability\tLifetime\tAccess method\nStatic runtime context\tUser metadata, tools, db connections passed at startup\tStatic\tSingle run\tcontext argument to invoke/stream\nDynamic runtime context (state)\tMutable data that evolves during a single run\tDynamic\tSingle run\tLangGraph state object\nDynamic cross-conversation context (store)\tPersistent data shared across conversations\tDynamic\tCross-conversation\tLangGraph store\nï¿½\nStatic runtime context\nStatic runtime context represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the context argument to invoke/stream. This data does not change during execution.\nCopy\n@dataclass\nclass ContextSchema:\n    user_name: str\n\ngraph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]},\n    context={\"user_name\": \"John Smith\"}  \n)\n\nAgent prompt\nWorkflow node\nIn a tool\nCopy\nfrom dataclasses import dataclass\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\n@dataclass\nclass ContextSchema:\n    user_name: str\n\n@dynamic_prompt\ndef personalized_prompt(request: ModelRequest) -> str:  \n    user_name = request.runtime.context.user_name\n    return f\"You are a helpful assistant. Address the user as {user_name}.\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[get_weather],\n    middleware=[personalized_prompt],\n    context_schema=ContextSchema\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    context=ContextSchema(user_name=\"John Smith\")  \n)\n\nSee Agents for details.\nThe Runtime object can be used to access static context and other utilities like the active store and stream writer. See the Runtime documentation for details.\nï¿½\nDynamic runtime context\nDynamic runtime context represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as short-term memory during a run.\nIn an agent\nIn a workflow\nExample shows how to incorporate state into an agent prompt.\nState can also be accessed by the agentâ€™s tools, which can read or update the state as needed. See tool calling guide for details.\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\nfrom langchain.agents import AgentState\n\n\nclass CustomState(AgentState):  \n    user_name: str\n\n@dynamic_prompt\ndef personalized_prompt(request: ModelRequest) -> str:  \n    user_name = request.state.get(\"user_name\", \"User\")\n    return f\"You are a helpful assistant. User's name is {user_name}\"\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[...],\n    state_schema=CustomState,  \n    middleware=[personalized_prompt],  \n)\n\nagent.invoke({\n    \"messages\": \"hi!\",\n    \"user_name\": \"John Smith\"\n})\n\nTurning on memory Please see the memory guide for more details on how to enable memory. This is a powerful feature that allows you to persist the agentâ€™s state across multiple invocations. Otherwise, the state is scoped only to a single run.\nï¿½\nDynamic cross-conversation context\nDynamic cross-conversation context represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as long-term memory across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).\nï¿½\nLearn more\nMemory conceptual overview\nShort-term memory in LangChain\nMemory in LangGraph\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494446,
    "timestamp": "2026-02-25T08:10:59.991Z",
    "title": "Graph API overview - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langgraph/graph-api",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nChoosing APIs\nGraph API\nUse the graph API\nFunctional API\nRuntime\nOn this page\nGraphs\nStateGraph\nCompiling your graph\nState\nSchema\nMultiple schemas\nReducers\nDefault reducer\nOverwrite\nWorking with messages in graph state\nWhy use messages?\nUsing messages in your graph\nSerialization\nMessagesState\nNodes\nSTART node\nEND node\nNode caching\nEdges\nNormal edges\nConditional edges\nEntry point\nConditional entry point\nSend\nCommand\nWhen should I use command instead of conditional edges?\nNavigating to a node in a parent graph\nUsing inside tools\nHuman-in-the-loop\nGraph migrations\nRuntime context\nRecursion limit\nAccessing and handling the recursion counter\nHow it works\nAccessing the current step counter\nProactive recursion handling\nProactive vs reactive approaches\nOther available metadata\nVisualization\nObservability and Tracing\nLearn more\nLangGraph APIs\nGraph API\nGraph API overview\nCopy page\nï¿½\nGraphs\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\nState: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\nNodes: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\nEdges: Functions that determine which Node to execute next based on the current state. They can be conditional branches or fixed transitions.\nBy composing Nodes and Edges, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state.\nTo emphasize: Nodes and Edges are nothing more than functions â€“ they can contain an LLM or just good olâ€™ code.\nIn short: nodes do the work, edges tell what to do next.\nLangGraphâ€™s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Googleâ€™s Pregel system, the program proceeds in discrete â€œsuper-steps.â€\nA super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or â€œchannelsâ€). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive. The graph execution terminates when all nodes are inactive and no messages are in transit.\nï¿½\nStateGraph\nThe StateGraph class is the main graph class to use. This is parameterized by a user defined State object.\nï¿½\nCompiling your graph\nTo build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?\nCompiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the .compile method:\nCopy\ngraph = graph_builder.compile(...)\n\nYou MUST compile your graph before you can use it.\nï¿½\nState\nThe first thing you do when you define a graph is define the State of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function.\nï¿½\nSchema\nThe main documented way to specify the schema of a graph is by using a TypedDict. If you want to provide default values in your state, use a dataclass. We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that Pydantic is less performant than a TypedDict or dataclass).\nBy default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide for more information.\nThe higher-level create_agent factory in langchain does not support Pydantic state schemas.\nï¿½\nMultiple schemas\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:\nInternal nodes can pass information that is not required in the graphâ€™s input / output.\nWe may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, PrivateState.\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an â€œinternalâ€ schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the â€œinternalâ€ schema to constrain the input and output of the graph. See this guide for more detail.\nLetâ€™s look at an example:\nCopy\nclass InputState(TypedDict):\n    user_input: str\n\nclass OutputState(TypedDict):\n    graph_output: str\n\nclass OverallState(TypedDict):\n    foo: str\n    user_input: str\n    graph_output: str\n\nclass PrivateState(TypedDict):\n    bar: str\n\ndef node_1(state: InputState) -> OverallState:\n    # Write to OverallState\n    return {\"foo\": state[\"user_input\"] + \" name\"}\n\ndef node_2(state: OverallState) -> PrivateState:\n    # Read from OverallState, write to PrivateState\n    return {\"bar\": state[\"foo\"] + \" is\"}\n\ndef node_3(state: PrivateState) -> OutputState:\n    # Read from PrivateState, write to OutputState\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", \"node_3\")\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\ngraph.invoke({\"user_input\":\"My\"})\n# {'graph_output': 'My name is Lance'}\n\nThere are two subtle and important points to note here:\nWe pass state: InputState as the input schema to node_1. But, we write out to foo, a channel in OverallState. How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includes OverallState and the filters InputState and OutputState.\nWe initialize the graph with:\nCopy\nStateGraph(\n    OverallState,\n    input_schema=InputState,\n    output_schema=OutputState\n)\n\nSo, how can we write to PrivateState in node_2? How does the graph gain access to this schema if it was not passed in the StateGraph initialization?\nWe can do this because _nodes can also declare additional state channels_ as long as the state schema definition exists. In this case, the PrivateState schema is defined, so we can add bar as a new state channel in the graph and write to it.\nï¿½\nReducers\nReducers are key to understanding how updates from nodes are applied to the State. Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\nï¿½\nDefault reducer\nThese two examples show how to use the default reducer:\nExample A\nCopy\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: int\n    bar: list[str]\n\nIn this example, no reducer functions are specified for any key. Letâ€™s assume the input to the graph is:\n{\"foo\": 1, \"bar\": [\"hi\"]}. Letâ€™s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]}\nExample B\nCopy\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n\nIn this example, weâ€™ve used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Letâ€™s assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]}. Letâ€™s then assume the first Node returns {\"foo\": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]}. If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}. Notice here that the bar key is updated by adding the two lists together.\nï¿½\nOverwrite\nIn some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the Overwrite type for this purpose. Learn how to use Overwrite here.\nï¿½\nWorking with messages in graph state\nï¿½\nWhy use messages?\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChainâ€™s chat model interface in particular accepts a list of message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response).\nTo read more about what message objects are, please refer to the Messages conceptual guide.\nï¿½\nUsing messages in your graph\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you donâ€™t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\nï¿½\nSerialization\nIn addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel.\nSee more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\nCopy\n# this is supported\n{\"messages\": [HumanMessage(content=\"message\")]}\n\n# and this is also supported\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\n\nSince the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state[\"messages\"][-1].content.\nBelow is an example of a graph that uses add_messages as its reducer function.\nCopy\nfrom langchain.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nclass GraphState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n\nï¿½\nMessagesState\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\nCopy\nfrom langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    documents: list[str]\n\nï¿½\nNodes\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\nstate â€“ The state of the graph\nconfig â€“ A RunnableConfig object that contains configuration information like thread_id and tracing information like tags\nruntime â€“ A Runtime object that contains runtime context and other information like store and stream_writer\nSimilar to NetworkX, you add these nodes to a graph using the add_node method:\nCopy\nfrom dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\nfrom langgraph.runtime import Runtime\n\nclass State(TypedDict):\n    input: str\n    results: str\n\n@dataclass\nclass Context:\n    user_id: str\n\nbuilder = StateGraph(State)\n\ndef plain_node(state: State):\n    return state\n\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\n    print(\"In node: \", runtime.context.user_id)\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\ndef node_with_config(state: State, config: RunnableConfig):\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\n\nbuilder.add_node(\"plain_node\", plain_node)\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\nbuilder.add_node(\"node_with_config\", node_with_config)\n...\n\nBehind the scenes, functions are converted to RunnableLambda, which add batch and async support to your function, along with native tracing and debugging.\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\nCopy\nbuilder.add_node(my_node)\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\n\nï¿½\nSTART node\nThe START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\nCopy\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n\nï¿½\nEND node\nThe END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\nCopy\nfrom langgraph.graph import END\n\ngraph.add_edge(\"node_a\", END)\n\nï¿½\nNode caching\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\nSpecify a cache when compiling a graph (or specifying an entrypoint)\nSpecify a cache policy for nodes. Each cache policy supports:\nkey_func used to generate a cache key based on the input to a node, which defaults to a hash of the input with pickle.\nttl, the time to live for the cache in seconds. If not specified, the cache will never expire.\nFor example:\nCopy\nimport time\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.types import CachePolicy\n\n\nclass State(TypedDict):\n    x: int\n    result: int\n\n\nbuilder = StateGraph(State)\n\n\ndef expensive_node(state: State) -> dict[str, int]:\n    # expensive computation\n    time.sleep(2)\n    return {\"result\": state[\"x\"] * 2}\n\n\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\nbuilder.set_entry_point(\"expensive_node\")\nbuilder.set_finish_point(\"expensive_node\")\n\ngraph = builder.compile(cache=InMemoryCache())\n\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))    \n# [{'expensive_node': {'result': 10}}]\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))    \n# [{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]\n\nFirst run takes two seconds to run (due to mocked expensive computation).\nSecond run utilizes cache and returns quickly.\nï¿½\nEdges\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\nNormal Edges: Go directly from one node to the next.\nConditional Edges: Call a function to determine which node(s) to go to next.\nEntry Point: Which node to call first when user input arrives.\nConditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\nA node can have multiple outgoing edges. If a node has multiple outgoing edges, all of those destination nodes will be executed in parallel as a part of the next superstep.\nï¿½\nNormal edges\nIf you always want to go from node A to node B, you can use the add_edge method directly.\nCopy\ngraph.add_edge(\"node_a\", \"node_b\")\n\nï¿½\nConditional edges\nIf you want to optionally route to one or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a â€œrouting functionâ€ to call after that node is executed:\nCopy\ngraph.add_conditional_edges(\"node_a\", routing_function)\n\nSimilar to nodes, the routing_function accepts the current state of the graph and returns a value.\nBy default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\nYou can optionally provide a dictionary that maps the routing_functionâ€™s output to the name of the next node.\nCopy\ngraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\n\nUse Command instead of conditional edges if you want to combine state updates and routing in a single function.\nï¿½\nEntry point\nThe entry point is the first node(s) that are run when the graph starts. You can use the add_edge method from the virtual START node to the first node to execute to specify where to enter the graph.\nCopy\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n\nï¿½\nConditional entry point\nA conditional entry point lets you start at different nodes depending on custom logic. You can use add_conditional_edges from the virtual START node to accomplish this.\nCopy\nfrom langgraph.graph import START\n\ngraph.add_conditional_edges(START, routing_function)\n\nYou can optionally provide a dictionary that maps the routing_functionâ€™s output to the name of the next node.\nCopy\ngraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\n\nï¿½\nSend\nBy default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object).\nTo support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node.\nCopy\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\n\nï¿½\nCommand\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\nCopy\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n\nWith Command you can also achieve dynamic control flow behavior (identical to conditional edges):\nCopy\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    if state[\"foo\"] == \"bar\":\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\n\nNote that Command only adds dynamic edges, while static edges will still execute. In other words, Command doesnâ€™t override static edges.\nCopy\ndef node_a(state: State) -> Command[Literal[\"my_other_node\"]]:\n   if state[\"foo\"] == \"bar\":\n       return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\n\n# Add a static edge from \"node_a\" to \"node_b\"\ngraph.add_edge(\"node_a\", \"node_b\")\n\n# Command will NOT prevent \"node_a\" from going to \"node_b\"\n\nIn the example above, â€œnode_aâ€ will go to both â€œnode_bâ€ and â€œmy_other_nodeâ€.\nWhen returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]]. This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node.\nCheck out this how-to guide for an end-to-end example of how to use Command.\nï¿½\nWhen should I use command instead of conditional edges?\nUse Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where itâ€™s important to route to a different agent and pass some information to that agent.\nUse conditional edges to route between nodes conditionally without updating the state.\nï¿½\nNavigating to a node in a parent graph\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\nCopy\ndef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n\nSetting graph to Command.PARENT will navigate to the closest parent graph.\nWhen you send updates from a subgraph node to a parent graph node for a key thatâ€™s shared by both parent and subgraph state schemas, you must define a reducer for the key youâ€™re updating in the parent graph state. See this example.\nThis is particularly useful when implementing multi-agent handoffs.\nCheck out this guide for detail.\nï¿½\nUsing inside tools\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\nRefer to this guide for detail.\nï¿½\nHuman-in-the-loop\nCommand is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\"). Check out this conceptual guide for more information.\nï¿½\nGraph migrations\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.\nFor threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\nFor threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) â€” if this is a blocker please reach out and we can prioritize a solution.\nFor modifying state, we have full backwards and forwards compatibility for adding and removing keys\nState keys that are renamed lose their saved state in existing threads\nState keys whose types change in incompatible ways could currently cause issues in threads with state from before the change â€” if this is a blocker please reach out and we can prioritize a solution.\nï¿½\nRuntime context\nWhen creating a graph, you can specify a context_schema for runtime context passed to nodes. This is useful for passing information to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\nCopy\n@dataclass\nclass ContextSchema:\n    llm_provider: str = \"openai\"\n\ngraph = StateGraph(State, context_schema=ContextSchema)\n\nYou can then pass this context into the graph using the context parameter of the invoke method.\nCopy\ngraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\n\nYou can then access and use this context inside a node or conditional edge:\nCopy\nfrom langgraph.runtime import Runtime\n\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\n    llm = get_llm(runtime.context.llm_provider)\n    # ...\n\nSee this guide for a full breakdown on configuration.\nï¿½\nRecursion limit\nThe recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise GraphRecursionError. Starting in version 1.0.6, the deafult recursion limit is set to 1000 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke/stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:\nCopy\ngraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})\n\nRead this how-to to learn more about how the recursion limit works.\nï¿½\nAccessing and handling the recursion counter\nThe current step counter is accessible in config[\"metadata\"][\"langgraph_step\"] within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.\nï¿½\nHow it works\nThe step counter is stored in config[\"metadata\"][\"langgraph_step\"]. The recursion limit check follows the logic: step > stop where stop = step + recursion_limit + 1. When the limit is exceeded, LangGraph raises a GraphRecursionError.\nï¿½\nAccessing the current step counter\nYou can access the current step counter within any node to monitor execution progress.\nCopy\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\n\ndef my_node(state: dict, config: RunnableConfig) -> dict:\n    current_step = config[\"metadata\"][\"langgraph_step\"]\n    print(f\"Currently on step: {current_step}\")\n    return state\n\nï¿½\nProactive recursion handling\nLangGraph provides a RemainingSteps managed value that tracks how many steps remain before hitting the recursion limit. This allows for graceful degradation within your graph.\nCopy\nfrom typing import Annotated, Literal\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.managed import RemainingSteps\n\nclass State(TypedDict):\n    messages: Annotated[list, lambda x, y: x + y]\n    remaining_steps: RemainingSteps  # Managed value - tracks steps until limit\n\ndef reasoning_node(state: State) -> dict:\n    # RemainingSteps is automatically populated by LangGraph\n    remaining = state[\"remaining_steps\"]\n\n    # Check if we're running low on steps\n    if remaining <= 2:\n        return {\"messages\": [\"Approaching limit, wrapping up...\"]}\n\n    # Normal processing\n    return {\"messages\": [\"thinking...\"]}\n\ndef route_decision(state: State) -> Literal[\"reasoning_node\", \"fallback_node\"]:\n    \"\"\"Route based on remaining steps\"\"\"\n    if state[\"remaining_steps\"] <= 2:\n        return \"fallback_node\"\n    return \"reasoning_node\"\n\ndef fallback_node(state: State) -> dict:\n    \"\"\"Handle cases where recursion limit is approaching\"\"\"\n    return {\"messages\": [\"Reached complexity limit, providing best effort answer\"]}\n\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"reasoning_node\", reasoning_node)\nbuilder.add_node(\"fallback_node\", fallback_node)\nbuilder.add_edge(START, \"reasoning_node\")\nbuilder.add_conditional_edges(\"reasoning_node\", route_decision)\nbuilder.add_edge(\"fallback_node\", END)\n\ngraph = builder.compile()\n\n# RemainingSteps works with any recursion_limit\nresult = graph.invoke({\"messages\": []}, {\"recursion_limit\": 10})\n\nï¿½\nProactive vs reactive approaches\nThere are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).\nCopy\nfrom typing import Annotated, Literal, TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.managed import RemainingSteps\nfrom langgraph.errors import GraphRecursionError\n\nclass State(TypedDict):\n    messages: Annotated[list, lambda x, y: x + y]\n    remaining_steps: RemainingSteps\n\n# Proactive Approach (recommended) - using RemainingSteps\ndef agent_with_monitoring(state: State) -> dict:\n    \"\"\"Proactively monitor and handle recursion within the graph\"\"\"\n    remaining = state[\"remaining_steps\"]\n\n    # Early detection - route to internal handling\n    if remaining <= 2:\n        return {\n            \"messages\": [\"Approaching limit, returning partial result\"]\n        }\n\n    # Normal processing\n    return {\"messages\": [f\"Processing... ({remaining} steps remaining)\"]}\n\ndef route_decision(state: State) -> Literal[\"agent\", END]:\n    if state[\"remaining_steps\"] <= 2:\n        return END\n    return \"agent\"\n\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"agent\", agent_with_monitoring)\nbuilder.add_edge(START, \"agent\")\nbuilder.add_conditional_edges(\"agent\", route_decision)\ngraph = builder.compile()\n\n# Proactive: Graph completes gracefully\nresult = graph.invoke({\"messages\": []}, {\"recursion_limit\": 10})\n\n# Reactive Approach (fallback) - catching error externally\ntry:\n    result = graph.invoke({\"messages\": []}, {\"recursion_limit\": 10})\nexcept GraphRecursionError as e:\n    # Handle externally after graph execution fails\n    result = {\"messages\": [\"Fallback: recursion limit exceeded\"]}\n\nThe key differences between these approaches are:\nApproach\tDetection\tHandling\tControl Flow\nProactive (using RemainingSteps)\tBefore limit reached\tInside graph via conditional routing\tGraph continues to completion node\nReactive (catching GraphRecursionError)\tAfter limit exceeded\tOutside graph in try/catch\tGraph execution terminated\nProactive advantages:\nGraceful degradation within the graph\nCan save intermediate state in checkpoints\nBetter user experience with partial results\nGraph completes normally (no exception)\nReactive advantages:\nSimpler implementation\nNo need to modify graph logic\nCentralized error handling\nï¿½\nOther available metadata\nAlong with langgraph_step, the following metadata is also available in config[\"metadata\"]:\nCopy\ndef inspect_metadata(state: dict, config: RunnableConfig) -> dict:\n    metadata = config[\"metadata\"]\n\n    print(f\"Step: {metadata['langgraph_step']}\")\n    print(f\"Node: {metadata['langgraph_node']}\")\n    print(f\"Triggers: {metadata['langgraph_triggers']}\")\n    print(f\"Path: {metadata['langgraph_path']}\")\n    print(f\"Checkpoint NS: {metadata['langgraph_checkpoint_ns']}\")\n\n    return state\n\nï¿½\nVisualization\nItâ€™s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.\nï¿½\nObservability and Tracing\nTo trace, debug and evaluate your agents, use LangSmith.\nï¿½\nLearn more\nHow to use the Graph API\nFunctional API conceptual overview\nChoosing between Graph API and Functional API\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494441,
    "timestamp": "2026-02-25T08:10:59.993Z",
    "title": "Build a SQL assistant with on-demand skills - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nHow it works\nSetup\nInstallation\nLangSmith\nSelect an LLM\n1. Define skills\n2. Create skill loading tool\n3. Build skill middleware\n4. Create the agent with skill support\n5. Test progressive disclosure\n6. Advanced: Add constraints with custom state\nComplete example\nImplementation variations\nProgressive disclosure and context engineering\nNext steps\nTutorials\nMulti-agent\nBuild a SQL assistant with on-demand skills\nCopy page\nThis tutorial shows how to use progressive disclosure - a context management technique where the agent loads information on-demand rather than upfront - to implement skills (specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task.\nUse case: Imagine building an agent to help write SQL queries across different business verticals in a large enterprise. Your organization might have separate datastores for each vertical, or a single monolithic database with thousands of tables. Either way, loading all schemas upfront would overwhelm the context window. Progressive disclosure solves this by loading only the relevant schema when needed. This architecture also enables different product owners and stakeholders to independently contribute and maintain skills for their specific business verticals.\nWhat youâ€™ll build: A SQL query assistant with two skills (sales analytics and inventory management). The agent sees lightweight skill descriptions in its system prompt, then loads full database schemas and business logic through tool calls only when relevant to the userâ€™s query.\nFor a more complete example of a SQL agent with query execution, error correction, and validation, see our SQL Agent tutorial. This tutorial focuses on the progressive disclosure pattern which can be applied to any domain.\nProgressive disclosure was popularized by Anthropic as a technique for building scalable agent skills systems. This approach uses a three-level architecture (metadata â†’ core content â†’ detailed resources) where agents load information only as needed. For more on this technique, see Equipping agents for the real world with Agent Skills.\nï¿½\nHow it works\nHereâ€™s the flow when a user asks for a SQL query:\n\nðŸ’¬ User: Write SQL query\nfor high-value customers\n\nðŸ“‹ Agent sees skill descriptions:\nâ€¢ sales_analytics\nâ€¢ inventory_management\n\nðŸ¤” Need sales schema\n\nðŸ”§ load_skill\n'sales_analytics'\n\nðŸ“Š Schema loaded:\ncustomers, orders tables\n+ business logic\n\nâœï¸ Agent writes SQL query\nusing schema knowledge\n\nâœ… Returns valid SQL\nfollowing business rules\n\nWhy progressive disclosure:\nReduces context usage - load only the 2-3 skills needed for a task, not all available skills\nEnables team autonomy - different teams can develop specialized skills independently (similar to other multi-agent architectures)\nScales efficiently - add dozens or hundreds of skills without overwhelming context\nSimplifies conversation history - single agent with one conversation thread\nWhat are skills: Skills, as popularized by Claude Code, are primarily prompt-based: self-contained units of specialized instructions for specific business tasks. In Claude Code, skills are exposed as directories with files on the file system, discovered through file operations. Skills guide behavior through prompts and can provide information about tool usage or include sample code for a coding agent to execute.\nSkills with progressive disclosure can be viewed as a form of RAG (Retrieval-Augmented Generation), where each skill is a retrieval unitâ€”though not necessarily backed by embeddings or keyword search, but by tools for browsing content (like file operations or, in this tutorial, direct lookup).\nTrade-offs:\nLatency: Loading skills on-demand requires additional tool calls, which adds latency to the first request that needs each skill\nWorkflow control: Basic implementations rely on prompting to guide skill usage - you cannot enforce hard constraints like â€œalways try skill A before skill Bâ€ without custom logic\nImplementing your own skills system\nWhen building your own skills implementation (as we do in this tutorial), the core concept is progressive disclosure - loading information on-demand. Beyond that, you have full flexibility in implementation:\nStorage: databases, S3, in-memory data structures, or any backend\nDiscovery: direct lookup (this tutorial), RAG for large skill collections, file system scanning, or API calls\nLoading logic: customize latency characteristics and add logic to search through skill content or rank relevance\nSide effects: define what happens when a skill loads, such as exposing tools associated with that skill (covered in section 8)\nThis flexibility lets you optimize for your specific requirements around performance, storage, and workflow control.\nï¿½\nSetup\nï¿½\nInstallation\nThis tutorial requires the langchain package:\npip\nuv\nconda\nCopy\npip install langchain\n\nFor more details, see our Installation guide.\nï¿½\nLangSmith\nSet up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:\nbash\npython\nCopy\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\nï¿½\nSelect an LLM\nSelect a chat model from LangChainâ€™s suite of integrations:\nOpenAI\nAnthropic\nAzure\nGoogle Gemini\nAWS Bedrock\nHuggingFace\nðŸ‘‰ Read the OpenAI chat model integration docs\nCopy\npip install -U \"langchain[openai]\"\n\ninit_chat_model\nModel Class\nCopy\nimport os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nmodel = init_chat_model(\"gpt-4.1\")\n\nï¿½\n1. Define skills\nFirst, define the structure for skills. Each skill has a name, a brief description (shown in the system prompt), and full content (loaded on-demand):\nCopy\nfrom typing import TypedDict\n\nclass Skill(TypedDict):  \n    \"\"\"A skill that can be progressively disclosed to the agent.\"\"\"\n    name: str  # Unique identifier for the skill\n    description: str  # 1-2 sentence description to show in system prompt\n    content: str  # Full skill content with detailed instructions\n\nNow define example skills for a SQL query assistant. The skills are designed to be lightweight in description (shown to the agent upfront) but detailed in content (loaded only when needed):\n\nView complete skill definitions\n\nï¿½\n2. Create skill loading tool\nCreate a tool to load full skill content on-demand:\nCopy\nfrom langchain.tools import tool\n\n@tool\ndef load_skill(skill_name: str) -> str:\n    \"\"\"Load the full content of a skill into the agent's context.\n\n    Use this when you need detailed information about how to handle a specific\n    type of request. This will provide you with comprehensive instructions,\n    policies, and guidelines for the skill area.\n\n    Args:\n        skill_name: The name of the skill to load (e.g., \"expense_reporting\", \"travel_booking\")\n    \"\"\"\n    # Find and return the requested skill\n    for skill in SKILLS:\n        if skill[\"name\"] == skill_name:\n            return f\"Loaded skill: {skill_name}\\n\\n{skill['content']}\"\n\n    # Skill not found\n    available = \", \".join(s[\"name\"] for s in SKILLS)\n    return f\"Skill '{skill_name}' not found. Available skills: {available}\"\n\nThe load_skill tool returns the full skill content as a string, which becomes part of the conversation as a ToolMessage. For more details on creating and using tools, see the Tools guide.\nï¿½\n3. Build skill middleware\nCreate custom middleware that injects skill descriptions into the system prompt. This middleware makes skills discoverable without loading their full content upfront.\nThis guide demonstrates creating custom middleware. For a comprehensive guide on middleware concepts and patterns, see the custom middleware documentation.\nCopy\nfrom langchain.agents.middleware import ModelRequest, ModelResponse, AgentMiddleware\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n\nclass SkillMiddleware(AgentMiddleware):  \n    \"\"\"Middleware that injects skill descriptions into the system prompt.\"\"\"\n\n    # Register the load_skill tool as a class variable\n    tools = [load_skill]  \n\n    def __init__(self):\n        \"\"\"Initialize and generate the skills prompt from SKILLS.\"\"\"\n        # Build skills prompt from the SKILLS list\n        skills_list = []\n        for skill in SKILLS:\n            skills_list.append(\n                f\"- **{skill['name']}**: {skill['description']}\"\n            )\n        self.skills_prompt = \"\\n\".join(skills_list)\n\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse],\n    ) -> ModelResponse:\n        \"\"\"Sync: Inject skill descriptions into system prompt.\"\"\"\n        # Build the skills addendum\n        skills_addendum = ( \n            f\"\\n\\n## Available Skills\\n\\n{self.skills_prompt}\\n\\n\"\n            \"Use the load_skill tool when you need detailed information \"\n            \"about handling a specific type of request.\"\n        )\n\n        # Append to system message content blocks\n        new_content = list(request.system_message.content_blocks) + [\n            {\"type\": \"text\", \"text\": skills_addendum}\n        ]\n        new_system_message = SystemMessage(content=new_content)\n        modified_request = request.override(system_message=new_system_message)\n        return handler(modified_request)\n\nThe middleware appends skill descriptions to the system prompt, making the agent aware of available skills without loading their full content. The load_skill tool is registered as a class variable, making it available to the agent.\nProduction consideration: This tutorial loads the skill list in __init__ for simplicity. In a production system, you may want to load skills in the before_agent hook instead, allowing them to be refreshed periodically to reflect up-to-date changes (e.g., when new skills are added or existing ones are modified). See the before_agent hook documentation for details.\nï¿½\n4. Create the agent with skill support\nNow create the agent with the skill middleware and a checkpointer for state persistence:\nCopy\nfrom langchain.agents import create_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Create the agent with skill support\nagent = create_agent(\n    model,\n    system_prompt=(\n        \"You are a SQL query assistant that helps users \"\n        \"write queries against business databases.\"\n    ),\n    middleware=[SkillMiddleware()],  \n    checkpointer=InMemorySaver(),\n)\n\nThe agent now has access to skill descriptions in its system prompt and can call load_skill to retrieve full skill content when needed. The checkpointer maintains conversation history across turns.\nï¿½\n5. Test progressive disclosure\nTest the agent with a question that requires skill-specific knowledge:\nCopy\nimport uuid\n\n# Configuration for this conversation thread\nthread_id = str(uuid.uuid4())\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\n\n# Ask for a SQL query\nresult = agent.invoke(  \n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": (\n                    \"Write a SQL query to find all customers \"\n                    \"who made orders over $1000 in the last month\"\n                ),\n            }\n        ]\n    },\n    config\n)\n\n# Print the conversation\nfor message in result[\"messages\"]:\n    if hasattr(message, 'pretty_print'):\n        message.pretty_print()\n    else:\n        print(f\"{message.type}: {message.content}\")\n\nExpected output:\nCopy\n================================ Human Message =================================\n\nWrite a SQL query to find all customers who made orders over $1000 in the last month\n================================== Ai Message ==================================\nTool Calls:\n  load_skill (call_abc123)\n Call ID: call_abc123\n  Args:\n    skill_name: sales_analytics\n================================= Tool Message =================================\nName: load_skill\n\nLoaded skill: sales_analytics\n\n# Sales Analytics Schema\n\n## Tables\n\n### customers\n- customer_id (PRIMARY KEY)\n- name\n- email\n- signup_date\n- status (active/inactive)\n- customer_tier (bronze/silver/gold/platinum)\n\n### orders\n- order_id (PRIMARY KEY)\n- customer_id (FOREIGN KEY -> customers)\n- order_date\n- status (pending/completed/cancelled/refunded)\n- total_amount\n- sales_region (north/south/east/west)\n\n[... rest of schema ...]\n\n## Business Logic\n\n**High-value orders**: Orders with `total_amount > 1000`\n**Revenue calculation**: Only count orders with `status = 'completed'`\n\n================================== Ai Message ==================================\n\nHere's a SQL query to find all customers who made orders over $1000 in the last month:\n\n\\`\\`\\`sql\nSELECT DISTINCT\n    c.customer_id,\n    c.name,\n    c.email,\n    c.customer_tier\nFROM customers c\nJOIN orders o ON c.customer_id = o.customer_id\nWHERE o.total_amount > 1000\n  AND o.status = 'completed'\n  AND o.order_date >= CURRENT_DATE - INTERVAL '1 month'\nORDER BY c.customer_id;\n\\`\\`\\`\n\nThis query:\n- Joins customers with their orders\n- Filters for high-value orders (>$1000) using the total_amount field\n- Only includes completed orders (as per the business logic)\n- Restricts to orders from the last month\n- Returns distinct customers to avoid duplicates if they made multiple qualifying orders\n\nThe agent saw the lightweight skill description in its system prompt, recognized the question required sales database knowledge, called load_skill(\"sales_analytics\") to get the full schema and business logic, and then used that information to write a correct query following the database conventions.\nï¿½\n6. Advanced: Add constraints with custom state\n\nOptional: Track loaded skills and enforce tool constraints\n\nï¿½\nComplete example\n\nView complete runnable script\n\nï¿½\nImplementation variations\n\nView implementation options and trade-offs\n\nï¿½\nProgressive disclosure and context engineering\n\nCombining with few-shot prompting and other techniques\n\nï¿½\nNext steps\nLearn about middleware for more dynamic agent behaviors\nExplore context engineering techniques for managing agent context\nExplore the handoffs pattern for sequential workflows\nRead the subagents pattern for parallel task routing\nSee multi-agent patterns for other approaches to specialized agents\nUse LangSmith to debug and monitor skill loading\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494447,
    "timestamp": "2026-02-25T08:10:59.999Z",
    "title": "Functional API overview - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langgraph/functional-api",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nGet started\nInstall\nQuickstart\nLocal server\nChangelog\nThinking in LangGraph\nWorkflows + agents\nCapabilities\nPersistence\nDurable execution\nStreaming\nInterrupts\nTime travel\nMemory\nSubgraphs\nProduction\nApplication structure\nTest\nLangSmith Studio\nAgent Chat UI\nLangSmith Deployment\nLangSmith Observability\nLangGraph APIs\nGraph API\nFunctional API\nFunctional API\nUse the Functional API\nRuntime\nOn this page\nFunctional API vs. Graph API\nExample\nEntrypoint\nDefinition\nInjectable parameters\nExecuting\nResuming\nShort-term memory\nentrypoint.final\nTask\nDefinition\nExecution\nWhen to use a task\nSerialization\nDeterminism\nIdempotency\nCommon pitfalls\nHandling side effects\nNon-deterministic control flow\nLearn more\nLangGraph APIs\nFunctional API\nFunctional API overview\nCopy page\nThe Functional API allows you to add LangGraphâ€™s key features â€” persistence, memory, human-in-the-loop, and streaming â€” to your applications with minimal changes to your existing code.\nIt is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.\nThe Functional API uses two key building blocks:\n@entrypoint â€“ Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.\n@task â€“ Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.\nThis provides a minimal abstraction for building workflows with state management and streaming.\nFor information on how to use the functional API, see Use Functional API.\nï¿½\nFunctional API vs. Graph API\nFor users who prefer a more declarative approach, LangGraphâ€™s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.\nHere are some key differences:\nControl flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.\nShort-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and @tasks do not require explicit state management as their state is scoped to the function and is not shared across functions.\nCheckpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.\nVisualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.\nï¿½\nExample\nBelow we demonstrate a simple application that writes an essay and interrupts to request human review.\nCopy\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import interrupt\n\n@task\ndef write_essay(topic: str) -> str:\n    \"\"\"Write an essay about the given topic.\"\"\"\n    time.sleep(1) # A placeholder for a long-running task.\n    return f\"An essay about topic: {topic}\"\n\n@entrypoint(checkpointer=InMemorySaver())\ndef workflow(topic: str) -> dict:\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\n    essay = write_essay(\"cat\").result()\n    is_approved = interrupt({\n        # Any json-serializable payload provided to interrupt as argument.\n        # It will be surfaced on the client side as an Interrupt when streaming data\n        # from the workflow.\n        \"essay\": essay, # The essay we want reviewed.\n        # We can add any additional information that we need.\n        # For example, introduce a key called \"action\" with some instructions.\n        \"action\": \"Please approve/reject the essay\",\n    })\n\n    return {\n        \"essay\": essay, # The essay that was generated\n        \"is_approved\": is_approved, # Response from HIL\n    }\n\n\nDetailed Explanation\n\nï¿½\nEntrypoint\nThe @entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.\nï¿½\nDefinition\nAn entrypoint is defined by decorating a function with the @entrypoint decorator.\nThe function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.\nDecorating a function with an entrypoint produces a Pregel instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).\nYou will usually want to pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop.\nSync\nAsync\nCopy\nfrom langgraph.func import entrypoint\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(some_input: dict) -> int:\n    # some logic that may involve long-running tasks like API calls,\n    # and may be interrupted for human-in-the-loop.\n    ...\n    return result\n\nSerialization The inputs and outputs of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.\nï¿½\nInjectable parameters\nWhen declaring an entrypoint, you can request access to additional parameters that will be injected automatically at runtime. These parameters include:\nParameter\tDescription\nprevious\tAccess the state associated with the previous checkpoint for the given thread. See short-term-memory.\nstore\tAn instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory.\nwriter\tUse to access the StreamWriter when working with Async Python < 3.11. See streaming with functional API for details.\nconfig\tFor accessing run time configuration. See RunnableConfig for information.\nDeclare the parameters with the appropriate name and type annotation.\n\nRequesting Injectable Parameters\n\nï¿½\nExecuting\nUsing the @entrypoint yields a Pregel object that can be executed using the invoke, ainvoke, stream, and astream methods.\nInvoke\nAsync Invoke\nStream\nAsync Stream\nCopy\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\nmy_workflow.invoke(some_input, config)  # Wait for the result synchronously\n\nï¿½\nResuming\nResuming an execution after an interrupt can be done by passing a resume value to the Command primitive.\nInvoke\nAsync Invoke\nStream\nAsync Stream\nCopy\nfrom langgraph.types import Command\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nmy_workflow.invoke(Command(resume=some_resume_value), config)\n\nResuming after an error\nTo resume after an error, run the entrypoint with a None and the same thread id (config).\nThis assumes that the underlying error has been resolved and execution can proceed successfully.\nInvoke\nAsync Invoke\nStream\nAsync Stream\nCopy\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nmy_workflow.invoke(None, config)\n\nï¿½\nShort-term memory\nWhen an entrypoint is defined with a checkpointer, it stores information between successive invocations on the same thread id in checkpoints.\nThis allows accessing the state from the previous invocation using the previous parameter.\nBy default, the previous parameter is the return value of the previous invocation.\nCopy\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(number: int, *, previous: Any = None) -> int:\n    previous = previous or 0\n    return number + previous\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nmy_workflow.invoke(1, config)  # 1 (previous was None)\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)\n\nï¿½\nentrypoint.final\nentrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.\nThe first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type].\nCopy\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(number: int, *, previous: Any = None) -> entrypoint.final[int, int]:\n    previous = previous or 0\n    # This will return the previous value to the caller, saving\n    # 2 * number to the checkpoint, which will be used in the next invocation\n    # for the `previous` parameter.\n    return entrypoint.final(value=previous, save=2 * number)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\n\nmy_workflow.invoke(3, config)  # 0 (previous was None)\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\n\nï¿½\nTask\nA task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:\nAsynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.\nCheckpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).\nï¿½\nDefinition\nTasks are defined using the @task decorator, which wraps a regular Python function.\nCopy\nfrom langgraph.func import task\n\n@task()\ndef slow_computation(input_value):\n    # Simulate a long-running operation\n    ...\n    return result\n\nSerialization The outputs of tasks must be JSON-serializable to support checkpointing.\nï¿½\nExecution\nTasks can only be called from within an entrypoint, another task, or a state graph node.\nTasks cannot be called directly from the main application code.\nWhen you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.\nTo obtain the result of a task, you can either wait for it synchronously (using result()) or await it asynchronously (using await).\nSynchronous Invocation\nAsynchronous Invocation\nCopy\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(some_input: int) -> int:\n    future = slow_computation(some_input)\n    return future.result()  # Wait for the result synchronously\n\nï¿½\nWhen to use a task\nTasks are useful in the following scenarios:\nCheckpointing: When you need to save the result of a long-running operation to a checkpoint, so you donâ€™t need to recompute it when resuming the workflow.\nHuman-in-the-loop: If youâ€™re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.\nParallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).\nObservability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.\nRetryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic.\nï¿½\nSerialization\nThere are two key aspects to serialization in LangGraph:\nentrypoint inputs and outputs must be JSON-serializable.\ntask outputs must be JSON-serializable.\nThese requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.\nSerialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.\nProviding non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.\nï¿½\nDeterminism\nTo utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic.\nLangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.\nWhile different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.\nï¿½\nIdempotency\nIdempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.\nï¿½\nCommon pitfalls\nï¿½\nHandling side effects\nEncapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.\nIncorrect\nCorrect\nIn this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.\nCopy\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -> int:\n    # This code will be executed a second time when resuming the workflow.\n    # Which is likely not what you want.\n    with open(\"output.txt\", \"w\") as f:  \n        f.write(\"Side effect executed\")  \n    value = interrupt(\"question\")\n    return value\n\nï¿½\nNon-deterministic control flow\nOperations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.\nIn a task: Get random number (5) â†’ interrupt â†’ resume â†’ (returns 5 again) â†’ â€¦\nNot in a task: Get random number (5) â†’ interrupt â†’ resume â†’ get new random number (7) â†’ â€¦\nThis is especially important when using human-in-the-loop workflows with multiple interrupt calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, itâ€™s matched with the corresponding resume value. This matching is strictly index-based, so the order of the resume values should match the order of the interrupts.\nIf order of execution is not maintained when resuming, one interrupt call may be matched with the wrong resume value, leading to incorrect results.\nPlease read the section on determinism for more details.\nIncorrect\nCorrect\nIn this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.\nCopy\nfrom langgraph.func import entrypoint\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -> int:\n    t0 = inputs[\"t0\"]\n    t1 = time.time()  \n\n    delta_t = t1 - t0\n\n    if delta_t > 1:\n        result = slow_task(1).result()\n        value = interrupt(\"question\")\n    else:\n        result = slow_task(2).result()\n        value = interrupt(\"question\")\n\n    return {\n        \"result\": result,\n        \"value\": value\n    }\n\nï¿½\nLearn more\nHow to use the Functional API\nGraph API conceptual overview\nChoosing between Graph API and Functional API\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494449,
    "timestamp": "2026-02-25T08:11:00.007Z",
    "title": "Get help - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langchain/get-help",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nOn this page\nLearning resources\nCommunity support\nProfessional support\nContribute\nStay connected\nAdditional resources\nGet help\nCopy page\nConnect with the LangChain community, access learning resources, and get the support you need to build with confidence.\nï¿½\nLearning resources\nStart your journey or deepen your knowledge with our comprehensive learning materials.\nChat LangChain: Ask the docs anything about LangChain, powered by real-time docs\nAPI Reference: Complete documentation for all LangChain packages\nï¿½\nCommunity support\nGet help from fellow developers and the LangChain team through our active community channels.\nCommunity Forum: Ask questions, share solutions, and discuss best practices\nCommunity Slack: Connect with other builders and get quick help\nï¿½\nProfessional support\nFor enterprise needs and critical applications, access dedicated support channels.\nSupport portal: Submit tickets and track support requests\nLangSmith status: Real-time status of LangSmith services and APIs\nï¿½\nContribute\nHelp us improve LangChain for everyone. Whether youâ€™re fixing bugs, adding features, or improving documentation, we welcome your contributions.\nContributing Guide: Everything you need to know about contributing to LangChain\nï¿½\nStay connected\nFollow us for the latest updates, announcements, and community highlights.\nX (Twitter): Daily updates and community spotlights\nLinkedIn: Professional network and company updates\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494448,
    "timestamp": "2026-02-25T08:11:00.027Z",
    "title": "Case studies - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/langgraph/case-studies",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nLearn\nTutorials\nDeep Agents\nData Analysis\nLangChain\nSemantic search\nRAG agent\nSQL agent\nVoice agent\nMulti-agent\nSubagents: Personal assistant\nHandoffs: Customer support\nRouter: Knowledge base\nSkills: SQL assistant\nLangGraph\nCustom RAG agent\nCustom SQL agent\nConceptual overviews\nLangChain vs. LangGraph vs. Deep Agents\nComponent architecture\nMemory\nContext\nGraph API\nFunctional API\nAdditional resources\nLangChain Academy\nCase studies\nGet help\nAdditional resources\nCase studies\nCopy page\nThis list of companies using LangGraph and their success stories is compiled from public sources. If your company uses LangGraph, weâ€™d love for you to share your story and add it to the list. Youâ€™re also welcome to contribute updates based on publicly available information from other companies, such as blog posts or press releases.\nCompany\tIndustry\tUse case\tReference\nAirTop\tSoftware & Technology (GenAI Native)\tBrowser automation for AI agents\tCase study, 2024\nAppFolio\tReal Estate\tCopilot for domain-specific task\tCase study, 2024\nAthena Intelligence\tSoftware & Technology (GenAI Native)\tResearch & summarization\tCase study, 2024\nBlackRock\tFinancial Services\tCopilot for domain-specific task\tInterrupt talk, 2025\nCaptide\tSoftware & Technology (GenAI Native)\tData extraction\tCase study, 2025\nCisco CX\tSoftware & Technology\tCustomer support\tInterrupt Talk, 2025\nCisco Outshift\tSoftware & Technology\tDevOps\tVideo story, 2025; Case study, 2025; Blog post, 2025\nCisco TAC\tSoftware & Technology\tCustomer support\tVideo story, 2025\nCity of Hope\tNon-profit\tCopilot for domain-specific task\tVideo story, 2025\nC.H. Robinson\tLogistics\tAutomation\tCase study, 2025\nDefinely\tLegal\tCopilot for domain-specific task\tCase study, 2025\nDocent Pro\tTravel\tGenAI embedded product experiences\tCase study, 2025\nElastic\tSoftware & Technology\tCopilot for domain-specific task\tBlog post, 2025\nExa\tSoftware & Technology (GenAI Native)\tSearch\tCase study, 2025\nGitLab\tSoftware & Technology\tCode generation\tDuo workflow docs\nHarmonic\tSoftware & Technology\tSearch\tCase study, 2025\nInconvo\tSoftware & Technology\tCode generation\tCase study, 2025\nInfor\tSoftware & Technology\tGenAI embedded product experiences; customer support; copilot\tCase study, 2025\nJ.P. Morgan\tFinancial Services\tCopilot for domain-specific task\tInterrupt talk, 2025\nKlarna\tFintech\tCopilot for domain-specific task\tCase study, 2025\nKomodo Health\tHealthcare\tCopilot for domain-specific task\tBlog post\nLinkedIn\tSocial Media\tCode generation; Search & discovery\tInterrupt talk, 2025; Blog post, 2025; Blog post, 2024\nMinimal\tE-commerce\tCustomer support\tCase study, 2025\nModern Treasury\tFintech\tGenAI embedded product experiences\tVideo story, 2025\nMonday\tSoftware & Technology\tGenAI embedded product experiences\tInterrupt talk, 2025\nMorningstar\tFinancial Services\tResearch & summarization\tVideo story, 2025\nOpenRecovery\tHealthcare\tCopilot for domain-specific task\tCase study, 2024\nPigment\tFintech\tGenAI embedded product experiences\tVideo story, 2025\nProsper\tFintech\tCustomer support\tVideo story, 2025\nQodo\tSoftware & Technology (GenAI Native)\tCode generation\tBlog post, 2025\nRakuten\tE-commerce / Fintech\tCopilot for domain-specific task\tVideo story, 2025; Blog post, 2025\nReplit\tSoftware & Technology\tCode generation\tBlog post, 2024; Breakout agent story, 2024; Fireside chat video, 2024\nRexera\tReal Estate (GenAI Native)\tCopilot for domain-specific task\tCase study, 2024\nAbu Dhabi Government\tGovernment\tSearch\tCase study, 2025\nTradestack\tSoftware & Technology (GenAI Native)\tCopilot for domain-specific task\tCase study, 2024\nUber\tTransportation\tDeveloper productivity; Code generation\tInterrupt talk, 2025; Presentation, 2024; Video, 2024\nUnify\tSoftware & Technology (GenAI Native)\tCopilot for domain-specific task\tInterrupt talk, 2025; Blog post, 2024\nVizient\tHealthcare\tCopilot for domain-specific task\tVideo story, 2025; Case study, 2025\nVodafone\tTelecommunications\tCode generation; internal search\tCase study, 2025\nWebToon\tMedia & Entertainment\tData extraction\tCase study, 2025\n11x\tSoftware & Technology (GenAI Native)\tResearch & outreach\tInterrupt talk, 2025\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494613,
    "timestamp": "2026-02-25T08:13:51.270Z",
    "title": "Reference - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/reference/overview",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nReference\nLangChain SDK\nLangGraph SDK\nIntegrations\nDeep Agents\nErrors\nReference\nReleases\nVersioning\nChangelog\nReleases\nMigration guides\nPolicies\nRelease policy\nSecurity\nOn this page\nReference sites\nReference\nCopy page\nComprehensive API reference documentation for the LangChain and LangGraph Python and TypeScript libraries.\nï¿½\nReference sites\nLangChain\nComplete API reference for LangChain Python, including chat models, tools, agents, and more.\nLangGraph\nComplete API reference for LangGraph Python, including graph APIs, state management, checkpointing, and more.\nLangChain Integrations\nLangChain packages to connect with popular LLM providers, vector stores, tools, and other services.\nMCP Adapter\nUse Model Context Protocol (MCP) tools within LangChain and LangGraph applications.\nDeep Agents\nBuild agents that can plan, use subagents, and leverage file systems for complex tasks.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494604,
    "timestamp": "2026-02-25T08:13:51.278Z",
    "title": "Errors - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/common-errors",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nReference\nLangChain SDK\nLangGraph SDK\nIntegrations\nDeep Agents\nErrors\nReference\nReleases\nVersioning\nChangelog\nReleases\nMigration guides\nPolicies\nRelease policy\nSecurity\nErrors\nErrors\nCopy page\nThis page contains guides around resolving common errors you may find while building with LangChain and LangGraph.\nErrors referenced below will have an lc_error_code property corresponding to one of the below codes when they are thrown in code.\nError code\nGRAPH_RECURSION_LIMIT\nINVALID_CHAT_HISTORY\nINVALID_CONCURRENT_GRAPH_UPDATE\nINVALID_GRAPH_NODE_RETURN_VALUE\nINVALID_PROMPT_INPUT\nINVALID_TOOL_RESULTS\nMESSAGE_COERCION_FAILURE\nMISSING_CHECKPOINTER\nMODEL_AUTHENTICATION\nMODEL_NOT_FOUND\nMODEL_RATE_LIMIT\nMULTIPLE_SUBGRAPHS\nOUTPUT_PARSING_FAILURE\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494605,
    "timestamp": "2026-02-25T08:13:51.278Z",
    "title": "Versioning - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/versioning",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nReference\nLangChain SDK\nLangGraph SDK\nIntegrations\nDeep Agents\nErrors\nReference\nReleases\nVersioning\nChangelog\nReleases\nMigration guides\nPolicies\nRelease policy\nSecurity\nOn this page\nAPI stability\nStable APIs\nBeta APIs\nAlpha APIs\nDeprecated APIs\nInternal APIs\nRelease cycles\nVersion support policy\nLong-term support (LTS) releases\nPre-1.0 packages\nCheck your version\nUpgrade\nPre-release versions\nSee also\nReleases\nVersioning\nCopy page\nOur OSS version numbers follow the format: MAJOR.MINOR.PATCH, as defined by Semantic Versioning.\nMajor: Breaking API updates that require code changes.\nMinor: New features and improvements that maintain backward compatibility.\nPatch: Bug fixes and minor improvements.\nFor example:\n1.0.0: First stable release with production-ready APIs\n1.1.0: New features added in a backward-compatible manner\n1.0.1: Backward-compatible bug fixes\nï¿½\nAPI stability\nWe communicate the stability of our APIs as follows:\nï¿½\nStable APIs\nAll APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features and only introduce breaking changes in major releases.\nï¿½\nBeta APIs\nAPIs marked as beta are feature-complete but may undergo minor changes based on user feedback. They are safe for production use but may require small adjustments in future releases.\nï¿½\nAlpha APIs\nAPIs marked as alpha are experimental and subject to significant changes. Use these with caution in production environments.\nï¿½\nDeprecated APIs\nAPIs marked as deprecated will be removed in future major releases. When possible, we specify the intended version of removal. To handle deprecations:\nSwitch to the recommended alternative API\nFollow the migration guide (released alongside major releases)\nUse automated migration tools when available\nï¿½\nInternal APIs\nCertain APIs are explicitly marked as â€œinternalâ€ in a couple of ways:\nSome documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.\nFunctions, methods, and other objects prefixed by a leading underscore (_). This is the standard Python convention of indicating that something is private; if any method starts with a single _, itâ€™s an internal API.\nException: Certain methods are prefixed with _ , but do not contain an implementation. These methods are meant to be overridden by sub-classes that provide the implementation. Such methods are generally part of the Public API of LangChain.\nï¿½\nRelease cycles\n\nMajor releases\n\nMinor releases\n\nPatch releases\n\nï¿½\nVersion support policy\nLatest major version: Full support with active development (ACTIVE status)\nPrevious major version: Security updates and critical bug fixes for 12 months after the next major release (MAINTENANCE status)\nOlder versions: Community support only\nï¿½\nLong-term support (LTS) releases\nBoth LangChain and LangGraph 1.0 are designated as LTS releases:\nVersion 1.0 will remain in ACTIVE status until version 2.0 is released\nAfter version 2.0 is released, version 1.0 will enter MAINTENANCE mode for at least 1 year\nLTS releases follow semantic versioning (semver), allowing safe upgrades between minor versions\nLegacy versions (LangChain 0.3 and LangGraph 0.4) are in MAINTENANCE mode until December 2026\nï¿½\nPre-1.0 packages\nDeep Agents (deepagents) is a pre-1.0 package under active development. As a rapidly evolving package, the API may change between minor versions, though we minimize breaking changes when possible. Deep Agents will adopt the same LTS policies as LangChain and LangGraph after reaching version 1.0.\nFor detailed information about release status and support timelines, see the Release policy.\nï¿½\nCheck your version\nTo check your installed version:\nLangChain\nLangGraph\nCopy\nimport langchain_core\nprint(langchain_core.__version__)\n\nï¿½\nUpgrade\nLangChain\nLangGraph\nDeep Agents\nCopy\n# Upgrade to the latest version\npip install -U langchain-core langchain\n\n# Upgrade to a specific version\npip install langchain-core==1.0.0\n\nï¿½\nPre-release versions\nWe occasionally release alpha and beta versions for early testing:\nAlpha (e.g., 1.0.0a1): Early preview, significant changes expected\nBeta (e.g., 1.0.0b1): Feature-complete, minor changes possible\nRelease Candidate (e.g., 1.0.0rc1): Final testing before stable release\nï¿½\nSee also\nRelease policy - Detailed release and deprecation policies\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494606,
    "timestamp": "2026-02-25T08:13:51.291Z",
    "title": "Changelog - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/releases/changelog",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nReference\nLangChain SDK\nLangGraph SDK\nIntegrations\nDeep Agents\nErrors\nReference\nReleases\nVersioning\nChangelog\nReleases\nMigration guides\nPolicies\nRelease policy\nSecurity\nFilters\nlangchain\nintegrations\ndeepagents\nlanggraph\nReleases\nChangelog\nCopy page\n\nLog of updates and improvements to our Python packages\n\nSubscribe: Our changelog includes an RSS feed that can integrate with Slack, email, Discord bots like Readybot or RSS Feeds to Discord Bot, and other subscription tools.\nï¿½\nFeb 10, 2026\ndeepagents\nï¿½\ndeepagents v0.4\nNew integration packages for pluggable sandboxes: langchain-modal, langchain-daytona, and langchain-runloop. See sandboxes guide and example data analysis tutorial.\nChanges to conversation history summarization:\nSummarization now happens in the model node via wrap_model_call events. Due to this we retain the full message history in the graph state.\nMore accurate token counting.\nSummarization will now automatically trigger if a chat model raises a ContextOverflowError (defined in langchain-core). Currently langchain-anthropic and langchain-openai support this.\nWe now default to the Responses API for model strings prefixed with \"openai:\".\nNote: to disable data retention with the Responses API, use\nCopy\nfrom langchain.chat_models import init_chat_model\n\nagent = create_deep_agent(\n    model=init_chat_model(\n        \"openai:...\",\n        use_responses_api=True,\n        store=False,\n        include=[\"reasoning.encrypted_content\"],\n    )\n)\n\nï¿½\nDec 15, 2025\nlangchain\nintegrations\nï¿½\nlangchain v1.2.0\ncreate_agent: Simplified support for provider-specific tool parameters and definitions via a new extras attribute on tools. Examples:\nProvider-specific configuration such as Anthropicâ€™s programmatic tool calling and tool search.\nBuilt-in tools that are executed client-side, as supported by Anthropic, OpenAI, and other providers.\nSupport for strict schema-adherence in agent response_format (see ProviderStrategy docs).\nï¿½\nDec 8, 2025\nlangchain\nintegrations\nï¿½\nlangchain-google-genai v4.0.0\nWeâ€™ve re-written the Google GenAI integration to use Googleâ€™s consolidated Generative AI SDK, which provides access to the Gemini API and Vertex AI Platform under the same interface. This includes minimal breaking changes as well as deprecated packages in langchain-google-vertexai.\nSee the full release notes and migration guide for details.\nï¿½\nNov 25, 2025\nlangchain\nï¿½\nlangchain v1.1.0\nModel profiles: Chat models now expose supported features and capabilities through a .profile attribute. These data are derived from models.dev, an open source project providing model capability data.\nSummarization middleware: Updated to support flexible trigger points using model profiles for context-aware summarization.\nStructured output: ProviderStrategy support (native structured output) can now be inferred from model profiles.\nSystemMessage for create_agent: Support for passing SystemMessage instances directly to create_agentâ€™s system_prompt parameter, enabling advanced features like cache control and structured content blocks.\nModel retry middleware: New middleware for automatically retrying failed model calls with configurable exponential backoff.\nContent moderation middleware: OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results.\nï¿½\nOct 20, 2025\nlangchain\nlanggraph\nï¿½\nv1.0.0\nï¿½\nlangchain\nRelease notes\nMigration guide\nï¿½\nlanggraph\nRelease notes\nMigration guide\nIf you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content and API reference.\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494607,
    "timestamp": "2026-02-25T08:13:51.304Z",
    "title": "What's new in LangChain v1 - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/releases/langchain-v1",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nReference\nLangChain SDK\nLangGraph SDK\nIntegrations\nDeep Agents\nErrors\nReference\nReleases\nVersioning\nChangelog\nReleases\nLangChain v1\nLangGraph v1\nMigration guides\nPolicies\nRelease policy\nSecurity\nOn this page\ncreate_agent\nMiddleware\nPrebuilt middleware\nCustom middleware\nBuilt on LangGraph\nStructured output\nStandard content blocks\nBenefits\nSimplified package\nNamespace\nlangchain-classic\nMigration guide\nReporting issues\nAdditional resources\nSee also\nReleases\nReleases\nWhat's new in LangChain v1\nCopy page\nLangChain v1 is a focused, production-ready foundation for building agents. Weâ€™ve streamlined the framework around three core improvements:\ncreate_agent\nThe new standard for building agents in LangChain, replacing langgraph.prebuilt.create_react_agent.\nStandard content blocks\nA new content_blocks property that provides unified access to modern LLM features across providers.\nSimplified namespace\nThe langchain namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to langchain-classic.\nTo upgrade,\npip\nuv\nCopy\npip install -U langchain\n\nFor a complete list of changes, see the migration guide.\nï¿½\ncreate_agent\ncreate_agent is the standard way to build agents in LangChain 1.0. It provides a simpler interface than langgraph.prebuilt.create_react_agent while offering greater customization potential by using middleware.\nCopy\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[search_web, analyze_data, send_email],\n    system_prompt=\"You are a helpful research assistant.\"\n)\n\nresult = agent.invoke({\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Research AI safety trends\"}\n    ]\n})\n\nUnder the hood, create_agent is built on the basic agent loop â€” calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\nFor more information, see Agents.\nï¿½\nMiddleware\nMiddleware is the defining feature of create_agent. It offers a highly customizable entry-point, raising the ceiling for what you can build.\nGreat agents require context engineering: getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\nï¿½\nPrebuilt middleware\nLangChain provides a few prebuilt middlewares for common patterns, including:\nPIIMiddleware: Redact sensitive information before sending to the model\nSummarizationMiddleware: Condense conversation history when it gets too long\nHumanInTheLoopMiddleware: Require approval for sensitive tool calls\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    PIIMiddleware,\n    SummarizationMiddleware,\n    HumanInTheLoopMiddleware\n)\n\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[read_email, send_email],\n    middleware=[\n        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n        PIIMiddleware(\n            \"phone_number\",\n            detector=(\n                r\"(?:\\+?\\d{1,3}[\\s.-]?)?\"\n                r\"(?:\\(?\\d{2,4}\\)?[\\s.-]?)?\"\n                r\"\\d{3,4}[\\s.-]?\\d{4}\"\n\t\t\t),\n\t\t\tstrategy=\"block\"\n        ),\n        SummarizationMiddleware(\n            model=\"claude-sonnet-4-5-20250929\",\n            trigger={\"tokens\": 500}\n        ),\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                \"send_email\": {\n                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]\n                }\n            }\n        ),\n    ]\n)\n\nï¿½\nCustom middleware\nYou can also build custom middleware to fit your needs. Middleware exposes hooks at each step in an agentâ€™s execution:\nBuild custom middleware by implementing any of these hooks on a subclass of the AgentMiddleware class:\nHook\tWhen it runs\tUse cases\nbefore_agent\tBefore calling the agent\tLoad memory, validate input\nbefore_model\tBefore each LLM call\tUpdate prompts, trim messages\nwrap_model_call\tAround each LLM call\tIntercept and modify requests/responses\nwrap_tool_call\tAround each tool call\tIntercept and modify tool execution\nafter_model\tAfter each LLM response\tValidate output, apply guardrails\nafter_agent\tAfter agent completes\tSave results, cleanup\nExample custom middleware:\nCopy\nfrom dataclasses import dataclass\nfrom typing import Callable\n\nfrom langchain_openai import ChatOpenAI\n\nfrom langchain.agents.middleware import (\n    AgentMiddleware,\n    ModelRequest\n)\nfrom langchain.agents.middleware.types import ModelResponse\n\n@dataclass\nclass Context:\n    user_expertise: str = \"beginner\"\n\nclass ExpertiseBasedToolMiddleware(AgentMiddleware):\n    def wrap_model_call(\n        self,\n        request: ModelRequest,\n        handler: Callable[[ModelRequest], ModelResponse]\n    ) -> ModelResponse:\n        user_level = request.runtime.context.user_expertise\n\n        if user_level == \"expert\":\n            # More powerful model\n            model = ChatOpenAI(model=\"gpt-5\")\n            tools = [advanced_search, data_analysis]\n        else:\n            # Less powerful model\n            model = ChatOpenAI(model=\"gpt-5-nano\")\n            tools = [simple_search, basic_calculator]\n\n        return handler(request.override(model=model, tools=tools))\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[\n        simple_search,\n        advanced_search,\n        basic_calculator,\n        data_analysis\n    ],\n    middleware=[ExpertiseBasedToolMiddleware()],\n    context_schema=Context\n)\n\nSee all 45 lines\nFor more information, see the complete middleware guide.\nï¿½\nBuilt on LangGraph\nBecause create_agent is built on LangGraph, you automatically get built in support for long running and reliable agents via:\nPersistence\nConversations automatically persist across sessions with built-in checkpointing\nStreaming\nStream tokens, tool calls, and reasoning traces in real-time\nHuman-in-the-loop\nPause agent execution for human approval before sensitive actions\nTime travel\nRewind conversations to any point and explore alternate paths and prompts\nYou donâ€™t need to learn LangGraph to use these featuresâ€”they work out of the box.\nï¿½\nStructured output\ncreate_agent has improved structured output generation:\nMain loop integration: Structured output is now generated in the main loop instead of requiring an additional LLM call\nStructured output strategy: Models can choose between calling tools or using provider-side structured output generation\nCost reduction: Eliminates extra expense from additional LLM calls\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\nfrom pydantic import BaseModel\n\n\nclass Weather(BaseModel):\n    temperature: float\n    condition: str\n\ndef weather_tool(city: str) -> str:\n    \"\"\"Get the weather for a city.\"\"\"\n    return f\"it's sunny and 70 degrees in {city}\"\n\nagent = create_agent(\n    \"gpt-4.1-mini\",\n    tools=[weather_tool],\n    response_format=ToolStrategy(Weather)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}]\n})\n\nprint(repr(result[\"structured_response\"]))\n# results in `Weather(temperature=70.0, condition='sunny')`\n\nError handling: Control error handling via the handle_errors parameter to ToolStrategy:\nParsing errors: Model generates data that doesnâ€™t match desired structure\nMultiple tool calls: Model generates 2+ tool calls for structured output schemas\nï¿½\nStandard content blocks\nContent block support is currently only available for the following integrations:\nlangchain-anthropic\nlangchain-aws\nlangchain-openai\nlangchain-google-genai\nlangchain-ollama\nBroader support for content blocks will be rolled out gradually across more providers.\nThe new content_blocks property introduces a standard representation for message content that works across providers:\nCopy\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\nresponse = model.invoke(\"What's the capital of France?\")\n\n# Unified access to content blocks\nfor block in response.content_blocks:\n    if block[\"type\"] == \"reasoning\":\n        print(f\"Model reasoning: {block['reasoning']}\")\n    elif block[\"type\"] == \"text\":\n        print(f\"Response: {block['text']}\")\n    elif block[\"type\"] == \"tool_call\":\n        print(f\"Tool call: {block['name']}({block['args']})\")\n\nï¿½\nBenefits\nProvider agnostic: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider\nType safe: Full type hints for all content block types\nBackward compatible: Standard content can be loaded lazily, so there are no associated breaking changes\nFor more information, see our guide on content blocks.\nï¿½\nSimplified package\nLangChain v1 streamlines the langchain package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:\nï¿½\nNamespace\nModule\tWhatâ€™s available\tNotes\nlangchain.agents\tcreate_agent, AgentState\tCore agent creation functionality\nlangchain.messages\tMessage types, content blocks, trim_messages\tRe-exported from langchain-core\nlangchain.tools\t@tool, BaseTool, injection helpers\tRe-exported from langchain-core\nlangchain.chat_models\tinit_chat_model, BaseChatModel\tUnified model initialization\nlangchain.embeddings\tEmbeddings, init_embeddings\tEmbedding models\nMost of these are re-exported from langchain-core for convenience, which gives you a focused API surface for building agents.\nCopy\n# Agent building\nfrom langchain.agents import create_agent\n\n# Messages and content\nfrom langchain.messages import AIMessage, HumanMessage\n\n# Tools\nfrom langchain.tools import tool\n\n# Model initialization\nfrom langchain.chat_models import init_chat_model\nfrom langchain.embeddings import init_embeddings\n\nï¿½\nlangchain-classic\nLegacy functionality has moved to langchain-classic to keep the core packages lean and focused.\nWhatâ€™s in langchain-classic:\nLegacy chains and chain implementations\nRetrievers (e.g. MultiQueryRetriever or anything from the previous langchain.retrievers module)\nThe indexing API\nThe hub module (for managing prompts programmatically)\nlangchain-community exports\nOther deprecated functionality\nIf you use any of this functionality, install langchain-classic:\npip\nuv\nCopy\npip install langchain-classic\n\nThen update your imports:\nCopy\nfrom langchain import ...\nfrom langchain_classic import ...\n\nfrom langchain.chains import ...\nfrom langchain_classic.chains import ...\n\nfrom langchain.retrievers import ...\nfrom langchain_classic.retrievers import ...\n\nfrom langchain import hub  \nfrom langchain_classic import hub  \n\nï¿½\nMigration guide\nSee our migration guide for help updating your code to LangChain v1.\nï¿½\nReporting issues\nPlease report any issues discovered with 1.0 on GitHub using the 'v1' label.\nï¿½\nAdditional resources\nLangChain 1.0\nRead the announcement\nMiddleware guide\nDeep dive into middleware\nAgents Documentation\nFull agent documentation\nMessage Content\nNew content blocks API\nMigration guide\nHow to migrate to LangChain v1\nGitHub\nReport issues or contribute\nï¿½\nSee also\nVersioning â€“ Understanding version numbers\nRelease policy â€“ Detailed release policies\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494610,
    "timestamp": "2026-02-25T08:13:51.304Z",
    "title": "LangChain v1 migration guide - Docs by LangChain",
    "url": "https://docs.langchain.com/oss/python/migrate/langchain-v1",
    "text": "Skip to main content\nDocs by LangChain home page\nOpen source\nSearch...\nCtrl K\nAsk AI\nGitHub\nTry LangSmith\nDeep Agents\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContribute\n\nPython\n\nOverview\nReference\nLangChain SDK\nLangGraph SDK\nIntegrations\nDeep Agents\nErrors\nReference\nReleases\nVersioning\nChangelog\nReleases\nMigration guides\nLangChain v1\nLangGraph v1\nPolicies\nRelease policy\nSecurity\nOn this page\nSimplified package\nNamespace\nlangchain-classic\nMigrate to create_agent\nImport path\nPrompts\nStatic prompt rename\nSystemMessage to string\nDynamic prompts\nPre-model hook\nPost-model hook\nCustom state\nDefining state via state_schema\nDefining state via middleware\nState type restrictions\nModel\nDynamic model selection\nPre-bound models\nTools\nHandling tool errors\nStructured output\nNode changes\nTool and provider strategies\nPrompted output removed\nStreaming node name rename\nRuntime context\nStandard content\nWhat changed\nRead standardized content\nCreate multimodal messages\nExample block shapes\nSerialize standard content\nSimplified package\nNamespace\nlangchain-classic\nBreaking changes\nDropped Python 3.9 support\nUpdated return type for chat models\nDefault message format for OpenAI responses API\nDefault max_tokens in langchain-anthropic\nLegacy code moved to langchain-classic\nRemoval of deprecated APIs\nText property\nexample parameter removed from AIMessage\nMinor changes\nArchived docs\nReleases\nMigration guides\nLangChain v1 migration guide\nCopy page\nThis guide outlines the major changes between LangChain v1 and previous versions.\nï¿½\nSimplified package\nThe langchain package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.\nï¿½\nNamespace\nModule\tWhatâ€™s available\tNotes\nlangchain.agents\tcreate_agent, AgentState\tCore agent creation functionality\nlangchain.messages\tMessage types, content blocks, trim_messages\tRe-exported from langchain-core\nlangchain.tools\t@tool, BaseTool, injection helpers\tRe-exported from langchain-core\nlangchain.chat_models\tinit_chat_model, BaseChatModel\tUnified model initialization\nlangchain.embeddings\tinit_embeddings, Embeddings\tEmbedding models\nï¿½\nlangchain-classic\nIf you were using any of the following from the langchain package, youâ€™ll need to install langchain-classic and update your imports:\nLegacy chains (LLMChain, ConversationChain, etc.)\nRetrievers (e.g. MultiQueryRetriever or anything from the previous langchain.retrievers module)\nThe indexing API\nThe hub module (for managing prompts programmatically)\nEmbeddings modules (e.g. CacheBackedEmbeddings and community embeddings)\nlangchain-community re-exports\nOther deprecated functionality\nv1 (new)\nv0 (old)\nCopy\n# Chains\nfrom langchain_classic.chains import LLMChain\n\n# Retrievers\nfrom langchain_classic.retrievers import ...\n\n# Indexing\nfrom langchain_classic.indexes import ...\n\n# Hub\nfrom langchain_classic import hub\n\nInstall with:\npip\nuv\nCopy\npip install langchain-classic\n\nï¿½\nMigrate to create_agent\nPrior to v1.0, we recommended using langgraph.prebuilt.create_react_agent to build agents. Now, we recommend you use langchain.agents.create_agent to build agents.\nThe table below outlines what functionality has changed from create_react_agent to create_agent:\nSection\tTL;DR - Whatâ€™s changed\nImport path\tPackage moved from langgraph.prebuilt to langchain.agents\nPrompts\tParameter renamed to system_prompt, dynamic prompts use middleware\nPre-model hook\tReplaced by middleware with before_model method\nPost-model hook\tReplaced by middleware with after_model method\nCustom state\tTypedDict only, can be defined via state_schema or middleware\nModel\tDynamic selection via middleware, pre-bound models not supported\nTools\tTool error handling moved to middleware with wrap_tool_call\nStructured output\tprompted output removed, use ToolStrategy/ProviderStrategy\nStreaming node name\tNode name changed from \"agent\" to \"model\"\nRuntime context\tDependency injection via context argument instead of config[\"configurable\"]\nNamespace\tStreamlined to focus on agent building blocks, legacy code moved to langchain-classic\nï¿½\nImport path\nThe import path for the agent prebuilt has changed from langgraph.prebuilt to langchain.agents. The name of the function has changed from create_react_agent to create_agent:\nCopy\nfrom langgraph.prebuilt import create_react_agent \nfrom langchain.agents import create_agent \n\nFor more information, see Agents.\nï¿½\nPrompts\nï¿½\nStatic prompt rename\nThe prompt parameter has been renamed to system_prompt:\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[check_weather],\n    system_prompt=\"You are a helpful assistant\"\n)\n\nï¿½\nSystemMessage to string\nIf using SystemMessage objects in the system prompt, extract the string content:\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[check_weather],\n    system_prompt=\"You are a helpful assistant\"\n)\n\nï¿½\nDynamic prompts\nDynamic prompts are a core context engineering patternâ€” they adapt what you tell the model based on the current conversation state. To do this, use the @dynamic_prompt decorator:\nv1 (new)\nv0 (old)\nCopy\nfrom dataclasses import dataclass\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass Context:  \n    user_role: str = \"user\"\n\n@dynamic_prompt\ndef dynamic_prompt(request: ModelRequest) -> str:  \n    user_role = request.runtime.context.user_role\n    base_prompt = \"You are a helpful assistant.\"\n\n    if user_role == \"expert\":\n        prompt = (\n            f\"{base_prompt} Provide detailed technical responses.\"\n        )\n    elif user_role == \"beginner\":\n        prompt = (\n            f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n        )\n    else:\n        prompt = base_prompt\n\n    return prompt  \n\nagent = create_agent(\n    model=\"gpt-4.1\",\n    tools=tools,\n    middleware=[dynamic_prompt],  \n    context_schema=Context\n)\n\n# Use with context\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Explain async programming\"}]},\n    context=Context(user_role=\"expert\")\n)\n\nï¿½\nPre-model hook\nPre-model hooks are now implemented as middleware with the before_model method. This new pattern is more extensibleâ€”you can define multiple middlewares to run before the model is called, reusing common patterns across different agents.\nCommon use cases include:\nSummarizing conversation history\nTrimming messages\nInput guardrails, like PII redaction\nv1 now has summarization middleware as a built in option:\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=tools,\n    middleware=[\n        SummarizationMiddleware(  \n            model=\"claude-sonnet-4-5-20250929\",  \n            trigger={\"tokens\": 1000}  \n        )  \n    ]  \n)\n\nï¿½\nPost-model hook\nPost-model hooks are now implemented as middleware with the after_model method. This new pattern is more extensibleâ€”you can define multiple middlewares to run after the model is called, reusing common patterns across different agents.\nCommon use cases include:\nHuman in the loop\nOutput guardrails\nv1 has a built in middleware for human in the loop approval for tool calls:\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import HumanInTheLoopMiddleware\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[read_email, send_email],\n    middleware=[\n        HumanInTheLoopMiddleware(\n            interrupt_on={\n                \"send_email\": {\n                    \"description\": \"Please review this email before sending\",\n                    \"allowed_decisions\": [\"approve\", \"reject\"]\n                }\n            }\n        )\n    ]\n)\n\nï¿½\nCustom state\nCustom state extends the default agent state with additional fields. You can define custom state in two ways:\nVia state_schema on create_agent - Best for state used in tools\nVia middleware - Best for state managed by specific middleware hooks and tools attached to said middleware\nDefining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.\nstate_schema is still supported for backwards compatibility on create_agent.\nï¿½\nDefining state via state_schema\nUse the state_schema parameter when your custom state needs to be accessed by tools:\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.tools import tool, ToolRuntime\nfrom langchain.agents import create_agent, AgentState  \n\n\n# Define custom state extending AgentState\nclass CustomState(AgentState):\n    user_name: str\n\n@tool\ndef greet(\n    runtime: ToolRuntime[None, CustomState]\n) -> str:\n    \"\"\"Use this to greet the user by name.\"\"\"\n    user_name = runtime.state.get(\"user_name\", \"Unknown\")  \n    return f\"Hello {user_name}!\"\n\nagent = create_agent(  \n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[greet],\n    state_schema=CustomState  \n)\n\nï¿½\nDefining state via middleware\nMiddleware can also define custom state by setting the state_schema attribute. This helps to keep state extensions conceptually scoped to the relevant middleware and tools.\nCopy\nfrom langchain.agents.middleware import AgentState, AgentMiddleware\nfrom typing_extensions import NotRequired\nfrom typing import Any\n\nclass CustomState(AgentState):\n    model_call_count: NotRequired[int]\n\nclass CallCounterMiddleware(AgentMiddleware[CustomState]):\n    state_schema = CustomState  \n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        count = state.get(\"model_call_count\", 0)\n        if count > 10:\n            return {\"jump_to\": \"end\"}\n        return None\n\n    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[...],\n    middleware=[CallCounterMiddleware()]  \n)\n\nSee the middleware documentation for more details on defining custom state via middleware.\nï¿½\nState type restrictions\ncreate_agent only supports TypedDict for state schemas. Pydantic models and dataclasses are no longer supported.\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.agents import AgentState, create_agent\n\n# AgentState is a TypedDict\nclass CustomAgentState(AgentState):  \n    user_id: str\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=tools,\n    state_schema=CustomAgentState  \n)\n\nSimply inherit from langchain.agents.AgentState instead of BaseModel or decorating with dataclass. If you need to perform validation, handle it in middleware hooks instead.\nï¿½\nModel\nDynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences). create_react_agent released in v0.6 of langgraph-prebuilt supported dynamic model and tool selection via a callable passed to the model parameter.\nThis functionality has been ported to the middleware interface in v1.\nï¿½\nDynamic model selection\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import (\n    AgentMiddleware, ModelRequest\n)\nfrom langchain.agents.middleware.types import ModelResponse\nfrom langchain_openai import ChatOpenAI\nfrom typing import Callable\n\nbasic_model = ChatOpenAI(model=\"gpt-5-nano\")\nadvanced_model = ChatOpenAI(model=\"gpt-5\")\n\nclass DynamicModelMiddleware(AgentMiddleware):\n\n    def wrap_model_call(self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n        if len(request.state.messages) > self.messages_threshold:\n            model = advanced_model\n        else:\n            model = basic_model\n        return handler(request.override(model=model))\n\n    def __init__(self, messages_threshold: int) -> None:\n        self.messages_threshold = messages_threshold\n\nagent = create_agent(\n    model=basic_model,\n    tools=tools,\n    middleware=[DynamicModelMiddleware(messages_threshold=10)]\n)\n\nï¿½\nPre-bound models\nTo better support structured output, create_agent no longer accepts pre-bound models with tools or configuration:\nCopy\n# No longer supported\nmodel_with_tools = ChatOpenAI().bind_tools([some_tool])\nagent = create_agent(model_with_tools, tools=[])\n\n# Use instead\nagent = create_agent(\"gpt-4.1-mini\", tools=[some_tool])\n\nDynamic model functions can return pre-bound models if structured output is not used.\nï¿½\nTools\nThe tools argument to create_agent accepts a list of:\nLangChain BaseTool instances (functions decorated with @tool)\nCallable objects (functions) with proper type hints and a docstring\ndict that represents a built-in provider tools\nThe argument will no longer accept ToolNode instances.\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[check_weather, search_web]\n)\n\nï¿½\nHandling tool errors\nYou can now configure the handling of tool errors with middleware implementing the wrap_tool_call method.\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain.messages import ToolMessage\n\n\n@wrap_tool_call\ndef handle_tool_errors(request, handler):\n    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n    try:\n        return handler(request)\n    except Exception as e:\n        # Only handle errors that occur during tool execution due to invalid inputs\n        # that pass schema validation but fail at runtime (e.g., invalid SQL syntax).\n        # Do NOT handle:\n        # - Network failures (use tool retry middleware instead)\n        # - Incorrect tool implementation errors (should bubble up)\n        # - Schema mismatch errors (already auto-handled by the framework)\n        #\n        # Return a custom error message to the model\n        return ToolMessage(\n            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n            tool_call_id=request.tool_call[\"id\"]\n        )\n\nagent = create_agent(\n    model=\"claude-sonnet-4-5-20250929\",\n    tools=[check_weather, search_web],\n    middleware=[handle_tool_errors]\n)\n\nï¿½\nStructured output\nï¿½\nNode changes\nStructured output used to be generated in a separate node from the main agent. This is no longer the case. We generate structured output in the main loop, reducing cost and latency.\nï¿½\nTool and provider strategies\nIn v1, there are two new structured output strategies:\nToolStrategy uses artificial tool calling to generate structured output\nProviderStrategy uses provider-native structured output generation\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy, ProviderStrategy\nfrom pydantic import BaseModel\n\n\nclass OutputSchema(BaseModel):\n    summary: str\n    sentiment: str\n\n# Using ToolStrategy\nagent = create_agent(\n    model=\"gpt-4.1-mini\",\n    tools=tools,\n    # explicitly using tool strategy\n    response_format=ToolStrategy(OutputSchema)  \n)\n\nï¿½\nPrompted output removed\nPrompted output is no longer supported via the response_format argument. Compared to strategies like artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable.\nï¿½\nStreaming node name rename\nWhen streaming events from agents, the node name has changed from \"agent\" to \"model\" to better reflect the nodeâ€™s purpose.\nï¿½\nRuntime context\nWhen you invoke an agent, itâ€™s often the case that you want to pass two types of data:\nDynamic state that changes throughout the conversation (e.g., message history)\nStatic context that doesnâ€™t change during the conversation (e.g., user metadata)\nIn v1, static context is supported by setting the context parameter to invoke and stream.\nv1 (new)\nv0 (old)\nCopy\nfrom dataclasses import dataclass\n\nfrom langchain.agents import create_agent\n\n\n@dataclass\nclass Context:\n    user_id: str\n    session_id: str\n\nagent = create_agent(\n    model=model,\n    tools=tools,\n    context_schema=Context  \n)\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]},\n    context=Context(user_id=\"123\", session_id=\"abc\")  \n)\n\nThe old config[\"configurable\"] pattern still works for backward compatibility, but using the new context parameter is recommended for new applications or applications migrating to v1.\nï¿½\nStandard content\nIn v1, messages gain provider-agnostic standard content blocks. Access them via message.content_blocks for a consistent, typed view across providers. The existing message.content field remains unchanged for strings or provider-native structures.\nï¿½\nWhat changed\nNew content_blocks property on messages for normalized content\nStandardized block shapes, documented in Messages\nOptional serialization of standard blocks into content via LC_OUTPUT_VERSION=v1 or output_version=\"v1\"\nï¿½\nRead standardized content\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-5-nano\")\nresponse = model.invoke(\"Explain AI\")\n\nfor block in response.content_blocks:\n    if block[\"type\"] == \"reasoning\":\n        print(block.get(\"reasoning\"))\n    elif block[\"type\"] == \"text\":\n        print(block.get(\"text\"))\n\nï¿½\nCreate multimodal messages\nv1 (new)\nv0 (old)\nCopy\nfrom langchain.messages import HumanMessage\n\nmessage = HumanMessage(content_blocks=[\n    {\"type\": \"text\", \"text\": \"Describe this image.\"},\n    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n])\nres = model.invoke([message])\n\nï¿½\nExample block shapes\nCopy\n# Text block\ntext_block = {\n    \"type\": \"text\",\n    \"text\": \"Hello world\",\n}\n\n# Image block\nimage_block = {\n    \"type\": \"image\",\n    \"url\": \"https://example.com/image.png\",\n    \"mime_type\": \"image/png\",\n}\n\nSee the content blocks reference for more details.\nï¿½\nSerialize standard content\nStandard content blocks are not serialized into the content attribute by default. If you need to access standard content blocks in the content attribute (e.g., when sending messages to a client), you can opt-in to serializing them into content.\nEnvironment variable\nInitialization parameter\nCopy\nexport LC_OUTPUT_VERSION=v1\n\nLearn more: Messages, Standard content blocks, and Multimodal.\nï¿½\nSimplified package\nThe langchain package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.\nï¿½\nNamespace\nModule\tWhatâ€™s available\tNotes\nlangchain.agents\tcreate_agent, AgentState\tCore agent creation functionality\nlangchain.messages\tMessage types, content blocks, trim_messages\tRe-exported from langchain-core\nlangchain.tools\t@tool, BaseTool, injection helpers\tRe-exported from langchain-core\nlangchain.chat_models\tinit_chat_model, BaseChatModel\tUnified model initialization\nlangchain.embeddings\tinit_embeddings, Embeddings\tEmbedding models\nï¿½\nlangchain-classic\nIf you were using any of the following from the langchain package, youâ€™ll need to install langchain-classic and update your imports:\nLegacy chains (LLMChain, ConversationChain, etc.)\nRetrievers (e.g. MultiQueryRetriever or anything from the previous langchain.retrievers module)\nThe indexing API\nThe hub module (for managing prompts programmatically)\nEmbeddings modules (e.g. CacheBackedEmbeddings and community embeddings)\nlangchain-community re-exports\nOther deprecated functionality\nv1 (new)\nv0 (old)\nCopy\n# Chains\nfrom langchain_classic.chains import LLMChain\n\n# Retrievers\nfrom langchain_classic.retrievers import ...\n\n# Indexing\nfrom langchain_classic.indexes import ...\n\n# Hub\nfrom langchain_classic import hub\n\nInstallation:\nCopy\nuv pip install langchain-classic\n\nï¿½\nBreaking changes\nï¿½\nDropped Python 3.9 support\nAll LangChain packages now require Python 3.10 or higher. Python 3.9 reaches end of life in October 2025.\nï¿½\nUpdated return type for chat models\nThe return type signature for chat model invocation has been fixed from BaseMessage to AIMessage. Custom chat models implementing bind_tools should update their return signature:\nv1 (new)\nv0 (old)\nCopy\ndef bind_tools(\n        ...\n    ) -> Runnable[LanguageModelInput, AIMessage]:\n\nï¿½\nDefault message format for OpenAI responses API\nWhen interacting with the Responses API, langchain-openai now defaults to storing response items in message content. To restore previous behavior, set the LC_OUTPUT_VERSION environment variable to v0, or specify output_version=\"v0\" when instantiating ChatOpenAI.\nCopy\n# Enforce previous behavior with output_version flag\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\", output_version=\"v0\")\n\nï¿½\nDefault max_tokens in langchain-anthropic\nThe max_tokens parameter in langchain-anthropic now defaults to higher values based on the model chosen, rather than the previous default of 1024. If you relied on the old default, explicitly set max_tokens=1024.\nï¿½\nLegacy code moved to langchain-classic\nExisting functionality outside the focus of standard interfaces and agents has been moved to the langchain-classic package. See the Simplified namespace section for details on whatâ€™s available in the core langchain package and what moved to langchain-classic.\nï¿½\nRemoval of deprecated APIs\nMethods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the deprecation notices from previous versions for replacement APIs.\nï¿½\nText property\nUse of the .text() method on message objects should drop the parentheses, as it is now a property:\nCopy\n# Property access\ntext = response.text\n\n# Deprecated method call\ntext = response.text()\n\nExisting usage patterns (i.e., .text()) will continue to function but now emit a warning. The method form will be removed in v2.\nï¿½\nexample parameter removed from AIMessage\nThe example parameter has been removed from AIMessage objects. We recommend migrating to use additional_kwargs for passing extra metadata as needed.\nï¿½\nMinor changes\nAIMessageChunk objects now include a chunk_position attribute with position 'last' to indicate the final chunk in a stream. This allows for clearer handling of streamed messages. If the chunk is not the final one, chunk_position will be None.\nLanguageModelOutputVar is now typed to AIMessage instead of BaseMessage.\nThe logic for merging message chunks (AIMessageChunk.add) has been updated with more sophisticated selection handling for the final id for the merged chunk. It prioritizes provider-assigned IDs over LangChain-generated IDs.\nWe now open files with utf-8 encoding by default.\nStandard tests now use multimodal content blocks.\nï¿½\nArchived docs\nOld docs are archived for reference:\nv0.3 docs content\nv0.3 API reference\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.\n\nWas this page helpful?\n\nYes\nNo\nDocs by LangChain home page\n\nResources\n\nForum\nChangelog\nLangChain Academy\nTrust Center\n\nCompany\n\nHome\nAbout\nCareers\nBlog\ngithub\nx\nlinkedin\nyoutube\nPowered by"
  },
  {
    "id": 60494621,
    "timestamp": "2026-02-25T08:15:29.529Z",
    "title": "LangChain Full Crash Course - AI Agents in Python",
    "url": "https://www.youtube.com/watch?v=J7j5tCB_y4w&t=70s",
    "text": "Today we're going to do a crash course\nToday we're going to do a crash course on Langchain, the Python framework for\non Langchain, the Python framework for\non Langchain, the Python framework for working with and building AI agents. It\nworking with and building AI agents. It\nworking with and building AI agents. It makes it super simple to interact with\nmakes it super simple to interact with\nmakes it super simple to interact with standalone models, build complex agents,\nstandalone models, build complex agents,\nstandalone models, build complex agents, and integrate all sorts of other\nand integrate all sorts of other\nand integrate all sorts of other components like embedding models or\ncomponents like embedding models or\ncomponents like embedding models or vector stores. And all of this without\nvector stores. And all of this without\nvector stores. And all of this without caring about specific differences in API\ncaring about specific differences in API\ncaring about specific differences in API definitions of the various providers.\ndefinitions of the various providers.\ndefinitions of the various providers. The goal of this video today is to cover\nThe goal of this video today is to cover\nThe goal of this video today is to cover as much as possible in a short amount of\nas much as possible in a short amount of\nas much as possible in a short amount of time. We're going to start by talking\ntime. We're going to start by talking\ntime. We're going to start by talking about Langchain and its ecosystem. Then\nabout Langchain and its ecosystem. Then\nabout Langchain and its ecosystem. Then we're going to take a look at a couple\nwe're going to take a look at a couple\nwe're going to take a look at a couple of simple examples for building agents\nof simple examples for building agents\nof simple examples for building agents and working with standalone models.\nand working with standalone models.\nand working with standalone models. We'll learn how to work with message\nWe'll learn how to work with message\nWe'll learn how to work with message histories, stream responses, use tools,\nhistories, stream responses, use tools,\nhistories, stream responses, use tools, generate structured output, handle\ngenerate structured output, handle\ngenerate structured output, handle multimodal input, pass context, and keep\nmultimodal input, pass context, and keep\nmultimodal input, pass context, and keep track of memory. After that, we'll also\ntrack of memory. After that, we'll also\ntrack of memory. After that, we'll also build a simple rag example. And finally,\nbuild a simple rag example. And finally,\nbuild a simple rag example. And finally, we're going to take a look at\nwe're going to take a look at\nwe're going to take a look at Langchain's powerful middleware, as well\nLangchain's powerful middleware, as well\nLangchain's powerful middleware, as well as some interesting use cases for it.\nas some interesting use cases for it.\nas some interesting use cases for it. That's quite a few things to cover, and\nThat's quite a few things to cover, and\nThat's quite a few things to cover, and I think you can learn a lot today,\nI think you can learn a lot today,\nI think you can learn a lot today, especially since we're going to work\nespecially since we're going to work\nespecially since we're going to work with the latest version of Langchain\nwith the latest version of Langchain\nwith the latest version of Langchain 1.0. If you like this video, let me know\n1.0. If you like this video, let me know\n1.0. If you like this video, let me know by hitting the like button and\nby hitting the like button and\nby hitting the like button and subscribing. But now, let us get right\nsubscribing. But now, let us get right\nsubscribing. But now, let us get right into it.\n&gt;&gt; All right. So, we're going to cover\n&gt;&gt; All right. So, we're going to cover quite a lot today, which is why I'm\nquite a lot today, which is why I'm\nquite a lot today, which is why I'm going to try to speed this up to cover\ngoing to try to speed this up to cover\ngoing to try to speed this up to cover as much as possible in a concise way so\nas much as possible in a concise way so\nas much as possible in a concise way so we don't waste too much time here. I\nwe don't waste too much time here. I\nwe don't waste too much time here. I want to start by just briefly talking\nwant to start by just briefly talking\nwant to start by just briefly talking one or two minutes about Langchain, the\none or two minutes about Langchain, the\none or two minutes about Langchain, the Langchain ecosystem, and also its\nLangchain ecosystem, and also its\nLangchain ecosystem, and also its development history. Now, langchain\ndevelopment history. Now, langchain\ndevelopment history. Now, langchain itself as I mentioned is a Python\nitself as I mentioned is a Python\nitself as I mentioned is a Python framework for building and working with\nframework for building and working with\nframework for building and working with AI agents. The main use case or the main\nAI agents. The main use case or the main\nAI agents. The main use case or the main benefit of using Langchain is that we\nbenefit of using Langchain is that we\nbenefit of using Langchain is that we can use all these models and related\ncan use all these models and related\ncan use all these models and related tools in an abstract way. So, regardless\ntools in an abstract way. So, regardless\ntools in an abstract way. So, regardless of the provider, regardless of whether\nof the provider, regardless of whether\nof the provider, regardless of whether you're using OpenAI or Enthropic or\nyou're using OpenAI or Enthropic or\nyou're using OpenAI or Enthropic or Google, you basically have the same\nGoogle, you basically have the same\nGoogle, you basically have the same classes and methods everywhere. And if\nclasses and methods everywhere. And if\nclasses and methods everywhere. And if you have a system built on langchain\nyou have a system built on langchain\nyou have a system built on langchain that uses vector stores, embedding\nthat uses vector stores, embedding\nthat uses vector stores, embedding models, uh AI models, agents, whatever,\nmodels, uh AI models, agents, whatever,\nmodels, uh AI models, agents, whatever, you can easily just swap out the\nyou can easily just swap out the\nyou can easily just swap out the underlying technology and you can keep\nunderlying technology and you can keep\nunderlying technology and you can keep the code the same for the most part. So\nthe code the same for the most part. So\nthe code the same for the most part. So that's what I would consider the main\nthat's what I would consider the main\nthat's what I would consider the main selling point of Langchain that you have\nselling point of Langchain that you have\nselling point of Langchain that you have this abstract highlevel way of working\nthis abstract highlevel way of working\nthis abstract highlevel way of working with agents and related tooling. Now in\nwith agents and related tooling. Now in\nwith agents and related tooling. Now in addition to lang, we also have lang\naddition to lang, we also have lang\naddition to lang, we also have lang graph which is more low-level. We have\ngraph which is more low-level. We have\ngraph which is more low-level. We have more granular control. We can build\nmore granular control. We can build\nmore granular control. We can build complex graph-based event-driven agents\ncomplex graph-based event-driven agents\ncomplex graph-based event-driven agents or agentic systems. We're not going to\nor agentic systems. We're not going to\nor agentic systems. We're not going to cover Langraph in this video today. I do\ncover Langraph in this video today. I do\ncover Langraph in this video today. I do have videos on this channel where we use\nhave videos on this channel where we use\nhave videos on this channel where we use Langraph, but it's not going to be the\nLangraph, but it's not going to be the\nLangraph, but it's not going to be the topic of this crash course. And finally,\ntopic of this crash course. And finally,\ntopic of this crash course. And finally, we also have Lang Smith, which is like a\nwe also have Lang Smith, which is like a\nwe also have Lang Smith, which is like a collection of tools for observing,\ncollection of tools for observing,\ncollection of tools for observing, evaluating, monitoring, and deploying\nevaluating, monitoring, and deploying\nevaluating, monitoring, and deploying models. We're not going to cover that\nmodels. We're not going to cover that\nmodels. We're not going to cover that either. We're going to focus fully on\neither. We're going to focus fully on\neither. We're going to focus fully on Langchain today. Speaking of which,\nLangchain today. Speaking of which,\nLangchain today. Speaking of which, Langchain is now on version 1.0. So,\nLangchain is now on version 1.0. So,\nLangchain is now on version 1.0. So, some of you guys watching this might\nsome of you guys watching this might\nsome of you guys watching this might have already worked with Langchain in\nhave already worked with Langchain in\nhave already worked with Langchain in the past. Maybe you're already familiar\nthe past. Maybe you're already familiar\nthe past. Maybe you're already familiar with the package to some degree. But if\nwith the package to some degree. But if\nwith the package to some degree. But if you take a look at 1.0, you will notice\nyou take a look at 1.0, you will notice\nyou take a look at 1.0, you will notice that some things have changed. So, when\nthat some things have changed. So, when\nthat some things have changed. So, when I first used Langchain, the structure\nI first used Langchain, the structure\nI first used Langchain, the structure was a little bit different. We had\nwas a little bit different. We had\nwas a little bit different. We had Langchain, then we had Langchain core,\nLangchain, then we had Langchain core,\nLangchain, then we had Langchain core, and langchain community. And then you\nand langchain community. And then you\nand langchain community. And then you also had langai, langchain anthropic as\nalso had langai, langchain anthropic as\nalso had langai, langchain anthropic as separate import packages. So you would\nseparate import packages. So you would\nseparate import packages. So you would do something like from langchain core\ndo something like from langchain core\ndo something like from langchain core import something from langchain openai\nimport something from langchain openai\nimport something from langchain openai import something. Now with 1.0 it seems\nimport something. Now with 1.0 it seems\nimport something. Now with 1.0 it seems like if you look in the documentation\nlike if you look in the documentation\nlike if you look in the documentation everything is going through the main\neverything is going through the main\neverything is going through the main langchain package. So from langchain do\nlangchain package. So from langchain do\nlangchain package. So from langchain do something we import stuff. The second\nsomething we import stuff. The second\nsomething we import stuff. The second thing you will notice is that langchain\nthing you will notice is that langchain\nthing you will notice is that langchain is now much more focused on being a\nis now much more focused on being a\nis now much more focused on being a library for agents. Not just for\nlibrary for agents. Not just for\nlibrary for agents. Not just for integrating tools not just for using\nintegrating tools not just for using\nintegrating tools not just for using models and vector stores and all that\nmodels and vector stores and all that\nmodels and vector stores and all that but actually for building agents. Which\nbut actually for building agents. Which\nbut actually for building agents. Which means that we now have here langchain\nmeans that we now have here langchain\nmeans that we now have here langchain agents with create agent and in the past\nagents with create agent and in the past\nagents with create agent and in the past in my opinion it was more like langchain\nin my opinion it was more like langchain\nin my opinion it was more like langchain is this general toolkit for working with\nis this general toolkit for working with\nis this general toolkit for working with these tools and lang graph was more\nthese tools and lang graph was more\nthese tools and lang graph was more about the agent side. So we can still\nabout the agent side. So we can still\nabout the agent side. So we can still use the packages like langchain core and\nuse the packages like langchain core and\nuse the packages like langchain core and langchain community but for the most\nlangchain community but for the most\nlangchain community but for the most part it's enough to just go with the\npart it's enough to just go with the\npart it's enough to just go with the main lang package that is the modern way\nmain lang package that is the modern way\nmain lang package that is the modern way of working with this framework. Cool. So\nof working with this framework. Cool. So\nof working with this framework. Cool. So now that we covered that let us go ahead\nnow that we covered that let us go ahead\nnow that we covered that let us go ahead and set up our environment. In my case\nand set up our environment. In my case\nand set up our environment. In my case I'm going to use uv which is a rustbased\nI'm going to use uv which is a rustbased\nI'm going to use uv which is a rustbased python package manager. Of course, feel\npython package manager. Of course, feel\npython package manager. Of course, feel free to use pip or pip 3 install. Feel\nfree to use pip or pip 3 install. Feel\nfree to use pip or pip 3 install. Feel free to use virtual environments with\nfree to use virtual environments with\nfree to use virtual environments with virtual env or ven, whatever you want to\nvirtual env or ven, whatever you want to\nvirtual env or ven, whatever you want to choose. In my case, I'm going to go to\nchoose. In my case, I'm going to go to\nchoose. In my case, I'm going to go to my tutorial directory, say UV init, and\nmy tutorial directory, say UV init, and\nmy tutorial directory, say UV init, and then I'm also going to say UV at\nthen I'm also going to say UV at\nthen I'm also going to say UV at langchain. Now, one thing that's\nlangchain. Now, one thing that's\nlangchain. Now, one thing that's important is if you want to use lang\nimportant is if you want to use lang\nimportant is if you want to use lang chain together with some AI provider,\nchain together with some AI provider,\nchain together with some AI provider, which you usually want to do, you need\nwhich you usually want to do, you need\nwhich you usually want to do, you need to also provide it in square brackets.\nto also provide it in square brackets.\nto also provide it in square brackets. So for example, if I want to install\nSo for example, if I want to install\nSo for example, if I want to install lang chain with the openai dependencies,\nlang chain with the openai dependencies,\nlang chain with the openai dependencies, what I will do is I will say open AI in\nwhat I will do is I will say open AI in\nwhat I will do is I will say open AI in square brackets here. So in my case, UV\nsquare brackets here. So in my case, UV\nsquare brackets here. So in my case, UV add langchain openai. If you use pip,\nadd langchain openai. If you use pip,\nadd langchain openai. If you use pip, you do pip or pip 3 install langchain\nyou do pip or pip 3 install langchain\nyou do pip or pip 3 install langchain openai. And this will also install\nopenai. And this will also install\nopenai. And this will also install packages like of course openai. And you\npackages like of course openai. And you\npackages like of course openai. And you have to do that basically for all the\nhave to do that basically for all the\nhave to do that basically for all the providers that you want to use. So you\nproviders that you want to use. So you\nproviders that you want to use. So you want to do that for enthropic, you want\nwant to do that for enthropic, you want\nwant to do that for enthropic, you want to do that for mistral AI. I'm going to\nto do that for mistral AI. I'm going to\nto do that for mistral AI. I'm going to actually just do it now to show you how\nactually just do it now to show you how\nactually just do it now to show you how this works. Mistral AI and you also want\nthis works. Mistral AI and you also want\nthis works. Mistral AI and you also want to do this for Google genai if you plan\nto do this for Google genai if you plan\nto do this for Google genai if you plan to use all these models. Now what we're\nto use all these models. Now what we're\nto use all these models. Now what we're also going to need is API keys. If you\nalso going to need is API keys. If you\nalso going to need is API keys. If you want to use models from providers of\nwant to use models from providers of\nwant to use models from providers of course you have to authenticate\ncourse you have to authenticate\ncourse you have to authenticate yourself. So we're going to create a\nyourself. So we're going to create a\nyourself. So we're going to create a file called and this file will contain\nfile called and this file will contain\nfile called and this file will contain our keys. So these keys will be the\nour keys. So these keys will be the\nour keys. So these keys will be the open_appi_key.\nopen_appi_key.\nopen_appi_key. It's going to also be the Mistral API\nIt's going to also be the Mistral API\nIt's going to also be the Mistral API key, the Anthropic\nkey, the Anthropic\nkey, the Anthropic API key, and I'm not sure what the\nAPI key, and I'm not sure what the\nAPI key, and I'm not sure what the correct name is for the Google one, but\ncorrect name is for the Google one, but\ncorrect name is for the Google one, but I'm just going to say Google API key,\nI'm just going to say Google API key,\nI'm just going to say Google API key, even though that's probably not the\neven though that's probably not the\neven though that's probably not the correct one. So, what you want to do is\ncorrect one. So, what you want to do is\ncorrect one. So, what you want to do is you want to go to the providers if you\nyou want to go to the providers if you\nyou want to go to the providers if you have accounts there if you have API keys\nhave accounts there if you have API keys\nhave accounts there if you have API keys and you want to paste them here. So, I'm\nand you want to paste them here. So, I'm\nand you want to paste them here. So, I'm going to go to OpenAI console to Mistral\ngoing to go to OpenAI console to Mistral\ngoing to go to OpenAI console to Mistral Console, Enthropic Console, and to my\nConsole, Enthropic Console, and to my\nConsole, Enthropic Console, and to my Google Cloud Platform. There I'm going\nGoogle Cloud Platform. There I'm going\nGoogle Cloud Platform. There I'm going to take the API keys, copy paste them\nto take the API keys, copy paste them\nto take the API keys, copy paste them here. I'm not going to show how to\nhere. I'm not going to show how to\nhere. I'm not going to show how to obtain them or maybe I'm going to show\nobtain them or maybe I'm going to show\nobtain them or maybe I'm going to show them for one of them. So yeah, here for\nthem for one of them. So yeah, here for\nthem for one of them. So yeah, here for example, we go to\nexample, we go to\nexample, we go to platform.openai.com/api\nplatform.openai.com/api\nplatform.openai.com/api keys or you just go to API keys here on\nkeys or you just go to API keys here on\nkeys or you just go to API keys here on the left and here you can create a new\nthe left and here you can create a new\nthe left and here you can create a new secret key or API key. You do that for\nsecret key or API key. You do that for\nsecret key or API key. You do that for the various providers. Then you just\nthe various providers. Then you just\nthe various providers. Then you just copy paste the keys here like this or\ncopy paste the keys here like this or\ncopy paste the keys here like this or you use quotation marks if you want to\nyou use quotation marks if you want to\nyou use quotation marks if you want to and that is our end file. So in my case\nand that is our end file. So in my case\nand that is our end file. So in my case I already did this and now I'm going to\nI already did this and now I'm going to\nI already did this and now I'm going to install an additional package called\ninstall an additional package called\ninstall an additional package called python-.end.\npython-.end.\npython-.end. This is going to allow us to load these\nThis is going to allow us to load these\nThis is going to allow us to load these API keys into our Python script. Now, to\nAPI keys into our Python script. Now, to\nAPI keys into our Python script. Now, to get autocomp completion, I'm also going\nget autocomp completion, I'm also going\nget autocomp completion, I'm also going to activate this environment here. This\nto activate this environment here. This\nto activate this environment here. This is something I just have to do for Neoim\nis something I just have to do for Neoim\nis something I just have to do for Neoim to get autocomp completion. So, you can\nto get autocomp completion. So, you can\nto get autocomp completion. So, you can ignore that if you're not coding in the\nignore that if you're not coding in the\nignore that if you're not coding in the terminal. And then we're going to go\nterminal. And then we're going to go\nterminal. And then we're going to go into our main py file and get started\ninto our main py file and get started\ninto our main py file and get started with a first simple example. So, instead\nwith a first simple example. So, instead\nwith a first simple example. So, instead of just covering the concepts one by\nof just covering the concepts one by\nof just covering the concepts one by one, I'm going to show you how to build\none, I'm going to show you how to build\none, I'm going to show you how to build a simple agent. And we're going to cover\na simple agent. And we're going to cover\na simple agent. And we're going to cover a couple of concepts while we're doing\na couple of concepts while we're doing\na couple of concepts while we're doing that. So, we're going to start with the\nthat. So, we're going to start with the\nthat. So, we're going to start with the import. Let's import requests. Then also\nimport. Let's import requests. Then also\nimport. Let's import requests. Then also as I said from we're going to import\nas I said from we're going to import\nas I said from we're going to import load.n this is for loading the API keys\nload.n this is for loading the API keys\nload.n this is for loading the API keys into the environment. And then from\ninto the environment. And then from\ninto the environment. And then from langchain\nlangchain\nlangchain agents as we saw already in the docs\nagents as we saw already in the docs\nagents as we saw already in the docs we're going to import the create agent\nwe're going to import the create agent\nwe're going to import the create agent function. This is like the central\nfunction. This is like the central\nfunction. This is like the central function that is going to create agents\nfunction that is going to create agents\nfunction that is going to create agents that we then use to do stuff. And in\nthat we then use to do stuff. And in\nthat we then use to do stuff. And in this function we can do a lot of things.\nthis function we can do a lot of things.\nthis function we can do a lot of things. So we can provide middleware, we can\nSo we can provide middleware, we can\nSo we can provide middleware, we can provide tools, we can do a lot of things\nprovide tools, we can do a lot of things\nprovide tools, we can do a lot of things in that function or with that function.\nin that function or with that function.\nin that function or with that function. In addition to that, I'm also going to\nIn addition to that, I'm also going to\nIn addition to that, I'm also going to say now from langchain.tools\nsay now from langchain.tools\nsay now from langchain.tools import tool. This is a decorator that we\nimport tool. This is a decorator that we\nimport tool. This is a decorator that we can use to annotate a function to\ncan use to annotate a function to\ncan use to annotate a function to basically make that function a tool. We\nbasically make that function a tool. We\nbasically make that function a tool. We can also provide some uh information\ncan also provide some uh information\ncan also provide some uh information like a description or the name of the\nlike a description or the name of the\nlike a description or the name of the function. So for example, we can say add\nfunction. So for example, we can say add\nfunction. So for example, we can say add tool and I want to define a tool that is\ntool and I want to define a tool that is\ntool and I want to define a tool that is called get weather. This is going to\ncalled get weather. This is going to\ncalled get weather. This is going to obviously get the weather and I can\nobviously get the weather and I can\nobviously get the weather and I can provide a description here for the agent\nprovide a description here for the agent\nprovide a description here for the agent to know what this function is about or\nto know what this function is about or\nto know what this function is about or what this tool can do. In my case, I'm\nwhat this tool can do. In my case, I'm\nwhat this tool can do. In my case, I'm going to say here return weather\ngoing to say here return weather\ngoing to say here return weather information for a given city.\ninformation for a given city.\ninformation for a given city. Theoretically, you can also say return\nTheoretically, you can also say return\nTheoretically, you can also say return direct is equal to true if you want to\ndirect is equal to true if you want to\ndirect is equal to true if you want to have just the output of that tool\nhave just the output of that tool\nhave just the output of that tool immediately returned to the user. In\nimmediately returned to the user. In\nimmediately returned to the user. In this case, set this to true. We can also\nthis case, set this to true. We can also\nthis case, set this to true. We can also explicitly set it to false. I think it's\nexplicitly set it to false. I think it's\nexplicitly set it to false. I think it's the default, but just so we know it\nthe default, but just so we know it\nthe default, but just so we know it exists, we're going to set it to false.\nexists, we're going to set it to false.\nexists, we're going to set it to false. And then we're going to say now the\nAnd then we're going to say now the\nAnd then we're going to say now the function is get weather. It's going to\nfunction is get weather. It's going to\nfunction is get weather. It's going to get a city, which is going to be a\nget a city, which is going to be a\nget a city, which is going to be a string as input. And we're going to make\nstring as input. And we're going to make\nstring as input. And we're going to make it very simple here. Here I don't want\nit very simple here. Here I don't want\nit very simple here. Here I don't want to use a mock function. So I'm actually\nto use a mock function. So I'm actually\nto use a mock function. So I'm actually going to ping a weather API. But this\ngoing to ping a weather API. But this\ngoing to ping a weather API. But this one is actually open source and free to\none is actually open source and free to\none is actually open source and free to use or maybe not open source, but you\nuse or maybe not open source, but you\nuse or maybe not open source, but you don't need an account to use it. So\ndon't need an account to use it. So\ndon't need an account to use it. So we're going to say here response is\nwe're going to say here response is\nwe're going to say here response is equal to requests.get.\nequal to requests.get.\nequal to requests.get. And we're going to use https/wtr.in/\ncity question mark format equals j1. And\ncity question mark format equals j1. And of course this needs to be an fstring\nof course this needs to be an fstring\nof course this needs to be an fstring otherwise we cannot parse or we cannot\notherwise we cannot parse or we cannot\notherwise we cannot parse or we cannot format the city into the string here.\nformat the city into the string here.\nformat the city into the string here. But this basically gives us a JSON\nBut this basically gives us a JSON\nBut this basically gives us a JSON object with temperature and the weather\nobject with temperature and the weather\nobject with temperature and the weather information for a given city. And all we\ninformation for a given city. And all we\ninformation for a given city. And all we want to do here is we want to return the\nwant to do here is we want to return the\nwant to do here is we want to return the JSON object that we get here. So return\nJSON object that we get here. So return\nJSON object that we get here. So return response.json and then the model the\nresponse.json and then the model the\nresponse.json and then the model the agent can do whatever it wants with that\nagent can do whatever it wants with that\nagent can do whatever it wants with that information or whatever it thinks is the\ninformation or whatever it thinks is the\ninformation or whatever it thinks is the most reasonable thing to do. So to keep\nmost reasonable thing to do. So to keep\nmost reasonable thing to do. So to keep it simple we're going to have just this\nit simple we're going to have just this\nit simple we're going to have just this one tool. And now we're going to say\none tool. And now we're going to say\none tool. And now we're going to say agent is equal to create agent. And the\nagent is equal to create agent. And the\nagent is equal to create agent. And the first thing we want to do here is we\nfirst thing we want to do here is we\nfirst thing we want to do here is we want to provide the model. Now the model\nwant to provide the model. Now the model\nwant to provide the model. Now the model can just be a string. For example, I can\ncan just be a string. For example, I can\ncan just be a string. For example, I can say GPT-40.\nsay GPT-40.\nsay GPT-40. Now 40 doesn't support tool use. So\nNow 40 doesn't support tool use. So\nNow 40 doesn't support tool use. So we're not going to use that. But I can\nwe're not going to use that. But I can\nwe're not going to use that. But I can go with 4.1 mini. For example, this\ngo with 4.1 mini. For example, this\ngo with 4.1 mini. For example, this would be open AI. It is automatically\nwould be open AI. It is automatically\nwould be open AI. It is automatically recognized as OpenAI. So if I don't have\nrecognized as OpenAI. So if I don't have\nrecognized as OpenAI. So if I don't have Langchain OpenAI installed, it's not\nLangchain OpenAI installed, it's not\nLangchain OpenAI installed, it's not going to work. So for this, a couple of\ngoing to work. So for this, a couple of\ngoing to work. So for this, a couple of things need to be present. One is an\nthings need to be present. One is an\nthings need to be present. One is an OpenAI API key in the environment. And\nOpenAI API key in the environment. And\nOpenAI API key in the environment. And the other thing is langchain open AAI\nthe other thing is langchain open AAI\nthe other thing is langchain open AAI needs to be installed. So these two\nneeds to be installed. So these two\nneeds to be installed. So these two things need to be given so that we can\nthings need to be given so that we can\nthings need to be given so that we can actually use this model. After that I\nactually use this model. After that I\nactually use this model. After that I can just say give me a list of tools and\ncan just say give me a list of tools and\ncan just say give me a list of tools and this list will contain just get weather\nthis list will contain just get weather\nthis list will contain just get weather as an entity. So we're not calling the\nas an entity. So we're not calling the\nas an entity. So we're not calling the function we're passing it. And then we\nfunction we're passing it. And then we\nfunction we're passing it. And then we can provide a system prompt like you are\ncan provide a system prompt like you are\ncan provide a system prompt like you are a helpful weather assistant who always\na helpful weather assistant who always\na helpful weather assistant who always cracks jokes and is humorous while\ncracks jokes and is humorous while\ncracks jokes and is humorous while remaining\nremaining\nremaining helpful. So just to give you an example\nhelpful. So just to give you an example\nhelpful. So just to give you an example so that you can see that the system\nso that you can see that the system\nso that you can see that the system prompt actually affects the agent and\nprompt actually affects the agent and\nprompt actually affects the agent and then in order to get something from this\nthen in order to get something from this\nthen in order to get something from this agent in order to send something to the\nagent in order to send something to the\nagent in order to send something to the agent I should say we can just invoke\nagent I should say we can just invoke\nagent I should say we can just invoke it. So I can say agent.invoke invoke and\nit. So I can say agent.invoke invoke and\nit. So I can say agent.invoke invoke and here what we do is we pass a dictionary\nhere what we do is we pass a dictionary\nhere what we do is we pass a dictionary that contains a field messages and this\nthat contains a field messages and this\nthat contains a field messages and this messages has to point so this key\nmessages has to point so this key\nmessages has to point so this key messages has to point to the value which\nmessages has to point to the value which\nmessages has to point to the value which is a list and this list will contain all\nis a list and this list will contain all\nis a list and this list will contain all the messages as dictionaries with ro and\nthe messages as dictionaries with ro and\nthe messages as dictionaries with ro and content as we know it from the typical\ncontent as we know it from the typical\ncontent as we know it from the typical API. So ro is going to be user and\nAPI. So ro is going to be user and\nAPI. So ro is going to be user and content is going to be whatever we want\ncontent is going to be whatever we want\ncontent is going to be whatever we want to ask for. For example, what is the\nto ask for. For example, what is the\nto ask for. For example, what is the weather like in Vienna? Question mark.\nweather like in Vienna? Question mark.\nweather like in Vienna? Question mark. So I need to then save this as a\nSo I need to then save this as a\nSo I need to then save this as a response. And down here I can then print\nresponse. And down here I can then print\nresponse. And down here I can then print the entire response. So print response\nthe entire response. So print response\nthe entire response. So print response or I can say if I want to get just the\nor I can say if I want to get just the\nor I can say if I want to get just the message, I can get from response the\nmessage, I can get from response the\nmessage, I can get from response the messages. Maybe let me do that here with\nmessages. Maybe let me do that here with\nmessages. Maybe let me do that here with square brackets and a string. And then I\nsquare brackets and a string. And then I\nsquare brackets and a string. And then I want to have the last message. So\nwant to have the last message. So\nwant to have the last message. So negative one. And then from this\nnegative one. And then from this\nnegative one. And then from this message, I'm interested in the content.\nmessage, I'm interested in the content.\nmessage, I'm interested in the content. So I can run this now. And I get a\nSo I can run this now. And I get a\nSo I can run this now. And I get a problem because because I'm of course\nproblem because because I'm of course\nproblem because because I'm of course not loading the environment variables.\nnot loading the environment variables.\nnot loading the environment variables. So I need to import load. But I also\nSo I need to import load. But I also\nSo I need to import load. But I also need to call it to load the data from\nneed to call it to load the data from\nneed to call it to load the data from the end file. So let's go ahead and say\nthe end file. So let's go ahead and say\nthe end file. So let's go ahead and say uvun main py. I'm doing it outside here\nuvun main py. I'm doing it outside here\nuvun main py. I'm doing it outside here so I can see all the output. And what\nso I can see all the output. And what\nso I can see all the output. And what you see up here is the raw response\nyou see up here is the raw response\nyou see up here is the raw response object which contains the entire message\nobject which contains the entire message\nobject which contains the entire message history also with all the data that was\nhistory also with all the data that was\nhistory also with all the data that was provided by the API. And as a result\nprovided by the API. And as a result\nprovided by the API. And as a result down here I get the actual message. The\ndown here I get the actual message. The\ndown here I get the actual message. The weather in Vienna right now is partly\nweather in Vienna right now is partly\nweather in Vienna right now is partly cloudy with a comfortable temperature of\ncloudy with a comfortable temperature of\ncloudy with a comfortable temperature of about 15Â° C and then some information\nabout 15Â° C and then some information\nabout 15Â° C and then some information about the wind speed and humidity and so\nabout the wind speed and humidity and so\nabout the wind speed and humidity and so on. So actually quite comprehensive.\non. So actually quite comprehensive.\non. So actually quite comprehensive. We're going to build on top of this\nWe're going to build on top of this\nWe're going to build on top of this example and extend it later on. But\nexample and extend it later on. But\nexample and extend it later on. But before we go deeper into agents, let me\nbefore we go deeper into agents, let me\nbefore we go deeper into agents, let me remove all of this. I want to show you\nremove all of this. I want to show you\nremove all of this. I want to show you how to use standalone models. So maybe\nhow to use standalone models. So maybe\nhow to use standalone models. So maybe you don't want to have an agent. You\nyou don't want to have an agent. You\nyou don't want to have an agent. You just want to interact with a simple\njust want to interact with a simple\njust want to interact with a simple model and that's it. And you want to do\nmodel and that's it. And you want to do\nmodel and that's it. And you want to do that in a more abstract way. So you can\nthat in a more abstract way. So you can\nthat in a more abstract way. So you can replace the models that you're using.\nreplace the models that you're using.\nreplace the models that you're using. For this you can also use lang chain but\nFor this you can also use lang chain but\nFor this you can also use lang chain but not agents. You want to use lang chain\nnot agents. You want to use lang chain\nnot agents. You want to use lang chain chat models. And you want to import the\nchat models. And you want to import the\nchat models. And you want to import the function init chat model. And for this\nfunction init chat model. And for this\nfunction init chat model. And for this we're also not going to use tools. We're\nwe're also not going to use tools. We're\nwe're also not going to use tools. We're just going to do it like this. And it's\njust going to do it like this. And it's\njust going to do it like this. And it's actually quite simple and\nactually quite simple and\nactually quite simple and straightforward. I just say model is\nstraightforward. I just say model is\nstraightforward. I just say model is equal to init chat model. Then I can\nequal to init chat model. Then I can\nequal to init chat model. Then I can provide basic parameters like again the\nprovide basic parameters like again the\nprovide basic parameters like again the model identifier which is going to be\nmodel identifier which is going to be\nmodel identifier which is going to be 4.1 mini again. And theoretically if I\n4.1 mini again. And theoretically if I\n4.1 mini again. And theoretically if I want to I can do stuff like temperature\nwant to I can do stuff like temperature\nwant to I can do stuff like temperature 0.1 for example. And once I'm done with\n0.1 for example. And once I'm done with\n0.1 for example. And once I'm done with all this I can just say response is\nall this I can just say response is\nall this I can just say response is equal to model.invoke and here I just\nequal to model.invoke and here I just\nequal to model.invoke and here I just provide a prompt now like hello what is\nprovide a prompt now like hello what is\nprovide a prompt now like hello what is python question mark and then I can\npython question mark and then I can\npython question mark and then I can print the entire response object or I\nprint the entire response object or I\nprint the entire response object or I can just print the response content. So\ncan just print the response content. So\ncan just print the response content. So in this case, response.content since we\nin this case, response.content since we\nin this case, response.content since we didn't pass a message history or\ndidn't pass a message history or\ndidn't pass a message history or conversation history. And of course, one\nconversation history. And of course, one\nconversation history. And of course, one more time, I forgot the load. So we're\nmore time, I forgot the load. So we're\nmore time, I forgot the load. So we're going to add this and run this again.\ngoing to add this and run this again.\ngoing to add this and run this again. And there you go. We get an answer that\nAnd there you go. We get an answer that\nAnd there you go. We get an answer that tells us what Python is, a highle\ntells us what Python is, a highle\ntells us what Python is, a highle interpreted programming language. And we\ninterpreted programming language. And we\ninterpreted programming language. And we also have again this entire response\nalso have again this entire response\nalso have again this entire response object if we want to access different\nobject if we want to access different\nobject if we want to access different fields like the total amount of tokens\nfields like the total amount of tokens\nfields like the total amount of tokens and stuff like this. But you can see how\nand stuff like this. But you can see how\nand stuff like this. But you can see how easily this is done in lenchain. And we\neasily this is done in lenchain. And we\neasily this is done in lenchain. And we can also just swap the model if we want\ncan also just swap the model if we want\ncan also just swap the model if we want to use a different one. So instead of\nto use a different one. So instead of\nto use a different one. So instead of using GPT I can say I want to use\nusing GPT I can say I want to use\nusing GPT I can say I want to use Mistral medium. I just have to provide\nMistral medium. I just have to provide\nMistral medium. I just have to provide the proper string from the website and\nthe proper string from the website and\nthe proper string from the website and then I can run this and everything else\nthen I can run this and everything else\nthen I can run this and everything else in the application stays the same. I'm\nin the application stays the same. I'm\nin the application stays the same. I'm now just using Mistral uh medium instead\nnow just using Mistral uh medium instead\nnow just using Mistral uh medium instead of GPT4.1 mini and I'm going to get the\nof GPT4.1 mini and I'm going to get the\nof GPT4.1 mini and I'm going to get the response and everything's going to work\nresponse and everything's going to work\nresponse and everything's going to work in the same way. There you go. It's a\nin the same way. There you go. It's a\nin the same way. There you go. It's a quite comprehensive response but we get\nquite comprehensive response but we get\nquite comprehensive response but we get the answer here from Mistl. Let me now\nthe answer here from Mistl. Let me now\nthe answer here from Mistl. Let me now switch back to 4.1 mini. Now if we want\nswitch back to 4.1 mini. Now if we want\nswitch back to 4.1 mini. Now if we want to pass a conversation history and not\nto pass a conversation history and not\nto pass a conversation history and not just a single prompt, we can do that as\njust a single prompt, we can do that as\njust a single prompt, we can do that as well with this list and dictionary sort\nwell with this list and dictionary sort\nwell with this list and dictionary sort of notation that we used before. But we\nof notation that we used before. But we\nof notation that we used before. But we can also import specific classes for\ncan also import specific classes for\ncan also import specific classes for that. I can also say from langchain dot\nthat. I can also say from langchain dot\nthat. I can also say from langchain dot messages import human message AI message\nmessages import human message AI message\nmessages import human message AI message and system message. This makes it then\nand system message. This makes it then\nand system message. This makes it then super simple to work with. I can just\nsuper simple to work with. I can just\nsuper simple to work with. I can just say conversation is equal to a list and\nsay conversation is equal to a list and\nsay conversation is equal to a list and in here I can say I first have a system\nin here I can say I first have a system\nin here I can say I first have a system message. For example, you are a helpful\nmessage. For example, you are a helpful\nmessage. For example, you are a helpful assistant for questions regarding\nassistant for questions regarding\nassistant for questions regarding programming. Then the second message\nprogramming. Then the second message\nprogramming. Then the second message could be something that a human asks.\ncould be something that a human asks.\ncould be something that a human asks. Like for example, what is Python?\nLike for example, what is Python?\nLike for example, what is Python? Maybe let's stay consistent here with a\nMaybe let's stay consistent here with a\nMaybe let's stay consistent here with a quotation marks. Let's use single\nquotation marks. Let's use single\nquotation marks. Let's use single quotations everywhere. And then we're\nquotations everywhere. And then we're\nquotations everywhere. And then we're going to say an AI already answered\ngoing to say an AI already answered\ngoing to say an AI already answered that. We're going to say that it told us\nthat. We're going to say that it told us\nthat. We're going to say that it told us Python is an interpreted\nPython is an interpreted\nPython is an interpreted programming language. not question mark\nprogramming language. not question mark\nprogramming language. not question mark but period and then we're going to say\nbut period and then we're going to say\nbut period and then we're going to say the human has a follow-up question which\nthe human has a follow-up question which\nthe human has a follow-up question which relates to the previous messages and\nrelates to the previous messages and\nrelates to the previous messages and this is when was it released question\nthis is when was it released question\nthis is when was it released question mark. So now instead of invoking on a\nmark. So now instead of invoking on a\nmark. So now instead of invoking on a string I can also invoke on a\nstring I can also invoke on a\nstring I can also invoke on a conversation and actually I'm missing a\nconversation and actually I'm missing a\nconversation and actually I'm missing a t here and here as well and now the rest\nt here and here as well and now the rest\nt here and here as well and now the rest stays the same and we have a\nstays the same and we have a\nstays the same and we have a conversation. So Python was first\nconversation. So Python was first\nconversation. So Python was first released in 1991 by gofo fun rosesome.\nreleased in 1991 by gofo fun rosesome.\nreleased in 1991 by gofo fun rosesome. Now, we saw with Mistl that responses\nNow, we saw with Mistl that responses\nNow, we saw with Mistl that responses can be quite long and we need to wait\ncan be quite long and we need to wait\ncan be quite long and we need to wait for them to be finished before we can\nfor them to be finished before we can\nfor them to be finished before we can start reading them. If we don't want\nstart reading them. If we don't want\nstart reading them. If we don't want that, if we want to read them in real\nthat, if we want to read them in real\nthat, if we want to read them in real time as they're generated, we can also\ntime as they're generated, we can also\ntime as they're generated, we can also stream the response. So, let us maybe go\nstream the response. So, let us maybe go\nstream the response. So, let us maybe go back here to the prompt and let us also\nback here to the prompt and let us also\nback here to the prompt and let us also change the model back to mistrol. So,\nchange the model back to mistrol. So,\nchange the model back to mistrol. So, here I'm going to say mistrol medium 258\nhere I'm going to say mistrol medium 258\nhere I'm going to say mistrol medium 258 and we're going to ask the same\nand we're going to ask the same\nand we're going to ask the same question, but instead of just getting\nquestion, but instead of just getting\nquestion, but instead of just getting the response here, we're going to stream\nthe response here, we're going to stream\nthe response here, we're going to stream it. So we're not going to invoke. We're\nit. So we're not going to invoke. We're\nit. So we're not going to invoke. We're going to say model stream and we're\ngoing to say model stream and we're\ngoing to say model stream and we're going to iterate over this generator\ngoing to iterate over this generator\ngoing to iterate over this generator here to generate the chunks. So I'm\nhere to generate the chunks. So I'm\nhere to generate the chunks. So I'm going to say for chunk in model stream\ngoing to say for chunk in model stream\ngoing to say for chunk in model stream I'm going to print the chunk.ext.\nI'm going to print the chunk.ext.\nI'm going to print the chunk.ext. I'm going to have no line breaks after\nI'm going to have no line breaks after\nI'm going to have no line breaks after each print and I'm going to say flush is\neach print and I'm going to say flush is\neach print and I'm going to say flush is equal true equal to true so I can see\nequal true equal to true so I can see\nequal true equal to true so I can see the output in real time. And we're going\nthe output in real time. And we're going\nthe output in real time. And we're going to delete that and run this. And now you\nto delete that and run this. And now you\nto delete that and run this. And now you can see how this is generated in real\ncan see how this is generated in real\ncan see how this is generated in real time and I can read while it's still\ntime and I can read while it's still\ntime and I can read while it's still generating. So let us now come back to\ngenerating. So let us now come back to\ngenerating. So let us now come back to our initial example with the agent here\nour initial example with the agent here\nour initial example with the agent here and the weather function. We're now\nand the weather function. We're now\nand the weather function. We're now going to extend it to incorporate more\ngoing to extend it to incorporate more\ngoing to extend it to incorporate more concepts. So on the one hand I want to\nconcepts. So on the one hand I want to\nconcepts. So on the one hand I want to have structured output. I want to have\nhave structured output. I want to have\nhave structured output. I want to have the output message or summary and I also\nthe output message or summary and I also\nthe output message or summary and I also want to have some key information like\nwant to have some key information like\nwant to have some key information like the temperature or the humidity. Also I\nthe temperature or the humidity. Also I\nthe temperature or the humidity. Also I want the agent to be able to realize\nwant the agent to be able to realize\nwant the agent to be able to realize what location I'm asking this question\nwhat location I'm asking this question\nwhat location I'm asking this question from. So I don't have to specify the\nfrom. So I don't have to specify the\nfrom. So I don't have to specify the city. So I just want to say what's the\ncity. So I just want to say what's the\ncity. So I just want to say what's the weather like? And the agent should\nweather like? And the agent should\nweather like? And the agent should realize that I'm asking from a specific\nrealize that I'm asking from a specific\nrealize that I'm asking from a specific city based on mock database entries that\ncity based on mock database entries that\ncity based on mock database entries that we're going to provide here. And with\nwe're going to provide here. And with\nwe're going to provide here. And with this context, it's going to then\nthis context, it's going to then\nthis context, it's going to then retrieve the proper information. And\nretrieve the proper information. And\nretrieve the proper information. And finally, I would also like to add memory\nfinally, I would also like to add memory\nfinally, I would also like to add memory to this agent so it can remember that we\nto this agent so it can remember that we\nto this agent so it can remember that we had a conversation and we're now\nhad a conversation and we're now\nhad a conversation and we're now continuing that conversation. So we're\ncontinuing that conversation. So we're\ncontinuing that conversation. So we're going to add some imports for all of\ngoing to add some imports for all of\ngoing to add some imports for all of this. First of all, from core Python,\nthis. First of all, from core Python,\nthis. First of all, from core Python, we're going to add here from data\nwe're going to add here from data\nwe're going to add here from data classes the data class. And for\nclasses the data class. And for\nclasses the data class. And for langchain here, we're going to say from\nlangchain here, we're going to say from\nlangchain here, we're going to say from langchain.models\nlangchain.models\nlangchain.models or chat models importit\nor chat models importit\nor chat models importit chat model. In addition to tool, we're\nchat model. In addition to tool, we're\nchat model. In addition to tool, we're also going to import tool runtime. And\nalso going to import tool runtime. And\nalso going to import tool runtime. And finally here from lang graph. Now I said\nfinally here from lang graph. Now I said\nfinally here from lang graph. Now I said we're not going to do lang graph. We're\nwe're not going to do lang graph. We're\nwe're not going to do lang graph. We're not going to cover the langraph\nnot going to cover the langraph\nnot going to cover the langraph framework, but we're going to use uh one\nframework, but we're going to use uh one\nframework, but we're going to use uh one specific class from there. It's from\nspecific class from there. It's from\nspecific class from there. It's from checkpoint memory the inmemory saver\ncheckpoint memory the inmemory saver\ncheckpoint memory the inmemory saver which is going to be important for\nwhich is going to be important for\nwhich is going to be important for remembering the message history. Cool.\nremembering the message history. Cool.\nremembering the message history. Cool. So now let us create two data classes.\nSo now let us create two data classes.\nSo now let us create two data classes. One is going to be for the context.\nOne is going to be for the context.\nOne is going to be for the context. We're going to keep track of the user ID\nWe're going to keep track of the user ID\nWe're going to keep track of the user ID that the model is communicating with so\nthat the model is communicating with so\nthat the model is communicating with so that we can actually look up the\nthat we can actually look up the\nthat we can actually look up the location of that user from our database\nlocation of that user from our database\nlocation of that user from our database which we're going to just model as a\nwhich we're going to just model as a\nwhich we're going to just model as a match case statement in a function. And\nmatch case statement in a function. And\nmatch case statement in a function. And then we're also going to have a data\nthen we're also going to have a data\nthen we're also going to have a data class for the response format. So we're\nclass for the response format. So we're\nclass for the response format. So we're going to say here at data class and then\ngoing to say here at data class and then\ngoing to say here at data class and then it's going to be a class called context.\nit's going to be a class called context.\nit's going to be a class called context. Quite simple and it's just going to have\nQuite simple and it's just going to have\nQuite simple and it's just going to have a user ID which is a string. And then\na user ID which is a string. And then\na user ID which is a string. And then for the response format we're going to\nfor the response format we're going to\nfor the response format we're going to say data class response format. And here\nsay data class response format. And here\nsay data class response format. And here I want to have a summary which is going\nI want to have a summary which is going\nI want to have a summary which is going to be a string. I want to have a\nto be a string. I want to have a\nto be a string. I want to have a temperature in Celsius which is going to\ntemperature in Celsius which is going to\ntemperature in Celsius which is going to be a float. I want to have the same in\nbe a float. I want to have the same in\nbe a float. I want to have the same in Fahrenheit. And I also want to have a\nFahrenheit. And I also want to have a\nFahrenheit. And I also want to have a humidity whatever the unit is here.\nhumidity whatever the unit is here.\nhumidity whatever the unit is here. Cool. So now what we're going to do is\nCool. So now what we're going to do is\nCool. So now what we're going to do is we're going to add an additional tool\nwe're going to add an additional tool\nwe're going to add an additional tool and this tool is going to be locate\nand this tool is going to be locate\nand this tool is going to be locate user. So the name is going to be locate\nuser. So the name is going to be locate\nuser. So the name is going to be locate user and it's going to have the\nuser and it's going to have the\nuser and it's going to have the following description. Look up a user's\nfollowing description. Look up a user's\nfollowing description. Look up a user's now I have to use double quotations\nnow I have to use double quotations\nnow I have to use double quotations here. Look up a user's city based on the\nhere. Look up a user's city based on the\nhere. Look up a user's city based on the context. Now the interesting thing is\ncontext. Now the interesting thing is\ncontext. Now the interesting thing is we're not going to pass the user ID as a\nwe're not going to pass the user ID as a\nwe're not going to pass the user ID as a parameter. We're going to have a tool\nparameter. We're going to have a tool\nparameter. We're going to have a tool runtime which contains context. So the\nruntime which contains context. So the\nruntime which contains context. So the context is going to contain the user ID\ncontext is going to contain the user ID\ncontext is going to contain the user ID and we're going to get it from this\nand we're going to get it from this\nand we're going to get it from this context. How do we do that? We say\ncontext. How do we do that? We say\ncontext. How do we do that? We say deflocate user and here we have a\ndeflocate user and here we have a\ndeflocate user and here we have a runtime. This runtime is going to be a\nruntime. This runtime is going to be a\nruntime. This runtime is going to be a tool runtime and we're going to pass\ntool runtime and we're going to pass\ntool runtime and we're going to pass here in square brackets the context\nhere in square brackets the context\nhere in square brackets the context class the data class we just created.\nclass the data class we just created.\nclass the data class we just created. And here what we're going to do is we're\nAnd here what we're going to do is we're\nAnd here what we're going to do is we're going to get the user ID from\ngoing to get the user ID from\ngoing to get the user ID from runtime.context.\nruntime.context.\nruntime.context. And I'm going to use a match statement.\nAnd I'm going to use a match statement.\nAnd I'm going to use a match statement. So match runtime dot context dot user\nSo match runtime dot context dot user\nSo match runtime dot context dot user ID. And depending on the value, we're\nID. And depending on the value, we're\nID. And depending on the value, we're going to return a different city. So\ngoing to return a different city. So\ngoing to return a different city. So let's make up some cases here. If the\nlet's make up some cases here. If the\nlet's make up some cases here. If the user ID is ABC123,\nuser ID is ABC123,\nuser ID is ABC123, I'm just going to return Vienna. Another\nI'm just going to return Vienna. Another\nI'm just going to return Vienna. Another case could be if the user ID is XYZ456,\ncase could be if the user ID is XYZ456,\ncase could be if the user ID is XYZ456, then I'm going to say that we're in\nthen I'm going to say that we're in\nthen I'm going to say that we're in London. Then another case could be HJKL\nLondon. Then another case could be HJKL\nLondon. Then another case could be HJKL for Vim and then 111. That would return\nfor Vim and then 111. That would return\nfor Vim and then 111. That would return Paris. And if it's none of these, if\nParis. And if it's none of these, if\nParis. And if it's none of these, if it's unknown, we're going to say case\nit's unknown, we're going to say case\nit's unknown, we're going to say case default is just going to return unknown.\ndefault is just going to return unknown.\ndefault is just going to return unknown. Now, you can define unknown behavior in\nNow, you can define unknown behavior in\nNow, you can define unknown behavior in multiple ways. You can provide it in the\nmultiple ways. You can provide it in the\nmultiple ways. You can provide it in the description. You can even provide it in\ndescription. You can even provide it in\ndescription. You can even provide it in the return value itself. You can provide\nthe return value itself. You can provide\nthe return value itself. You can provide it in the system prompt. You can also\nit in the system prompt. You can also\nit in the system prompt. You can also add some custom logic. But basically, we\nadd some custom logic. But basically, we\nadd some custom logic. But basically, we need to somehow instruct the model that\nneed to somehow instruct the model that\nneed to somehow instruct the model that if it's unknown, just say it's unknown.\nif it's unknown, just say it's unknown.\nif it's unknown, just say it's unknown. Maybe it can do it automatically as\nMaybe it can do it automatically as\nMaybe it can do it automatically as well. Uh, wherever you want to put that,\nwell. Uh, wherever you want to put that,\nwell. Uh, wherever you want to put that, put that somewhere how you want to\nput that somewhere how you want to\nput that somewhere how you want to handle unknown values. But we're going\nhandle unknown values. But we're going\nhandle unknown values. But we're going to look up based on the user ID in the\nto look up based on the user ID in the\nto look up based on the user ID in the context. So remember the connection here\ncontext. So remember the connection here\ncontext. So remember the connection here we have the runtime the tool runtime\nwe have the runtime the tool runtime\nwe have the runtime the tool runtime passed to this tool which is based on\npassed to this tool which is based on\npassed to this tool which is based on the data class context which we defined\nthe data class context which we defined\nthe data class context which we defined up here which contains the user ID. So\nup here which contains the user ID. So\nup here which contains the user ID. So now we're going to go down here create\nnow we're going to go down here create\nnow we're going to go down here create the model to show you that we can also\nthe model to show you that we can also\nthe model to show you that we can also pass a model instance. So model is going\npass a model instance. So model is going\npass a model instance. So model is going to be init chat model GPT-4.1-\nto be init chat model GPT-4.1-\nto be init chat model GPT-4.1- mini and the temperature not this sort\nmini and the temperature not this sort\nmini and the temperature not this sort of temperature but the model temperature\nof temperature but the model temperature\nof temperature but the model temperature is going to be 0.3 for example and then\nis going to be 0.3 for example and then\nis going to be 0.3 for example and then we're going to create a checkpointer\nwe're going to create a checkpointer\nwe're going to create a checkpointer which is going to be an in-memory saver.\nwhich is going to be an in-memory saver.\nwhich is going to be an in-memory saver. As I said this is for remembering\nAs I said this is for remembering\nAs I said this is for remembering conversations. We're going to add to the\nconversations. We're going to add to the\nconversations. We're going to add to the agent invocations a thread ID and this\nagent invocations a thread ID and this\nagent invocations a thread ID and this is going to determine the conversation\nis going to determine the conversation\nis going to determine the conversation that we're focusing on. So we can keep\nthat we're focusing on. So we can keep\nthat we're focusing on. So we can keep asking questions about the same\nasking questions about the same\nasking questions about the same conversation. And now we can combine all\nconversation. And now we can combine all\nconversation. And now we can combine all of this into the create agent function.\nof this into the create agent function.\nof this into the create agent function. So we're going to say agent is equal to\nSo we're going to say agent is equal to\nSo we're going to say agent is equal to create agent. Model is equal to model.\ncreate agent. Model is equal to model.\ncreate agent. Model is equal to model. Tools is equal to get weather and locate\nTools is equal to get weather and locate\nTools is equal to get weather and locate user. System prompt can stay the same.\nuser. System prompt can stay the same.\nuser. System prompt can stay the same. Now new stuff here is context schema.\nNow new stuff here is context schema.\nNow new stuff here is context schema. This is going to be the class the data\nThis is going to be the class the data\nThis is going to be the class the data class of our context. So just context.\nclass of our context. So just context.\nclass of our context. So just context. Then also response format. I think it's\nThen also response format. I think it's\nThen also response format. I think it's not surprising that this will be our\nnot surprising that this will be our\nnot surprising that this will be our class response format. And finally,\nclass response format. And finally,\nclass response format. And finally, checkpointer also not surprising is\ncheckpointer also not surprising is\ncheckpointer also not surprising is going to be the checkpoint. So what our\ngoing to be the checkpoint. So what our\ngoing to be the checkpoint. So what our agent now does is it has access to a\nagent now does is it has access to a\nagent now does is it has access to a model GPT4.1 mini. It has access to two\nmodel GPT4.1 mini. It has access to two\nmodel GPT4.1 mini. It has access to two tools. One for getting weather\ntools. One for getting weather\ntools. One for getting weather information about the city, one for\ninformation about the city, one for\ninformation about the city, one for locating the user, so getting the city\nlocating the user, so getting the city\nlocating the user, so getting the city of the user based on the user ID. We\nof the user based on the user ID. We\nof the user based on the user ID. We have a system prompt here. We now also\nhave a system prompt here. We now also\nhave a system prompt here. We now also have the ability to pass context. So for\nhave the ability to pass context. So for\nhave the ability to pass context. So for this we use the data class context which\nthis we use the data class context which\nthis we use the data class context which again contains the user ID. Then we have\nagain contains the user ID. Then we have\nagain contains the user ID. Then we have a response format which means that our\na response format which means that our\na response format which means that our model is forced now to answer in a\nmodel is forced now to answer in a\nmodel is forced now to answer in a specific format. This format is going to\nspecific format. This format is going to\nspecific format. This format is going to be a summary string and then three\nbe a summary string and then three\nbe a summary string and then three floats for temperature and humidity. And\nfloats for temperature and humidity. And\nfloats for temperature and humidity. And finally we add memory to the model so it\nfinally we add memory to the model so it\nfinally we add memory to the model so it can keep track of conversations based on\ncan keep track of conversations based on\ncan keep track of conversations based on a thread ID. So now when we invoke\na thread ID. So now when we invoke\na thread ID. So now when we invoke something we also need to pass context\nsomething we also need to pass context\nsomething we also need to pass context and thread ID. For this we're going to\nand thread ID. For this we're going to\nand thread ID. For this we're going to start by saying config is equal to\nstart by saying config is equal to\nstart by saying config is equal to dictionary which is going to have a key\ndictionary which is going to have a key\ndictionary which is going to have a key called configurable. And this\ncalled configurable. And this\ncalled configurable. And this configurable is going to point to\nconfigurable is going to point to\nconfigurable is going to point to another dictionary which contains thread\nanother dictionary which contains thread\nanother dictionary which contains thread ID which itself points to one for\nID which itself points to one for\nID which itself points to one for example. And now for the invocation I'm\nexample. And now for the invocation I'm\nexample. And now for the invocation I'm going to say what is the weather like\ngoing to say what is the weather like\ngoing to say what is the weather like without specifying a city but I'm going\nwithout specifying a city but I'm going\nwithout specifying a city but I'm going to specify context and I'm going to pass\nto specify context and I'm going to pass\nto specify context and I'm going to pass the configuration. So for the\nthe configuration. So for the\nthe configuration. So for the configuration just config equals config\nconfiguration just config equals config\nconfiguration just config equals config and for the context we're going to say\nand for the context we're going to say\nand for the context we're going to say context is equal to context an instance\ncontext is equal to context an instance\ncontext is equal to context an instance of the data class where we set the user\nof the data class where we set the user\nof the data class where we set the user ID to be equal to ABC123.\nID to be equal to ABC123.\nID to be equal to ABC123. So that would result in Vienna again. We\nSo that would result in Vienna again. We\nSo that would result in Vienna again. We should get the same response. And since\nshould get the same response. And since\nshould get the same response. And since we're now working with this response\nwe're now working with this response\nwe're now working with this response format, since we're forcing structured\nformat, since we're forcing structured\nformat, since we're forcing structured output, we're going to not access just\noutput, we're going to not access just\noutput, we're going to not access just messages negative1 content. We're going\nmessages negative1 content. We're going\nmessages negative1 content. We're going to print the entire response object if\nto print the entire response object if\nto print the entire response object if we want to. Actually, I don't want to do\nwe want to. Actually, I don't want to do\nwe want to. Actually, I don't want to do that. I just want to get the structured\nthat. I just want to get the structured\nthat. I just want to get the structured output itself. And for that, I'm going\noutput itself. And for that, I'm going\noutput itself. And for that, I'm going to say here response structured\nto say here response structured\nto say here response structured response. And this will give us the\nresponse. And this will give us the\nresponse. And this will give us the entire response object. If I'm\nentire response object. If I'm\nentire response object. If I'm interested in specific parts of that, I\ninterested in specific parts of that, I\ninterested in specific parts of that, I can just say dot summary or dot\ncan just say dot summary or dot\ncan just say dot summary or dot temperature Celsius for example. So now\ntemperature Celsius for example. So now\ntemperature Celsius for example. So now I can run this and we have a problem\nI can run this and we have a problem\nI can run this and we have a problem because we're not closing this curly\nbecause we're not closing this curly\nbecause we're not closing this curly bracket early enough. So of course it\nbracket early enough. So of course it\nbracket early enough. So of course it belongs to this message history. But\nbelongs to this message history. But\nbelongs to this message history. But these are now just keyword arguments. So\nthese are now just keyword arguments. So\nthese are now just keyword arguments. So let me run this again. And we can see\nlet me run this again. And we can see\nlet me run this again. And we can see the current weather in Vienna is partly\nthe current weather in Vienna is partly\nthe current weather in Vienna is partly cloudy. And then I also get 15.0 zero\ncloudy. And then I also get 15.0 zero\ncloudy. And then I also get 15.0 zero for the Celsius temperature. Now, if I\nfor the Celsius temperature. Now, if I\nfor the Celsius temperature. Now, if I change my user ID to something else like\nchange my user ID to something else like\nchange my user ID to something else like XYZ, what was it? 456, then I should get\nXYZ, what was it? 456, then I should get\nXYZ, what was it? 456, then I should get the weather for London. There you go.\nthe weather for London. There you go.\nthe weather for London. There you go. The weather in London is currently sunny\nThe weather in London is currently sunny\nThe weather in London is currently sunny with a temperature of 12. Then, if I try\nwith a temperature of 12. Then, if I try\nwith a temperature of 12. Then, if I try something completely different like\nsomething completely different like\nsomething completely different like this, something it doesn't recognize,\nthis, something it doesn't recognize,\nthis, something it doesn't recognize, probably it's going to tell me unknown\nprobably it's going to tell me unknown\nprobably it's going to tell me unknown or it's going to do something else. I\nor it's going to do something else. I\nor it's going to do something else. I couldn't find your location, so I can't\ncouldn't find your location, so I can't\ncouldn't find your location, so I can't tell the weather, but hey, if you tell\ntell the weather, but hey, if you tell\ntell the weather, but hey, if you tell me your city, I'll fetch the weather.\nme your city, I'll fetch the weather.\nme your city, I'll fetch the weather. and zero is the default value for\nand zero is the default value for\nand zero is the default value for temperature. And to show you that this\ntemperature. And to show you that this\ntemperature. And to show you that this actually works with a follow-up, if I\nactually works with a follow-up, if I\nactually works with a follow-up, if I provide here again a valid user ID like\nprovide here again a valid user ID like\nprovide here again a valid user ID like this for Vienna here, I can also follow\nthis for Vienna here, I can also follow\nthis for Vienna here, I can also follow up with the same config to keep track of\nup with the same config to keep track of\nup with the same config to keep track of this conversation. So I can just copy\nthis conversation. So I can just copy\nthis conversation. So I can just copy this here, paste it down here and I can\nthis here, paste it down here and I can\nthis here, paste it down here and I can say and is this usual question mark. So\nsay and is this usual question mark. So\nsay and is this usual question mark. So when I run this now and of course maybe\nwhen I run this now and of course maybe\nwhen I run this now and of course maybe before running this I should also print\nbefore running this I should also print\nbefore running this I should also print the result. So just copy this from up\nthe result. So just copy this from up\nthe result. So just copy this from up here. print structured response summary\nhere. print structured response summary\nhere. print structured response summary and now it should keep track of the\nand now it should keep track of the\nand now it should keep track of the information. So we have this inmemory\ninformation. So we have this inmemory\ninformation. So we have this inmemory saver. So we have still the information\nsaver. So we have still the information\nsaver. So we have still the information that uh the weather in Vienna is what it\nthat uh the weather in Vienna is what it\nthat uh the weather in Vienna is what it is and then yes the weather in Vienna\nis and then yes the weather in Vienna\nis and then yes the weather in Vienna being partly cloudy with mild\nbeing partly cloudy with mild\nbeing partly cloudy with mild temperatures around 15Â° C and so on is\ntemperatures around 15Â° C and so on is\ntemperatures around 15Â° C and so on is usual for this time of the year.\nusual for this time of the year.\nusual for this time of the year. However, you will notice that if I take\nHowever, you will notice that if I take\nHowever, you will notice that if I take this and I change this before I do that.\nthis and I change this before I do that.\nthis and I change this before I do that. So if I say now the thread ID is two,\nSo if I say now the thread ID is two,\nSo if I say now the thread ID is two, it's no longer going to be related to\nit's no longer going to be related to\nit's no longer going to be related to that thread. So, it doesn't know what\nthat thread. So, it doesn't know what\nthat thread. So, it doesn't know what I'm talking about. So, here I get the\nI'm talking about. So, here I get the\nI'm talking about. So, here I get the information about Vienna. And now I\ninformation about Vienna. And now I\ninformation about Vienna. And now I would need to know the specific weather\nwould need to know the specific weather\nwould need to know the specific weather conditions or location you're referring\nconditions or location you're referring\nconditions or location you're referring to in order to determine if it's usual\nto in order to determine if it's usual\nto in order to determine if it's usual or not. So, since we're in a different\nor not. So, since we're in a different\nor not. So, since we're in a different threat, it doesn't know what we talked\nthreat, it doesn't know what we talked\nthreat, it doesn't know what we talked about up until this point. If I may for\nabout up until this point. If I may for\nabout up until this point. If I may for a second, I would like to plug myself in\na second, I would like to plug myself in\na second, I would like to plug myself in as the sponsor of my own video. If you\nas the sponsor of my own video. If you\nas the sponsor of my own video. If you go to my website, neural9.com, you will\ngo to my website, neural9.com, you will\ngo to my website, neural9.com, you will find a tab services and a tab tutoring.\nfind a tab services and a tab tutoring.\nfind a tab services and a tab tutoring. Here you can hire me for all sorts of\nHere you can hire me for all sorts of\nHere you can hire me for all sorts of stuff like data science, machine\nstuff like data science, machine\nstuff like data science, machine learning, web development. If you need\nlearning, web development. If you need\nlearning, web development. If you need help with something in a project here,\nhelp with something in a project here,\nhelp with something in a project here, you can book me for one-on-one tutoring.\nyou can book me for one-on-one tutoring.\nyou can book me for one-on-one tutoring. If you want me to teach you personally\nIf you want me to teach you personally\nIf you want me to teach you personally something that you don't understand, if\nsomething that you don't understand, if\nsomething that you don't understand, if you like my teaching style on both pages\nyou like my teaching style on both pages\nyou like my teaching style on both pages at the bottom, you can contact me via\nat the bottom, you can contact me via\nat the bottom, you can contact me via mail and also via LinkedIn. Just wanted\nmail and also via LinkedIn. Just wanted\nmail and also via LinkedIn. Just wanted to let you know about this. Next, I want\nto let you know about this. Next, I want\nto let you know about this. Next, I want to show you how we can work with\nto show you how we can work with\nto show you how we can work with multimodal input. So, how can we pass to\nmultimodal input. So, how can we pass to\nmultimodal input. So, how can we pass to a model not just text, but for example,\na model not just text, but for example,\na model not just text, but for example, image data. For this, I'm going to say\nimage data. For this, I'm going to say\nimage data. For this, I'm going to say here model is equal to in a chat model.\nhere model is equal to in a chat model.\nhere model is equal to in a chat model. I'm going to use again GPT-4.1-\nmini. And we're now going to create a\nmini. And we're now going to create a message in the dictionary format. Again,\nmessage in the dictionary format. Again,\nmessage in the dictionary format. Again, I'm going to show you a different way in\nI'm going to show you a different way in\nI'm going to show you a different way in a second as well. The role for this\na second as well. The role for this\na second as well. The role for this message is going to be user. And the\nmessage is going to be user. And the\nmessage is going to be user. And the content field now is going to have\ncontent field now is going to have\ncontent field now is going to have multiple values. So, we're going to say\nmultiple values. So, we're going to say\nmultiple values. So, we're going to say content is pointing to a list. And this\ncontent is pointing to a list. And this\ncontent is pointing to a list. And this list will contain multiple pieces of\nlist will contain multiple pieces of\nlist will contain multiple pieces of content. For example, the first one will\ncontent. For example, the first one will\ncontent. For example, the first one will be of type text and we're going to say\nbe of type text and we're going to say\nbe of type text and we're going to say that the actual text content. So again\nthat the actual text content. So again\nthat the actual text content. So again text here as a key not as a value will\ntext here as a key not as a value will\ntext here as a key not as a value will be describe the contents of this image.\nbe describe the contents of this image.\nbe describe the contents of this image. Now we can copy that and we can say type\nNow we can copy that and we can say type\nNow we can copy that and we can say type image and now we have two ways to\nimage and now we have two ways to\nimage and now we have two ways to provide image content. One is by using\nprovide image content. One is by using\nprovide image content. One is by using URL. So this basically points to an\nURL. So this basically points to an\nURL. So this basically points to an image somewhere on the web or we can\nimage somewhere on the web or we can\nimage somewhere on the web or we can also pass base 64 encoded image bytes.\nalso pass base 64 encoded image bytes.\nalso pass base 64 encoded image bytes. Now we're going to do both but I'm going\nNow we're going to do both but I'm going\nNow we're going to do both but I'm going to start with a URL and for that I have\nto start with a URL and for that I have\nto start with a URL and for that I have here a link from my website. So just\nhere a link from my website. So just\nhere a link from my website. So just neural9.com and the logo on my website.\nneural9.com and the logo on my website.\nneural9.com and the logo on my website. I'm going to pass this here as image\nI'm going to pass this here as image\nI'm going to pass this here as image content. And what I'm going to do then\ncontent. And what I'm going to do then\ncontent. And what I'm going to do then is I'm going to say model.invoke.\nis I'm going to say model.invoke.\nis I'm going to say model.invoke. I'm going to pass a list of messages and\nI'm going to pass a list of messages and\nI'm going to pass a list of messages and just my one message in here. That's\njust my one message in here. That's\njust my one message in here. That's going to be the response. And then I can\ngoing to be the response. And then I can\ngoing to be the response. And then I can just say print response.content.\njust say print response.content.\njust say print response.content. So if I run this, this will take a look\nSo if I run this, this will take a look\nSo if I run this, this will take a look at my image and tell me that this is a\nat my image and tell me that this is a\nat my image and tell me that this is a logo that reads neural 9 and orange text\nlogo that reads neural 9 and orange text\nlogo that reads neural 9 and orange text on a black background. The text is\non a black background. The text is\non a black background. The text is stylized with a number one. I think this\nstylized with a number one. I think this\nstylized with a number one. I think this is just a mistake in the font. And it\nis just a mistake in the font. And it\nis just a mistake in the font. And it explains what my logo looks like\nexplains what my logo looks like\nexplains what my logo looks like essentially. So this is what we want.\nessentially. So this is what we want.\nessentially. So this is what we want. And we can do the same thing with an\nAnd we can do the same thing with an\nAnd we can do the same thing with an image from disk. So if I open the\nimage from disk. So if I open the\nimage from disk. So if I open the sidebar here, you can see I have the\nsidebar here, you can see I have the\nsidebar here, you can see I have the logo.png. png. We can also load this and\nlogo.png. png. We can also load this and\nlogo.png. png. We can also load this and encode it with b 64. So in this case\nencode it with b 64. So in this case\nencode it with b 64. So in this case here just b 64 as the field as the key\nhere just b 64 as the field as the key\nhere just b 64 as the field as the key here and I'm also going to say from b 64\nhere and I'm also going to say from b 64\nhere and I'm also going to say from b 64 import b 64 encode. So the idea is we\nimport b 64 encode. So the idea is we\nimport b 64 encode. So the idea is we load the bytes we encode them with b 64\nload the bytes we encode them with b 64\nload the bytes we encode them with b 64 and then we decode it into a string. So\nand then we decode it into a string. So\nand then we decode it into a string. So we're going to say here b64 encode. What\nwe're going to say here b64 encode. What\nwe're going to say here b64 encode. What are we encoding? We're opening a file\nare we encoding? We're opening a file\nare we encoding? We're opening a file from disk called logo.png\nfrom disk called logo.png\nfrom disk called logo.png in reading bytes mode. Then we're\nin reading bytes mode. Then we're\nin reading bytes mode. Then we're reading the content of that file. We're\nreading the content of that file. We're\nreading the content of that file. We're encoding it with B 64 and then we're\nencoding it with B 64 and then we're\nencoding it with B 64 and then we're decoding it into a string. So we can\ndecoding it into a string. So we can\ndecoding it into a string. So we can actually pass it here to the model. Now\nactually pass it here to the model. Now\nactually pass it here to the model. Now what we also need to pass here if we use\nwhat we also need to pass here if we use\nwhat we also need to pass here if we use B 64 is a mime type. So let's actually\nB 64 is a mime type. So let's actually\nB 64 is a mime type. So let's actually format it like this. And the mime type\nformat it like this. And the mime type\nformat it like this. And the mime type is going to be image / PNG. And actually\nis going to be image / PNG. And actually\nis going to be image / PNG. And actually I think we need to use underscore not\nI think we need to use underscore not\nI think we need to use underscore not dash. So now when I run this you can see\ndash. So now when I run this you can see\ndash. So now when I run this you can see the image shows a logo with a text\nthe image shows a logo with a text\nthe image shows a logo with a text neural 9 written in orange and basically\nneural 9 written in orange and basically\nneural 9 written in orange and basically the same thing as before. Now we can\nthe same thing as before. Now we can\nthe same thing as before. Now we can also do it with the message classes but\nalso do it with the message classes but\nalso do it with the message classes but we have to do it basically in the same\nwe have to do it basically in the same\nwe have to do it basically in the same way. So I can say langchain do messages\nway. So I can say langchain do messages\nway. So I can say langchain do messages import human message and the only thing\nimport human message and the only thing\nimport human message and the only thing I would change is I would get rid of the\nI would change is I would get rid of the\nI would change is I would get rid of the role but I would still keep the content\nrole but I would still keep the content\nrole but I would still keep the content as it is. So I would say here the\nas it is. So I would say here the\nas it is. So I would say here the message is equal to human message and\nmessage is equal to human message and\nmessage is equal to human message and then I would say content is equal to the\nthen I would say content is equal to the\nthen I would say content is equal to the list. So that is the small difference\nlist. So that is the small difference\nlist. So that is the small difference here. So context would be equal to the\nhere. So context would be equal to the\nhere. So context would be equal to the list of these two things and then we\nlist of these two things and then we\nlist of these two things and then we would close that with an ordinary\nwould close that with an ordinary\nwould close that with an ordinary bracket but basically the rest stays the\nbracket but basically the rest stays the\nbracket but basically the rest stays the same. So when I run this we should get\nsame. So when I run this we should get\nsame. So when I run this we should get the same response. It's just a different\nthe same response. It's just a different\nthe same response. It's just a different way to write it. Now for the next\nway to write it. Now for the next\nway to write it. Now for the next example we're going to build a simple\nexample we're going to build a simple\nexample we're going to build a simple rack use case. So retrieval augmented\nrack use case. So retrieval augmented\nrack use case. So retrieval augmented generation. Basically we're going to use\ngeneration. Basically we're going to use\ngeneration. Basically we're going to use a vector store and an embedding model to\na vector store and an embedding model to\na vector store and an embedding model to find the most similar pieces of content.\nfind the most similar pieces of content.\nfind the most similar pieces of content. In our case, simple messages or simple\nIn our case, simple messages or simple\nIn our case, simple messages or simple statements, let's say. And for this,\nstatements, let's say. And for this,\nstatements, let's say. And for this, we're going to use lang chain in the old\nwe're going to use lang chain in the old\nwe're going to use lang chain in the old school way. So, we're actually going to\nschool way. So, we're actually going to\nschool way. So, we're actually going to say up here from langchain openai import\nsay up here from langchain openai import\nsay up here from langchain openai import openai embeddings. This is for the\nopenai embeddings. This is for the\nopenai embeddings. This is for the embeddings model. And then we're also\nembeddings model. And then we're also\nembeddings model. And then we're also going to use a vector store. So, a\ngoing to use a vector store. So, a\ngoing to use a vector store. So, a vector database in my case face. And for\nvector database in my case face. And for\nvector database in my case face. And for this, we're going to say from langchain\nthis, we're going to say from langchain\nthis, we're going to say from langchain community\ncommunity\ncommunity import. And we're going to import uh\nimport. And we're going to import uh\nimport. And we're going to import uh face. But actually not from community\nface. But actually not from community\nface. But actually not from community directly but from community.\ndirectly but from community.\ndirectly but from community. Vector stores and since I'm not getting\nVector stores and since I'm not getting\nVector stores and since I'm not getting autocomp completion I assume I have to\nautocomp completion I assume I have to\nautocomp completion I assume I have to install this separately. So I'm going to\ninstall this separately. So I'm going to\ninstall this separately. So I'm going to say here now uv at and then\nsay here now uv at and then\nsay here now uv at and then langchain-ash\nlangchain-ash\nlangchain-ash community. So this installs now\ncommunity. So this installs now\ncommunity. So this installs now additional packages langchain community\nadditional packages langchain community\nadditional packages langchain community langchain classic langchain textlitter.\nlangchain classic langchain textlitter.\nlangchain classic langchain textlitter. So if I now go back into the code\nSo if I now go back into the code\nSo if I now go back into the code hopefully if I type dot something here\nhopefully if I type dot something here\nhopefully if I type dot something here there you go. We can see all the modules\nthere you go. We can see all the modules\nthere you go. We can see all the modules here. So, langin community vector stores\nhere. So, langin community vector stores\nhere. So, langin community vector stores import face. Now, I think actually for\nimport face. Now, I think actually for\nimport face. Now, I think actually for face, we also need to install the face\nface, we also need to install the face\nface, we also need to install the face package. So, let me leave this and let's\npackage. So, let me leave this and let's\npackage. So, let me leave this and let's say UV add face. And I'm just going to\nsay UV add face. And I'm just going to\nsay UV add face. And I'm just going to go save here with the CPU. So, I don't\ngo save here with the CPU. So, I don't\ngo save here with the CPU. So, I don't have to care too much about GPU stuff.\nhave to care too much about GPU stuff.\nhave to care too much about GPU stuff. This is just face CPU. And now, if I go\nThis is just face CPU. And now, if I go\nThis is just face CPU. And now, if I go back into the code, we should be able to\nback into the code, we should be able to\nback into the code, we should be able to use them. So, the basic idea is I'm\nuse them. So, the basic idea is I'm\nuse them. So, the basic idea is I'm going to have a list of statements.\ngoing to have a list of statements.\ngoing to have a list of statements. These statements will be stuff like I\nThese statements will be stuff like I\nThese statements will be stuff like I love apples or I like oranges or I like\nlove apples or I like oranges or I like\nlove apples or I like oranges or I like pears and then something about computers\npears and then something about computers\npears and then something about computers also related to Apple but semantically\nalso related to Apple but semantically\nalso related to Apple but semantically different because Apple is a company.\ndifferent because Apple is a company.\ndifferent because Apple is a company. Apple is also a fruit. So we're going to\nApple is also a fruit. So we're going to\nApple is also a fruit. So we're going to see if the embeddings can distinguish\nsee if the embeddings can distinguish\nsee if the embeddings can distinguish the concepts and we're going to retrieve\nthe concepts and we're going to retrieve\nthe concepts and we're going to retrieve the most similar statements from the\nthe most similar statements from the\nthe most similar statements from the vector store. So let us start by saying\nvector store. So let us start by saying\nvector store. So let us start by saying embeddings is going to be equal to\nembeddings is going to be equal to\nembeddings is going to be equal to openAI embeddings and we're going to\nopenAI embeddings and we're going to\nopenAI embeddings and we're going to provide the embedding model text-\nprovide the embedding model text-\nprovide the embedding model text- embedding-large\nembedding-large\nembedding-large three. I think this was the identifier\nthree. I think this was the identifier\nthree. I think this was the identifier and we're going to say of course that\nand we're going to say of course that\nand we're going to say of course that this is the model. So model is equal to\nthis is the model. So model is equal to\nthis is the model. So model is equal to text embedding three large. Yeah. So\ntext embedding three large. Yeah. So\ntext embedding three large. Yeah. So actually three large not large three.\nactually three large not large three.\nactually three large not large three. Then I'm going to copy paste some very\nThen I'm going to copy paste some very\nThen I'm going to copy paste some very very simple statements. Nothing too\nvery simple statements. Nothing too\nvery simple statements. Nothing too fancy. Apple makes very good computers.\nfancy. Apple makes very good computers.\nfancy. Apple makes very good computers. Whether that's true or not, I'm just\nWhether that's true or not, I'm just\nWhether that's true or not, I'm just stating it here. I believe Apple is\nstating it here. I believe Apple is\nstating it here. I believe Apple is innovative. I love apples. I'm a fan of\ninnovative. I love apples. I'm a fan of\ninnovative. I love apples. I'm a fan of MacBooks. I enjoy oranges. I like Lenovo\nMacBooks. I enjoy oranges. I like Lenovo\nMacBooks. I enjoy oranges. I like Lenovo ThinkPads. I think pairs taste very\nThinkPads. I think pairs taste very\nThinkPads. I think pairs taste very good. So, theoretically, this message\ngood. So, theoretically, this message\ngood. So, theoretically, this message here, I like Lenovo ThinkPads, should be\nhere, I like Lenovo ThinkPads, should be\nhere, I like Lenovo ThinkPads, should be closer to I'm a fan of MacBooks. And\ncloser to I'm a fan of MacBooks. And\ncloser to I'm a fan of MacBooks. And even to stuff like Apple makes very good\neven to stuff like Apple makes very good\neven to stuff like Apple makes very good computers. uh it should be closer and\ncomputers. uh it should be closer and\ncomputers. uh it should be closer and more related than I love apples because\nmore related than I love apples because\nmore related than I love apples because that has nothing to do with apple only\nthat has nothing to do with apple only\nthat has nothing to do with apple only syntactically only in terms of like the\nsyntactically only in terms of like the\nsyntactically only in terms of like the name it has to do something with apple\nname it has to do something with apple\nname it has to do something with apple but it's a different concept. So we're\nbut it's a different concept. So we're\nbut it's a different concept. So we're going to create a vector store from\ngoing to create a vector store from\ngoing to create a vector store from these texts. I'm going to say vector\nthese texts. I'm going to say vector\nthese texts. I'm going to say vector store is going to be equal to face dot\nstore is going to be equal to face dot\nstore is going to be equal to face dot from texts and we're going to pass the\nfrom texts and we're going to pass the\nfrom texts and we're going to pass the list of texts here. Now in order for\nlist of texts here. Now in order for\nlist of texts here. Now in order for this to actually work we need to pass an\nthis to actually work we need to pass an\nthis to actually work we need to pass an embedding model as well. So embedding is\nembedding model as well. So embedding is\nembedding model as well. So embedding is going to be equal to embeddings. And\ngoing to be equal to embeddings. And\ngoing to be equal to embeddings. And this is going to basically take all\nthis is going to basically take all\nthis is going to basically take all these texts, embed them into vector\nthese texts, embed them into vector\nthese texts, embed them into vector space and then store them in the vector\nspace and then store them in the vector\nspace and then store them in the vector store. So to see how this works, we can\nstore. So to see how this works, we can\nstore. So to see how this works, we can say print vector store dot similarity\nsay print vector store dot similarity\nsay print vector store dot similarity search and then I can add something new\nsearch and then I can add something new\nsearch and then I can add something new like apples\nlike apples\nlike apples are my favorite\nare my favorite\nare my favorite food. K equals 7 to get all of them uh\nfood. K equals 7 to get all of them uh\nfood. K equals 7 to get all of them uh just in a specific ranking. And then I'm\njust in a specific ranking. And then I'm\njust in a specific ranking. And then I'm going to do the same thing with Linux is\ngoing to do the same thing with Linux is\ngoing to do the same thing with Linux is a great operating system. Of course,\na great operating system. Of course,\na great operating system. Of course, Linux is just a kernel, but you get the\nLinux is just a kernel, but you get the\nLinux is just a kernel, but you get the idea. So, I'm going to comment this out\nidea. So, I'm going to comment this out\nidea. So, I'm going to comment this out and run the first one alone to see what\nand run the first one alone to see what\nand run the first one alone to see what are the most similar statements in the\nare the most similar statements in the\nare the most similar statements in the vector store. So, you can see here the\nvector store. So, you can see here the\nvector store. So, you can see here the most similar was I love apples. Then the\nmost similar was I love apples. Then the\nmost similar was I love apples. Then the second one was I think pears taste very\nsecond one was I think pears taste very\nsecond one was I think pears taste very good. So, even though the word apple\ngood. So, even though the word apple\ngood. So, even though the word apple occurred, we or actually there you go. I\noccurred, we or actually there you go. I\noccurred, we or actually there you go. I enjoy oranges was before that then I\nenjoy oranges was before that then I\nenjoy oranges was before that then I think pears taste very good and then\nthink pears taste very good and then\nthink pears taste very good and then only it gives us Apple makes very good\nonly it gives us Apple makes very good\nonly it gives us Apple makes very good computers. I know this is not the best\ncomputers. I know this is not the best\ncomputers. I know this is not the best view. Uh but we have these individual\nview. Uh but we have these individual\nview. Uh but we have these individual documents the most similar the second\ndocuments the most similar the second\ndocuments the most similar the second most similar the third most similar and\nmost similar the third most similar and\nmost similar the third most similar and you can see that everything related to\nyou can see that everything related to\nyou can see that everything related to fruits was ranked as more important and\nfruits was ranked as more important and\nfruits was ranked as more important and more relevant than apple. Even though\nmore relevant than apple. Even though\nmore relevant than apple. Even though that apple here and apple here is the\nthat apple here and apple here is the\nthat apple here and apple here is the same string it recognizes that the\nsame string it recognizes that the\nsame string it recognizes that the concepts are different. So let's see\nconcepts are different. So let's see\nconcepts are different. So let's see what we get for Linux. We get I like\nwhat we get for Linux. We get I like\nwhat we get for Linux. We get I like Lenovo ThinkPads. I'm a fan of MacBooks.\nLenovo ThinkPads. I'm a fan of MacBooks.\nLenovo ThinkPads. I'm a fan of MacBooks. And then we get I love apples. For\nAnd then we get I love apples. For\nAnd then we get I love apples. For whatever reason, the company Apple is\nwhatever reason, the company Apple is\nwhatever reason, the company Apple is ranked lower. Whatever. So that is how\nranked lower. Whatever. So that is how\nranked lower. Whatever. So that is how you can use the vector store as a\nyou can use the vector store as a\nyou can use the vector store as a separate component. So just interact\nseparate component. So just interact\nseparate component. So just interact with embeddings and vector stores. But\nwith embeddings and vector stores. But\nwith embeddings and vector stores. But now what we want to do is we want to\nnow what we want to do is we want to\nnow what we want to do is we want to make this part of agents with lang. So\nmake this part of agents with lang. So\nmake this part of agents with lang. So we want to take this capability of using\nwe want to take this capability of using\nwe want to take this capability of using this rack feature of doing a similarity\nthis rack feature of doing a similarity\nthis rack feature of doing a similarity search. We want to take this and build\nsearch. We want to take this and build\nsearch. We want to take this and build it into our agent. For this, I want to\nit into our agent. For this, I want to\nit into our agent. For this, I want to use a slightly different example. So,\nuse a slightly different example. So,\nuse a slightly different example. So, the code is the same. We still have the\nthe code is the same. We still have the\nthe code is the same. We still have the same structure here of embeddings,\nsame structure here of embeddings,\nsame structure here of embeddings, texts, and then from text similarity\ntexts, and then from text similarity\ntexts, and then from text similarity search, but we have different content.\nsearch, but we have different content.\nsearch, but we have different content. We have I love apples. I enjoy oranges.\nWe have I love apples. I enjoy oranges.\nWe have I love apples. I enjoy oranges. I think pears taste very good. I hate\nI think pears taste very good. I hate\nI think pears taste very good. I hate bananas. I dislike raspberries. I\nbananas. I dislike raspberries. I\nbananas. I dislike raspberries. I despise mangoes. I love Linux. I hate\ndespise mangoes. I love Linux. I hate\ndespise mangoes. I love Linux. I hate Windows. And then we have here again the\nWindows. And then we have here again the\nWindows. And then we have here again the vector store. And here we look for what\nvector store. And here we look for what\nvector store. And here we look for what fruits does the person like and what\nfruits does the person like and what\nfruits does the person like and what fruits does the person hate. So if I run\nfruits does the person hate. So if I run\nfruits does the person hate. So if I run this, you can see that these two lookups\nthis, you can see that these two lookups\nthis, you can see that these two lookups give us the information that we want. I\ngive us the information that we want. I\ngive us the information that we want. I enjoy oranges. I love apples. I think\nenjoy oranges. I love apples. I think\nenjoy oranges. I love apples. I think pears taste very good. Then I despise\npears taste very good. Then I despise\npears taste very good. Then I despise mangoes. I hate bananas. I dislike\nmangoes. I hate bananas. I dislike\nmangoes. I hate bananas. I dislike raspberries. So we have the information\nraspberries. So we have the information\nraspberries. So we have the information by using the similarity search. What we\nby using the similarity search. What we\nby using the similarity search. What we can do now with this vector store is we\ncan do now with this vector store is we\ncan do now with this vector store is we can turn it into a retriever for our\ncan turn it into a retriever for our\ncan turn it into a retriever for our agent. So I can say down here retriever\nagent. So I can say down here retriever\nagent. So I can say down here retriever is equal to vector store as retriever.\nis equal to vector store as retriever.\nis equal to vector store as retriever. And we're just going to pass the search\nAnd we're just going to pass the search\nAnd we're just going to pass the search keyword arguments. So search_quarks\nkeyword arguments. So search_quarks\nkeyword arguments. So search_quarks is going to be equal to a dictionary\nis going to be equal to a dictionary\nis going to be equal to a dictionary where K is going to be equal to three.\nwhere K is going to be equal to three.\nwhere K is going to be equal to three. So this basically tells us just give me\nSo this basically tells us just give me\nSo this basically tells us just give me the top three answers. We're hard coding\nthe top three answers. We're hard coding\nthe top three answers. We're hard coding this to keep it simple here. But\nthis to keep it simple here. But\nthis to keep it simple here. But basically this now is a retriever that\nbasically this now is a retriever that\nbasically this now is a retriever that we can use in our agent. But in order to\nwe can use in our agent. But in order to\nwe can use in our agent. But in order to actually use this retriever in our\nactually use this retriever in our\nactually use this retriever in our agent, we need to turn it into a tool.\nagent, we need to turn it into a tool.\nagent, we need to turn it into a tool. So what we're going to do is another old\nSo what we're going to do is another old\nSo what we're going to do is another old school import up here from\nschool import up here from\nschool import up here from langchain_core.\nlangchain_core.\nlangchain_core. tools. We're going to import the create\ntools. We're going to import the create\ntools. We're going to import the create retriever tool. So the idea is we pass a\nretriever tool. So the idea is we pass a\nretriever tool. So the idea is we pass a retriever and we turn it into a tool\nretriever and we turn it into a tool\nretriever and we turn it into a tool that the agent can use. So quite simple,\nthat the agent can use. So quite simple,\nthat the agent can use. So quite simple, the retriever\nthe retriever\nthe retriever tool is going to be equal to create\ntool is going to be equal to create\ntool is going to be equal to create retriever tool. We pass the retriever\nretriever tool. We pass the retriever\nretriever tool. We pass the retriever and then we pass the name which is going\nand then we pass the name which is going\nand then we pass the name which is going to be our fruit search I guess or\nto be our fruit search I guess or\nto be our fruit search I guess or actually let's call it knowledgebased\nactually let's call it knowledgebased\nactually let's call it knowledgebased search. So KB search and then the\nsearch. So KB search and then the\nsearch. So KB search and then the description is search the small\ndescription is search the small\ndescription is search the small product/fruit\nproduct/fruit\nproduct/fruit database for information. Let me call\ndatabase for information. Let me call\ndatabase for information. Let me call this knowledge base and then we can\nthis knowledge base and then we can\nthis knowledge base and then we can close that. And now the cool thing is\nclose that. And now the cool thing is\nclose that. And now the cool thing is this is now just another tool. We can\nthis is now just another tool. We can\nthis is now just another tool. We can just add it to the list of tools when we\njust add it to the list of tools when we\njust add it to the list of tools when we create an agent. So for this we need to\ncreate an agent. So for this we need to\ncreate an agent. So for this we need to import again from langchain. This is now\nimport again from langchain. This is now\nimport again from langchain. This is now the modern way of doing things from\nthe modern way of doing things from\nthe modern way of doing things from langchain.agents agents import create\nlangchain.agents agents import create\nlangchain.agents agents import create agent and now I can just say as we did\nagent and now I can just say as we did\nagent and now I can just say as we did before agent is equal to create agent\nbefore agent is equal to create agent\nbefore agent is equal to create agent the model is going to be GPT4.1 mini\nthe model is going to be GPT4.1 mini\nthe model is going to be GPT4.1 mini again then we can say tools is equal to\nagain then we can say tools is equal to\nagain then we can say tools is equal to retriever tool then I'm going to copy\nretriever tool then I'm going to copy\nretriever tool then I'm going to copy paste the system prompt nothing too\npaste the system prompt nothing too\npaste the system prompt nothing too fancy here just explaining again your\nfancy here just explaining again your\nfancy here just explaining again your helpful assistant and if there's any\nhelpful assistant and if there's any\nhelpful assistant and if there's any questions about Macs apples laptops\nquestions about Macs apples laptops\nquestions about Macs apples laptops whatever use the tool that you have\nwhatever use the tool that you have\nwhatever use the tool that you have first the retriever tool basically\nfirst the retriever tool basically\nfirst the retriever tool basically retrieve the context answer in a concise\nretrieve the context answer in a concise\nretrieve the context answer in a concise way and the hint that maybe you have to\nway and the hint that maybe you have to\nway and the hint that maybe you have to use this tool multiple times because if\nuse this tool multiple times because if\nuse this tool multiple times because if I'm asking a question that needs to\nI'm asking a question that needs to\nI'm asking a question that needs to combine information, you might have to\ncombine information, you might have to\ncombine information, you might have to do it multiple times, which the agent\ndo it multiple times, which the agent\ndo it multiple times, which the agent can do since it's not just a prompt.\ncan do since it's not just a prompt.\ncan do since it's not just a prompt. It's an agent that can use tools, then\nIt's an agent that can use tools, then\nIt's an agent that can use tools, then think, then use tools again, and so on.\nthink, then use tools again, and so on.\nthink, then use tools again, and so on. So then we do what we already did\nSo then we do what we already did\nSo then we do what we already did before. We pass a prompt. I'm just going\nbefore. We pass a prompt. I'm just going\nbefore. We pass a prompt. I'm just going to say here, agent invoke, what three\nto say here, agent invoke, what three\nto say here, agent invoke, what three fruits does the person like and what\nfruits does the person like and what\nfruits does the person like and what three fruits does the person dislike?\nthree fruits does the person dislike?\nthree fruits does the person dislike? And then we get the answer. So, I'm\nAnd then we get the answer. So, I'm\nAnd then we get the answer. So, I'm asking two things in a single prompt.\nasking two things in a single prompt.\nasking two things in a single prompt. And up here, we use two different\nAnd up here, we use two different\nAnd up here, we use two different similarity searches for that. Let's see\nsimilarity searches for that. Let's see\nsimilarity searches for that. Let's see if the agent can handle that. I'm going\nif the agent can handle that. I'm going\nif the agent can handle that. I'm going to run this. And it says here, the\nto run this. And it says here, the\nto run this. And it says here, the person likes oranges, apples, and pears.\nperson likes oranges, apples, and pears.\nperson likes oranges, apples, and pears. The person dislikes mangoes,\nThe person dislikes mangoes,\nThe person dislikes mangoes, raspberries, and bananas. You can see\nraspberries, and bananas. You can see\nraspberries, and bananas. You can see also the retrieval um or actually cannot\nalso the retrieval um or actually cannot\nalso the retrieval um or actually cannot see it up here. You can see the\nsee it up here. You can see the\nsee it up here. You can see the retrieval that was done one. And then a\nretrieval that was done one. And then a\nretrieval that was done one. And then a second call to the tool was done down\nsecond call to the tool was done down\nsecond call to the tool was done down here. And this gave us the information\nhere. And this gave us the information\nhere. And this gave us the information for this final response. Now we get to a\nfor this final response. Now we get to a\nfor this final response. Now we get to a very powerful concept in lang chain. The\nvery powerful concept in lang chain. The\nvery powerful concept in lang chain. The middleware. Middleware basically meaning\nmiddleware. Middleware basically meaning\nmiddleware. Middleware basically meaning it sits between request and response.\nit sits between request and response.\nit sits between request and response. And middleware allows us to do a lot of\nAnd middleware allows us to do a lot of\nAnd middleware allows us to do a lot of different things to enhance the\ndifferent things to enhance the\ndifferent things to enhance the capabilities of our agents. For example,\ncapabilities of our agents. For example,\ncapabilities of our agents. For example, we can choose different system prompts\nwe can choose different system prompts\nwe can choose different system prompts depending on the context. We can choose\ndepending on the context. We can choose\ndepending on the context. We can choose different models based on certain\ndifferent models based on certain\ndifferent models based on certain criteria. We can summarize stuff. We can\ncriteria. We can summarize stuff. We can\ncriteria. We can summarize stuff. We can have rate limits. All sorts of things\nhave rate limits. All sorts of things\nhave rate limits. All sorts of things can be done in between this window of\ncan be done in between this window of\ncan be done in between this window of getting a request and sending a\ngetting a request and sending a\ngetting a request and sending a response. We can do a lot of stuff\nresponse. We can do a lot of stuff\nresponse. We can do a lot of stuff behind the scenes. And instead of\nbehind the scenes. And instead of\nbehind the scenes. And instead of talking about this too long in a\ntalking about this too long in a\ntalking about this too long in a theoretical way, what I want to do here\ntheoretical way, what I want to do here\ntheoretical way, what I want to do here is I just want to show you a couple of\nis I just want to show you a couple of\nis I just want to show you a couple of examples of custom middleware of already\nexamples of custom middleware of already\nexamples of custom middleware of already existing middleware of different use\nexisting middleware of different use\nexisting middleware of different use cases and then you can just go and\ncases and then you can just go and\ncases and then you can just go and explore it yourself. So I want to get\nexplore it yourself. So I want to get\nexplore it yourself. So I want to get started with a simple example right\nstarted with a simple example right\nstarted with a simple example right away. I want to show you how we can swap\naway. I want to show you how we can swap\naway. I want to show you how we can swap the system prompt based on the level of\nthe system prompt based on the level of\nthe system prompt based on the level of expertise that the user has. So for this\nexpertise that the user has. So for this\nexpertise that the user has. So for this we're going to use context again and I'm\nwe're going to use context again and I'm\nwe're going to use context again and I'm going to start by importing from\ngoing to start by importing from\ngoing to start by importing from langchain.agents.m\nlangchain.agents.m\nlangchain.agents.m middleware the following things. model\nmiddleware the following things. model\nmiddleware the following things. model request, model response and in this case\nrequest, model response and in this case\nrequest, model response and in this case now the decorator called dynamic prompt.\nnow the decorator called dynamic prompt.\nnow the decorator called dynamic prompt. So this is middleware that is already\nSo this is middleware that is already\nSo this is middleware that is already implemented. So we don't have to build\nimplemented. So we don't have to build\nimplemented. So we don't have to build it and define it ourselves. But what we\nit and define it ourselves. But what we\nit and define it ourselves. But what we can do now is we can say that if the\ncan do now is we can say that if the\ncan do now is we can say that if the user has a certain role like he's an\nuser has a certain role like he's an\nuser has a certain role like he's an expert or he's a beginner or maybe even\nexpert or he's a beginner or maybe even\nexpert or he's a beginner or maybe even he's a child, we're going to say adjust\nhe's a child, we're going to say adjust\nhe's a child, we're going to say adjust your answer, adjust your style based on\nyour answer, adjust your style based on\nyour answer, adjust your style based on that. So we're going to create a data\nthat. So we're going to create a data\nthat. So we're going to create a data class again called context. And actually\nclass again called context. And actually\nclass again called context. And actually we need to again import here from data\nwe need to again import here from data\nwe need to again import here from data classes import data class. And this\nclasses import data class. And this\nclasses import data class. And this context this time will not contain the\ncontext this time will not contain the\ncontext this time will not contain the ID but the user role. So I'm going to\nID but the user role. So I'm going to\nID but the user role. So I'm going to say here data class user ro is going to\nsay here data class user ro is going to\nsay here data class user ro is going to be a string. And now we're going to\nbe a string. And now we're going to\nbe a string. And now we're going to write a function that takes in the\nwrite a function that takes in the\nwrite a function that takes in the context from the model request and\ncontext from the model request and\ncontext from the model request and returns a system prompt dynamically\nreturns a system prompt dynamically\nreturns a system prompt dynamically based on the user role. So I'm going to\nbased on the user role. So I'm going to\nbased on the user role. So I'm going to use here the decorator dynamic prompt.\nuse here the decorator dynamic prompt.\nuse here the decorator dynamic prompt. And we're going to call this function\nAnd we're going to call this function\nAnd we're going to call this function user ro prompt. For example, it will get\nuser ro prompt. For example, it will get\nuser ro prompt. For example, it will get a request as input which is a model\na request as input which is a model\na request as input which is a model request and then we return the string.\nrequest and then we return the string.\nrequest and then we return the string. So that is the structure and now we can\nSo that is the structure and now we can\nSo that is the structure and now we can take the context from the request. So I\ntake the context from the request. So I\ntake the context from the request. So I can say here the user role is going to\ncan say here the user role is going to\ncan say here the user role is going to be request runtime context do user role\nbe request runtime context do user role\nbe request runtime context do user role and we can do the same thing as before\nand we can do the same thing as before\nand we can do the same thing as before with the user ID just a match case\nwith the user ID just a match case\nwith the user ID just a match case statement. So match user role and then\nstatement. So match user role and then\nstatement. So match user role and then we can have different cases. For\nwe can have different cases. For\nwe can have different cases. For example, the user can be an expert. And\nexample, the user can be an expert. And\nexample, the user can be an expert. And in this case, we will return a different\nin this case, we will return a different\nin this case, we will return a different system prompt. Now, what I want to do\nsystem prompt. Now, what I want to do\nsystem prompt. Now, what I want to do here is I want to define a base prompt.\nhere is I want to define a base prompt.\nhere is I want to define a base prompt. So, we're going to have just a base\nSo, we're going to have just a base\nSo, we're going to have just a base prompt. You are a helpful and very\nprompt. You are a helpful and very\nprompt. You are a helpful and very concise assistant. And now, if it's an\nconcise assistant. And now, if it's an\nconcise assistant. And now, if it's an expert, we're going to say here, give me\nexpert, we're going to say here, give me\nexpert, we're going to say here, give me the base prompt. And then just add to\nthe base prompt. And then just add to\nthe base prompt. And then just add to it, provide detail\nit, provide detail\nit, provide detail technical responses. On the other hand,\ntechnical responses. On the other hand,\ntechnical responses. On the other hand, if we have a beginner, we're going to\nif we have a beginner, we're going to\nif we have a beginner, we're going to say keep your explanations\nsay keep your explanations\nsay keep your explanations simple and basic. And finally, to see\nsimple and basic. And finally, to see\nsimple and basic. And finally, to see the biggest difference, we're going to\nthe biggest difference, we're going to\nthe biggest difference, we're going to say case child. And it's going to\nsay case child. And it's going to\nsay case child. And it's going to basically be explain everything\nbasically be explain everything\nbasically be explain everything as if you were literally\nas if you were literally\nas if you were literally talking to a 5year-old. And then the\ntalking to a 5year-old. And then the\ntalking to a 5year-old. And then the default case would just be okay. If it's\ndefault case would just be okay. If it's\ndefault case would just be okay. If it's none of that, then we're just going to\nnone of that, then we're just going to\nnone of that, then we're just going to return the base prompt. Cool. So now\nreturn the base prompt. Cool. So now\nreturn the base prompt. Cool. So now basically we do the same thing as\nbasically we do the same thing as\nbasically we do the same thing as before, but this time we don't pass it\nbefore, but this time we don't pass it\nbefore, but this time we don't pass it as context. We don't pass it as a tool.\nas context. We don't pass it as a tool.\nas context. We don't pass it as a tool. We pass it as middleware. So I say here\nWe pass it as middleware. So I say here\nWe pass it as middleware. So I say here create agent. The model is going to be\ncreate agent. The model is going to be\ncreate agent. The model is going to be the same as before. GBT4.1\nthe same as before. GBT4.1\nthe same as before. GBT4.1 mini. And now I'm going to say here\nmini. And now I'm going to say here\nmini. And now I'm going to say here middleware is equal to and that's going\nmiddleware is equal to and that's going\nmiddleware is equal to and that's going to be a list and it's going to contain\nto be a list and it's going to contain\nto be a list and it's going to contain the user role prompt function. But of\nthe user role prompt function. But of\nthe user role prompt function. But of course I still have to pass the context\ncourse I still have to pass the context\ncourse I still have to pass the context otherwise it of course doesn't work. So\notherwise it of course doesn't work. So\notherwise it of course doesn't work. So context schema still has to be this data\ncontext schema still has to be this data\ncontext schema still has to be this data class. So then I can say response is\nclass. So then I can say response is\nclass. So then I can say response is equal to agent.invoke.\nequal to agent.invoke.\nequal to agent.invoke. I'm going to have to provide the\nI'm going to have to provide the\nI'm going to have to provide the dictionary again here with messages role\ndictionary again here with messages role\ndictionary again here with messages role is user and content is explain PCA. And\nis user and content is explain PCA. And\nis user and content is explain PCA. And here now I pass the context. The context\nhere now I pass the context. The context\nhere now I pass the context. The context is going to be that the user role let's\nis going to be that the user role let's\nis going to be that the user role let's say in the beginning is going to be\nsay in the beginning is going to be\nsay in the beginning is going to be beginner. So user role is equal to\nbeginner. So user role is equal to\nbeginner. So user role is equal to beginner. Print response. And we can see\nbeginner. Print response. And we can see\nbeginner. Print response. And we can see here content PCA principal component\nhere content PCA principal component\nhere content PCA principal component analysis is the technique used to reduce\nanalysis is the technique used to reduce\nanalysis is the technique used to reduce the number of variables in data while\nthe number of variables in data while\nthe number of variables in data while keeping the most important information.\nkeeping the most important information.\nkeeping the most important information. It transforms the original data into new\nIt transforms the original data into new\nIt transforms the original data into new variables and so on. So quite simple but\nvariables and so on. So quite simple but\nvariables and so on. So quite simple but still not something a 5-year-old would\nstill not something a 5-year-old would\nstill not something a 5-year-old would understand. So let's go and see what the\nunderstand. So let's go and see what the\nunderstand. So let's go and see what the export explanation would be. And there\nexport explanation would be. And there\nexport explanation would be. And there we can see principal component analysis\nwe can see principal component analysis\nwe can see principal component analysis is a dimensionality reduction technique\nis a dimensionality reduction technique\nis a dimensionality reduction technique used to transform a large set of\nused to transform a large set of\nused to transform a large set of correlated variables into a smaller set\ncorrelated variables into a smaller set\ncorrelated variables into a smaller set of uncorrelated variables called\nof uncorrelated variables called\nof uncorrelated variables called principal components. Yeah. Then we get\nprincipal components. Yeah. Then we get\nprincipal components. Yeah. Then we get here projection and component selection.\nhere projection and component selection.\nhere projection and component selection. Still not I mean I can decomposition.\nStill not I mean I can decomposition.\nStill not I mean I can decomposition. Okay, it's a little bit more technical.\nOkay, it's a little bit more technical.\nOkay, it's a little bit more technical. But if I go now to the lowest level here\nBut if I go now to the lowest level here\nBut if I go now to the lowest level here which is the child then I should get\nwhich is the child then I should get\nwhich is the child then I should get something really really simple. So here\nsomething really really simple. So here\nsomething really really simple. So here okay imagine you have a big box of\nokay imagine you have a big box of\nokay imagine you have a big box of crayons with lots of colors. Sometimes\ncrayons with lots of colors. Sometimes\ncrayons with lots of colors. Sometimes you want to choose just a few crayons\nyou want to choose just a few crayons\nyou want to choose just a few crayons that can still help you color many\nthat can still help you color many\nthat can still help you color many pictures nicely. PCA is like a magic\npictures nicely. PCA is like a magic\npictures nicely. PCA is like a magic helper. Yeah, you get the idea. So this\nhelper. Yeah, you get the idea. So this\nhelper. Yeah, you get the idea. So this is something we can do with middleware\nis something we can do with middleware\nis something we can do with middleware here. In between the request and\nhere. In between the request and\nhere. In between the request and response, we can have this dynamic\nresponse, we can have this dynamic\nresponse, we can have this dynamic prompt choice based on the user role,\nprompt choice based on the user role,\nprompt choice based on the user role, based on the context. We can say uh that\nbased on the context. We can say uh that\nbased on the context. We can say uh that we want to do different things. We want\nwe want to do different things. We want\nwe want to do different things. We want to instruct the model to do different\nto instruct the model to do different\nto instruct the model to do different things. Now, another thing that we can\nthings. Now, another thing that we can\nthings. Now, another thing that we can do is we can not only select a prompt\ndo is we can not only select a prompt\ndo is we can not only select a prompt dynamically, we can also select a model\ndynamically, we can also select a model\ndynamically, we can also select a model dynamically. For this here, I'm also\ndynamically. For this here, I'm also\ndynamically. For this here, I'm also going to import from langchain.hat\ngoing to import from langchain.hat\ngoing to import from langchain.hat models our trusted init chat model\nmodels our trusted init chat model\nmodels our trusted init chat model function. I'm going to get rid of\nfunction. I'm going to get rid of\nfunction. I'm going to get rid of dynamic prompt and I'm going to import\ndynamic prompt and I'm going to import\ndynamic prompt and I'm going to import wrap model call. This basically means it\nwrap model call. This basically means it\nwrap model call. This basically means it happens around the model call whenever\nhappens around the model call whenever\nhappens around the model call whenever we want to call the model that is part\nwe want to call the model that is part\nwe want to call the model that is part of the agent. We're going to run our\nof the agent. We're going to run our\nof the agent. We're going to run our middleware in that moment or I should\nmiddleware in that moment or I should\nmiddleware in that moment or I should say in between the model request and the\nsay in between the model request and the\nsay in between the model request and the model response. So the idea here would\nmodel response. So the idea here would\nmodel response. So the idea here would be I want to have a basic model that\nbe I want to have a basic model that\nbe I want to have a basic model that would be in it chat model and then I can\nwould be in it chat model and then I can\nwould be in it chat model and then I can say for example model is equal to GPT4\nsay for example model is equal to GPT4\nsay for example model is equal to GPT4 mini and then I would also have an\nmini and then I would also have an\nmini and then I would also have an advanced model which could be 4.1 mini.\nadvanced model which could be 4.1 mini.\nadvanced model which could be 4.1 mini. The idea now is I define a function\nThe idea now is I define a function\nThe idea now is I define a function called dynamic model selection. This\ncalled dynamic model selection. This\ncalled dynamic model selection. This function takes in a request returns a\nfunction takes in a request returns a\nfunction takes in a request returns a response. So I take in a model request\nresponse. So I take in a model request\nresponse. So I take in a model request and the output of that function is a\nand the output of that function is a\nand the output of that function is a model response. In addition to that we\nmodel response. In addition to that we\nmodel response. In addition to that we also get a handler as parameter here and\nalso get a handler as parameter here and\nalso get a handler as parameter here and I have to annotate this now with wrap\nI have to annotate this now with wrap\nI have to annotate this now with wrap model call. So this actually happens\nmodel call. So this actually happens\nmodel call. So this actually happens when the model is called. In case you're\nwhen the model is called. In case you're\nwhen the model is called. In case you're interested in that, you can also go to\ninterested in that, you can also go to\ninterested in that, you can also go to the langchain documentation. And there\nthe langchain documentation. And there\nthe langchain documentation. And there you can see how this basically works.\nyou can see how this basically works.\nyou can see how this basically works. Request comes in. Then we have before\nRequest comes in. Then we have before\nRequest comes in. Then we have before agent, before model, wrap tool call,\nagent, before model, wrap tool call,\nagent, before model, wrap tool call, wrap model call, after model, after\nwrap model call, after model, after\nwrap model call, after model, after agent, result. And we're now doing this\nagent, result. And we're now doing this\nagent, result. And we're now doing this here, wrapping the model call with our\nhere, wrapping the model call with our\nhere, wrapping the model call with our middleware. So what kind of logic you\nmiddleware. So what kind of logic you\nmiddleware. So what kind of logic you want to apply here is up to you. In my\nwant to apply here is up to you. In my\nwant to apply here is up to you. In my case, I'm going to keep it simple. If I\ncase, I'm going to keep it simple. If I\ncase, I'm going to keep it simple. If I have more than three messages, I'm going\nhave more than three messages, I'm going\nhave more than three messages, I'm going to use the stronger model. Otherwise,\nto use the stronger model. Otherwise,\nto use the stronger model. Otherwise, I'm going to use the basic model. Not\nI'm going to use the basic model. Not\nI'm going to use the basic model. Not the most intelligent choice here, but\nthe most intelligent choice here, but\nthe most intelligent choice here, but we're going to do it like this. So\nwe're going to do it like this. So\nwe're going to do it like this. So message count is going to be equal to\nmessage count is going to be equal to\nmessage count is going to be equal to length of request state and then\nlength of request state and then\nlength of request state and then messages and then just if the message\nmessages and then just if the message\nmessages and then just if the message count is greater than three the model is\ncount is greater than three the model is\ncount is greater than three the model is going to be equal to basic model\ngoing to be equal to basic model\ngoing to be equal to basic model otherwise we're going to say it's going\notherwise we're going to say it's going\notherwise we're going to say it's going to be the advanced model and then we say\nto be the advanced model and then we say\nto be the advanced model and then we say request domodel is equal to model and we\nrequest domodel is equal to model and we\nrequest domodel is equal to model and we return a handler for that request. So\nreturn a handler for that request. So\nreturn a handler for that request. So basically we're just taking the request\nbasically we're just taking the request\nbasically we're just taking the request and setting the model to something else\nand setting the model to something else\nand setting the model to something else depending on some criteria. This could\ndepending on some criteria. This could\ndepending on some criteria. This could be something more intelligent and then\nbe something more intelligent and then\nbe something more intelligent and then we just handle the request with this\nwe just handle the request with this\nwe just handle the request with this change. So that's super easy to\nchange. So that's super easy to\nchange. So that's super easy to integrate. Again agent is going to be\nintegrate. Again agent is going to be\nintegrate. Again agent is going to be equal to create agent. Model is going to\nequal to create agent. Model is going to\nequal to create agent. Model is going to be basic model by default. And then the\nbe basic model by default. And then the\nbe basic model by default. And then the middleware is going to be our dynamic\nmiddleware is going to be our dynamic\nmiddleware is going to be our dynamic model selection. Now to keep things a\nmodel selection. Now to keep things a\nmodel selection. Now to keep things a little bit more beautiful here, I'm\nlittle bit more beautiful here, I'm\nlittle bit more beautiful here, I'm going to import again from langchain\ngoing to import again from langchain\ngoing to import again from langchain messages the system message, the human\nmessages the system message, the human\nmessages the system message, the human message, and the AI message. And then\nmessage, and the AI message. And then\nmessage, and the AI message. And then down here, I'm going to say response is\ndown here, I'm going to say response is\ndown here, I'm going to say response is equal to agent invoke. I'm going to\nequal to agent invoke. I'm going to\nequal to agent invoke. I'm going to invoke here on the following dictionary.\ninvoke here on the following dictionary.\ninvoke here on the following dictionary. Messages is going to point to a list of\nMessages is going to point to a list of\nMessages is going to point to a list of messages. I'm going to start with a\nmessages. I'm going to start with a\nmessages. I'm going to start with a system message. You are a helpful\nsystem message. You are a helpful\nsystem message. You are a helpful assistant. Human message. What is\nassistant. Human message. What is\nassistant. Human message. What is oneplus 1? Keeping it very basic here\noneplus 1? Keeping it very basic here\noneplus 1? Keeping it very basic here and essentially we can take a look at\nand essentially we can take a look at\nand essentially we can take a look at two things. Uh on the one hand you can\ntwo things. Uh on the one hand you can\ntwo things. Uh on the one hand you can get the idea if you look at the quality\nget the idea if you look at the quality\nget the idea if you look at the quality of the output. I mean oneplus 1 is\nof the output. I mean oneplus 1 is\nof the output. I mean oneplus 1 is something that every model should be\nsomething that every model should be\nsomething that every model should be able to do. But you can say here again\nable to do. But you can say here again\nable to do. But you can say here again print response messages\nprint response messages\nprint response messages -1.content\n-1.content\n-1.content to just get the answer but then you can\nto just get the answer but then you can\nto just get the answer but then you can also take a look at the actual model\nalso take a look at the actual model\nalso take a look at the actual model that was used to produce this response.\nthat was used to produce this response.\nthat was used to produce this response. So instead of saying content you can say\nSo instead of saying content you can say\nSo instead of saying content you can say response metadata and you can target the\nresponse metadata and you can target the\nresponse metadata and you can target the field called model name. And if we run\nfield called model name. And if we run\nfield called model name. And if we run this now we can see that we use the\nthis now we can see that we use the\nthis now we can see that we use the wrong model. Why is that? Because we're\nwrong model. Why is that? Because we're\nwrong model. Why is that? Because we're we have to swap this. Of course if we\nwe have to swap this. Of course if we\nwe have to swap this. Of course if we have more messages we're using the\nhave more messages we're using the\nhave more messages we're using the advanced model otherwise the basic\nadvanced model otherwise the basic\nadvanced model otherwise the basic model. But in theory it works. So if I\nmodel. But in theory it works. So if I\nmodel. But in theory it works. So if I run this now I should see that we're\nrun this now I should see that we're\nrun this now I should see that we're using 40 mini. And if I add more\nusing 40 mini. And if I add more\nusing 40 mini. And if I add more messages. So, for example, let's just\nmessages. So, for example, let's just\nmessages. So, for example, let's just add the same message a couple of times\nadd the same message a couple of times\nadd the same message a couple of times here. This is going to trigger the\nhere. This is going to trigger the\nhere. This is going to trigger the change and now we're using 4.1 mini.\nchange and now we're using 4.1 mini.\nchange and now we're using 4.1 mini. Now, in general, I want to show you how\nNow, in general, I want to show you how\nNow, in general, I want to show you how you can define your own custom\nyou can define your own custom\nyou can define your own custom middleware by using these hooks. So,\nmiddleware by using these hooks. So,\nmiddleware by using these hooks. So, we're going to do that with a class\nwe're going to do that with a class\nwe're going to do that with a class here. For this, I'm going to create a\nhere. For this, I'm going to create a\nhere. For this, I'm going to create a class called hooks. Let's call it hooks\nclass called hooks. Let's call it hooks\nclass called hooks. Let's call it hooks demo. It's going to inherit from a class\ndemo. It's going to inherit from a class\ndemo. It's going to inherit from a class that we need to import called agent\nthat we need to import called agent\nthat we need to import called agent middleware. So, we're now defining our\nmiddleware. So, we're now defining our\nmiddleware. So, we're now defining our own agent middleware as a class here.\nown agent middleware as a class here.\nown agent middleware as a class here. And we can just override these methods\nAnd we can just override these methods\nAnd we can just override these methods that are representing the hooks. So in\nthat are representing the hooks. So in\nthat are representing the hooks. So in order to do that, let me get rid of all\norder to do that, let me get rid of all\norder to do that, let me get rid of all of this. We're also going to need\nof this. We're also going to need\nof this. We're also going to need something called the agent state. And\nsomething called the agent state. And\nsomething called the agent state. And basically the constructor is going to be\nbasically the constructor is going to be\nbasically the constructor is going to be the init method taking self as a\nthe init method taking self as a\nthe init method taking self as a parameter here calling the constructor\nparameter here calling the constructor\nparameter here calling the constructor of the parent class. So super init. And\nof the parent class. So super init. And\nof the parent class. So super init. And what we're going to do is we're going to\nwhat we're going to do is we're going to\nwhat we're going to do is we're going to set a start time equal to 0.0. So this\nset a start time equal to 0.0. So this\nset a start time equal to 0.0. So this is not going to be an actually useful\nis not going to be an actually useful\nis not going to be an actually useful example. I'm just going to show you how\nexample. I'm just going to show you how\nexample. I'm just going to show you how we can trigger when the different parts\nwe can trigger when the different parts\nwe can trigger when the different parts are triggered. And now we can just\nare triggered. And now we can just\nare triggered. And now we can just overwrite before agent. For example,\noverwrite before agent. For example,\noverwrite before agent. For example, before agent is obviously before we get\nbefore agent is obviously before we get\nbefore agent is obviously before we get to the agent, before the request comes\nto the agent, before the request comes\nto the agent, before the request comes to the agent, we're going to be running\nto the agent, we're going to be running\nto the agent, we're going to be running this. It takes self and an agent state.\nthis. It takes self and an agent state.\nthis. It takes self and an agent state. So state, which is agent state, and also\nSo state, which is agent state, and also\nSo state, which is agent state, and also the runtime. And in my case, what I want\nthe runtime. And in my case, what I want\nthe runtime. And in my case, what I want to do here now is I want to say self dot\nto do here now is I want to say self dot\nto do here now is I want to say self dot start time is going to be equal to and\nstart time is going to be equal to and\nstart time is going to be equal to and now I need to import from core python\nnow I need to import from core python\nnow I need to import from core python here time. I'm just going to say time.\nhere time. I'm just going to say time.\nhere time. I'm just going to say time. So we're importing core python package\nSo we're importing core python package\nSo we're importing core python package time. self.start time is equal to time.\ntime. self.start time is equal to time.\ntime. self.start time is equal to time. And then just so we know it happened.\nAnd then just so we know it happened.\nAnd then just so we know it happened. I'm going to say here before agent\nI'm going to say here before agent\nI'm going to say here before agent triggered. Now I'm going to copy paste\ntriggered. Now I'm going to copy paste\ntriggered. Now I'm going to copy paste it here. For the other ones, it's just\nit here. For the other ones, it's just\nit here. For the other ones, it's just before model, after model, and after\nbefore model, after model, and after\nbefore model, after model, and after agent. Here we're going to do something\nagent. Here we're going to do something\nagent. Here we're going to do something else. So I didn't copy it. But basically\nelse. So I didn't copy it. But basically\nelse. So I didn't copy it. But basically here we're just printing before model,\nhere we're just printing before model,\nhere we're just printing before model, after model. Whatever you want to do\nafter model. Whatever you want to do\nafter model. Whatever you want to do with that. It's basically also you have\nwith that. It's basically also you have\nwith that. It's basically also you have wrap and so wrap tool call, wrap model\nwrap and so wrap tool call, wrap model\nwrap and so wrap tool call, wrap model call. We saw that already. We're just\ncall. We saw that already. We're just\ncall. We saw that already. We're just going to keep doing it with these four.\ngoing to keep doing it with these four.\ngoing to keep doing it with these four. And what I'm going to do here is I'm now\nAnd what I'm going to do here is I'm now\nAnd what I'm going to do here is I'm now going to calculate the time difference.\ngoing to calculate the time difference.\ngoing to calculate the time difference. And I'm going to print the result. So\nAnd I'm going to print the result. So\nAnd I'm going to print the result. So I'm going to say here print\nI'm going to say here print\nI'm going to say here print after agent and then colon time. So the\nafter agent and then colon time. So the\nafter agent and then colon time. So the current time minus self start time. And\ncurrent time minus self start time. And\ncurrent time minus self start time. And now we can go and say agent is equal to\nnow we can go and say agent is equal to\nnow we can go and say agent is equal to create agent GPT4.1 mini. And then we\ncreate agent GPT4.1 mini. And then we\ncreate agent GPT4.1 mini. And then we can say middleware is equal to and now\ncan say middleware is equal to and now\ncan say middleware is equal to and now we're going to pass hooks demo as an\nwe're going to pass hooks demo as an\nwe're going to pass hooks demo as an instance here. So we're creating\ninstance here. So we're creating\ninstance here. So we're creating instance of this class. This is another\ninstance of this class. This is another\ninstance of this class. This is another way to use middleware. And then I can\nway to use middleware. And then I can\nway to use middleware. And then I can just do the usual stuff from before just\njust do the usual stuff from before just\njust do the usual stuff from before just agent invoke some question. And now when\nagent invoke some question. And now when\nagent invoke some question. And now when I run this we get a problem of course\nI run this we get a problem of course\nI run this we get a problem of course because I cannot just call the module\nbecause I cannot just call the module\nbecause I cannot just call the module time. Of course I need to say time or\ntime. Of course I need to say time or\ntime. Of course I need to say time or time perf counter whatever you want to\ntime perf counter whatever you want to\ntime perf counter whatever you want to use here but that shouldn't be too much\nuse here but that shouldn't be too much\nuse here but that shouldn't be too much of an issue. So now we get before agent\nof an issue. So now we get before agent\nof an issue. So now we get before agent triggered before model. I forgot to add\ntriggered before model. I forgot to add\ntriggered before model. I forgot to add trigger here. But then we should get at\ntrigger here. But then we should get at\ntrigger here. But then we should get at some point after model and then we\nsome point after model and then we\nsome point after model and then we should get after agent and the\nshould get after agent and the\nshould get after agent and the difference here in time. And since PCA\ndifference here in time. And since PCA\ndifference here in time. And since PCA is probably quite complicated uh for a\nis probably quite complicated uh for a\nis probably quite complicated uh for a model to put into words. It takes some\nmodel to put into words. It takes some\nmodel to put into words. It takes some time but you can see 14 seconds is what\ntime but you can see 14 seconds is what\ntime but you can see 14 seconds is what we measured here. This is essentially\nwe measured here. This is essentially\nwe measured here. This is essentially what you can do with these hooks in um\nwhat you can do with these hooks in um\nwhat you can do with these hooks in um lang chain when it comes to middleware.\nlang chain when it comes to middleware.\nlang chain when it comes to middleware. You can customize what happens at what\nYou can customize what happens at what\nYou can customize what happens at what point in the workflow in the cycle. So\npoint in the workflow in the cycle. So\npoint in the workflow in the cycle. So now, last but not least, I would just\nnow, last but not least, I would just\nnow, last but not least, I would just like to show you a couple of examples of\nlike to show you a couple of examples of\nlike to show you a couple of examples of already existing middleware. We're not\nalready existing middleware. We're not\nalready existing middleware. We're not going to go too deep into this. I'm\ngoing to go too deep into this. I'm\ngoing to go too deep into this. I'm going to just copy paste one of these\ngoing to just copy paste one of these\ngoing to just copy paste one of these examples here into this uh file, which\nexamples here into this uh file, which\nexamples here into this uh file, which is going to be summarizing conversation.\nis going to be summarizing conversation.\nis going to be summarizing conversation. We're not going to actually use this.\nWe're not going to actually use this.\nWe're not going to actually use this. I'm just showing you how to use this.\nI'm just showing you how to use this.\nI'm just showing you how to use this. Basically, you go from langchain agents\nBasically, you go from langchain agents\nBasically, you go from langchain agents middleware summarization middleware. One\nmiddleware summarization middleware. One\nmiddleware summarization middleware. One of the many choices that you have. What\nof the many choices that you have. What\nof the many choices that you have. What this basically does is you have a couple\nthis basically does is you have a couple\nthis basically does is you have a couple of parameters here. You can specify the\nof parameters here. You can specify the\nof parameters here. You can specify the model for the summarization, which is\nmodel for the summarization, which is\nmodel for the summarization, which is not necessarily the same as the model\nnot necessarily the same as the model\nnot necessarily the same as the model that the agent uses. The idea is that\nthat the agent uses. The idea is that\nthat the agent uses. The idea is that after a certain number of tokens, you\nafter a certain number of tokens, you\nafter a certain number of tokens, you summarize the conversation. You keep the\nsummarize the conversation. You keep the\nsummarize the conversation. You keep the last 20 messages after 4,000 tokens and\nlast 20 messages after 4,000 tokens and\nlast 20 messages after 4,000 tokens and GPT40 mini in this case summarizes the\nGPT40 mini in this case summarizes the\nGPT40 mini in this case summarizes the important key points of the conversation\nimportant key points of the conversation\nimportant key points of the conversation up until this point. And then you just\nup until this point. And then you just\nup until this point. And then you just continue the conversation. That is one\ncontinue the conversation. That is one\ncontinue the conversation. That is one middleware that we can use. And then on\nmiddleware that we can use. And then on\nmiddleware that we can use. And then on the middleware page in the\nthe middleware page in the\nthe middleware page in the documentation, you can also find some\ndocumentation, you can also find some\ndocumentation, you can also find some examples. For example, here human in the\nexamples. For example, here human in the\nexamples. For example, here human in the loop middleware. This basically means on\nloop middleware. This basically means on\nloop middleware. This basically means on certain events on certain tool users, we\ncertain events on certain tool users, we\ncertain events on certain tool users, we get an interrupt and then the user has\nget an interrupt and then the user has\nget an interrupt and then the user has to manually say okay continue, edit,\nto manually say okay continue, edit,\nto manually say okay continue, edit, approve, reject, whatever. This is of\napprove, reject, whatever. This is of\napprove, reject, whatever. This is of course important in key steps. If you\ncourse important in key steps. If you\ncourse important in key steps. If you have to do some payment, if you have to\nhave to do some payment, if you have to\nhave to do some payment, if you have to send an email or something, it makes\nsend an email or something, it makes\nsend an email or something, it makes sense to have this human in the loop\nsense to have this human in the loop\nsense to have this human in the loop middleware. Then we also have model call\nmiddleware. Then we also have model call\nmiddleware. Then we also have model call limit. So basically saying we cannot\nlimit. So basically saying we cannot\nlimit. So basically saying we cannot call the model too often per run or per\ncall the model too often per run or per\ncall the model too often per run or per threat. Same thing exists for tool\nthreat. Same thing exists for tool\nthreat. Same thing exists for tool calls. Basically just a limiter for how\ncalls. Basically just a limiter for how\ncalls. Basically just a limiter for how often you can call a tool in a run or in\noften you can call a tool in a run or in\noften you can call a tool in a run or in a limit. Same thing also exists for tool\na limit. Same thing also exists for tool\na limit. Same thing also exists for tool calls. Basically how often can you call\ncalls. Basically how often can you call\ncalls. Basically how often can you call a tool in a specific thread or run. Then\na tool in a specific thread or run. Then\na tool in a specific thread or run. Then also some interesting stuff here like\nalso some interesting stuff here like\nalso some interesting stuff here like model fallback. If something doesn't\nmodel fallback. If something doesn't\nmodel fallback. If something doesn't work, if you cannot use a model, fall\nwork, if you cannot use a model, fall\nwork, if you cannot use a model, fall back to a different one. This can be\nback to a different one. This can be\nback to a different one. This can be quite useful here. Also PII detection.\nquite useful here. Also PII detection.\nquite useful here. Also PII detection. Basically personally identifiable\nBasically personally identifiable\nBasically personally identifiable information. You can make it redact\ninformation. You can make it redact\ninformation. You can make it redact certain key pieces of information for\ncertain key pieces of information for\ncertain key pieces of information for compliance reasons for example and much\ncompliance reasons for example and much\ncompliance reasons for example and much more other stuff like retrying tools or\nmore other stuff like retrying tools or\nmore other stuff like retrying tools or having a to-do list. So basically a\nhaving a to-do list. So basically a\nhaving a to-do list. So basically a planning middleware. You can take a look\nplanning middleware. You can take a look\nplanning middleware. You can take a look at these examples but basically\nat these examples but basically\nat these examples but basically middleware allows you to extend the\nmiddleware allows you to extend the\nmiddleware allows you to extend the capabilities of your model or of your\ncapabilities of your model or of your\ncapabilities of your model or of your agent I should say because you can tell\nagent I should say because you can tell\nagent I should say because you can tell it what to do in certain scenarios make\nit what to do in certain scenarios make\nit what to do in certain scenarios make it more dynamic make it more reactive so\nit more dynamic make it more reactive so\nit more dynamic make it more reactive so to say which is very useful. So that's\nto say which is very useful. So that's\nto say which is very useful. So that's it for this video today. I hope you\nit for this video today. I hope you\nit for this video today. I hope you enjoyed it and hope you learned\nenjoyed it and hope you learned\nenjoyed it and hope you learned something. If so, let me know by hitting\nsomething. If so, let me know by hitting\nsomething. If so, let me know by hitting a like button and leaving a comment in\na like button and leaving a comment in\na like button and leaving a comment in the comment section down below. Also, if\nthe comment section down below. Also, if\nthe comment section down below. Also, if you're interested on my website, you\nyou're interested on my website, you\nyou're interested on my website, you will find a tutoring tab and also a\nwill find a tutoring tab and also a\nwill find a tutoring tab and also a services tab. There you can hire me\nservices tab. There you can hire me\nservices tab. There you can hire me basically for one-on-one tutoring, for\nbasically for one-on-one tutoring, for\nbasically for one-on-one tutoring, for one-on-one teaching you something or\none-on-one teaching you something or\none-on-one teaching you something or also for services, machine learning,\nalso for services, machine learning,\nalso for services, machine learning, backend development, consulting,\nbackend development, consulting,\nbackend development, consulting, freelancing, whatever. If you're\nfreelancing, whatever. If you're\nfreelancing, whatever. If you're interested in that, check it out. You\ninterested in that, check it out. You\ninterested in that, check it out. You can contact me via mail or LinkedIn. And\ncan contact me via mail or LinkedIn. And\ncan contact me via mail or LinkedIn. And besides that, don't forget to subscribe\nbesides that, don't forget to subscribe\nbesides that, don't forget to subscribe to this channel and hit the notification\nto this channel and hit the notification\nto this channel and hit the notification bell to not miss a single future video\nbell to not miss a single future video\nbell to not miss a single future video for free. Other than that, thank you\nfor free. Other than that, thank you\nfor free. Other than that, thank you much for watching. See you in the next\nmuch for watching. See you in the next\nmuch for watching. See you in the next video and bye."
  },
  {
    "id": 60494622,
    "timestamp": "2026-02-25T08:15:30.100Z",
    "title": "Updated Langchain Version V1 Crash Course- Build Autonomous Agents",
    "url": "https://www.youtube.com/watch?v=vzJOAnwIokM&t=74s",
    "text": "Hello all, my name is Krishna and\nHello all, my name is Krishna and welcome to my YouTube channel. So guys,\nwelcome to my YouTube channel. So guys,\nwelcome to my YouTube channel. So guys, uh if you have been following my lang\nuh if you have been following my lang\nuh if you have been following my lang chain playlist, my langraph playlist,\nchain playlist, my langraph playlist,\nchain playlist, my langraph playlist, I've uploaded tons and tons of videos.\nI've uploaded tons and tons of videos.\nI've uploaded tons and tons of videos. Uh I have made end to-end projects. I\nUh I have made end to-end projects. I\nUh I have made end to-end projects. I have taught each and everything\nhave taught each and everything\nhave taught each and everything specifically uh on this particular\nspecifically uh on this particular\nspecifically uh on this particular frameworks. Now uh this particular video\nframeworks. Now uh this particular video\nframeworks. Now uh this particular video is just like a oneshot video uh on\nis just like a oneshot video uh on\nis just like a oneshot video uh on langchain itself because recently lang\nlangchain itself because recently lang\nlangchain itself because recently lang has come up with its uh recent version\nhas come up with its uh recent version\nhas come up with its uh recent version that is version v1 and uh there are some\nthat is version v1 and uh there are some\nthat is version v1 and uh there are some various changes specifically in terms of\nvarious changes specifically in terms of\nvarious changes specifically in terms of creating agents applying memories. Uh\ncreating agents applying memories. Uh\ncreating agents applying memories. Uh there is a new concepts that have uh\nthere is a new concepts that have uh\nthere is a new concepts that have uh recently been come that is called as\nrecently been come that is called as\nrecently been come that is called as middleware. So considering all these\nmiddleware. So considering all these\nmiddleware. So considering all these things I thought why not make a oneshot\nthings I thought why not make a oneshot\nthings I thought why not make a oneshot video with all the recent updates and uh\nvideo with all the recent updates and uh\nvideo with all the recent updates and uh you can watch this entire tutorial.\nyou can watch this entire tutorial.\nyou can watch this entire tutorial. It'll be a longer tutorial where I have\nIt'll be a longer tutorial where I have\nIt'll be a longer tutorial where I have included each and everything. So uh go\nincluded each and everything. So uh go\nincluded each and everything. So uh go ahead enjoy this and make sure to hit\nahead enjoy this and make sure to hit\nahead enjoy this and make sure to hit like uh we'll keep a like target of\nlike uh we'll keep a like target of\nlike uh we'll keep a like target of thousand so that after completing this\nthousand so that after completing this\nthousand so that after completing this particular video I'm also parallely\nparticular video I'm also parallely\nparticular video I'm also parallely recording the updates with respect to\nrecording the updates with respect to\nrecording the updates with respect to langraph and there is one more new topic\nlangraph and there is one more new topic\nlangraph and there is one more new topic that is coming which is called as deep\nthat is coming which is called as deep\nthat is coming which is called as deep agents. So everything will be getting\nagents. So everything will be getting\nagents. So everything will be getting recorded as we go ahead. So go ahead\nrecorded as we go ahead. So go ahead\nrecorded as we go ahead. So go ahead enjoy this particular crash course on\nenjoy this particular crash course on\nenjoy this particular crash course on Langchen version B1. Hello guys. So\nLangchen version B1. Hello guys. So\nLangchen version B1. Hello guys. So recently Langchain has come up with lot\nrecently Langchain has come up with lot\nrecently Langchain has come up with lot of updates in their specific\nof updates in their specific\nof updates in their specific documentation in the recent version and\ndocumentation in the recent version and\ndocumentation in the recent version and uh in this entire series of videos and\nuh in this entire series of videos and\nuh in this entire series of videos and in this module we are going to see the\nin this module we are going to see the\nin this module we are going to see the various changes uh which langin has\nvarious changes uh which langin has\nvarious changes uh which langin has specifically come up with you know so\nspecifically come up with you know so\nspecifically come up with you know so here inside the documentation if I just\nhere inside the documentation if I just\nhere inside the documentation if I just go ahead and click on docs. Okay. So\ngo ahead and click on docs. Okay. So\ngo ahead and click on docs. Okay. So here you'll be able to see there is lang\nhere you'll be able to see there is lang\nhere you'll be able to see there is lang chain lang graph and deep python. So we\nchain lang graph and deep python. So we\nchain lang graph and deep python. So we will be covering all these things all\nwill be covering all these things all\nwill be covering all these things all these modules uh again in an updated way\nthese modules uh again in an updated way\nthese modules uh again in an updated way so that we are always up to date with\nso that we are always up to date with\nso that we are always up to date with langen documentation. So first of all we\nlangen documentation. So first of all we\nlangen documentation. So first of all we will go ahead with langen documentation\nwill go ahead with langen documentation\nwill go ahead with langen documentation over here. Now here with respect to this\nover here. Now here with respect to this\nover here. Now here with respect to this particular documentation uh there are a\nparticular documentation uh there are a\nparticular documentation uh there are a lot of changes uh specifically with\nlot of changes uh specifically with\nlot of changes uh specifically with respect to syntaxes with respect to\nrespect to syntaxes with respect to\nrespect to syntaxes with respect to creating agents you know uh how to\ncreating agents you know uh how to\ncreating agents you know uh how to integrate with multiple different models\nintegrate with multiple different models\nintegrate with multiple different models um how to go ahead and call a tool how\num how to go ahead and call a tool how\num how to go ahead and call a tool how to you know come up with a structured\nto you know come up with a structured\nto you know come up with a structured output along with that it also has lot\noutput along with that it also has lot\noutput along with that it also has lot support of messages you know different\nsupport of messages you know different\nsupport of messages you know different types of messages like AI message human\ntypes of messages like AI message human\ntypes of messages like AI message human message uh tool message. Along with\nmessage uh tool message. Along with\nmessage uh tool message. Along with that, we'll also be seeing something\nthat, we'll also be seeing something\nthat, we'll also be seeing something called a short-term memory. We'll be\ncalled a short-term memory. We'll be\ncalled a short-term memory. We'll be seeing how to perform streaming, you\nseeing how to perform streaming, you\nseeing how to perform streaming, you know, and there is a new concept uh that\nknow, and there is a new concept uh that\nknow, and there is a new concept uh that has basically come up with respect to\nhas basically come up with respect to\nhas basically come up with respect to middleware like built-in middlewares,\nmiddleware like built-in middlewares,\nmiddleware like built-in middlewares, custom middleware and uh we'll also be\ncustom middleware and uh we'll also be\ncustom middleware and uh we'll also be learning about guard drills and many\nlearning about guard drills and many\nlearning about guard drills and many more things. So in this entire series of\nmore things. So in this entire series of\nmore things. So in this entire series of video we are first of all going to cover\nvideo we are first of all going to cover\nvideo we are first of all going to cover the entire lang chain uh recent\nthe entire lang chain uh recent\nthe entire lang chain uh recent framework whatever the updates are there\nframework whatever the updates are there\nframework whatever the updates are there and uh you know and we are also going to\nand uh you know and we are also going to\nand uh you know and we are also going to use an amazing package which is called\nuse an amazing package which is called\nuse an amazing package which is called as UV package manager. Now everybody if\nas UV package manager. Now everybody if\nas UV package manager. Now everybody if you have heard about UV package manager\nyou have heard about UV package manager\nyou have heard about UV package manager this is an extremely fast python package\nthis is an extremely fast python package\nthis is an extremely fast python package uh and project manager and it is\nuh and project manager and it is\nuh and project manager and it is completely written in rust. So I'll give\ncompletely written in rust. So I'll give\ncompletely written in rust. So I'll give you an idea how you can actually go\nyou an idea how you can actually go\nyou an idea how you can actually go ahead and work with UV package manager\nahead and work with UV package manager\nahead and work with UV package manager and this is the entire installation you\nand this is the entire installation you\nand this is the entire installation you know uh how to probably go ahead step by\nknow uh how to probably go ahead step by\nknow uh how to probably go ahead step by step I'll be showing you how to how you\nstep I'll be showing you how to how you\nstep I'll be showing you how to how you can go ahead and create an environment\ncan go ahead and create an environment\ncan go ahead and create an environment um and along with this you can use any\num and along with this you can use any\num and along with this you can use any ID right there are also various ids that\nID right there are also various ids that\nID right there are also various ids that are now available we have VS code we\nare now available we have VS code we\nare now available we have VS code we have cursor we also have Google\nhave cursor we also have Google\nhave cursor we also have Google anti-gravity nowadays I'm actually\nanti-gravity nowadays I'm actually\nanti-gravity nowadays I'm actually specifically using Google anti-gravity\nspecifically using Google anti-gravity\nspecifically using Google anti-gravity also so uh all these things we will try\nalso so uh all these things we will try\nalso so uh all these things we will try to cover and uh our main aim is always\nto cover and uh our main aim is always\nto cover and uh our main aim is always to stay up to date with respect to\nto stay up to date with respect to\nto stay up to date with respect to anything that is basically coming in\nanything that is basically coming in\nanything that is basically coming in lang chain. Okay. So uh from as we go\nlang chain. Okay. So uh from as we go\nlang chain. Okay. So uh from as we go ahead now we will be covering this step\nahead now we will be covering this step\nahead now we will be covering this step by step and uh I will show you how step\nby step and uh I will show you how step\nby step and uh I will show you how step by step how to go ahead and create an\nby step how to go ahead and create an\nby step how to go ahead and create an environment and we will just start with\nenvironment and we will just start with\nenvironment and we will just start with a specific uh new project itself. So\na specific uh new project itself. So\na specific uh new project itself. So here you'll be able to see that I have\nhere you'll be able to see that I have\nhere you'll be able to see that I have already opened uh Google anti-gravity\nalready opened uh Google anti-gravity\nalready opened uh Google anti-gravity which I will show you in front of you\nwhich I will show you in front of you\nwhich I will show you in front of you right and I have created a folder which\nright and I have created a folder which\nright and I have created a folder which is called as langin updated now we'll\nis called as langin updated now we'll\nis called as langin updated now we'll start from basics now Google\nstart from basics now Google\nstart from basics now Google anti-gravity also you can go ahead and\nanti-gravity also you can go ahead and\nanti-gravity also you can go ahead and download it in order to download all you\ndownload it in order to download all you\ndownload it in order to download all you have to do is that just go ahead and\nhave to do is that just go ahead and\nhave to do is that just go ahead and search for Google anti-gravity\nsearch for Google anti-gravity\nsearch for Google anti-gravity okay and then this ID this is like a\nokay and then this ID this is like a\nokay and then this ID this is like a aentic ID like how we have VS code how\naentic ID like how we have VS code how\naentic ID like how we have VS code how we have cursor right you can also\nwe have cursor right you can also\nwe have cursor right you can also download for windows this It will be\ndownload for windows this It will be\ndownload for windows this It will be just be like a .exe file and then once\njust be like a .exe file and then once\njust be like a .exe file and then once you go ahead and install this uh you\nyou go ahead and install this uh you\nyou go ahead and install this uh you will be able to start working on it. So\nwill be able to start working on it. So\nwill be able to start working on it. So this is the ID that we are going to\nthis is the ID that we are going to\nthis is the ID that we are going to specifically work on. The best part\nspecifically work on. The best part\nspecifically work on. The best part about this ID is like cursor you know it\nabout this ID is like cursor you know it\nabout this ID is like cursor you know it provides access to agent and it also\nprovides access to agent and it also\nprovides access to agent and it also provides you completely for free. Uh I\nprovides you completely for free. Uh I\nprovides you completely for free. Uh I think for some number of requests not\nthink for some number of requests not\nthink for some number of requests not for uh infinity requests right but uh\nfor uh infinity requests right but uh\nfor uh infinity requests right but uh yes with the help of agents you will be\nyes with the help of agents you will be\nyes with the help of agents you will be able to write the code in a much more\nable to write the code in a much more\nable to write the code in a much more efficient way. Right. So now as we go\nefficient way. Right. So now as we go\nefficient way. Right. So now as we go ahead uh we will be covering uh the\nahead uh we will be covering uh the\nahead uh we will be covering uh the recent updated lang chain version and\nrecent updated lang chain version and\nrecent updated lang chain version and we'll try to see that how we can go\nwe'll try to see that how we can go\nwe'll try to see that how we can go ahead and create agents how we can go\nahead and create agents how we can go\nahead and create agents how we can go ahead and work with tools each and\nahead and work with tools each and\nahead and work with tools each and everything. So let's go ahead and start\neverything. So let's go ahead and start\neverything. So let's go ahead and start that. So guys now let's go ahead and\nthat. So guys now let's go ahead and\nthat. So guys now let's go ahead and start with first of all creating a\nstart with first of all creating a\nstart with first of all creating a virtual environment and it is always a\nvirtual environment and it is always a\nvirtual environment and it is always a good practice that we start with\ngood practice that we start with\ngood practice that we start with creating a virtual environment and uh\ncreating a virtual environment and uh\ncreating a virtual environment and uh for any kind of projects that we work\nfor any kind of projects that we work\nfor any kind of projects that we work with. So the first thing is that uh we\nwith. So the first thing is that uh we\nwith. So the first thing is that uh we will try to create a virtual environment\nwill try to create a virtual environment\nwill try to create a virtual environment with the help of UV. Okay, UV package\nwith the help of UV. Okay, UV package\nwith the help of UV. Okay, UV package manager. But before we go ahead, you\nmanager. But before we go ahead, you\nmanager. But before we go ahead, you know, uh we need to install the UV\nknow, uh we need to install the UV\nknow, uh we need to install the UV package manager, right? So how to go\npackage manager, right? So how to go\npackage manager, right? So how to go ahead and install it. So if you just go\nahead and install it. So if you just go\nahead and install it. So if you just go ahead and search for UV package manager.\nahead and search for UV package manager.\nahead and search for UV package manager. So this is the first link that you will\nSo this is the first link that you will\nSo this is the first link that you will be able to see it. Okay. So once you\nbe able to see it. Okay. So once you\nbe able to see it. Okay. So once you click it here you'll be able to see in\nclick it here you'll be able to see in\nclick it here you'll be able to see in the installation you have options for\nthe installation you have options for\nthe installation you have options for Mac OS Linux and you also have options\nMac OS Linux and you also have options\nMac OS Linux and you also have options for Windows right so out of both of\nfor Windows right so out of both of\nfor Windows right so out of both of these options you can go ahead and do\nthese options you can go ahead and do\nthese options you can go ahead and do it. Let's say that if you're using Mac\nit. Let's say that if you're using Mac\nit. Let's say that if you're using Mac OS or Linux, you can use this command. U\nOS or Linux, you can use this command. U\nOS or Linux, you can use this command. U if you are using Windows, you can\nif you are using Windows, you can\nif you are using Windows, you can directly open a PowerShell and you can\ndirectly open a PowerShell and you can\ndirectly open a PowerShell and you can execute this command. Right? So, how to\nexecute this command. Right? So, how to\nexecute this command. Right? So, how to open a PowerShell? So, first of all,\nopen a PowerShell? So, first of all,\nopen a PowerShell? So, first of all, what I will do, I'll copy this\nwhat I will do, I'll copy this\nwhat I will do, I'll copy this particular command. And now I will go to\nparticular command. And now I will go to\nparticular command. And now I will go to my um you know the ID and here I will\nmy um you know the ID and here I will\nmy um you know the ID and here I will open my terminal. The opening of the\nopen my terminal. The opening of the\nopen my terminal. The opening of the terminal is similar like VS code if\nterminal is similar like VS code if\nterminal is similar like VS code if you're using VS code till now. Okay. Now\nyou're using VS code till now. Okay. Now\nyou're using VS code till now. Okay. Now here inside this PowerShell see you have\nhere inside this PowerShell see you have\nhere inside this PowerShell see you have PowerShell option you have command\nPowerShell option you have command\nPowerShell option you have command prompt option. So here inside this\nprompt option. So here inside this\nprompt option. So here inside this PowerShell only you can just go ahead\nPowerShell only you can just go ahead\nPowerShell only you can just go ahead and paste this command and just press\nand paste this command and just press\nand paste this command and just press enter. So once you press enter the UV\nenter. So once you press enter the UV\nenter. So once you press enter the UV package manager you know will get\npackage manager you know will get\npackage manager you know will get automatically installed. Okay. So I've\nautomatically installed. Okay. So I've\nautomatically installed. Okay. So I've already done that installation so I\nalready done that installation so I\nalready done that installation so I don't have to do it again. But just to\ndon't have to do it again. But just to\ndon't have to do it again. But just to show it to you I have actually done it.\nshow it to you I have actually done it.\nshow it to you I have actually done it. Okay. Now I will remove this. I will\nOkay. Now I will remove this. I will\nOkay. Now I will remove this. I will open my command prompt. Okay. or\nopen my command prompt. Okay. or\nopen my command prompt. Okay. or whatever like let's say that you're\nwhatever like let's say that you're\nwhatever like let's say that you're using Mac OS whether you're using u uh\nusing Mac OS whether you're using u uh\nusing Mac OS whether you're using u uh Linux you know it is up to you whatever\nLinux you know it is up to you whatever\nLinux you know it is up to you whatever things you really want to use you can go\nthings you really want to use you can go\nthings you really want to use you can go ahead and use it okay so till then I'll\nahead and use it okay so till then I'll\nahead and use it okay so till then I'll just go ahead and close this now here\njust go ahead and close this now here\njust go ahead and close this now here the first step is that how do I go ahead\nthe first step is that how do I go ahead\nthe first step is that how do I go ahead and create my virtual environment with\nand create my virtual environment with\nand create my virtual environment with the help of UV package uh package\nthe help of UV package uh package\nthe help of UV package uh package manager so first of all what I will do I\nmanager so first of all what I will do I\nmanager so first of all what I will do I will initialize this entire folder as a\nwill initialize this entire folder as a\nwill initialize this entire folder as a working repository now in order order to\nworking repository now in order order to\nworking repository now in order order to initialize it you know we will go ahead\ninitialize it you know we will go ahead\ninitialize it you know we will go ahead and use one command which is called as\nand use one command which is called as\nand use one command which is called as uv init okay so please make sure to\nuv init okay so please make sure to\nuv init okay so please make sure to remember this command so if you want to\nremember this command so if you want to\nremember this command so if you want to go ahead and just initialize a working\ngo ahead and just initialize a working\ngo ahead and just initialize a working repository let's say this is my working\nrepository let's say this is my working\nrepository let's say this is my working repository so first of all I will\nrepository so first of all I will\nrepository so first of all I will initialize it with the help of uv so for\ninitialize it with the help of uv so for\ninitialize it with the help of uv so for that I will just go ahead and write uv\nthat I will just go ahead and write uv\nthat I will just go ahead and write uv init pro uh command once I execute this\ninit pro uh command once I execute this\ninit pro uh command once I execute this so here you can see it has initialized\nso here you can see it has initialized\nso here you can see it has initialized the project which is called as langchain\nthe project which is called as langchain\nthe project which is called as langchain updated so as soon as you init\nupdated so as soon as you init\nupdated so as soon as you init initialize the working repository. Here\ninitialize the working repository. Here\ninitialize the working repository. Here you get some of the basic information,\nyou get some of the basic information,\nyou get some of the basic information, right? So here you'll see pi\nright? So here you'll see pi\nright? So here you'll see pi project.2ml. This will give the\nproject.2ml. This will give the\nproject.2ml. This will give the information like which versions we are\ninformation like which versions we are\ninformation like which versions we are specifically working with. So here we\nspecifically working with. So here we\nspecifically working with. So here we are working with python 3.13. So recent\nare working with python 3.13. So recent\nare working with python 3.13. So recent updated Python package manager. Uh later\nupdated Python package manager. Uh later\nupdated Python package manager. Uh later on like let's say if python package is\non like let's say if python package is\non like let's say if python package is also getting updated, you know again\nalso getting updated, you know again\nalso getting updated, you know again when you write uv in it, it'll just take\nwhen you write uv in it, it'll just take\nwhen you write uv in it, it'll just take the recent python version. Okay. And it\nthe recent python version. Okay. And it\nthe recent python version. Okay. And it is always a good practice to work with\nis always a good practice to work with\nis always a good practice to work with the rethin version. That's the reason\nthe rethin version. That's the reason\nthe rethin version. That's the reason you can actually go ahead and directly\nyou can actually go ahead and directly\nyou can actually go ahead and directly use this. Now along with that you'll be\nuse this. Now along with that you'll be\nuse this. Now along with that you'll be seeing that a default main. py is\nseeing that a default main. py is\nseeing that a default main. py is basically there. Then you also have\nbasically there. Then you also have\nbasically there. Then you also have something called as python version file.\nsomething called as python version file.\nsomething called as python version file. So here you can see that I'm getting\nSo here you can see that I'm getting\nSo here you can see that I'm getting 3.13. Okay. So all this information you\n3.13. Okay. So all this information you\n3.13. Okay. So all this information you can see over here in a very simple way.\ncan see over here in a very simple way.\ncan see over here in a very simple way. Now the next thing what I will do is\nNow the next thing what I will do is\nNow the next thing what I will do is that I will just go ahead and write uv\nthat I will just go ahead and write uv\nthat I will just go ahead and write uv venv. Now see as soon as I write UV venv\nvenv. Now see as soon as I write UV venv\nvenv. Now see as soon as I write UV venv and I put a slash. Okay. Now what this\nand I put a slash. Okay. Now what this\nand I put a slash. Okay. Now what this will do is that it will go ahead and\nwill do is that it will go ahead and\nwill do is that it will go ahead and create a virtual environment. So once\ncreate a virtual environment. So once\ncreate a virtual environment. So once let me press enter. So here you can see\nlet me press enter. So here you can see\nlet me press enter. So here you can see that uh unrecognized subcomand slash. So\nthat uh unrecognized subcomand slash. So\nthat uh unrecognized subcomand slash. So by def by mistake I have put this slash\nby def by mistake I have put this slash\nby def by mistake I have put this slash I should not have put that. So what I\nI should not have put that. So what I\nI should not have put that. So what I will do I will just write uv venv. Now\nwill do I will just write uv venv. Now\nwill do I will just write uv venv. Now in order to create a virtual environment\nin order to create a virtual environment\nin order to create a virtual environment this is the most simplest command right.\nthis is the most simplest command right.\nthis is the most simplest command right. UV venv. As soon as I write uv VNV and I\nUV venv. As soon as I write uv VNV and I\nUV venv. As soon as I write uv VNV and I press enter. So here you can see that\npress enter. So here you can see that\npress enter. So here you can see that now it is using this Python 3.13.2\nnow it is using this Python 3.13.2\nnow it is using this Python 3.13.2 and it has created a virtual environment\nand it has created a virtual environment\nand it has created a virtual environment at this specific location.\nat this specific location.\nat this specific location. So Venv is my virtual environment. Right\nSo Venv is my virtual environment. Right\nSo Venv is my virtual environment. Right now in order to activate it see if as\nnow in order to activate it see if as\nnow in order to activate it see if as soon as you create a virtual environment\nsoon as you create a virtual environment\nsoon as you create a virtual environment you need to install the libraries inside\nyou need to install the libraries inside\nyou need to install the libraries inside that virtual environment. Right now in\nthat virtual environment. Right now in\nthat virtual environment. Right now in order to install the specific libraries\norder to install the specific libraries\norder to install the specific libraries inside that virtual environment, I will\ninside that virtual environment, I will\ninside that virtual environment, I will first of all activate that virtual\nfirst of all activate that virtual\nfirst of all activate that virtual environment. Now in order to activate\nenvironment. Now in order to activate\nenvironment. Now in order to activate it, the command is given over here. See\nit, the command is given over here. See\nit, the command is given over here. See it is written activate with VNV\nit is written activate with VNV\nit is written activate with VNV script/activate.\nscript/activate.\nscript/activate. So if you go inside the script, there is\nSo if you go inside the script, there is\nSo if you go inside the script, there is something called as activate. I just\nsomething called as activate. I just\nsomething called as activate. I just need to go ahead and call this\nneed to go ahead and call this\nneed to go ahead and call this particular uh or execute this particular\nparticular uh or execute this particular\nparticular uh or execute this particular command. So what I will do, I will copy\ncommand. So what I will do, I will copy\ncommand. So what I will do, I will copy this over here. I will paste it over\nthis over here. I will paste it over\nthis over here. I will paste it over here and I will just execute it. Now as\nhere and I will just execute it. Now as\nhere and I will just execute it. Now as soon as I do that here you can see that\nsoon as I do that here you can see that\nsoon as I do that here you can see that my\nmy\nmy my virtual environment right is\nmy virtual environment right is\nmy virtual environment right is activated which is called nothing but\nactivated which is called nothing but\nactivated which is called nothing but lunction updated. So this virtual\nlunction updated. So this virtual\nlunction updated. So this virtual environment has got updated. Okay. Now\nenvironment has got updated. Okay. Now\nenvironment has got updated. Okay. Now the next step is that how do I go ahead\nthe next step is that how do I go ahead\nthe next step is that how do I go ahead and start the installation of all the\nand start the installation of all the\nand start the installation of all the libraries. Okay. Now installation of the\nlibraries. Okay. Now installation of the\nlibraries. Okay. Now installation of the libraries is very important. Till now uh\nlibraries is very important. Till now uh\nlibraries is very important. Till now uh you know uh whenever we install a\nyou know uh whenever we install a\nyou know uh whenever we install a virtual a libraries you know you also\nvirtual a libraries you know you also\nvirtual a libraries you know you also need to make sure to keep an updated\nneed to make sure to keep an updated\nneed to make sure to keep an updated track of which version we are installing\ntrack of which version we are installing\ntrack of which version we are installing right uh but now with the help of UV\nright uh but now with the help of UV\nright uh but now with the help of UV package manager this is becoming very\npackage manager this is becoming very\npackage manager this is becoming very very easy now okay so let's say that I\nvery easy now okay so let's say that I\nvery easy now okay so let's say that I go ahead and first of all create a\ngo ahead and first of all create a\ngo ahead and first of all create a requirement txt file and please make\nrequirement txt file and please make\nrequirement txt file and please make sure to create that particular file\nsure to create that particular file\nsure to create that particular file outside VNV folder so I will go ahead\noutside VNV folder so I will go ahead\noutside VNV folder so I will go ahead and write requirement txt now inside\nand write requirement txt now inside\nand write requirement txt now inside this I will be using some of the\nthis I will be using some of the\nthis I will be using some of the libraries. Let's say one of the\nlibraries. Let's say one of the\nlibraries. Let's say one of the libraries that I'm using is Langchin.\nlibraries that I'm using is Langchin.\nlibraries that I'm using is Langchin. Then I have Langchin community. Okay,\nThen I have Langchin community. Okay,\nThen I have Langchin community. Okay, Langchin community because I will be\nLangchin community because I will be\nLangchin community because I will be requiring this. Okay, then I also have\nrequiring this. Okay, then I also have\nrequiring this. Okay, then I also have Langchin- OpenAI because I want to use\nLangchin- OpenAI because I want to use\nLangchin- OpenAI because I want to use this Langchin OpenAI. I also have to use\nthis Langchin OpenAI. I also have to use\nthis Langchin OpenAI. I also have to use Langchin Grock because I may also use\nLangchin Grock because I may also use\nLangchin Grock because I may also use Grock models. Then I also have\nGrock models. Then I also have\nGrock models. Then I also have Python-Env,\nPython-Env,\nPython-Env, right? So I will also be using this.\nright? So I will also be using this.\nright? So I will also be using this. Along with this I will also use langin/\nAlong with this I will also use langin/\nAlong with this I will also use langin/ google jenna my main aim over here is to\ngoogle jenna my main aim over here is to\ngoogle jenna my main aim over here is to install all these libraries is very\ninstall all these libraries is very\ninstall all these libraries is very simple because I want to show you all\nsimple because I want to show you all\nsimple because I want to show you all the examples with different different\nthe examples with different different\nthe examples with different different libraries and all okay so these are my\nlibraries and all okay so these are my\nlibraries and all okay so these are my default libraries and uh here you can\ndefault libraries and uh here you can\ndefault libraries and uh here you can see that it is also giving you some\nsee that it is also giving you some\nsee that it is also giving you some suggestions but don't go through that\nsuggestions but don't go through that\nsuggestions but don't go through that suggestion go ahead and type each and\nsuggestion go ahead and type each and\nsuggestion go ahead and type each and everything in front of you okay now the\neverything in front of you okay now the\neverything in front of you okay now the time comes is that I have to go ahead\ntime comes is that I have to go ahead\ntime comes is that I have to go ahead and install all these particular\nand install all these particular\nand install all these particular libraries inside in my virtual\nlibraries inside in my virtual\nlibraries inside in my virtual environment. Now here the best thing is\nenvironment. Now here the best thing is\nenvironment. Now here the best thing is that see I'm not given any specific\nthat see I'm not given any specific\nthat see I'm not given any specific version. We are going to work with the\nversion. We are going to work with the\nversion. We are going to work with the recent version of all these lang\nrecent version of all these lang\nrecent version of all these lang libraries over here. Now what is the\nlibraries over here. Now what is the\nlibraries over here. Now what is the recent version that also we will go\nrecent version that also we will go\nrecent version that also we will go ahead and check it out. So here what I\nahead and check it out. So here what I\nahead and check it out. So here what I will do I will write uv add minus r\nwill do I will write uv add minus r\nwill do I will write uv add minus r requirements\nrequirements\nrequirements txt right so this is how you go ahead\ntxt right so this is how you go ahead\ntxt right so this is how you go ahead and do the installation. See you can\nand do the installation. See you can\nand do the installation. See you can also go ahead and write uvp pip install\nalso go ahead and write uvp pip install\nalso go ahead and write uvp pip install minus r requirement.xt txt you you used\nminus r requirement.xt txt you you used\nminus r requirement.xt txt you you used to install all the requirement.txt by\nto install all the requirement.txt by\nto install all the requirement.txt by writing pip install minus our\nwriting pip install minus our\nwriting pip install minus our requirement.txt txt but with the help of\nrequirement.txt txt but with the help of\nrequirement.txt txt but with the help of UV you can just go ahead and write uv\nUV you can just go ahead and write uv\nUV you can just go ahead and write uv add minus r requirement txt now once I\nadd minus r requirement txt now once I\nadd minus r requirement txt now once I execute this so here you can see that\nexecute this so here you can see that\nexecute this so here you can see that all my installation will start happening\nall my installation will start happening\nall my installation will start happening okay it'll give you some warnings but\nokay it'll give you some warnings but\nokay it'll give you some warnings but it's okay we can skip this warnings now\nit's okay we can skip this warnings now\nit's okay we can skip this warnings now here you can see by default all the\nhere you can see by default all the\nhere you can see by default all the libraries has got installed now here you\nlibraries has got installed now here you\nlibraries has got installed now here you can also see all the version of the\ncan also see all the version of the\ncan also see all the version of the specific libraries that has got\nspecific libraries that has got\nspecific libraries that has got installed now just by seeing this you'll\ninstalled now just by seeing this you'll\ninstalled now just by seeing this you'll not be able to identify it so what I\nnot be able to identify it so what I\nnot be able to identify it so what I will do I will go ahead and Open this\nwill do I will go ahead and Open this\nwill do I will go ahead and Open this pipro.2mml.\npipro.2mml.\npipro.2mml. Now inside this you will be seeing that\nNow inside this you will be seeing that\nNow inside this you will be seeing that okay langchin 1.1.0 has been installed\nokay langchin 1.1.0 has been installed\nokay langchin 1.1.0 has been installed and this is the recent version. Langchin\nand this is the recent version. Langchin\nand this is the recent version. Langchin community.4.1\ncommunity.4.1\ncommunity.4.1 is installed. Langchin Google geni 3.2.0\nis installed. Langchin Google geni 3.2.0\nis installed. Langchin Google geni 3.2.0 is installed and all the different\nis installed and all the different\nis installed and all the different libraries has been installed. Now\nlibraries has been installed. Now\nlibraries has been installed. Now because of this you will be at least\nbecause of this you will be at least\nbecause of this you will be at least able to identify it because I will also\nable to identify it because I will also\nable to identify it because I will also pass you this pi project.2ml to ML file\npass you this pi project.2ml to ML file\npass you this pi project.2ml to ML file to just get you understand that okay\nto just get you understand that okay\nto just get you understand that okay right now we are in the specific\nright now we are in the specific\nright now we are in the specific versions tomorrow any number of updates\nversions tomorrow any number of updates\nversions tomorrow any number of updates that specifically comes you don't have\nthat specifically comes you don't have\nthat specifically comes you don't have to actually worry about it you know at\nto actually worry about it you know at\nto actually worry about it you know at least you know which is the base version\nleast you know which is the base version\nleast you know which is the base version right but my suggestion will be always\nright but my suggestion will be always\nright but my suggestion will be always that try to work with the recent version\nthat try to work with the recent version\nthat try to work with the recent version of langin because there are many many\nof langin because there are many many\nof langin because there are many many functionalities that will get deprecated\nfunctionalities that will get deprecated\nfunctionalities that will get deprecated some of the functionalities may may get\nsome of the functionalities may may get\nsome of the functionalities may may get moved to some other libraries and many\nmoved to some other libraries and many\nmoved to some other libraries and many more things now this is where we have\nmore things now this is where we have\nmore things now this is where we have actually gone ahead and uh you know\nactually gone ahead and uh you know\nactually gone ahead and uh you know created or installed all our libraries.\ncreated or installed all our libraries.\ncreated or installed all our libraries. Okay. Now the next thing is that I will\nOkay. Now the next thing is that I will\nOkay. Now the next thing is that I will also go ahead and create some keys.\nalso go ahead and create some keys.\nalso go ahead and create some keys. Okay. So I will be requiring three keys.\nOkay. So I will be requiring three keys.\nOkay. So I will be requiring three keys. One is the Google API key. So I will go\nOne is the Google API key. So I will go\nOne is the Google API key. So I will go ahead and write Google API key. Okay. So\nahead and write Google API key. Okay. So\nahead and write Google API key. Okay. So I will go to Google AI studio API key.\nI will go to Google AI studio API key.\nI will go to Google AI studio API key. And here you can see this is my\nAnd here you can see this is my\nAnd here you can see this is my dashboard. And there is an option which\ndashboard. And there is an option which\ndashboard. And there is an option which says create an API key. So I will go\nsays create an API key. So I will go\nsays create an API key. So I will go ahead and select one of the project. So\nahead and select one of the project. So\nahead and select one of the project. So let's say this is my project and I'll\nlet's say this is my project and I'll\nlet's say this is my project and I'll say okay this is my set key that I\nsay okay this is my set key that I\nsay okay this is my set key that I really want to go ahead and create or\nreally want to go ahead and create or\nreally want to go ahead and create or I'll go ahead and name it as lang chain\nI'll go ahead and name it as lang chain\nI'll go ahead and name it as lang chain updated and I will just go ahead and\nupdated and I will just go ahead and\nupdated and I will just go ahead and create the key. Okay\ncreate the key. Okay\ncreate the key. Okay now you know how to create a keys right\nnow you know how to create a keys right\nnow you know how to create a keys right at least uh that I think you should be\nat least uh that I think you should be\nat least uh that I think you should be familiar with. I will go ahead and copy\nfamiliar with. I will go ahead and copy\nfamiliar with. I will go ahead and copy the API key. Similarly I will go ahead\nthe API key. Similarly I will go ahead\nthe API key. Similarly I will go ahead with gro API key. So I will write gro\nwith gro API key. So I will write gro\nwith gro API key. So I will write gro API key and here is my API keys. Okay.\nAPI key and here is my API keys. Okay.\nAPI key and here is my API keys. Okay. And I will just go ahead and click on\nAnd I will just go ahead and click on\nAnd I will just go ahead and click on create API key and I can go ahead and\ncreate API key and I can go ahead and\ncreate API key and I can go ahead and create it. Right. Similarly with respect\ncreate it. Right. Similarly with respect\ncreate it. Right. Similarly with respect to open AI API. So I have created all\nto open AI API. So I have created all\nto open AI API. So I have created all these keys and what I will do I will\nthese keys and what I will do I will\nthese keys and what I will do I will quickly go ahead and create one file\nquickly go ahead and create one file\nquickly go ahead and create one file which is called asenv\nwhich is called asenv\nwhich is called asenv and I will go ahead and install uh paste\nand I will go ahead and install uh paste\nand I will go ahead and install uh paste this API keys over here. Right. So these\nthis API keys over here. Right. So these\nthis API keys over here. Right. So these are my API keys that I will be\nare my API keys that I will be\nare my API keys that I will be specifically using for my project. There\nspecifically using for my project. There\nspecifically using for my project. There is also one more library that I want to\nis also one more library that I want to\nis also one more library that I want to install for my uh Jupyter notebook that\ninstall for my uh Jupyter notebook that\ninstall for my uh Jupyter notebook that is nothing but UV add IPI kernel. Okay,\nis nothing but UV add IPI kernel. Okay,\nis nothing but UV add IPI kernel. Okay, IPI kernel. So IPi kernel you will be\nIPI kernel. So IPi kernel you will be\nIPI kernel. So IPi kernel you will be able to see that that is also installed.\nable to see that that is also installed.\nable to see that that is also installed. IPI kernel is just like a kernel\nIPI kernel is just like a kernel\nIPI kernel is just like a kernel provided to the Jupyter notebook. Again\nprovided to the Jupyter notebook. Again\nprovided to the Jupyter notebook. Again let me repeat it guys. Whenever you want\nlet me repeat it guys. Whenever you want\nlet me repeat it guys. Whenever you want to add any independent libraries, you\nto add any independent libraries, you\nto add any independent libraries, you use this command which is called as uv\nuse this command which is called as uv\nuse this command which is called as uv add. Okay. And then you give the library\nadd. Okay. And then you give the library\nadd. Okay. And then you give the library name. Okay, library name. If you want to\nname. Okay, library name. If you want to\nname. Okay, library name. If you want to directly install it from the\ndirectly install it from the\ndirectly install it from the requirement.txt, then you can just go\nrequirement.txt, then you can just go\nrequirement.txt, then you can just go ahead and write ue add minus r\nahead and write ue add minus r\nahead and write ue add minus r requirement. TXT. Okay, like it's just\nrequirement. TXT. Okay, like it's just\nrequirement. TXT. Okay, like it's just like you are doing the installation from\nlike you are doing the installation from\nlike you are doing the installation from requirement.txt.\nrequirement.txt.\nrequirement.txt. So in this video what we have actually\nSo in this video what we have actually\nSo in this video what we have actually done is that in this section we have\ndone is that in this section we have\ndone is that in this section we have created a virtual environment. We have\ncreated a virtual environment. We have\ncreated a virtual environment. We have created a requirement.txt file which has\ncreated a requirement.txt file which has\ncreated a requirement.txt file which has all the recent libraries and we have\nall the recent libraries and we have\nall the recent libraries and we have installed it by using this command uv\ninstalled it by using this command uv\ninstalled it by using this command uv minus r requirement.txt.\nminus r requirement.txt.\nminus r requirement.txt. Otherwise you can also go ahead and\nOtherwise you can also go ahead and\nOtherwise you can also go ahead and individually you can go ahead and\nindividually you can go ahead and\nindividually you can go ahead and install all the libraries by writing uv\ninstall all the libraries by writing uv\ninstall all the libraries by writing uv the library name whatever library name\nthe library name whatever library name\nthe library name whatever library name that you want. Let's say you want to go\nthat you want. Let's say you want to go\nthat you want. Let's say you want to go ahead and install lang. So here you can\nahead and install lang. So here you can\nahead and install lang. So here you can just go ahead and see that and here I\njust go ahead and see that and here I\njust go ahead and see that and here I have already installed it. So it is\nhave already installed it. So it is\nhave already installed it. So it is showing me resolve this and that right\nshowing me resolve this and that right\nshowing me resolve this and that right now in the next step what we are going\nnow in the next step what we are going\nnow in the next step what we are going to do is that we will start working on\nto do is that we will start working on\nto do is that we will start working on our lang uh updated documentation and we\nour lang uh updated documentation and we\nour lang uh updated documentation and we will start implementing agents. We'll\nwill start implementing agents. We'll\nwill start implementing agents. We'll show you how you can go ahead and\nshow you how you can go ahead and\nshow you how you can go ahead and integrate different kind of models. So\nintegrate different kind of models. So\nintegrate different kind of models. So let's go ahead and start with that. So\nlet's go ahead and start with that. So\nlet's go ahead and start with that. So guys now we have created a virtual\nguys now we have created a virtual\nguys now we have created a virtual environment. uh we have done the\nenvironment. uh we have done the\nenvironment. uh we have done the installation of all the libraries that\ninstallation of all the libraries that\ninstallation of all the libraries that we require in our virtual environment.\nwe require in our virtual environment.\nwe require in our virtual environment. Uh with the help of UV package manager\nUh with the help of UV package manager\nUh with the help of UV package manager uh you can also see all those things\nuh you can also see all those things\nuh you can also see all those things updated in pi project toml file and here\nupdated in pi project toml file and here\nupdated in pi project toml file and here you can see all these libraries we are\nyou can see all these libraries we are\nyou can see all these libraries we are going to specifically use it. Okay. Now\ngoing to specifically use it. Okay. Now\ngoing to specifically use it. Okay. Now uh you can also add your any\nuh you can also add your any\nuh you can also add your any descriptions that you specifically want\ndescriptions that you specifically want\ndescriptions that you specifically want to add also. Now what we are going to do\nto add also. Now what we are going to do\nto add also. Now what we are going to do is that we will start with the updated\nis that we will start with the updated\nis that we will start with the updated lang chain folder. Okay. So I've created\nlang chain folder. Okay. So I've created\nlang chain folder. Okay. So I've created a folder over here. So let me first of\na folder over here. So let me first of\na folder over here. So let me first of all delete this and create a new folder.\nall delete this and create a new folder.\nall delete this and create a new folder. So I will write updated\nSo I will write updated\nSo I will write updated lang chain. Okay. And uh I will start\nlang chain. Okay. And uh I will start\nlang chain. Okay. And uh I will start with the first file which is called as\nwith the first file which is called as\nwith the first file which is called as lang chain intro doip yb file. Okay. So,\nlang chain intro doip yb file. Okay. So,\nlang chain intro doip yb file. Okay. So, let me minimize this. I will go ahead\nlet me minimize this. I will go ahead\nlet me minimize this. I will go ahead and select the kernel. I want Python\nand select the kernel. I want Python\nand select the kernel. I want Python environment VNV. Okay. And uh we'll\nenvironment VNV. Okay. And uh we'll\nenvironment VNV. Okay. And uh we'll write a markdown saying that this is the\nwrite a markdown saying that this is the\nwrite a markdown saying that this is the lang chain version v1. Okay. I will just\nlang chain version v1. Okay. I will just\nlang chain version v1. Okay. I will just go ahead and write it. And I will just\ngo ahead and write it. And I will just\ngo ahead and write it. And I will just go ahead and execute it. Perfect. Now,\ngo ahead and execute it. Perfect. Now,\ngo ahead and execute it. Perfect. Now, just to check everything is working fine\njust to check everything is working fine\njust to check everything is working fine or not. So I will also go ahead and open\nor not. So I will also go ahead and open\nor not. So I will also go ahead and open my code and I'll execute something.\nmy code and I'll execute something.\nmy code and I'll execute something. Okay. So this is working. The Python\nOkay. So this is working. The Python\nOkay. So this is working. The Python code is also working like oneplus 1 is a\ncode is also working like oneplus 1 is a\ncode is also working like oneplus 1 is a kind of a numerical operation. Now uh my\nkind of a numerical operation. Now uh my\nkind of a numerical operation. Now uh my env file is also been loaded. Everything\nenv file is also been loaded. Everything\nenv file is also been loaded. Everything is ready. So first of all as usual we'll\nis ready. So first of all as usual we'll\nis ready. So first of all as usual we'll go ahead and import OS. Then from env we\ngo ahead and import OS. Then from env we\ngo ahead and import OS. Then from env we are going to import load_.env.\nare going to import load_.env.\nare going to import load_.env. And we will go ahead and initialize\nAnd we will go ahead and initialize\nAnd we will go ahead and initialize load_.env.\nload_.env.\nload_.env. and we will initialize our open AI API\nand we will initialize our open AI API\nand we will initialize our open AI API key. So I will write OST\nkey. So I will write OST\nkey. So I will write OST environment\nenvironment\nenvironment and here you can see that I will go\nand here you can see that I will go\nand here you can see that I will go ahead and write open AI API key. So uh\nahead and write open AI API key. So uh\nahead and write open AI API key. So uh one thing about uh Google anti-gravity\none thing about uh Google anti-gravity\none thing about uh Google anti-gravity is that it provides you a lot of\nis that it provides you a lot of\nis that it provides you a lot of suggestion. Okay. So you will be seeing\nsuggestion. Okay. So you will be seeing\nsuggestion. Okay. So you will be seeing okay quickly when you're coding it\nokay quickly when you're coding it\nokay quickly when you're coding it you'll quickly see all the suggestions\nyou'll quickly see all the suggestions\nyou'll quickly see all the suggestions that is coming up. Right. So OS.getb and\nthat is coming up. Right. So OS.getb and\nthat is coming up. Right. So OS.getb and open AI API key. So I will go ahead and\nopen AI API key. So I will go ahead and\nopen AI API key. So I will go ahead and execute this. So perfect. Um now the\nexecute this. So perfect. Um now the\nexecute this. So perfect. Um now the first thing that I'm just going to start\nfirst thing that I'm just going to start\nfirst thing that I'm just going to start okay um that is all about agents. Okay.\nokay um that is all about agents. Okay.\nokay um that is all about agents. Okay. Now first of all you need to understand\nNow first of all you need to understand\nNow first of all you need to understand what exactly is agents. So what I will\nwhat exactly is agents. So what I will\nwhat exactly is agents. So what I will do I will just open my file over here. I\ndo I will just open my file over here. I\ndo I will just open my file over here. I will create a new file so that I write\nwill create a new file so that I write\nwill create a new file so that I write something to you so that you get an\nsomething to you so that you get an\nsomething to you so that you get an understanding. Okay. See uh before uh\nunderstanding. Okay. See uh before uh\nunderstanding. Okay. See uh before uh you know when we started working right\nyou know when we started working right\nyou know when we started working right when initially we got generative AI at\nwhen initially we got generative AI at\nwhen initially we got generative AI at that time generative AI is becoming very\nthat time generative AI is becoming very\nthat time generative AI is becoming very very much as a important topic but\nvery much as a important topic but\nvery much as a important topic but nowadays everybody is specifically\nnowadays everybody is specifically\nnowadays everybody is specifically talking about a okay so everybody is\ntalking about a okay so everybody is\ntalking about a okay so everybody is talking about agents and agents is\ntalking about agents and agents is\ntalking about agents and agents is altogether a very very handy topic okay\naltogether a very very handy topic okay\naltogether a very very handy topic okay very very important and handy topic\nvery very important and handy topic\nvery very important and handy topic altogether so initially if you go ahead\naltogether so initially if you go ahead\naltogether so initially if you go ahead and see you know initially we were just\nand see you know initially we were just\nand see you know initially we were just talking about LLM models so let's say\ntalking about LLM models so let's say\ntalking about LLM models so let's say that this is one of my LLM model now the\nthat this is one of my LLM model now the\nthat this is one of my LLM model now the LLM model can be anything it can be an\nLLM model can be anything it can be an\nLLM model can be anything it can be an open AI LLM model it can be a generative\nopen AI LLM model it can be a generative\nopen AI LLM model it can be a generative AI LLM model it can be uh you know grock\nAI LLM model it can be uh you know grock\nAI LLM model it can be uh you know grock LLM model any open source LLM models the\nLLM model any open source LLM models the\nLLM model any open source LLM models the main task of the LLM model was that uh\nmain task of the LLM model was that uh\nmain task of the LLM model was that uh whenever we give any kind of input right\nwhenever we give any kind of input right\nwhenever we give any kind of input right input let's say if I go ahead and ask\ninput let's say if I go ahead and ask\ninput let's say if I go ahead and ask Hey uh write me a paragraph about\nHey uh write me a paragraph about\nHey uh write me a paragraph about artificial intelligence. So LLM will\nartificial intelligence. So LLM will\nartificial intelligence. So LLM will take that particular input and then it\ntake that particular input and then it\ntake that particular input and then it will give you a specific output. Okay,\nwill give you a specific output. Okay,\nwill give you a specific output. Okay, it'll give you a specific output like\nit'll give you a specific output like\nit'll give you a specific output like okay if I'm asking write a paragraph 200\nokay if I'm asking write a paragraph 200\nokay if I'm asking write a paragraph 200 words paragraph on artificial\nwords paragraph on artificial\nwords paragraph on artificial intelligence it'll give me a 200 words\nintelligence it'll give me a 200 words\nintelligence it'll give me a 200 words paragraph as an output. Okay, this was a\nparagraph as an output. Okay, this was a\nparagraph as an output. Okay, this was a simple generative AI application. Okay,\nsimple generative AI application. Okay,\nsimple generative AI application. Okay, I used to say this as a Gen AI\nI used to say this as a Gen AI\nI used to say this as a Gen AI application.\napplication.\napplication. So, Gen AI application.\nSo, Gen AI application.\nSo, Gen AI application. Okay, application. Perfect.\nOkay, application. Perfect.\nOkay, application. Perfect. But now, uh as we move ahead, you know,\nBut now, uh as we move ahead, you know,\nBut now, uh as we move ahead, you know, so let's say that for this particular\nso let's say that for this particular\nso let's say that for this particular LLM model, if I ask a question, hey,\nLLM model, if I ask a question, hey,\nLLM model, if I ask a question, hey, provide me with the current AI news or\nprovide me with the current AI news or\nprovide me with the current AI news or today's current AI news. So let's say\ntoday's current AI news. So let's say\ntoday's current AI news. So let's say that I want to know the today's\nAI news\nAI news AI news\nAI news\nAI news okay now in this particular scenario you\nokay now in this particular scenario you\nokay now in this particular scenario you know that LLM has a cutff training\nknow that LLM has a cutff training\nknow that LLM has a cutff training training date okay so we basically say\ntraining date okay so we basically say\ntraining date okay so we basically say that LLM is already trained from\nthat LLM is already trained from\nthat LLM is already trained from previous data it does not have the\nprevious data it does not have the\nprevious data it does not have the current information right recent\ncurrent information right recent\ncurrent information right recent information like let's say tomorrow's or\ninformation like let's say tomorrow's or\ninformation like let's say tomorrow's or today's information it does not have\ntoday's information it does not have\ntoday's information it does not have right so LLM has to be dependent on some\nright so LLM has to be dependent on some\nright so LLM has to be dependent on some thirdparty tool why it should be\nthirdparty tool why it should be\nthirdparty tool why it should be dependent on third party tool because\ndependent on third party tool because\ndependent on third party tool because when I'm asking this specific question\nwhen I'm asking this specific question\nwhen I'm asking this specific question or tell me about the today's AI news LLM\nor tell me about the today's AI news LLM\nor tell me about the today's AI news LLM does not have that particular\ndoes not have that particular\ndoes not have that particular information right because it is already\ninformation right because it is already\ninformation right because it is already trained with the previous data it is not\ntrained with the previous data it is not\ntrained with the previous data it is not trained with today's data and there is\ntrained with today's data and there is\ntrained with today's data and there is always a cut off training date right\nalways a cut off training date right\nalways a cut off training date right this is really really important for you\nthis is really really important for you\nthis is really really important for you all to understand so this is one of the\nall to understand so this is one of the\nall to understand so this is one of the problem of just using a plain LLM. Now\nproblem of just using a plain LLM. Now\nproblem of just using a plain LLM. Now that's the reason whenever we say that\nthat's the reason whenever we say that\nthat's the reason whenever we say that if we need to answer if my LLM needs to\nif we need to answer if my LLM needs to\nif we need to answer if my LLM needs to answer this particular question, it\nanswer this particular question, it\nanswer this particular question, it needs to be dependent on\nneeds to be dependent on\nneeds to be dependent on some thirdparty tool, some tool. Okay,\nsome thirdparty tool, some tool. Okay,\nsome thirdparty tool, some tool. Okay, whenever we say some tool that it can be\nwhenever we say some tool that it can be\nwhenever we say some tool that it can be a third party tool, it can be any kind\na third party tool, it can be any kind\na third party tool, it can be any kind of tool. Okay, it can be a third party\nof tool. Okay, it can be a third party\nof tool. Okay, it can be a third party APIs, it can be Google search, it can be\nAPIs, it can be Google search, it can be\nAPIs, it can be Google search, it can be something else, right?\nsomething else, right?\nsomething else, right? And based on this particular tool what\nAnd based on this particular tool what\nAnd based on this particular tool what should happen is that whenever we give\nshould happen is that whenever we give\nshould happen is that whenever we give an input saying that today's AI what are\nan input saying that today's AI what are\nan input saying that today's AI what are the today's AI news the LLM should be\nthe today's AI news the LLM should be\nthe today's AI news the LLM should be able to make a decision okay I will not\nable to make a decision okay I will not\nable to make a decision okay I will not be able to answer this particular\nbe able to answer this particular\nbe able to answer this particular question so now I'm dependent on some\nquestion so now I'm dependent on some\nquestion so now I'm dependent on some other tool which will be able to answer\nother tool which will be able to answer\nother tool which will be able to answer this particular question because this\nthis particular question because this\nthis particular question because this tool is currently connected to the\ntool is currently connected to the\ntool is currently connected to the current data or current like today's\ncurrent data or current like today's\ncurrent data or current like today's news it is basically connected to right\nnews it is basically connected to right\nnews it is basically connected to right specifically AI news and this will be\nspecifically AI news and this will be\nspecifically AI news and this will be able to give me the response and This\nable to give me the response and This\nable to give me the response and This response that you basically get from\nresponse that you basically get from\nresponse that you basically get from here it is basically called as context\nhere it is basically called as context\nhere it is basically called as context right and then only the LLM will be able\nright and then only the LLM will be able\nright and then only the LLM will be able to generate the output right so in this\nto generate the output right so in this\nto generate the output right so in this particular scenario where we have a LLM\nparticular scenario where we have a LLM\nparticular scenario where we have a LLM being dependent on some other tool and\nbeing dependent on some other tool and\nbeing dependent on some other tool and from where we are basically getting a\nfrom where we are basically getting a\nfrom where we are basically getting a context as soon as we give an input the\ncontext as soon as we give an input the\ncontext as soon as we give an input the LLM is able to make a decision okay I'm\nLLM is able to make a decision okay I'm\nLLM is able to make a decision okay I'm not able to answer this I have to\nnot able to answer this I have to\nnot able to answer this I have to probably dependent on the tool which\nprobably dependent on the tool which\nprobably dependent on the tool which tool will be able to answer this\ntool will be able to answer this\ntool will be able to answer this particular question and it will be able\nparticular question and it will be able\nparticular question and it will be able to give the context and generate the\nto give the context and generate the\nto give the context and generate the output so This is nothing but it is a\noutput so This is nothing but it is a\noutput so This is nothing but it is a basic agent. It is a basic\nbasic agent. It is a basic\nbasic agent. It is a basic agent. Okay, it is a basic agent. This\nagent. Okay, it is a basic agent. This\nagent. Okay, it is a basic agent. This is the simple functionality of an agent.\nis the simple functionality of an agent.\nis the simple functionality of an agent. Okay, so uh that is what an agent is all\nOkay, so uh that is what an agent is all\nOkay, so uh that is what an agent is all about. So I hope till now you have got a\nabout. So I hope till now you have got a\nabout. So I hope till now you have got a clear understanding what a basic agent\nclear understanding what a basic agent\nclear understanding what a basic agent looks like, right? So autonomously here\nlooks like, right? So autonomously here\nlooks like, right? So autonomously here you can see that it is able to make any\nyou can see that it is able to make any\nyou can see that it is able to make any kind of a simple decision like when to\nkind of a simple decision like when to\nkind of a simple decision like when to route what kind of query and how to\nroute what kind of query and how to\nroute what kind of query and how to properly solve that particular task.\nproperly solve that particular task.\nproperly solve that particular task. Okay. So uh whenever we talk with\nOkay. So uh whenever we talk with\nOkay. So uh whenever we talk with respect to an agent before creating a\nrespect to an agent before creating a\nrespect to an agent before creating a agent with the help of langin was little\nagent with the help of langin was little\nagent with the help of langin was little bit tough you know so before we used to\nbit tough you know so before we used to\nbit tough you know so before we used to use an LLM model then we used to create\nuse an LLM model then we used to create\nuse an LLM model then we used to create a a tool separately then we had to\na a tool separately then we had to\na a tool separately then we had to probably go ahead and do a linkage\nprobably go ahead and do a linkage\nprobably go ahead and do a linkage between this particular tool to the LLM\nbetween this particular tool to the LLM\nbetween this particular tool to the LLM we used to use a architecture which is\nwe used to use a architecture which is\nwe used to use a architecture which is called as react architecture okay react\ncalled as react architecture okay react\ncalled as react architecture okay react architecture now with the help of this\narchitecture now with the help of this\narchitecture now with the help of this particular architecture we were building\nparticular architecture we were building\nparticular architecture we were building this specific agent but now Creating\nthis specific agent but now Creating\nthis specific agent but now Creating this agent has become simpler with the\nthis agent has become simpler with the\nthis agent has become simpler with the recent langchain version that is\nrecent langchain version that is\nrecent langchain version that is langchain uh version one. Okay. So now\nlangchain uh version one. Okay. So now\nlangchain uh version one. Okay. So now let me go ahead and show you that how we\nlet me go ahead and show you that how we\nlet me go ahead and show you that how we can quickly create an agent uh and how\ncan quickly create an agent uh and how\ncan quickly create an agent uh and how easy it is basically to create an agent.\neasy it is basically to create an agent.\neasy it is basically to create an agent. So first of all in order to create an\nSo first of all in order to create an\nSo first of all in order to create an agent what we will be doing is that we\nagent what we will be doing is that we\nagent what we will be doing is that we will just go ahead and define something\nwill just go ahead and define something\nwill just go ahead and define something called as from langchain.\ncalled as from langchain.\ncalled as from langchain. Okay. Langchain dot agents we import\nOkay. Langchain dot agents we import\nOkay. Langchain dot agents we import something called as create\nsomething called as create\nsomething called as create agent. Okay. Create agent. Now as soon\nagent. Okay. Create agent. Now as soon\nagent. Okay. Create agent. Now as soon as we write like this langen dot aagents\nas we write like this langen dot aagents\nas we write like this langen dot aagents create agent. Here we go ahead and\ncreate agent. Here we go ahead and\ncreate agent. Here we go ahead and define agent is equal to create agent.\ndefine agent is equal to create agent.\ndefine agent is equal to create agent. And inside this first of all we give our\nAnd inside this first of all we give our\nAnd inside this first of all we give our model name. Now model name can be given\nmodel name. Now model name can be given\nmodel name. Now model name can be given through different ways. So directly if\nthrough different ways. So directly if\nthrough different ways. So directly if I'm importing the open AAI library I\nI'm importing the open AAI library I\nI'm importing the open AAI library I will be giving the my model name. Let's\nwill be giving the my model name. Let's\nwill be giving the my model name. Let's say my model name is GPT5 which is the\nsay my model name is GPT5 which is the\nsay my model name is GPT5 which is the recent uh you know specific open AAI\nrecent uh you know specific open AAI\nrecent uh you know specific open AAI model. And then I will go ahead and use\nmodel. And then I will go ahead and use\nmodel. And then I will go ahead and use tools. So right now I will keep this\ntools. So right now I will keep this\ntools. So right now I will keep this tools and empty because I don't have any\ntools and empty because I don't have any\ntools and empty because I don't have any other tools created yet. Okay. And then\nother tools created yet. Okay. And then\nother tools created yet. Okay. And then apart from this tool we also provide\napart from this tool we also provide\napart from this tool we also provide some kind of system prompt. So here I\nsome kind of system prompt. So here I\nsome kind of system prompt. So here I will go ahead and write my system prompt\nwill go ahead and write my system prompt\nwill go ahead and write my system prompt saying that hey you are an helpful\nsaying that hey you are an helpful\nsaying that hey you are an helpful assistant. Okay. And then we have also\nassistant. Okay. And then we have also\nassistant. Okay. And then we have also kept verbose is equal to true. I will\nkept verbose is equal to true. I will\nkept verbose is equal to true. I will talk about what is verbose. Okay. It'll\ntalk about what is verbose. Okay. It'll\ntalk about what is verbose. Okay. It'll give you more information with respect\ngive you more information with respect\ngive you more information with respect to the invocation. Now as soon as I go\nto the invocation. Now as soon as I go\nto the invocation. Now as soon as I go ahead and just run or uh write this. So\nahead and just run or uh write this. So\nahead and just run or uh write this. So here you can see it got an unexpected\nhere you can see it got an unexpected\nhere you can see it got an unexpected keyword argument. I think verbose is not\nkeyword argument. I think verbose is not\nkeyword argument. I think verbose is not supported yet for this. So let me remove\nsupported yet for this. So let me remove\nsupported yet for this. So let me remove it. Okay. Now let me just go ahead and\nit. Okay. Now let me just go ahead and\nit. Okay. Now let me just go ahead and uh execute this agent. Now you'll be\nuh execute this agent. Now you'll be\nuh execute this agent. Now you'll be able to see some kind of diagram over\nable to see some kind of diagram over\nable to see some kind of diagram over here. Okay. And that diagram will\nhere. Okay. And that diagram will\nhere. Okay. And that diagram will definitely match this diagram that we\ndefinitely match this diagram that we\ndefinitely match this diagram that we have created. Okay. So this is how we\nhave created. Okay. So this is how we\nhave created. Okay. So this is how we basically go ahead and create a basic\nbasically go ahead and create a basic\nbasically go ahead and create a basic agent. But right now you can just see\nagent. But right now you can just see\nagent. But right now you can just see that tools is right now empty. Okay. So\nthat tools is right now empty. Okay. So\nthat tools is right now empty. Okay. So what we have we have start, we have\nwhat we have we have start, we have\nwhat we have we have start, we have model and we have end. So that basically\nmodel and we have end. So that basically\nmodel and we have end. So that basically means\nmeans\nmeans we just have this input lm and output.\nwe just have this input lm and output.\nwe just have this input lm and output. still that tool connection is not there\nstill that tool connection is not there\nstill that tool connection is not there because we have not created any tool\nbecause we have not created any tool\nbecause we have not created any tool right so now what I will do I will just\nright so now what I will do I will just\nright so now what I will do I will just go ahead and define one function so\ngo ahead and define one function so\ngo ahead and define one function so let's say this is my get weather\nlet's say this is my get weather\nlet's say this is my get weather function I will give my city over here\nfunction I will give my city over here\nfunction I will give my city over here which will be in the form of string and\nwhich will be in the form of string and\nwhich will be in the form of string and this will also return string okay and\nthis will also return string okay and\nthis will also return string okay and here I will just say return the weather\nhere I will just say return the weather\nhere I will just say return the weather in this city is sunny that's it okay I\nin this city is sunny that's it okay I\nin this city is sunny that's it okay I can also provide some dock string to\ncan also provide some dock string to\ncan also provide some dock string to provide some more information related to\nprovide some more information related to\nprovide some more information related to this particular function. It's like get\nthis particular function. It's like get\nthis particular function. It's like get the weather for a city. Okay. And now\nthe weather for a city. Okay. And now\nthe weather for a city. Okay. And now this same tool I can add it over here.\nthis same tool I can add it over here.\nthis same tool I can add it over here. So that basically means if I ask what is\nSo that basically means if I ask what is\nSo that basically means if I ask what is the weather of Bangalore now the LM will\nthe weather of Bangalore now the LM will\nthe weather of Bangalore now the LM will be much more smarter enough to know that\nbe much more smarter enough to know that\nbe much more smarter enough to know that which tool it needs to call. Right? So\nwhich tool it needs to call. Right? So\nwhich tool it needs to call. Right? So now what I have done is that we have\nnow what I have done is that we have\nnow what I have done is that we have created a function which is called as\ncreated a function which is called as\ncreated a function which is called as which is called as get weather. Okay. So\nwhich is called as get weather. Okay. So\nwhich is called as get weather. Okay. So here what we have done is that we have\nhere what we have done is that we have\nhere what we have done is that we have created a function which is called as\ncreated a function which is called as\ncreated a function which is called as get weather. And this get weather is\nget weather. And this get weather is\nget weather. And this get weather is added as a tool to this particular LLM.\nadded as a tool to this particular LLM.\nadded as a tool to this particular LLM. Okay. Now if I ask hey what is the\nOkay. Now if I ask hey what is the\nOkay. Now if I ask hey what is the weather for this particular city? Now\nweather for this particular city? Now\nweather for this particular city? Now the LLM will make a decision. It will\nthe LLM will make a decision. It will\nthe LLM will make a decision. It will not have the current information\nnot have the current information\nnot have the current information obviously right because it is already\nobviously right because it is already\nobviously right because it is already trained with the previous data. So it\ntrained with the previous data. So it\ntrained with the previous data. So it knows that it has to call this get\nknows that it has to call this get\nknows that it has to call this get weather function or tool right and then\nweather function or tool right and then\nweather function or tool right and then it'll try to get the context. The\nit'll try to get the context. The\nit'll try to get the context. The context is nothing but whatever this\ncontext is nothing but whatever this\ncontext is nothing but whatever this function is returning that is the\nfunction is returning that is the\nfunction is returning that is the context and finally it will be\ncontext and finally it will be\ncontext and finally it will be generating the output. Okay now see this\ngenerating the output. Okay now see this\ngenerating the output. Okay now see this as soon as I created a function get\nas soon as I created a function get\nas soon as I created a function get weather and I updated inside this tools.\nweather and I updated inside this tools.\nweather and I updated inside this tools. Right now I have this tool that is\nRight now I have this tool that is\nRight now I have this tool that is available. Now you see how this agent\navailable. Now you see how this agent\navailable. Now you see how this agent diagram will change. Now you can see\ndiagram will change. Now you can see\ndiagram will change. Now you can see that right. So now I have start the\nthat right. So now I have start the\nthat right. So now I have start the model which is my LLM and this is\nmodel which is my LLM and this is\nmodel which is my LLM and this is basically connected to my tools. Now\nbasically connected to my tools. Now\nbasically connected to my tools. Now whenever I ask any question with respect\nwhenever I ask any question with respect\nwhenever I ask any question with respect to weather this model will definitely go\nto weather this model will definitely go\nto weather this model will definitely go ahead and hit the tool get the response\nahead and hit the tool get the response\nahead and hit the tool get the response and it will display the output. Now in\nand it will display the output. Now in\nand it will display the output. Now in order to run the agent it is very\norder to run the agent it is very\norder to run the agent it is very simple. I will go ahead and run the\nsimple. I will go ahead and run the\nsimple. I will go ahead and run the agent over here. And running the agent\nagent over here. And running the agent\nagent over here. And running the agent is very simple. Well, I will write\nis very simple. Well, I will write\nis very simple. Well, I will write agent.invoke\nagent.invoke\nagent.invoke agent.invoke.\nagent.invoke.\nagent.invoke. And let's say that I will go ahead and\nAnd let's say that I will go ahead and\nAnd let's say that I will go ahead and uh you know just write what is the\nuh you know just write what is the\nuh you know just write what is the weather like in New York. I know this is\nweather like in New York. I know this is\nweather like in New York. I know this is not going to give me New York weather\nnot going to give me New York weather\nnot going to give me New York weather because here I'm just returning a simple\nbecause here I'm just returning a simple\nbecause here I'm just returning a simple string. But just imagine that here we\nstring. But just imagine that here we\nstring. But just imagine that here we had some API calls that was basically\nhad some API calls that was basically\nhad some API calls that was basically made in order to get the weather\nmade in order to get the weather\nmade in order to get the weather information and uh here we can display\ninformation and uh here we can display\ninformation and uh here we can display that particular information. So now if I\nthat particular information. So now if I\nthat particular information. So now if I go ahead and execute this clearly you\ngo ahead and execute this clearly you\ngo ahead and execute this clearly you will be able to see that I'm getting one\nwill be able to see that I'm getting one\nwill be able to see that I'm getting one error. Let's see uh expected dictionary.\nerror. Let's see uh expected dictionary.\nerror. Let's see uh expected dictionary. Okay. So this is not the right format to\nOkay. So this is not the right format to\nOkay. So this is not the right format to give it. The simple reason is that we\ngive it. The simple reason is that we\ngive it. The simple reason is that we need to give it in the form of a\nneed to give it in the form of a\nneed to give it in the form of a messages. So I will write messages\nmessages. So I will write messages\nmessages. So I will write messages colon.\ncolon.\ncolon. Okay. And then I will give it in the\nOkay. And then I will give it in the\nOkay. And then I will give it in the form of a role. like role is like user\nform of a role. like role is like user\nform of a role. like role is like user because it is a user message human\nbecause it is a user message human\nbecause it is a user message human message. We will talk more about these\nmessage. We will talk more about these\nmessage. We will talk more about these different types of messages as we go\ndifferent types of messages as we go\ndifferent types of messages as we go ahead but right now I just want to show\nahead but right now I just want to show\nahead but right now I just want to show you how you can go ahead and run this\nyou how you can go ahead and run this\nyou how you can go ahead and run this particular agent. So role is equal to\nparticular agent. So role is equal to\nparticular agent. So role is equal to user and content we are writing what is\nuser and content we are writing what is\nuser and content we are writing what is the weather like in New York. Now if I'm\nthe weather like in New York. Now if I'm\nthe weather like in New York. Now if I'm giving in this fun see the error is very\ngiving in this fun see the error is very\ngiving in this fun see the error is very simple over here that you have got\nsimple over here that you have got\nsimple over here that you have got expected dictionary. So whenever we are\nexpected dictionary. So whenever we are\nexpected dictionary. So whenever we are using this inbuilt function called as\nusing this inbuilt function called as\nusing this inbuilt function called as create agent for creating the agent in\ncreate agent for creating the agent in\ncreate agent for creating the agent in this particular scenario we have to give\nthis particular scenario we have to give\nthis particular scenario we have to give the input in the form of a dictionary\nthe input in the form of a dictionary\nthe input in the form of a dictionary wherein my dictionary key will be in the\nwherein my dictionary key will be in the\nwherein my dictionary key will be in the form of a messages. Okay here if I'm\nform of a messages. Okay here if I'm\nform of a messages. Okay here if I'm writing messages and I'm executing this.\nwriting messages and I'm executing this.\nwriting messages and I'm executing this. So here you can see I'm given the role\nSo here you can see I'm given the role\nSo here you can see I'm given the role is equal to user and content what is the\nis equal to user and content what is the\nis equal to user and content what is the weather like in New York. So if I go\nweather like in New York. So if I go\nweather like in New York. So if I go ahead and execute it now you'll be able\nahead and execute it now you'll be able\nahead and execute it now you'll be able to clearly see the response. See guys,\nto clearly see the response. See guys,\nto clearly see the response. See guys, I'm showing you the error. I will not\nI'm showing you the error. I will not\nI'm showing you the error. I will not cut that particular error part because I\ncut that particular error part because I\ncut that particular error part because I want to show you each and everything.\nwant to show you each and everything.\nwant to show you each and everything. Okay. So now what is the weather like in\nOkay. So now what is the weather like in\nOkay. So now what is the weather like in New York? So first of all, this was the\nNew York? So first of all, this was the\nNew York? So first of all, this was the human message that has gone. You can\nhuman message that has gone. You can\nhuman message that has gone. You can also give it in the form of human\nalso give it in the form of human\nalso give it in the form of human message. We'll discuss more about the\nmessage. We'll discuss more about the\nmessage. We'll discuss more about the messages as we go ahead. Then here you\nmessages as we go ahead. Then here you\nmessages as we go ahead. Then here you can see the AI message is making a tool\ncan see the AI message is making a tool\ncan see the AI message is making a tool call. So somewhere here you'll be able\ncall. So somewhere here you'll be able\ncall. So somewhere here you'll be able to see that it has made a tool call. So\nto see that it has made a tool call. So\nto see that it has made a tool call. So let me just go ahead and see it. uh\nlet me just go ahead and see it. uh\nlet me just go ahead and see it. uh somewhere here you can see that it has\nsomewhere here you can see that it has\nsomewhere here you can see that it has made a tool call and it also knows which\nmade a tool call and it also knows which\nmade a tool call and it also knows which tool to call right get weather because\ntool to call right get weather because\ntool to call right get weather because it has this particular information. Now\nit has this particular information. Now\nit has this particular information. Now the question arises that how does LLM\nthe question arises that how does LLM\nthe question arises that how does LLM model knows that it has to make a get\nmodel knows that it has to make a get\nmodel knows that it has to make a get weather tool call because when we define\nweather tool call because when we define\nweather tool call because when we define this particular function get weather we\nthis particular function get weather we\nthis particular function get weather we have also put a dock string right and\nhave also put a dock string right and\nhave also put a dock string right and when we are assigning this tool to this\nwhen we are assigning this tool to this\nwhen we are assigning this tool to this particular agent this dock string it the\nparticular agent this dock string it the\nparticular agent this dock string it the LLM will understand which is the dock\nLLM will understand which is the dock\nLLM will understand which is the dock string over here like get the weather\nstring over here like get the weather\nstring over here like get the weather for a city now it knows that it will go\nfor a city now it knows that it will go\nfor a city now it knows that it will go ahead and call this particular function\nahead and call this particular function\nahead and call this particular function right so it made a tool message so here\nright so it made a tool message so here\nright so it made a tool message so here you can see the weather in New York is\nyou can see the weather in New York is\nyou can see the weather in New York is sunny it has just taken this particular\nsunny it has just taken this particular\nsunny it has just taken this particular uh city data and it is basically giving\nuh city data and it is basically giving\nuh city data and it is basically giving you the output like sunny right and then\nyou the output like sunny right and then\nyou the output like sunny right and then finally you can see the AI message it's\nfinally you can see the AI message it's\nfinally you can see the AI message it's sunny in New York right so once you get\nsunny in New York right so once you get\nsunny in New York right so once you get the context from the tool the model will\nthe context from the tool the model will\nthe context from the tool the model will be displaying the output right so if you\nbe displaying the output right so if you\nbe displaying the output right so if you want to also display the output over\nwant to also display the output over\nwant to also display the output over here you can just go ahead and write\nhere you can just go ahead and write\nhere you can just go ahead and write response is equal to agent this one and\nresponse is equal to agent this one and\nresponse is equal to agent this one and then I will just go ahead and write\nthen I will just go ahead and write\nthen I will just go ahead and write response of messages\nresponse of messages\nresponse of messages messages\nmessages\nmessages right so you can see messages and then\nright so you can see messages and then\nright so you can see messages and then you can just go ahead and take the last\nyou can just go ahead and take the last\nyou can just go ahead and take the last message dot contain and here you should\nmessage dot contain and here you should\nmessage dot contain and here you should be getting the output right so when you\nbe getting the output right so when you\nbe getting the output right so when you write messages\nwrite messages\nwrite messages last one you'll be getting the last\nlast one you'll be getting the last\nlast one you'll be getting the last output if you remember if you remove\noutput if you remember if you remove\noutput if you remember if you remove this also you'll be getting the entire\nthis also you'll be getting the entire\nthis also you'll be getting the entire conversation and all right let me also\nconversation and all right let me also\nconversation and all right let me also show you one more way you can also\nshow you one more way you can also\nshow you one more way you can also directly go ahead and write like this\ndirectly go ahead and write like this\ndirectly go ahead and write like this agent do invoke and here you can\nagent do invoke and here you can\nagent do invoke and here you can write in the form of messages\nwrite in the form of messages\nwrite in the form of messages and here you can just go ahead and Write\nand here you can just go ahead and Write\nand here you can just go ahead and Write something like this. What is the\nsomething like this. What is the\nsomething like this. What is the what is the\nwhat is the\nwhat is the weather in New York? So let's see\nweather in New York? So let's see\nweather in New York? So let's see whether we'll be able to get the output\nwhether we'll be able to get the output\nwhether we'll be able to get the output or not.\nor not.\nor not. Like this also you can directly write.\nLike this also you can directly write.\nLike this also you can directly write. You don't need to even specify that\nYou don't need to even specify that\nYou don't need to even specify that whether it is an human message or not.\nwhether it is an human message or not.\nwhether it is an human message or not. It will automatically identify it. Okay.\nIt will automatically identify it. Okay.\nIt will automatically identify it. Okay. So now here you'll be able to see that\nSo now here you'll be able to see that\nSo now here you'll be able to see that and there is a small spelling mistake\nand there is a small spelling mistake\nand there is a small spelling mistake and here also you can see that I'm\nand here also you can see that I'm\nand here also you can see that I'm getting a output. Okay and this is\ngetting a output. Okay and this is\ngetting a output. Okay and this is really really good even though I made a\nreally really good even though I made a\nreally really good even though I made a spelling mistake it is being able to\nspelling mistake it is being able to\nspelling mistake it is being able to give me the right output also. Okay so\ngive me the right output also. Okay so\ngive me the right output also. Okay so that's the most amazing part out there.\nthat's the most amazing part out there.\nthat's the most amazing part out there. So I hope you got a very basic idea of\nSo I hope you got a very basic idea of\nSo I hope you got a very basic idea of how to create an agent. What was an\nhow to create an agent. What was an\nhow to create an agent. What was an agent? We basically say this as an\nagent? We basically say this as an\nagent? We basically say this as an autonomous agent because based on the\nautonomous agent because based on the\nautonomous agent because based on the input the model is taking the decision\ninput the model is taking the decision\ninput the model is taking the decision which tools to call get the context and\nwhich tools to call get the context and\nwhich tools to call get the context and give you the output right so everything\ngive you the output right so everything\ngive you the output right so everything is happening over here but as we go\nis happening over here but as we go\nis happening over here but as we go ahead we will be creating multiple tools\nahead we will be creating multiple tools\nahead we will be creating multiple tools so this is one of the tool like that we\nso this is one of the tool like that we\nso this is one of the tool like that we can go ahead and create any number of\ncan go ahead and create any number of\ncan go ahead and create any number of tools as we like okay so this was just a\ntools as we like okay so this was just a\ntools as we like okay so this was just a basic way of creating an agent with a\nbasic way of creating an agent with a\nbasic way of creating an agent with a recent version now which version we are\nrecent version now which version we are\nrecent version now which version we are talking with right so I'll write import\ntalking with right so I'll write import\ntalking with right so I'll write import lang chain\nlang chain\nlang chain and import lang chain and I will just go\nand import lang chain and I will just go\nand import lang chain and I will just go ahead and print lang chain\nahead and print lang chain\nahead and print lang chain version\nversion\nversion right so it is 1.1.0\nright so it is 1.1.0\nright so it is 1.1.0 So I hope uh in this video we have\nSo I hope uh in this video we have\nSo I hope uh in this video we have understood about agents\nunderstood about agents\nunderstood about agents basic agents right uh agents are like\nbasic agents right uh agents are like\nbasic agents right uh agents are like you know autonomously it will be doing\nyou know autonomously it will be doing\nyou know autonomously it will be doing this specific task that is assigned to\nthis specific task that is assigned to\nthis specific task that is assigned to it. So yeah in the next video now we\nit. So yeah in the next video now we\nit. So yeah in the next video now we will see how to integrate different\nwill see how to integrate different\nwill see how to integrate different different models we will talk about\ndifferent models we will talk about\ndifferent models we will talk about different kind of messages each and\ndifferent kind of messages each and\ndifferent kind of messages each and everything and uh as we go ahead like in\neverything and uh as we go ahead like in\neverything and uh as we go ahead like in this series we will be discussing about\nthis series we will be discussing about\nthis series we will be discussing about that. So let's go ahead and discuss the\nthat. So let's go ahead and discuss the\nthat. So let's go ahead and discuss the next thing that is model integration. So\nnext thing that is model integration. So\nnext thing that is model integration. So guys, now we are going to discuss about\nguys, now we are going to discuss about\nguys, now we are going to discuss about model integration with your LLM\nmodel integration with your LLM\nmodel integration with your LLM application or with your generative AI\napplication or with your generative AI\napplication or with your generative AI application and uh we will see three\napplication and uh we will see three\napplication and uh we will see three popular models that is open AI, Google\npopular models that is open AI, Google\npopular models that is open AI, Google geminy and gro you know in gro you have\ngeminy and gro you know in gro you have\ngeminy and gro you know in gro you have various open source models Google geminy\nvarious open source models Google geminy\nvarious open source models Google geminy whenever we talk about you know there\nwhenever we talk about you know there\nwhenever we talk about you know there are different geiny models and open AAI\nare different geiny models and open AAI\nare different geiny models and open AAI like you have GPD models you know 4.5\nlike you have GPD models you know 4.5\nlike you have GPD models you know 4.5 whichever you want to specifically go\nwhichever you want to specifically go\nwhichever you want to specifically go ahead and use it okay now what we are\nahead and use it okay now what we are\nahead and use it okay now what we are going to do is that we I will just go\ngoing to do is that we I will just go\ngoing to do is that we I will just go ahead and show you with the recent\nahead and show you with the recent\nahead and show you with the recent updated langen like what are the\nupdated langen like what are the\nupdated langen like what are the different ways of invoking a specific\ndifferent ways of invoking a specific\ndifferent ways of invoking a specific model. Okay. So first of all I will go\nmodel. Okay. So first of all I will go\nmodel. Okay. So first of all I will go ahead and make a code sale you know and\nahead and make a code sale you know and\nahead and make a code sale you know and if you remember in our env file we have\nif you remember in our env file we have\nif you remember in our env file we have all the three API keys loaded over here.\nall the three API keys loaded over here.\nall the three API keys loaded over here. Okay. So first thing is that what I will\nOkay. So first thing is that what I will\nOkay. So first thing is that what I will do I will just go ahead and write import\ndo I will just go ahead and write import\ndo I will just go ahead and write import OS and then from env import load\nOS and then from env import load\nOS and then from env import load env right and I will go ahead and\nenv right and I will go ahead and\nenv right and I will go ahead and initialize the load env so that we load\ninitialize the load env so that we load\ninitialize the load env so that we load all the models right and then we are\nall the models right and then we are\nall the models right and then we are going to set our environment variable\ngoing to set our environment variable\ngoing to set our environment variable from the open AI API key so open AI API\nfrom the open AI API key so open AI API\nfrom the open AI API key so open AI API key is equal to os get env key and here\nkey is equal to os get env key and here\nkey is equal to os get env key and here also we are going to use the openi API\nalso we are going to use the openi API\nalso we are going to use the openi API key. Similarly, what you can actually do\nkey. Similarly, what you can actually do\nkey. Similarly, what you can actually do is that you can also load different\nis that you can also load different\nis that you can also load different different API keys like how you have\ndifferent API keys like how you have\ndifferent API keys like how you have seen over here. Grock API key uh you\nseen over here. Grock API key uh you\nseen over here. Grock API key uh you have Google API key and all right. So we\nhave Google API key and all right. So we\nhave Google API key and all right. So we will be using all these three models uh\nwill be using all these three models uh\nwill be using all these three models uh you know and uh we'll try to see that\nyou know and uh we'll try to see that\nyou know and uh we'll try to see that how we can go ahead and call them. Okay.\nhow we can go ahead and call them. Okay.\nhow we can go ahead and call them. Okay. So once I have initialized or once I\nSo once I have initialized or once I\nSo once I have initialized or once I have loaded all the environment\nhave loaded all the environment\nhave loaded all the environment variables specifically with respect to\nvariables specifically with respect to\nvariables specifically with respect to OpenAI API key, Google API key and gro\nOpenAI API key, Google API key and gro\nOpenAI API key, Google API key and gro API key. First I will show you how you\nAPI key. First I will show you how you\nAPI key. First I will show you how you can load the OpenAI model. Right. So for\ncan load the OpenAI model. Right. So for\ncan load the OpenAI model. Right. So for this uh first of all I will go ahead and\nthis uh first of all I will go ahead and\nthis uh first of all I will go ahead and initialize from langin dot chat_models\ninitialize from langin dot chat_models\ninitialize from langin dot chat_models import init chat model. Okay. So init\nimport init chat model. Okay. So init\nimport init chat model. Okay. So init chat model is one of the libraries that\nchat model is one of the libraries that\nchat model is one of the libraries that we specifically use in order to\nwe specifically use in order to\nwe specifically use in order to initialize any kind of chat model\ninitialize any kind of chat model\ninitialize any kind of chat model itself. Okay. Then we go to the next\nitself. Okay. Then we go to the next\nitself. Okay. Then we go to the next statement. We will use a variable called\nstatement. We will use a variable called\nstatement. We will use a variable called as models. So let's say I will write\nas models. So let's say I will write\nas models. So let's say I will write model. And here I will write init chat\nmodel. And here I will write init chat\nmodel. And here I will write init chat model. And you know by default you can\nmodel. And you know by default you can\nmodel. And you know by default you can directly provide your model name. Okay.\ndirectly provide your model name. Okay.\ndirectly provide your model name. Okay. Now, since I want to show you with\nNow, since I want to show you with\nNow, since I want to show you with OpenAI, so first of all, I will go ahead\nOpenAI, so first of all, I will go ahead\nOpenAI, so first of all, I will go ahead and write GPT. Let's say I want to go\nand write GPT. Let's say I want to go\nand write GPT. Let's say I want to go ahead and try 4.1. Okay, 4.1. And here I\nahead and try 4.1. Okay, 4.1. And here I\nahead and try 4.1. Okay, 4.1. And here I will just go ahead and write models.\nwill just go ahead and write models.\nwill just go ahead and write models. Okay, instead of writing models, I can\nOkay, instead of writing models, I can\nOkay, instead of writing models, I can also go ahead and write model. And now\nalso go ahead and write model. And now\nalso go ahead and write model. And now let's see uh what error I get. Okay,\nlet's see uh what error I get. Okay,\nlet's see uh what error I get. Okay, unable to inform provider for model is\nunable to inform provider for model is\nunable to inform provider for model is equal to okay, GTP I have written. It\nequal to okay, GTP I have written. It\nequal to okay, GTP I have written. It should be GPT 4.1, right? So I hope\nshould be GPT 4.1, right? So I hope\nshould be GPT 4.1, right? So I hope everybody knows different different\neverybody knows different different\neverybody knows different different models that are available in OpenAI. You\nmodels that are available in OpenAI. You\nmodels that are available in OpenAI. You have 4.1, you can have 4.5. Okay. So I\nhave 4.1, you can have 4.5. Okay. So I\nhave 4.1, you can have 4.5. Okay. So I will be using 4.1. So here you can see\nwill be using 4.1. So here you can see\nwill be using 4.1. So here you can see that now once I execute this, it gives\nthat now once I execute this, it gives\nthat now once I execute this, it gives me this information that it is a chat\nme this information that it is a chat\nme this information that it is a chat open AI model. Uh it has maximum output\nopen AI model. Uh it has maximum output\nopen AI model. Uh it has maximum output tokens all this information over here\ntokens all this information over here\ntokens all this information over here with respect to the model. Now comes\nwith respect to the model. Now comes\nwith respect to the model. Now comes like how do I go ahead and invoke the\nlike how do I go ahead and invoke the\nlike how do I go ahead and invoke the model? So let's invoke the model over\nmodel? So let's invoke the model over\nmodel? So let's invoke the model over here. Now in order to invoke the model\nhere. Now in order to invoke the model\nhere. Now in order to invoke the model uh with the help of init chat model or\nuh with the help of init chat model or\nuh with the help of init chat model or directly you can directly use this\ndirectly you can directly use this\ndirectly you can directly use this model.invoke invoke and let's say that I\nmodel.invoke invoke and let's say that I\nmodel.invoke invoke and let's say that I give a message saying that hello hello\ngive a message saying that hello hello\ngive a message saying that hello hello how are you okay how are you now here\nhow are you okay how are you now here\nhow are you okay how are you now here you can see that clearly I've given a\nyou can see that clearly I've given a\nyou can see that clearly I've given a simple message this is a human message\nsimple message this is a human message\nsimple message this is a human message itself and I will be able to get the\nitself and I will be able to get the\nitself and I will be able to get the response now let's go ahead and display\nresponse now let's go ahead and display\nresponse now let's go ahead and display the response so this is a simple like\nthe response so this is a simple like\nthe response so this is a simple like I'm giving this specific input this\nI'm giving this specific input this\nI'm giving this specific input this input goes to the model that is GPT 4.1\ninput goes to the model that is GPT 4.1\ninput goes to the model that is GPT 4.1 and it'll give us some kind of response\nand it'll give us some kind of response\nand it'll give us some kind of response okay now once I go ahead and see the\nokay now once I go ahead and see the\nokay now once I go ahead and see the response You will be able to see that I\nresponse You will be able to see that I\nresponse You will be able to see that I get an AI message content. Hello, I am\nget an AI message content. Hello, I am\nget an AI message content. Hello, I am just a program but I'm here and ready to\njust a program but I'm here and ready to\njust a program but I'm here and ready to help you. How can I assist you today? So\nhelp you. How can I assist you today? So\nhelp you. How can I assist you today? So this is the response from the LLM model\nthis is the response from the LLM model\nthis is the response from the LLM model that is GP 4.1. Okay, if I really want\nthat is GP 4.1. Okay, if I really want\nthat is GP 4.1. Okay, if I really want to just get directly see the content, I\nto just get directly see the content, I\nto just get directly see the content, I can also go ahead and write\ncan also go ahead and write\ncan also go ahead and write response.content.\nresponse.content.\nresponse.content. Okay, once I do this, this is the output\nOkay, once I do this, this is the output\nOkay, once I do this, this is the output of the model that you will be able to\nof the model that you will be able to\nof the model that you will be able to see. So any kind of models that you have\nsee. So any kind of models that you have\nsee. So any kind of models that you have with respect to open AAI let's say I\nwith respect to open AAI let's say I\nwith respect to open AAI let's say I want to go ahead and try GPT 4.5 you can\nwant to go ahead and try GPT 4.5 you can\nwant to go ahead and try GPT 4.5 you can go ahead and change this whatever model\ngo ahead and change this whatever model\ngo ahead and change this whatever model you require or whatever model you really\nyou require or whatever model you really\nyou require or whatever model you really want to use from openAI you can change\nwant to use from openAI you can change\nwant to use from openAI you can change the model name and you can actually get\nthe model name and you can actually get\nthe model name and you can actually get it over here\nit over here\nit over here now comes the next one like how do I\nnow comes the next one like how do I\nnow comes the next one like how do I call a Google geminy model right so here\ncall a Google geminy model right so here\ncall a Google geminy model right so here I'm going to talk about Google Germany\nI'm going to talk about Google Germany\nI'm going to talk about Google Germany model integration\nmodel integration\nmodel integration so let's try this also Okay. So for\nso let's try this also Okay. So for\nso let's try this also Okay. So for Google Germany what I will do I have\nGoogle Germany what I will do I have\nGoogle Germany what I will do I have already loaded the environment. So I\nalready loaded the environment. So I\nalready loaded the environment. So I will write from langchain from langchain\nwill write from langchain from langchain\nwill write from langchain from langchain dot chat models import\ndot chat models import\ndot chat models import init chat model. Okay. So init chat\ninit chat model. Okay. So init chat\ninit chat model. Okay. So init chat model and here I can use this. Okay. So\nmodel and here I can use this. Okay. So\nmodel and here I can use this. Okay. So this is a markdown right. So I will\nthis is a markdown right. So I will\nthis is a markdown right. So I will delete this and let me execute it over\ndelete this and let me execute it over\ndelete this and let me execute it over here. lot of suggestions usually comes\nhere. lot of suggestions usually comes\nhere. lot of suggestions usually comes with uh Google uh this Google\nwith uh Google uh this Google\nwith uh Google uh this Google anti-gravity and I specifically use this\nanti-gravity and I specifically use this\nanti-gravity and I specifically use this I like it because for coding purpose it\nI like it because for coding purpose it\nI like it because for coding purpose it it becomes easy for me to quickly you\nit becomes easy for me to quickly you\nit becomes easy for me to quickly you know autocomplete all the code so uh now\nknow autocomplete all the code so uh now\nknow autocomplete all the code so uh now what I will do is that I will go ahead\nwhat I will do is that I will go ahead\nwhat I will do is that I will go ahead and just use this specific code now see\nand just use this specific code now see\nand just use this specific code now see this code so I'm using from langen chat\nthis code so I'm using from langen chat\nthis code so I'm using from langen chat models import init chat model okay here\nmodels import init chat model okay here\nmodels import init chat model okay here we are loading the Google API key and\nwe are loading the Google API key and\nwe are loading the Google API key and then we are using init Chad model but to\nthen we are using init Chad model but to\nthen we are using init Chad model but to specify this Google Germany model we\nspecify this Google Germany model we\nspecify this Google Germany model we just have to write google genai colon\njust have to write google genai colon\njust have to write google genai colon whatever model name you are specifically\nwhatever model name you are specifically\nwhatever model name you are specifically using from Google gy right there may be\nusing from Google gy right there may be\nusing from Google gy right there may be different different models so I will use\ndifferent different models so I will use\ndifferent different models so I will use google genai colon geminy 2.5 that will\ngoogle genai colon geminy 2.5 that will\ngoogle genai colon geminy 2.5 that will be model and I've just written\nbe model and I've just written\nbe model and I've just written modelinvokes why do parrot talk you know\nmodelinvokes why do parrot talk you know\nmodelinvokes why do parrot talk you know so this is the question that I have\nso this is the question that I have\nso this is the question that I have given up from the human I get the\ngiven up from the human I get the\ngiven up from the human I get the response and I will just go ahead and\nresponse and I will just go ahead and\nresponse and I will just go ahead and display the response so once I display\ndisplay the response so once I display\ndisplay the response so once I display the response. Now you should be able to\nthe response. Now you should be able to\nthe response. Now you should be able to see that the output that you're getting\nsee that the output that you're getting\nsee that the output that you're getting will be from the Gemini 2.5 flashlight.\nwill be from the Gemini 2.5 flashlight.\nwill be from the Gemini 2.5 flashlight. Okay. So I have already initialized I\nOkay. So I have already initialized I\nOkay. So I have already initialized I have already loaded the Google API key.\nhave already loaded the Google API key.\nhave already loaded the Google API key. For the first request I think it is\nFor the first request I think it is\nFor the first request I think it is going to take some amount of time but\ngoing to take some amount of time but\ngoing to take some amount of time but after that uh if my API key is\nafter that uh if my API key is\nafter that uh if my API key is absolutely working I'm actually going to\nabsolutely working I'm actually going to\nabsolutely working I'm actually going to get the response. So this is how you can\nget the response. So this is how you can\nget the response. So this is how you can use init chat model and integrate with\nuse init chat model and integrate with\nuse init chat model and integrate with Google API key. So here you can see that\nGoogle API key. So here you can see that\nGoogle API key. So here you can see that I have got the answer. Parrots don't\nI have got the answer. Parrots don't\nI have got the answer. Parrots don't talk in the same way human dos with\ntalk in the same way human dos with\ntalk in the same way human dos with understanding intent behind every word\nunderstanding intent behind every word\nunderstanding intent behind every word instead they are remarkable something\ninstead they are remarkable something\ninstead they are remarkable something like that right so this is the output\nlike that right so this is the output\nlike that right so this is the output from the Google geminy 2.5 flash slide\nfrom the Google geminy 2.5 flash slide\nfrom the Google geminy 2.5 flash slide now I also want to show you instead of\nnow I also want to show you instead of\nnow I also want to show you instead of using init chat model we can also use\nusing init chat model we can also use\nusing init chat model we can also use one more way okay and that is basically\none more way okay and that is basically\none more way okay and that is basically by using chat open AI chat open AI now\nby using chat open AI chat open AI now\nby using chat open AI chat open AI now in order to use chat open AI what I will\nin order to use chat open AI what I will\nin order to use chat open AI what I will do first of all I will go ahead and see\ndo first of all I will go ahead and see\ndo first of all I will go ahead and see in my requirement txt okay\nin my requirement txt okay\nin my requirement txt okay requirement.txt txt do I have the\nrequirement.txt txt do I have the\nrequirement.txt txt do I have the necessary library that I'm actually\nnecessary library that I'm actually\nnecessary library that I'm actually looking for okay now you may be thinking\nlooking for okay now you may be thinking\nlooking for okay now you may be thinking kish what kind of libraries that you\nkish what kind of libraries that you\nkish what kind of libraries that you specifically require right so here we\nspecifically require right so here we\nspecifically require right so here we have already have lang chain open aai\nhave already have lang chain open aai\nhave already have lang chain open aai you know so langchain openai is\nyou know so langchain openai is\nyou know so langchain openai is basically installed or not so first of\nbasically installed or not so first of\nbasically installed or not so first of all you have to probably go ahead and\nall you have to probably go ahead and\nall you have to probably go ahead and check that so if that is installed I\ncheck that so if that is installed I\ncheck that so if that is installed I think you are good to go over here right\nthink you are good to go over here right\nthink you are good to go over here right now for chat openai if I really want to\nnow for chat openai if I really want to\nnow for chat openai if I really want to use what is the library that I need to\nuse what is the library that I need to\nuse what is the library that I need to import right so here I will go ahead and\nimport right so here I will go ahead and\nimport right so here I will go ahead and write from langun _ openai input chat\nwrite from langun _ openai input chat\nwrite from langun _ openai input chat openai then we will go ahead and\nopenai then we will go ahead and\nopenai then we will go ahead and initialize chat openai\ninitialize chat openai\ninitialize chat openai and here I'm just going to go ahead and\nand here I'm just going to go ahead and\nand here I'm just going to go ahead and give my model name so model is equal to\ngive my model name so model is equal to\ngive my model name so model is equal to and let's say I will just go ahead and\nand let's say I will just go ahead and\nand let's say I will just go ahead and use gpt 4.1 okay so this is my model is\nuse gpt 4.1 okay so this is my model is\nuse gpt 4.1 okay so this is my model is equal to and then if I go ahead and just\nequal to and then if I go ahead and just\nequal to and then if I go ahead and just write response is equal to model.invoke\nwrite response is equal to model.invoke\nwrite response is equal to model.invoke invoke\ninvoke\ninvoke invoke let's say I go ahead and write\ninvoke let's say I go ahead and write\ninvoke let's say I go ahead and write hello how are you I should be able to\nhello how are you I should be able to\nhello how are you I should be able to get the same output like how we got it\nget the same output like how we got it\nget the same output like how we got it over here okay so using init chat model\nover here okay so using init chat model\nover here okay so using init chat model basically gives you an option of\nbasically gives you an option of\nbasically gives you an option of indirectly using this chat open see here\nindirectly using this chat open see here\nindirectly using this chat open see here also when you see this specific model it\nalso when you see this specific model it\nalso when you see this specific model it is nothing but chat open AI so there is\nis nothing but chat open AI so there is\nis nothing but chat open AI so there is also one more way of basically calling\nalso one more way of basically calling\nalso one more way of basically calling this particular model now since you have\nthis particular model now since you have\nthis particular model now since you have chat open AI and in the requirement txt\nchat open AI and in the requirement txt\nchat open AI and in the requirement txt you have also installed lang function\nyou have also installed lang function\nyou have also installed lang function Google gen AI. So here also you have an\nGoogle gen AI. So here also you have an\nGoogle gen AI. So here also you have an option of something called as chat\noption of something called as chat\noption of something called as chat Google generative AI. Okay. So if I go\nGoogle generative AI. Okay. So if I go\nGoogle generative AI. Okay. So if I go ahead and paste it, you can see from\nahead and paste it, you can see from\nahead and paste it, you can see from langchain Google genai import chat\nlangchain Google genai import chat\nlangchain Google genai import chat Google generative AI. I've used again\nGoogle generative AI. I've used again\nGoogle generative AI. I've used again Germany 2.5 flashlight to pirate stock.\nGermany 2.5 flashlight to pirate stock.\nGermany 2.5 flashlight to pirate stock. If I go ahead and see the response\nIf I go ahead and see the response\nIf I go ahead and see the response uh you know this kind of suggestion will\nuh you know this kind of suggestion will\nuh you know this kind of suggestion will come. So don't get worried about it\ncome. So don't get worried about it\ncome. So don't get worried about it because we are not going to use the\nbecause we are not going to use the\nbecause we are not going to use the suggestion over here. Okay. So here is\nsuggestion over here. Okay. So here is\nsuggestion over here. Okay. So here is my output from this specific uh Google\nmy output from this specific uh Google\nmy output from this specific uh Google um means Google Germany integration. Now\num means Google Germany integration. Now\num means Google Germany integration. Now uh these are the ways you can either use\nuh these are the ways you can either use\nuh these are the ways you can either use in chat model, you can use chat open AI.\nin chat model, you can use chat open AI.\nin chat model, you can use chat open AI. If you if you're specifically using\nIf you if you're specifically using\nIf you if you're specifically using OpenAI models, if you want to go ahead\nOpenAI models, if you want to go ahead\nOpenAI models, if you want to go ahead and use uh Google Germany models, then\nand use uh Google Germany models, then\nand use uh Google Germany models, then you can use chat Google generative AI or\nyou can use chat Google generative AI or\nyou can use chat Google generative AI or within the init chat model, you can go\nwithin the init chat model, you can go\nwithin the init chat model, you can go ahead and call the Google uh Google\nahead and call the Google uh Google\nahead and call the Google uh Google models itself, Germany models. Now the\nmodels itself, Germany models. Now the\nmodels itself, Germany models. Now the third one that I'm going to use is Grock\nthird one that I'm going to use is Grock\nthird one that I'm going to use is Grock model integration. Now similarly Grock\nmodel integration. Now similarly Grock\nmodel integration. Now similarly Grock model integration will also be very very\nmodel integration will also be very very\nmodel integration will also be very very easy. Okay.\neasy. Okay.\neasy. Okay. So again two ways. One is by using init\nSo again two ways. One is by using init\nSo again two ways. One is by using init chat model. So here you can see now uh I\nchat model. So here you can see now uh I\nchat model. So here you can see now uh I am imported init chat model. I have my\nam imported init chat model. I have my\nam imported init chat model. I have my environment variable set up for grock\nenvironment variable set up for grock\nenvironment variable set up for grock API key. And then you can see I'm using\nAPI key. And then you can see I'm using\nAPI key. And then you can see I'm using init chat model with my gro. Now this\ninit chat model with my gro. Now this\ninit chat model with my gro. Now this time I'm writing grock over here. See\ntime I'm writing grock over here. See\ntime I'm writing grock over here. See before I I wrote what over here Google\nbefore I I wrote what over here Google\nbefore I I wrote what over here Google genai and the model name of Google\ngenai and the model name of Google\ngenai and the model name of Google Germany but here this time we are\nGermany but here this time we are\nGermany but here this time we are writing grock colon whatever model we\nwriting grock colon whatever model we\nwriting grock colon whatever model we want to specifically use from grock\nwant to specifically use from grock\nwant to specifically use from grock right you that quen is the recent model\nright you that quen is the recent model\nright you that quen is the recent model that has been uploaded over this I used\nthat has been uploaded over this I used\nthat has been uploaded over this I used this and then we are using model invoke\nthis and then we are using model invoke\nthis and then we are using model invoke why do parrot talk I'm able to get the\nwhy do parrot talk I'm able to get the\nwhy do parrot talk I'm able to get the response okay now you have init chat\nresponse okay now you have init chat\nresponse okay now you have init chat model so there should also be an option\nmodel so there should also be an option\nmodel so there should also be an option of chat gro okay so that will be my next\nof chat gro okay so that will be my next\nof chat gro okay so that will be my next one to show you so Here you can see that\none to show you so Here you can see that\none to show you so Here you can see that I'm getting the output. Okay. So why do\nI'm getting the output. Okay. So why do\nI'm getting the output. Okay. So why do parrots talk? Let me think about this. I\nparrots talk? Let me think about this. I\nparrots talk? Let me think about this. I know parrots are mimicking human speech\nknow parrots are mimicking human speech\nknow parrots are mimicking human speech and all and all all the information is\nand all and all all the information is\nand all and all all the information is over here. Now one more way that how we\nover here. Now one more way that how we\nover here. Now one more way that how we can basically call chatgro. So here\ncan basically call chatgro. So here\ncan basically call chatgro. So here you'll be able to see we can use lang\nyou'll be able to see we can use lang\nyou'll be able to see we can use lang grock. So again in the requirement.txt\ngrock. So again in the requirement.txt\ngrock. So again in the requirement.txt you can see we have imported lang grock.\nyou can see we have imported lang grock.\nyou can see we have imported lang grock. So I've imported from lang grock import\nSo I've imported from lang grock import\nSo I've imported from lang grock import chat gro. I'm calling the same model and\nchat gro. I'm calling the same model and\nchat gro. I'm calling the same model and I'm able to get the response. Okay. So\nI'm able to get the response. Okay. So\nI'm able to get the response. Okay. So here you will be able to see that I'm\nhere you will be able to see that I'm\nhere you will be able to see that I'm able to see the output with respect to\nable to see the output with respect to\nable to see the output with respect to the same thing. Okay. So this is uh\nthe same thing. Okay. So this is uh\nthe same thing. Okay. So this is uh pretty much clear I guess with respect\npretty much clear I guess with respect\npretty much clear I guess with respect to the model integration. I think uh we\nto the model integration. I think uh we\nto the model integration. I think uh we have done a pretty good job uh with\nhave done a pretty good job uh with\nhave done a pretty good job uh with respect to this and uh the all the\nrespect to this and uh the all the\nrespect to this and uh the all the integrations specifically uh one two\nintegrations specifically uh one two\nintegrations specifically uh one two ways of simple integration of calling\nways of simple integration of calling\nways of simple integration of calling the loading the model is from init chat\nthe loading the model is from init chat\nthe loading the model is from init chat model or let's say if you are using open\nmodel or let's say if you are using open\nmodel or let's say if you are using open AI then we use chat open AAI if you're\nAI then we use chat open AAI if you're\nAI then we use chat open AAI if you're using Google Germany let's say inside\nusing Google Germany let's say inside\nusing Google Germany let's say inside the init chat model you just write\nthe init chat model you just write\nthe init chat model you just write Google geni and then you basically write\nGoogle geni and then you basically write\nGoogle geni and then you basically write the model name whichever model name you\nthe model name whichever model name you\nthe model name whichever model name you so this you can change I can also use uh\nso this you can change I can also use uh\nso this you can change I can also use uh geminy 2.5 flash. Let's say I want to go\ngeminy 2.5 flash. Let's say I want to go\ngeminy 2.5 flash. Let's say I want to go ahead and use this flash. So here also I\nahead and use this flash. So here also I\nahead and use this flash. So here also I should be able to generate the content.\nshould be able to generate the content.\nshould be able to generate the content. It's like a very very easy approach of\nIt's like a very very easy approach of\nIt's like a very very easy approach of calling any kind of model specific to\ncalling any kind of model specific to\ncalling any kind of model specific to the LLM providers. Okay. So here you can\nthe LLM providers. Okay. So here you can\nthe LLM providers. Okay. So here you can see all the outputs you are basically\nsee all the outputs you are basically\nsee all the outputs you are basically getting. Okay. So this was about model\ngetting. Okay. So this was about model\ngetting. Okay. So this was about model integration. Now in my uh as we go ahead\nintegration. Now in my uh as we go ahead\nintegration. Now in my uh as we go ahead in this series, we will also be talking\nin this series, we will also be talking\nin this series, we will also be talking about the message structure, the\nabout the message structure, the\nabout the message structure, the streaming structure and all. Okay. uh so\nstreaming structure and all. Okay. uh so\nstreaming structure and all. Okay. uh so probably in this series now we should\nprobably in this series now we should\nprobably in this series now we should also go ahead and understand the\nalso go ahead and understand the\nalso go ahead and understand the streaming structure. So let's go ahead\nstreaming structure. So let's go ahead\nstreaming structure. So let's go ahead and discuss about that. So now we are\nand discuss about that. So now we are\nand discuss about that. So now we are going to discuss about this two\ngoing to discuss about this two\ngoing to discuss about this two important topics which is called as\nimportant topics which is called as\nimportant topics which is called as streaming and batch. Okay. Now why\nstreaming and batch. Okay. Now why\nstreaming and batch. Okay. Now why streaming and batch is important. So\nstreaming and batch is important. So\nstreaming and batch is important. So let's say that I go ahead and write\nlet's say that I go ahead and write\nlet's say that I go ahead and write model.invoke. You know how to invoke a\nmodel.invoke. You know how to invoke a\nmodel.invoke. You know how to invoke a specific model right? And let's say that\nspecific model right? And let's say that\nspecific model right? And let's say that I say hey write me a 200 words paragraph\nI say hey write me a 200 words paragraph\nI say hey write me a 200 words paragraph on artificial intelligence. So let's say\non artificial intelligence. So let's say\non artificial intelligence. So let's say if I'm asking this question to my model\nif I'm asking this question to my model\nif I'm asking this question to my model or to my LLM right and here you'll be\nor to my LLM right and here you'll be\nor to my LLM right and here you'll be seeing that we have to wait for the\nseeing that we have to wait for the\nseeing that we have to wait for the response to get generated and be\nresponse to get generated and be\nresponse to get generated and be displayed over here right. So this\ndisplayed over here right. So this\ndisplayed over here right. So this usually happens in invoke right but it\nusually happens in invoke right but it\nusually happens in invoke right but it is always a good practice that we try to\nis always a good practice that we try to\nis always a good practice that we try to stream the output as soon as is soon as\nstream the output as soon as is soon as\nstream the output as soon as is soon as it is generated from the LLM right so\nit is generated from the LLM right so\nit is generated from the LLM right so and that is where streaming can be very\nand that is where streaming can be very\nand that is where streaming can be very very handful so here you can see most\nvery handful so here you can see most\nvery handful so here you can see most model can stream the output content\nmodel can stream the output content\nmodel can stream the output content while it is being generated now in this\nwhile it is being generated now in this\nwhile it is being generated now in this particular case we had to wait till the\nparticular case we had to wait till the\nparticular case we had to wait till the LLM completely generated the content and\nLLM completely generated the content and\nLLM completely generated the content and then finally it displayed the output but\nthen finally it displayed the output but\nthen finally it displayed the output but in the case of streaming what we do is\nin the case of streaming what we do is\nin the case of streaming what we do is that we can also stream the output of\nthat we can also stream the output of\nthat we can also stream the output of the LLM model while it is being\nthe LLM model while it is being\nthe LLM model while it is being generated. So by displaying the output\ngenerated. So by displaying the output\ngenerated. So by displaying the output progressively streaming significantly\nprogressively streaming significantly\nprogressively streaming significantly improves user experience particularly\nimproves user experience particularly\nimproves user experience particularly for long responses. So for this we have\nfor long responses. So for this we have\nfor long responses. So for this we have to use this function which is called as\nto use this function which is called as\nto use this function which is called as stream. Okay, this returns an iterator\nstream. Okay, this returns an iterator\nstream. Okay, this returns an iterator that yields output chunk from the LLM\nthat yields output chunk from the LLM\nthat yields output chunk from the LLM and they also display it over here. So\nand they also display it over here. So\nand they also display it over here. So let's try to see that how this stream\nlet's try to see that how this stream\nlet's try to see that how this stream will basically work right now in order\nwill basically work right now in order\nwill basically work right now in order to do or work with streaming we will be\nto do or work with streaming we will be\nto do or work with streaming we will be using this inbuilt function called as\nusing this inbuilt function called as\nusing this inbuilt function called as model.stream stream. Okay. And let's say\nmodel.stream stream. Okay. And let's say\nmodel.stream stream. Okay. And let's say now I go ahead and ask, hey, write me a\nnow I go ahead and ask, hey, write me a\nnow I go ahead and ask, hey, write me a 200 words paragraph. Okay. On artificial\n200 words paragraph. Okay. On artificial\n200 words paragraph. Okay. On artificial intelligence. So let's display this\nintelligence. So let's display this\nintelligence. So let's display this right now. Okay. Let's execute this. So\nright now. Okay. Let's execute this. So\nright now. Okay. Let's execute this. So here you can see that it is creating a\nhere you can see that it is creating a\nhere you can see that it is creating a generator object. But our main aim is\ngenerator object. But our main aim is\ngenerator object. But our main aim is that this is of a stream type, right? We\nthat this is of a stream type, right? We\nthat this is of a stream type, right? We also need to display the output from the\nalso need to display the output from the\nalso need to display the output from the stream. So what I will do? I will use a\nstream. So what I will do? I will use a\nstream. So what I will do? I will use a for loop. So I'll say for chunk in\nfor loop. So I'll say for chunk in\nfor loop. So I'll say for chunk in model.stream\nmodel.stream\nmodel.stream and let's display sorry let's display\nand let's display sorry let's display\nand let's display sorry let's display the stream output. Okay. So here I will\nthe stream output. Okay. So here I will\nthe stream output. Okay. So here I will go ahead and print and I'll just go\ngo ahead and print and I'll just go\ngo ahead and print and I'll just go ahead and write chunk dot text. Now\nahead and write chunk dot text. Now\nahead and write chunk dot text. Now let's display this. Now here you can see\nlet's display this. Now here you can see\nlet's display this. Now here you can see that once we go ahead and execute this\nthat once we go ahead and execute this\nthat once we go ahead and execute this it did not wait for the entire content\nit did not wait for the entire content\nit did not wait for the entire content to be generated. So what we have done is\nto be generated. So what we have done is\nto be generated. So what we have done is that as the content is being generated\nthat as the content is being generated\nthat as the content is being generated from the LLM, the LLM is giving you the\nfrom the LLM, the LLM is giving you the\nfrom the LLM, the LLM is giving you the output. It is also getting displayed\noutput. It is also getting displayed\noutput. It is also getting displayed over here. Okay. In a much more better\nover here. Okay. In a much more better\nover here. Okay. In a much more better way for you all to see, what I will do,\nway for you all to see, what I will do,\nway for you all to see, what I will do, I will use some special character to\nI will use some special character to\nI will use some special character to just you know to just show you the\njust you know to just show you the\njust you know to just show you the content that is basically generated. So\ncontent that is basically generated. So\ncontent that is basically generated. So I will use this two parameter end is\nI will use this two parameter end is\nI will use this two parameter end is equal to that basically means I'm using\nequal to that basically means I'm using\nequal to that basically means I'm using some kind of delimiter over here. as\nsome kind of delimiter over here. as\nsome kind of delimiter over here. as soon as any token is generated from the\nsoon as any token is generated from the\nsoon as any token is generated from the llm and we also going to use flush is\nllm and we also going to use flush is\nllm and we also going to use flush is equal to true. Okay. Now see the output\nequal to true. Okay. Now see the output\nequal to true. Okay. Now see the output I have written write me a 200 work\nI have written write me a 200 work\nI have written write me a 200 work paragraph and here you can see that we\nparagraph and here you can see that we\nparagraph and here you can see that we are generating this particular text and\nare generating this particular text and\nare generating this particular text and this text is basically getting generated\nthis text is basically getting generated\nthis text is basically getting generated over here. Okay. Now let's try some more\nover here. Okay. Now let's try some more\nover here. Okay. Now let's try some more uh some more good things inside this uh\nuh some more good things inside this uh\nuh some more good things inside this uh instead of just writing like this you\ninstead of just writing like this you\ninstead of just writing like this you know I will also go ahead and uh you\nknow I will also go ahead and uh you\nknow I will also go ahead and uh you know just try to display something over\nknow just try to display something over\nknow just try to display something over here so let's go ahead and do this and\nhere so let's go ahead and do this and\nhere so let's go ahead and do this and here you can see that I'm just writing\nhere you can see that I'm just writing\nhere you can see that I'm just writing why do parrots have colorful feathers\nwhy do parrots have colorful feathers\nwhy do parrots have colorful feathers okay feathers so now it is going to\nokay feathers so now it is going to\nokay feathers so now it is going to print the chunk text and here you can\nprint the chunk text and here you can\nprint the chunk text and here you can see that paragraph by paragraph as the\nsee that paragraph by paragraph as the\nsee that paragraph by paragraph as the content is basically getting generated\ncontent is basically getting generated\ncontent is basically getting generated it is also being displayed in the\nit is also being displayed in the\nit is also being displayed in the output. So this is an example of stream\noutput. So this is an example of stream\noutput. So this is an example of stream and the main thing is that you can\nand the main thing is that you can\nand the main thing is that you can stream the output from the llm while it\nstream the output from the llm while it\nstream the output from the llm while it is being generated. Right? So we don't\nis being generated. Right? So we don't\nis being generated. Right? So we don't have to wait till the entire text is\nhave to wait till the entire text is\nhave to wait till the entire text is generated. Now if I go ahead and ask the\ngenerated. Now if I go ahead and ask the\ngenerated. Now if I go ahead and ask the same question over here. So let's say I\nsame question over here. So let's say I\nsame question over here. So let's say I go ahead and ask the same question and\ngo ahead and ask the same question and\ngo ahead and ask the same question and here instead of you know why do parrots\nhere instead of you know why do parrots\nhere instead of you know why do parrots have colorful\nhave colorful\nhave colorful feathers? If I just go ahead and use\nfeathers? If I just go ahead and use\nfeathers? If I just go ahead and use model.invoke invoke. So here what I will\nmodel.invoke invoke. So here what I will\nmodel.invoke invoke. So here what I will do I'll remove all these things. Okay, I\ndo I'll remove all these things. Okay, I\ndo I'll remove all these things. Okay, I will remove all these things and we will\nwill remove all these things and we will\nwill remove all these things and we will try to generate it by using\ntry to generate it by using\ntry to generate it by using model.invoke. Now see we have to wait\nmodel.invoke. Now see we have to wait\nmodel.invoke. Now see we have to wait for the output. Okay, model.invoke it is\nfor the output. Okay, model.invoke it is\nfor the output. Okay, model.invoke it is giving me some syntax. No worries I will\ngiving me some syntax. No worries I will\ngiving me some syntax. No worries I will fix it. Now see I'll wait for the\nfix it. Now see I'll wait for the\nfix it. Now see I'll wait for the output. I'm waiting waiting waiting and\noutput. I'm waiting waiting waiting and\noutput. I'm waiting waiting waiting and then finally the response gets\nthen finally the response gets\nthen finally the response gets generated. Right? Once the entire\ngenerated. Right? Once the entire\ngenerated. Right? Once the entire response is output is created then only\nresponse is output is created then only\nresponse is output is created then only it'll get generated. But in this case of\nit'll get generated. But in this case of\nit'll get generated. But in this case of streaming as it is generated we are also\nstreaming as it is generated we are also\nstreaming as it is generated we are also able to see. Okay. Now similarly there\nable to see. Okay. Now similarly there\nable to see. Okay. Now similarly there is also one more concept which is called\nis also one more concept which is called\nis also one more concept which is called as batch. Okay. Now batch is a\nas batch. Okay. Now batch is a\nas batch. Okay. Now batch is a collection of independent requests to a\ncollection of independent requests to a\ncollection of independent requests to a model which can significantly improve\nmodel which can significantly improve\nmodel which can significantly improve performance and reduce cost as the\nperformance and reduce cost as the\nperformance and reduce cost as the processing can be done parallel. Now\nprocessing can be done parallel. Now\nprocessing can be done parallel. Now there may be scenario that you may have\nthere may be scenario that you may have\nthere may be scenario that you may have multiple inputs. So let's say I will go\nmultiple inputs. So let's say I will go\nmultiple inputs. So let's say I will go ahead and create some kind of response.\nahead and create some kind of response.\nahead and create some kind of response. See, I'm using model.batch function.\nSee, I'm using model.batch function.\nSee, I'm using model.batch function. Inside this, I have a list of inputs.\nInside this, I have a list of inputs.\nInside this, I have a list of inputs. Like my first question is why do parrots\nLike my first question is why do parrots\nLike my first question is why do parrots have colorful weathers? I'm writing how\nhave colorful weathers? I'm writing how\nhave colorful weathers? I'm writing how do airplane fly? What is quantum\ndo airplane fly? What is quantum\ndo airplane fly? What is quantum computing? Now, I have three different\ncomputing? Now, I have three different\ncomputing? Now, I have three different questions and I want to send all this\nquestions and I want to send all this\nquestions and I want to send all this question as an input to the LLM model in\nquestion as an input to the LLM model in\nquestion as an input to the LLM model in a parallel way, right? I want the output\na parallel way, right? I want the output\na parallel way, right? I want the output parallelly, right? So that is what it is\nparallelly, right? So that is what it is\nparallelly, right? So that is what it is over here. You can see batch is a\nover here. You can see batch is a\nover here. You can see batch is a collection of independent requests to a\ncollection of independent requests to a\ncollection of independent requests to a model which can significantly improve\nmodel which can significantly improve\nmodel which can significantly improve performance and reduce cost as the\nperformance and reduce cost as the\nperformance and reduce cost as the processing can be done parallel. So if\nprocessing can be done parallel. So if\nprocessing can be done parallel. So if I'm giving three inputs, this will go\nI'm giving three inputs, this will go\nI'm giving three inputs, this will go parallelly to the model and generate the\nparallelly to the model and generate the\nparallelly to the model and generate the output. So let's go ahead and see the\noutput. So let's go ahead and see the\noutput. So let's go ahead and see the output. Now here you can see all these\noutput. Now here you can see all these\noutput. Now here you can see all these three questions has gone together by\nthree questions has gone together by\nthree questions has gone together by using this model.batch and automatically\nusing this model.batch and automatically\nusing this model.batch and automatically you'll be able to see all the output all\nyou'll be able to see all the output all\nyou'll be able to see all the output all at once. Okay, three responses it will\nat once. Okay, three responses it will\nat once. Okay, three responses it will generate and you are able to see the\ngenerate and you are able to see the\ngenerate and you are able to see the output all at once. Similarly, I can\noutput all at once. Similarly, I can\noutput all at once. Similarly, I can also set one more parameter inside this\nalso set one more parameter inside this\nalso set one more parameter inside this which is basically called as max\nwhich is basically called as max\nwhich is basically called as max currency. Right? So there is a config\ncurrency. Right? So there is a config\ncurrency. Right? So there is a config parameter which you can basically add\nparameter which you can basically add\nparameter which you can basically add along with this model.batch\nalong with this model.batch\nalong with this model.batch functionality which says that how many\nfunctionality which says that how many\nfunctionality which says that how many parallel calls you can actually make. So\nparallel calls you can actually make. So\nparallel calls you can actually make. So here you can go ahead and set it max\nhere you can go ahead and set it max\nhere you can go ahead and set it max concurrency is equal to five and then\nconcurrency is equal to five and then\nconcurrency is equal to five and then probably go ahead and do it right. So\nprobably go ahead and do it right. So\nprobably go ahead and do it right. So anyhow my questions are three if I'm\nanyhow my questions are three if I'm\nanyhow my questions are three if I'm giving 10 10 different questions all at\ngiving 10 10 different questions all at\ngiving 10 10 different questions all at a time. So it'll take five five and then\na time. So it'll take five five and then\na time. So it'll take five five and then it'll probably send it to the LLM and\nit'll probably send it to the LLM and\nit'll probably send it to the LLM and generate the output. Right? So I hope\ngenerate the output. Right? So I hope\ngenerate the output. Right? So I hope you got a very clear idea about\nyou got a very clear idea about\nyou got a very clear idea about streaming and batch and this is\nstreaming and batch and this is\nstreaming and batch and this is necessary because if you are working for\nnecessary because if you are working for\nnecessary because if you are working for any company developing chat bots most of\nany company developing chat bots most of\nany company developing chat bots most of the time you are definitely going to use\nthe time you are definitely going to use\nthe time you are definitely going to use streaming but there may be scenarios\nstreaming but there may be scenarios\nstreaming but there may be scenarios that you also want to probably go ahead\nthat you also want to probably go ahead\nthat you also want to probably go ahead and use batch functionality. So I hope\nand use batch functionality. So I hope\nand use batch functionality. So I hope you like this particular video.\nyou like this particular video.\nyou like this particular video. So I hope you have understood this. Now\nSo I hope you have understood this. Now\nSo I hope you have understood this. Now let's go ahead towards the next section\nlet's go ahead towards the next section\nlet's go ahead towards the next section wherein we are going to understand about\nwherein we are going to understand about\nwherein we are going to understand about tools creation. So guys till now we have\ntools creation. So guys till now we have\ntools creation. So guys till now we have already discussed about streaming and\nalready discussed about streaming and\nalready discussed about streaming and batch and along with this we also saw\nbatch and along with this we also saw\nbatch and along with this we also saw the model integration like how you can\nthe model integration like how you can\nthe model integration like how you can go ahead and integrate different kind of\ngo ahead and integrate different kind of\ngo ahead and integrate different kind of LLMs with the help of two important\nLLMs with the help of two important\nLLMs with the help of two important functionality or two important\nfunctionality or two important\nfunctionality or two important libraries. one is initate chat model and\nlibraries. one is initate chat model and\nlibraries. one is initate chat model and you can see that we have also used chat\nyou can see that we have also used chat\nyou can see that we have also used chat gro chat open AI and uh along with that\ngro chat open AI and uh along with that\ngro chat open AI and uh along with that we also had chat Google generative AI\nwe also had chat Google generative AI\nwe also had chat Google generative AI right now it's time that uh we move\nright now it's time that uh we move\nright now it's time that uh we move towards one more step ahead and we talk\ntowards one more step ahead and we talk\ntowards one more step ahead and we talk about how to go ahead and work with\nabout how to go ahead and work with\nabout how to go ahead and work with tools so if you remember we had\ntools so if you remember we had\ntools so if you remember we had discussed about a simple agent right in\ndiscussed about a simple agent right in\ndiscussed about a simple agent right in an agent basically an LLM will be\nan agent basically an LLM will be\nan agent basically an LLM will be connected to a tool now this tool is\nconnected to a tool now this tool is\nconnected to a tool now this tool is just some kind of functionality It can\njust some kind of functionality It can\njust some kind of functionality It can be a API request. It can be uh inbuilt\nbe a API request. It can be uh inbuilt\nbe a API request. It can be uh inbuilt tools. It can be news reporting tools.\ntools. It can be news reporting tools.\ntools. It can be news reporting tools. It can be Google search engine tool. It\nIt can be Google search engine tool. It\nIt can be Google search engine tool. It can be any kind of independent\ncan be any kind of independent\ncan be any kind of independent functionality tool. Right? Now in this\nfunctionality tool. Right? Now in this\nfunctionality tool. Right? Now in this series of videos now we are going to\nseries of videos now we are going to\nseries of videos now we are going to understand like how we are going to go\nunderstand like how we are going to go\nunderstand like how we are going to go ahead and create tools. Right. So first\nahead and create tools. Right. So first\nahead and create tools. Right. So first of all what I will do I will go ahead\nof all what I will do I will go ahead\nof all what I will do I will go ahead and create a ipynb file. And here you\nand create a ipynb file. And here you\nand create a ipynb file. And here you can see tools definition is basically\ncan see tools definition is basically\ncan see tools definition is basically given. Model can request to call tools\ngiven. Model can request to call tools\ngiven. Model can request to call tools that perform tasks such as fetching data\nthat perform tasks such as fetching data\nthat perform tasks such as fetching data from a database, searching the web or\nfrom a database, searching the web or\nfrom a database, searching the web or running code. Tools are pairing of a\nrunning code. Tools are pairing of a\nrunning code. Tools are pairing of a schema including the name of the tool,\nschema including the name of the tool,\nschema including the name of the tool, argument and definition and function or\nargument and definition and function or\nargument and definition and function or core routine to execute. So here what we\ncore routine to execute. So here what we\ncore routine to execute. So here what we are basically going to do is that first\nare basically going to do is that first\nare basically going to do is that first of all I will show you how you can\nof all I will show you how you can\nof all I will show you how you can basically create a tool right in a\nbasically create a tool right in a\nbasically create a tool right in a simple way. So first of all as usual I\nsimple way. So first of all as usual I\nsimple way. So first of all as usual I will use one of my LLM model. The LLM\nwill use one of my LLM model. The LLM\nwill use one of my LLM model. The LLM model that we are going to use is Grock\nmodel that we are going to use is Grock\nmodel that we are going to use is Grock Quen 332B. And here you can see that I'm\nQuen 332B. And here you can see that I'm\nQuen 332B. And here you can see that I'm also able to invoke the model. This we\nalso able to invoke the model. This we\nalso able to invoke the model. This we have already learned in the previous\nhave already learned in the previous\nhave already learned in the previous section. Right? So inside this response\nsection. Right? So inside this response\nsection. Right? So inside this response you will be able to understand like what\nyou will be able to understand like what\nyou will be able to understand like what is the response that you're getting from\nis the response that you're getting from\nis the response that you're getting from the LLM.\nthe LLM.\nthe LLM. But with this particular model I need to\nBut with this particular model I need to\nBut with this particular model I need to integrate some tool. Okay. Now in order\nintegrate some tool. Okay. Now in order\nintegrate some tool. Okay. Now in order to integrate what tool I will first of\nto integrate what tool I will first of\nto integrate what tool I will first of all create and what is the basic schema\nall create and what is the basic schema\nall create and what is the basic schema definition for creating a tool. So\ndefinition for creating a tool. So\ndefinition for creating a tool. So whenever we need to create a tool first\nwhenever we need to create a tool first\nwhenever we need to create a tool first of all I will go ahead and you know use\nof all I will go ahead and you know use\nof all I will go ahead and you know use a import function from langen.tools I\na import function from langen.tools I\na import function from langen.tools I will import something called as tool.\nwill import something called as tool.\nwill import something called as tool. Okay. Now this tool library that we are\nOkay. Now this tool library that we are\nOkay. Now this tool library that we are importing over here will be used as a\nimporting over here will be used as a\nimporting over here will be used as a decorator. So when we use this as a\ndecorator. So when we use this as a\ndecorator. So when we use this as a decorator on top of any function that\ndecorator on top of any function that\ndecorator on top of any function that function will actually become a tool in\nfunction will actually become a tool in\nfunction will actually become a tool in langin. Okay. So here first of all we'll\nlangin. Okay. So here first of all we'll\nlangin. Okay. So here first of all we'll write add the rate tool. I will go ahead\nwrite add the rate tool. I will go ahead\nwrite add the rate tool. I will go ahead and define my function. Let's say this\nand define my function. Let's say this\nand define my function. Let's say this function is nothing but get weather. Now\nfunction is nothing but get weather. Now\nfunction is nothing but get weather. Now inside this get weather I will be using\ninside this get weather I will be using\ninside this get weather I will be using a variable which is called as location.\na variable which is called as location.\na variable which is called as location. And this will give you a string type.\nAnd this will give you a string type.\nAnd this will give you a string type. Okay. And here I will also go ahead and\nOkay. And here I will also go ahead and\nOkay. And here I will also go ahead and define. See if you see the definition\ndefine. See if you see the definition\ndefine. See if you see the definition whenever we talk about tool it is\nwhenever we talk about tool it is\nwhenever we talk about tool it is nothing but is pairing of schema\nnothing but is pairing of schema\nnothing but is pairing of schema including the name of a tool description\nincluding the name of a tool description\nincluding the name of a tool description and or argument definition. So here I am\nand or argument definition. So here I am\nand or argument definition. So here I am going to go ahead and provide some dock\ngoing to go ahead and provide some dock\ngoing to go ahead and provide some dock string. Now this dock string will play a\nstring. Now this dock string will play a\nstring. Now this dock string will play a very important role. I will talk about\nvery important role. I will talk about\nvery important role. I will talk about it. Okay. So I'll say at a location.\nit. Okay. So I'll say at a location.\nit. Okay. So I'll say at a location. Okay. So get weather at a location. Now\nOkay. So get weather at a location. Now\nOkay. So get weather at a location. Now this definition uh this schema that we\nthis definition uh this schema that we\nthis definition uh this schema that we have or this doc string that we have\nhave or this doc string that we have\nhave or this doc string that we have defined over here this is important\ndefined over here this is important\ndefined over here this is important because when we bind this tool with the\nbecause when we bind this tool with the\nbecause when we bind this tool with the LLM the LLM will be able to identify the\nLLM the LLM will be able to identify the\nLLM the LLM will be able to identify the functionality of this particular\nfunctionality of this particular\nfunctionality of this particular function okay what exactly it is doing\nfunction okay what exactly it is doing\nfunction okay what exactly it is doing from this particular dock string okay so\nfrom this particular dock string okay so\nfrom this particular dock string okay so that's the reason we have written this\nthat's the reason we have written this\nthat's the reason we have written this dock string so now I will go ahead and\ndock string so now I will go ahead and\ndock string so now I will go ahead and say return and here you can write any\nsay return and here you can write any\nsay return and here you can write any functionality that you want okay so\nfunctionality that you want okay so\nfunctionality that you want okay so let's I'm hard coding right now the\nlet's I'm hard coding right now the\nlet's I'm hard coding right now the temperature. What you can actually do is\ntemperature. What you can actually do is\ntemperature. What you can actually do is that you can go ahead and hit a API\nthat you can go ahead and hit a API\nthat you can go ahead and hit a API request or database request over here\nrequest or database request over here\nrequest or database request over here and get the information. So I'll write\nand get the information. So I'll write\nand get the information. So I'll write it's sunny in this specific location\nit's sunny in this specific location\nit's sunny in this specific location which location I'm actually using. Okay,\nwhich location I'm actually using. Okay,\nwhich location I'm actually using. Okay, by default I'm saying it's sunny. Okay,\nby default I'm saying it's sunny. Okay,\nby default I'm saying it's sunny. Okay, now this is done right. I have a tool\nnow this is done right. I have a tool\nnow this is done right. I have a tool over here. Now this tool needs to be\nover here. Now this tool needs to be\nover here. Now this tool needs to be binded with my model, right? So if you\nbinded with my model, right? So if you\nbinded with my model, right? So if you see over here, if I want to bind this\nsee over here, if I want to bind this\nsee over here, if I want to bind this LLM with this particular tool, how do I\nLLM with this particular tool, how do I\nLLM with this particular tool, how do I do it? Okay. So for that I will be using\ndo it? Okay. So for that I will be using\ndo it? Okay. So for that I will be using model dotbind\nmodel dotbind\nmodel dotbind tools. Okay. So bind tools and here we\ntools. Okay. So bind tools and here we\ntools. Okay. So bind tools and here we are basically going to use this\nare basically going to use this\nare basically going to use this particular tool which is called as get\nparticular tool which is called as get\nparticular tool which is called as get weather. Okay. Get weather and here we\nweather. Okay. Get weather and here we\nweather. Okay. Get weather and here we can go ahead and write model_with\ncan go ahead and write model_with\ncan go ahead and write model_with tools. Now this is one way. Okay. The\ntools. Now this is one way. Okay. The\ntools. Now this is one way. Okay. The other way is that what we have learned\nother way is that what we have learned\nother way is that what we have learned right we can directly use this we can\nright we can directly use this we can\nright we can directly use this we can use this function right create agent\nuse this function right create agent\nuse this function right create agent give the model name give the tools and\ngive the model name give the tools and\ngive the model name give the tools and automatically this will get created\nautomatically this will get created\nautomatically this will get created right that we have already shown and\nright that we have already shown and\nright that we have already shown and this is one of the functionality which\nthis is one of the functionality which\nthis is one of the functionality which we used to use before also that is\nwe used to use before also that is\nwe used to use before also that is nothing but binding tools okay now once\nnothing but binding tools okay now once\nnothing but binding tools okay now once I bind this tool the next thing is that\nI bind this tool the next thing is that\nI bind this tool the next thing is that how do I call this okay see now in order\nhow do I call this okay see now in order\nhow do I call this okay see now in order to call it I will say model bit tools\nto call it I will say model bit tools\nto call it I will say model bit tools dot invoke what's the weather like in\ndot invoke what's the weather like in\ndot invoke what's the weather like in Boston and here now I can go ahead and\nBoston and here now I can go ahead and\nBoston and here now I can go ahead and iterate through this response tool calls\niterate through this response tool calls\niterate through this response tool calls see if I just go ahead and print the\nsee if I just go ahead and print the\nsee if I just go ahead and print the response first of all you'll be able to\nresponse first of all you'll be able to\nresponse first of all you'll be able to see\nsee\nsee I'll print this response\nI'll print this response\nI'll print this response now when we are printing this response\nnow when we are printing this response\nnow when we are printing this response here you'll be able to see that the\nhere you'll be able to see that the\nhere you'll be able to see that the reasoning contain is the user is asking\nreasoning contain is the user is asking\nreasoning contain is the user is asking for about the weather in Boston I need\nfor about the weather in Boston I need\nfor about the weather in Boston I need to use the get weather function see\nto use the get weather function see\nto use the get weather function see automatically now LLM is able to make a\nautomatically now LLM is able to make a\nautomatically now LLM is able to make a decision that what functionality needs\ndecision that what functionality needs\ndecision that what functionality needs to be called and here you can see that\nto be called and here you can see that\nto be called and here you can see that we also printing the tool calls that it\nwe also printing the tool calls that it\nwe also printing the tool calls that it is doing. So tool call of name is\nis doing. So tool call of name is\nis doing. So tool call of name is nothing but weather and argument it is\nnothing but weather and argument it is\nnothing but weather and argument it is basically requiring is nothing but\nbasically requiring is nothing but\nbasically requiring is nothing but location. Okay. So this is the most\nlocation. Okay. So this is the most\nlocation. Okay. So this is the most simplest way of you know working with a\nsimplest way of you know working with a\nsimplest way of you know working with a tool just directly go ahead and use a\ntool just directly go ahead and use a\ntool just directly go ahead and use a decorator provide some kind of schema or\ndecorator provide some kind of schema or\ndecorator provide some kind of schema or dock string and just go ahead and bind\ndock string and just go ahead and bind\ndock string and just go ahead and bind it with the tool. Either you can do like\nit with the tool. Either you can do like\nit with the tool. Either you can do like this or if you're directly creating an\nthis or if you're directly creating an\nthis or if you're directly creating an agent you would just define that\nagent you would just define that\nagent you would just define that particular schema means the function and\nparticular schema means the function and\nparticular schema means the function and then you use this create agent with the\nthen you use this create agent with the\nthen you use this create agent with the model name with the tool name and here\nmodel name with the tool name and here\nmodel name with the tool name and here you'll be able to get it right. This is\nyou'll be able to get it right. This is\nyou'll be able to get it right. This is the most simplest way. Okay.\nthe most simplest way. Okay.\nthe most simplest way. Okay. Now I really want to show you one more\nNow I really want to show you one more\nNow I really want to show you one more important technique which is called as\nimportant technique which is called as\nimportant technique which is called as tool execution loop.\ntool execution loop.\ntool execution loop. Tool execution loop. Now see first of\nTool execution loop. Now see first of\nTool execution loop. Now see first of all inside this what I will do I will\nall inside this what I will do I will\nall inside this what I will do I will paste this code now see initially we set\npaste this code now see initially we set\npaste this code now see initially we set up a message from the role user saying\nup a message from the role user saying\nup a message from the role user saying that the user is sending the message\nthat the user is sending the message\nthat the user is sending the message what's the weather in Boston now we are\nwhat's the weather in Boston now we are\nwhat's the weather in Boston now we are using model with tools do invoke of\nusing model with tools do invoke of\nusing model with tools do invoke of message then here I will be getting my\nmessage then here I will be getting my\nmessage then here I will be getting my AI message and inside this message we\nAI message and inside this message we\nAI message and inside this message we are also appending this particular AI\nare also appending this particular AI\nare also appending this particular AI message and you know inside this AI\nmessage and you know inside this AI\nmessage and you know inside this AI message it will be nothing but it will\nmessage it will be nothing but it will\nmessage it will be nothing but it will be a tool call right we are making a\nbe a tool call right we are making a\nbe a tool call right we are making a tool call which is nothing but weather\ntool call which is nothing but weather\ntool call which is nothing but weather data\ndata\ndata Right? We are making a tool call over\nRight? We are making a tool call over\nRight? We are making a tool call over here. Right? Now for tool calls in AI\nhere. Right? Now for tool calls in AI\nhere. Right? Now for tool calls in AI message.tool calls. Now that get\nmessage.tool calls. Now that get\nmessage.tool calls. Now that get weather.invoke of tool call we are\nweather.invoke of tool call we are\nweather.invoke of tool call we are doing. See at the end of the day if you\ndoing. See at the end of the day if you\ndoing. See at the end of the day if you see whenever we make a tool call we are\nsee whenever we make a tool call we are\nsee whenever we make a tool call we are basically calling get weather and\nbasically calling get weather and\nbasically calling get weather and internally we are using this get\ninternally we are using this get\ninternally we are using this get weather.invoke of tool call so that we\nweather.invoke of tool call so that we\nweather.invoke of tool call so that we get the response from this. See over\nget the response from this. See over\nget the response from this. See over here if I go ahead and show you when we\nhere if I go ahead and show you when we\nhere if I go ahead and show you when we are making the tool call the tool call\nare making the tool call the tool call\nare making the tool call the tool call will go ahead and provide us the output\nwill go ahead and provide us the output\nwill go ahead and provide us the output that is nothing but the context and with\nthat is nothing but the context and with\nthat is nothing but the context and with the help of this particular code get\nthe help of this particular code get\nthe help of this particular code get weather.invoke of tool call we are\nweather.invoke of tool call we are\nweather.invoke of tool call we are getting the tool results and that also\ngetting the tool results and that also\ngetting the tool results and that also we are appending it inside our message\nwe are appending it inside our message\nwe are appending it inside our message and finally you'll be able to see the\nand finally you'll be able to see the\nand finally you'll be able to see the message text since we are using model\nmessage text since we are using model\nmessage text since we are using model with tool.invoke invoke now see I will\nwith tool.invoke invoke now see I will\nwith tool.invoke invoke now see I will execute this step by step you'll be\nexecute this step by step you'll be\nexecute this step by step you'll be seeing the weather in Boston is sunny\nseeing the weather in Boston is sunny\nseeing the weather in Boston is sunny right and if you go ahead and just see\nright and if you go ahead and just see\nright and if you go ahead and just see this messages section messages section\nthis messages section messages section\nthis messages section messages section you should be able to see the role the\nyou should be able to see the role the\nyou should be able to see the role the AI message that you got and the tool\nAI message that you got and the tool\nAI message that you got and the tool message like when the tool got executed\nmessage like when the tool got executed\nmessage like when the tool got executed it is giving this particular response at\nit is giving this particular response at\nit is giving this particular response at sunny in Boston so when model with tools\nsunny in Boston so when model with tools\nsunny in Boston so when model with tools do invoke it is going it is basically\ndo invoke it is going it is basically\ndo invoke it is going it is basically getting the context from the tool and it\ngetting the context from the tool and it\ngetting the context from the tool and it is displaying the output Right. So\nis displaying the output Right. So\nis displaying the output Right. So that's easy with respect to the tool\nthat's easy with respect to the tool\nthat's easy with respect to the tool execution loop. Okay. So I hope uh you\nexecution loop. Okay. So I hope uh you\nexecution loop. Okay. So I hope uh you got an idea with respect to tools. Very\ngot an idea with respect to tools. Very\ngot an idea with respect to tools. Very basic way of creating this. Now the main\nbasic way of creating this. Now the main\nbasic way of creating this. Now the main thing is that internally you can write\nthing is that internally you can write\nthing is that internally you can write any definition you want to use inbuilt\nany definition you want to use inbuilt\nany definition you want to use inbuilt tools that are available in lang chain.\ntools that are available in lang chain.\ntools that are available in lang chain. You can directly go ahead and write the\nYou can directly go ahead and write the\nYou can directly go ahead and write the code. The most important thing is that\ncode. The most important thing is that\ncode. The most important thing is that what response you are basically\nwhat response you are basically\nwhat response you are basically generating out of that particular tool.\ngenerating out of that particular tool.\ngenerating out of that particular tool. Right? So this was a quick revision on\nRight? So this was a quick revision on\nRight? So this was a quick revision on understanding about how you can actually\nunderstanding about how you can actually\nunderstanding about how you can actually specifically work with a tool. So now as\nspecifically work with a tool. So now as\nspecifically work with a tool. So now as we go ahead now we are also going to\nwe go ahead now we are also going to\nwe go ahead now we are also going to discuss about one more uh important\ndiscuss about one more uh important\ndiscuss about one more uh important thing that is called as messages. Now\nthing that is called as messages. Now\nthing that is called as messages. Now what are the different types of messages\nwhat are the different types of messages\nwhat are the different types of messages there is something called a system\nthere is something called a system\nthere is something called a system message AI message human message. So\nmessage AI message human message. So\nmessage AI message human message. So that part we will go ahead and discuss\nthat part we will go ahead and discuss\nthat part we will go ahead and discuss it. So yes uh let's go ahead and discuss\nit. So yes uh let's go ahead and discuss\nit. So yes uh let's go ahead and discuss about that. So guys, till now we have\nabout that. So guys, till now we have\nabout that. So guys, till now we have covered various topics specific to tools\ncovered various topics specific to tools\ncovered various topics specific to tools and how you can integrate tools with the\nand how you can integrate tools with the\nand how you can integrate tools with the LLM models um and probably go ahead and\nLLM models um and probably go ahead and\nLLM models um and probably go ahead and create a generative AI application. Now\ncreate a generative AI application. Now\ncreate a generative AI application. Now we are going to move towards our next\nwe are going to move towards our next\nwe are going to move towards our next topic which is called as messages right\ntopic which is called as messages right\ntopic which is called as messages right and messages uh in short are a very\nand messages uh in short are a very\nand messages uh in short are a very important data structures that can be\nimportant data structures that can be\nimportant data structures that can be specifically used with langin.\nspecifically used with langin.\nspecifically used with langin. uh I will for I have written the\nuh I will for I have written the\nuh I will for I have written the definition I will go ahead and write the\ndefinition I will go ahead and write the\ndefinition I will go ahead and write the code in front of you each and everything\ncode in front of you each and everything\ncode in front of you each and everything we'll discuss step by step so first of\nwe'll discuss step by step so first of\nwe'll discuss step by step so first of all the messages are the fundamental\nall the messages are the fundamental\nall the messages are the fundamental unit of context for models in langen\nunit of context for models in langen\nunit of context for models in langen they represent the input and output of a\nthey represent the input and output of a\nthey represent the input and output of a model carrying both the content and\nmodel carrying both the content and\nmodel carrying both the content and metadata need to represent the state of\nmetadata need to represent the state of\nmetadata need to represent the state of a conversation when interacting with an\na conversation when interacting with an\na conversation when interacting with an LLM messages are object that contain\nLLM messages are object that contain\nLLM messages are object that contain role content and metadata role is super\nrole content and metadata role is super\nrole content and metadata role is super important. Okay, role basically\nimportant. Okay, role basically\nimportant. Okay, role basically identifies the message type. Now first\nidentifies the message type. Now first\nidentifies the message type. Now first of all what I'll do is that in order to\nof all what I'll do is that in order to\nof all what I'll do is that in order to show you the messages till now we have\nshow you the messages till now we have\nshow you the messages till now we have discussed in various places right you\ndiscussed in various places right you\ndiscussed in various places right you can see that I'm getting an output with\ncan see that I'm getting an output with\ncan see that I'm getting an output with respect to tool. So this is one kind of\nrespect to tool. So this is one kind of\nrespect to tool. So this is one kind of message whenever you see an output from\nmessage whenever you see an output from\nmessage whenever you see an output from a specific model right like here we have\na specific model right like here we have\na specific model right like here we have written model.invoke invoke on a\nwritten model.invoke invoke on a\nwritten model.invoke invoke on a specific question. The model when it is\nspecific question. The model when it is\nspecific question. The model when it is giving its output will be in the form of\ngiving its output will be in the form of\ngiving its output will be in the form of a message. So whenever a model gives an\na message. So whenever a model gives an\na message. So whenever a model gives an output, it is basically a AI message.\noutput, it is basically a AI message.\noutput, it is basically a AI message. Whenever a human is giving an input, it\nWhenever a human is giving an input, it\nWhenever a human is giving an input, it is nothing but a human message. So there\nis nothing but a human message. So there\nis nothing but a human message. So there are different kind of message structures\nare different kind of message structures\nare different kind of message structures that we are going to see. There are\nthat we are going to see. There are\nthat we are going to see. There are specifically three types which we are\nspecifically three types which we are\nspecifically three types which we are going to discuss one by one. Okay. So\ngoing to discuss one by one. Okay. So\ngoing to discuss one by one. Okay. So first thing first, what I am actually\nfirst thing first, what I am actually\nfirst thing first, what I am actually going to do, first of all, I will go\ngoing to do, first of all, I will go\ngoing to do, first of all, I will go ahead and initialize my model. Okay, now\nahead and initialize my model. Okay, now\nahead and initialize my model. Okay, now you know how to initialize your model.\nyou know how to initialize your model.\nyou know how to initialize your model. So I have imported OS from\nSo I have imported OS from\nSo I have imported OS from langchin.ch_model.\nlangchin.ch_model.\nlangchin.ch_model. Import init chat model. I'm using the\nImport init chat model. I'm using the\nImport init chat model. I'm using the gro API key as my environment variable.\ngro API key as my environment variable.\ngro API key as my environment variable. And then we have used init chat model\nAnd then we have used init chat model\nAnd then we have used init chat model with this quen model from grock. Okay.\nwith this quen model from grock. Okay.\nwith this quen model from grock. Okay. So I'll go ahead and execute this. Now\nSo I'll go ahead and execute this. Now\nSo I'll go ahead and execute this. Now see this is really important. Okay.\nsee this is really important. Okay.\nsee this is really important. Okay. Whenever I go ahead and write\nWhenever I go ahead and write\nWhenever I go ahead and write model.invoke on any specific input.\nmodel.invoke on any specific input.\nmodel.invoke on any specific input. Okay. So let's say I'll say uh please\nOkay. So let's say I'll say uh please\nOkay. So let's say I'll say uh please tell me\ntell me\ntell me what is artificial intelligence. Okay.\nwhat is artificial intelligence. Okay.\nwhat is artificial intelligence. Okay. So this is my question.\nSo this is my question.\nSo this is my question. Now by default when I give this specific\nNow by default when I give this specific\nNow by default when I give this specific input to the model this is treated as a\ninput to the model this is treated as a\ninput to the model this is treated as a human input or a human message. So once\nhuman input or a human message. So once\nhuman input or a human message. So once I execute this this as an input to the\nI execute this this as an input to the\nI execute this this as an input to the LLM is going as an input which is\nLLM is going as an input which is\nLLM is going as an input which is nothing but a human message. And when I\nnothing but a human message. And when I\nnothing but a human message. And when I want to see the output, I'll just go\nwant to see the output, I'll just go\nwant to see the output, I'll just go ahead and execute this. Now my output\nahead and execute this. Now my output\nahead and execute this. Now my output will be basically an output from the LLM\nwill be basically an output from the LLM\nwill be basically an output from the LLM which is nothing but an AI message. And\nwhich is nothing but an AI message. And\nwhich is nothing but an AI message. And the content that is inside this is the\nthe content that is inside this is the\nthe content that is inside this is the output from the LLM model. Okay. Now\noutput from the LLM model. Okay. Now\noutput from the LLM model. Okay. Now this is what a simple message basically\nthis is what a simple message basically\nthis is what a simple message basically looks like. Now two things we have\nlooks like. Now two things we have\nlooks like. Now two things we have discussed about human message AI\ndiscussed about human message AI\ndiscussed about human message AI message. I will deep dive more into it\nmessage. I will deep dive more into it\nmessage. I will deep dive more into it and probably talk more about it. Okay.\nand probably talk more about it. Okay.\nand probably talk more about it. Okay. But first of all before going to human\nBut first of all before going to human\nBut first of all before going to human message AI message we will start with a\nmessage AI message we will start with a\nmessage AI message we will start with a text prompt. Okay. So text prompt are\ntext prompt. Okay. So text prompt are\ntext prompt. Okay. So text prompt are nothing but they are strings idle for\nnothing but they are strings idle for\nnothing but they are strings idle for straightforward generation task where\nstraightforward generation task where\nstraightforward generation task where you don't need to retain conversation\nyou don't need to retain conversation\nyou don't need to retain conversation history. Okay. Now here you can clearly\nhistory. Okay. Now here you can clearly\nhistory. Okay. Now here you can clearly see that whenever I'm writing\nsee that whenever I'm writing\nsee that whenever I'm writing model.invoke with some specific\nmodel.invoke with some specific\nmodel.invoke with some specific question. Okay. So here when I'm writing\nquestion. Okay. So here when I'm writing\nquestion. Okay. So here when I'm writing model.invoke invoke with some question.\nmodel.invoke invoke with some question.\nmodel.invoke invoke with some question. Let's say I'll say what is langchain?\nLet's say I'll say what is langchain?\nLet's say I'll say what is langchain? Okay. Now in this particular scenario, I\nOkay. Now in this particular scenario, I\nOkay. Now in this particular scenario, I have not specified anything to the model\nhave not specified anything to the model\nhave not specified anything to the model like how the model should behave, right?\nlike how the model should behave, right?\nlike how the model should behave, right? I'm just providing a simple text, right?\nI'm just providing a simple text, right?\nI'm just providing a simple text, right? This text is treated as an human message\nThis text is treated as an human message\nThis text is treated as an human message internally. But I can also say this as a\ninternally. But I can also say this as a\ninternally. But I can also say this as a text prompt. Okay, so idle for\ntext prompt. Okay, so idle for\ntext prompt. Okay, so idle for straightforward generation task where\nstraightforward generation task where\nstraightforward generation task where you don't need to retain any\nyou don't need to retain any\nyou don't need to retain any conversation history. Let's say that I\nconversation history. Let's say that I\nconversation history. Let's say that I just want to give an input and get an\njust want to give an input and get an\njust want to give an input and get an output from the model. So in this\noutput from the model. So in this\noutput from the model. So in this particular scenario I will just go ahead\nparticular scenario I will just go ahead\nparticular scenario I will just go ahead and use this phenomena right. So when I\nand use this phenomena right. So when I\nand use this phenomena right. So when I say model.invoke what is langchain I\nsay model.invoke what is langchain I\nsay model.invoke what is langchain I will directly get an output in the form\nwill directly get an output in the form\nwill directly get an output in the form of AI message. Okay now use text prompts\nof AI message. Okay now use text prompts\nof AI message. Okay now use text prompts when you have a single standalone\nwhen you have a single standalone\nwhen you have a single standalone request. You don't need conversation\nrequest. You don't need conversation\nrequest. You don't need conversation history. You want minimal code\nhistory. You want minimal code\nhistory. You want minimal code complexity and all. Right now we will\ncomplexity and all. Right now we will\ncomplexity and all. Right now we will see in the different type like one more\nsee in the different type like one more\nsee in the different type like one more category which is called as message\ncategory which is called as message\ncategory which is called as message prompts. So here we have seen about text\nprompts. So here we have seen about text\nprompts. So here we have seen about text prompts. In text prompts I just specify\nprompts. In text prompts I just specify\nprompts. In text prompts I just specify my input. I get the output from the\nmy input. I get the output from the\nmy input. I get the output from the model. So now we will try to understand\nmodel. So now we will try to understand\nmodel. So now we will try to understand how is message prompts different than\nhow is message prompts different than\nhow is message prompts different than the text prompt. Okay. Now here we'll\nthe text prompt. Okay. Now here we'll\nthe text prompt. Okay. Now here we'll first of all see the definition.\nfirst of all see the definition.\nfirst of all see the definition. Alternatively, you can pass\nAlternatively, you can pass\nAlternatively, you can pass messages in the list of messages to the\nmessages in the list of messages to the\nmessages in the list of messages to the model by providing a list of message\nmodel by providing a list of message\nmodel by providing a list of message object. Okay. Now you need to first of\nobject. Okay. Now you need to first of\nobject. Okay. Now you need to first of all understand if I want to provide a\nall understand if I want to provide a\nall understand if I want to provide a list of messages, it can be a human\nlist of messages, it can be a human\nlist of messages, it can be a human message, it can be an AI message, it can\nmessage, it can be an AI message, it can\nmessage, it can be an AI message, it can be a system message. Now you should\nbe a system message. Now you should\nbe a system message. Now you should understand what exactly is a system\nunderstand what exactly is a system\nunderstand what exactly is a system message. System message is just like an\nmessage. System message is just like an\nmessage. System message is just like an instruction like how the LLM should\ninstruction like how the LLM should\ninstruction like how the LLM should behave. Okay, again let me repeat it.\nbehave. Okay, again let me repeat it.\nbehave. Okay, again let me repeat it. What is a system message? It is nothing\nWhat is a system message? It is nothing\nWhat is a system message? It is nothing but it is a kind of a instruction to the\nbut it is a kind of a instruction to the\nbut it is a kind of a instruction to the LLM like how it should basically behave.\nLLM like how it should basically behave.\nLLM like how it should basically behave. So in the case of message types you have\nSo in the case of message types you have\nSo in the case of message types you have different messages like system message,\ndifferent messages like system message,\ndifferent messages like system message, human message, AI message and tool\nhuman message, AI message and tool\nhuman message, AI message and tool message. First of all we'll understand\nmessage. First of all we'll understand\nmessage. First of all we'll understand the definition of system message. System\nthe definition of system message. System\nthe definition of system message. System message tells the model how to behave\nmessage tells the model how to behave\nmessage tells the model how to behave and provide context for interaction.\nand provide context for interaction.\nand provide context for interaction. Human message is nothing but it\nHuman message is nothing but it\nHuman message is nothing but it represents user input and interaction\nrepresents user input and interaction\nrepresents user input and interaction with the model. AI message is nothing\nwith the model. AI message is nothing\nwith the model. AI message is nothing but response generated by the model\nbut response generated by the model\nbut response generated by the model including text content, tools and\nincluding text content, tools and\nincluding text content, tools and metadata. Tool message represents the\nmetadata. Tool message represents the\nmetadata. Tool message represents the output of a tool call. Okay. So all this\noutput of a tool call. Okay. So all this\noutput of a tool call. Okay. So all this information is basically over here. So\ninformation is basically over here. So\ninformation is basically over here. So here you can see system message\nhere you can see system message\nhere you can see system message definition, human message, AI message\ndefinition, human message, AI message\ndefinition, human message, AI message and tool message. So let's go ahead and\nand tool message. So let's go ahead and\nand tool message. So let's go ahead and see this particular example. Okay. So\nsee this particular example. Okay. So\nsee this particular example. Okay. So first of all what I will do I will go\nfirst of all what I will do I will go\nfirst of all what I will do I will go ahead and import all these messages\nahead and import all these messages\nahead and import all these messages types. So in order to import I will use\ntypes. So in order to import I will use\ntypes. So in order to import I will use from langchain dot messages import\nfrom langchain dot messages import\nfrom langchain dot messages import system message\nsystem message\nsystem message human message\nhuman message\nhuman message AI message. Okay now I will create a\nAI message. Okay now I will create a\nAI message. Okay now I will create a list of messages. Let's say that I'm\nlist of messages. Let's say that I'm\nlist of messages. Let's say that I'm having a conversation history also. So\nhaving a conversation history also. So\nhaving a conversation history also. So that's the reason I'm creating this list\nthat's the reason I'm creating this list\nthat's the reason I'm creating this list of messages. So let's say first of all I\nof messages. So let's say first of all I\nof messages. So let's say first of all I use a system message. Now this system\nuse a system message. Now this system\nuse a system message. Now this system message is just like an instruction to\nmessage is just like an instruction to\nmessage is just like an instruction to the LLM model like how the LLM model\nthe LLM model like how the LLM model\nthe LLM model like how the LLM model should behave. So I'll go ahead and\nshould behave. So I'll go ahead and\nshould behave. So I'll go ahead and write you are a poetry expert.\nwrite you are a poetry expert.\nwrite you are a poetry expert. Okay. So this is my first message. Let\nOkay. So this is my first message. Let\nOkay. So this is my first message. Let me go ahead and write human message over\nme go ahead and write human message over\nme go ahead and write human message over here. In the human message I will go\nhere. In the human message I will go\nhere. In the human message I will go ahead and say um this will be an input.\nahead and say um this will be an input.\nahead and say um this will be an input. I'll say write an write a poem on\nI'll say write an write a poem on\nI'll say write an write a poem on artificial intelligence.\nartificial intelligence.\nartificial intelligence. Okay,\nOkay,\nOkay, artificial intelligence because in\nartificial intelligence because in\nartificial intelligence because in chatbot usually this kind of\nchatbot usually this kind of\nchatbot usually this kind of conversation happens in a conversation\nconversation happens in a conversation\nconversation happens in a conversation history, right? There'll be a list of\nhistory, right? There'll be a list of\nhistory, right? There'll be a list of messages that will be happening. So\nmessages that will be happening. So\nmessages that will be happening. So let's say here uh I give the output or\nlet's say here uh I give the output or\nlet's say here uh I give the output or let's say I give this two information\nlet's say I give this two information\nlet's say I give this two information like a system message and a human\nlike a system message and a human\nlike a system message and a human message. Okay. So this is my list of\nmessage. Okay. So this is my list of\nmessage. Okay. So this is my list of messages. Now I'll use model.invoke and\nmessages. Now I'll use model.invoke and\nmessages. Now I'll use model.invoke and I'll give this messages over here. Okay.\nI'll give this messages over here. Okay.\nI'll give this messages over here. Okay. Over here I'll get it and I'll go ahead\nOver here I'll get it and I'll go ahead\nOver here I'll get it and I'll go ahead and get my response. Now let me do one\nand get my response. Now let me do one\nand get my response. Now let me do one thing. Let me go ahead and print my\nthing. Let me go ahead and print my\nthing. Let me go ahead and print my response dot content. Okay. Now see this\nresponse dot content. Okay. Now see this\nresponse dot content. Okay. Now see this both the messages are basically going.\nboth the messages are basically going.\nboth the messages are basically going. Okay. First is the instruction to the LM\nOkay. First is the instruction to the LM\nOkay. First is the instruction to the LM like how you should basically go ahead\nlike how you should basically go ahead\nlike how you should basically go ahead and u act like I'm saying you are a\nand u act like I'm saying you are a\nand u act like I'm saying you are a poetry expert and then probably I've\npoetry expert and then probably I've\npoetry expert and then probably I've given the input and based on this input\ngiven the input and based on this input\ngiven the input and based on this input I will be getting my AI message as the\nI will be getting my AI message as the\nI will be getting my AI message as the output okay the user wants a poem about\noutput okay the user wants a poem about\noutput okay the user wants a poem about artificial and let me start thinking\nartificial and let me start thinking\nartificial and let me start thinking about the key themes related to AI\nabout the key themes related to AI\nabout the key themes related to AI creating aspects how human build AI then\ncreating aspects how human build AI then\ncreating aspects how human build AI then maybe do and all this information is\nmaybe do and all this information is\nmaybe do and all this information is basically there and you're getting the\nbasically there and you're getting the\nbasically there and you're getting the output okay so this is what a simple\noutput okay so this is what a simple\noutput okay so this is what a simple you know a list of messages prompts look\nyou know a list of messages prompts look\nyou know a list of messages prompts look like okay here we can pass a list of\nlike okay here we can pass a list of\nlike okay here we can pass a list of messages in the form of a conversation\nmessages in the form of a conversation\nmessages in the form of a conversation history here I can also go ahead and\nhistory here I can also go ahead and\nhistory here I can also go ahead and write I provide AI messages over here\nwrite I provide AI messages over here\nwrite I provide AI messages over here and probably go ahead and try it out\nand probably go ahead and try it out\nand probably go ahead and try it out okay now let's see some more important\nokay now let's see some more important\nokay now let's see some more important thing right here the kind of examples\nthing right here the kind of examples\nthing right here the kind of examples that you have seen is with respect to a\nthat you have seen is with respect to a\nthat you have seen is with respect to a system message okay this was just a\nsystem message okay this was just a\nsystem message okay this was just a basic message itself right you just have\nbasic message itself right you just have\nbasic message itself right you just have a oneliner let me see one more example\na oneliner let me see one more example\na oneliner let me see one more example So here I've written system message I'm\nSo here I've written system message I'm\nSo here I've written system message I'm writing you are a helpful coding\nwriting you are a helpful coding\nwriting you are a helpful coding assistant. I've given this messages list\nassistant. I've given this messages list\nassistant. I've given this messages list human message as an input. How do I\nhuman message as an input. How do I\nhuman message as an input. How do I create a rest API? Right? And now you\ncreate a rest API? Right? And now you\ncreate a rest API? Right? And now you can also see this specific response. It\ncan also see this specific response. It\ncan also see this specific response. It will go ahead and try to create a rest\nwill go ahead and try to create a rest\nwill go ahead and try to create a rest API. So here you can see all the\nAPI. So here you can see all the\nAPI. So here you can see all the information. Okay the user is asking all\ninformation. Okay the user is asking all\ninformation. Okay the user is asking all this information is basically can\nthis information is basically can\nthis information is basically can creating a rest API invoid involves\ncreating a rest API invoid involves\ncreating a rest API invoid involves defining endpoints that handle this and\ndefining endpoints that handle this and\ndefining endpoints that handle this and that all the information is there. Okay.\nthat all the information is there. Okay.\nthat all the information is there. Okay. So that basically means we are able to\nSo that basically means we are able to\nSo that basically means we are able to get a good answer. Now till now the\nget a good answer. Now till now the\nget a good answer. Now till now the system message that we have specified is\nsystem message that we have specified is\nsystem message that we have specified is just a oneliner message. Sometime we\njust a oneliner message. Sometime we\njust a oneliner message. Sometime we want a detailed information provided in\nwant a detailed information provided in\nwant a detailed information provided in the system message so that we give more\nthe system message so that we give more\nthe system message so that we give more information to the LLMs. Right? So what\ninformation to the LLMs. Right? So what\ninformation to the LLMs. Right? So what we will do I will show you one more\nwe will do I will show you one more\nwe will do I will show you one more example where we give detailed\nexample where we give detailed\nexample where we give detailed information detailed info to the LLM\ninformation detailed info to the LLM\ninformation detailed info to the LLM through system message. Okay system\nthrough system message. Okay system\nthrough system message. Okay system message. So let's see this example. So\nmessage. So let's see this example. So\nmessage. So let's see this example. So this example is also really good. So\nthis example is also really good. So\nthis example is also really good. So here now I will say like this. Now see\nhere now I will say like this. Now see\nhere now I will say like this. Now see inside this we have provided a system\ninside this we have provided a system\ninside this we have provided a system message. I'm saying you are a senior\nmessage. I'm saying you are a senior\nmessage. I'm saying you are a senior Python developer with expertise in web\nPython developer with expertise in web\nPython developer with expertise in web frameworks. Now more context is\nframeworks. Now more context is\nframeworks. Now more context is basically given here. Before that I've\nbasically given here. Before that I've\nbasically given here. Before that I've just told that hey you are a helpful\njust told that hey you are a helpful\njust told that hey you are a helpful coding assistant. We have not specified\ncoding assistant. We have not specified\ncoding assistant. We have not specified any specific programming language like\nany specific programming language like\nany specific programming language like Python, Java, you know it can be C, C++,\nPython, Java, you know it can be C, C++,\nPython, Java, you know it can be C, C++, anything as such right? I've just\nanything as such right? I've just\nanything as such right? I've just provided a generic information. The\nprovided a generic information. The\nprovided a generic information. The answer was also very generic. Okay. But\nanswer was also very generic. Okay. But\nanswer was also very generic. Okay. But in this particular scenario, you can see\nin this particular scenario, you can see\nin this particular scenario, you can see that I provided a detailed information.\nthat I provided a detailed information.\nthat I provided a detailed information. You're assistant senior Python developer\nYou're assistant senior Python developer\nYou're assistant senior Python developer with expertise in web frameworks. Always\nwith expertise in web frameworks. Always\nwith expertise in web frameworks. Always provide code examples and explain your\nprovide code examples and explain your\nprovide code examples and explain your reasoning. Be concise but thorough with\nreasoning. Be concise but thorough with\nreasoning. Be concise but thorough with in your explanation. Now I wrote how do\nin your explanation. Now I wrote how do\nin your explanation. Now I wrote how do I create a rest API? The same thing. Now\nI create a rest API? The same thing. Now\nI create a rest API? The same thing. Now you see the response. The re response\nyou see the response. The re response\nyou see the response. The re response will be much more practical and it will\nwill be much more practical and it will\nwill be much more practical and it will be related to definitely Python. So if\nbe related to definitely Python. So if\nbe related to definitely Python. So if you go ahead and see this here you can\nyou go ahead and see this here you can\nyou go ahead and see this here you can see start the steps choosing flask\nsee start the steps choosing flask\nsee start the steps choosing flask install it the code example everything\ninstall it the code example everything\ninstall it the code example everything is over here and lot of messages are\nis over here and lot of messages are\nis over here and lot of messages are over here disable debug mode in\nover here disable debug mode in\nover here disable debug mode in production add input validation and all\nproduction add input validation and all\nproduction add input validation and all right so\nright so\nright so you can clearly see that if we provide\nyou can clearly see that if we provide\nyou can clearly see that if we provide more information inside this system uh\nmore information inside this system uh\nmore information inside this system uh message we will be able to get more\nmessage we will be able to get more\nmessage we will be able to get more proper response okay now this is what a\nproper response okay now this is what a\nproper response okay now this is what a simple things is right now I have also\nsimple things is right now I have also\nsimple things is right now I have also told you that whenever we provide the\ntold you that whenever we provide the\ntold you that whenever we provide the messages, we can also provide this three\nmessages, we can also provide this three\nmessages, we can also provide this three important information. One is role,\nimportant information. One is role,\nimportant information. One is role, content and metadata. Role basically\ncontent and metadata. Role basically\ncontent and metadata. Role basically identifies the message type whether it\nidentifies the message type whether it\nidentifies the message type whether it is system, user or human. Content\nis system, user or human. Content\nis system, user or human. Content represents the actual content of the\nrepresents the actual content of the\nrepresents the actual content of the messages. It can be text, audio and\nmessages. It can be text, audio and\nmessages. It can be text, audio and documents. Metadata is some kind like an\ndocuments. Metadata is some kind like an\ndocuments. Metadata is some kind like an optional fields. Okay. So let's say that\noptional fields. Okay. So let's say that\noptional fields. Okay. So let's say that I want to go ahead and define some kind\nI want to go ahead and define some kind\nI want to go ahead and define some kind of human message over here. Okay. My my\nof human message over here. Okay. My my\nof human message over here. Okay. My my human message with some metadata. So\nhuman message with some metadata. So\nhuman message with some metadata. So here you can see content is hello name\nhere you can see content is hello name\nhere you can see content is hello name is Alice ID is message 1 2 3. So here\nis Alice ID is message 1 2 3. So here\nis Alice ID is message 1 2 3. So here you can see we are providing two\nyou can see we are providing two\nyou can see we are providing two metadata information one is for\nmetadata information one is for\nmetadata information one is for identifying for different users and one\nidentifying for different users and one\nidentifying for different users and one is uniquely identified for tracing. So\nis uniquely identified for tracing. So\nis uniquely identified for tracing. So this is basically for tracing you know\nthis is basically for tracing you know\nthis is basically for tracing you know so that we can go ahead and trace it.\nso that we can go ahead and trace it.\nso that we can go ahead and trace it. Now if I go ahead and see the response\nNow if I go ahead and see the response\nNow if I go ahead and see the response see if I go ahead and just use\nsee if I go ahead and just use\nsee if I go ahead and just use model.invoke on this human message you\nmodel.invoke on this human message you\nmodel.invoke on this human message you should be able to see the response. So\nshould be able to see the response. So\nshould be able to see the response. So the user said hello. So I should be in a\nthe user said hello. So I should be in a\nthe user said hello. So I should be in a very friendly way. It'll be able to\nvery friendly way. It'll be able to\nvery friendly way. It'll be able to probably provide the response based on\nprobably provide the response based on\nprobably provide the response based on the metadata information also that we\nthe metadata information also that we\nthe metadata information also that we specifically have. Okay. Now this is\nspecifically have. Okay. Now this is\nspecifically have. Okay. Now this is just a specific idea about how you can\njust a specific idea about how you can\njust a specific idea about how you can play with system message, human message.\nplay with system message, human message.\nplay with system message, human message. Uh you know you can also have uh AI\nUh you know you can also have uh AI\nUh you know you can also have uh AI messages. You we have also spoken about\nmessages. You we have also spoken about\nmessages. You we have also spoken about the list of messages that you really\nthe list of messages that you really\nthe list of messages that you really want to work on. Right. Right. Let's see\nwant to work on. Right. Right. Let's see\nwant to work on. Right. Right. Let's see one more example. Okay. Now this example\none more example. Okay. Now this example\none more example. Okay. Now this example is also amazing. So here you can see I\nis also amazing. So here you can see I\nis also amazing. So here you can see I have written from langchen messages. I\nhave written from langchen messages. I\nhave written from langchen messages. I have imported AI message, system\nhave imported AI message, system\nhave imported AI message, system message, human message. AI message is\nmessage, human message. AI message is\nmessage, human message. AI message is that I'd be happy to help you with the\nthat I'd be happy to help you with the\nthat I'd be happy to help you with the question. Okay, create an AI message\nquestion. Okay, create an AI message\nquestion. Okay, create an AI message manually. We have created it. Okay, this\nmanually. We have created it. Okay, this\nmanually. We have created it. Okay, this is not generated by AI itself but we are\nis not generated by AI itself but we are\nis not generated by AI itself but we are creating our own message and we are\ncreating our own message and we are\ncreating our own message and we are assigning as label as AI message. Now I\nassigning as label as AI message. Now I\nassigning as label as AI message. Now I am adding all the information into the\nam adding all the information into the\nam adding all the information into the conversation history. So say they say\nconversation history. So say they say\nconversation history. So say they say you are a helpful assistant. have\nyou are a helpful assistant. have\nyou are a helpful assistant. have written as a human input initially we\nwritten as a human input initially we\nwritten as a human input initially we have written can you help me AI message\nhave written can you help me AI message\nhave written can you help me AI message is nothing but I'd be happy to help you\nis nothing but I'd be happy to help you\nis nothing but I'd be happy to help you with that question human message great\nwith that question human message great\nwith that question human message great what is 2 + 2 now this entire list of\nwhat is 2 + 2 now this entire list of\nwhat is 2 + 2 now this entire list of conversation can be also understood by\nconversation can be also understood by\nconversation can be also understood by the llm and based on this it should also\nthe llm and based on this it should also\nthe llm and based on this it should also be able to give you the output so here\nbe able to give you the output so here\nbe able to give you the output so here you can see clearly how the output is\nyou can see clearly how the output is\nyou can see clearly how the output is right first 2 + 2 is four that's\nright first 2 + 2 is four that's\nright first 2 + 2 is four that's straightforward but may I should I\nstraightforward but may I should I\nstraightforward but may I should I explain this this so this is a reasoning\nexplain this this so this is a reasoning\nexplain this this so this is a reasoning model that we have specifically used\nmodel that we have specifically used\nmodel that we have specifically used right so That's the reason it is\nright so That's the reason it is\nright so That's the reason it is providing a lot of reasoning stuff and\nproviding a lot of reasoning stuff and\nproviding a lot of reasoning stuff and all. Now inside this can I also go ahead\nall. Now inside this can I also go ahead\nall. Now inside this can I also go ahead and see my metadata. So in order to see\nand see my metadata. So in order to see\nand see my metadata. So in order to see the metadata so I can write response dot\nthe metadata so I can write response dot\nthe metadata so I can write response dot metadata. Okay or usage metadata. And\nmetadata. Okay or usage metadata. And\nmetadata. Okay or usage metadata. And here you can see that you'll also be\nhere you can see that you'll also be\nhere you can see that you'll also be able to get the information like uh how\nable to get the information like uh how\nable to get the information like uh how much was the input token, how much\nmuch was the input token, how much\nmuch was the input token, how much output token was generated by the LLM\noutput token was generated by the LLM\noutput token was generated by the LLM and what are the total number of tokens.\nand what are the total number of tokens.\nand what are the total number of tokens. So that you will be able to see all the\nSo that you will be able to see all the\nSo that you will be able to see all the information out there. Right? So this is\ninformation out there. Right? So this is\ninformation out there. Right? So this is also there. Now we have discussed about\nalso there. Now we have discussed about\nalso there. Now we have discussed about all the things. There is only one\nall the things. There is only one\nall the things. There is only one message that is remaining that is\nmessage that is remaining that is\nmessage that is remaining that is nothing but tool messages. Now already\nnothing but tool messages. Now already\nnothing but tool messages. Now already in our previous example we have\nin our previous example we have\nin our previous example we have understood about tools. Tools message is\nunderstood about tools. Tools message is\nunderstood about tools. Tools message is nothing but it's just like an output\nnothing but it's just like an output\nnothing but it's just like an output that is provided by the tools. Right? So\nthat is provided by the tools. Right? So\nthat is provided by the tools. Right? So whenever an LLM require a help of a\nwhenever an LLM require a help of a\nwhenever an LLM require a help of a tool, it will go ahead and make a tool\ntool, it will go ahead and make a tool\ntool, it will go ahead and make a tool call and whenever that tool specifically\ncall and whenever that tool specifically\ncall and whenever that tool specifically gets executed, it is just going to go\ngets executed, it is just going to go\ngets executed, it is just going to go ahead and give you the output. Okay. So\nahead and give you the output. Okay. So\nahead and give you the output. Okay. So finally let's go ahead and talk about\nfinally let's go ahead and talk about\nfinally let's go ahead and talk about tool. Okay. And here I will be taking\ntool. Okay. And here I will be taking\ntool. Okay. And here I will be taking another example.\nanother example.\nanother example. Right. So here you can see I've used AI\nRight. So here you can see I've used AI\nRight. So here you can see I've used AI message and tool message. In the AI\nmessage and tool message. In the AI\nmessage and tool message. In the AI message I've empty content but we are\nmessage I've empty content but we are\nmessage I've empty content but we are making a tool call. Okay. The name of\nmaking a tool call. Okay. The name of\nmaking a tool call. Okay. The name of the tool that we are going to make is\nthe tool that we are going to make is\nthe tool that we are going to make is get weather. Argument is nothing but\nget weather. Argument is nothing but\nget weather. Argument is nothing but location as San Francisco and we have\nlocation as San Francisco and we have\nlocation as San Francisco and we have also used id. Weather result whatever we\nalso used id. Weather result whatever we\nalso used id. Weather result whatever we are basically getting we have hardcoded\nare basically getting we have hardcoded\nare basically getting we have hardcoded it. Now I'm using this tool message with\nit. Now I'm using this tool message with\nit. Now I'm using this tool message with content is equal to weather result and\ncontent is equal to weather result and\ncontent is equal to weather result and tool ID. Now see I've asked the question\ntool ID. Now see I've asked the question\ntool ID. Now see I've asked the question what's the weather in San Francisco then\nwhat's the weather in San Francisco then\nwhat's the weather in San Francisco then AI message is basically given from here.\nAI message is basically given from here.\nAI message is basically given from here. Okay. And then tool message is nothing\nOkay. And then tool message is nothing\nOkay. And then tool message is nothing but the output uh that we get from here\nbut the output uh that we get from here\nbut the output uh that we get from here after executing the weather result. So\nafter executing the weather result. So\nafter executing the weather result. So now if I go ahead and execute this and\nnow if I go ahead and execute this and\nnow if I go ahead and execute this and probably go ahead and see the response I\nprobably go ahead and see the response I\nprobably go ahead and see the response I should be able to get the output as\nshould be able to get the output as\nshould be able to get the output as okay. Uh uh one more very important\nokay. Uh uh one more very important\nokay. Uh uh one more very important thing is that if you see this specific\nthing is that if you see this specific\nthing is that if you see this specific tool message okay so if I go ahead and\ntool message okay so if I go ahead and\ntool message okay so if I go ahead and see my tool message it is nothing but it\nsee my tool message it is nothing but it\nsee my tool message it is nothing but it is coming as a tool message okay and\nis coming as a tool message okay and\nis coming as a tool message okay and once this tool message is there we are\nonce this tool message is there we are\nonce this tool message is there we are giving as an in uh input to the model so\ngiving as an in uh input to the model so\ngiving as an in uh input to the model so that is the reason model.invoke invoke\nthat is the reason model.invoke invoke\nthat is the reason model.invoke invoke of messages when we execute we are\nof messages when we execute we are\nof messages when we execute we are getting the response as a message. So I\ngetting the response as a message. So I\ngetting the response as a message. So I hope you got an idea with respect to\nhope you got an idea with respect to\nhope you got an idea with respect to different types of messages. You can go\ndifferent types of messages. You can go\ndifferent types of messages. You can go ahead and try it out. Explore more about\nahead and try it out. Explore more about\nahead and try it out. Explore more about it. You know u since this is the updated\nit. You know u since this is the updated\nit. You know u since this is the updated version of langchain uh my\nversion of langchain uh my\nversion of langchain uh my responsibility is to cover all the\nresponsibility is to cover all the\nresponsibility is to cover all the specific topics as the updates are\nspecific topics as the updates are\nspecific topics as the updates are basically coming up. Okay. Now uh as we\nbasically coming up. Okay. Now uh as we\nbasically coming up. Okay. Now uh as we go ahead we'll also be talking about\ngo ahead we'll also be talking about\ngo ahead we'll also be talking about different structured output where we\ndifferent structured output where we\ndifferent structured output where we talk about pentic we talk about nested\ntalk about pentic we talk about nested\ntalk about pentic we talk about nested structures we talk about typed deck. Um\nstructures we talk about typed deck. Um\nstructures we talk about typed deck. Um so that part uh we will be covering now.\nso that part uh we will be covering now.\nso that part uh we will be covering now. So now we are going to discuss about\nSo now we are going to discuss about\nSo now we are going to discuss about structured output. Till now uh we have\nstructured output. Till now uh we have\nstructured output. Till now uh we have already seen messages. We have also got\nalready seen messages. We have also got\nalready seen messages. We have also got to know about the different type of\nto know about the different type of\nto know about the different type of message prompts like system message,\nmessage prompts like system message,\nmessage prompts like system message, human message, AI message and tool\nhuman message, AI message and tool\nhuman message, AI message and tool message. And we also saw multiple\nmessage. And we also saw multiple\nmessage. And we also saw multiple examples and how to implement it with\nexamples and how to implement it with\nexamples and how to implement it with the help of langen. Now in the\nthe help of langen. Now in the\nthe help of langen. Now in the structured output, why is structured\nstructured output, why is structured\nstructured output, why is structured output actually required? Okay. Now see\noutput actually required? Okay. Now see\noutput actually required? Okay. Now see guys uh we will definitely be using\nguys uh we will definitely be using\nguys uh we will definitely be using different different LLMs and we want\ndifferent different LLMs and we want\ndifferent different LLMs and we want this LLM models to be requested in such\nthis LLM models to be requested in such\nthis LLM models to be requested in such a way that so they provide the response\na way that so they provide the response\na way that so they provide the response in a format matching a given schema. So\nin a format matching a given schema. So\nin a format matching a given schema. So let's say that hey I am requesting a LLM\nlet's say that hey I am requesting a LLM\nlet's say that hey I am requesting a LLM model to write me an essay on some\nmodel to write me an essay on some\nmodel to write me an essay on some specific topic and I definitely want the\nspecific topic and I definitely want the\nspecific topic and I definitely want the response of that LLM model to follow\nresponse of that LLM model to follow\nresponse of that LLM model to follow some structure and that is where I would\nsome structure and that is where I would\nsome structure and that is where I would definitely want some kind of structured\ndefinitely want some kind of structured\ndefinitely want some kind of structured output right so this is where we\noutput right so this is where we\noutput right so this is where we implement or we make the LLM to give a\nimplement or we make the LLM to give a\nimplement or we make the LLM to give a kind of structured output and we do it\nkind of structured output and we do it\nkind of structured output and we do it by using different techniques some of\nby using different techniques some of\nby using different techniques some of the techniques are like pidentic we can\nthe techniques are like pidentic we can\nthe techniques are like pidentic we can also use type date we can use data\nalso use type date we can use data\nalso use type date we can use data classes and that is what we will be\nclasses and that is what we will be\nclasses and that is what we will be discussing in this particular section\ndiscussing in this particular section\ndiscussing in this particular section okay so over here you can see I have\nokay so over here you can see I have\nokay so over here you can see I have written a very detailed explanation\nwritten a very detailed explanation\nwritten a very detailed explanation about structured output it says that\nabout structured output it says that\nabout structured output it says that model can be requested to provide a\nmodel can be requested to provide a\nmodel can be requested to provide a response in a format matching a given\nresponse in a format matching a given\nresponse in a format matching a given schema is useful for ensuring the output\nschema is useful for ensuring the output\nschema is useful for ensuring the output can be easily passed and be used in\ncan be easily passed and be used in\ncan be easily passed and be used in subsequent processes langchen supports\nsubsequent processes langchen supports\nsubsequent processes langchen supports multiple schema types and methods for\nmultiple schema types and methods for\nmultiple schema types and methods for enforcing structured output\nenforcing structured output\nenforcing structured output So the first uh technique that we are\nSo the first uh technique that we are\nSo the first uh technique that we are going to use or first type that we are\ngoing to use or first type that we are\ngoing to use or first type that we are going to use is something called as\ngoing to use is something called as\ngoing to use is something called as pyntentic. Now pyntic model it provides\npyntentic. Now pyntic model it provides\npyntentic. Now pyntic model it provides a richest feature set with field\na richest feature set with field\na richest feature set with field validation description and nested\nvalidation description and nested\nvalidation description and nested structure. So I will show you one\nstructure. So I will show you one\nstructure. So I will show you one example like how we can create a\nexample like how we can create a\nexample like how we can create a structured output from the LLM\nstructured output from the LLM\nstructured output from the LLM specifically for the LM response itself.\nspecifically for the LM response itself.\nspecifically for the LM response itself. Right? So first of all what I will do I\nRight? So first of all what I will do I\nRight? So first of all what I will do I will go ahead and import OS along with\nwill go ahead and import OS along with\nwill go ahead and import OS along with this what I am actually going to do is\nthis what I am actually going to do is\nthis what I am actually going to do is that I will also go ahead and the first\nthat I will also go ahead and the first\nthat I will also go ahead and the first step is obviously I have to load my uh\nstep is obviously I have to load my uh\nstep is obviously I have to load my uh lm model right so I'll write from langin\nlm model right so I'll write from langin\nlm model right so I'll write from langin dot chat models importit\nchat models right then I will write osen\nchat models right then I will write osen environment\nenvironment\nenvironment and here I'm going to specifically write\nand here I'm going to specifically write\nand here I'm going to specifically write gro_i\ngro_i\ngro_i is equal to os.get get env since I'm\nis equal to os.get get env since I'm\nis equal to os.get get env since I'm actually going to use my gro API key\nactually going to use my gro API key\nactually going to use my gro API key right so I'll write gro API key\nright so I'll write gro API key\nright so I'll write gro API key now the model that I'm actually going to\nnow the model that I'm actually going to\nnow the model that I'm actually going to use is nothing but I will be using this\nuse is nothing but I will be using this\nuse is nothing but I will be using this gro model groan\ngro model groan\ngro model groan uh the model name is nothing but quen\nuh the model name is nothing but quen\nuh the model name is nothing but quen and we will be using quen 32\nand we will be using quen 32\nand we will be using quen 32 billion parameters model it's a\nbillion parameters model it's a\nbillion parameters model it's a reasoning model right so this is the\nreasoning model right so this is the\nreasoning model right so this is the model that I\nmodel that I\nmodel that I So here you can see cannot import uh\nSo here you can see cannot import uh\nSo here you can see cannot import uh name in it chat models. Okay let's see\nname in it chat models. Okay let's see\nname in it chat models. Okay let's see what is the issue. So here you can see\nwhat is the issue. So here you can see\nwhat is the issue. So here you can see there is something called as init chat\nthere is something called as init chat\nthere is something called as init chat model. Uh we had made a different import\nmodel. Uh we had made a different import\nmodel. Uh we had made a different import but it's okay. We have actually loaded\nbut it's okay. We have actually loaded\nbut it's okay. We have actually loaded our LLM model.\nour LLM model.\nour LLM model. Now let me quickly show you that how\nNow let me quickly show you that how\nNow let me quickly show you that how with the help of pyentic you will be\nwith the help of pyentic you will be\nwith the help of pyentic you will be able to generate a structured output.\nable to generate a structured output.\nable to generate a structured output. The best part about pyntic is that it\nThe best part about pyntic is that it\nThe best part about pyntic is that it also has field validation descriptions\nalso has field validation descriptions\nalso has field validation descriptions and also nested structure. So first of\nand also nested structure. So first of\nand also nested structure. So first of all in order to use pyntic we need to\nall in order to use pyntic we need to\nall in order to use pyntic we need to import one library which is called as\nimport one library which is called as\nimport one library which is called as from pidentic import base models.\nfrom pidentic import base models.\nfrom pidentic import base models. Okay, comma field. So this field is uh\nOkay, comma field. So this field is uh\nOkay, comma field. So this field is uh what we are going to specifically use in\nwhat we are going to specifically use in\nwhat we are going to specifically use in order to use field validation. Now here\norder to use field validation. Now here\norder to use field validation. Now here let's say I want my lln to give the\nlet's say I want my lln to give the\nlet's say I want my lln to give the output in a some kind of structured\noutput in a some kind of structured\noutput in a some kind of structured schema. Okay. Now what schema it will\nschema. Okay. Now what schema it will\nschema. Okay. Now what schema it will basically follow. So what I will do for\nbasically follow. So what I will do for\nbasically follow. So what I will do for that I will create let's say a class\nthat I will create let's say a class\nthat I will create let's say a class called as movie. And inside this movie\ncalled as movie. And inside this movie\ncalled as movie. And inside this movie we will inherit with this specific base\nwe will inherit with this specific base\nwe will inherit with this specific base model. Okay. The base model that we have\nmodel. Okay. The base model that we have\nmodel. Okay. The base model that we have imported over here. And if you see that\nimported over here. And if you see that\nimported over here. And if you see that it is nothing but it is a base class for\nit is nothing but it is a base class for\nit is nothing but it is a base class for creating pyic models. Right. and pying\ncreating pyic models. Right. and pying\ncreating pyic models. Right. and pying model has a very important property that\nmodel has a very important property that\nmodel has a very important property that it provides you field validation\nit provides you field validation\nit provides you field validation description and it also provides you\ndescription and it also provides you\ndescription and it also provides you nested structure. Now let's say my\nnested structure. Now let's say my\nnested structure. Now let's say my structure output from the LLM needs to\nstructure output from the LLM needs to\nstructure output from the LLM needs to have different fields. Okay. So one of\nhave different fields. Okay. So one of\nhave different fields. Okay. So one of the field is nothing but title. So let's\nthe field is nothing but title. So let's\nthe field is nothing but title. So let's say this title should be of only type\nsay this title should be of only type\nsay this title should be of only type string. Okay. So here we are writing\nstring. Okay. So here we are writing\nstring. Okay. So here we are writing colon string. Okay. And this will be of\ncolon string. Okay. And this will be of\ncolon string. Okay. And this will be of type field. And here I can go ahead and\ntype field. And here I can go ahead and\ntype field. And here I can go ahead and provide some description saying that\nprovide some description saying that\nprovide some description saying that this title is nothing but it is the\nthis title is nothing but it is the\nthis title is nothing but it is the title of the movie.\ntitle of the movie.\ntitle of the movie. Okay. Now see my LLM needs to definitely\nOkay. Now see my LLM needs to definitely\nOkay. Now see my LLM needs to definitely generate some kind of output and it will\ngenerate some kind of output and it will\ngenerate some kind of output and it will generate based on whatever fields I'm\ngenerate based on whatever fields I'm\ngenerate based on whatever fields I'm actually creating over here. And we are\nactually creating over here. And we are\nactually creating over here. And we are going to make sure that this title\ngoing to make sure that this title\ngoing to make sure that this title should only be having string value over\nshould only be having string value over\nshould only be having string value over here. If it has a numerical value then\nhere. If it has a numerical value then\nhere. If it has a numerical value then it'll give us a error because pientic uh\nit'll give us a error because pientic uh\nit'll give us a error because pientic uh will do this kind of field validation\nwill do this kind of field validation\nwill do this kind of field validation also. Okay. the pyntic model. Now,\nalso. Okay. the pyntic model. Now,\nalso. Okay. the pyntic model. Now, similarly, my second field will be\nsimilarly, my second field will be\nsimilarly, my second field will be nothing but year. Let's say my year is\nnothing but year. Let's say my year is\nnothing but year. Let's say my year is there and it will be of int type and I\nthere and it will be of int type and I\nthere and it will be of int type and I will go ahead and write field and here\nwill go ahead and write field and here\nwill go ahead and write field and here again this will be my description and I\nagain this will be my description and I\nagain this will be my description and I will say hey this is the year of this\nwill say hey this is the year of this\nwill say hey this is the year of this year.\nyear.\nyear. This year the movie was released.\nOkay, the movie was released. And then\nOkay, the movie was released. And then coming to the third important let's say\ncoming to the third important let's say\ncoming to the third important let's say that I want to also create one more\nthat I want to also create one more\nthat I want to also create one more field inside this. It is nothing but\nfield inside this. It is nothing but\nfield inside this. It is nothing but director string. And again I can\ndirector string. And again I can\ndirector string. And again I can basically say this is a field. Now see\nbasically say this is a field. Now see\nbasically say this is a field. Now see you understand what this field is right.\nyou understand what this field is right.\nyou understand what this field is right. If you go ahead and uh just hover over\nIf you go ahead and uh just hover over\nIf you go ahead and uh just hover over it and if you see what exactly field is\nit and if you see what exactly field is\nit and if you see what exactly field is this is basically providing you lot of\nthis is basically providing you lot of\nthis is basically providing you lot of different different parameters that you\ndifferent different parameters that you\ndifferent different parameters that you can set which represents this particular\ncan set which represents this particular\ncan set which represents this particular uh variable right the year. Okay. So\nuh variable right the year. Okay. So\nuh variable right the year. Okay. So here I've just given that this\nhere I've just given that this\nhere I've just given that this particular field is nothing but this is\nparticular field is nothing but this is\nparticular field is nothing but this is the year the movie was released. This\nthe year the movie was released. This\nthe year the movie was released. This information will be very much important\ninformation will be very much important\ninformation will be very much important for the LLM right because once we are\nfor the LLM right because once we are\nfor the LLM right because once we are giving this kind of descriptions we can\ngiving this kind of descriptions we can\ngiving this kind of descriptions we can also set some other parameters like uh\nalso set some other parameters like uh\nalso set some other parameters like uh you know we can set a liar this will be\nyou know we can set a liar this will be\nyou know we can set a liar this will be very very handy because it will help the\nvery very handy because it will help the\nvery very handy because it will help the LLM to know in which field it needs to\nLLM to know in which field it needs to\nLLM to know in which field it needs to place the output that it is coming from\nplace the output that it is coming from\nplace the output that it is coming from the LLM itself when we are displaying\nthe LLM itself when we are displaying\nthe LLM itself when we are displaying that in a structured output. Okay. So\nthat in a structured output. Okay. So\nthat in a structured output. Okay. So this is my uh director field. In the\nthis is my uh director field. In the\nthis is my uh director field. In the director field, I will also go ahead and\ndirector field, I will also go ahead and\ndirector field, I will also go ahead and write some kind of description so that\nwrite some kind of description so that\nwrite some kind of description so that it gives some idea to my LLM saying that\nit gives some idea to my LLM saying that\nit gives some idea to my LLM saying that okay, the director of the movie,\nokay, the director of the movie,\nokay, the director of the movie, director of the movie. Okay. Then I have\ndirector of the movie. Okay. Then I have\ndirector of the movie. Okay. Then I have my ratings. This is also one field that\nmy ratings. This is also one field that\nmy ratings. This is also one field that I definitely want. My rating can be a\nI definitely want. My rating can be a\nI definitely want. My rating can be a float value. Okay. It needs to be a\nfloat value. Okay. It needs to be a\nfloat value. Okay. It needs to be a float value because it can have\nfloat value because it can have\nfloat value because it can have different float value itself. And then I\ndifferent float value itself. And then I\ndifferent float value itself. And then I have my description. Inside my\nhave my description. Inside my\nhave my description. Inside my description, I will go ahead and say the\ndescription, I will go ahead and say the\ndescription, I will go ahead and say the movies\nratings\nratings out of 10. Okay, out of 10. Now see this\nout of 10. Okay, out of 10. Now see this\nout of 10. Okay, out of 10. Now see this is the output that my LLM should be\nis the output that my LLM should be\nis the output that my LLM should be generating it. So that's the reason I've\ngenerating it. So that's the reason I've\ngenerating it. So that's the reason I've created a class called as movie and it\ncreated a class called as movie and it\ncreated a class called as movie and it is inheriting base model. Inheriting\nis inheriting base model. Inheriting\nis inheriting base model. Inheriting base model basically means it is it is\nbase model basically means it is it is\nbase model basically means it is it is just going to go ahead and if you just\njust going to go ahead and if you just\njust going to go ahead and if you just hover towards this base model it is it\nhover towards this base model it is it\nhover towards this base model it is it is nothing but it is a base class for\nis nothing but it is a base class for\nis nothing but it is a base class for creating pyic models. And one important\ncreating pyic models. And one important\ncreating pyic models. And one important thing is that this pyantic model has\nthing is that this pyantic model has\nthing is that this pyantic model has real validation description and all.\nreal validation description and all.\nreal validation description and all. Okay. Now let me go ahead and execute\nOkay. Now let me go ahead and execute\nOkay. Now let me go ahead and execute this. Okay. Now if I want my model to\nthis. Okay. Now if I want my model to\nthis. Okay. Now if I want my model to generate the structured output. So what\ngenerate the structured output. So what\ngenerate the structured output. So what I will do I will just go ahead and write\nI will do I will just go ahead and write\nI will do I will just go ahead and write model with structure output and we will\nmodel with structure output and we will\nmodel with structure output and we will give like what structure output it needs\ngive like what structure output it needs\ngive like what structure output it needs to give of this particular movie class.\nto give of this particular movie class.\nto give of this particular movie class. Okay. So this movie class is over here\nOkay. So this movie class is over here\nOkay. So this movie class is over here and here I will go ahead and define\nand here I will go ahead and define\nand here I will go ahead and define model with structure.\nmodel with structure.\nmodel with structure. Okay. Model with structure. So this is\nOkay. Model with structure. So this is\nOkay. Model with structure. So this is what uh my uh important model way is you\nwhat uh my uh important model way is you\nwhat uh my uh important model way is you know now see as soon as I go ahead and\nknow now see as soon as I go ahead and\nknow now see as soon as I go ahead and execute this. Okay, I go ahead and\nexecute this. Okay, I go ahead and\nexecute this. Okay, I go ahead and execute this and if you go ahead and\nexecute this and if you go ahead and\nexecute this and if you go ahead and just display it what is this model with\njust display it what is this model with\njust display it what is this model with structure,\nstructure,\nstructure, it shows that it is a runnable binding.\nit shows that it is a runnable binding.\nit shows that it is a runnable binding. It has uh information from Chad Gro\nIt has uh information from Chad Gro\nIt has uh information from Chad Gro model and then it also has this py tool\nmodel and then it also has this py tool\nmodel and then it also has this py tool parser. Okay, now this is really\nparser. Okay, now this is really\nparser. Okay, now this is really important because now I'm going to\nimportant because now I'm going to\nimportant because now I'm going to display how the output will get\ndisplay how the output will get\ndisplay how the output will get displayed whenever we ask any question\ndisplayed whenever we ask any question\ndisplayed whenever we ask any question to this particular model with structure.\nto this particular model with structure.\nto this particular model with structure. So for that I will go ahead and write\nSo for that I will go ahead and write\nSo for that I will go ahead and write model with structure dot invoke\nmodel with structure dot invoke\nmodel with structure dot invoke and here I will go ahead and ask a\nand here I will go ahead and ask a\nand here I will go ahead and ask a question provide details. Let's say I\nquestion provide details. Let's say I\nquestion provide details. Let's say I want a details about the movie movie\nwant a details about the movie movie\nwant a details about the movie movie inception. So if I go ahead and use\ninception. So if I go ahead and use\ninception. So if I go ahead and use model invokework if you remember model\nmodel invokework if you remember model\nmodel invokework if you remember model is nothing but it is not having any\nis nothing but it is not having any\nis nothing but it is not having any schema attached right. So here if you\nschema attached right. So here if you\nschema attached right. So here if you are attaching any schema or any\nare attaching any schema or any\nare attaching any schema or any structure output it is nothing but model\nstructure output it is nothing but model\nstructure output it is nothing but model with structure. So if I just go ahead\nwith structure. So if I just go ahead\nwith structure. So if I just go ahead and write model.invoke and I say hey\nand write model.invoke and I say hey\nand write model.invoke and I say hey provide me the details of the movie\nprovide me the details of the movie\nprovide me the details of the movie inception. So here you can see this is\ninception. So here you can see this is\ninception. So here you can see this is how is the default output we will get.\nhow is the default output we will get.\nhow is the default output we will get. Okay once we execute this see this is my\nOkay once we execute this see this is my\nOkay once we execute this see this is my AI message. Okay I need to provide a\nAI message. Okay I need to provide a\nAI message. Okay I need to provide a detail about the movieception. The\ndetail about the movieception. The\ndetail about the movieception. The inception probably has to do a concept\ninception probably has to do a concept\ninception probably has to do a concept of planting an idea. So it is probably\nof planting an idea. So it is probably\nof planting an idea. So it is probably providing all the details but I don't\nproviding all the details but I don't\nproviding all the details but I don't want all these details. I want the\nwant all these details. I want the\nwant all these details. I want the details in this structure output. I want\ndetails in this structure output. I want\ndetails in this structure output. I want it in the form of title, year, director,\nit in the form of title, year, director,\nit in the form of title, year, director, rating. Right? So I should be able to\nrating. Right? So I should be able to\nrating. Right? So I should be able to get that. Now if I go ahead and use this\nget that. Now if I go ahead and use this\nget that. Now if I go ahead and use this model with output dot invoke and now I\nmodel with output dot invoke and now I\nmodel with output dot invoke and now I want to go ahead and create I'm asking\nwant to go ahead and create I'm asking\nwant to go ahead and create I'm asking the same question. See over here I'm\nthe same question. See over here I'm\nthe same question. See over here I'm asking the same question response. Now\nasking the same question response. Now\nasking the same question response. Now if I go ahead and display the response,\nif I go ahead and display the response,\nif I go ahead and display the response, you should be able to see that I will\nyou should be able to see that I will\nyou should be able to see that I will get in this structured output. Right? So\nget in this structured output. Right? So\nget in this structured output. Right? So here you can see title is nothing but\nhere you can see title is nothing but\nhere you can see title is nothing but inception. Here it got released on 2010.\ninception. Here it got released on 2010.\ninception. Here it got released on 2010. director is nothing but Christopher\ndirector is nothing but Christopher\ndirector is nothing but Christopher Nolan rating is 8.8 eight right so\nNolan rating is 8.8 eight right so\nNolan rating is 8.8 eight right so sometimes now this information I can use\nsometimes now this information I can use\nsometimes now this information I can use it anywhere right this is a vague\nit anywhere right this is a vague\nit anywhere right this is a vague information it has all the information\ninformation it has all the information\ninformation it has all the information probably from the internet data that it\nprobably from the internet data that it\nprobably from the internet data that it has been trained with but if I just want\nhas been trained with but if I just want\nhas been trained with but if I just want some kind of structured output which is\nsome kind of structured output which is\nsome kind of structured output which is important for me because if I'm able to\nimportant for me because if I'm able to\nimportant for me because if I'm able to generate this output I will be able to\ngenerate this output I will be able to\ngenerate this output I will be able to use this in the same structured manner\nuse this in the same structured manner\nuse this in the same structured manner for some other purpose right so our main\nfor some other purpose right so our main\nfor some other purpose right so our main aim over here is that where models can\naim over here is that where models can\naim over here is that where models can be requested to provide the respon in a\nbe requested to provide the respon in a\nbe requested to provide the respon in a format matching a given schema and here\nformat matching a given schema and here\nformat matching a given schema and here my given schema is basically following\nmy given schema is basically following\nmy given schema is basically following this. Now there is very much one more\nthis. Now there is very much one more\nthis. Now there is very much one more very important thing. Okay. Now see I'm\nvery important thing. Okay. Now see I'm\nvery important thing. Okay. Now see I'm getting the output over here as\ngetting the output over here as\ngetting the output over here as inception which is in the form of string\ninception which is in the form of string\ninception which is in the form of string integer Christopher Ner and rating.\ninteger Christopher Ner and rating.\ninteger Christopher Ner and rating. Let's say one very important property\nLet's say one very important property\nLet's say one very important property about pyic is that\nabout pyic is that\nabout pyic is that if if this title has some integer value\nif if this title has some integer value\nif if this title has some integer value will have some integer value then it is\nwill have some integer value then it is\nwill have some integer value then it is definitely going to give us an error and\ndefinitely going to give us an error and\ndefinitely going to give us an error and that is what this field validation is\nthat is what this field validation is\nthat is what this field validation is supported in pentic. Okay. Because of\nsupported in pentic. Okay. Because of\nsupported in pentic. Okay. Because of this field validation you always need to\nthis field validation you always need to\nthis field validation you always need to have values of this title as a string\nhave values of this title as a string\nhave values of this title as a string only. For this year you should have it\nonly. For this year you should have it\nonly. For this year you should have it in the form of integer. If in this\nin the form of integer. If in this\nin the form of integer. If in this director field you need to always have a\ndirector field you need to always have a\ndirector field you need to always have a string and in this rating you can either\nstring and in this rating you can either\nstring and in this rating you can either have integer or a floating value. If you\nhave integer or a floating value. If you\nhave integer or a floating value. If you have some other values it is going to\nhave some other values it is going to\nhave some other values it is going to give you an error. Okay. So that is the\ngive you an error. Okay. So that is the\ngive you an error. Okay. So that is the most important property about pyntic\nmost important property about pyntic\nmost important property about pyntic with respect to field validation. Okay.\nwith respect to field validation. Okay.\nwith respect to field validation. Okay. So now I hope you got an idea uh about\nSo now I hope you got an idea uh about\nSo now I hope you got an idea uh about how does a pyic basically work. Okay.\nhow does a pyic basically work. Okay.\nhow does a pyic basically work. Okay. Now what I will do I can also go ahead\nNow what I will do I can also go ahead\nNow what I will do I can also go ahead and create a message output. Okay,\nand create a message output. Okay,\nand create a message output. Okay, message output alongside alongside pared\nmessage output alongside alongside pared\nmessage output alongside alongside pared structure. Okay, parse structure. So\nstructure. Okay, parse structure. So\nstructure. Okay, parse structure. So let's see this example. Now you may be\nlet's see this example. Now you may be\nlet's see this example. Now you may be thinking what exactly this is. Okay, so\nthinking what exactly this is. Okay, so\nthinking what exactly this is. Okay, so here you'll be able to see that I will\nhere you'll be able to see that I will\nhere you'll be able to see that I will just go ahead and do the same thing. If\njust go ahead and do the same thing. If\njust go ahead and do the same thing. If you can see over here from pyentic\nyou can see over here from pyentic\nyou can see over here from pyentic import base model field I've created a\nimport base model field I've created a\nimport base model field I've created a class movie here this is just like an\nclass movie here this is just like an\nclass movie here this is just like an optional field okay and then I have put\noptional field okay and then I have put\noptional field okay and then I have put the description all the information over\nthe description all the information over\nthe description all the information over here and model with structure I have\nhere and model with structure I have\nhere and model with structure I have written model do with structure output\nwritten model do with structure output\nwritten model do with structure output movie and I have written include raw is\nmovie and I have written include raw is\nmovie and I have written include raw is equal to true see one of the feature\nequal to true see one of the feature\nequal to true see one of the feature include raw is equal to true now what\ninclude raw is equal to true now what\ninclude raw is equal to true now what this actually does you'll try to\nthis actually does you'll try to\nthis actually does you'll try to understand it okay what this feature\nunderstand it okay what this feature\nunderstand it okay what this feature will actually do so now if I just go\nwill actually do so now if I just go\nwill actually do so now if I just go ahead and execute this I'll create some\nahead and execute this I'll create some\nahead and execute this I'll create some more code and see the response model\nmore code and see the response model\nmore code and see the response model with structure.inote input will provide\nwith structure.inote input will provide\nwith structure.inote input will provide a detail about the movie Inception and\na detail about the movie Inception and\na detail about the movie Inception and remember we have kept this parameter as\nremember we have kept this parameter as\nremember we have kept this parameter as include to raw is equal to true. Now\ninclude to raw is equal to true. Now\ninclude to raw is equal to true. Now once we get includes raw is equal to\nonce we get includes raw is equal to\nonce we get includes raw is equal to true by default how the raw message will\ntrue by default how the raw message will\ntrue by default how the raw message will come that is also displayed over here\ncome that is also displayed over here\ncome that is also displayed over here right the initial raw message this raw\nright the initial raw message this raw\nright the initial raw message this raw message like how it is basically getting\nmessage like how it is basically getting\nmessage like how it is basically getting displayed that will also get displayed\ndisplayed that will also get displayed\ndisplayed that will also get displayed over here and this is my parsed message\nover here and this is my parsed message\nover here and this is my parsed message right based on the structure so I can\nright based on the structure so I can\nright based on the structure so I can also display that also and there is also\nalso display that also and there is also\nalso display that also and there is also option by including this particular\noption by including this particular\noption by including this particular parameter okay now along with this there\nparameter okay now along with this there\nparameter okay now along with this there is also one more important thing which\nis also one more important thing which\nis also one more important thing which is supported in pyntic which is called\nis supported in pyntic which is called\nis supported in pyntic which is called as nested structure. Okay. Now let's see\nas nested structure. Okay. Now let's see\nas nested structure. Okay. Now let's see or let's understand what exactly is\nor let's understand what exactly is\nor let's understand what exactly is nested structure. Let's say I am\nnested structure. Let's say I am\nnested structure. Let's say I am importing pentic and I have this class\nimporting pentic and I have this class\nimporting pentic and I have this class actor. Okay. So inside the actor you\nactor. Okay. So inside the actor you\nactor. Okay. So inside the actor you have two variables name and role.\nhave two variables name and role.\nhave two variables name and role. Obviously every actor will have a name\nObviously every actor will have a name\nObviously every actor will have a name and role and it is of type string. Okay.\nand role and it is of type string. Okay.\nand role and it is of type string. Okay. Now inside my movie right there may be\nNow inside my movie right there may be\nNow inside my movie right there may be multiple actors right. So what I can do\nmultiple actors right. So what I can do\nmultiple actors right. So what I can do I can use this direct class inside this.\nI can use this direct class inside this.\nI can use this direct class inside this. So here you can see I have written class\nSo here you can see I have written class\nSo here you can see I have written class movie details base model title this is\nmovie details base model title this is\nmovie details base model title this is the movie title year the movie release\nthe movie title year the movie release\nthe movie title year the movie release date cast will be the list of actor see\ndate cast will be the list of actor see\ndate cast will be the list of actor see this is the same actor over here and\nthis is the same actor over here and\nthis is the same actor over here and here I can have list of actors so that\nhere I can have list of actors so that\nhere I can have list of actors so that is the reason we are saying next\nis the reason we are saying next\nis the reason we are saying next structure okay John Jonner's list of\nstructure okay John Jonner's list of\nstructure okay John Jonner's list of strings it can also be a list of jonors\nstrings it can also be a list of jonors\nstrings it can also be a list of jonors right and budget is nothing but a\nright and budget is nothing but a\nright and budget is nothing but a floating point and here you can see that\nfloating point and here you can see that\nfloating point and here you can see that I've created uh by default none okay\nI've created uh by default none okay\nI've created uh by default none okay otherwise we specy specifically provide\notherwise we specy specifically provide\notherwise we specy specifically provide some kind of description budget in\nsome kind of description budget in\nsome kind of description budget in million USD. Okay. Now this way we are\nmillion USD. Okay. Now this way we are\nmillion USD. Okay. Now this way we are using a nested structure that basically\nusing a nested structure that basically\nusing a nested structure that basically means inside the movie details we are\nmeans inside the movie details we are\nmeans inside the movie details we are using this particular actor. Okay. Now\nusing this particular actor. Okay. Now\nusing this particular actor. Okay. Now if I go ahead and use the same thing and\nif I go ahead and use the same thing and\nif I go ahead and use the same thing and ask the same question model with\nask the same question model with\nask the same question model with structure output and this time I have\nstructure output and this time I have\nstructure output and this time I have written movie details right over here.\nwritten movie details right over here.\nwritten movie details right over here. Now if I just go ahead and ask the same\nNow if I just go ahead and ask the same\nNow if I just go ahead and ask the same question from this particular structure\nquestion from this particular structure\nquestion from this particular structure output saying that hey model with\noutput saying that hey model with\noutput saying that hey model with structure.invoke Invoke provide the\nstructure.invoke Invoke provide the\nstructure.invoke Invoke provide the details about the movie Inception. I\ndetails about the movie Inception. I\ndetails about the movie Inception. I should be getting the response in this\nshould be getting the response in this\nshould be getting the response in this specific way and in the cast I will be\nspecific way and in the cast I will be\nspecific way and in the cast I will be getting a list of actors. In genres I'll\ngetting a list of actors. In genres I'll\ngetting a list of actors. In genres I'll be getting a list of genres. Right? So\nbe getting a list of genres. Right? So\nbe getting a list of genres. Right? So here you can see I will just go ahead\nhere you can see I will just go ahead\nhere you can see I will just go ahead and display this. This is amazing. See\nand display this. This is amazing. See\nand display this. This is amazing. See in title I got assumption year 201 cast\nin title I got assumption year 201 cast\nin title I got assumption year 201 cast actor name Leonardo Darpo role Dom Cobb.\nactor name Leonardo Darpo role Dom Cobb.\nactor name Leonardo Darpo role Dom Cobb. Right. Then the next actor is nothing\nRight. Then the next actor is nothing\nRight. Then the next actor is nothing but Joseph\nbut Joseph\nbut Joseph Levit role is Arthur. actor name. So\nLevit role is Arthur. actor name. So\nLevit role is Arthur. actor name. So here you can see multiple actors are\nhere you can see multiple actors are\nhere you can see multiple actors are there. Jon also you can see the list of\nthere. Jon also you can see the list of\nthere. Jon also you can see the list of this is there science fiction action and\nthis is there science fiction action and\nthis is there science fiction action and budget is 160.0\nbudget is 160.0\nbudget is 160.0 um um 160.0 zero based on the millions\num um 160.0 zero based on the millions\num um 160.0 zero based on the millions uh budget in millions USD right so 160\nuh budget in millions USD right so 160\nuh budget in millions USD right so 160 million uh dollars were actually spent\nmillion uh dollars were actually spent\nmillion uh dollars were actually spent in this so I hope you got a specific\nin this so I hope you got a specific\nin this so I hope you got a specific idea about paidentic the main aim is\nidea about paidentic the main aim is\nidea about paidentic the main aim is that you're providing field validation\nthat you're providing field validation\nthat you're providing field validation and you're actually making the model to\nand you're actually making the model to\nand you're actually making the model to provide the response in a format that\nprovide the response in a format that\nprovide the response in a format that matches your given schema based on the\nmatches your given schema based on the\nmatches your given schema based on the schema that you have actually designed\nschema that you have actually designed\nschema that you have actually designed so I hope you have understood about uh\nso I hope you have understood about uh\nso I hope you have understood about uh pentic now In uh as we go ahead we'll\npentic now In uh as we go ahead we'll\npentic now In uh as we go ahead we'll also be discussing about one more type\nalso be discussing about one more type\nalso be discussing about one more type which is called as type deck and there\nwhich is called as type deck and there\nwhich is called as type deck and there is also one more type which is called as\nis also one more type which is called as\nis also one more type which is called as data class. Okay. So we will see both of\ndata class. Okay. So we will see both of\ndata class. Okay. So we will see both of them as we go ahead. So guys now we are\nthem as we go ahead. So guys now we are\nthem as we go ahead. So guys now we are going to continue the discussion for the\ngoing to continue the discussion for the\ngoing to continue the discussion for the structured output. Uh we have already\nstructured output. Uh we have already\nstructured output. Uh we have already covered how we can make an LLM to you\ncovered how we can make an LLM to you\ncovered how we can make an LLM to you know provide a response in a format\nknow provide a response in a format\nknow provide a response in a format matching a given schema using pentic\nmatching a given schema using pentic\nmatching a given schema using pentic model. Uh now the same thing we will try\nmodel. Uh now the same thing we will try\nmodel. Uh now the same thing we will try to do it with the help of typed dick.\nto do it with the help of typed dick.\nto do it with the help of typed dick. Now typed dick provides a simple\nNow typed dick provides a simple\nNow typed dick provides a simple alternative using python built-in typing\nalternative using python built-in typing\nalternative using python built-in typing idle when you don't need runtime\nidle when you don't need runtime\nidle when you don't need runtime validation. So whenever we are trying to\nvalidation. So whenever we are trying to\nvalidation. So whenever we are trying to uh use typed deck there runtime\nuh use typed deck there runtime\nuh use typed deck there runtime validation is not there as how we had in\nvalidation is not there as how we had in\nvalidation is not there as how we had in pentic models. Okay. So now we'll try to\npentic models. Okay. So now we'll try to\npentic models. Okay. So now we'll try to do the same thing uh like how an LLM can\ndo the same thing uh like how an LLM can\ndo the same thing uh like how an LLM can provide a response in a specific schema\nprovide a response in a specific schema\nprovide a response in a specific schema wherein runtime validation is not\nwherein runtime validation is not\nwherein runtime validation is not required. Let's say if I'm actually\nrequired. Let's say if I'm actually\nrequired. Let's say if I'm actually creating title and we are saying that it\ncreating title and we are saying that it\ncreating title and we are saying that it is of type string it if integer is also\nis of type string it if integer is also\nis of type string it if integer is also getting displayed in the output it is\ngetting displayed in the output it is\ngetting displayed in the output it is fine because there we do not focus much\nfine because there we do not focus much\nfine because there we do not focus much on runtime validation. Okay. So first of\non runtime validation. Okay. So first of\non runtime validation. Okay. So first of all uh to do this I will go ahead and\nall uh to do this I will go ahead and\nall uh to do this I will go ahead and use from typing extension import\nuse from typing extension import\nuse from typing extension import type deck. Okay. So we are going to use\ntype deck. Okay. So we are going to use\ntype deck. Okay. So we are going to use typed dict and along with this I'm also\ntyped dict and along with this I'm also\ntyped dict and along with this I'm also going to use annotated. So this two uh\ngoing to use annotated. So this two uh\ngoing to use annotated. So this two uh are the important libraries that I'm\nare the important libraries that I'm\nare the important libraries that I'm actually going to use. If you see type\nactually going to use. If you see type\nactually going to use. If you see type dict it is a simple type name ses at\ndict it is a simple type name ses at\ndict it is a simple type name ses at runtime. It is equivalent to a plain\nruntime. It is equivalent to a plain\nruntime. It is equivalent to a plain dictionary. It is just going to create a\ndictionary. It is just going to create a\ndictionary. It is just going to create a simple dictionary in short. Right? So\nsimple dictionary in short. Right? So\nsimple dictionary in short. Right? So that is the reason we don't need runtime\nthat is the reason we don't need runtime\nthat is the reason we don't need runtime validation over here. Now I will try to\nvalidation over here. Now I will try to\nvalidation over here. Now I will try to use the same kind of data. Okay. So here\nuse the same kind of data. Okay. So here\nuse the same kind of data. Okay. So here I will say hey let's create a class\nI will say hey let's create a class\nI will say hey let's create a class which is called as a movie. Okay. So now\nwhich is called as a movie. Okay. So now\nwhich is called as a movie. Okay. So now I will just go ahead and create it. Now\nI will just go ahead and create it. Now\nI will just go ahead and create it. Now here you can see I've created a movie\nhere you can see I've created a movie\nhere you can see I've created a movie dictionary and this is this time\ndictionary and this is this time\ndictionary and this is this time inheriting type dict instead of pentic.\ninheriting type dict instead of pentic.\ninheriting type dict instead of pentic. Right? If we inherit pentic then this\nRight? If we inherit pentic then this\nRight? If we inherit pentic then this all will have a runtime validation but\nall will have a runtime validation but\nall will have a runtime validation but we right now don't require it. Uh we are\nwe right now don't require it. Uh we are\nwe right now don't require it. Uh we are saying that hey we are going to probably\nsaying that hey we are going to probably\nsaying that hey we are going to probably go ahead and inherit with uh pent type\ngo ahead and inherit with uh pent type\ngo ahead and inherit with uh pent type dict itself. Okay. And here first of all\ndict itself. Okay. And here first of all\ndict itself. Okay. And here first of all my first field is title and we are\nmy first field is title and we are\nmy first field is title and we are annotating it saying that it is a string\nannotating it saying that it is a string\nannotating it saying that it is a string and the description is this. These are\nand the description is this. These are\nand the description is this. These are some optional fields which we can keep\nsome optional fields which we can keep\nsome optional fields which we can keep it as empty. Okay. So the next field\nit as empty. Okay. So the next field\nit as empty. Okay. So the next field over here year it will be of type. We\nover here year it will be of type. We\nover here year it will be of type. We are annotating it as int. And here you\nare annotating it as int. And here you\nare annotating it as int. And here you can see the description is mentioned.\ncan see the description is mentioned.\ncan see the description is mentioned. Similarly I have director and ratings.\nSimilarly I have director and ratings.\nSimilarly I have director and ratings. Okay. So uh this is how we actually uh\nOkay. So uh this is how we actually uh\nOkay. So uh this is how we actually uh create this particular structure. So now\ncreate this particular structure. So now\ncreate this particular structure. So now once I have created this schema now it's\nonce I have created this schema now it's\nonce I have created this schema now it's time that I will call my model and I'll\ntime that I will call my model and I'll\ntime that I will call my model and I'll use with structure output and and I\nuse with structure output and and I\nuse with structure output and and I apply this particular schema that is\napply this particular schema that is\napply this particular schema that is movie dictionary okay which we have\nmovie dictionary okay which we have\nmovie dictionary okay which we have created it over here and I will just go\ncreated it over here and I will just go\ncreated it over here and I will just go ahead and say this is my model with type\nahead and say this is my model with type\nahead and say this is my model with type dick structure okay with type dict with\ndick structure okay with type dict with\ndick structure okay with type dict with type dict okay I'll just go ahead and\ntype dict okay I'll just go ahead and\ntype dict okay I'll just go ahead and write this then the next step will be\nwrite this then the next step will be\nwrite this then the next step will be that I will use this model with type\nthat I will use this model with type\nthat I will use this model with type dict dot invoke and I'll say please\ndict dot invoke and I'll say please\ndict dot invoke and I'll say please provide the details\nprovide the details\nprovide the details of\nof\nof the movie Avengers let's say this time\nthe movie Avengers let's say this time\nthe movie Avengers let's say this time I'm going to take the Avengers movie\nI'm going to take the Avengers movie\nI'm going to take the Avengers movie okay and then if I go ahead and see the\nokay and then if I go ahead and see the\nokay and then if I go ahead and see the response\nresponse\nresponse okay and then you will be able to see\nokay and then you will be able to see\nokay and then you will be able to see the response over here so here you can\nthe response over here so here you can\nthe response over here so here you can see director Jos widen rating a title\nsee director Jos widen rating a title\nsee director Jos widen rating a title Avengers year 2012. Okay. So now you can\nAvengers year 2012. Okay. So now you can\nAvengers year 2012. Okay. So now you can see the response over here. Now what I\nsee the response over here. Now what I\nsee the response over here. Now what I will do I will also go ahead and create\nwill do I will also go ahead and create\nwill do I will also go ahead and create a next structure and this time instead\na next structure and this time instead\na next structure and this time instead of using base model I will just directly\nof using base model I will just directly\nof using base model I will just directly go ahead and use my type dict. Let's say\ngo ahead and use my type dict. Let's say\ngo ahead and use my type dict. Let's say I will go ahead and use my type dict\nI will go ahead and use my type dict\nI will go ahead and use my type dict over here. I'll use my type dict over\nover here. I'll use my type dict over\nover here. I'll use my type dict over here. So we can also go ahead and imple\nhere. So we can also go ahead and imple\nhere. So we can also go ahead and imple uh implement the nested structure but\nuh implement the nested structure but\nuh implement the nested structure but and over here the validation will not be\nand over here the validation will not be\nand over here the validation will not be compulsory. Right? uh let's say if the\ncompulsory. Right? uh let's say if the\ncompulsory. Right? uh let's say if the directory is having string if I give an\ndirectory is having string if I give an\ndirectory is having string if I give an integer then also it is fine so here the\ninteger then also it is fine so here the\ninteger then also it is fine so here the input validation will not happen like\ninput validation will not happen like\ninput validation will not happen like how it happens in pentic so if I go\nhow it happens in pentic so if I go\nhow it happens in pentic so if I go ahead and execute this so here you can\nahead and execute this so here you can\nahead and execute this so here you can see I'm able to see please provide me\nsee I'm able to see please provide me\nsee I'm able to see please provide me the details about uh the movie inception\nthe details about uh the movie inception\nthe details about uh the movie inception so 16 millions 160 million all the\nso 16 millions 160 million all the\nso 16 millions 160 million all the information is there let's say I want to\ninformation is there let's say I want to\ninformation is there let's say I want to go ahead and try out for Avengers so you\ngo ahead and try out for Avengers so you\ngo ahead and try out for Avengers so you should be able to even see the response\nshould be able to even see the response\nshould be able to even see the response okay so this was a brief idea about how\nokay so this was a brief idea about how\nokay so this was a brief idea about how we can quickly use the type deck all we\nwe can quickly use the type deck all we\nwe can quickly use the type deck all we are doing is that whatever schema we are\nare doing is that whatever schema we are\nare doing is that whatever schema we are actually creating we are inheriting that\nactually creating we are inheriting that\nactually creating we are inheriting that specific module right in over here we\nspecific module right in over here we\nspecific module right in over here we are using typed deck in the previous\nare using typed deck in the previous\nare using typed deck in the previous stage we used base model which was\nstage we used base model which was\nstage we used base model which was specifically for pentic itself okay and\nspecifically for pentic itself okay and\nspecifically for pentic itself okay and this gives you a clear idea like how we\nthis gives you a clear idea like how we\nthis gives you a clear idea like how we can actually go ahead and use uh pentic\ncan actually go ahead and use uh pentic\ncan actually go ahead and use uh pentic over here and clearly and how we are\nover here and clearly and how we are\nover here and clearly and how we are able to see the output now along with\nable to see the output now along with\nable to see the output now along with this there is also a very important\nthis there is also a very important\nthis there is also a very important property which is called as profile so\nproperty which is called as profile so\nproperty which is called as profile so if I write model with structure dot\nif I write model with structure dot\nif I write model with structure dot profile or instead of writing model with\nprofile or instead of writing model with\nprofile or instead of writing model with structure I'll use model type\nstructure I'll use model type\nstructure I'll use model type dick.profile and if I just go ahead and\ndick.profile and if I just go ahead and\ndick.profile and if I just go ahead and display it here you can see runnable se\ndisplay it here you can see runnable se\ndisplay it here you can see runnable se sequence has no attribute profile okay\nsequence has no attribute profile okay\nsequence has no attribute profile okay so this is what is the error that we are\nso this is what is the error that we are\nso this is what is the error that we are getting now whenever we try to create\ngetting now whenever we try to create\ngetting now whenever we try to create some kind of structured output there we\nsome kind of structured output there we\nsome kind of structured output there we should not be able to see the profile\nshould not be able to see the profile\nshould not be able to see the profile but if I go ahead and write\nbut if I go ahead and write\nbut if I go ahead and write model.profile profile which was my base\nmodel.profile profile which was my base\nmodel.profile profile which was my base model. Here you can see that all the\nmodel. Here you can see that all the\nmodel. Here you can see that all the necessary information like how many\nnecessary information like how many\nnecessary information like how many maximum input tokens are there? Maximum\nmaximum input tokens are there? Maximum\nmaximum input tokens are there? Maximum output tokens. This specific model can\noutput tokens. This specific model can\noutput tokens. This specific model can actually do image input does it suppose\nactually do image input does it suppose\nactually do image input does it suppose image right the answer is false audio\nimage right the answer is false audio\nimage right the answer is false audio inputs false video input false audio\ninputs false video input false audio\ninputs false video input false audio input false reasoning output true tool\ninput false reasoning output true tool\ninput false reasoning output true tool calling true. So these are the\ncalling true. So these are the\ncalling true. So these are the information which talks about like what\ninformation which talks about like what\ninformation which talks about like what all things the model is basically\nall things the model is basically\nall things the model is basically supporting and we have used quen 3 model\nsupporting and we have used quen 3 model\nsupporting and we have used quen 3 model over here. So, Quen 3 model actually\nover here. So, Quen 3 model actually\nover here. So, Quen 3 model actually specifically has all this particular uh\nspecifically has all this particular uh\nspecifically has all this particular uh supporting tools or supporting features\nsupporting tools or supporting features\nsupporting tools or supporting features which you can actually understand about\nwhich you can actually understand about\nwhich you can actually understand about the model also. Right. So, now we have\nthe model also. Right. So, now we have\nthe model also. Right. So, now we have understood about typed date. We have\nunderstood about typed date. We have\nunderstood about typed date. We have understood about pyic. Now the next\nunderstood about pyic. Now the next\nunderstood about pyic. Now the next thing that we need to understand about\nthing that we need to understand about\nthing that we need to understand about one more uh way like how we can go ahead\none more uh way like how we can go ahead\none more uh way like how we can go ahead and apply this kind of schema that is\nand apply this kind of schema that is\nand apply this kind of schema that is called as data classes. Okay. So now\ncalled as data classes. Okay. So now\ncalled as data classes. Okay. So now let's go ahead and discuss about this\nlet's go ahead and discuss about this\nlet's go ahead and discuss about this data classes.\ndata classes.\ndata classes. So now let's go ahead and discuss about\nSo now let's go ahead and discuss about\nSo now let's go ahead and discuss about data classes and how we can actually go\ndata classes and how we can actually go\ndata classes and how we can actually go ahead and how our LLM can create a\nahead and how our LLM can create a\nahead and how our LLM can create a structured output based on a specific\nstructured output based on a specific\nstructured output based on a specific schema with the help of data classes.\nschema with the help of data classes.\nschema with the help of data classes. We'll be discussing about that. Now see\nWe'll be discussing about that. Now see\nWe'll be discussing about that. Now see data class has already been there from\ndata class has already been there from\ndata class has already been there from Python 3.7 version. Okay. So a data\nPython 3.7 version. Okay. So a data\nPython 3.7 version. Okay. So a data class is a class typically containing\nclass is a class typically containing\nclass is a class typically containing mainly data although there aren't really\nmainly data although there aren't really\nmainly data although there aren't really any uh restriction like data validation\nany uh restriction like data validation\nany uh restriction like data validation nothing as such input data validation\nnothing as such input data validation\nnothing as such input data validation but you can create it by directly using\nbut you can create it by directly using\nbut you can create it by directly using this particular decorator. So let's do\nthis particular decorator. So let's do\nthis particular decorator. So let's do one thing quickly. Uh let's take one\none thing quickly. Uh let's take one\none thing quickly. Uh let's take one example. First of all, we'll start with\nexample. First of all, we'll start with\nexample. First of all, we'll start with Pentic. Okay, because we have already\nPentic. Okay, because we have already\nPentic. Okay, because we have already know about Pentic now. Okay. And here we\nknow about Pentic now. Okay. And here we\nknow about Pentic now. Okay. And here we are using GPT5 uh for creating the\nare using GPT5 uh for creating the\nare using GPT5 uh for creating the agent. So what I will do? I will just go\nagent. So what I will do? I will just go\nagent. So what I will do? I will just go ahead and write import OS uh and I'll\nahead and write import OS uh and I'll\nahead and write import OS uh and I'll say OS. Environ okay in run. And this\nsay OS. Environ okay in run. And this\nsay OS. Environ okay in run. And this time I'm just going to go ahead and use\ntime I'm just going to go ahead and use\ntime I'm just going to go ahead and use my OpenAI API key. Open AI API key. I\nmy OpenAI API key. Open AI API key. I\nmy OpenAI API key. Open AI API key. I just I'm using this just to show you how\njust I'm using this just to show you how\njust I'm using this just to show you how we can actually go ahead and create our\nwe can actually go ahead and create our\nwe can actually go ahead and create our agents also. Okay. Uh open AI\nagents also. Okay. Uh open AI\nagents also. Okay. Uh open AI API key. Perfect. Now this is done.\nAPI key. Perfect. Now this is done.\nAPI key. Perfect. Now this is done. Okay. And here you can see that uh I\nOkay. And here you can see that uh I\nOkay. And here you can see that uh I have imported pentic import base model\nhave imported pentic import base model\nhave imported pentic import base model and you know uh already in this series\nand you know uh already in this series\nand you know uh already in this series we have covered how to create an agent\nwe have covered how to create an agent\nwe have covered how to create an agent how to create a simple agent. So from\nhow to create a simple agent. So from\nhow to create a simple agent. So from langin.tagents we are importing create\nlangin.tagents we are importing create\nlangin.tagents we are importing create agent. So first of all I have this\nagent. So first of all I have this\nagent. So first of all I have this particular schema contact info. We are\nparticular schema contact info. We are\nparticular schema contact info. We are inheriting base model. Whenever we\ninheriting base model. Whenever we\ninheriting base model. Whenever we inherit a base model of pentic that\ninherit a base model of pentic that\ninherit a base model of pentic that basically means we have some kind of\nbasically means we have some kind of\nbasically means we have some kind of input validation. Name should always be\ninput validation. Name should always be\ninput validation. Name should always be string. Email should always be string.\nstring. Email should always be string.\nstring. Email should always be string. Phone should always be string. Okay.\nPhone should always be string. Okay.\nPhone should always be string. Okay. Then we are creating this agent over\nThen we are creating this agent over\nThen we are creating this agent over here. So here you can see create agent\nhere. So here you can see create agent\nhere. So here you can see create agent uh and the response format. Okay. This\nuh and the response format. Okay. This\nuh and the response format. Okay. This time I'm not using this with structured\ntime I'm not using this with structured\ntime I'm not using this with structured output. Instead what I'm actually doing\noutput. Instead what I'm actually doing\noutput. Instead what I'm actually doing I'm directly showing you how you can\nI'm directly showing you how you can\nI'm directly showing you how you can integrate with the agent itself. So in\nintegrate with the agent itself. So in\nintegrate with the agent itself. So in create agent model is equal to GPT5. We\ncreate agent model is equal to GPT5. We\ncreate agent model is equal to GPT5. We are using this model and we are writing\nare using this model and we are writing\nare using this model and we are writing response format is nothing but contact\nresponse format is nothing but contact\nresponse format is nothing but contact info this specific class. Okay. So\ninfo this specific class. Okay. So\ninfo this specific class. Okay. So whatever agent this is basically there\nwhatever agent this is basically there\nwhatever agent this is basically there we are going to always get the output in\nwe are going to always get the output in\nwe are going to always get the output in this particular schema. Okay. So here we\nthis particular schema. Okay. So here we\nthis particular schema. Okay. So here we are written agent in invoke message role\nare written agent in invoke message role\nare written agent in invoke message role with user contact extract contact from\nwith user contact extract contact from\nwith user contact extract contact from John doing john at the rate example.com\nJohn doing john at the rate example.com\nJohn doing john at the rate example.com with this particular information. So\nwith this particular information. So\nwith this particular information. So here this is my entire content. Okay.\nhere this is my entire content. Okay.\nhere this is my entire content. Okay. And I have written extract contact info\nAnd I have written extract contact info\nAnd I have written extract contact info from this particular information and\nfrom this particular information and\nfrom this particular information and this information is going to my agent.\nthis information is going to my agent.\nthis information is going to my agent. Now agent what it is basically going to\nNow agent what it is basically going to\nNow agent what it is basically going to do based on this particular format. It\ndo based on this particular format. It\ndo based on this particular format. It is going to take the name over here,\nis going to take the name over here,\nis going to take the name over here, email over here, phone number over here,\nemail over here, phone number over here,\nemail over here, phone number over here, right? And then we can go ahead and\nright? And then we can go ahead and\nright? And then we can go ahead and print the result structured response\nprint the result structured response\nprint the result structured response whatever response we have. Right? So let\nwhatever response we have. Right? So let\nwhatever response we have. Right? So let me do one thing. Let me first of all\nme do one thing. Let me first of all\nme do one thing. Let me first of all just directly go ahead and display the\njust directly go ahead and display the\njust directly go ahead and display the result. Okay. So my result is nothing\nresult. Okay. So my result is nothing\nresult. Okay. So my result is nothing but over here. You'll be able to see\nbut over here. You'll be able to see\nbut over here. You'll be able to see quickly after I use this particular\nquickly after I use this particular\nquickly after I use this particular model.\nmodel.\nmodel. So here you can see message human\nSo here you can see message human\nSo here you can see message human message extract contact info from this\nmessage extract contact info from this\nmessage extract contact info from this AI message is over here and structured\nAI message is over here and structured\nAI message is over here and structured response is over here. So if I just go\nresponse is over here. So if I just go\nresponse is over here. So if I just go ahead and write result of\nahead and write result of\nahead and write result of structured\nresponse. Okay. So here you'll be able\nresponse. Okay. So here you'll be able to see this is my contact info. The name\nto see this is my contact info. The name\nto see this is my contact info. The name is John Doe. Email is johntheample.com\nis John Doe. Email is johntheample.com\nis John Doe. Email is johntheample.com and this is there right? So based on\nand this is there right? So based on\nand this is there right? So based on this specific schema we are able to get\nthis specific schema we are able to get\nthis specific schema we are able to get this that is the useful property about\nthis that is the useful property about\nthis that is the useful property about pyic over here. Validation is applied on\npyic over here. Validation is applied on\npyic over here. Validation is applied on every field. Okay. Now similarly if I\nevery field. Okay. Now similarly if I\nevery field. Okay. Now similarly if I want to do it type dect type duct is\nwant to do it type dect type duct is\nwant to do it type dect type duct is very simple which we have already\nvery simple which we have already\nvery simple which we have already discussed. So this is nothing but with\ndiscussed. So this is nothing but with\ndiscussed. So this is nothing but with the help of type dict\nthe help of type dict\nthe help of type dict because I really want to make that\nbecause I really want to make that\nbecause I really want to make that comparison. So from typing extension\ncomparison. So from typing extension\ncomparison. So from typing extension import type dict then we are using from\nimport type dict then we are using from\nimport type dict then we are using from langchen.tag agents create agent. This\nlangchen.tag agents create agent. This\nlangchen.tag agents create agent. This is my schema. This time we are\nis my schema. This time we are\nis my schema. This time we are inheriting type deck. Over here the data\ninheriting type deck. Over here the data\ninheriting type deck. Over here the data input validation will not get applied\ninput validation will not get applied\ninput validation will not get applied but definitely we have provided a schema\nbut definitely we have provided a schema\nbut definitely we have provided a schema wherein we are saying the name should be\nwherein we are saying the name should be\nwherein we are saying the name should be string and all. So here you can see\nstring and all. So here you can see\nstring and all. So here you can see create agent. I will remove the tools. I\ncreate agent. I will remove the tools. I\ncreate agent. I will remove the tools. I don't want the tools right now. So\ndon't want the tools right now. So\ndon't want the tools right now. So create agent with model GP5 response\ncreate agent with model GP5 response\ncreate agent with model GP5 response format is contact info. Now I have\nformat is contact info. Now I have\nformat is contact info. Now I have written contract extract contact info\nwritten contract extract contact info\nwritten contract extract contact info from this information and I will just go\nfrom this information and I will just go\nfrom this information and I will just go ahead and print my structured response.\nahead and print my structured response.\nahead and print my structured response. So here should also be able to see that\nSo here should also be able to see that\nSo here should also be able to see that I'm able to get the output which looks\nI'm able to get the output which looks\nI'm able to get the output which looks something like this in the form of a\nsomething like this in the form of a\nsomething like this in the form of a dictionary pair right like it will be in\ndictionary pair right like it will be in\ndictionary pair right like it will be in the form of a dictionary. So that also\nthe form of a dictionary. So that also\nthe form of a dictionary. So that also you will be able to see it. Uh so here\nyou will be able to see it. Uh so here\nyou will be able to see it. Uh so here you can see name John do email example\nyou can see name John do email example\nyou can see name John do email example and all. Now I will show you how with\nand all. Now I will show you how with\nand all. Now I will show you how with the help of data class you can do the\nthe help of data class you can do the\nthe help of data class you can do the same thing. Okay. So now I will show you\nsame thing. Okay. So now I will show you\nsame thing. Okay. So now I will show you with the help of data class. So with the\nwith the help of data class. So with the\nwith the help of data class. So with the help of these are just different ways\nhelp of these are just different ways\nhelp of these are just different ways you can use any one of them. So first of\nyou can use any one of them. So first of\nyou can use any one of them. So first of all what I'll do I will go ahead and\nall what I'll do I will go ahead and\nall what I'll do I will go ahead and import from data classes. import data\nimport from data classes. import data\nimport from data classes. import data class. Okay. Then I will go ahead and\nclass. Okay. Then I will go ahead and\nclass. Okay. Then I will go ahead and import from langchain agents.\nimport from langchain agents.\nimport from langchain agents. Agents import\ncreate agent. Okay. And then I will\ncreate agent. Okay. And then I will write add the rate data class. I will\nwrite add the rate data class. I will\nwrite add the rate data class. I will create the class as contact info\ncreate the class as contact info\ncreate the class as contact info whatever class I have. So this will be\nwhatever class I have. So this will be\nwhatever class I have. So this will be my contact info class. And how we define\nmy contact info class. And how we define\nmy contact info class. And how we define a variables inside my data class. So it\na variables inside my data class. So it\na variables inside my data class. So it will be nothing like this. We just\nwill be nothing like this. We just\nwill be nothing like this. We just specify uh the information over here.\nspecify uh the information over here.\nspecify uh the information over here. Right? So this is my data class. So let\nRight? So this is my data class. So let\nRight? So this is my data class. So let me write it properly because of the\nme write it properly because of the\nme write it properly because of the validation. So here you can see that\nvalidation. So here you can see that\nvalidation. So here you can see that I've used name is equal to steer str.\nI've used name is equal to steer str.\nI've used name is equal to steer str. name of the person, email str, phone\nname of the person, email str, phone\nname of the person, email str, phone number str okay now the next thing is\nnumber str okay now the next thing is\nnumber str okay now the next thing is that I will just go ahead and use my\nthat I will just go ahead and use my\nthat I will just go ahead and use my create my agent here you can also call\ncreate my agent here you can also call\ncreate my agent here you can also call tools if you have any kind of tools I\ntools if you have any kind of tools I\ntools if you have any kind of tools I don't have any tools so I'll remove this\ndon't have any tools so I'll remove this\ndon't have any tools so I'll remove this but the response format will be in the\nbut the response format will be in the\nbut the response format will be in the form of contact info and finally I will\nform of contact info and finally I will\nform of contact info and finally I will just go ahead and display the response\njust go ahead and display the response\njust go ahead and display the response like how we display the result itself\nlike how we display the result itself\nlike how we display the result itself right so with the help of data class\nright so with the help of data class\nright so with the help of data class also you can actually do the same thing\nalso you can actually do the same thing\nalso you can actually do the same thing okay now this is really important and I\nokay now this is really important and I\nokay now this is really important and I hope uh you got a very good\nhope uh you got a very good\nhope uh you got a very good understanding that how you can actually\nunderstanding that how you can actually\nunderstanding that how you can actually work with data class you got work with\nwork with data class you got work with\nwork with data class you got work with structured output uh you work with type\nstructured output uh you work with type\nstructured output uh you work with type dig you work with pyic and here the in\ndig you work with pyic and here the in\ndig you work with pyic and here the in the data class we have discussed about\nthe data class we have discussed about\nthe data class we have discussed about all the three examples right from type\nall the three examples right from type\nall the three examples right from type dick to data classes and all so yeah uh\ndick to data classes and all so yeah uh\ndick to data classes and all so yeah uh I hope you have understood this\nI hope you have understood this\nI hope you have understood this particular section now uh the next\nparticular section now uh the next\nparticular section now uh the next section that uh we will be discussing\nsection that uh we will be discussing\nsection that uh we will be discussing about is like streaming we'll be\nabout is like streaming we'll be\nabout is like streaming we'll be discussing about uh sorry we have\ndiscussing about uh sorry we have\ndiscussing about uh sorry we have discussed about streaming uh we'll be\ndiscussed about streaming uh we'll be\ndiscussed about streaming uh we'll be discussing about short-term memory and\ndiscussing about short-term memory and\ndiscussing about short-term memory and other things right so let's continue the\nother things right so let's continue the\nother things right so let's continue the discussion so guys now we are going to\ndiscussion so guys now we are going to\ndiscussion so guys now we are going to discuss about middleware now this\ndiscuss about middleware now this\ndiscuss about middleware now this specific topic is a very meaningful\nspecific topic is a very meaningful\nspecific topic is a very meaningful topic that has been included in languin\ntopic that has been included in languin\ntopic that has been included in languin and it has some amazing functionalities\nand it has some amazing functionalities\nand it has some amazing functionalities uh what we'll do in this section is that\nuh what we'll do in this section is that\nuh what we'll do in this section is that we'll talk talk about middleware uh how\nwe'll talk talk about middleware uh how\nwe'll talk talk about middleware uh how you can implement middleware by\nyou can implement middleware by\nyou can implement middleware by different different uh inbuilt\ndifferent different uh inbuilt\ndifferent different uh inbuilt functionalities that are available in\nfunctionalities that are available in\nfunctionalities that are available in lang\nlang\nlang uh we'll take some good use cases in\nuh we'll take some good use cases in\nuh we'll take some good use cases in making you understand. So first of all\nmaking you understand. So first of all\nmaking you understand. So first of all we'll try to understand the definition.\nwe'll try to understand the definition.\nwe'll try to understand the definition. Okay. So let's say over here the\nOkay. So let's say over here the\nOkay. So let's say over here the definition is written. Middleware\ndefinition is written. Middleware\ndefinition is written. Middleware provides a way to uh more tightly\nprovides a way to uh more tightly\nprovides a way to uh more tightly control what happens inside the agent.\ncontrol what happens inside the agent.\ncontrol what happens inside the agent. Middleware is useful for the following.\nMiddleware is useful for the following.\nMiddleware is useful for the following. It tracks agent behavior with logging\nIt tracks agent behavior with logging\nIt tracks agent behavior with logging analytics and debugging. Transforming\nanalytics and debugging. Transforming\nanalytics and debugging. Transforming prompts tool selection output\nprompts tool selection output\nprompts tool selection output formatting. adding retries, fallbacks,\nformatting. adding retries, fallbacks,\nformatting. adding retries, fallbacks, early termination logic, apply rate\nearly termination logic, apply rate\nearly termination logic, apply rate limits, guardrail and PII detection. Now\nlimits, guardrail and PII detection. Now\nlimits, guardrail and PII detection. Now just by seeing this definition uh I know\njust by seeing this definition uh I know\njust by seeing this definition uh I know many of you will be specifically\nmany of you will be specifically\nmany of you will be specifically confused. So it is always better that I\nconfused. So it is always better that I\nconfused. So it is always better that I try to show you with a very good\ntry to show you with a very good\ntry to show you with a very good example. Okay. So let's consider one\nexample. Okay. So let's consider one\nexample. Okay. So let's consider one example over here.\nexample over here.\nexample over here. Let's consider an example wherein we\nLet's consider an example wherein we\nLet's consider an example wherein we take something like airport security.\ntake something like airport security.\ntake something like airport security. Okay. So I hope everybody may have been\nOkay. So I hope everybody may have been\nOkay. So I hope everybody may have been to airports. Okay. So in the airport\nto airports. Okay. So in the airport\nto airports. Okay. So in the airport security if you go ahead and see that\nsecurity if you go ahead and see that\nsecurity if you go ahead and see that right. So in the airport security when\nright. So in the airport security when\nright. So in the airport security when you enter the airport right when you\nyou enter the airport right when you\nyou enter the airport right when you enter the airport you let's say you are\nenter the airport you let's say you are\nenter the airport you let's say you are the passenger.\nthe passenger.\nthe passenger. So let's say if this is your boarding\nSo let's say if this is your boarding\nSo let's say if this is your boarding gate or this is your flight right the\ngate or this is your flight right the\ngate or this is your flight right the boarding gate is somewhere on 18 number\nboarding gate is somewhere on 18 number\nboarding gate is somewhere on 18 number right now to go to this boarding gate\nright now to go to this boarding gate\nright now to go to this boarding gate you have to when you're entering the\nyou have to when you're entering the\nyou have to when you're entering the airport you have to cross to various\nairport you have to cross to various\nairport you have to cross to various stages right so you need to cross\nstages right so you need to cross\nstages right so you need to cross through security check so let's say\nthrough security check so let's say\nthrough security check so let's say there is a security check over here then\nthere is a security check over here then\nthere is a security check over here then after crossing the security check you\nafter crossing the security check you\nafter crossing the security check you may have to probably go to the\nmay have to probably go to the\nmay have to probably go to the immigration\nAfter going through the immigration, you\nAfter going through the immigration, you need to go ahead and board the flights\nneed to go ahead and board the flights\nneed to go ahead and board the flights and then finally you go to this\nand then finally you go to this\nand then finally you go to this particular gate number where you catch\nparticular gate number where you catch\nparticular gate number where you catch your flight. Right? Now in every of this\nyour flight. Right? Now in every of this\nyour flight. Right? Now in every of this step in the security check what happens\nstep in the security check what happens\nstep in the security check what happens you know we go ahead and apply or over\nyou know we go ahead and apply or over\nyou know we go ahead and apply or over here what will happen in the security\nhere what will happen in the security\nhere what will happen in the security check they will probably go ahead and\ncheck they will probably go ahead and\ncheck they will probably go ahead and see your luggage what is there in the\nsee your luggage what is there in the\nsee your luggage what is there in the luggage and all like you should not be\nluggage and all like you should not be\nluggage and all like you should not be carrying any batteries that kind of\ncarrying any batteries that kind of\ncarrying any batteries that kind of check will happen so this I can\ncheck will happen so this I can\ncheck will happen so this I can basically say this as my middleware one\nbasically say this as my middleware one\nbasically say this as my middleware one okay so I'm going to probably go ahead\nokay so I'm going to probably go ahead\nokay so I'm going to probably go ahead and implement one middleware over here\nand implement one middleware over here\nand implement one middleware over here okay let me write it much more properly\nokay let me write it much more properly\nokay let me write it much more properly so that you should be able to understand\nso that you should be able to understand\nso that you should be able to understand right. So here what I can do I can go\nright. So here what I can do I can go\nright. So here what I can do I can go ahead and develop my middleware one over\nahead and develop my middleware one over\nahead and develop my middleware one over here and this middleware one\nhere and this middleware one\nhere and this middleware one functionality is that it will go ahead\nfunctionality is that it will go ahead\nfunctionality is that it will go ahead and do all the necessary check that is\nand do all the necessary check that is\nand do all the necessary check that is required so that with respect to luggage\nrequired so that with respect to luggage\nrequired so that with respect to luggage with respect to other things. Now the\nwith respect to other things. Now the\nwith respect to other things. Now the second thing over here in the\nsecond thing over here in the\nsecond thing over here in the immigration counter right in the\nimmigration counter right in the\nimmigration counter right in the immigration counter what immigration\nimmigration counter what immigration\nimmigration counter what immigration people will do basically uh they check\npeople will do basically uh they check\npeople will do basically uh they check your passport whether your passport\nyour passport whether your passport\nyour passport whether your passport valid date is there or not each and\nvalid date is there or not each and\nvalid date is there or not each and everything. So that kind of checks can\neverything. So that kind of checks can\neverything. So that kind of checks can basically happen in my middleware too\nbasically happen in my middleware too\nbasically happen in my middleware too right and before boarding you know here\nright and before boarding you know here\nright and before boarding you know here we will probably go the the people will\nwe will probably go the the people will\nwe will probably go the the people will go ahead and see your boarding pass\ngo ahead and see your boarding pass\ngo ahead and see your boarding pass right and see whether the boarding pass\nright and see whether the boarding pass\nright and see whether the boarding pass is right or not. So here we can go ahead\nis right or not. So here we can go ahead\nis right or not. So here we can go ahead and develop our middleware three.\nand develop our middleware three.\nand develop our middleware three. Now just by seeing this example before\nNow just by seeing this example before\nNow just by seeing this example before any important let's consider that this\nany important let's consider that this\nany important let's consider that this is my agent one this is my agent two\nis my agent one this is my agent two\nis my agent one this is my agent two this is my agent three before the agents\nthis is my agent three before the agents\nthis is my agent three before the agents we are doing something we are doing we\nwe are doing something we are doing we\nwe are doing something we are doing we it can be a normal check it can be\nit can be a normal check it can be\nit can be a normal check it can be logging it can be exceptional handling\nlogging it can be exceptional handling\nlogging it can be exceptional handling it can be model calling right it can be\nit can be model calling right it can be\nit can be model calling right it can be anything as such so that's the reason we\nanything as such so that's the reason we\nanything as such so that's the reason we have given this specific definition\nhave given this specific definition\nhave given this specific definition here let's say it provides a way to\nhere let's say it provides a way to\nhere let's say it provides a way to tightly control what happens inside the\ntightly control what happens inside the\ntightly control what happens inside the agent now here We are considering this\nagent now here We are considering this\nagent now here We are considering this as a agent and within this particular\nas a agent and within this particular\nas a agent and within this particular agent we can do multiple things right.\nagent we can do multiple things right.\nagent we can do multiple things right. We can create middleware 1, middleware\nWe can create middleware 1, middleware\nWe can create middleware 1, middleware 2, middleware 3, right? And here we can\n2, middleware 3, right? And here we can\n2, middleware 3, right? And here we can track agent behavior with logging\ntrack agent behavior with logging\ntrack agent behavior with logging analytics, debugging, transforming\nanalytics, debugging, transforming\nanalytics, debugging, transforming prompts tool selections. We can do\nprompts tool selections. We can do\nprompts tool selections. We can do multiple things in short of or different\nmultiple things in short of or different\nmultiple things in short of or different kind of functionalities over here.\nkind of functionalities over here.\nkind of functionalities over here. Right? So uh this is what it is. See\nRight? So uh this is what it is. See\nRight? So uh this is what it is. See this can be considered as a very good\nthis can be considered as a very good\nthis can be considered as a very good example. So before we have let's\nexample. So before we have let's\nexample. So before we have let's consider this is my agent inside this\nconsider this is my agent inside this\nconsider this is my agent inside this agent I have my model I have my tools\nagent I have my model I have my tools\nagent I have my model I have my tools okay and this is nothing but this is a\nokay and this is nothing but this is a\nokay and this is nothing but this is a react agent right so model will when we\nreact agent right so model will when we\nreact agent right so model will when we once we make a request to the model the\nonce we make a request to the model the\nonce we make a request to the model the model will see whether that request\nmodel will see whether that request\nmodel will see whether that request needs to be passed to the tool then the\nneeds to be passed to the tool then the\nneeds to be passed to the tool then the tool will execute it give it give the\ntool will execute it give it give the\ntool will execute it give it give the context back and finally we get the\ncontext back and finally we get the\ncontext back and finally we get the result right with the help of middleware\nresult right with the help of middleware\nresult right with the help of middleware now my agent will look something like\nnow my agent will look something like\nnow my agent will look something like this so agent with middleware\nthis so agent with middleware\nthis so agent with middleware so So in agent with middleware\nso So in agent with middleware\nso So in agent with middleware here we will be able to see that there\nhere we will be able to see that there\nhere we will be able to see that there will be different different triggers.\nwill be different different triggers.\nwill be different different triggers. Okay. So clearly you can see over here\nOkay. So clearly you can see over here\nOkay. So clearly you can see over here what what is the best thing that is\nwhat what is the best thing that is\nwhat what is the best thing that is available this middleware right? It\navailable this middleware right? It\navailable this middleware right? It exposes hooks. We basically say it as\nexposes hooks. We basically say it as\nexposes hooks. We basically say it as hooks. Okay hooks means what? Hooks\nhooks. Okay hooks means what? Hooks\nhooks. Okay hooks means what? Hooks means trigger points. Before the agent\nmeans trigger points. Before the agent\nmeans trigger points. Before the agent we can add something. Before the model\nwe can add something. Before the model\nwe can add something. Before the model we can add something. This uh tools\nwe can add something. This uh tools\nwe can add something. This uh tools calls you can see you can add something.\ncalls you can see you can add something.\ncalls you can see you can add something. After the model call we can add\nAfter the model call we can add\nAfter the model call we can add something. After the agent we can add\nsomething. After the agent we can add\nsomething. After the agent we can add something. It can be logging. It can be\nsomething. It can be logging. It can be\nsomething. It can be logging. It can be summarization. It can be multiple things\nsummarization. It can be multiple things\nsummarization. It can be multiple things in sure. So here in short we are adding\nin sure. So here in short we are adding\nin sure. So here in short we are adding some kind of hooks. Okay. And we are\nsome kind of hooks. Okay. And we are\nsome kind of hooks. Okay. And we are adding these hooks so that we can do\nadding these hooks so that we can do\nadding these hooks so that we can do something over here. Right now the best\nsomething over here. Right now the best\nsomething over here. Right now the best way is that uh we will see first of all\nway is that uh we will see first of all\nway is that uh we will see first of all some built-in built-in middlewares. Okay\nsome built-in built-in middlewares. Okay\nsome built-in built-in middlewares. Okay that is available. So we'll see some\nthat is available. So we'll see some\nthat is available. So we'll see some built-in middlewares. One of the\nbuilt-in middlewares. One of the\nbuilt-in middlewares. One of the middleware which is very commonly used\nmiddleware which is very commonly used\nmiddleware which is very commonly used is something called a summarization\nis something called a summarization\nis something called a summarization middleware.\nmiddleware.\nmiddleware. Now this summarization middleware is a\nNow this summarization middleware is a\nNow this summarization middleware is a uh kind of a middleware that we can use\nuh kind of a middleware that we can use\nuh kind of a middleware that we can use in the agent and it task is only to\nin the agent and it task is only to\nin the agent and it task is only to summarize. So let's say if this is my\nsummarize. So let's say if this is my\nsummarize. So let's say if this is my LLM model or this is my agent.\nLLM model or this is my agent.\nLLM model or this is my agent. This is my agent and let's say this\nThis is my agent and let's say this\nThis is my agent and let's say this agent is basically connected to a tool.\nagent is basically connected to a tool.\nagent is basically connected to a tool. Okay. And this tool is return connected.\nOkay. And this tool is return connected.\nOkay. And this tool is return connected. And here we get the output.\nAnd here we get the output.\nAnd here we get the output. Now here you can see that what this\nNow here you can see that what this\nNow here you can see that what this summarization will do. Okay. What this\nsummarization will do. Okay. What this\nsummarization will do. Okay. What this summarization will be specifically doing\nsummarization will be specifically doing\nsummarization will be specifically doing is that we add this middleware over\nis that we add this middleware over\nis that we add this middleware over here. We add this summarization\nhere. We add this summarization\nhere. We add this summarization middleware over here. So whenever we\nmiddleware over here. So whenever we\nmiddleware over here. So whenever we give any input and once we generate the\ngive any input and once we generate the\ngive any input and once we generate the output let's say after some number of\noutput let's say after some number of\noutput let's say after some number of messages\nmessages\nmessages after some number of input and output\nafter some number of input and output\nafter some number of input and output messages you know that this messages\nmessages you know that this messages\nmessages you know that this messages list will keep on growing. So if I apply\nlist will keep on growing. So if I apply\nlist will keep on growing. So if I apply this summarization middleware what it is\nthis summarization middleware what it is\nthis summarization middleware what it is going to do it is just going to\ngoing to do it is just going to\ngoing to do it is just going to summarize this entire list of messages\nsummarize this entire list of messages\nsummarize this entire list of messages after it reaches some some number let's\nafter it reaches some some number let's\nafter it reaches some some number let's say after it reaches some count after\nsay after it reaches some count after\nsay after it reaches some count after after 10 messages I want this to\nafter 10 messages I want this to\nafter 10 messages I want this to summarize right all these 10 messages I\nsummarize right all these 10 messages I\nsummarize right all these 10 messages I want to summarize then what we can do we\nwant to summarize then what we can do we\nwant to summarize then what we can do we can apply the summarization middle layer\ncan apply the summarization middle layer\ncan apply the summarization middle layer within the agent and it task will be\nwithin the agent and it task will be\nwithin the agent and it task will be that once it reaches 10 when once the\nthat once it reaches 10 when once the\nthat once it reaches 10 when once the count of the message reaches reaches 10.\ncount of the message reaches reaches 10.\ncount of the message reaches reaches 10. We are just going to quickly summarize\nWe are just going to quickly summarize\nWe are just going to quickly summarize the message and this summarization of\nthe message and this summarization of\nthe message and this summarization of the message will be taken care by the\nthe message will be taken care by the\nthe message will be taken care by the LLM. Right? So this kind of middleware\nLLM. Right? So this kind of middleware\nLLM. Right? So this kind of middleware we can add it over here. Okay. Similarly\nwe can add it over here. Okay. Similarly\nwe can add it over here. Okay. Similarly there are other middleware. One of the\nthere are other middleware. One of the\nthere are other middleware. One of the middleware example is human in the loop\nmiddleware example is human in the loop\nmiddleware example is human in the loop feedback. I can basically say human in\nfeedback. I can basically say human in\nfeedback. I can basically say human in the feedback. So this summarization also\nthe feedback. So this summarization also\nthe feedback. So this summarization also this middleware also I can add. There is\nthis middleware also I can add. There is\nthis middleware also I can add. There is a model tool calling.\na model tool calling.\na model tool calling. There is one more uh very good uh\nThere is one more uh very good uh\nThere is one more uh very good uh built-in middleware and there are list\nbuilt-in middleware and there are list\nbuilt-in middleware and there are list of middlewares which you can basically\nof middlewares which you can basically\nof middlewares which you can basically use it like model call limit. Okay,\nuse it like model call limit. Okay,\nuse it like model call limit. Okay, model call limit basically means uh what\nmodel call limit basically means uh what\nmodel call limit basically means uh what limit the number of models to prevent uh\nlimit the number of models to prevent uh\nlimit the number of models to prevent uh you know excessive cost. So there there\nyou know excessive cost. So there there\nyou know excessive cost. So there there are many okay I'll just show you the\nare many okay I'll just show you the\nare many okay I'll just show you the I'll just show you the documentation. So\nI'll just show you the documentation. So\nI'll just show you the documentation. So here you can see I have summarization\nhere you can see I have summarization\nhere you can see I have summarization middle where it automatically summarizes\nmiddle where it automatically summarizes\nmiddle where it automatically summarizes conversation history when approaching\nconversation history when approaching\nconversation history when approaching token limits human in the loop. It saves\ntoken limits human in the loop. It saves\ntoken limits human in the loop. It saves pause the execution for human approval\npause the execution for human approval\npause the execution for human approval of tool calls. Then you have model call\nof tool calls. Then you have model call\nof tool calls. Then you have model call limits limit the number of model calls\nlimits limit the number of model calls\nlimits limit the number of model calls to prevent excessive cost. Then you have\nto prevent excessive cost. Then you have\nto prevent excessive cost. Then you have tool call limit control tool execution\ntool call limit control tool execution\ntool call limit control tool execution by limiting call counts. You have model\nby limiting call counts. You have model\nby limiting call counts. You have model fallback to-do list LLM tool selector\nfallback to-do list LLM tool selector\nfallback to-do list LLM tool selector tool retry. So many different options\ntool retry. So many different options\ntool retry. So many different options are there. Okay. So we I will now go\nare there. Okay. So we I will now go\nare there. Okay. So we I will now go ahead and show you that how you can go\nahead and show you that how you can go\nahead and show you that how you can go ahead and apply this middleware itself.\nahead and apply this middleware itself.\nahead and apply this middleware itself. Right? So first of all what I will do I\nRight? So first of all what I will do I\nRight? So first of all what I will do I will go ahead and quickly open my\nwill go ahead and quickly open my\nwill go ahead and quickly open my Jupyter notebook. So this is my uh some\nJupyter notebook. So this is my uh some\nJupyter notebook. So this is my uh some middleware over here. You can see I will\nmiddleware over here. You can see I will\nmiddleware over here. You can see I will close this. Okay. This is my middleware\nclose this. Okay. This is my middleware\nclose this. Okay. This is my middleware code. So first of all we go ahead and\ncode. So first of all we go ahead and\ncode. So first of all we go ahead and import or we go ahead and load our\nimport or we go ahead and load our\nimport or we go ahead and load our environment variable with open AI API\nenvironment variable with open AI API\nenvironment variable with open AI API key. Okay. Now the next step is that we\nkey. Okay. Now the next step is that we\nkey. Okay. Now the next step is that we will go ahead and write our code.\nwill go ahead and write our code.\nwill go ahead and write our code. Now writing our code is very simple over\nNow writing our code is very simple over\nNow writing our code is very simple over here. Okay. Here first of all we will go\nhere. Okay. Here first of all we will go\nhere. Okay. Here first of all we will go ahead with our summarization\nahead with our summarization\nahead with our summarization summarization\nsummarization\nsummarization middleware. Okay.\nmiddleware. Okay.\nmiddleware. Okay. Summarization middleware. Again it is\nSummarization middleware. Again it is\nSummarization middleware. Again it is not possible to cover all the different\nnot possible to cover all the different\nnot possible to cover all the different types of middleware that is available\ntypes of middleware that is available\ntypes of middleware that is available over here. But I'll try my level best to\nover here. But I'll try my level best to\nover here. But I'll try my level best to cover some very important so that you\ncover some very important so that you\ncover some very important so that you can independently\ncan independently\ncan independently do all the things uh you know after\ndo all the things uh you know after\ndo all the things uh you know after seeing some examples. Okay, because at\nseeing some examples. Okay, because at\nseeing some examples. Okay, because at the end of the day it's up to you for\nthe end of the day it's up to you for\nthe end of the day it's up to you for what kind of use cases you are\nwhat kind of use cases you are\nwhat kind of use cases you are specifically using this. Okay, so let's\nspecifically using this. Okay, so let's\nspecifically using this. Okay, so let's go ahead with the summarization. Now\ngo ahead with the summarization. Now\ngo ahead with the summarization. Now summarization middleware I will also go\nsummarization middleware I will also go\nsummarization middleware I will also go ahead and probably provide you some\nahead and probably provide you some\nahead and probably provide you some definition over here. Okay, so here you\ndefinition over here. Okay, so here you\ndefinition over here. Okay, so here you can see it automatically\ncan see it automatically\ncan see it automatically summarizes. So let me see I will try to\nsummarizes. So let me see I will try to\nsummarizes. So let me see I will try to provide you a material which will be\nprovide you a material which will be\nprovide you a material which will be very meaningful and you should be able\nvery meaningful and you should be able\nvery meaningful and you should be able to learn read it later on. So\nto learn read it later on. So\nto learn read it later on. So summarization middleware is nothing but\nsummarization middleware is nothing but\nsummarization middleware is nothing but it automatically summarizes conversation\nit automatically summarizes conversation\nit automatically summarizes conversation history when approaching token limits\nhistory when approaching token limits\nhistory when approaching token limits preserving recent messages while\npreserving recent messages while\npreserving recent messages while compressing the older context. Okay. So\ncompressing the older context. Okay. So\ncompressing the older context. Okay. So what it does is that it compresses the\nwhat it does is that it compresses the\nwhat it does is that it compresses the older context and it just use the recent\nolder context and it just use the recent\nolder context and it just use the recent messages whenever the token limit is\nmessages whenever the token limit is\nmessages whenever the token limit is reached. Summarization is useful for the\nreached. Summarization is useful for the\nreached. Summarization is useful for the following longunning conversation. So\nfollowing longunning conversation. So\nfollowing longunning conversation. So specifically in a chatbot when you have\nspecifically in a chatbot when you have\nspecifically in a chatbot when you have a longunning conversation it is always\na longunning conversation it is always\na longunning conversation it is always good that we try to summarize the\ngood that we try to summarize the\ngood that we try to summarize the previous context multi-turn dialogues\nprevious context multi-turn dialogues\nprevious context multi-turn dialogues with extensive history application while\nwith extensive history application while\nwith extensive history application while preserving full conversation context\npreserving full conversation context\npreserving full conversation context matters. Okay. So now let me quickly go\nmatters. Okay. So now let me quickly go\nmatters. Okay. So now let me quickly go ahead and show you one example that how\nahead and show you one example that how\nahead and show you one example that how you can go ahead and implement this.\nyou can go ahead and implement this.\nyou can go ahead and implement this. Okay. So first of all what I'll do and\nOkay. So first of all what I'll do and\nOkay. So first of all what I'll do and uh we can use different different\nuh we can use different different\nuh we can use different different triggers also. Okay. I will show you in\ntriggers also. Okay. I will show you in\ntriggers also. Okay. I will show you in summarization there are multiple\nsummarization there are multiple\nsummarization there are multiple triggers which you can actually use. Uh\ntriggers which you can actually use. Uh\ntriggers which you can actually use. Uh there is a token trigger there is uh uh\nthere is a token trigger there is uh uh\nthere is a token trigger there is uh uh messages trigger and all. Okay. So first\nmessages trigger and all. Okay. So first\nmessages trigger and all. Okay. So first of all what I will do I will go ahead\nof all what I will do I will go ahead\nof all what I will do I will go ahead and show you how we can go ahead and\nand show you how we can go ahead and\nand show you how we can go ahead and create an agent. So from langin uh dot\ncreate an agent. So from langin uh dot\ncreate an agent. So from langin uh dot agents I'm going to go ahead and import\nagents I'm going to go ahead and import\nagents I'm going to go ahead and import create\ncreate\ncreate agent. Okay. So this is the first one.\nagent. Okay. So this is the first one.\nagent. Okay. So this is the first one. Then from langchain dot agents\nThen from langchain dot agents\nThen from langchain dot agents uh dot middleware I'm going to go ahead\nuh dot middleware I'm going to go ahead\nuh dot middleware I'm going to go ahead and import summarization middleware.\nand import summarization middleware.\nand import summarization middleware. Okay. Then from langchen\nOkay. Then from langchen\nOkay. Then from langchen dot uh we are also going to go ahead and\ndot uh we are also going to go ahead and\ndot uh we are also going to go ahead and use checkpoint. Okay. The checkpoint is\nuse checkpoint. Okay. The checkpoint is\nuse checkpoint. Okay. The checkpoint is required so that I go ahead and apply\nrequired so that I go ahead and apply\nrequired so that I go ahead and apply some memory also. So I will go ahead and\nsome memory also. So I will go ahead and\nsome memory also. So I will go ahead and say memory. Okay. from langin dot\nsay memory. Okay. from langin dot\nsay memory. Okay. from langin dot checkpoint dotmemory import inmemory so\ncheckpoint dotmemory import inmemory so\ncheckpoint dotmemory import inmemory so I'm going to also go ahead and apply in\nI'm going to also go ahead and apply in\nI'm going to also go ahead and apply in memory so that I can go ahead and apply\nmemory so that I can go ahead and apply\nmemory so that I can go ahead and apply checkpoints uh within my chat bots right\ncheckpoints uh within my chat bots right\ncheckpoints uh within my chat bots right then from langchin\nthen from langchin\nthen from langchin core dot messages I'm going to use uh\ncore dot messages I'm going to use uh\ncore dot messages I'm going to use uh human message\nhuman message\nhuman message and then I'm also going to use system\nand then I'm also going to use system\nand then I'm also going to use system message\nmessage\nmessage system message. Okay. So these are the\nsystem message. Okay. So these are the\nsystem message. Okay. So these are the basic libraries uh that I'm going to\nbasic libraries uh that I'm going to\nbasic libraries uh that I'm going to use. The first example that we are going\nuse. The first example that we are going\nuse. The first example that we are going to do is that message based\nto do is that message based\nto do is that message based summarization. Okay. Message\nsummarization. Okay. Message\nsummarization. Okay. Message based summarization.\nSo I'll go ahead and create my agent. My\nSo I'll go ahead and create my agent. My agent is equal to create agent. And\nagent is equal to create agent. And\nagent is equal to create agent. And inside say this create agent. First of\ninside say this create agent. First of\ninside say this create agent. First of all I'll go ahead and use my model.\nall I'll go ahead and use my model.\nall I'll go ahead and use my model. Let's say the model that I use is GPT 40\nLet's say the model that I use is GPT 40\nLet's say the model that I use is GPT 40 mini. Okay, 40 mini. So this is the\nmini. Okay, 40 mini. So this is the\nmini. Okay, 40 mini. So this is the model that we are going to use. Uh\nmodel that we are going to use. Uh\nmodel that we are going to use. Uh tools, you can go ahead and define your\ntools, you can go ahead and define your\ntools, you can go ahead and define your tools but right now I did not define any\ntools but right now I did not define any\ntools but right now I did not define any tools as such. So I'm just going to go\ntools as such. So I'm just going to go\ntools as such. So I'm just going to go ahead and keep like this. Then we going\nahead and keep like this. Then we going\nahead and keep like this. Then we going to use checkpointter. This is for my\nto use checkpointter. This is for my\nto use checkpointter. This is for my checkpointing uh the whatever\ncheckpointing uh the whatever\ncheckpointing uh the whatever conversation history is there. I'm\nconversation history is there. I'm\nconversation history is there. I'm trying to save it within my local\ntrying to save it within my local\ntrying to save it within my local hardware like in my hard disk itself.\nhardware like in my hard disk itself.\nhardware like in my hard disk itself. Okay. Now to give the middleware as an\nOkay. Now to give the middleware as an\nOkay. Now to give the middleware as an option inside this agent. See our main\noption inside this agent. See our main\noption inside this agent. See our main aim is that I want to add a middleware\naim is that I want to add a middleware\naim is that I want to add a middleware inside this agent. Right? So here you\ninside this agent. Right? So here you\ninside this agent. Right? So here you can see I've given model information.\ncan see I've given model information.\ncan see I've given model information. I've given checkpoint. So here you can\nI've given checkpoint. So here you can\nI've given checkpoint. So here you can also go ahead and give your\nalso go ahead and give your\nalso go ahead and give your summarization uh sorry middleware as a\nsummarization uh sorry middleware as a\nsummarization uh sorry middleware as a parameter. So inside this middleware you\nparameter. So inside this middleware you\nparameter. So inside this middleware you can give a list of middleware like what\ncan give a list of middleware like what\ncan give a list of middleware like what all middleares you really want to apply.\nall middleares you really want to apply.\nall middleares you really want to apply. So now here we are applying the\nSo now here we are applying the\nSo now here we are applying the summarization middleware within our\nsummarization middleware within our\nsummarization middleware within our agent. So inside this particular agent\nagent. So inside this particular agent\nagent. So inside this particular agent we are applying summarization. But when\nwe are applying summarization. But when\nwe are applying summarization. But when do the summarization actually happen?\ndo the summarization actually happen?\ndo the summarization actually happen? Right? That is the major question.\nRight? That is the major question.\nRight? That is the major question. Right? So inside the summarization, we\nRight? So inside the summarization, we\nRight? So inside the summarization, we have an option to give multiple\nhave an option to give multiple\nhave an option to give multiple parameters. First of all, what LLM model\nparameters. First of all, what LLM model\nparameters. First of all, what LLM model we are going to use in order to do the\nwe are going to use in order to do the\nwe are going to use in order to do the summarization. So let's say I want to go\nsummarization. So let's say I want to go\nsummarization. So let's say I want to go ahead and it's always a better idea that\nahead and it's always a better idea that\nahead and it's always a better idea that we use uh models that cost less for the\nwe use uh models that cost less for the\nwe use uh models that cost less for the summarization because uh whenever the\nsummarization because uh whenever the\nsummarization because uh whenever the message expands\nmessage expands\nmessage expands uh up to a certain count we are again\nuh up to a certain count we are again\nuh up to a certain count we are again going to do this uh summarization in\ngoing to do this uh summarization in\ngoing to do this uh summarization in short right so it is always good that\nshort right so it is always good that\nshort right so it is always good that you try to use a model LLM model which\nyou try to use a model LLM model which\nyou try to use a model LLM model which has lesser cost you know with respect to\nhas lesser cost you know with respect to\nhas lesser cost you know with respect to tokens then I want this summarization to\ntokens then I want this summarization to\ntokens then I want this summarization to trigger right so there will be another\ntrigger right so there will be another\ntrigger right so there will be another parameter which is called as trigger And\nparameter which is called as trigger And\nparameter which is called as trigger And inside this trigger what we are going to\ninside this trigger what we are going to\ninside this trigger what we are going to do we are going to put our condition\ndo we are going to put our condition\ndo we are going to put our condition like when I want the summarization to\nlike when I want the summarization to\nlike when I want the summarization to happen. So here I will say when my\nhappen. So here I will say when my\nhappen. So here I will say when my messages length is becoming 10 at least\nmessages length is becoming 10 at least\nmessages length is becoming 10 at least okay my input output all the messages\nokay my input output all the messages\nokay my input output all the messages that is which which is getting generated\nthat is which which is getting generated\nthat is which which is getting generated uh whenever it becomes 10 usually\nuh whenever it becomes 10 usually\nuh whenever it becomes 10 usually whenever you create a chatbot this\nwhenever you create a chatbot this\nwhenever you create a chatbot this number is a bigger number right but just\nnumber is a bigger number right but just\nnumber is a bigger number right but just to show you in this use case we are\nto show you in this use case we are\nto show you in this use case we are going to set it as 10. Okay, then I'm\ngoing to set it as 10. Okay, then I'm\ngoing to set it as 10. Okay, then I'm also going to say that at this point you\nalso going to say that at this point you\nalso going to say that at this point you go ahead and trigger it. But when you\ngo ahead and trigger it. But when you\ngo ahead and trigger it. But when you trigger it, you summarize the previous\ntrigger it, you summarize the previous\ntrigger it, you summarize the previous contest and keep the recent\ncontest and keep the recent\ncontest and keep the recent top four messages. Okay, recent top four\ntop four messages. Okay, recent top four\ntop four messages. Okay, recent top four messages like that, right? So that we\nmessages like that, right? So that we\nmessages like that, right? So that we get the context and we go ahead and\nget the context and we go ahead and\nget the context and we go ahead and apply it. So here what you can do this\napply it. So here what you can do this\napply it. So here what you can do this is just one of the summarization which I\nis just one of the summarization which I\nis just one of the summarization which I have actually applied. Now you can keep\nhave actually applied. Now you can keep\nhave actually applied. Now you can keep on adding any number of summarization,\non adding any number of summarization,\non adding any number of summarization, any number of middlewares, right? You\nany number of middlewares, right? You\nany number of middlewares, right? You just need to put comma over here. Then\njust need to put comma over here. Then\njust need to put comma over here. Then you go ahead and define your next some\nyou go ahead and define your next some\nyou go ahead and define your next some next middleware after this. Right? Any\nnext middleware after this. Right? Any\nnext middleware after this. Right? Any number of middleares you can actually go\nnumber of middleares you can actually go\nnumber of middleares you can actually go ahead and add it. Okay. So now this is a\nahead and add it. Okay. So now this is a\nahead and add it. Okay. So now this is a basic agent that I've actually created\nbasic agent that I've actually created\nbasic agent that I've actually created wherein I have added a middleware of\nwherein I have added a middleware of\nwherein I have added a middleware of summarization middleware. Okay. So now\nsummarization middleware. Okay. So now\nsummarization middleware. Okay. So now once I execute this cell my agent is\nonce I execute this cell my agent is\nonce I execute this cell my agent is ready. Okay. Now all I have to do is\nready. Okay. Now all I have to do is\nready. Okay. Now all I have to do is that in order to test this out right\nthat in order to test this out right\nthat in order to test this out right whether this summarization is happening\nwhether this summarization is happening\nwhether this summarization is happening or not let's check it out how we can\nor not let's check it out how we can\nor not let's check it out how we can actually do it. Okay. So first of all\nactually do it. Okay. So first of all\nactually do it. Okay. So first of all before I invoke anything with this\nbefore I invoke anything with this\nbefore I invoke anything with this particular agent I want to go ahead and\nparticular agent I want to go ahead and\nparticular agent I want to go ahead and create a thread. Okay. So I will go\ncreate a thread. Okay. So I will go\ncreate a thread. Okay. So I will go ahead and run with a thread ID. And for\nahead and run with a thread ID. And for\nahead and run with a thread ID. And for this I will go ahead and create my\nthis I will go ahead and create my\nthis I will go ahead and create my config. Inside my config I'm going to go\nconfig. Inside my config I'm going to go\nconfig. Inside my config I'm going to go ahead and create my variable called as\nahead and create my variable called as\nahead and create my variable called as configurable. Okay. And then I'm going\nconfigurable. Okay. And then I'm going\nconfigurable. Okay. And then I'm going to go ahead and create my thread ID.\nto go ahead and create my thread ID.\nto go ahead and create my thread ID. This will actually uniquely identify\nThis will actually uniquely identify\nThis will actually uniquely identify a user. Okay. So here I will say test\na user. Okay. So here I will say test\na user. Okay. So here I will say test one. So this is my unique user. Let's\none. So this is my unique user. Let's\none. So this is my unique user. Let's say this particular thread is my unique\nsay this particular thread is my unique\nsay this particular thread is my unique user. And I'm going to go ahead and do\nuser. And I'm going to go ahead and do\nuser. And I'm going to go ahead and do this. Okay. Now let's create some kind\nthis. Okay. Now let's create some kind\nthis. Okay. Now let's create some kind of test data. Okay. So let's say these\nof test data. Okay. So let's say these\nof test data. Okay. So let's say these are my convers. These are my human\nare my convers. These are my human\nare my convers. These are my human questions I need to ask the agent to\nquestions I need to ask the agent to\nquestions I need to ask the agent to this particular agent like what is 2 + 2\nthis particular agent like what is 2 + 2\nthis particular agent like what is 2 + 2 what is 10 * 5 what is 10 100 divid by 4\nwhat is 10 * 5 what is 10 100 divid by 4\nwhat is 10 * 5 what is 10 100 divid by 4 what is 15 - 7 and then my llm will also\nwhat is 15 - 7 and then my llm will also\nwhat is 15 - 7 and then my llm will also keep on my agent will keep on generating\nkeep on my agent will keep on generating\nkeep on my agent will keep on generating the answer so here what I will do I will\nthe answer so here what I will do I will\nthe answer so here what I will do I will say for Q in questions okay and I will\nsay for Q in questions okay and I will\nsay for Q in questions okay and I will go ahead and generate my response my\ngo ahead and generate my response my\ngo ahead and generate my response my response will be using this agent invoke\nresponse will be using this agent invoke\nresponse will be using this agent invoke agent invoke and here we are going to go\nagent invoke and here we are going to go\nagent invoke and here we are going to go ahead and set this in the form of a\nahead and set this in the form of a\nahead and set this in the form of a messages because we need to provide in\nmessages because we need to provide in\nmessages because we need to provide in the form of a message and here I'm going\nthe form of a message and here I'm going\nthe form of a message and here I'm going to go ahead and use my human message. My\nto go ahead and use my human message. My\nto go ahead and use my human message. My human message is nothing but whatever\nhuman message is nothing but whatever\nhuman message is nothing but whatever questions I have which I'm reading in\nquestions I have which I'm reading in\nquestions I have which I'm reading in this Q variable I will be giving it over\nthis Q variable I will be giving it over\nthis Q variable I will be giving it over here. Right? And then I will have my\nhere. Right? And then I will have my\nhere. Right? And then I will have my config variable. Clear? Then what I'm\nconfig variable. Clear? Then what I'm\nconfig variable. Clear? Then what I'm going to do, I'm going to print whatever\ngoing to do, I'm going to print whatever\ngoing to do, I'm going to print whatever response I'm actually going to get. And\nresponse I'm actually going to get. And\nresponse I'm actually going to get. And along with that, I'm also going to print\nalong with that, I'm also going to print\nalong with that, I'm also going to print the length of the response messages.\nthe length of the response messages.\nthe length of the response messages. Okay, the reason why I'm printing the\nOkay, the reason why I'm printing the\nOkay, the reason why I'm printing the length of the response messages to show\nlength of the response messages to show\nlength of the response messages to show you because here we have set up that\nyou because here we have set up that\nyou because here we have set up that whenever the message size increases more\nwhenever the message size increases more\nwhenever the message size increases more than 10, the summarization should\nthan 10, the summarization should\nthan 10, the summarization should happen. And when the summarization\nhappen. And when the summarization\nhappen. And when the summarization happens, this message length will get\nhappens, this message length will get\nhappens, this message length will get reduced. Okay. So here you can see I'm\nreduced. Okay. So here you can see I'm\nreduced. Okay. So here you can see I'm testing all these messages. So first of\ntesting all these messages. So first of\ntesting all these messages. So first of all first question will go what is 2+2\nall first question will go what is 2+2\nall first question will go what is 2+2 and uh uh you know my llm my agent will\nand uh uh you know my llm my agent will\nand uh uh you know my llm my agent will uh provide me the answer 2 + 2 is four.\nuh provide me the answer 2 + 2 is four.\nuh provide me the answer 2 + 2 is four. Then we are going to print that entire\nThen we are going to print that entire\nThen we are going to print that entire response and then we also going to see\nresponse and then we also going to see\nresponse and then we also going to see the length of the message response.\nthe length of the message response.\nthe length of the message response. Okay. And when this length of the\nOkay. And when this length of the\nOkay. And when this length of the message response increases more than 10\nmessage response increases more than 10\nmessage response increases more than 10 automatically the summarization will\nautomatically the summarization will\nautomatically the summarization will happen with this particular LLM model.\nhappen with this particular LLM model.\nhappen with this particular LLM model. So let's go ahead and try this out.\nSo let's go ahead and try this out.\nSo let's go ahead and try this out. Okay. So here you can see message\nOkay. So here you can see message\nOkay. So here you can see message message 2 message 4 message 6 message 8\nmessage 2 message 4 message 6 message 8\nmessage 2 message 4 message 6 message 8 10. Now automatically my summarization\n10. Now automatically my summarization\n10. Now automatically my summarization should happen over here. See now it has\nshould happen over here. See now it has\nshould happen over here. See now it has gone message 6. And here is the content.\ngone message 6. And here is the content.\ngone message 6. And here is the content. Here is the summary of the conversation\nHere is the summary of the conversation\nHere is the summary of the conversation to date. Human asked several arithmetic\nto date. Human asked several arithmetic\nto date. Human asked several arithmetic question. What is 2 + 2? A responded 2 +\nquestion. What is 2 + 2? A responded 2 +\nquestion. What is 2 + 2? A responded 2 + 2= 4. Uh what is 10 * 5? 10 mult* 5= 50.\n2= 4. Uh what is 10 * 5? 10 mult* 5= 50.\n2= 4. Uh what is 10 * 5? 10 mult* 5= 50. So here the summarization has happened.\nSo here the summarization has happened.\nSo here the summarization has happened. Why it has happened over here? because\nWhy it has happened over here? because\nWhy it has happened over here? because when my message length got triggered to\nwhen my message length got triggered to\nwhen my message length got triggered to 10 right triggered to 10 then it is\n10 right triggered to 10 then it is\n10 right triggered to 10 then it is going to go ahead and do the entire\ngoing to go ahead and do the entire\ngoing to go ahead and do the entire summarization and that's the very\nsummarization and that's the very\nsummarization and that's the very important property of middleware right I\nimportant property of middleware right I\nimportant property of middleware right I hope you are able to understand the\nhope you are able to understand the\nhope you are able to understand the power of middleware right let's see one\npower of middleware right let's see one\npower of middleware right let's see one more example see one of the trigger is\nmore example see one of the trigger is\nmore example see one of the trigger is through this way right where we have\nthrough this way right where we have\nthrough this way right where we have what we have done is that here I've\nwhat we have done is that here I've\nwhat we have done is that here I've applied this trigger based on the\napplied this trigger based on the\napplied this trigger based on the message length right 10 now there is\nmessage length right 10 now there is\nmessage length right 10 now there is also different way. Uh one of the way is\nalso different way. Uh one of the way is\nalso different way. Uh one of the way is basically based on token size, right? So\nbasically based on token size, right? So\nbasically based on token size, right? So let's go ahead and do based on token\nlet's go ahead and do based on token\nlet's go ahead and do based on token size.\nsize.\nsize. This was based on the length of the\nThis was based on the length of the\nThis was based on the length of the message. Now based on token size also\nmessage. Now based on token size also\nmessage. Now based on token size also you can actually do it. Now let's go\nyou can actually do it. Now let's go\nyou can actually do it. Now let's go ahead and do it. Now here what I'm\nahead and do it. Now here what I'm\nahead and do it. Now here what I'm actually going to do, I will first of\nactually going to do, I will first of\nactually going to do, I will first of all import all the libraries. So these\nall import all the libraries. So these\nall import all the libraries. So these are all my libraries that I'm actually\nare all my libraries that I'm actually\nare all my libraries that I'm actually going to import. From langin aents\ngoing to import. From langin aents\ngoing to import. From langin aents import uh create agent. Then from\nimport uh create agent. Then from\nimport uh create agent. Then from langin.tagents. middleware import\nlangin.tagents. middleware import\nlangin.tagents. middleware import summarization middleware then we also\nsummarization middleware then we also\nsummarization middleware then we also going to create tools uh over here we\ngoing to create tools uh over here we\ngoing to create tools uh over here we used human message in memory and then\nused human message in memory and then\nused human message in memory and then this is the tool that we have created\nthis is the tool that we have created\nthis is the tool that we have created let's say that this is my search hotel\nlet's say that this is my search hotel\nlet's say that this is my search hotel functionality and here I have hardcoded\nfunctionality and here I have hardcoded\nfunctionality and here I have hardcoded some things okay hotels in this and\nsome things okay hotels in this and\nsome things okay hotels in this and these are all the possible hotels that\nthese are all the possible hotels that\nthese are all the possible hotels that are available let's consider that this\nare available let's consider that this\nare available let's consider that this is probably returned from some API okay\nis probably returned from some API okay\nis probably returned from some API okay now what I will do I will go ahead and\nnow what I will do I will go ahead and\nnow what I will do I will go ahead and create my agent and this time my trigger\ncreate my agent and this time my trigger\ncreate my agent and this time my trigger will be token count okay token count So\nwill be token count okay token count So\nwill be token count okay token count So uh token count basically means how many\nuh token count basically means how many\nuh token count basically means how many tokens has been generated by the model.\ntokens has been generated by the model.\ntokens has been generated by the model. Right? So here I'm again going to use\nRight? So here I'm again going to use\nRight? So here I'm again going to use agent create agent. Okay. And then we\nagent create agent. Okay. And then we\nagent create agent. Okay. And then we are going to go ahead and use model is\nare going to go ahead and use model is\nare going to go ahead and use model is equal to\nequal to\nequal to GPT 40\nGPT 40\nGPT 40 mini. Okay. And then I'm going to go\nmini. Okay. And then I'm going to go\nmini. Okay. And then I'm going to go ahead and use my tools.\nahead and use my tools.\nahead and use my tools. My tools will be nothing but let's\nMy tools will be nothing but let's\nMy tools will be nothing but let's consider that I'm going to use search\nconsider that I'm going to use search\nconsider that I'm going to use search hotels over here. my checkpointer.\nLet's see whether I've imported\nLet's see whether I've imported checkpointer or not.\nCheckpointer is over here\nCheckpointer is over here is equal to inmemory\nis equal to inmemory\nis equal to inmemory saver.\nsaver.\nsaver. And then I'm going to go ahead and apply\nAnd then I'm going to go ahead and apply\nAnd then I'm going to go ahead and apply my middleware again. And this time the\nmy middleware again. And this time the\nmy middleware again. And this time the middleware that I'm going to apply is\nmiddleware that I'm going to apply is\nmiddleware that I'm going to apply is nothing but summarization middleware.\nnothing but summarization middleware.\nnothing but summarization middleware. And here I'm going to give my\nAnd here I'm going to give my\nAnd here I'm going to give my parameters. Let's say the first\nparameters. Let's say the first\nparameters. Let's say the first parameter is my model which is nothing\nparameter is my model which is nothing\nparameter is my model which is nothing but GPT 40 mini.\nbut GPT 40 mini.\nbut GPT 40 mini. This time my trigger will be not based\nThis time my trigger will be not based\nThis time my trigger will be not based on messages but based on tokens. So now\non messages but based on tokens. So now\non messages but based on tokens. So now I'm going to specify tokens and token\nI'm going to specify tokens and token\nI'm going to specify tokens and token length I'll keep it to 550. Let's say\nlength I'll keep it to 550. Let's say\nlength I'll keep it to 550. Let's say that if it increases more than 550 then\nthat if it increases more than 550 then\nthat if it increases more than 550 then what I'm actually going to do the\nwhat I'm actually going to do the\nwhat I'm actually going to do the summarization will happen. And when the\nsummarization will happen. And when the\nsummarization will happen. And when the summarization is basically happening, we\nsummarization is basically happening, we\nsummarization is basically happening, we are going to go ahead and keep the\nare going to go ahead and keep the\nare going to go ahead and keep the recent 200 tokens. Okay. So recent 200\nrecent 200 tokens. Okay. So recent 200\nrecent 200 tokens. Okay. So recent 200 tokens.\ntokens.\ntokens. These are the parameters that is\nThese are the parameters that is\nThese are the parameters that is available out there, right? And inbuilt\navailable out there, right? And inbuilt\navailable out there, right? And inbuilt parameters, right? So this is done. This\nparameters, right? So this is done. This\nparameters, right? So this is done. This is my summarization that is basically\nis my summarization that is basically\nis my summarization that is basically going to get applied. Let's see. Did I\ngoing to get applied. Let's see. Did I\ngoing to get applied. Let's see. Did I miss out anything over here? This should\nmiss out anything over here? This should\nmiss out anything over here? This should be trigger is equal to. Okay, perfect.\nbe trigger is equal to. Okay, perfect.\nbe trigger is equal to. Okay, perfect. Now this is my agent that has got\nNow this is my agent that has got\nNow this is my agent that has got created. Now what I will do I will go\ncreated. Now what I will do I will go\ncreated. Now what I will do I will go ahead and create my config. Okay config\nahead and create my config. Okay config\nahead and create my config. Okay config will be nothing but this config so that\nwill be nothing but this config so that\nwill be nothing but this config so that we apply for a specific user and uh just\nwe apply for a specific user and uh just\nwe apply for a specific user and uh just to display or print how many tokens has\nto display or print how many tokens has\nto display or print how many tokens has been generated. I will create this\nbeen generated. I will create this\nbeen generated. I will create this function called as count tokens. Total\nfunction called as count tokens. Total\nfunction called as count tokens. Total character is equal to some length of\ncharacter is equal to some length of\ncharacter is equal to some length of whatever content is there. Right? That\nwhatever content is there. Right? That\nwhatever content is there. Right? That length and we are saying that we are\nlength and we are saying that we are\nlength and we are saying that we are considering okay four characters is\nconsidering okay four characters is\nconsidering okay four characters is equal to one token. Okay, four character\nequal to one token. Okay, four character\nequal to one token. Okay, four character is equal to one token. So this is what\nis equal to one token. So this is what\nis equal to one token. So this is what is basically happening. Okay, so now I'm\nis basically happening. Okay, so now I'm\nis basically happening. Okay, so now I'm getting an error. Let's see. Unable to\ngetting an error. Let's see. Unable to\ngetting an error. Let's see. Unable to find GBD 40. I've written 40. It should\nfind GBD 40. I've written 40. It should\nfind GBD 40. I've written 40. It should be 4 ohm mini. 4 mini. It's okay. Uh\nbe 4 ohm mini. 4 mini. It's okay. Uh\nbe 4 ohm mini. 4 mini. It's okay. Uh please specify model directly. Okay. GTP\nplease specify model directly. Okay. GTP\nplease specify model directly. Okay. GTP I have written it over here. It should\nI have written it over here. It should\nI have written it over here. It should be GPT.\nbe GPT.\nbe GPT. GPD. Okay. Now done. This is done. Okay.\nGPD. Okay. Now done. This is done. Okay.\nGPD. Okay. Now done. This is done. Okay. Now we are going to go ahead and run it.\nNow we are going to go ahead and run it.\nNow we are going to go ahead and run it. Okay. And we are going to run this test\nOkay. And we are going to run this test\nOkay. And we are going to run this test for the so here you can see I have\nfor the so here you can see I have\nfor the so here you can see I have created cities like Paris, London,\ncreated cities like Paris, London,\ncreated cities like Paris, London, Tokyo, New York, Dubai and Singapore.\nTokyo, New York, Dubai and Singapore.\nTokyo, New York, Dubai and Singapore. And this is my question. Find hotels in\nAnd this is my question. Find hotels in\nAnd this is my question. Find hotels in this specific city. Right? And we are\nthis specific city. Right? And we are\nthis specific city. Right? And we are doing agent.invoke.\ndoing agent.invoke.\ndoing agent.invoke. Then we are counting the total number of\nThen we are counting the total number of\nThen we are counting the total number of tokens from this response dossage and\ntokens from this response dossage and\ntokens from this response dossage and I'm printing both these things. Now here\nI'm printing both these things. Now here\nI'm printing both these things. Now here you can see one very important thing is\nyou can see one very important thing is\nyou can see one very important thing is that when the token size increases 550\nthat when the token size increases 550\nthat when the token size increases 550 more than 550 then the summarization\nmore than 550 then the summarization\nmore than 550 then the summarization will happen right so now let's go ahead\nwill happen right so now let's go ahead\nwill happen right so now let's go ahead and execute this\nand execute this\nand execute this this is good okay you'll be able to see\nthis is good okay you'll be able to see\nthis is good okay you'll be able to see the response so here 149 tokens is there\nthe response so here 149 tokens is there\nthe response so here 149 tokens is there four messages okay now this will\nfour messages okay now this will\nfour messages okay now this will increase 302\nincrease 302\nincrease 302 then 456\nthen 456\nthen 456 then when see it increases to 550 so see\nthen when see it increases to 550 so see\nthen when see it increases to 550 so see now from 450 56 it has become 396 that\nnow from 450 56 it has become 396 that\nnow from 450 56 it has become 396 that basically means uh over here after this\nbasically means uh over here after this\nbasically means uh over here after this 550 had expanded. So we are able to do\n550 had expanded. So we are able to do\n550 had expanded. So we are able to do the summarization. So after 396 again it\nthe summarization. So after 396 again it\nthe summarization. So after 396 again it went to 232 that basically means\nwent to 232 that basically means\nwent to 232 that basically means summarization has happened here also.\nsummarization has happened here also.\nsummarization has happened here also. See here is a summary here is a summary\nSee here is a summary here is a summary\nSee here is a summary here is a summary and here also summary right. So the\nand here also summary right. So the\nand here also summary right. So the summary is basically happening over here\nsummary is basically happening over here\nsummary is basically happening over here right and based on this you are\nright and based on this you are\nright and based on this you are basically creating the response. Okay\nbasically creating the response. Okay\nbasically creating the response. Okay including the grand hotels all this\nincluding the grand hotels all this\nincluding the grand hotels all this information. So summarization is\ninformation. So summarization is\ninformation. So summarization is specifically happening once your 550 tok\nspecifically happening once your 550 tok\nspecifically happening once your 550 tok to tokens is getting over. Okay. Now\nto tokens is getting over. Okay. Now\nto tokens is getting over. Okay. Now this is one more way and one more way I\nthis is one more way and one more way I\nthis is one more way and one more way I want to go ahead with uh you know which\nwant to go ahead with uh you know which\nwant to go ahead with uh you know which is basically called as based on\nis basically called as based on\nis basically called as based on fraction. Okay. Now what is based on\nfraction. Okay. Now what is based on\nfraction. Okay. Now what is based on fraction? How based on fraction it is\nfraction? How based on fraction it is\nfraction? How based on fraction it is going to apply. Okay. Here this time\ngoing to apply. Okay. Here this time\ngoing to apply. Okay. Here this time I'll copy and paste some code and you\nI'll copy and paste some code and you\nI'll copy and paste some code and you you can just go ahead and see to it.\nyou can just go ahead and see to it.\nyou can just go ahead and see to it. Okay.\nOkay.\nOkay. So here you can see I have my search\nSo here you can see I have my search\nSo here you can see I have my search totals. This time the trigger will be\ntotals. This time the trigger will be\ntotals. This time the trigger will be based on fraction not on token and\nbased on fraction not on token and\nbased on fraction not on token and fraction I have given 0.005005 005\nfraction I have given 0.005005 005\nfraction I have given 0.005005 005 0005 this is this fraction is based on\n0005 this is this fraction is based on\n0005 this is this fraction is based on the context of the LLM model right so if\nthe context of the LLM model right so if\nthe context of the LLM model right so if the LLM model is able to accommodate\nthe LLM model is able to accommodate\nthe LLM model is able to accommodate 160k tokens right uh if I give the\n160k tokens right uh if I give the\n160k tokens right uh if I give the fraction as 0.5 that basically means 0.5\nfraction as 0.5 that basically means 0.5\nfraction as 0.5 that basically means 0.5 of six of that many number of tokens is\nof six of that many number of tokens is\nof six of that many number of tokens is equal to 640 tokens that is what I've\nequal to 640 tokens that is what I've\nequal to 640 tokens that is what I've given as an example okay we can also\ngiven as an example okay we can also\ngiven as an example okay we can also convert that so it is based on different\nconvert that so it is based on different\nconvert that so it is based on different different LLM context size here we are\ndifferent LLM context size here we are\ndifferent LLM context size here we are going to use fraction okay so fraction\ngoing to use fraction okay so fraction\ngoing to use fraction okay so fraction is 0005 that basically means 0.5% 02\nis 0005 that basically means 0.5% 02\nis 0005 that basically means 0.5% 02 that is nothing but 2%. U and here again\nthat is nothing but 2%. U and here again\nthat is nothing but 2%. U and here again you can see I'm counting the count\nyou can see I'm counting the count\nyou can see I'm counting the count tokens everything is same and here we\ntokens everything is same and here we\ntokens everything is same and here we are using config and here you can also\nare using config and here you can also\nare using config and here you can also go ahead and see the fraction so\ngo ahead and see the fraction so\ngo ahead and see the fraction so whenever the fra this fraction increases\nwhenever the fra this fraction increases\nwhenever the fra this fraction increases 0.5 then we are good to go see 9\n0.5 then we are good to go see 9\n0.5 then we are good to go see 9 here 133 tokens.15\n21\n21 whenever it reaches 0 five okay 5%.\nwhenever it reaches 0 five okay 5%.\nwhenever it reaches 0 five okay 5%. You can see if it does not reaches 0.5\nYou can see if it does not reaches 0.5\nYou can see if it does not reaches 0.5 that basically means summarization. So\nthat basically means summarization. So\nthat basically means summarization. So here it has increased. So here you can\nhere it has increased. So here you can\nhere it has increased. So here you can see summary of the conversation\nsee summary of the conversation\nsee summary of the conversation it has increased from here to and uh\nit has increased from here to and uh\nit has increased from here to and uh what we have done is that here the\nwhat we have done is that here the\nwhat we have done is that here the summary has been created. Right? So that\nsummary has been created. Right? So that\nsummary has been created. Right? So that basically means that percentage of the\nbasically means that percentage of the\nbasically means that percentage of the token has got uh the fraction has got\ntoken has got uh the fraction has got\ntoken has got uh the fraction has got increased right. So this was just about\nincreased right. So this was just about\nincreased right. So this was just about summarization and three types we have\nsummarization and three types we have\nsummarization and three types we have learned. One is based on token size, one\nlearned. One is based on token size, one\nlearned. One is based on token size, one is based on u you know the number of\nis based on u you know the number of\nis based on u you know the number of messages and all right and uh an amazing\nmessages and all right and uh an amazing\nmessages and all right and uh an amazing uh summarization technique and if you go\nuh summarization technique and if you go\nuh summarization technique and if you go ahead and see this is the summarization\nahead and see this is the summarization\nahead and see this is the summarization over here you can see some examples but\nover here you can see some examples but\nover here you can see some examples but I I have probably given you a very good\nI I have probably given you a very good\nI I have probably given you a very good example and there are also other\nexample and there are also other\nexample and there are also other built-in uh middleware now you can use\nbuilt-in uh middleware now you can use\nbuilt-in uh middleware now you can use any of them like tool call limit you\nany of them like tool call limit you\nany of them like tool call limit you know how to apply it so inside the\nknow how to apply it so inside the\nknow how to apply it so inside the middleware you go ahead and apply it\nmiddleware you go ahead and apply it\nmiddleware you go ahead and apply it like this right and uh let's say you\nlike this right and uh let's say you\nlike this right and uh let's say you want to probably go ahead and apply\nwant to probably go ahead and apply\nwant to probably go ahead and apply model fall back right so model fall back\nmodel fall back right so model fall back\nmodel fall back right so model fall back basically means from one model if some\nbasically means from one model if some\nbasically means from one model if some model is not there you can fall back to\nmodel is not there you can fall back to\nmodel is not there you can fall back to the other model right let's say if this\nthe other model right let's say if this\nthe other model right let's say if this API cost or API key is not working then\nAPI cost or API key is not working then\nAPI cost or API key is not working then it will fall back to the other model\nit will fall back to the other model\nit will fall back to the other model right so what I will show you is that in\nright so what I will show you is that in\nright so what I will show you is that in the next uh section I will show you how\nthe next uh section I will show you how\nthe next uh section I will show you how you can also go ahead and apply human in\nyou can also go ahead and apply human in\nyou can also go ahead and apply human in the loop a very good example because\nthe loop a very good example because\nthe loop a very good example because human feedback is always required right\nhuman feedback is always required right\nhuman feedback is always required right whenever a task is basically happening\nwhenever a task is basically happening\nwhenever a task is basically happening in the agent and that is what we are\nin the agent and that is what we are\nin the agent and that is what we are basically going to go ahead and discuss\nbasically going to go ahead and discuss\nbasically going to go ahead and discuss but I hope you got a clear idea about\nbut I hope you got a clear idea about\nbut I hope you got a clear idea about summarization middleware. So now we are\nsummarization middleware. So now we are\nsummarization middleware. So now we are going to continue a discussion with\ngoing to continue a discussion with\ngoing to continue a discussion with respect to middleware and uh we are\nrespect to middleware and uh we are\nrespect to middleware and uh we are going to discuss one more type which is\ngoing to discuss one more type which is\ngoing to discuss one more type which is called as human in the loop. Okay. And\ncalled as human in the loop. Okay. And\ncalled as human in the loop. Okay. And this is a very important uh\nthis is a very important uh\nthis is a very important uh functionality in terms of middleware. So\nfunctionality in terms of middleware. So\nfunctionality in terms of middleware. So here uh what this does is that it pauses\nhere uh what this does is that it pauses\nhere uh what this does is that it pauses agent execution for human approval,\nagent execution for human approval,\nagent execution for human approval, editing or rejection of a tool call\nediting or rejection of a tool call\nediting or rejection of a tool call before they execute. Human in the loop\nbefore they execute. Human in the loop\nbefore they execute. Human in the loop is useful for the following. High stakes\nis useful for the following. High stakes\nis useful for the following. High stakes operation require human approval like\noperation require human approval like\noperation require human approval like database rights, financial transaction,\ndatabase rights, financial transaction,\ndatabase rights, financial transaction, compliance workflows where human\ncompliance workflows where human\ncompliance workflows where human oversight is mandatory. Longunning\noversight is mandatory. Longunning\noversight is mandatory. Longunning conversation where human feedback guides\nconversation where human feedback guides\nconversation where human feedback guides the agent. Okay. Now let me just open my\nthe agent. Okay. Now let me just open my\nthe agent. Okay. Now let me just open my scribble notebook and let me talk more\nscribble notebook and let me talk more\nscribble notebook and let me talk more about it. Let's say that I have a\nabout it. Let's say that I have a\nabout it. Let's say that I have a specific agent and my human in the loop\nspecific agent and my human in the loop\nspecific agent and my human in the loop is actually required. Let's say this\nis actually required. Let's say this\nis actually required. Let's say this agent uh does some kind of task. Okay.\nagent uh does some kind of task. Okay.\nagent uh does some kind of task. Okay. And whenever we talk about agent these\nAnd whenever we talk about agent these\nAnd whenever we talk about agent these are basically autonomous agent\nare basically autonomous agent\nare basically autonomous agent autonomous agent when we say autonomous\nautonomous agent when we say autonomous\nautonomous agent when we say autonomous agent that basically means without much\nagent that basically means without much\nagent that basically means without much human intervention it'll be able to do\nhuman intervention it'll be able to do\nhuman intervention it'll be able to do some specific task let's say this agent\nsome specific task let's say this agent\nsome specific task let's say this agent actually does a work and uh it is a\nactually does a work and uh it is a\nactually does a work and uh it is a critical work let's say with respect to\ncritical work let's say with respect to\ncritical work let's say with respect to financial transaction okay financial\nfinancial transaction okay financial\nfinancial transaction okay financial transaction now when I say financial\ntransaction now when I say financial\ntransaction now when I say financial transaction let's say this agent helps\ntransaction let's say this agent helps\ntransaction let's say this agent helps me to buy stocks\nme to buy stocks\nme to buy stocks Okay.\nOkay.\nOkay. Now let's say\nNow let's say\nNow let's say and see this is definitely a very\nand see this is definitely a very\nand see this is definitely a very critical task. I hope you agree with\ncritical task. I hope you agree with\ncritical task. I hope you agree with this. This is a critical task. We cannot\nthis. This is a critical task. We cannot\nthis. This is a critical task. We cannot just directly uh we cannot uh you know\njust directly uh we cannot uh you know\njust directly uh we cannot uh you know completely be dependent on the agent to\ncompletely be dependent on the agent to\ncompletely be dependent on the agent to do this specific task. Some kind of\ndo this specific task. Some kind of\ndo this specific task. Some kind of human intervention is definitely\nhuman intervention is definitely\nhuman intervention is definitely required. Let's say for the next day the\nrequired. Let's say for the next day the\nrequired. Let's say for the next day the agent is going to probably go ahead and\nagent is going to probably go ahead and\nagent is going to probably go ahead and buy a stock and uh you know\nbuy a stock and uh you know\nbuy a stock and uh you know automatically goes and does some kind of\nautomatically goes and does some kind of\nautomatically goes and does some kind of mistake. So there may be a huge loss of\nmistake. So there may be a huge loss of\nmistake. So there may be a huge loss of finance in this side. So we cannot be\nfinance in this side. So we cannot be\nfinance in this side. So we cannot be completely dependent on the autonomous\ncompletely dependent on the autonomous\ncompletely dependent on the autonomous agent. What we can actually do is that\nagent. What we can actually do is that\nagent. What we can actually do is that we can add a human over here, right? And\nwe can add a human over here, right? And\nwe can add a human over here, right? And we can make sure that whenever an agent\nwe can make sure that whenever an agent\nwe can make sure that whenever an agent takes any decision in this kind of\ntakes any decision in this kind of\ntakes any decision in this kind of critical task, first of all, it will go\ncritical task, first of all, it will go\ncritical task, first of all, it will go ahead and request this human to provide\nahead and request this human to provide\nahead and request this human to provide a confirmation, right? And that is the\na confirmation, right? And that is the\na confirmation, right? And that is the reason we say human in the loop, right?\nreason we say human in the loop, right?\nreason we say human in the loop, right? We always asking feedbacks to the human\nWe always asking feedbacks to the human\nWe always asking feedbacks to the human being because at the end of the day uh\nbeing because at the end of the day uh\nbeing because at the end of the day uh unless until this feedback is not given\nunless until this feedback is not given\nunless until this feedback is not given to the agent this kind of task will not\nto the agent this kind of task will not\nto the agent this kind of task will not get completed right and this is really\nget completed right and this is really\nget completed right and this is really important because for any kind of\nimportant because for any kind of\nimportant because for any kind of critical task we need to have human\ncritical task we need to have human\ncritical task we need to have human intervention\nintervention\nintervention intervention because there may be\nintervention because there may be\nintervention because there may be mistakes that may that agent can make\nmistakes that may that agent can make\nmistakes that may that agent can make that an LLM can specifically make right\nthat an LLM can specifically make right\nthat an LLM can specifically make right so now we are going to understand how we\nso now we are going to understand how we\nso now we are going to understand how we can actually go ahead and implement this\ncan actually go ahead and implement this\ncan actually go ahead and implement this kind of middleware. Okay. So here you\nkind of middleware. Okay. So here you\nkind of middleware. Okay. So here you can see I have I'm I'm actually working\ncan see I have I'm I'm actually working\ncan see I have I'm I'm actually working in the same notebook. Okay. What I will\nin the same notebook. Okay. What I will\nin the same notebook. Okay. What I will do is that I will go ahead and import\ndo is that I will go ahead and import\ndo is that I will go ahead and import some of the libraries. The first library\nsome of the libraries. The first library\nsome of the libraries. The first library is that with respect to create agent.\nis that with respect to create agent.\nis that with respect to create agent. The second library I'm going to import\nThe second library I'm going to import\nThe second library I'm going to import is from langen.agents.m middleware\nis from langen.agents.m middleware\nis from langen.agents.m middleware import human in the loop middleware.\nimport human in the loop middleware.\nimport human in the loop middleware. Before we just using summarization\nBefore we just using summarization\nBefore we just using summarization middleware right then we are using\nmiddleware right then we are using\nmiddleware right then we are using checkpoint dotmemory in memory. Right\ncheckpoint dotmemory in memory. Right\ncheckpoint dotmemory in memory. Right now let's say that I want to do a\nnow let's say that I want to do a\nnow let's say that I want to do a specific task which needs to be done\nspecific task which needs to be done\nspecific task which needs to be done which needs to be intervened by the\nwhich needs to be intervened by the\nwhich needs to be intervened by the human being again and again. Basically\nhuman being again and again. Basically\nhuman being again and again. Basically my agent should go ahead and ask\nmy agent should go ahead and ask\nmy agent should go ahead and ask continuous feedback you know with\ncontinuous feedback you know with\ncontinuous feedback you know with respect to any task that it does right\nrespect to any task that it does right\nrespect to any task that it does right now what I will do I will go ahead and\nnow what I will do I will go ahead and\nnow what I will do I will go ahead and create two important function let's say\ncreate two important function let's say\ncreate two important function let's say one of my agent work is basically to\none of my agent work is basically to\none of my agent work is basically to send emails okay so here you can see\nsend emails okay so here you can see\nsend emails okay so here you can see that I have two different\nthat I have two different\nthat I have two different functionalities one is read email here\nfunctionalities one is read email here\nfunctionalities one is read email here we give the email id email content for\nwe give the email id email content for\nwe give the email id email content for ID this one is there where we are\nID this one is there where we are\nID this one is there where we are reading the email then second is send\nreading the email then second is send\nreading the email then second is send email tool okay So this basically sends\nemail tool okay So this basically sends\nemail tool okay So this basically sends a email right here. I know I've just\na email right here. I know I've just\na email right here. I know I've just written some kind of dummy information\nwritten some kind of dummy information\nwritten some kind of dummy information saying that email send to recipient with\nsaying that email send to recipient with\nsaying that email send to recipient with subject this subject. Okay, this is what\nsubject this subject. Okay, this is what\nsubject this subject. Okay, this is what is my basic thing over here. Again, if\nis my basic thing over here. Again, if\nis my basic thing over here. Again, if you really want to implement a end toend\nyou really want to implement a end toend\nyou really want to implement a end toend email thing, you need to use SMTP server\nemail thing, you need to use SMTP server\nemail thing, you need to use SMTP server and based on that you can actually do\nand based on that you can actually do\nand based on that you can actually do it. But the core idea over here is that\nit. But the core idea over here is that\nit. But the core idea over here is that I just want to show you to do this\nI just want to show you to do this\nI just want to show you to do this particular task, I want my agent to be\nparticular task, I want my agent to be\nparticular task, I want my agent to be always intervened by human beings. Okay.\nalways intervened by human beings. Okay.\nalways intervened by human beings. Okay. So these are the two functionalities\nSo these are the two functionalities\nSo these are the two functionalities that I have like kind of a tool. Now\nthat I have like kind of a tool. Now\nthat I have like kind of a tool. Now what I will do I will go ahead and\nwhat I will do I will go ahead and\nwhat I will do I will go ahead and create my agent. My agent will be\ncreate my agent. My agent will be\ncreate my agent. My agent will be nothing but create agent. Here the first\nnothing but create agent. Here the first\nnothing but create agent. Here the first thing that I'm going to use is model. So\nthing that I'm going to use is model. So\nthing that I'm going to use is model. So model I'll write GBD40.\nmodel I'll write GBD40.\nmodel I'll write GBD40. Okay. The second parameter that I'm\nOkay. The second parameter that I'm\nOkay. The second parameter that I'm actually going to use is tools. Tools\nactually going to use is tools. Tools\nactually going to use is tools. Tools here I'm going to go ahead and provide\nhere I'm going to go ahead and provide\nhere I'm going to go ahead and provide my tool called as read email tool. Send\nmy tool called as read email tool. Send\nmy tool called as read email tool. Send email tool. whatever tools I have\nemail tool. whatever tools I have\nemail tool. whatever tools I have written over here on the top because my\nwritten over here on the top because my\nwritten over here on the top because my agent work is basically to send a uh\nagent work is basically to send a uh\nagent work is basically to send a uh email right then here I'm going to use a\nemail right then here I'm going to use a\nemail right then here I'm going to use a checkpointer this is for my memory so\ncheckpointer this is for my memory so\ncheckpointer this is for my memory so inmemory saver in memory\ninmemory saver in memory\ninmemory saver in memory inmemory saver I'll go ahead and\ninmemory saver I'll go ahead and\ninmemory saver I'll go ahead and initialize this now I'm going to go\ninitialize this now I'm going to go\ninitialize this now I'm going to go ahead and add my middleware okay\nahead and add my middleware okay\nahead and add my middleware okay middleware as I said you can also add\nmiddleware as I said you can also add\nmiddleware as I said you can also add summarization middleware over here but\nsummarization middleware over here but\nsummarization middleware over here but this example I want to So human in the\nthis example I want to So human in the\nthis example I want to So human in the loop middleware and inside this human in\nloop middleware and inside this human in\nloop middleware and inside this human in the loop middleware way I will have\nthe loop middleware way I will have\nthe loop middleware way I will have multiple options. One is interrupt. So I\nmultiple options. One is interrupt. So I\nmultiple options. One is interrupt. So I can go ahead and use interrupt. So I\ncan go ahead and use interrupt. So I\ncan go ahead and use interrupt. So I will uh go ahead and use something\nwill uh go ahead and use something\nwill uh go ahead and use something called as interrupt on. Okay is equal to\ncalled as interrupt on. Okay is equal to\ncalled as interrupt on. Okay is equal to now where I need to interrupt right that\nnow where I need to interrupt right that\nnow where I need to interrupt right that is what we really need to understand\nis what we really need to understand\nis what we really need to understand where we need to interrupt it on what\nwhere we need to interrupt it on what\nwhere we need to interrupt it on what kind of action I want to interrupt it.\nkind of action I want to interrupt it.\nkind of action I want to interrupt it. Now in this particular scenario if my\nNow in this particular scenario if my\nNow in this particular scenario if my agent is sending a mail I really want to\nagent is sending a mail I really want to\nagent is sending a mail I really want to make a confirmation from the human being\nmake a confirmation from the human being\nmake a confirmation from the human being or get an approval before the human\nor get an approval before the human\nor get an approval before the human being before sending the mail right so\nbeing before sending the mail right so\nbeing before sending the mail right so here what I'll do interrupt on I will\nhere what I'll do interrupt on I will\nhere what I'll do interrupt on I will write okay this functionality which is\nwrite okay this functionality which is\nwrite okay this functionality which is called as send email tool so whenever\ncalled as send email tool so whenever\ncalled as send email tool so whenever this functionality or this tool is\nthis functionality or this tool is\nthis functionality or this tool is basically getting called I need to go\nbasically getting called I need to go\nbasically getting called I need to go ahead and ask for the human permission\nahead and ask for the human permission\nahead and ask for the human permission right whether we should allow it or not\nright whether we should allow it or not\nright whether we should allow it or not so here I will say allowed\nso here I will say allowed\nso here I will say allowed decision which you can hardcode it Okay,\ndecision which you can hardcode it Okay,\ndecision which you can hardcode it Okay, decision and here I will say I will have\ndecision and here I will say I will have\ndecision and here I will say I will have three important things. Okay, three\nthree important things. Okay, three\nthree important things. Okay, three important thing. One is approved,\nimportant thing. One is approved,\nimportant thing. One is approved, edit\nedit\nedit or reject. Okay, so I'm saying that\nor reject. Okay, so I'm saying that\nor reject. Okay, so I'm saying that there are three important options that\nthere are three important options that\nthere are three important options that you can basically interrupt on and human\nyou can basically interrupt on and human\nyou can basically interrupt on and human can basically approve it or edit it or\ncan basically approve it or edit it or\ncan basically approve it or edit it or reject it. Okay, either it can approve\nreject it. Okay, either it can approve\nreject it. Okay, either it can approve okay go ahead and send the mail either\nokay go ahead and send the mail either\nokay go ahead and send the mail either it can say no no no don't send the mail\nit can say no no no don't send the mail\nit can say no no no don't send the mail to this email id to some other mail\nto this email id to some other mail\nto this email id to some other mail email id that is reject edit and third\nemail id that is reject edit and third\nemail id that is reject edit and third one is something called as reject okay\none is something called as reject okay\none is something called as reject okay so this on send email tool I definitely\nso this on send email tool I definitely\nso this on send email tool I definitely want um I definitely want a kind of\nwant um I definitely want a kind of\nwant um I definitely want a kind of interrupt right now with respect to read\ninterrupt right now with respect to read\ninterrupt right now with respect to read email tool I don't want anything so what\nemail tool I don't want anything so what\nemail tool I don't want anything so what I will do for this particular tool I\nI will do for this particular tool I\nI will do for this particular tool I will go ahead and say hey go ahead and\nwill go ahead and say hey go ahead and\nwill go ahead and say hey go ahead and make it false.\nmake it false.\nmake it false. So whenever I'm making this particular\nSo whenever I'm making this particular\nSo whenever I'm making this particular tool call for this particular tool call\ntool call for this particular tool call\ntool call for this particular tool call I definitely go need to go ahead and\nI definitely go need to go ahead and\nI definitely go need to go ahead and take an approval from the human being.\ntake an approval from the human being.\ntake an approval from the human being. The human being can provide three\nThe human being can provide three\nThe human being can provide three options. One is approve, edit and\noptions. One is approve, edit and\noptions. One is approve, edit and reject. Okay. So this is done very\nreject. Okay. So this is done very\nreject. Okay. So this is done very clear. So I will go ahead and execute\nclear. So I will go ahead and execute\nclear. So I will go ahead and execute and create my agent. Now once I have my\nand create my agent. Now once I have my\nand create my agent. Now once I have my specific agent over here, now the next\nspecific agent over here, now the next\nspecific agent over here, now the next step is that what I will do? I will just\nstep is that what I will do? I will just\nstep is that what I will do? I will just go ahead and create a config file. See\ngo ahead and create a config file. See\ngo ahead and create a config file. See config over here. I'll say test approve.\nconfig over here. I'll say test approve.\nconfig over here. I'll say test approve. Let's go ahead and do the test approve\nLet's go ahead and do the test approve\nLet's go ahead and do the test approve with this thread ID. Thread ID indicates\nwith this thread ID. Thread ID indicates\nwith this thread ID. Thread ID indicates unique ID. Okay, I'm using\nunique ID. Okay, I'm using\nunique ID. Okay, I'm using message.invoke messages human message\nmessage.invoke messages human message\nmessage.invoke messages human message and I said send email to\nand I said send email to\nand I said send email to john@thetest.com\njohn@thetest.com\njohn@thetest.com with subject hello and body how are you?\nwith subject hello and body how are you?\nwith subject hello and body how are you? Okay, so this is my input that is given\nOkay, so this is my input that is given\nOkay, so this is my input that is given over here. Now once I give this\nover here. Now once I give this\nover here. Now once I give this particular input the agent will know\nparticular input the agent will know\nparticular input the agent will know okay it has two tools. One is read email\nokay it has two tools. One is read email\nokay it has two tools. One is read email tool and one is send email tool. So it\ntool and one is send email tool. So it\ntool and one is send email tool. So it will first of all go ahead and execute\nwill first of all go ahead and execute\nwill first of all go ahead and execute read email tool. Read email tool is\nread email tool. Read email tool is\nread email tool. Read email tool is nothing but it goes and uh read the\nnothing but it goes and uh read the\nnothing but it goes and uh read the email by its id and send email is\nemail by its id and send email is\nemail by its id and send email is nothing but it mock sends mox function\nnothing but it mock sends mox function\nnothing but it mock sends mox function to send an email. Okay.\nto send an email. Okay.\nto send an email. Okay. Now while reading this particular read\nNow while reading this particular read\nNow while reading this particular read email tool it will not do anything but\nemail tool it will not do anything but\nemail tool it will not do anything but once it goes to send email tool it is\nonce it goes to send email tool it is\nonce it goes to send email tool it is going to create an interrupt. Okay. So\ngoing to create an interrupt. Okay. So\ngoing to create an interrupt. Okay. So let's see this. So I'll go ahead and\nlet's see this. So I'll go ahead and\nlet's see this. So I'll go ahead and execute it. And now I will go ahead and\nexecute it. And now I will go ahead and\nexecute it. And now I will go ahead and see my result. See there is something\nsee my result. See there is something\nsee my result. See there is something called as interrupt. Now why interrupt\ncalled as interrupt. Now why interrupt\ncalled as interrupt. Now why interrupt is basically happening over here? It is\nis basically happening over here? It is\nis basically happening over here? It is very much clear because we have created\nvery much clear because we have created\nvery much clear because we have created a trigger over here right in this\na trigger over here right in this\na trigger over here right in this particular middle uh in in this\nparticular middle uh in in this\nparticular middle uh in in this particular middleware wherein wherever\nparticular middleware wherein wherever\nparticular middleware wherein wherever the send email tool is basically\nthe send email tool is basically\nthe send email tool is basically executed we need to go ahead and take a\nexecuted we need to go ahead and take a\nexecuted we need to go ahead and take a permission from the human being. Now\npermission from the human being. Now\npermission from the human being. Now what is basically happening for this\nwhat is basically happening for this\nwhat is basically happening for this send email tool now we need to take a\nsend email tool now we need to take a\nsend email tool now we need to take a approval from the human being. Now for\napproval from the human being. Now for\napproval from the human being. Now for the approval process it is very simple.\nthe approval process it is very simple.\nthe approval process it is very simple. I will go ahead and write this\nI will go ahead and write this\nI will go ahead and write this particular condition. Now see this I\nparticular condition. Now see this I\nparticular condition. Now see this I will write if\nwill write if\nwill write if interrupt is present in result print\ninterrupt is present in result print\ninterrupt is present in result print pause approving then I will say\npause approving then I will say\npause approving then I will say agent.invoke invoke. Now see human needs\nagent.invoke invoke. Now see human needs\nagent.invoke invoke. Now see human needs to see give the confirmation. Okay, go\nto see give the confirmation. Okay, go\nto see give the confirmation. Okay, go ahead and send the mail, right? Then how\nahead and send the mail, right? Then how\nahead and send the mail, right? Then how that execution will basically happen.\nthat execution will basically happen.\nthat execution will basically happen. For that we use this particular uh uh\nFor that we use this particular uh uh\nFor that we use this particular uh uh this particular library which is called\nthis particular library which is called\nthis particular library which is called as command. Okay. Now this command what\nas command. Okay. Now this command what\nas command. Okay. Now this command what it does is that it executes a command.\nit does is that it executes a command.\nit does is that it executes a command. Okay. Now what command it basically\nOkay. Now what command it basically\nOkay. Now what command it basically executes? It executes says that hey\nexecutes? It executes says that hey\nexecutes? It executes says that hey execute the workflow resume the workflow\nexecute the workflow resume the workflow\nexecute the workflow resume the workflow and there the decision type will be\nand there the decision type will be\nand there the decision type will be approve. Now this approve if you\napprove. Now this approve if you\napprove. Now this approve if you remember it matches this right so we are\nremember it matches this right so we are\nremember it matches this right so we are saying approve right so here we are\nsaying approve right so here we are\nsaying approve right so here we are saying approve right and for the same\nsaying approve right and for the same\nsaying approve right and for the same config then we will be able to see that\nconfig then we will be able to see that\nconfig then we will be able to see that the mail will be sent so this is the\nthe mail will be sent so this is the\nthe mail will be sent so this is the code wherein the human is approving\ncode wherein the human is approving\ncode wherein the human is approving right if you instead of approve if you\nright if you instead of approve if you\nright if you instead of approve if you write reject over here it'll get\nwrite reject over here it'll get\nwrite reject over here it'll get rejected right so this is the human\nrejected right so this is the human\nrejected right so this is the human approval that is basically happening so\napproval that is basically happening so\napproval that is basically happening so once I execute this I'm getting an execu\nonce I execute this I'm getting an execu\nonce I execute this I'm getting an execu error saying the command okay command is\nerror saying the command okay command is\nerror saying the command okay command is not there we need to probably go ahead\nnot there we need to probably go ahead\nnot there we need to probably go ahead and uh you know uh import the library\nand uh you know uh import the library\nand uh you know uh import the library which is basically called as command.\nwhich is basically called as command.\nwhich is basically called as command. Okay. Now command libraries uh will be\nOkay. Now command libraries uh will be\nOkay. Now command libraries uh will be available uh let me just open my browser\navailable uh let me just open my browser\navailable uh let me just open my browser and here I will search for langchain\nand here I will search for langchain\nand here I will search for langchain command. Okay so lchain command\ncommand. Okay so lchain command\ncommand. Okay so lchain command let's see there is interrupts.\nlet's see there is interrupts.\nlet's see there is interrupts. So interrupt command command command\nSo interrupt command command command\nSo interrupt command command command rumÃ©. So here you can see from lang\nrumÃ©. So here you can see from lang\nrumÃ©. So here you can see from lang graph.types import command. So I'll go\ngraph.types import command. So I'll go\ngraph.types import command. So I'll go over here. I will\nover here. I will\nover here. I will paste it here itself. Okay. So here you\npaste it here itself. Okay. So here you\npaste it here itself. Okay. So here you can see that I'm basically pasting it\ncan see that I'm basically pasting it\ncan see that I'm basically pasting it over here. I'll execute it. Now this\nover here. I'll execute it. Now this\nover here. I'll execute it. Now this should basically execute it. Now here\nshould basically execute it. Now here\nshould basically execute it. Now here you can see the email has been sent to\nyou can see the email has been sent to\nyou can see the email has been sent to john@therettest.com with subject hello.\njohn@therettest.com with subject hello.\njohn@therettest.com with subject hello. So before my result was this. Now if I\nSo before my result was this. Now if I\nSo before my result was this. Now if I go ahead and see my result, it will\ngo ahead and see my result, it will\ngo ahead and see my result, it will basically have the tool message which is\nbasically have the tool message which is\nbasically have the tool message which is nothing but email sent to this because\nnothing but email sent to this because\nnothing but email sent to this because this is the tool that is basically\nthis is the tool that is basically\nthis is the tool that is basically getting called right the send email\ngetting called right the send email\ngetting called right the send email tool. This tool is basically getting\ntool. This tool is basically getting\ntool. This tool is basically getting called and that has executed wherein it\ncalled and that has executed wherein it\ncalled and that has executed wherein it has said that okay we have sent a email\nhas said that okay we have sent a email\nhas said that okay we have sent a email to this and finally the AI message is\nto this and finally the AI message is\nto this and finally the AI message is saying that the email has been sent to\nsaying that the email has been sent to\nsaying that the email has been sent to John test with subject hello. Okay. Now\nJohn test with subject hello. Okay. Now\nJohn test with subject hello. Okay. Now similarly let's say you want to do it\nsimilarly let's say you want to do it\nsimilarly let's say you want to do it for reject. Okay. So how do I do it for\nfor reject. Okay. So how do I do it for\nfor reject. Okay. So how do I do it for reject? Let's say the human wants to\nreject? Let's say the human wants to\nreject? Let's say the human wants to reject this. Okay. Uh uh it does not\nreject this. Okay. Uh uh it does not\nreject this. Okay. Uh uh it does not want to continue with this, right? So\nwant to continue with this, right? So\nwant to continue with this, right? So for reject again I will use the same\nfor reject again I will use the same\nfor reject again I will use the same code. Let's say this is my agent entire\ncode. Let's say this is my agent entire\ncode. Let's say this is my agent entire thing. Okay. I will execute this. I'll\nthing. Okay. I will execute this. I'll\nthing. Okay. I will execute this. I'll open more code cell. Now I will go ahead\nopen more code cell. Now I will go ahead\nopen more code cell. Now I will go ahead and set my config. Now here we are\nand set my config. Now here we are\nand set my config. Now here we are basically saying that okay fine\nbasically saying that okay fine\nbasically saying that okay fine agent.invoke.\nagent.invoke.\nagent.invoke. Okay. I have to basically close the\nOkay. I have to basically close the\nOkay. I have to basically close the brackets. Okay. Now I'm using this test\nbrackets. Okay. Now I'm using this test\nbrackets. Okay. Now I'm using this test do- reject for this particular thread.\ndo- reject for this particular thread.\ndo- reject for this particular thread. I'm using this unique. And then for\nI'm using this unique. And then for\nI'm using this unique. And then for rejecting I will just go ahead and\nrejecting I will just go ahead and\nrejecting I will just go ahead and update my code. Instead of making that\nupdate my code. Instead of making that\nupdate my code. Instead of making that decision type as approve, I'm going to\ndecision type as approve, I'm going to\ndecision type as approve, I'm going to use this as reject. So this reject and\nuse this as reject. So this reject and\nuse this as reject. So this reject and this reject are matching. Right? And\nthis reject are matching. Right? And\nthis reject are matching. Right? And then I will just go ahead and execute\nthen I will just go ahead and execute\nthen I will just go ahead and execute it. Pause approving. You can see it\nit. Pause approving. You can see it\nit. Pause approving. You can see it seems that there is was an issue with\nseems that there is was an issue with\nseems that there is was an issue with sending an email. Now if you go ahead\nsending an email. Now if you go ahead\nsending an email. Now if you go ahead and see the result, you'll be able to\nand see the result, you'll be able to\nand see the result, you'll be able to see that user rejected the tool call.\nsee that user rejected the tool call.\nsee that user rejected the tool call. Right?\nRight?\nRight? Very simple. Here we are using this\nVery simple. Here we are using this\nVery simple. Here we are using this command. Okay, this command is really\ncommand. Okay, this command is really\ncommand. Okay, this command is really really important. It's just to execute\nreally important. It's just to execute\nreally important. It's just to execute something in the specific workflow.\nsomething in the specific workflow.\nsomething in the specific workflow. Right? And finally, you can also do it\nRight? And finally, you can also do it\nRight? And finally, you can also do it for editing. Right? Let's say that I\nfor editing. Right? Let's say that I\nfor editing. Right? Let's say that I don't want to drop a mail by mistakenly\ndon't want to drop a mail by mistakenly\ndon't want to drop a mail by mistakenly have given some other email id. I want\nhave given some other email id. I want\nhave given some other email id. I want to change the email ID. Right? So\nto change the email ID. Right? So\nto change the email ID. Right? So everything is same over here with\neverything is same over here with\neverything is same over here with respect to creating an agent. I'll go to\nrespect to creating an agent. I'll go to\nrespect to creating an agent. I'll go to the next step. I will go ahead and\nthe next step. I will go ahead and\nthe next step. I will go ahead and create my config. Let's say I go ahead\ncreate my config. Let's say I go ahead\ncreate my config. Let's say I go ahead and send an email to wrongthe\nand send an email to wrongthe\nand send an email to wrongthe ratemail.com with subject text and body\nratemail.com with subject text and body\nratemail.com with subject text and body hello. If I go ahead and execute this, I\nhello. If I go ahead and execute this, I\nhello. If I go ahead and execute this, I will go ahead and show you the result.\nwill go ahead and show you the result.\nwill go ahead and show you the result. It'll be interrupted waiting for the\nIt'll be interrupted waiting for the\nIt'll be interrupted waiting for the human feedback. Now the human can\nhuman feedback. Now the human can\nhuman feedback. Now the human can basically say hey go ahead and execute\nbasically say hey go ahead and execute\nbasically say hey go ahead and execute the type edit. So here you can see if\nthe type edit. So here you can see if\nthe type edit. So here you can see if interrupt in result agent.invoke Invoke\ninterrupt in result agent.invoke Invoke\ninterrupt in result agent.invoke Invoke command resume is equal to decision type\ncommand resume is equal to decision type\ncommand resume is equal to decision type edit and edited action we have said that\nedit and edited action we have said that\nedit and edited action we have said that okay name send email to we are changing\nokay name send email to we are changing\nokay name send email to we are changing the argument recipient subject and body\nthe argument recipient subject and body\nthe argument recipient subject and body okay so this was edited by human before\nokay so this was edited by human before\nokay so this was edited by human before sending and I'm giving the same config\nsending and I'm giving the same config\nsending and I'm giving the same config if I go ahead and execute this\nif I go ahead and execute this\nif I go ahead and execute this you should be able to see what is the\nyou should be able to see what is the\nyou should be able to see what is the output that will be the email has been\noutput that will be the email has been\noutput that will be the email has been sent successfully now if you go ahead\nsent successfully now if you go ahead\nsent successfully now if you go ahead and see the result you'll be able to see\nand see the result you'll be able to see\nand see the result you'll be able to see that the email send to correct at the\nthat the email send to correct at the\nthat the email send to correct at the rategmail.\nrategmail.\nrategmail. Right? So here we have edited right. So\nRight? So here we have edited right. So\nRight? So here we have edited right. So for edit you have something called as\nfor edit you have something called as\nfor edit you have something called as edit action.\nedit action.\nedit action. So this is basically with respect to the\nSo this is basically with respect to the\nSo this is basically with respect to the human in the uh loop uh middleware which\nhuman in the uh loop uh middleware which\nhuman in the uh loop uh middleware which you can go ahead and try it and do it\nyou can go ahead and try it and do it\nyou can go ahead and try it and do it from your side based on your\nfrom your side based on your\nfrom your side based on your requirement. Okay. Now the next thing is\nrequirement. Okay. Now the next thing is\nrequirement. Okay. Now the next thing is that you can still go ahead and explore\nthat you can still go ahead and explore\nthat you can still go ahead and explore all the other built-in built-in\nall the other built-in built-in\nall the other built-in built-in middleares like model call limit. Let's\nmiddleares like model call limit. Let's\nmiddleares like model call limit. Let's say you want to have the limit the\nsay you want to have the limit the\nsay you want to have the limit the number of model calls to prevent\nnumber of model calls to prevent\nnumber of model calls to prevent infinite loops. You can go ahead and use\ninfinite loops. You can go ahead and use\ninfinite loops. You can go ahead and use this thread limit run limit. You can go\nthis thread limit run limit. You can go\nthis thread limit run limit. You can go ahead and see what are the configuration\nahead and see what are the configuration\nahead and see what are the configuration options. So this entire page you can go\noptions. So this entire page you can go\noptions. So this entire page you can go ahead and explore it by yourself and you\nahead and explore it by yourself and you\nahead and explore it by yourself and you can do multiple things. You can do LM\ncan do multiple things. You can do LM\ncan do multiple things. You can do LM tool selector option is also there\ntool selector option is also there\ntool selector option is also there right. So here you can see tool selector\nright. So here you can see tool selector\nright. So here you can see tool selector middle middleware you can see agent with\nmiddle middleware you can see agent with\nmiddle middleware you can see agent with tools where most aren't relevant per\ntools where most aren't relevant per\ntools where most aren't relevant per query reducing token usage by filtering.\nquery reducing token usage by filtering.\nquery reducing token usage by filtering. So for different different task you\nSo for different different task you\nSo for different different task you definitely have these amazing middleares\ndefinitely have these amazing middleares\ndefinitely have these amazing middleares okay which you can actually use. So I\nokay which you can actually use. So I\nokay which you can actually use. So I hope you have understood about\nhope you have understood about\nhope you have understood about middleares."
  },
  {
    "id": 60494623,
    "timestamp": "2026-02-25T08:15:30.878Z",
    "title": "Langchain Tutorial For Beginners (2026 Guide) | AI Agents For Data Engineers",
    "url": "https://www.youtube.com/watch?v=AOQyRiwydyo",
    "text": "I am in the data domain. Why do I need\nI am in the data domain. Why do I need to learn lang chain in? So in 206, every\nto learn lang chain in? So in 206, every\nto learn lang chain in? So in 206, every other organization is looking for the\nother organization is looking for the\nother organization is looking for the data engineers who know lang chain\ndata engineers who know lang chain\ndata engineers who know lang chain because they want you to build their AI\nbecause they want you to build their AI\nbecause they want you to build their AI orchestration pipelines as well. So I\norchestration pipelines as well. So I\norchestration pipelines as well. So I know now you will say hey I also want to\nknow now you will say hey I also want to\nknow now you will say hey I also want to learn lang chain but I do not have any\nlearn lang chain but I do not have any\nlearn lang chain but I do not have any kind of resources any structure any\nkind of resources any structure any\nkind of resources any structure any guide nothing don't worry that's why I\nguide nothing don't worry that's why I\nguide nothing don't worry that's why I have created this complete 5 hours long\nhave created this complete 5 hours long\nhave created this complete 5 hours long beginnerfriendly lang chain full course\nbeginnerfriendly lang chain full course\nbeginnerfriendly lang chain full course which will literally hold your hand and\nwhich will literally hold your hand and\nwhich will literally hold your hand and will make you a pro lchain developer by\nwill make you a pro lchain developer by\nwill make you a pro lchain developer by the end of this video. So first you will\nthe end of this video. So first you will\nthe end of this video. So first you will learn the fundamentals of lang chain and\nlearn the fundamentals of lang chain and\nlearn the fundamentals of lang chain and as you follow along you will learn much\nas you follow along you will learn much\nas you follow along you will learn much more complex areas such as parallel\nmore complex areas such as parallel\nmore complex areas such as parallel conditional branches penting integration\nconditional branches penting integration\nconditional branches penting integration tool calling react agents and much more.\ntool calling react agents and much more.\ntool calling react agents and much more. Not just this by the end of this video\nNot just this by the end of this video\nNot just this by the end of this video you will build your own SQL agent which\nyou will build your own SQL agent which\nyou will build your own SQL agent which will take the user input in the natural\nwill take the user input in the natural\nwill take the user input in the natural language and will automatically fetch\nlanguage and will automatically fetch\nlanguage and will automatically fetch the necessary data from the SQL database\nthe necessary data from the SQL database\nthe necessary data from the SQL database and will return the insights back to the\nand will return the insights back to the\nand will return the insights back to the user. Wow. So on what are the prerequest\nuser. Wow. So on what are the prerequest\nuser. Wow. So on what are the prerequest required for this video? Nothing just\nrequired for this video? Nothing just\nrequired for this video? Nothing just basic Python is more than enough. The\nbasic Python is more than enough. The\nbasic Python is more than enough. The best part everything is explained\nbest part everything is explained\nbest part everything is explained visually with cool graphics so that you\nvisually with cool graphics so that you\nvisually with cool graphics so that you can even understand the complex areas\ncan even understand the complex areas\ncan even understand the complex areas easily. Plus this video is designed in\neasily. Plus this video is designed in\neasily. Plus this video is designed in the form of chapters so that you can\nthe form of chapters so that you can\nthe form of chapters so that you can just follow along in a structured guide\njust follow along in a structured guide\njust follow along in a structured guide and all the notes and code examples are\nand all the notes and code examples are\nand all the notes and code examples are also provided with this video. In short,\nalso provided with this video. In short,\nalso provided with this video. In short, this is the one-stop solution for you to\nthis is the one-stop solution for you to\nthis is the one-stop solution for you to master lang chain in 2026. So bro, just\nmaster lang chain in 2026. So bro, just\nmaster lang chain in 2026. So bro, just take out your laptop, make your coffee\ntake out your laptop, make your coffee\ntake out your laptop, make your coffee and just hit the subscribe button right\nand just hit the subscribe button right\nand just hit the subscribe button right now and let's master lang chain in 26\nnow and let's master lang chain in 26\nnow and let's master lang chain in 26 and let's crack the dream offer, dream\nand let's crack the dream offer, dream\nand let's crack the dream offer, dream job this year. I want to help you. Let's\njob this year. I want to help you. Let's\njob this year. I want to help you. Let's go. So what's up? What's up? What's up\ngo. So what's up? What's up? What's up\ngo. So what's up? What's up? What's up my fam? Happy Sunday and this is this\nmy fam? Happy Sunday and this is this\nmy fam? Happy Sunday and this is this this this is the video that I wish I had\nthis this is the video that I wish I had\nthis this is the video that I wish I had when I was learning about these things.\nwhen I was learning about these things.\nwhen I was learning about these things. So without wasting any time, let me just\nSo without wasting any time, let me just\nSo without wasting any time, let me just tell you what is this video about.\ntell you what is this video about.\ntell you what is this video about. Obviously you have watched the\nObviously you have watched the\nObviously you have watched the thumbnail. Obviously you have watched\nthumbnail. Obviously you have watched\nthumbnail. Obviously you have watched the introduction. So this video is all\nthe introduction. So this video is all\nthe introduction. So this video is all about lang chain and if you are still\nabout lang chain and if you are still\nabout lang chain and if you are still thinking to learn about lang chain and\nthinking to learn about lang chain and\nthinking to learn about lang chain and do you know what this is my personal\ndo you know what this is my personal\ndo you know what this is my personal analysis if you are someone in the data\nanalysis if you are someone in the data\nanalysis if you are someone in the data domain or who is trying to enter into\ndomain or who is trying to enter into\ndomain or who is trying to enter into this domain let me just tell you one\nthis domain let me just tell you one\nthis domain let me just tell you one thing lang chain is that thing which is\nthing lang chain is that thing which is\nthing lang chain is that thing which is becoming more and more popular and which\nbecoming more and more popular and which\nbecoming more and more popular and which is in demand. So let's say you are\nis in demand. So let's say you are\nis in demand. So let's say you are applying for a position let's say data\napplying for a position let's say data\napplying for a position let's say data engineering position or any position\nengineering position or any position\nengineering position or any position like in the data domain. Okay. Now there\nlike in the data domain. Okay. Now there\nlike in the data domain. Okay. Now there are obviously hundreds or even thousands\nare obviously hundreds or even thousands\nare obviously hundreds or even thousands of applicants applying for the same\nof applicants applying for the same\nof applicants applying for the same position. But if you add these kinds of\nposition. But if you add these kinds of\nposition. But if you add these kinds of frameworks especially lang chain because\nframeworks especially lang chain because\nframeworks especially lang chain because lang chain is one of the most mature\nlang chain is one of the most mature\nlang chain is one of the most mature agentic frameworks right now. Even if\nagentic frameworks right now. Even if\nagentic frameworks right now. Even if the first of all I know that lang chain\nthe first of all I know that lang chain\nthe first of all I know that lang chain is written the in the job descriptions\nis written the in the job descriptions\nis written the in the job descriptions everywhere. Even if langchain is not\neverywhere. Even if langchain is not\neverywhere. Even if langchain is not written in the job description if\nwritten in the job description if\nwritten in the job description if someone is reading your resume and if\nsomeone is reading your resume and if\nsomeone is reading your resume and if you write that you have experience with\nyou write that you have experience with\nyou write that you have experience with lang chain that will add a cherry on the\nlang chain that will add a cherry on the\nlang chain that will add a cherry on the top of the cake. Make sense? So that\ntop of the cake. Make sense? So that\ntop of the cake. Make sense? So that means lang chain is a necessary stuff\nmeans lang chain is a necessary stuff\nmeans lang chain is a necessary stuff that you have to have to learn. And let\nthat you have to have to learn. And let\nthat you have to have to learn. And let me just tell you one thing which is a\nme just tell you one thing which is a\nme just tell you one thing which is a good news for you. Lang is not just for\ngood news for you. Lang is not just for\ngood news for you. Lang is not just for like data scientist or machine learning\nlike data scientist or machine learning\nlike data scientist or machine learning engineers. It has nothing to do with all\nengineers. It has nothing to do with all\nengineers. It has nothing to do with all those domains. And I know why you didn't\nthose domains. And I know why you didn't\nthose domains. And I know why you didn't learn this technology because you were\nlearn this technology because you were\nlearn this technology because you were like thinking the same. You were still\nlike thinking the same. You were still\nlike thinking the same. You were still thinking like okay this technology is\nthinking like okay this technology is\nthinking like okay this technology is all about machine learning. This\nall about machine learning. This\nall about machine learning. This technology is all about AI and all. No\ntechnology is all about AI and all. No\ntechnology is all about AI and all. No this thing is for data engineers. And\nthis thing is for data engineers. And\nthis thing is for data engineers. And let me just tell you one more thing.\nlet me just tell you one more thing.\nlet me just tell you one more thing. Lang chain is very very very similar for\nLang chain is very very very similar for\nLang chain is very very very similar for or you can say it will look very similar\nor you can say it will look very similar\nor you can say it will look very similar to ETLs or those pipelines that data\nto ETLs or those pipelines that data\nto ETLs or those pipelines that data engineers build. And yes, this video is\nengineers build. And yes, this video is\nengineers build. And yes, this video is for everyone. But yes, if you are a data\nfor everyone. But yes, if you are a data\nfor everyone. But yes, if you are a data engineer, you should feel happy that you\nengineer, you should feel happy that you\nengineer, you should feel happy that you are learning lang because it is very\nare learning lang because it is very\nare learning lang because it is very easy for you to learn this. It is very\neasy for you to learn this. It is very\neasy for you to learn this. It is very very very very similar to all of those\nvery very very similar to all of those\nvery very very similar to all of those tasks that you do on a daily basis.\ntasks that you do on a daily basis.\ntasks that you do on a daily basis. Simple. Okay. Makes sense. So now what\nSimple. Okay. Makes sense. So now what\nSimple. Okay. Makes sense. So now what is required for this video? what we\nis required for this video? what we\nis required for this video? what we going to learn in this video. I know you\ngoing to learn in this video. I know you\ngoing to learn in this video. I know you have all of these questions so that you\nhave all of these questions so that you\nhave all of these questions so that you can decide if you want to continue with\ncan decide if you want to continue with\ncan decide if you want to continue with this video because I know you are very\nthis video because I know you are very\nthis video because I know you are very important person and you you your time\nimportant person and you you your time\nimportant person and you you your time is like very important. I know that. But\nis like very important. I know that. But\nis like very important. I know that. But yes, let me just answer all of those\nyes, let me just answer all of those\nyes, let me just answer all of those questions. So first of all, in order to\nquestions. So first of all, in order to\nquestions. So first of all, in order to continue with this video, you should\ncontinue with this video, you should\ncontinue with this video, you should have basic Python understanding. Basic\nhave basic Python understanding. Basic\nhave basic Python understanding. Basic Python means you should know how to\nPython means you should know how to\nPython means you should know how to write functions in Python especially\nwrite functions in Python especially\nwrite functions in Python especially some basic for loops and if conditions\nsome basic for loops and if conditions\nsome basic for loops and if conditions and I would say some basic level\nand I would say some basic level\nand I would say some basic level understanding of oops such as classes\nunderstanding of oops such as classes\nunderstanding of oops such as classes and objects and that's it. If you have\nand objects and that's it. If you have\nand objects and that's it. If you have this understanding that means you are\nthis understanding that means you are\nthis understanding that means you are good to go and now what we will be\ngood to go and now what we will be\ngood to go and now what we will be covering in this particular master class\ncovering in this particular master class\ncovering in this particular master class and how the you can say agenda will look\nand how the you can say agenda will look\nand how the you can say agenda will look like. So let me just show you that as\nlike. So let me just show you that as\nlike. So let me just show you that as well. So agenda of this particular video\nwell. So agenda of this particular video\nwell. So agenda of this particular video is very very simple. I will expect you\nis very very simple. I will expect you\nis very very simple. I will expect you have zero knowledge. That means you are\nhave zero knowledge. That means you are\nhave zero knowledge. That means you are totally a beginner. Literally zero\ntotally a beginner. Literally zero\ntotally a beginner. Literally zero knowledge. Zero knowledge means zero\nknowledge. Zero knowledge means zero\nknowledge. Zero knowledge means zero knowledge in the AI world as well. Okay,\nknowledge in the AI world as well. Okay,\nknowledge in the AI world as well. Okay, forget about these things like\nforget about these things like\nforget about these things like technologies and all. Zero knowledge in\ntechnologies and all. Zero knowledge in\ntechnologies and all. Zero knowledge in the AI world as well. So you are totally\nthe AI world as well. So you are totally\nthe AI world as well. So you are totally a beginner who just knows basics of\na beginner who just knows basics of\na beginner who just knows basics of Python and that's it. This video has\nPython and that's it. This video has\nPython and that's it. This video has dedicated chapters. This video has\ndedicated chapters. This video has\ndedicated chapters. This video has dedicated notebooks. This video has\ndedicated notebooks. This video has\ndedicated notebooks. This video has dedicated sessions that will teach you\ndedicated sessions that will teach you\ndedicated sessions that will teach you everything from scratch. And we're going\neverything from scratch. And we're going\neverything from scratch. And we're going to literally cover from scratch. Let me\nto literally cover from scratch. Let me\nto literally cover from scratch. Let me just show you my notes that I have\njust show you my notes that I have\njust show you my notes that I have prepared for this particular video. And\nprepared for this particular video. And\nprepared for this particular video. And by the way, everything is available in\nby the way, everything is available in\nby the way, everything is available in my GitHub repo. So let me just show you\nmy GitHub repo. So let me just show you\nmy GitHub repo. So let me just show you that as well. So this is the GitHub repo\nthat as well. So this is the GitHub repo\nthat as well. So this is the GitHub repo for this video. Okay. And as you can see\nfor this video. Okay. And as you can see\nfor this video. Okay. And as you can see there are like major three chapters. But\nthere are like major three chapters. But\nthere are like major three chapters. But in each chapter we have so many\nin each chapter we have so many\nin each chapter we have so many notebooks. If you just open that, you\nnotebooks. If you just open that, you\nnotebooks. If you just open that, you will see like 1 2 3 and then chapter two\nwill see like 1 2 3 and then chapter two\nwill see like 1 2 3 and then chapter two we have these things. And then chapter\nwe have these things. And then chapter\nwe have these things. And then chapter three, we have these things. And yes,\nthree, we have these things. And yes,\nthree, we have these things. And yes, we'll be just building so much of stuff\nwe'll be just building so much of stuff\nwe'll be just building so much of stuff here. And yes, as I just mentioned the\nhere. And yes, as I just mentioned the\nhere. And yes, as I just mentioned the introduction as well, you'll be building\nintroduction as well, you'll be building\nintroduction as well, you'll be building an autonomous SQL agent as well on your\nan autonomous SQL agent as well on your\nan autonomous SQL agent as well on your own from scratch. And\nown from scratch. And\nown from scratch. And regarding nodes, if you just click here,\nregarding nodes, if you just click here,\nregarding nodes, if you just click here, I have just put my live illustration\nI have just put my live illustration\nI have just put my live illustration notes that I have just used here. And\nnotes that I have just used here. And\nnotes that I have just used here. And everything will be illustrated and\neverything will be illustrated and\neverything will be illustrated and demonstrated live. But these are the\ndemonstrated live. But these are the\ndemonstrated live. But these are the notes that you can just refer and you\nnotes that you can just refer and you\nnotes that you can just refer and you will literally see everything like\nwill literally see everything like\nwill literally see everything like everything everything is written in\neverything everything is written in\neverything everything is written in detail. So all the notes are provided\ndetail. So all the notes are provided\ndetail. So all the notes are provided here and we're going to just cover\nhere and we're going to just cover\nhere and we're going to just cover everything in detail. Make sense? So\neverything in detail. Make sense? So\neverything in detail. Make sense? So everything is there for you. You just\neverything is there for you. You just\neverything is there for you. You just need to actually sit back and relax and\nneed to actually sit back and relax and\nneed to actually sit back and relax and just en you should enjoy this video. And\njust en you should enjoy this video. And\njust en you should enjoy this video. And as I just said everything will be\nas I just said everything will be\nas I just said everything will be covered from scratch. So no prerequisy\ncovered from scratch. So no prerequisy\ncovered from scratch. So no prerequisy and agenda chapters as you can see that\nand agenda chapters as you can see that\nand agenda chapters as you can see that we are just covering every single thing\nwe are just covering every single thing\nwe are just covering every single thing which is connected to lang chain. So by\nwhich is connected to lang chain. So by\nwhich is connected to lang chain. So by the end of this video you will become a\nthe end of this video you will become a\nthe end of this video you will become a langchain developer. That means you will\nlangchain developer. That means you will\nlangchain developer. That means you will know like how to write code in lang. You\nknow like how to write code in lang. You\nknow like how to write code in lang. You will know how to build agent with lang.\nwill know how to build agent with lang.\nwill know how to build agent with lang. You will know how to connect dots with\nYou will know how to connect dots with\nYou will know how to connect dots with lang. You will be a confident langen\nlang. You will be a confident langen\nlang. You will be a confident langen developer and then you will be able to\ndeveloper and then you will be able to\ndeveloper and then you will be able to build your projects on your own if you\nbuild your projects on your own if you\nbuild your projects on your own if you want to. And if you are um if let's say\nwant to. And if you are um if let's say\nwant to. And if you are um if let's say your organization is asking you to work\nyour organization is asking you to work\nyour organization is asking you to work on Langjen, you'll be easily able to\non Langjen, you'll be easily able to\non Langjen, you'll be easily able to work on lang. I'm so sure about that.\nwork on lang. I'm so sure about that.\nwork on lang. I'm so sure about that. Simple. I will also tell you some about\nSimple. I will also tell you some about\nSimple. I will also tell you some about about some resources that you can just\nabout some resources that you can just\nabout some resources that you can just use along with this video and everything\nuse along with this video and everything\nuse along with this video and everything will be there in the video as well.\nwill be there in the video as well.\nwill be there in the video as well. Perfect. So I think now you have\nPerfect. So I think now you have\nPerfect. So I think now you have everything clear. Now you have the\neverything clear. Now you have the\neverything clear. Now you have the decision and I know your decision. So\ndecision and I know your decision. So\ndecision and I know your decision. So let me just quick like let me just\nlet me just quick like let me just\nlet me just quick like let me just quickly start this video and I want to\nquickly start this video and I want to\nquickly start this video and I want to start this video like today's session\nstart this video like today's session\nstart this video like today's session especially with\nespecially with\nespecially with this positive positive positive vibe. So\nthis positive positive positive vibe. So\nthis positive positive positive vibe. So what are what is this what is this? So\nwhat are what is this what is this? So\nwhat are what is this what is this? So basically these are my lida family\nbasically these are my lida family\nbasically these are my lida family members who have recently cracked their\nmembers who have recently cracked their\nmembers who have recently cracked their job offers\njob offers\njob offers and it can be their dream job role,\nand it can be their dream job role,\nand it can be their dream job role, dream job offer, dream job company\ndream job offer, dream job company\ndream job offer, dream job company whatever but they are literally happy\nwhatever but they are literally happy\nwhatever but they are literally happy and I am also happy because they are\nand I am also happy because they are\nand I am also happy because they are happy and this literally makes me feel\nhappy and this literally makes me feel\nhappy and this literally makes me feel proud. Okay and congratulations to all\nproud. Okay and congratulations to all\nproud. Okay and congratulations to all of you and congratulations to all of\nof you and congratulations to all of\nof you and congratulations to all of you. Congratulations to all of you.\nyou. Congratulations to all of you.\nyou. Congratulations to all of you. Congratulations to all of you.\nCongratulations to all of you.\nCongratulations to all of you. Congratulations to all of you.\nCongratulations to all of you.\nCongratulations to all of you. Congratulations to all of you.\nCongratulations to all of you.\nCongratulations to all of you. Congratulations to all of you.\nCongratulations to all of you.\nCongratulations to all of you. Congratulations to all of you.\nCongratulations to all of you.\nCongratulations to all of you. Congratulations to all of you.\nCongratulations to all of you.\nCongratulations to all of you. Congratulations to all of you. So, I'm\nCongratulations to all of you. So, I'm\nCongratulations to all of you. So, I'm lit little literally happy. So, this is\nlit little literally happy. So, this is\nlit little literally happy. So, this is the favorite part like one of the\nthe favorite part like one of the\nthe favorite part like one of the favorite parts of my video when I just\nfavorite parts of my video when I just\nfavorite parts of my video when I just say congratulations because see why are\nsay congratulations because see why are\nsay congratulations because see why are you learning? Why are you learning?\nyou learning? Why are you learning?\nyou learning? Why are you learning? There's a goal, right? So, when you\nThere's a goal, right? So, when you\nThere's a goal, right? So, when you achieve it, you feel happy. So, they\nachieve it, you feel happy. So, they\nachieve it, you feel happy. So, they have achieved their goals and they are\nhave achieved their goals and they are\nhave achieved their goals and they are happy and when they are happy, I'm also\nhappy and when they are happy, I'm also\nhappy and when they are happy, I'm also happy.\nhappy.\nhappy. So you should also feel happy because\nSo you should also feel happy because\nSo you should also feel happy because you will also achieve your goals. Don't\nyou will also achieve your goals. Don't\nyou will also achieve your goals. Don't need to worry at all because you are\nneed to worry at all because you are\nneed to worry at all because you are landed on this right channel. And if you\nlanded on this right channel. And if you\nlanded on this right channel. And if you haven't subscribed this channel yet, I\nhaven't subscribed this channel yet, I\nhaven't subscribed this channel yet, I don't know what are you thinking. Uh\ndon't know what are you thinking. Uh\ndon't know what are you thinking. Uh because maybe you do not want to come\nbecause maybe you do not want to come\nbecause maybe you do not want to come into this list. If you want to just hit\ninto this list. If you want to just hit\ninto this list. If you want to just hit the subscribe button right now because\nthe subscribe button right now because\nthe subscribe button right now because I'm literally putting the content which\nI'm literally putting the content which\nI'm literally putting the content which is made for your success. Make sense?\nis made for your success. Make sense?\nis made for your success. Make sense? Okay. And let me just quickly show you\nOkay. And let me just quickly show you\nOkay. And let me just quickly show you my handles as well. So these are my\nmy handles as well. So these are my\nmy handles as well. So these are my handles. If you just want to follow me,\nhandles. If you just want to follow me,\nhandles. If you just want to follow me, if you just want to check my content,\nif you just want to check my content,\nif you just want to check my content, you can just go here and you can just\nyou can just go here and you can just\nyou can just go here and you can just learn more and more stuff as well. Okay,\nlearn more and more stuff as well. Okay,\nlearn more and more stuff as well. Okay, so let's quickly get started with this\nso let's quickly get started with this\nso let's quickly get started with this video because I'm literally excited for\nvideo because I'm literally excited for\nvideo because I'm literally excited for this video lang because I love lang. Let\nthis video lang because I love lang. Let\nthis video lang because I love lang. Let me just show you everything and let's\nme just show you everything and let's\nme just show you everything and let's start learning. So let's start this\nstart learning. So let's start this\nstart learning. So let's start this master class with our cool heading\nmaster class with our cool heading\nmaster class with our cool heading heading heading heading aentic AI and\nheading heading heading aentic AI and\nheading heading heading aentic AI and frameworks basically because agenticare\nframeworks basically because agenticare\nframeworks basically because agenticare are incomplete without the frameworks\nare incomplete without the frameworks\nare incomplete without the frameworks and don't worry we're going to just\nand don't worry we're going to just\nand don't worry we're going to just understand everything from scratch and\nunderstand everything from scratch and\nunderstand everything from scratch and when I say when I when I say scratch\nwhen I say when I when I say scratch\nwhen I say when I when I say scratch that means scratch okay so agentic AI\nthat means scratch okay so agentic AI\nthat means scratch okay so agentic AI and frameworks okay makes sense so let's\nand frameworks okay makes sense so let's\nand frameworks okay makes sense so let's say we want to talk about this this\nsay we want to talk about this this\nsay we want to talk about this this topic agentic AI in frameworks. First of\ntopic agentic AI in frameworks. First of\ntopic agentic AI in frameworks. First of all, I specifically mentioned that\nall, I specifically mentioned that\nall, I specifically mentioned that agentic AI for data engineers, right?\nagentic AI for data engineers, right?\nagentic AI for data engineers, right? Why did I do that? Do you know what the\nWhy did I do that? Do you know what the\nWhy did I do that? Do you know what the best professional\nbest professional\nbest professional the best professional out among you can\nthe best professional out among you can\nthe best professional out among you can say all of the professionals? I think\nsay all of the professionals? I think\nsay all of the professionals? I think data engineer is a person who already\ndata engineer is a person who already\ndata engineer is a person who already knows much about agent. You will say no.\nknows much about agent. You will say no.\nknows much about agent. You will say no. I will say yes because agenti is just\nI will say yes because agenti is just\nI will say yes because agenti is just the new name for everything that we are\nthe new name for everything that we are\nthe new name for everything that we are doing right now and data engineers are\ndoing right now and data engineers are\ndoing right now and data engineers are already wellvered about that. So if and\nalready wellvered about that. So if and\nalready wellvered about that. So if and and do you know what do you know what do\nand do you know what do you know what do\nand do you know what do you know what do you know the funny thing data engineers\nyou know the funny thing data engineers\nyou know the funny thing data engineers are the ones who are entering into the\nare the ones who are entering into the\nare the ones who are entering into the AI world after all of those\nAI world after all of those\nAI world after all of those professionals that is the I would say a\nprofessionals that is the I would say a\nprofessionals that is the I would say a kind of pain point. Data engineers\nkind of pain point. Data engineers\nkind of pain point. Data engineers should be the first ones to enter into\nshould be the first ones to enter into\nshould be the first ones to enter into this particular domain and they are the\nthis particular domain and they are the\nthis particular domain and they are the last ones. I don't know why because they\nlast ones. I don't know why because they\nlast ones. I don't know why because they think like data engineers do not need to\nthink like data engineers do not need to\nthink like data engineers do not need to learn AI and it is something else. It is\nlearn AI and it is something else. It is\nlearn AI and it is something else. It is something out of the topic. It is for\nsomething out of the topic. It is for\nsomething out of the topic. It is for data science. I don't know but that is\ndata science. I don't know but that is\ndata science. I don't know but that is why this channel is for you. This\nwhy this channel is for you. This\nwhy this channel is for you. This channel is just actually teaching you\nchannel is just actually teaching you\nchannel is just actually teaching you and providing you the knowledge which is\nand providing you the knowledge which is\nand providing you the knowledge which is required to be in that particular group\nrequired to be in that particular group\nrequired to be in that particular group which is always in demand. Right? That\nwhich is always in demand. Right? That\nwhich is always in demand. Right? That is why by the way if you are not aware\nis why by the way if you are not aware\nis why by the way if you are not aware about this thing if you just go on any\nabout this thing if you just go on any\nabout this thing if you just go on any job portal if you just type AI data\njob portal if you just type AI data\njob portal if you just type AI data engineers you will see that particular\nengineers you will see that particular\nengineers you will see that particular job postings as well because currently\njob postings as well because currently\njob postings as well because currently industry has also accepted the fact that\nindustry has also accepted the fact that\nindustry has also accepted the fact that AI engineering is is incomplete without\nAI engineering is is incomplete without\nAI engineering is is incomplete without data engineering. So they have just\ndata engineering. So they have just\ndata engineering. So they have just started putting out those labels called\nstarted putting out those labels called\nstarted putting out those labels called AI data engineers. So I think it is the\nAI data engineers. So I think it is the\nAI data engineers. So I think it is the high time and the best time to learn I\nhigh time and the best time to learn I\nhigh time and the best time to learn I wouldn't say best time it is the high\nwouldn't say best time it is the high\nwouldn't say best time it is the high time when when you will learn about\ntime when when you will learn about\ntime when when you will learn about agent AI and all. So let's start this\nagent AI and all. So let's start this\nagent AI and all. So let's start this particular conversation. So I told you\nparticular conversation. So I told you\nparticular conversation. So I told you that I do not expect any kind of\nthat I do not expect any kind of\nthat I do not expect any kind of knowledge from you. So let's say you are\nknowledge from you. So let's say you are\nknowledge from you. So let's say you are this person who do not have any kind of\nthis person who do not have any kind of\nthis person who do not have any kind of knowledge about anything. Okay. Let's\nknowledge about anything. Okay. Let's\nknowledge about anything. Okay. Let's say this is you and your name is what is\nsay this is you and your name is what is\nsay this is you and your name is what is your name? Let's say Rahul. Do you know\nyour name? Let's say Rahul. Do you know\nyour name? Let's say Rahul. Do you know what I just picked the random names and\nwhat I just picked the random names and\nwhat I just picked the random names and there was a real Rahul. He I think\nthere was a real Rahul. He I think\nthere was a real Rahul. He I think dropped a comment. Hey I feel so happy\ndropped a comment. Hey I feel so happy\ndropped a comment. Hey I feel so happy whenever you take my name. So shout out\nwhenever you take my name. So shout out\nwhenever you take my name. So shout out to Rahul whoever the person is. Okay. So\nto Rahul whoever the person is. Okay. So\nto Rahul whoever the person is. Okay. So let's say he is Rahul. Okay. And this\nlet's say he is Rahul. Okay. And this\nlet's say he is Rahul. Okay. And this person doesn't even know anything about\nperson doesn't even know anything about\nperson doesn't even know anything about agentic AI frameworks. Like why do we\nagentic AI frameworks. Like why do we\nagentic AI frameworks. Like why do we talk about these things? So first of\ntalk about these things? So first of\ntalk about these things? So first of all, do you even know the difference\nall, do you even know the difference\nall, do you even know the difference between AI agent and agentic AI? Do you\nbetween AI agent and agentic AI? Do you\nbetween AI agent and agentic AI? Do you even know the difference? No, I I I know\neven know the difference? No, I I I know\neven know the difference? No, I I I know this. So we will first of all discuss\nthis. So we will first of all discuss\nthis. So we will first of all discuss about this thing because once you get\nabout this thing because once you get\nabout this thing because once you get this concept only then you should learn\nthis concept only then you should learn\nthis concept only then you should learn anything else. Then you should learn the\nanything else. Then you should learn the\nanything else. Then you should learn the frameworks. You first need to understand\nframeworks. You first need to understand\nframeworks. You first need to understand the concept like what is agent and AI\nthe concept like what is agent and AI\nthe concept like what is agent and AI agent. Frameworks is not a very critical\nagent. Frameworks is not a very critical\nagent. Frameworks is not a very critical or you can say very uh complex thing.\nor you can say very uh complex thing.\nor you can say very uh complex thing. Framework is just like a kind of module.\nFramework is just like a kind of module.\nFramework is just like a kind of module. It's just a kind of SDK. It's just a\nIt's just a kind of SDK. It's just a\nIt's just a kind of SDK. It's just a kind of package. We just call it as\nkind of package. We just call it as\nkind of package. We just call it as framework. Right? The main thing is\nframework. Right? The main thing is\nframework. Right? The main thing is agentic AI. What is agent? And in order\nagentic AI. What is agent? And in order\nagentic AI. What is agent? And in order to do that, we need to just understand\nto do that, we need to just understand\nto do that, we need to just understand this concept agent versus AI agent. So\nthis concept agent versus AI agent. So\nthis concept agent versus AI agent. So let's start with AI agent like what\nlet's start with AI agent like what\nlet's start with AI agent like what exactly is AI agent? Let's say this\nexactly is AI agent? Let's say this\nexactly is AI agent? Let's say this Rahul. Let's let's let's say this is\nRahul. Let's let's let's say this is\nRahul. Let's let's let's say this is Rahul. Okay. And this person needs to\nRahul. Okay. And this person needs to\nRahul. Okay. And this person needs to know everything about agent and all. And\nknow everything about agent and all. And\nknow everything about agent and all. And do you know what this person for first\ndo you know what this person for first\ndo you know what this person for first needs to understand the AI agents? Okay.\nneeds to understand the AI agents? Okay.\nneeds to understand the AI agents? Okay. So now when I just talk about AI agent,\nSo now when I just talk about AI agent,\nSo now when I just talk about AI agent, what is actually an AI agent?\nwhat is actually an AI agent?\nwhat is actually an AI agent? AI\nAI\nAI agent. What is this thing? Like why this\nagent. What is this thing? Like why this\nagent. What is this thing? Like why this term is so popular right now and\nterm is so popular right now and\nterm is so popular right now and everyone is talking about this thing AI\neveryone is talking about this thing AI\neveryone is talking about this thing AI agent. AI agent is actually nothing\nagent. AI agent is actually nothing\nagent. AI agent is actually nothing about you can say is like nothing but a\nabout you can say is like nothing but a\nabout you can say is like nothing but a kind of framework\nkind of framework\nkind of framework in itself not the framework let let's\nin itself not the framework let let's\nin itself not the framework let let's not call it as framework you will be\nnot call it as framework you will be\nnot call it as framework you will be confused it is a kind of let's say real\nconfused it is a kind of let's say real\nconfused it is a kind of let's say real person what is agent forget about AI\nperson what is agent forget about AI\nperson what is agent forget about AI what is agent if you just try to find\nwhat is agent if you just try to find\nwhat is agent if you just try to find out the word agent agent is someone who\nout the word agent agent is someone who\nout the word agent agent is someone who performs a specific task\nperforms a specific task\nperforms a specific task you would have um seen you can say\nyou would have um seen you can say\nyou would have um seen you can say insurance agents and then you can say\ninsurance agents and then you can say\ninsurance agents and then you can say customer care agents and then you have\ncustomer care agents and then you have\ncustomer care agents and then you have flight agent like there are some agents\nflight agent like there are some agents\nflight agent like there are some agents like why do we use the word agent this\nlike why do we use the word agent this\nlike why do we use the word agent this particular thing this AI agent is\nparticular thing this AI agent is\nparticular thing this AI agent is actually\nactually\nactually aligned towards a specific task okay\naligned towards a specific task okay\naligned towards a specific task okay specific task\nspecific task\nspecific task nothing fancy\nnothing fancy\nnothing fancy this thing so now we Have\nthis thing so now we Have\nthis thing so now we Have we have added the keyword called AI?\nwe have added the keyword called AI?\nwe have added the keyword called AI? That means this agent is not the real\nThat means this agent is not the real\nThat means this agent is not the real person.\nperson.\nperson. Oh, okay. This agent is not the real\nOh, okay. This agent is not the real\nOh, okay. This agent is not the real person. This is AI performing the task\nperson. This is AI performing the task\nperson. This is AI performing the task of an agent.\nof an agent.\nof an agent. Okay. And in this case, it will be let's\nOkay. And in this case, it will be let's\nOkay. And in this case, it will be let's say LLM.\nsay LLM.\nsay LLM. LLM, right? Okay.\nLLM, right? Okay.\nLLM, right? Okay. This will be LLM. When LLM that means\nThis will be LLM. When LLM that means\nThis will be LLM. When LLM that means large language model the entire AI\nlarge language model the entire AI\nlarge language model the entire AI industry is built on top of LLM right\nindustry is built on top of LLM right\nindustry is built on top of LLM right now makes sense like large language\nnow makes sense like large language\nnow makes sense like large language models. So LLM when\nmodels. So LLM when\nmodels. So LLM when able to perform the task of a real human\nable to perform the task of a real human\nable to perform the task of a real human that will be called as an AI agent. But\nthat will be called as an AI agent. But\nthat will be called as an AI agent. But An Lamba\nAn Lamba\nAn Lamba can we say that all the LLMs are AI\ncan we say that all the LLMs are AI\ncan we say that all the LLMs are AI agents? No, I didn't say that here. Hear\nagents? No, I didn't say that here. Hear\nagents? No, I didn't say that here. Hear me out. I said when LLM is able when LLM\nme out. I said when LLM is able when LLM\nme out. I said when LLM is able when LLM when this LLM let's say this is the LLM.\nwhen this LLM let's say this is the LLM.\nwhen this LLM let's say this is the LLM. Okay, and this can be like any normal\nOkay, and this can be like any normal\nOkay, and this can be like any normal LLM. It can be chat GBT. It can be\nLLM. It can be chat GBT. It can be\nLLM. It can be chat GBT. It can be anything. Let's say this is your LLM.\nanything. Let's say this is your LLM.\nanything. Let's say this is your LLM. Okay,\nOkay,\nOkay, this is your LLM.\nthis is your LLM.\nthis is your LLM. Wow.\nWow.\nWow. Let's say this is your LLM. Now, this\nLet's say this is your LLM. Now, this\nLet's say this is your LLM. Now, this LLM can be anyone. Open AI. Let's let's\nLLM can be anyone. Open AI. Let's let's\nLLM can be anyone. Open AI. Let's let's take open eye because open is very\ntake open eye because open is very\ntake open eye because open is very popular. This is open LLM. This is\npopular. This is open LLM. This is\npopular. This is open LLM. This is simple LLM that just knows how to\nsimple LLM that just knows how to\nsimple LLM that just knows how to generate the text. Simple.\ngenerate the text. Simple.\ngenerate the text. Simple. Now when we we developers\nNow when we we developers\nNow when we we developers add some capabilities\nadd some capabilities\nadd some capabilities to this LLM in order to perform a\nto this LLM in order to perform a\nto this LLM in order to perform a specific task in or\nspecific task in or\nspecific task in or sorry in order to achieve a specific\nsorry in order to achieve a specific\nsorry in order to achieve a specific goal. this LLM becomes AI agent.\ngoal. this LLM becomes AI agent.\ngoal. this LLM becomes AI agent. Okay, that is clear. But what are those\nOkay, that is clear. But what are those\nOkay, that is clear. But what are those specific task or specific capabilities\nspecific task or specific capabilities\nspecific task or specific capabilities that we add? Very good question. In\nthat we add? Very good question. In\nthat we add? Very good question. In short, in simple language, we add\nshort, in simple language, we add\nshort, in simple language, we add something called as tools. Okay,\nsomething called as tools. Okay,\nsomething called as tools. Okay, we add something called as tools.\nwe add something called as tools.\nwe add something called as tools. And now you will say what kind of tools?\nAnd now you will say what kind of tools?\nAnd now you will say what kind of tools? I will say any tool. Any kind of tool.\nSo when we add this tools\nSo when we add this tools okay so we have provided it who we this\nokay so we have provided it who we this\nokay so we have provided it who we this let's say developer Rahul Rahul knows\nlet's say developer Rahul Rahul knows\nlet's say developer Rahul Rahul knows how to develop the tools okay now these\nhow to develop the tools okay now these\nhow to develop the tools okay now these tools when these tools are integrated\ntools when these tools are integrated\ntools when these tools are integrated with this LLM\nPerfect. Now when we integrate this\nPerfect. Now when we integrate this these tools\nthese tools\nthese tools with this LLM, let me just write it for\nwith this LLM, let me just write it for\nwith this LLM, let me just write it for you. Integrated\nyou. Integrated\nyou. Integrated then it becomes this LLM. Simple. This\nthen it becomes this LLM. Simple. This\nthen it becomes this LLM. Simple. This is your entire definition, entire flow.\nis your entire definition, entire flow.\nis your entire definition, entire flow. And now this AI agent can perform this\nAnd now this AI agent can perform this\nAnd now this AI agent can perform this task.\ntask.\ntask. Right? Now you will say um which task? I\nRight? Now you will say um which task? I\nRight? Now you will say um which task? I would say any task. Let's say reporting\nwould say any task. Let's say reporting\nwould say any task. Let's say reporting data, email, anything. Let's say this is\ndata, email, anything. Let's say this is\ndata, email, anything. Let's say this is a task. Simple.\na task. Simple.\na task. Simple. Let's say this agent is a kind of um\nLet's say this agent is a kind of um\nLet's say this agent is a kind of um customer assistant or basically customer\ncustomer assistant or basically customer\ncustomer assistant or basically customer service agent which receives the emails\nservice agent which receives the emails\nservice agent which receives the emails and maybe filtering out the emails which\nand maybe filtering out the emails which\nand maybe filtering out the emails which are bad, which are right and categorize\nare bad, which are right and categorize\nare bad, which are right and categorize it. It's a kind of task simple that you\nit. It's a kind of task simple that you\nit. It's a kind of task simple that you would hire a real human for this task.\nwould hire a real human for this task.\nwould hire a real human for this task. Now you have AI agent for it. Now\nNow you have AI agent for it. Now\nNow you have AI agent for it. Now this developer builds these particular\nthis developer builds these particular\nthis developer builds these particular tools. This is the main thing to\ntools. This is the main thing to\ntools. This is the main thing to understand.\nunderstand.\nunderstand. This is the main thing.\nThis is the main thing.\nThis is the main thing. This developer built these tools.\nThis developer built these tools.\nThis developer built these tools. Okay.\nOkay.\nOkay. Good. Now a very good question can be\nGood. Now a very good question can be\nGood. Now a very good question can be like you can ask like so many questions\nlike you can ask like so many questions\nlike you can ask like so many questions but let's say you have asked this\nbut let's say you have asked this\nbut let's say you have asked this question anala. Yeah.\nquestion anala. Yeah.\nquestion anala. Yeah. Why do we need to create tools and why\nWhy do we need to create tools and why\nWhy do we need to create tools and why LLMs? Because LLMs are so popular.\nLLMs? Because LLMs are so popular.\nLLMs? Because LLMs are so popular. Everyone is talking about LLMs. LLMs are\nEveryone is talking about LLMs. LLMs are\nEveryone is talking about LLMs. LLMs are like so big like they have raised\nlike so big like they have raised\nlike so big like they have raised billions of dollars. Why we need to\nbillions of dollars. Why we need to\nbillions of dollars. Why we need to create tools? Why we need to integrate\ncreate tools? Why we need to integrate\ncreate tools? Why we need to integrate tools? So very good question. Let me\ntools? So very good question. Let me\ntools? So very good question. Let me just tell you. Let's say you are using\njust tell you. Let's say you are using\njust tell you. Let's say you are using um Chat GPD. Let's say let me open ChatG\num Chat GPD. Let's say let me open ChatG\num Chat GPD. Let's say let me open ChatG in front of you. Okay. And I hope you\nin front of you. Okay. And I hope you\nin front of you. Okay. And I hope you know what is ChatG.\nknow what is ChatG.\nknow what is ChatG. If no um then very good slow collapse.\nIf no um then very good slow collapse.\nIf no um then very good slow collapse. So let's say this is the chat GPD right.\nSo let's say this is the chat GPD right.\nSo let's say this is the chat GPD right. Chad GPD is nothing but a kind of\nChad GPD is nothing but a kind of\nChad GPD is nothing but a kind of wrapper. It's just a kind of front end\nwrapper. It's just a kind of front end\nwrapper. It's just a kind of front end that is built on top of LLM. Simple very\nthat is built on top of LLM. Simple very\nthat is built on top of LLM. Simple very good. Now do you know what when Chad GPD\ngood. Now do you know what when Chad GPD\ngood. Now do you know what when Chad GPD was launched in its initial days if I\nwas launched in its initial days if I\nwas launched in its initial days if I ask something like this? Um my name is\nask something like this? Um my name is\nask something like this? Um my name is Anro.\nAnro.\nAnro. Let's say I write something like this.\nLet's say I write something like this.\nLet's say I write something like this. So Chad GBD would be able to generate\nSo Chad GBD would be able to generate\nSo Chad GBD would be able to generate this text and that's it. And when I was\nthis text and that's it. And when I was\nthis text and that's it. And when I was asking Chad GBD, hey\nasking Chad GBD, hey\nasking Chad GBD, hey what's the temperature\nwhat's the temperature\nwhat's the temperature here in Canada?\nhere in Canada?\nhere in Canada? Right? If I hit enter, if I hit enter, I\nRight? If I hit enter, if I hit enter, I\nRight? If I hit enter, if I hit enter, I will not hit enter right now because I\nwill not hit enter right now because I\nwill not hit enter right now because I need to explain this thing. In the\nneed to explain this thing. In the\nneed to explain this thing. In the earlier days, in the initial days when I\nearlier days, in the initial days when I\nearlier days, in the initial days when I would be asking this question,\nwould be asking this question,\nwould be asking this question, Chad GPT would not be able to answer\nChad GPT would not be able to answer\nChad GPT would not be able to answer this question.\nthis question.\nthis question. Okay. Why? Because it doesn't have the\nOkay. Why? Because it doesn't have the\nOkay. Why? Because it doesn't have the capability to track the recent data\ncapability to track the recent data\ncapability to track the recent data because these LLMs this these large\nbecause these LLMs this these large\nbecause these LLMs this these large language models are trained on specific\nlanguage models are trained on specific\nlanguage models are trained on specific data sets, right? And they cannot train\ndata sets, right? And they cannot train\ndata sets, right? And they cannot train the data sets like they cannot train the\nthe data sets like they cannot train the\nthe data sets like they cannot train the model on data sets on a daily basis. It\nmodel on data sets on a daily basis. It\nmodel on data sets on a daily basis. It is a very expensive task. So they do it\nis a very expensive task. So they do it\nis a very expensive task. So they do it I think once a year or twice a year. So\nI think once a year or twice a year. So\nI think once a year or twice a year. So they do not have like realtime access\nthey do not have like realtime access\nthey do not have like realtime access but you know what when I hit enter now\njust bait and watch Canada's huge oh it\njust bait and watch Canada's huge oh it is very smart let's say temperature in\nis very smart let's say temperature in\nis very smart let's say temperature in Calgary I'm not in Calgary by the way\nCalgary I'm not in Calgary by the way\nCalgary I'm not in Calgary by the way but yeah let's say in Calgary so now see\nbut yeah let's say in Calgary so now see\nbut yeah let's say in Calgary so now see see did you see I know it's cold -4Â°\nsee did you see I know it's cold -4Â°\nsee did you see I know it's cold -4Â° centigrade yeah so now the thing is how\ncentigrade yeah so now the thing is how\ncentigrade yeah so now the thing is how it is able to just get the right\nit is able to just get the right\nit is able to just get the right temperature and it is actually -4Â° -15Â°\ntemperature and it is actually -4Â° -15Â°\ntemperature and it is actually -4Â° -15Â° I know this okay because here in my city\nI know this okay because here in my city\nI know this okay because here in my city it is 11Â° so yeah Alberta is like the\nit is 11Â° so yeah Alberta is like the\nit is 11Â° so yeah Alberta is like the coldest not coldest but yeah one of the\ncoldest not coldest but yeah one of the\ncoldest not coldest but yeah one of the coldest so now I know that the\ncoldest so now I know that the\ncoldest so now I know that the temperature is -4Â° centiggrade how it is\ntemperature is -4Â° centiggrade how it is\ntemperature is -4Â° centiggrade how it is able to track that now you will say\nable to track that now you will say\nable to track that now you will say because LLMs are becoming smart not at\nbecause LLMs are becoming smart not at\nbecause LLMs are becoming smart not at all not at all now do you know what\nall not at all now do you know what\nall not at all now do you know what happened this chat GBD team built tools\nhappened this chat GBD team built tools\nhappened this chat GBD team built tools okay this chad GBD built tools and they\nokay this chad GBD built tools and they\nokay this chad GBD built tools and they integrated those tools with this LLM\nintegrated those tools with this LLM\nintegrated those tools with this LLM original LLM. It can be let's say GPT5,\noriginal LLM. It can be let's say GPT5,\noriginal LLM. It can be let's say GPT5, GPT4, any LLM model. They built these\nGPT4, any LLM model. They built these\nGPT4, any LLM model. They built these tools and this tool can be let's say\ntools and this tool can be let's say\ntools and this tool can be let's say getting the u weather information and\ngetting the u weather information and\ngetting the u weather information and did you notice when we just hit that\ndid you notice when we just hit that\ndid you notice when we just hit that particular question it showed that um\nparticular question it showed that um\nparticular question it showed that um thing called searching the web is that\nthing called searching the web is that\nthing called searching the web is that thing that is a tool call.\nthing that is a tool call.\nthing that is a tool call. Okay, so we build these tools on our\nOkay, so we build these tools on our\nOkay, so we build these tools on our own. These are nothing but the functions\nown. These are nothing but the functions\nown. These are nothing but the functions and then we integrate these things to\nand then we integrate these things to\nand then we integrate these things to the LLM and then this becomes an AI\nthe LLM and then this becomes an AI\nthe LLM and then this becomes an AI agent\nagent\nagent like obviously we will be just doing so\nlike obviously we will be just doing so\nlike obviously we will be just doing so much of stuff to make it as an agent to\nmuch of stuff to make it as an agent to\nmuch of stuff to make it as an agent to make it autonomous as much as we can but\nmake it autonomous as much as we can but\nmake it autonomous as much as we can but this is the backbone of AI agent LLM\nthis is the backbone of AI agent LLM\nthis is the backbone of AI agent LLM with tools that's it because can you\nwith tools that's it because can you\nwith tools that's it because can you think of an agent which doesn't have any\nthink of an agent which doesn't have any\nthink of an agent which doesn't have any capability so let's say we are hiring a\ncapability so let's say we are hiring a\ncapability so let's say we are hiring a real person right and person needs to\nreal person right and person needs to\nreal person right and person needs to write email, read email and do so many\nwrite email, read email and do so many\nwrite email, read email and do so many stuff. So that agent, that human also\nstuff. So that agent, that human also\nstuff. So that agent, that human also needs some access, needs an email\nneeds some access, needs an email\nneeds some access, needs an email address, all those things, right? So it\naddress, all those things, right? So it\naddress, all those things, right? So it needs those capabilities. Same thing we\nneeds those capabilities. Same thing we\nneeds those capabilities. Same thing we will provide to this LLM and then it\nwill provide to this LLM and then it\nwill provide to this LLM and then it will become an AI agent. This is the\nwill become an AI agent. This is the\nwill become an AI agent. This is the definition of AI agent. Okay? Now what\ndefinition of AI agent. Okay? Now what\ndefinition of AI agent. Okay? Now what is the difference between AI agent and\nis the difference between AI agent and\nis the difference between AI agent and agenti? Because this is clear. So always\nagenti? Because this is clear. So always\nagenti? Because this is clear. So always remember this thing. This is a kind of\nremember this thing. This is a kind of\nremember this thing. This is a kind of formula you can say or just a thing just\nformula you can say or just a thing just\nformula you can say or just a thing just a you can say way to remember it. So AI\na you can say way to remember it. So AI\na you can say way to remember it. So AI agent is built to fulfill a specific\nagent is built to fulfill a specific\nagent is built to fulfill a specific task. Okay is built to fuli fulfill a\ntask. Okay is built to fuli fulfill a\ntask. Okay is built to fuli fulfill a specific task.\nspecific task.\nspecific task. Let's say you are hiring an AI agent\nLet's say you are hiring an AI agent\nLet's say you are hiring an AI agent basically building an AI agent. You are\nbasically building an AI agent. You are\nbasically building an AI agent. You are hiring a person same thing. So you are\nhiring a person same thing. So you are\nhiring a person same thing. So you are getting an AI agent for this task. Let's\ngetting an AI agent for this task. Let's\ngetting an AI agent for this task. Let's say now this email task is done. Now you\nsay now this email task is done. Now you\nsay now this email task is done. Now you need to add an agent in your team who\nneed to add an agent in your team who\nneed to add an agent in your team who will do the marketing like person is\nwill do the marketing like person is\nwill do the marketing like person is reading the emails categorizing it and\nreading the emails categorizing it and\nreading the emails categorizing it and that's it. Now this particular agent\nthat's it. Now this particular agent\nthat's it. Now this particular agent needs to talk to another agent who is a\nneeds to talk to another agent who is a\nneeds to talk to another agent who is a marketing agent who will do the\nmarketing agent who will do the\nmarketing agent who will do the marketing stuff who will run ads on\nmarketing stuff who will run ads on\nmarketing stuff who will run ads on social platforms.\nsocial platforms.\nsocial platforms. Make sense? And then that marketing\nMake sense? And then that marketing\nMake sense? And then that marketing agent needs to talk to let's say\nagent needs to talk to let's say\nagent needs to talk to let's say business agent because it needs to\nbusiness agent because it needs to\nbusiness agent because it needs to communicate with the business team or\ncommunicate with the business team or\ncommunicate with the business team or finance team. Hey, how much money do we\nfinance team. Hey, how much money do we\nfinance team. Hey, how much money do we have to run ads? Hey, how much money do\nhave to run ads? Hey, how much money do\nhave to run ads? Hey, how much money do we need to spend this month? Maybe\nwe need to spend this month? Maybe\nwe need to spend this month? Maybe right. So when we need to\nright. So when we need to\nright. So when we need to enable the communication between the\nenable the communication between the\nenable the communication between the agents that becomes agentic AI that\nagents that becomes agentic AI that\nagents that becomes agentic AI that means orchestration of AI agents when we\nmeans orchestration of AI agents when we\nmeans orchestration of AI agents when we have multiple AI agents and do you know\nhave multiple AI agents and do you know\nhave multiple AI agents and do you know what the fun part agentic AI is always\nwhat the fun part agentic AI is always\nwhat the fun part agentic AI is always is always aligned towards fulfilling the\nis always aligned towards fulfilling the\nis always aligned towards fulfilling the goal. Now we'll say AI agent is also\ngoal. Now we'll say AI agent is also\ngoal. Now we'll say AI agent is also inclined towards fulfilling a goal. The\ninclined towards fulfilling a goal. The\ninclined towards fulfilling a goal. The difference is AI agent is only\ndifference is AI agent is only\ndifference is AI agent is only fulfilling the sub goal basically the\nfulfilling the sub goal basically the\nfulfilling the sub goal basically the task but agentic AI is fulfilling the\ntask but agentic AI is fulfilling the\ntask but agentic AI is fulfilling the entire you can say goal or you can say\nentire you can say goal or you can say\nentire you can say goal or you can say bigger project. So let's say you have a\nbigger project. So let's say you have a\nbigger project. So let's say you have a kind of social media managing company\nkind of social media managing company\nkind of social media managing company and you have those AI agents built in\nand you have those AI agents built in\nand you have those AI agents built in specific areas and that will be called\nspecific areas and that will be called\nspecific areas and that will be called as agent AI.\nas agent AI.\nas agent AI. I hope it makes sense.\nI hope it makes sense.\nI hope it makes sense. Now let's say let's relate this analogy\nNow let's say let's relate this analogy\nNow let's say let's relate this analogy to our data engineering. Let's say you\nto our data engineering. Let's say you\nto our data engineering. Let's say you are a developer okay and you have built\nare a developer okay and you have built\nare a developer okay and you have built some tools to run the spark notebook\nsome tools to run the spark notebook\nsome tools to run the spark notebook let's say or let's say spark job in\nlet's say or let's say spark job in\nlet's say or let's say spark job in pispark right if you do not know what\npispark right if you do not know what\npispark right if you do not know what pispark basically pispark is the\npispark basically pispark is the\npispark basically pispark is the framework that we use to process big\nframework that we use to process big\nframework that we use to process big data right so you have built this AI\ndata right so you have built this AI\ndata right so you have built this AI agent to run the task and task is\nagent to run the task and task is\nagent to run the task and task is running spark\nrunning spark\nrunning spark right running spark job perfect this is\nright running spark job perfect this is\nright running spark job perfect this is the task\nthe task\nthe task now this is agent one right as we just\nnow this is agent one right as we just\nnow this is agent one right as we just mentioned this is agent one.\nmentioned this is agent one.\nmentioned this is agent one. Okay, this is agent one and let me\nOkay, this is agent one and let me\nOkay, this is agent one and let me remove this Rahul. This is very obvious\nremove this Rahul. This is very obvious\nremove this Rahul. This is very obvious now because they have the tools.\nnow because they have the tools.\nnow because they have the tools. So this is only and only agent one makes\nSo this is only and only agent one makes\nSo this is only and only agent one makes sense.\nsense.\nsense. Perfect.\nAgent one. Now do you know what? Before\nAgent one. Now do you know what? Before running this job, you obviously need to\nrunning this job, you obviously need to\nrunning this job, you obviously need to configure the spark cluster size like\nconfigure the spark cluster size like\nconfigure the spark cluster size like what cluster do we need. So actually\nwhat cluster do we need. So actually\nwhat cluster do we need. So actually this agent needs to communicate with\nthis agent needs to communicate with\nthis agent needs to communicate with another agent, right? Let's say this\nanother agent, right? Let's say this\nanother agent, right? Let's say this agent or you can say it can be like two\nagent or you can say it can be like two\nagent or you can say it can be like two and four two and fro communication as\nand four two and fro communication as\nand four two and fro communication as well. So it's not a big deal. So let's\nwell. So it's not a big deal. So let's\nwell. So it's not a big deal. So let's say this is the this is another agent\nsay this is the this is another agent\nsay this is the this is another agent and what this agent does this agent\nand what this agent does this agent\nand what this agent does this agent configure the spark cluster size that\nconfigure the spark cluster size that\nconfigure the spark cluster size that what should be the cluster size on the\nwhat should be the cluster size on the\nwhat should be the cluster size on the basis of the data what should be the you\nbasis of the data what should be the you\nbasis of the data what should be the you can say course number of coursees what\ncan say course number of coursees what\ncan say course number of coursees what should be the memory we should allocate\nshould be the memory we should allocate\nshould be the memory we should allocate to our jobs like it will give you the\nto our jobs like it will give you the\nto our jobs like it will give you the exact or ideal cluster size and then you\nexact or ideal cluster size and then you\nexact or ideal cluster size and then you can just run your job before that you do\ncan just run your job before that you do\ncan just run your job before that you do not need to run the job so we'll edit\nnot need to run the job so we'll edit\nnot need to run the job so we'll edit the task and we can say\nthe task and we can say\nthe task and we can say we are simply let's Okay, talking about\nwe are simply let's Okay, talking about\nwe are simply let's Okay, talking about clusters basically these servers.\nclusters basically these servers.\nclusters basically these servers. Make sense?\nMake sense?\nMake sense? Make sense? Okay. So now the task is to\nMake sense? Okay. So now the task is to\nMake sense? Okay. So now the task is to give us the right server size. Basically\ngive us the right server size. Basically\ngive us the right server size. Basically the right cluster size and then we need\nthe right cluster size and then we need\nthe right cluster size and then we need to talk to each other. So now this\nto talk to each other. So now this\nto talk to each other. So now this framework will be called as agentic\nframework will be called as agentic\nframework will be called as agentic framework. Why? Because they are\nframework. Why? Because they are\nframework. Why? Because they are communicating with each other and\ncommunicating with each other and\ncommunicating with each other and there's an orchestration. This is a very\nthere's an orchestration. This is a very\nthere's an orchestration. This is a very simple uh you can say a kind of\nsimple uh you can say a kind of\nsimple uh you can say a kind of orchestration where where we just have\norchestration where where we just have\norchestration where where we just have like two people talking to each other.\nlike two people talking to each other.\nlike two people talking to each other. In the real world, we build the complex\nIn the real world, we build the complex\nIn the real world, we build the complex workflows where we have let's say 10 to\nworkflows where we have let's say 10 to\nworkflows where we have let's say 10 to 20 agents talking to each other. And the\n20 agents talking to each other. And the\n20 agents talking to each other. And the main thing is it is not linear. What do\nmain thing is it is not linear. What do\nmain thing is it is not linear. What do I mean? It can be branched out to many\nI mean? It can be branched out to many\nI mean? It can be branched out to many things, right? Let me just show you. So\nthings, right? Let me just show you. So\nthings, right? Let me just show you. So let's say this is agent one. Okay, this\nlet's say this is agent one. Okay, this\nlet's say this is agent one. Okay, this is agent one. And this agent one\nis agent one. And this agent one\nis agent one. And this agent one can be connected to this one. Okay,\ncan be connected to this one. Okay,\ncan be connected to this one. Okay, let me just make it small. So let's say\nlet me just make it small. So let's say\nlet me just make it small. So let's say like this because I want to just show\nlike this because I want to just show\nlike this because I want to just show you the complex one. So that is why I'm\nyou the complex one. So that is why I'm\nyou the complex one. So that is why I'm just zooming it out. So do not need to\njust zooming it out. So do not need to\njust zooming it out. So do not need to worry. Okay, so let's say this is agent\nworry. Okay, so let's say this is agent\nworry. Okay, so let's say this is agent one. This is connected to this one\none. This is connected to this one\none. This is connected to this one and let's use this one. Perfect. So this\nand let's use this one. Perfect. So this\nand let's use this one. Perfect. So this is agent two and then let's say this\nis agent two and then let's say this\nis agent two and then let's say this will be connected to this agent.\nwill be connected to this agent.\nwill be connected to this agent. Can be it's not a big deal,\nCan be it's not a big deal,\nCan be it's not a big deal, right? Let's say like this. And then we\nright? Let's say like this. And then we\nright? Let's say like this. And then we have let's say two agents.\nhave let's say two agents.\nhave let's say two agents. Let's say like this. Let's say like\nLet's say like this. Let's say like\nLet's say like this. Let's say like this. Wow. Can we have these complex\nthis. Wow. Can we have these complex\nthis. Wow. Can we have these complex workflows? Obviously, we have it.\nworkflows? Obviously, we have it.\nworkflows? Obviously, we have it. So, let's say this is the thing, right?\nSo, let's say this is the thing, right?\nSo, let's say this is the thing, right? And then we have let's say the final\nAnd then we have let's say the final\nAnd then we have let's say the final node or basically the agent like this.\nnode or basically the agent like this.\nnode or basically the agent like this. So, this is a complex\nSo, this is a complex\nSo, this is a complex agentic workflow. Let's say we have a\nagentic workflow. Let's say we have a\nagentic workflow. Let's say we have a conditional here.\nconditional here.\nconditional here. that we can use.\nPerfect. So let's say this is the\nPerfect. So let's say this is the complex workflow. This is agenti.\ncomplex workflow. This is agenti.\ncomplex workflow. This is agenti. Now I hope that you have the clear\nNow I hope that you have the clear\nNow I hope that you have the clear understanding of agenti\nunderstanding of agenti\nunderstanding of agenti like what is agenti and what is AI\nlike what is agenti and what is AI\nlike what is agenti and what is AI agent?\nagent?\nagent? I hope so. This is also aki by the way\nI hope so. This is also aki by the way\nI hope so. This is also aki by the way but this is a very simple one but\nbut this is a very simple one but\nbut this is a very simple one but obviously this was required to make you\nobviously this was required to make you\nobviously this was required to make you understand the concepts right\nunderstand the concepts right\nunderstand the concepts right you can say agent here\nyou can say agent here\nyou can say agent here like here and don't worry I'll just\nlike here and don't worry I'll just\nlike here and don't worry I'll just provide all of these nodes in my repo\nprovide all of these nodes in my repo\nprovide all of these nodes in my repo GitHub repo so that you can just enjoy\nGitHub repo so that you can just enjoy\nGitHub repo so that you can just enjoy it agent I can simply say simple\nit agent I can simply say simple\nit agent I can simply say simple and then we have\nand then we have\nand then we have complex as\nmake sense perfect so now if you have\nmake sense perfect so now if you have this clear now you can just learn any\nthis clear now you can just learn any\nthis clear now you can just learn any framework because see now let's go back\nframework because see now let's go back\nframework because see now let's go back to our main agenda agentic AI and\nto our main agenda agentic AI and\nto our main agenda agentic AI and frameworks so now we know the difference\nframeworks so now we know the difference\nframeworks so now we know the difference of agentic AI and AI agents obviously\nof agentic AI and AI agents obviously\nof agentic AI and AI agents obviously and now you also know why agentic is\nand now you also know why agentic is\nand now you also know why agentic is becoming more and more popular because\nbecoming more and more popular because\nbecoming more and more popular because we do not create just one AI agent and\nwe do not create just one AI agent and\nwe do not create just one AI agent and that's it we need the entire flow Now\nthat's it we need the entire flow Now\nthat's it we need the entire flow Now just tell me one thing. Come here. If I\njust tell me one thing. Come here. If I\njust tell me one thing. Come here. If I just show you this agentic AI and not\njust show you this agentic AI and not\njust show you this agentic AI and not the simple one, complex one. Have you\nthe simple one, complex one. Have you\nthe simple one, complex one. Have you seen this kind of thing in your\nseen this kind of thing in your\nseen this kind of thing in your day-to-day activities? And I'm talking\nday-to-day activities? And I'm talking\nday-to-day activities? And I'm talking to you data engineer. Have you seen this\nto you data engineer. Have you seen this\nto you data engineer. Have you seen this kind of thing? I'm not asking you to\nkind of thing? I'm not asking you to\nkind of thing? I'm not asking you to think about LLM. No, no, no. Just take\nthink about LLM. No, no, no. Just take\nthink about LLM. No, no, no. Just take about like I'm just talking about these\nabout like I'm just talking about these\nabout like I'm just talking about these things, these boxes. I'm not talking\nthings, these boxes. I'm not talking\nthings, these boxes. I'm not talking about the stuff that is built inside\nabout the stuff that is built inside\nabout the stuff that is built inside this just the nodes.\nthis just the nodes.\nthis just the nodes. Have you seen this? You will say yeah we\nHave you seen this? You will say yeah we\nHave you seen this? You will say yeah we have seen this in our pipelines.\nhave seen this in our pipelines.\nhave seen this in our pipelines. Exactly. That's what I'm trying to tell\nExactly. That's what I'm trying to tell\nExactly. That's what I'm trying to tell you that we are actually building the\nyou that we are actually building the\nyou that we are actually building the pipelines and we are calling call\npipelines and we are calling call\npipelines and we are calling call calling it as agentic.\ncalling it as agentic.\ncalling it as agentic. Now yes I'm not saying that it is not\nNow yes I'm not saying that it is not\nNow yes I'm not saying that it is not agentic. It is agentic. Why? Because we\nagentic. It is agentic. Why? Because we\nagentic. It is agentic. Why? Because we have AI workflows basically AI\nhave AI workflows basically AI\nhave AI workflows basically AI activities within each node. But on the\nactivities within each node. But on the\nactivities within each node. But on the bigger picture side this is just a\nbigger picture side this is just a\nbigger picture side this is just a pipeline. That's it. This is just a DAG\npipeline. That's it. This is just a DAG\npipeline. That's it. This is just a DAG directed asyclic graph that we build in\ndirected asyclic graph that we build in\ndirected asyclic graph that we build in our day-to-day activities.\nour day-to-day activities.\nour day-to-day activities. Make sense? So I hope now you are very\nMake sense? So I hope now you are very\nMake sense? So I hope now you are very much clear and now you should know why\nmuch clear and now you should know why\nmuch clear and now you should know why data engineers need to lead this\ndata engineers need to lead this\ndata engineers need to lead this industry and they are not leading the\nindustry and they are not leading the\nindustry and they are not leading the industry right now because they are too\nindustry right now because they are too\nindustry right now because they are too late and they are not actually learning\nlate and they are not actually learning\nlate and they are not actually learning AI because this is the right time high\nAI because this is the right time high\nAI because this is the right time high time to learn it and don't worry I will\ntime to learn it and don't worry I will\ntime to learn it and don't worry I will just\njust\njust make it understand everything. Okay,\nmake it understand everything. Okay,\nmake it understand everything. Okay, perfect. So this is the whole concept\nperfect. So this is the whole concept\nperfect. So this is the whole concept and now let's talk about the frameworks\nand now let's talk about the frameworks\nand now let's talk about the frameworks quickly. Framework is not a very kind of\nquickly. Framework is not a very kind of\nquickly. Framework is not a very kind of complex thing. Framework is nothing but\ncomplex thing. Framework is nothing but\ncomplex thing. Framework is nothing but just a word that we use to portray\njust a word that we use to portray\njust a word that we use to portray something about you can say a module.\nsomething about you can say a module.\nsomething about you can say a module. Let's say I'm building a Python package,\nLet's say I'm building a Python package,\nLet's say I'm building a Python package, right? I have so many classes, I have so\nright? I have so many classes, I have so\nright? I have so many classes, I have so many functions, I have so many variables\nmany functions, I have so many variables\nmany functions, I have so many variables and blah blah blah blah blah. When I\nand blah blah blah blah blah. When I\nand blah blah blah blah blah. When I package them together, when it is ready\npackage them together, when it is ready\npackage them together, when it is ready to be reusable by anyone, it will be\nto be reusable by anyone, it will be\nto be reusable by anyone, it will be called as framework.\ncalled as framework.\ncalled as framework. Simple if you just want to understand\nSimple if you just want to understand\nSimple if you just want to understand it. Okay. And if you just want to learn\nit. Okay. And if you just want to learn\nit. Okay. And if you just want to learn the definition of framework, go to\nthe definition of framework, go to\nthe definition of framework, go to Wikipedia and learn it. This is the\nWikipedia and learn it. This is the\nWikipedia and learn it. This is the understanding of framework, right? So\nunderstanding of framework, right? So\nunderstanding of framework, right? So the thing is now in order to build all\nthe thing is now in order to build all\nthe thing is now in order to build all of these things, obviously we can use\nof these things, obviously we can use\nof these things, obviously we can use our traditional Python language or any\nour traditional Python language or any\nour traditional Python language or any programming language, it's not a big\nprogramming language, it's not a big\nprogramming language, it's not a big deal. But in order to integrate with AI\ndeal. But in order to integrate with AI\ndeal. But in order to integrate with AI models, those LLM tool calling\nmodels, those LLM tool calling\nmodels, those LLM tool calling capabilities, all those things, we need\ncapabilities, all those things, we need\ncapabilities, all those things, we need to write our own package, right? every\nto write our own package, right? every\nto write our own package, right? every time let's say if I am um if I'm just\ntime let's say if I am um if I'm just\ntime let's say if I am um if I'm just trying to build this particular logic I\ntrying to build this particular logic I\ntrying to build this particular logic I know that I will be just reusing my code\nknow that I will be just reusing my code\nknow that I will be just reusing my code again and again so let's build a package\nagain and again so let's build a package\nagain and again so let's build a package so I will build my package you will say\nso I will build my package you will say\nso I will build my package you will say you you will say the same thing you will\nyou you will say the same thing you will\nyou you will say the same thing you will build your own package then third person\nbuild your own package then third person\nbuild your own package then third person will say the person will build the build\nwill say the person will build the build\nwill say the person will build the build his own package so now we have some\nhis own package so now we have some\nhis own package so now we have some solutions in the industry very popular\nsolutions in the industry very popular\nsolutions in the industry very popular ones are like lang chain okay we have\nones are like lang chain okay we have\nones are like lang chain okay we have one to two very popular more we have\none to two very popular more we have\none to two very popular more we have llama index we have I think there's one\nllama index we have I think there's one\nllama index we have I think there's one to two more like there are so But these\nto two more like there are so But these\nto two more like there are so But these are the popular and trusted ones that\nare the popular and trusted ones that\nare the popular and trusted ones that you should use. So, Langchen is the most\nyou should use. So, Langchen is the most\nyou should use. So, Langchen is the most popular one right now that everyone is\npopular one right now that everyone is\npopular one right now that everyone is talking about because it is open source\ntalking about because it is open source\ntalking about because it is open source plus it is the most mature I would say\nplus it is the most mature I would say\nplus it is the most mature I would say um framework plus it is well integrated\num framework plus it is well integrated\num framework plus it is well integrated with all of the tools and technologies.\nwith all of the tools and technologies.\nwith all of the tools and technologies. Wow, this is amazing. So now let's talk\nWow, this is amazing. So now let's talk\nWow, this is amazing. So now let's talk about the lang. Now what is lang chain?\nabout the lang. Now what is lang chain?\nabout the lang. Now what is lang chain? Lang chain under the hood I would say.\nLang chain under the hood.\nLang chain under the hood. Okay, lang chain. I know the name is\nOkay, lang chain. I know the name is\nOkay, lang chain. I know the name is funny but it is really really cool. I\nfunny but it is really really cool. I\nfunny but it is really really cool. I still remember like when I was learning\nstill remember like when I was learning\nstill remember like when I was learning lang chain I would say years back it was\nlang chain I would say years back it was\nlang chain I would say years back it was like really really cool and it was\nlike really really cool and it was\nlike really really cool and it was really new and after that we have got so\nreally new and after that we have got so\nreally new and after that we have got so so so many new new things. But yeah,\nso so many new new things. But yeah,\nso so many new new things. But yeah, let's talk about lang chain because lang\nlet's talk about lang chain because lang\nlet's talk about lang chain because lang chain was the early mover of the\nchain was the early mover of the\nchain was the early mover of the AI agentic or basically you can say\nAI agentic or basically you can say\nAI agentic or basically you can say agentic AI and AI agents industry.\nYeah, stay hydrated even if it is cold.\nYeah, stay hydrated even if it is cold. So lang chain under the hood. Now what\nSo lang chain under the hood. Now what\nSo lang chain under the hood. Now what is lang chain? Now\nlet's talk about lang chain. So let's\nlet's talk about lang chain. So let's say you have something within your code\nsay you have something within your code\nsay you have something within your code right you want to build this particular\nright you want to build this particular\nright you want to build this particular thing if you are a data engineer you\nthing if you are a data engineer you\nthing if you are a data engineer you would have built something similar to\nwould have built something similar to\nwould have built something similar to this using your Python functions you\nthis using your Python functions you\nthis using your Python functions you would have built it right it's not a big\nwould have built it right it's not a big\nwould have built it right it's not a big deal because you are a data engineer you\ndeal because you are a data engineer you\ndeal because you are a data engineer you are a data professional you know already\nare a data professional you know already\nare a data professional you know already Python\nPython\nPython right let's say you built this okay\nright let's say you built this okay\nright let's say you built this okay where is Rahul let's bring that bring\nwhere is Rahul let's bring that bring\nwhere is Rahul let's bring that bring that Rahul let's say this Rahul\nthat Rahul let's say this Rahul\nthat Rahul let's say this Rahul I'm making Rahul famous right So now we\nI'm making Rahul famous right So now we\nI'm making Rahul famous right So now we need to build a framework. Okay, I\nneed to build a framework. Okay, I\nneed to build a framework. Okay, I basically I need to build the agentic AI\nbasically I need to build the agentic AI\nbasically I need to build the agentic AI solution,\nright? And you know what we need to\nright? And you know what we need to build and let me just bring this thing\nbuild and let me just bring this thing\nbuild and let me just bring this thing as well like it will make more sense.\nas well like it will make more sense.\nas well like it will make more sense. Let's say I want to build this,\nLet's say I want to build this,\nLet's say I want to build this, right? Let's say we want to build this.\nright? Let's say we want to build this.\nright? Let's say we want to build this. Our goal is this one\nOur goal is this one\nOur goal is this one build this Rahul.\nbuild this Rahul.\nbuild this Rahul. So now this Rahul can build this this\nSo now this Rahul can build this this\nSo now this Rahul can build this this entire thing. It's not a big deal for\nentire thing. It's not a big deal for\nentire thing. It's not a big deal for Rahul because Rahul is a pro coder even\nRahul because Rahul is a pro coder even\nRahul because Rahul is a pro coder even not a procoder but he can build this\nnot a procoder but he can build this\nnot a procoder but he can build this using something called as let's say\nusing something called as let's say\nusing something called as let's say Python\nPython\nPython right because Python is like amazing\nright because Python is like amazing\nright because Python is like amazing language I love Python and you should\nlanguage I love Python and you should\nlanguage I love Python and you should also love Python if you're data\nalso love Python if you're data\nalso love Python if you're data engineer. So let's say you are building\nengineer. So let's say you are building\nengineer. So let's say you are building this thing right simple\nthis thing right simple\nthis thing right simple perfect so this Rahul can build this\nperfect so this Rahul can build this\nperfect so this Rahul can build this thing using Python okay\nthing using Python okay\nthing using Python okay makes sense\nmakes sense\nmakes sense okay using Python but now what will\nokay using Python but now what will\nokay using Python but now what will happen what will happen all of these\nhappen what will happen all of these\nhappen what will happen all of these code cells all of these LLM tool calls\ncode cells all of these LLM tool calls\ncode cells all of these LLM tool calls um integration with the tools and then\num integration with the tools and then\num integration with the tools and then just making it as an AI agent and then\njust making it as an AI agent and then\njust making it as an AI agent and then orchestrating it it will take a lot of\norchestrating it it will take a lot of\norchestrating it it will take a lot of time Right?\ntime Right?\ntime Right? Everything is manual.\nEverything is manual.\nEverything is manual. That means everything is manual. That\nThat means everything is manual. That\nThat means everything is manual. That means Rahul\nmeans Rahul\nmeans Rahul needs to\nneeds to\nneeds to build\nbuild\nbuild all the classes. Now you will say bro\nall the classes. Now you will say bro\nall the classes. Now you will say bro why Rahul needs to build the classes? He\nwhy Rahul needs to build the classes? He\nwhy Rahul needs to build the classes? He will simply write the code because Rahul\nwill simply write the code because Rahul\nwill simply write the code because Rahul is working in an organization right and\nis working in an organization right and\nis working in an organization right and in in that organization there are lots\nin in that organization there are lots\nin in that organization there are lots of developers who are working. So in\nof developers who are working. So in\nof developers who are working. So in order to use that code, we need to build\norder to use that code, we need to build\norder to use that code, we need to build the classes, right? And I hope that you\nthe classes, right? And I hope that you\nthe classes, right? And I hope that you have some basic understanding of Python\nhave some basic understanding of Python\nhave some basic understanding of Python like Python class, Python object. If\nlike Python class, Python object. If\nlike Python class, Python object. If not, bro, just go and just learn Python\nnot, bro, just go and just learn Python\nnot, bro, just go and just learn Python as soon as possible. And if you want to\nas soon as possible. And if you want to\nas soon as possible. And if you want to learn from me, if you want to learn\nlearn from me, if you want to learn\nlearn from me, if you want to learn Python from me, I have a dedicated\nPython from me, I have a dedicated\nPython from me, I have a dedicated Python course on my channel. And you can\nPython course on my channel. And you can\nPython course on my channel. And you can just simply search on YouTube An Lamba\njust simply search on YouTube An Lamba\njust simply search on YouTube An Lamba Python full course and you will get the\nPython full course and you will get the\nPython full course and you will get the Python full course and just make your\nPython full course and just make your\nPython full course and just make your Python skills or take your Python skills\nPython skills or take your Python skills\nPython skills or take your Python skills uh through the roof because it is really\nuh through the roof because it is really\nuh through the roof because it is really important. So Rahul needs to build all\nimportant. So Rahul needs to build all\nimportant. So Rahul needs to build all the classes from scratch. Right\nthe classes from scratch. Right\nthe classes from scratch. Right now I have a solution\nnow I have a solution\nnow I have a solution or\nor\nor Rahul can use a framework.\ncalled langj.\ncalled langj. Make sense?\nMake sense?\nMake sense? Now in that particular framework all the\nNow in that particular framework all the\nNow in that particular framework all the classes are built. Obviously there's not\nclasses are built. Obviously there's not\nclasses are built. Obviously there's not like very specific use cases as per your\nlike very specific use cases as per your\nlike very specific use cases as per your organization. Obviously you need to make\norganization. Obviously you need to make\norganization. Obviously you need to make so many changes like so many tweaks so\nso many changes like so many tweaks so\nso many changes like so many tweaks so many things but it is very much\nmany things but it is very much\nmany things but it is very much efficient. Why? Because all the test\nefficient. Why? Because all the test\nefficient. Why? Because all the test cases all the error handling all of\ncases all the error handling all of\ncases all the error handling all of those things boilerplate code everything\nthose things boilerplate code everything\nthose things boilerplate code everything is gone. It will save us a lot of time.\nis gone. It will save us a lot of time.\nis gone. It will save us a lot of time. So Rahul will take this approach.\nSo Rahul will take this approach.\nSo Rahul will take this approach. Okay. Rahul will take this approach\nOkay. Rahul will take this approach\nOkay. Rahul will take this approach because Rahul is smart.\nbecause Rahul is smart.\nbecause Rahul is smart. So now lang chain is what langchain is\nSo now lang chain is what langchain is\nSo now lang chain is what langchain is doing. So, Langjain\nhas built the Python classes\nhas built the Python classes for your agentic solutions\nfor your agentic solutions\nfor your agentic solutions that\nthat\nthat you can use anytime or you can say that\nyou can use anytime or you can say that\nyou can use anytime or you can say that you can use and\nyou can use and\nyou can use and modify\nmodify\nmodify as per your\nas per your\nas per your use cases. Simple. This is lang chain\nuse cases. Simple. This is lang chain\nuse cases. Simple. This is lang chain under the hood. Nothing special. Nothing\nunder the hood. Nothing special. Nothing\nunder the hood. Nothing special. Nothing special. It is not magic. It is not\nspecial. It is not magic. It is not\nspecial. It is not magic. It is not actually no magic. You can write the\nactually no magic. You can write the\nactually no magic. You can write the exact code that Langchain has built. All\nexact code that Langchain has built. All\nexact code that Langchain has built. All the classes you can build on your own as\nthe classes you can build on your own as\nthe classes you can build on your own as well. But the good thing is Langchain\nwell. But the good thing is Langchain\nwell. But the good thing is Langchain has built all those classes for you. So\nhas built all those classes for you. So\nhas built all those classes for you. So that you do not need to build everything\nthat you do not need to build everything\nthat you do not need to build everything from scratch again.\nfrom scratch again.\nfrom scratch again. And I hope it makes sense.\nAnd I hope it makes sense.\nAnd I hope it makes sense. It makes sense to me. Yes.\nIt makes sense to me. Yes.\nIt makes sense to me. Yes. Okay. So this is lanchin under the hood.\nOkay. So this is lanchin under the hood.\nOkay. So this is lanchin under the hood. So that means we are simply trying to\nSo that means we are simply trying to\nSo that means we are simply trying to learn lang chain from the Python point\nlearn lang chain from the Python point\nlearn lang chain from the Python point of view. There are like type I think\nof view. There are like type I think\nof view. There are like type I think TypeScript or JavaScript support as well\nTypeScript or JavaScript support as well\nTypeScript or JavaScript support as well but we are simply learning from Python\nbut we are simply learning from Python\nbut we are simply learning from Python point of view. Make sense? So let's help\npoint of view. Make sense? So let's help\npoint of view. Make sense? So let's help Rahul to build all of these things and\nRahul to build all of these things and\nRahul to build all of these things and we know that Rahul doesn't know anything\nwe know that Rahul doesn't know anything\nwe know that Rahul doesn't know anything about Langchain because Rahul is new to\nabout Langchain because Rahul is new to\nabout Langchain because Rahul is new to Langchain. So we have a cool\nLangchain. So we have a cool\nLangchain. So we have a cool documentation about lang chain and let\ndocumentation about lang chain and let\ndocumentation about lang chain and let me just bring that lang chain\nme just bring that lang chain\nme just bring that lang chain documentation\ndocumentation\ndocumentation and this is the official documentation\nand this is the official documentation\nand this is the official documentation for lang chain that they keep on\nfor lang chain that they keep on\nfor lang chain that they keep on updating and see python and typescript.\nupdating and see python and typescript.\nupdating and see python and typescript. So we have both capabilities. I will\nSo we have both capabilities. I will\nSo we have both capabilities. I will simply say lang chain python and this is\nsimply say lang chain python and this is\nsimply say lang chain python and this is the documentation of it. See we have\nthe documentation of it. See we have\nthe documentation of it. See we have everything here. We have installation,\neverything here. We have installation,\neverything here. We have installation, quick start, change log and blah blah\nquick start, change log and blah blah\nquick start, change log and blah blah blah. And then we have agents.\nblah. And then we have agents.\nblah. And then we have agents. Very long documentation, very very\nVery long documentation, very very\nVery long documentation, very very detailed documentation and I love it to\ndetailed documentation and I love it to\ndetailed documentation and I love it to be honest and you can also go and check\nbe honest and you can also go and check\nbe honest and you can also go and check it out. But yes, initially Rahul wants\nit out. But yes, initially Rahul wants\nit out. But yes, initially Rahul wants quick overview, quick tutorial so that\nquick overview, quick tutorial so that\nquick overview, quick tutorial so that Rahul can start building using lang\nRahul can start building using lang\nRahul can start building using lang chain and sideways Rahul can also\nchain and sideways Rahul can also\nchain and sideways Rahul can also explore the documentation but Rahul\nexplore the documentation but Rahul\nexplore the documentation but Rahul wants some guidance right some\nwants some guidance right some\nwants some guidance right some structured approach because obviously\nstructured approach because obviously\nstructured approach because obviously when I was also learning lang chain\nwhen I was also learning lang chain\nwhen I was also learning lang chain instead of directly going to the\ninstead of directly going to the\ninstead of directly going to the documentation I also like was studying\ndocumentation I also like was studying\ndocumentation I also like was studying through so many sources like books um\nthrough so many sources like books um\nthrough so many sources like books um playlist courses and then blogs and then\nplaylist courses and then blogs and then\nplaylist courses and then blogs and then documentation like everything because I\ndocumentation like everything because I\ndocumentation like everything because I like I I'm not I will not say like there\nlike I I'm not I will not say like there\nlike I I'm not I will not say like there was no structured learning available or\nwas no structured learning available or\nwas no structured learning available or video available. Um I just wanted to\nvideo available. Um I just wanted to\nvideo available. Um I just wanted to learn on my own like how things are\nlearn on my own like how things are\nlearn on my own like how things are working and then I realized let me just\nworking and then I realized let me just\nworking and then I realized let me just pass on this knowledge of all the\npass on this knowledge of all the\npass on this knowledge of all the sources. It is like a condensed form of\nsources. It is like a condensed form of\nsources. It is like a condensed form of knowledge which is you can say the\nknowledge which is you can say the\nknowledge which is you can say the integration or condensed um curated\nintegration or condensed um curated\nintegration or condensed um curated knowledge of so many sources like blogs,\nknowledge of so many sources like blogs,\nknowledge of so many sources like blogs, courses, videos, documentation. So this\ncourses, videos, documentation. So this\ncourses, videos, documentation. So this is like one-stop solution for you to\nis like one-stop solution for you to\nis like one-stop solution for you to master lang chain and literally master.\nmaster lang chain and literally master.\nmaster lang chain and literally master. Let me be very honest because you will\nLet me be very honest because you will\nLet me be very honest because you will be able to just build anything using\nbe able to just build anything using\nbe able to just build anything using langun after this. Okay. So let me just\nlangun after this. Okay. So let me just\nlangun after this. Okay. So let me just start taking you to the practical lab\nstart taking you to the practical lab\nstart taking you to the practical lab session because enough concept enough\nsession because enough concept enough\nsession because enough concept enough thing is built and we will be just\nthing is built and we will be just\nthing is built and we will be just learning so many things in the meantime\nlearning so many things in the meantime\nlearning so many things in the meantime as we go along with this video. Okay.\nas we go along with this video. Okay.\nas we go along with this video. Okay. And I will just provide this notes as\nAnd I will just provide this notes as\nAnd I will just provide this notes as well. Everything will be available. So\nwell. Everything will be available. So\nwell. Everything will be available. So do not need to worry writing down and\ndo not need to worry writing down and\ndo not need to worry writing down and all. If you want to write it down it's\nall. If you want to write it down it's\nall. If you want to write it down it's good but I will just provide it as well.\ngood but I will just provide it as well.\ngood but I will just provide it as well. Okay. So let's see how we can just get\nOkay. So let's see how we can just get\nOkay. So let's see how we can just get started with lank chain. Okay let's see.\nstarted with lank chain. Okay let's see.\nstarted with lank chain. Okay let's see. So now let's talk about the prerequests\nSo now let's talk about the prerequests\nSo now let's talk about the prerequests not from the skill wise because it is\nnot from the skill wise because it is\nnot from the skill wise because it is like very very very basic and very\nlike very very very basic and very\nlike very very very basic and very beginner friendly. You just need like\nbeginner friendly. You just need like\nbeginner friendly. You just need like basic Python knowledge and that's it.\nbasic Python knowledge and that's it.\nbasic Python knowledge and that's it. I'm talking about the system\nI'm talking about the system\nI'm talking about the system requirements. So you need first of all\nrequirements. So you need first of all\nrequirements. So you need first of all code editor or basically IDE. It can be\ncode editor or basically IDE. It can be\ncode editor or basically IDE. It can be VS code.\nVS code.\nVS code. It can be anti-gravity if you're using\nIt can be anti-gravity if you're using\nIt can be anti-gravity if you're using anti-gravity which is launched by\nanti-gravity which is launched by\nanti-gravity which is launched by Google. There are so many code editors.\nGoogle. There are so many code editors.\nGoogle. There are so many code editors. You can use whatever you want. Okay. And\nYou can use whatever you want. Okay. And\nYou can use whatever you want. Okay. And we would need Python as well. So let's\nwe would need Python as well. So let's\nwe would need Python as well. So let's say\nsay\nsay system\nsystem\nsystem system requirements not system I would\nsystem requirements not system I would\nsystem requirements not system I would say\nsay\nsay softwares we'll be using\nsoftwares we'll be using\nsoftwares we'll be using simple. So we will be using some cool\nsimple. So we will be using some cool\nsimple. So we will be using some cool softwares.\nsoftwares.\nsoftwares. Cool softwares.\nSo software will be first of all you\nSo software will be first of all you need VS code or anyone like if you like\nneed VS code or anyone like if you like\nneed VS code or anyone like if you like VS code I love VS code I love so many\nVS code I love VS code I love so many\nVS code I love VS code I love so many new new AI code editors code editors as\nnew new AI code editors code editors as\nnew new AI code editors code editors as well because I keep on playing with so\nwell because I keep on playing with so\nwell because I keep on playing with so many code editors so I just keep\nmany code editors so I just keep\nmany code editors so I just keep switching okay you can also try a lot of\nswitching okay you can also try a lot of\nswitching okay you can also try a lot of code editors and then we will be needing\ncode editors and then we will be needing\ncode editors and then we will be needing Python. So if you do not have Python\nPython. So if you do not have Python\nPython. So if you do not have Python installed on your system that I would\ninstalled on your system that I would\ninstalled on your system that I would assume that you should install\nassume that you should install\nassume that you should install okay and you would need one more thing\nokay and you would need one more thing\nokay and you would need one more thing get now why get because let's say you're\nget now why get because let's say you're\nget now why get because let's say you're building something and you need to push\nbuilding something and you need to push\nbuilding something and you need to push that code in your repo you would need\nthat code in your repo you would need\nthat code in your repo you would need git right so if you do not have these\ngit right so if you do not have these\ngit right so if you do not have these things available in your laptop first of\nthings available in your laptop first of\nthings available in your laptop first of all slow collapse and very well done tap\nall slow collapse and very well done tap\nall slow collapse and very well done tap your back very good developer you are so\nyour back very good developer you are so\nyour back very good developer you are so you can just download all of these\nyou can just download all of these\nyou can just download all of these things first of all just go to any\nthings first of all just go to any\nthings first of all just go to any browser search VS code download\nbrowser search VS code download\nbrowser search VS code download and this is the visual called download\nand this is the visual called download\nand this is the visual called download and you should download it. Okay, this\nand you should download it. Okay, this\nand you should download it. Okay, this one, this one or this one. Then once you\none, this one or this one. Then once you\none, this one or this one. Then once you have this, you can download Python\nand Python.org or you can simply say\nand Python.org or you can simply say Python download just to directly land on\nPython download just to directly land on\nPython download just to directly land on that page. Perfect. So here you can\nthat page. Perfect. So here you can\nthat page. Perfect. So here you can simply download Python install manager.\nsimply download Python install manager.\nsimply download Python install manager. It will download the latest one which is\nIt will download the latest one which is\nIt will download the latest one which is 3.14.\n3.14.\n3.14. And here you can see all the active\nAnd here you can see all the active\nAnd here you can see all the active Python release. By the way, this is\nPython release. By the way, this is\nPython release. By the way, this is cool. Like I was not expecting this. So\ncool. Like I was not expecting this. So\ncool. Like I was not expecting this. So currently most of the people are using\ncurrently most of the people are using\ncurrently most of the people are using 3.12 most of the people and you can see\n3.12 most of the people and you can see\n3.12 most of the people and you can see that this has a lifpan of almost like it\nthat this has a lifpan of almost like it\nthat this has a lifpan of almost like it has security around 2028. So that is\nhas security around 2028. So that is\nhas security around 2028. So that is fine. I am also using 3.13 but currently\nfine. I am also using 3.13 but currently\nfine. I am also using 3.13 but currently it is under bug fix. So there are so\nit is under bug fix. So there are so\nit is under bug fix. So there are so many things that are breaking or that\nmany things that are breaking or that\nmany things that are breaking or that can break. So that is not a very good\ncan break. So that is not a very good\ncan break. So that is not a very good time period to try this. And for 3.14 it\ntime period to try this. And for 3.14 it\ntime period to try this. And for 3.14 it is totally new. So I will just simply\nis totally new. So I will just simply\nis totally new. So I will just simply recommend you to download 3.12. And in\nrecommend you to download 3.12. And in\nrecommend you to download 3.12. And in order to download this, you can just go\norder to download this, you can just go\norder to download this, you can just go here and you will see all of these um\nhere and you will see all of these um\nhere and you will see all of these um packages installed. So you will see it\npackages installed. So you will see it\npackages installed. So you will see it here. No installers. Um\nhere. No installers. Um\nhere. No installers. Um you can just go specifically on Wait,\nyou can just go specifically on Wait,\nyou can just go specifically on Wait, really? Should you not this one?\nreally? Should you not this one?\nreally? Should you not this one? Yeah, you can just search it here. So\nYeah, you can just search it here. So\nYeah, you can just search it here. So you can just scroll it here and it\nyou can just scroll it here and it\nyou can just scroll it here and it should be 3.12 something. Maybe 3.1212.\nshould be 3.12 something. Maybe 3.1212.\nshould be 3.12 something. Maybe 3.1212. Uh it also doesn't have any kind of\nUh it also doesn't have any kind of\nUh it also doesn't have any kind of installers. 3.1 2.3.\ninstallers. 3.1 2.3.\ninstallers. 3.1 2.3. Uh do does it have any kind of\nUh do does it have any kind of\nUh do does it have any kind of installers? Like you can just check if\ninstallers? Like you can just check if\ninstallers? Like you can just check if you have Yeah, it has installers. So if\nyou have Yeah, it has installers. So if\nyou have Yeah, it has installers. So if yeah, Windows 64 or Mac whatever. So you\nyeah, Windows 64 or Mac whatever. So you\nyeah, Windows 64 or Mac whatever. So you should download 3.12 Python because it\nshould download 3.12 Python because it\nshould download 3.12 Python because it is the most stable version available\nis the most stable version available\nis the most stable version available right now. 3.13 is also good but 3.12 is\nright now. 3.13 is also good but 3.12 is\nright now. 3.13 is also good but 3.12 is better. Then you need to download git.\nbetter. Then you need to download git.\nbetter. Then you need to download git. Simply say get install\nSimply say get install\nSimply say get install and click here. And here you can just\nand click here. And here you can just\nand click here. And here you can just choose your you can say provider and you\nchoose your you can say provider and you\nchoose your you can say provider and you will simply download it from here from\nwill simply download it from here from\nwill simply download it from here from Mac from Linux and whatever. Download\nMac from Linux and whatever. Download\nMac from Linux and whatever. Download these three things and let's get\nthese three things and let's get\nthese three things and let's get started. So for this particular video I\nstarted. So for this particular video I\nstarted. So for this particular video I will be using anti-gravity. Anti-gravity\nwill be using anti-gravity. Anti-gravity\nwill be using anti-gravity. Anti-gravity is a new code editor. If you don't know\nis a new code editor. If you don't know\nis a new code editor. If you don't know about it it is by Google and they have\nabout it it is by Google and they have\nabout it it is by Google and they have amazing amazing amazing free features\namazing amazing amazing free features\namazing amazing amazing free features available. So if I go here anti-gravity\navailable. So if I go here anti-gravity\navailable. So if I go here anti-gravity IDE\nIDE\nIDE see this is the one you can download\nsee this is the one you can download\nsee this is the one you can download this on your system and you will love it\nthis on your system and you will love it\nthis on your system and you will love it okay because this is like a gentic code\nokay because this is like a gentic code\nokay because this is like a gentic code editor that you can just use within your\neditor that you can just use within your\neditor that you can just use within your codebase and it is amazing okay so let's\ncodebase and it is amazing okay so let's\ncodebase and it is amazing okay so let's get started with our setup on the system\nget started with our setup on the system\nget started with our setup on the system okay let's see so welcome to my\nokay let's see so welcome to my\nokay let's see so welcome to my anti-gravity code editor and as you can\nanti-gravity code editor and as you can\nanti-gravity code editor and as you can see this is the interface very\nsee this is the interface very\nsee this is the interface very straightforward about very simple very\nstraightforward about very simple very\nstraightforward about very simple very similar to VS code and obviously\nsimilar to VS code and obviously\nsimilar to VS code and obviously everyone loves VS code so it is also\neveryone loves VS code so it is also\neveryone loves VS code so it is also having the same UI everything maybe it\nhaving the same UI everything maybe it\nhaving the same UI everything maybe it is built on top of VS code because VS\nis built on top of VS code because VS\nis built on top of VS code because VS code is open source so here you will see\ncode is open source so here you will see\ncode is open source so here you will see this is the agentic area by the way you\nthis is the agentic area by the way you\nthis is the agentic area by the way you can collapse it by clicking here and\ncan collapse it by clicking here and\ncan collapse it by clicking here and we'll be using throughout the project or\nwe'll be using throughout the project or\nwe'll be using throughout the project or throughout the tutorial don't worry and\nthroughout the tutorial don't worry and\nthroughout the tutorial don't worry and this is the folder that I have created\nthis is the folder that I have created\nthis is the folder that I have created lang chain tutorial you can also create\nlang chain tutorial you can also create\nlang chain tutorial you can also create this folder and as you can see this\nthis folder and as you can see this\nthis folder and as you can see this tutorial is empty this tutorial this\ntutorial is empty this tutorial this\ntutorial is empty this tutorial this this tutorial is not empty. This\nthis tutorial is not empty. This\nthis tutorial is not empty. This tutorial folder is empty. Okay. So, as\ntutorial folder is empty. Okay. So, as\ntutorial folder is empty. Okay. So, as you can see, you can also create it.\nyou can see, you can also create it.\nyou can see, you can also create it. Make sense? And how to do that? I think\nMake sense? And how to do that? I think\nMake sense? And how to do that? I think you you should know how to create a\nyou you should know how to create a\nyou you should know how to create a folder, right? And then you can just\nfolder, right? And then you can just\nfolder, right? And then you can just open it. Go to file, click on open\nopen it. Go to file, click on open\nopen it. Go to file, click on open folder, and then open that empty folder.\nfolder, and then open that empty folder.\nfolder, and then open that empty folder. Simple. It will just pop up like this.\nSimple. It will just pop up like this.\nSimple. It will just pop up like this. Do not worry about these things because\nDo not worry about these things because\nDo not worry about these things because this can be different in your case\nthis can be different in your case\nthis can be different in your case because I have downloaded some\nbecause I have downloaded some\nbecause I have downloaded some extensions. Let's say DBT and there are\nextensions. Let's say DBT and there are\nextensions. Let's say DBT and there are so many extensions that I downloaded.\nso many extensions that I downloaded.\nso many extensions that I downloaded. So, just ignore. So um I would say\nit is any gravity and this is not very\nit is any gravity and this is not very very complex to explain you. So this is\nvery complex to explain you. So this is\nvery complex to explain you. So this is our uh you can say code editor that we\nour uh you can say code editor that we\nour uh you can say code editor that we are trying to use. Perfect. So without\nare trying to use. Perfect. So without\nare trying to use. Perfect. So without wasting any time let me just initiate my\nwasting any time let me just initiate my\nwasting any time let me just initiate my particularly part particularly\nparticularly part particularly\nparticularly part particularly particular project. So I will open one\nparticular project. So I will open one\nparticular project. So I will open one terminal. In order to open a terminal in\nterminal. In order to open a terminal in\nterminal. In order to open a terminal in anti-gravity, you can click here in the\nanti-gravity, you can click here in the\nanti-gravity, you can click here in the terminal and new terminal or I can use\nterminal and new terminal or I can use\nterminal and new terminal or I can use the um shortcut control shift and back\nthe um shortcut control shift and back\nthe um shortcut control shift and back tech. Okay, choice is yours. So this is\ntech. Okay, choice is yours. So this is\ntech. Okay, choice is yours. So this is my terminal. So what is the first thing\nmy terminal. So what is the first thing\nmy terminal. So what is the first thing we do whenever we just initiate anything\nwe do whenever we just initiate anything\nwe do whenever we just initiate anything for especially for Python? The answer is\nfor especially for Python? The answer is\nfor especially for Python? The answer is creating the virtual environment. Common\ncreating the virtual environment. Common\ncreating the virtual environment. Common sense. So now let's create the virtual\nsense. So now let's create the virtual\nsense. So now let's create the virtual environment. And you know what? We will\nenvironment. And you know what? We will\nenvironment. And you know what? We will not be using um pip anala. We have to\nnot be using um pip anala. We have to\nnot be using um pip anala. We have to use pip. No, we do not have to. We have\nuse pip. No, we do not have to. We have\nuse pip. No, we do not have to. We have something called as UV. If you do not\nsomething called as UV. If you do not\nsomething called as UV. If you do not know about UV, I think you should know\nknow about UV, I think you should know\nknow about UV, I think you should know about UV because UV is the modern Python\nabout UV because UV is the modern Python\nabout UV because UV is the modern Python package manager and we can just do\npackage manager and we can just do\npackage manager and we can just do everything with UV that we used to do\neverything with UV that we used to do\neverything with UV that we used to do with pip. How you can also get started\nwith pip. How you can also get started\nwith pip. How you can also get started with it in just one line. You simply\nwith it in just one line. You simply\nwith it in just one line. You simply need to say pip install uv and hit\nneed to say pip install uv and hit\nneed to say pip install uv and hit enter. In my case, I have already\nenter. In my case, I have already\nenter. In my case, I have already downloaded it. So it will simply say\ndownloaded it. So it will simply say\ndownloaded it. So it will simply say requirement already satisfied. In your\nrequirement already satisfied. In your\nrequirement already satisfied. In your case, it will say like it is downloaded.\ncase, it will say like it is downloaded.\ncase, it will say like it is downloaded. Okay. Once you have pip like UV\nOkay. Once you have pip like UV\nOkay. Once you have pip like UV installed in your system then you can\ninstalled in your system then you can\ninstalled in your system then you can just start using UV and it is so\njust start using UV and it is so\njust start using UV and it is so convenient. I will just show you in\nconvenient. I will just show you in\nconvenient. I will just show you in order to initiate a project and creating\norder to initiate a project and creating\norder to initiate a project and creating the virtual environment. You do not need\nthe virtual environment. You do not need\nthe virtual environment. You do not need to write multiple commands. You will\nto write multiple commands. You will\nto write multiple commands. You will simply say UV in it and hit enter.\nsimply say UV in it and hit enter.\nsimply say UV in it and hit enter. Perfect. It has created the entire\nPerfect. It has created the entire\nPerfect. It has created the entire project for me. dot get ignore python\nproject for me. dot get ignore python\nproject for me. dot get ignore python version main.py py pyro project.tomal\nversion main.py py pyro project.tomal\nversion main.py py pyro project.tomal automal readme.md\nautomal readme.md\nautomal readme.md make sense very good now in this python\nmake sense very good now in this python\nmake sense very good now in this python version if in your system you already\nversion if in your system you already\nversion if in your system you already have let's say 3.13 installed or let's\nhave let's say 3.13 installed or let's\nhave let's say 3.13 installed or let's say 3.11 installed and you want to use\nsay 3.11 installed and you want to use\nsay 3.11 installed and you want to use 3.12 just for this tutorial you can\n3.12 just for this tutorial you can\n3.12 just for this tutorial you can simply write it here as I have written\nsimply write it here as I have written\nsimply write it here as I have written 3.12 like I have not written it u has\n3.12 like I have not written it u has\n3.12 like I have not written it u has written it for me but you can just\nwritten it for me but you can just\nwritten it for me but you can just change it here okay and what to do after\nchange it here okay and what to do after\nchange it here okay and what to do after that hold on I will just tell you just\nthat hold on I will just tell you just\nthat hold on I will just tell you just change it here for now just write 3.1 12\nchange it here for now just write 3.1 12\nchange it here for now just write 3.1 12 simple good now in order to initiate the\nsimple good now in order to initiate the\nsimple good now in order to initiate the virtual enrollment we do not need to\nvirtual enrollment we do not need to\nvirtual enrollment we do not need to write any special command by the way we\nwrite any special command by the way we\nwrite any special command by the way we have a command in UV called let's say\nhave a command in UV called let's say\nhave a command in UV called let's say UVNV then dot VNV but I can also say UV\nUVNV then dot VNV but I can also say UV\nUVNV then dot VNV but I can also say UV sync this command is a very special\nsync this command is a very special\nsync this command is a very special command and the combination of commands\ncommand and the combination of commands\ncommand and the combination of commands it will simply sync everything if I have\nit will simply sync everything if I have\nit will simply sync everything if I have written 3.12 or even let's say if I\nwritten 3.12 or even let's say if I\nwritten 3.12 or even let's say if I write 3.11 or whatever version it will\nwrite 3.11 or whatever version it will\nwrite 3.11 or whatever version it will take that Python version it will create\ntake that Python version it will create\ntake that Python version it will create the virtual environment for me. Two\nthe virtual environment for me. Two\nthe virtual environment for me. Two things and the third thing is whatever\nthings and the third thing is whatever\nthings and the third thing is whatever dependencies I have it will install all\ndependencies I have it will install all\ndependencies I have it will install all of those dependencies. But An Lama, do\nof those dependencies. But An Lama, do\nof those dependencies. But An Lama, do you know what? You do not even have\nyou know what? You do not even have\nyou know what? You do not even have requirements dot requirements.txt.\nrequirements dot requirements.txt.\nrequirements dot requirements.txt. How will you install the dependencies?\nHow will you install the dependencies?\nHow will you install the dependencies? When we work with UV, we do not need to\nWhen we work with UV, we do not need to\nWhen we work with UV, we do not need to worry about requirements. TXT because we\nworry about requirements. TXT because we\nworry about requirements. TXT because we work with pyro.l.\nwork with pyro.l.\nwork with pyro.l. So if you open this file you will see\nSo if you open this file you will see\nSo if you open this file you will see dependencies list. So whatever package\ndependencies list. So whatever package\ndependencies list. So whatever package you write here automatically it will add\nyou write here automatically it will add\nyou write here automatically it will add all of those dependencies. Okay. So\nall of those dependencies. Okay. So\nall of those dependencies. Okay. So first of all let me just write u sync\nfirst of all let me just write u sync\nfirst of all let me just write u sync and hit enter. Perfect. So now you can\nand hit enter. Perfect. So now you can\nand hit enter. Perfect. So now you can see that it has created VNV. And by the\nsee that it has created VNV. And by the\nsee that it has created VNV. And by the way if you are not seeing this uh get\nway if you are not seeing this uh get\nway if you are not seeing this uh get folder it is fine because it is hidden\nfolder it is fine because it is hidden\nfolder it is fine because it is hidden by default. I have enabled it because I\nby default. I have enabled it because I\nby default. I have enabled it because I just like seeing it here. But it has no\njust like seeing it here. But it has no\njust like seeing it here. But it has no use because we should not touch this\nuse because we should not touch this\nuse because we should not touch this dotgit folder because it is a database\ndotgit folder because it is a database\ndotgit folder because it is a database that it uses under the hood to manage\nthat it uses under the hood to manage\nthat it uses under the hood to manage our git. Okay, you should not touch it\nour git. Okay, you should not touch it\nour git. Okay, you should not touch it and open it and make some changes\nand open it and make some changes\nand open it and make some changes otherwise your codebase will be broken,\notherwise your codebase will be broken,\notherwise your codebase will be broken, right? So yeah, but I have just put it\nright? So yeah, but I have just put it\nright? So yeah, but I have just put it here you so you do not need to worry if\nhere you so you do not need to worry if\nhere you so you do not need to worry if you do not seegget. It has nothing to do\nyou do not seegget. It has nothing to do\nyou do not seegget. It has nothing to do with your git. Okay. And if you just\nwith your git. Okay. And if you just\nwith your git. Okay. And if you just observe in the bottom side, I have the\nobserve in the bottom side, I have the\nobserve in the bottom side, I have the master uh branch as well. That means it\nmaster uh branch as well. That means it\nmaster uh branch as well. That means it has also run the command called get init\nhas also run the command called get init\nhas also run the command called get init automatically in the back end. When I\nautomatically in the back end. When I\nautomatically in the back end. When I write uv in it, it runs get init as\nwrite uv in it, it runs get init as\nwrite uv in it, it runs get init as well. So let me just define my branch\nwell. So let me just define my branch\nwell. So let me just define my branch quickly. Let me just rename my branch\nquickly. Let me just rename my branch\nquickly. Let me just rename my branch and I will say\nand I will say\nand I will say get branch minus m main. If you do not\nget branch minus m main. If you do not\nget branch minus m main. If you do not have git fundamentals clear, do not\nhave git fundamentals clear, do not\nhave git fundamentals clear, do not worry. Just follow this command. It\nworry. Just follow this command. It\nworry. Just follow this command. It renames my branch. So now as you can see\nrenames my branch. So now as you can see\nrenames my branch. So now as you can see my branch name is main. Let's create the\nmy branch name is main. Let's create the\nmy branch name is main. Let's create the initial commit. I will say get uh then\ninitial commit. I will say get uh then\ninitial commit. I will say get uh then add dot okay and then get commit minus m\nadd dot okay and then get commit minus m\nadd dot okay and then get commit minus m initial commit. Basically this is the\ninitial commit. Basically this is the\ninitial commit. Basically this is the initial commit that we are creating for\ninitial commit that we are creating for\ninitial commit that we are creating for this branch so that it will be\nthis branch so that it will be\nthis branch so that it will be registered. Hit enter and that's it. Now\nregistered. Hit enter and that's it. Now\nregistered. Hit enter and that's it. Now we will not be touching this main\nwe will not be touching this main\nwe will not be touching this main branch. Don't worry. Makes sense. If you\nbranch. Don't worry. Makes sense. If you\nbranch. Don't worry. Makes sense. If you do not know about Git, don't worry. Just\ndo not know about Git, don't worry. Just\ndo not know about Git, don't worry. Just copy these two commands and that's it.\ncopy these two commands and that's it.\ncopy these two commands and that's it. Do not behave like oh oh what is this\nDo not behave like oh oh what is this\nDo not behave like oh oh what is this command? So now now we first of all need\ncommand? So now now we first of all need\ncommand? So now now we first of all need to install the lang chain library in\nto install the lang chain library in\nto install the lang chain library in Python. Okay. And how we can just do\nPython. Okay. And how we can just do\nPython. Okay. And how we can just do that? Simply say UV add. Same way you\nthat? Simply say UV add. Same way you\nthat? Simply say UV add. Same way you say pip install. Here we say uv add uv\nsay pip install. Here we say uv add uv\nsay pip install. Here we say uv add uv add lang chain.\nadd lang chain.\nadd lang chain. Okay. Just hit enter.\nOkay. Just hit enter.\nOkay. Just hit enter. It will add all of these libraries and\nIt will add all of these libraries and\nIt will add all of these libraries and dependencies automatically. If I open my\ndependencies automatically. If I open my\ndependencies automatically. If I open my tomal file, I will see all of these\ntomal file, I will see all of these\ntomal file, I will see all of these things are written here. See lang chain\nthings are written here. See lang chain\nthings are written here. See lang chain 1.2.0. Wow, make sense? Good. So, I have\n1.2.0. Wow, make sense? Good. So, I have\n1.2.0. Wow, make sense? Good. So, I have downloaded langchain library. Okay. And\ndownloaded langchain library. Okay. And\ndownloaded langchain library. Okay. And I can also verify it. If I just go to\nI can also verify it. If I just go to\nI can also verify it. If I just go to do, if I go to live, I should see lang.\ndo, if I go to live, I should see lang.\ndo, if I go to live, I should see lang. See, I have all of the things installed\nSee, I have all of the things installed\nSee, I have all of the things installed in my virtual environment. Makes sense.\nin my virtual environment. Makes sense.\nin my virtual environment. Makes sense. Now, let me just enable the virtual\nNow, let me just enable the virtual\nNow, let me just enable the virtual environment.\nenvironment.\nenvironment. dot when\ndot when\ndot when scripts activate VS code automatically\nscripts activate VS code automatically\nscripts activate VS code automatically uh en enables your virtual environment\nuh en enables your virtual environment\nuh en enables your virtual environment that is a plus sign but here we have to\nthat is a plus sign but here we have to\nthat is a plus sign but here we have to do it manually. So our virtual\ndo it manually. So our virtual\ndo it manually. So our virtual environment is enabled. That is a good\nenvironment is enabled. That is a good\nenvironment is enabled. That is a good thing. Okay. So yeah, makes sense. So\nthing. Okay. So yeah, makes sense. So\nthing. Okay. So yeah, makes sense. So now what we going to do? We will be\nnow what we going to do? We will be\nnow what we going to do? We will be creating chapters. Okay. Let me create\ncreating chapters. Okay. Let me create\ncreating chapters. Okay. Let me create our first chapter and let me just name\nour first chapter and let me just name\nour first chapter and let me just name it as chapter one. Um yeah, simply\nit as chapter one. Um yeah, simply\nit as chapter one. Um yeah, simply chapter one. So in this chapter one, we\nchapter one. So in this chapter one, we\nchapter one. So in this chapter one, we will be creating our notebooks like\nwill be creating our notebooks like\nwill be creating our notebooks like right like notebook one, notebook two,\nright like notebook one, notebook two,\nright like notebook one, notebook two, notebook 3 like this. So in this folder,\nnotebook 3 like this. So in this folder,\nnotebook 3 like this. So in this folder, let me just create my first notebook and\nlet me just create my first notebook and\nlet me just create my first notebook and let me just name it as basics dot ipy\nlet me just name it as basics dot ipy\nlet me just name it as basics dot ipy nv. If you're not familiar with ipynv,\nnv. If you're not familiar with ipynv,\nnv. If you're not familiar with ipynv, this is basically the IPI kernel. Okay,\nthis is basically the IPI kernel. Okay,\nthis is basically the IPI kernel. Okay, IPI kernel notebook format which is very\nIPI kernel notebook format which is very\nIPI kernel notebook format which is very similar to Jupyter notebook. Simple. If\nsimilar to Jupyter notebook. Simple. If\nsimilar to Jupyter notebook. Simple. If you do not know about Jupyter notebook,\nyou do not know about Jupyter notebook,\nyou do not know about Jupyter notebook, I don't know what you are doing. Okay,\nI don't know what you are doing. Okay,\nI don't know what you are doing. Okay, you should know about Jupyter notebook,\nyou should know about Jupyter notebook,\nyou should know about Jupyter notebook, bro. You are a data professional. So\nbro. You are a data professional. So\nbro. You are a data professional. So this has created this particular IPY NB.\nthis has created this particular IPY NB.\nthis has created this particular IPY NB. In your case, it will also ask you to\nIn your case, it will also ask you to\nIn your case, it will also ask you to download or install IPI kernel. Simply\ndownload or install IPI kernel. Simply\ndownload or install IPI kernel. Simply say okay and it will install it. And\nsay okay and it will install it. And\nsay okay and it will install it. And first of all, you need to click on\nfirst of all, you need to click on\nfirst of all, you need to click on select kernel and select the kernel\nselect kernel and select the kernel\nselect kernel and select the kernel Python environments and pick your\nPython environments and pick your\nPython environments and pick your virtual environment. Perfect. Because\nvirtual environment. Perfect. Because\nvirtual environment. Perfect. Because only then you'll be able to use the lang\nonly then you'll be able to use the lang\nonly then you'll be able to use the lang chain libraries. Perfect. Now first\nchain libraries. Perfect. Now first\nchain libraries. Perfect. Now first thing first thing that you need to do\nthing first thing that you need to do\nthing first thing that you need to do okay is that particular first thing. Now\nokay is that particular first thing. Now\nokay is that particular first thing. Now you have all of these things ready. Now\nyou have all of these things ready. Now\nyou have all of these things ready. Now do you know what? What you need to do?\ndo you know what? What you need to do?\ndo you know what? What you need to do? You need to go to OpenAI. Do we have\nYou need to go to OpenAI. Do we have\nYou need to go to OpenAI. Do we have OpenAI\nOpenAI\nOpenAI thing? Let's use this one.\nthing? Let's use this one.\nthing? Let's use this one. So now you need to go to OpenAI\nSo now you need to go to OpenAI\nSo now you need to go to OpenAI and\nOpenAI. Why? Because now you need to\nOpenAI. Why? Because now you need to create a kind of API key because you'll\ncreate a kind of API key because you'll\ncreate a kind of API key because you'll be using the hosted models, high quality\nbe using the hosted models, high quality\nbe using the hosted models, high quality models. We will not be using open source\nmodels. We will not be using open source\nmodels. We will not be using open source models because there are so many things\nmodels because there are so many things\nmodels because there are so many things that only hosted models can do and that\nthat only hosted models can do and that\nthat only hosted models can do and that you need to learn as well and that is\nyou need to learn as well and that is\nyou need to learn as well and that is why we will be just covering everything\nwhy we will be just covering everything\nwhy we will be just covering everything with openi models right so how you can\nwith openi models right so how you can\nwith openi models right so how you can just go to openai simply go to browser\njust go to openai simply go to browser\njust go to openai simply go to browser and search openai\nand search openai\nand search openai API simple so this is API this is not\nAPI simple so this is API this is not\nAPI simple so this is API this is not chat GPD by the way this is openai\nchat GPD by the way this is openai\nchat GPD by the way this is openai official\nofficial\nofficial API platform okay click here and here\nAPI platform okay click here and here\nAPI platform okay click here and here you will be landing on this page click\nyou will be landing on this page click\nyou will be landing on this page click on login and you can click here and do\non login and you can click here and do\non login and you can click here and do not select chat GPT select API platform\nnot select chat GPT select API platform\nnot select chat GPT select API platform make sense select API platform because\nmake sense select API platform because\nmake sense select API platform because once you land on this API platform then\nonce you land on this API platform then\nonce you land on this API platform then you also need to create the API key\nyou also need to create the API key\nyou also need to create the API key make sense API key okay let me just show\nmake sense API key okay let me just show\nmake sense API key okay let me just show you my account\nyou my account\nyou my account so this is my API platform account okay\nso this is my API platform account okay\nso this is my API platform account okay so in your case you will land on the\nso in your case you will land on the\nso in your case you will land on the homepage okay and then you simply need\nhomepage okay and then you simply need\nhomepage okay and then you simply need to click on settings and then you need\nto click on settings and then you need\nto click on settings and then you need to click on API keys then you will land\nto click on API keys then you will land\nto click on API keys then you will land on this page and here you can create\non this page and here you can create\non this page and here you can create your new secret key okay click on plus\nyour new secret key okay click on plus\nyour new secret key okay click on plus create new secret key and I can also\ncreate new secret key and I can also\ncreate new secret key and I can also create it right now and I will just show\ncreate it right now and I will just show\ncreate it right now and I will just show you okay not in front of you you will\nyou okay not in front of you you will\nyou okay not in front of you you will use my secret key obviously I will just\nuse my secret key obviously I will just\nuse my secret key obviously I will just revoke it I'm just kidding so yeah you\nrevoke it I'm just kidding so yeah you\nrevoke it I'm just kidding so yeah you can just simply say create new secret\ncan just simply say create new secret\ncan just simply say create new secret key so this will create a secret key\nkey so this will create a secret key\nkey so this will create a secret key like this sky- something something\nlike this sky- something something\nlike this sky- something something something something you need to note it\nsomething something you need to note it\nsomething something you need to note it down somewhere else because it will be\ndown somewhere else because it will be\ndown somewhere else because it will be hidden after that okay once you have the\nhidden after that okay once you have the\nhidden after that okay once you have the secret key ready once you have thing you\nsecret key ready once you have thing you\nsecret key ready once you have thing you need to go to billing okay and you need\nneed to go to billing okay and you need\nneed to go to billing okay and you need to add some amount in your account it\nto add some amount in your account it\nto add some amount in your account it can be very minimal it can be five bucks\ncan be very minimal it can be five bucks\ncan be very minimal it can be five bucks and yeah literally five bucks you can\nand yeah literally five bucks you can\nand yeah literally five bucks you can just add five bucks in your account what\njust add five bucks in your account what\njust add five bucks in your account what it will do it will just use that amount\nit will do it will just use that amount\nit will do it will just use that amount to make the API calls because whatever\nto make the API calls because whatever\nto make the API calls because whatever um you can say API call you are making\num you can say API call you are making\num you can say API call you are making it will use some amount right do not\nit will use some amount right do not\nit will use some amount right do not worry it will not charge you directly\nworry it will not charge you directly\nworry it will not charge you directly from the billing that you are using it\nfrom the billing that you are using it\nfrom the billing that you are using it will simply deduct the amount from the\nwill simply deduct the amount from the\nwill simply deduct the amount from the amount that you have added in your\namount that you have added in your\namount that you have added in your account It is a kind of you can say\naccount It is a kind of you can say\naccount It is a kind of you can say recharge that you do in your mobile\nrecharge that you do in your mobile\nrecharge that you do in your mobile phone or you used to do before because\nphone or you used to do before because\nphone or you used to do before because nowadays we have just bells but before\nnowadays we have just bells but before\nnowadays we have just bells but before remember we used to do the 10 rupees 20\nremember we used to do the 10 rupees 20\nremember we used to do the 10 rupees 20 rupees 50 rupees recharge coupon we used\nrupees 50 rupees recharge coupon we used\nrupees 50 rupees recharge coupon we used to scratch the number we need to dial it\nto scratch the number we need to dial it\nto scratch the number we need to dial it and then it gets added into your system\nand then it gets added into your system\nand then it gets added into your system into your mobile system right so it is s\ninto your mobile system right so it is s\ninto your mobile system right so it is s similar to that and you will say hey um\nsimilar to that and you will say hey um\nsimilar to that and you will say hey um is it enough five bucks are enough I\nis it enough five bucks are enough I\nis it enough five bucks are enough I would say more than enough because if\nwould say more than enough because if\nwould say more than enough because if you will be using um that particular\nyou will be using um that particular\nyou will be using um that particular five bucks for making API calls to LLMs.\nfive bucks for making API calls to LLMs.\nfive bucks for making API calls to LLMs. I would say it will hardly charge 0.00\nI would say it will hardly charge 0.00\nI would say it will hardly charge 0.00 something. I will just show you the you\nsomething. I will just show you the you\nsomething. I will just show you the you can say model cost, model pricing, it is\ncan say model cost, model pricing, it is\ncan say model cost, model pricing, it is very very minimal. So you can just feel\nvery very minimal. So you can just feel\nvery very minimal. So you can just feel very very good that if you add five\nvery very good that if you add five\nvery very good that if you add five bucks, it will be like more than enough.\nbucks, it will be like more than enough.\nbucks, it will be like more than enough. And not just like for this video, you\nAnd not just like for this video, you\nAnd not just like for this video, you can even practice so much of things with\ncan even practice so much of things with\ncan even practice so much of things with that five bucks. And I'm not kidding. I\nthat five bucks. And I'm not kidding. I\nthat five bucks. And I'm not kidding. I am not kidding. Like five bucks is like\nam not kidding. Like five bucks is like\nam not kidding. Like five bucks is like more than enough for you. Let me be\nmore than enough for you. Let me be\nmore than enough for you. Let me be honest. Okay. And obviously it depends\nhonest. Okay. And obviously it depends\nhonest. Okay. And obviously it depends like which model are you using but for\nlike which model are you using but for\nlike which model are you using but for most of the models it is like more than\nmost of the models it is like more than\nmost of the models it is like more than enough. Let me just show you the pricing\nenough. Let me just show you the pricing\nenough. Let me just show you the pricing quickly. If I go to\nquickly. If I go to\nquickly. If I go to OpenAI models pricing.\nOpenAI models pricing.\nOpenAI models pricing. So API pricing and here we have GPD 5.2\nSo API pricing and here we have GPD 5.2\nSo API pricing and here we have GPD 5.2 which is the most advanced version right\nwhich is the most advanced version right\nwhich is the most advanced version right now. So it is having 1.7 bucks per 1\nnow. So it is having 1.7 bucks per 1\nnow. So it is having 1.7 bucks per 1 million tokens and 1 million tokens is a\nmillion tokens and 1 million tokens is a\nmillion tokens and 1 million tokens is a big deal bro and this is the most\nbig deal bro and this is the most\nbig deal bro and this is the most curated model. And then we have GPT 5\ncurated model. And then we have GPT 5\ncurated model. And then we have GPT 5 mini which is the latest one but a mini\nmini which is the latest one but a mini\nmini which is the latest one but a mini model. It is very cheap. Ju just 25\nmodel. It is very cheap. Ju just 25\nmodel. It is very cheap. Ju just 25 cents per 1 million tokens. And can you\ncents per 1 million tokens. And can you\ncents per 1 million tokens. And can you imagine per 1 million tokens just 0.25\nimagine per 1 million tokens just 0.25\nimagine per 1 million tokens just 0.25 bucks. So five bucks is like more than\nbucks. So five bucks is like more than\nbucks. So five bucks is like more than enough. More than enough. And we can use\nenough. More than enough. And we can use\nenough. More than enough. And we can use let's say GPT5 mini. We'll be using this\nlet's say GPT5 mini. We'll be using this\nlet's say GPT5 mini. We'll be using this one GP5 mini. Okay. The cheap model but\none GP5 mini. Okay. The cheap model but\none GP5 mini. Okay. The cheap model but it has like all the features available\nit has like all the features available\nit has like all the features available that we want to use. So you need to do\nthat we want to use. So you need to do\nthat we want to use. So you need to do this homework. Okay. Once you have that\nthis homework. Okay. Once you have that\nthis homework. Okay. Once you have that homework ready then we are good to go.\nhomework ready then we are good to go.\nhomework ready then we are good to go. Okay. So now once we have done that\nOkay. So now once we have done that\nOkay. So now once we have done that homework now what we need to do we will\nhomework now what we need to do we will\nhomework now what we need to do we will simply go to our anti-gravity or your\nsimply go to our anti-gravity or your\nsimply go to our anti-gravity or your favorite you can say code editor create\nfavorite you can say code editor create\nfavorite you can say code editor create one file called\none file called\none file called env and then just add your API key and\nenv and then just add your API key and\nenv and then just add your API key and you need to create a variable called\nyou need to create a variable called\nyou need to create a variable called open AI\nopen AI\nopen AI API key equals\nAPI key equals\nAPI key equals um whatever your key is whatever\num whatever your key is whatever\num whatever your key is whatever obviously I will also not show you but\nobviously I will also not show you but\nobviously I will also not show you but you need to uh put that um value why it\nyou need to uh put that um value why it\nyou need to uh put that um value why it is showing like this SK or this is like\nis showing like this SK or this is like\nis showing like this SK or this is like demo value that it is showing SK\ndemo value that it is showing SK\ndemo value that it is showing SK something blah blah blah blah blah so it\nsomething blah blah blah blah blah so it\nsomething blah blah blah blah blah so it will be like looking like this because\nwill be like looking like this because\nwill be like looking like this because this is like kind of demo value that it\nthis is like kind of demo value that it\nthis is like kind of demo value that it generates like SK because open APK\ngenerates like SK because open APK\ngenerates like SK because open APK always starts with SK so you need to put\nalways starts with SK so you need to put\nalways starts with SK so you need to put this value in your particular case make\nthis value in your particular case make\nthis value in your particular case make sense because we'll be using this\nsense because we'll be using this\nsense because we'll be using this environment variable okay perfect\nenvironment variable okay perfect\nenvironment variable okay perfect so here is my API key okay and this is\nso here is my API key okay and this is\nso here is my API key okay and this is like income lead because I'm not sliding\nlike income lead because I'm not sliding\nlike income lead because I'm not sliding towards right and I will just revoke it\ntowards right and I will just revoke it\ntowards right and I will just revoke it I after this video. So this way you need\nI after this video. So this way you need\nI after this video. So this way you need to put your API key here. Why? Because\nto put your API key here. Why? Because\nto put your API key here. Why? Because we do not expose our API key in our\nwe do not expose our API key in our\nwe do not expose our API key in our code. It is not a best practice. We\ncode. It is not a best practice. We\ncode. It is not a best practice. We should always create an environment for\nshould always create an environment for\nshould always create an environment for our API key. Make sense? Okay. Very\nour API key. Make sense? Okay. Very\nour API key. Make sense? Okay. Very good. So let me just close it. And we\ngood. So let me just close it. And we\ngood. So let me just close it. And we have env\nhave env\nhave env codebase in the root directory not\ncodebase in the root directory not\ncodebase in the root directory not inside chapter one. It should be at the\ninside chapter one. It should be at the\ninside chapter one. It should be at the root directory level. Okay. Makes sense.\nroot directory level. Okay. Makes sense.\nroot directory level. Okay. Makes sense. And we can even hide it or basically\nAnd we can even hide it or basically\nAnd we can even hide it or basically exclude it from our codebase. go to get\nexclude it from our codebase. go to get\nexclude it from our codebase. go to get ignore and simply say totenv what will\nignore and simply say totenv what will\nignore and simply say totenv what will happen the advantage is now it is not\nhappen the advantage is now it is not\nhappen the advantage is now it is not the part of you can say your codebase\nthe part of you can say your codebase\nthe part of you can say your codebase your development now it will not be\nyour development now it will not be\nyour development now it will not be published whenever you will be pushing\npublished whenever you will be pushing\npublished whenever you will be pushing this code to github okay good these are\nthis code to github okay good these are\nthis code to github okay good these are the best practices that I'm just telling\nthe best practices that I'm just telling\nthe best practices that I'm just telling you in the meantime because if I'm\nyou in the meantime because if I'm\nyou in the meantime because if I'm teaching you something it's my\nteaching you something it's my\nteaching you something it's my responsibility that covers everything\nresponsibility that covers everything\nresponsibility that covers everything including best practices so first\nincluding best practices so first\nincluding best practices so first chapter is basics now what we are just\nchapter is basics now what we are just\nchapter is basics now what we are just trying to learn in the basics let me\ntrying to learn in the basics let me\ntrying to learn in the basics let me just show you something and you will be\njust show you something and you will be\njust show you something and you will be like okay that's how it is done okay so\nlike okay that's how it is done okay so\nlike okay that's how it is done okay so now let me just say markdown basically\nnow let me just say markdown basically\nnow let me just say markdown basically this is the markdown cell where we can\nthis is the markdown cell where we can\nthis is the markdown cell where we can just create headings and don't worry all\njust create headings and don't worry all\njust create headings and don't worry all of these notebooks will be available in\nof these notebooks will be available in\nof these notebooks will be available in my GitHub repo just for your notes but I\nmy GitHub repo just for your notes but I\nmy GitHub repo just for your notes but I want you to write everything code by\nwant you to write everything code by\nwant you to write everything code by code um you can say character by\ncode um you can say character by\ncode um you can say character by character so that you can also learn you\ncharacter so that you can also learn you\ncharacter so that you can also learn you can also build okay it's not like okay\ncan also build okay it's not like okay\ncan also build okay it's not like okay you will write everything we will simply\nyou will write everything we will simply\nyou will write everything we will simply read it no you are a coder you are a\nread it no you are a coder you are a\nread it no you are a coder you are a programmer you are a developer\nprogrammer you are a developer\nprogrammer you are a developer make sense so let me Okay,\nmake sense so let me Okay,\nmake sense so let me Okay, LLM call. Let's simply try to build this\nLLM call. Let's simply try to build this\nLLM call. Let's simply try to build this simple LLM call. Okay, so now what are\nsimple LLM call. Okay, so now what are\nsimple LLM call. Okay, so now what are we trying to do? We want to make an LLM\nwe trying to do? We want to make an LLM\nwe trying to do? We want to make an LLM call that chat GPT makes behind the\ncall that chat GPT makes behind the\ncall that chat GPT makes behind the scenes, right? And we'll be using lang\nscenes, right? And we'll be using lang\nscenes, right? And we'll be using lang chain for that. Now the thing is we can\nchain for that. Now the thing is we can\nchain for that. Now the thing is we can make this a LLM call very easily without\nmake this a LLM call very easily without\nmake this a LLM call very easily without lang chain as well. It's not a big deal.\nlang chain as well. It's not a big deal.\nlang chain as well. It's not a big deal. Let me just show you how we can just use\nLet me just show you how we can just use\nLet me just show you how we can just use or make API call to OpenAI. Okay. and\nor make API call to OpenAI. Okay. and\nor make API call to OpenAI. Okay. and how we can just use langen and why do we\nhow we can just use langen and why do we\nhow we can just use langen and why do we need to use lang. So let's say let's go\nneed to use lang. So let's say let's go\nneed to use lang. So let's say let's go to documentation and I'll simply say let\nto documentation and I'll simply say let\nto documentation and I'll simply say let me close so many tabs bro\nme close so many tabs bro\nme close so many tabs bro uh okay okay okay okay\nperfect\nperfect so now let's say I want to make an API\nso now let's say I want to make an API\nso now let's say I want to make an API call\ncall\ncall open API Python\nopen API Python\nopen API Python okay uh I want to see the API reference\nokay uh I want to see the API reference\nokay uh I want to see the API reference perfect because they keep on changing\nperfect because they keep on changing\nperfect because they keep on changing the code not frequently but yeah So this\nthe code not frequently but yeah So this\nthe code not frequently but yeah So this is where is that python python python\nis where is that python python python\nis where is that python python python um yeah they used to have one switch\num yeah they used to have one switch\num yeah they used to have one switch between the\nbetween the\nbetween the languages introduction it will be\nlanguages introduction it will be\nlanguages introduction it will be somewhere here but let me just check it\nsomewhere here but let me just check it\nsomewhere here but let me just check it openai or maybe I can just use this llm\nopenai or maybe I can just use this llm\nopenai or maybe I can just use this llm this is good so this is the code that I\nthis is good so this is the code that I\nthis is good so this is the code that I use if I just copy it and if I go to\nuse if I just copy it and if I go to\nuse if I just copy it and if I go to anti-gravity if I write it here and\nanti-gravity if I write it here and\nanti-gravity if I write it here and obviously I need to pick code\nobviously I need to pick code\nobviously I need to pick code right so I'm writing\nright so I'm writing\nright so I'm writing import OS right and then from open AI\nimport OS right and then from open AI\nimport OS right and then from open AI import openai and this is the exact\nimport openai and this is the exact\nimport openai and this is the exact thing that we are doing in case you do\nthing that we are doing in case you do\nthing that we are doing in case you do not have this open eye installed you can\nnot have this open eye installed you can\nnot have this open eye installed you can simply say UV add and first of all\nsimply say UV add and first of all\nsimply say UV add and first of all activate the environment\nactivate the environment\nactivate the environment so UV add openi\nso UV add openi\nso UV add openi perfect now we have openi library\nperfect now we have openi library\nperfect now we have openi library installed make sense so now what we\ninstalled make sense so now what we\ninstalled make sense so now what we doing very simple python function not\ndoing very simple python function not\ndoing very simple python function not even function I would say I will I I'm\neven function I would say I will I I'm\neven function I would say I will I I'm simply creating a client so this is OS\nsimply creating a client so this is OS\nsimply creating a client so this is OS module simple and why OS module because\nmodule simple and why OS module because\nmodule simple and why OS module because we want to get this environment variable\nwe want to get this environment variable\nwe want to get this environment variable as you can see I'm using the client\nas you can see I'm using the client\nas you can see I'm using the client using openi function which is built by\nusing openi function which is built by\nusing openi function which is built by openi not by me I'm passing this API key\nopeni not by me I'm passing this API key\nopeni not by me I'm passing this API key because without that we cannot work\nbecause without that we cannot work\nbecause without that we cannot work right because it needs to identify who\nright because it needs to identify who\nright because it needs to identify who is making the API call who is asking me\nis making the API call who is asking me\nis making the API call who is asking me to provide the answer it's you raul\nto provide the answer it's you raul\nto provide the answer it's you raul right is making the API call so Rahul\nright is making the API call so Rahul\nright is making the API call so Rahul needs to provide that particular API key\nneeds to provide that particular API key\nneeds to provide that particular API key and where Is the API key stored? Now you\nand where Is the API key stored? Now you\nand where Is the API key stored? Now you can hardcode the API key value here. But\ncan hardcode the API key value here. But\ncan hardcode the API key value here. But an Lamba just told you it is not a best\nan Lamba just told you it is not a best\nan Lamba just told you it is not a best practice because you can just simply\npractice because you can just simply\npractice because you can just simply expose your API key. So we store all the\nexpose your API key. So we store all the\nexpose your API key. So we store all the secret or you can say secret keys in the\nsecret or you can say secret keys in the\nsecret or you can say secret keys in the environment environ\nOS.inviron returns a dictionary of all\nOS.inviron returns a dictionary of all the environment variables with value as\nthe environment variables with value as\nthe environment variables with value as its value. And then we use dot get\nits value. And then we use dot get\nits value. And then we use dot get function. You can also use your simple\nfunction. You can also use your simple\nfunction. You can also use your simple square bracket um notation as well if\nsquare bracket um notation as well if\nsquare bracket um notation as well if you want to like this it works. But dot\nyou want to like this it works. But dot\nyou want to like this it works. But dot get is like the standard to pull the uh\nget is like the standard to pull the uh\nget is like the standard to pull the uh you can say environment file. So that is\nyou can say environment file. So that is\nyou can say environment file. So that is fine. Then we are simply creating a kind\nfine. Then we are simply creating a kind\nfine. Then we are simply creating a kind of variable chat completion client.comp\nof variable chat completion client.comp\nof variable chat completion client.comp completion.create. Okay. And then we are\ncompletion.create. Okay. And then we are\ncompletion.create. Okay. And then we are simply writing a messages. Now we are\nsimply writing a messages. Now we are\nsimply writing a messages. Now we are saying hey we want to send a message to\nsaying hey we want to send a message to\nsaying hey we want to send a message to the LLM which is the LLM model we are\nthe LLM which is the LLM model we are\nthe LLM which is the LLM model we are using. I will simply override it. Don't\nusing. I will simply override it. Don't\nusing. I will simply override it. Don't worry. And the role is user. That means\nworry. And the role is user. That means\nworry. And the role is user. That means I am the user who is sending the\nI am the user who is sending the\nI am the user who is sending the message. Content is say this is a test.\nmessage. Content is say this is a test.\nmessage. Content is say this is a test. No, I do not want to say this. I want to\nNo, I do not want to say this. I want to\nNo, I do not want to say this. I want to say\nbro,\nbro, tell me a joke about or let's say tell\ntell me a joke about or let's say tell\ntell me a joke about or let's say tell me a fun fact.\nme a fun fact.\nme a fun fact. Okay, tell me a fun fact. I want to know\nOkay, tell me a fun fact. I want to know\nOkay, tell me a fun fact. I want to know about a fun fact. Make sense? I want to\nabout a fun fact. Make sense? I want to\nabout a fun fact. Make sense? I want to ask this. Okay. And I want to use model\nask this. Okay. And I want to use model\nask this. Okay. And I want to use model which is GPT 5 mini\nwhich is GPT 5 mini\nwhich is GPT 5 mini GTP GPT 5 mini\nGTP GPT 5 mini\nGTP GPT 5 mini makes sense. I want to use this\nmakes sense. I want to use this\nmakes sense. I want to use this particular model GPD5 mini. I can pick\nparticular model GPD5 mini. I can pick\nparticular model GPD5 mini. I can pick any model. I can pick um 40. I can pick\nany model. I can pick um 40. I can pick\nany model. I can pick um 40. I can pick 4.1. I can pick 5.2. There are so many\n4.1. I can pick 5.2. There are so many\n4.1. I can pick 5.2. There are so many models. I want to pick this model GPD5\nmodels. I want to pick this model GPD5\nmodels. I want to pick this model GPD5 mini. Right? And then I'm simply\nmini. Right? And then I'm simply\nmini. Right? And then I'm simply printing the content of the response.\nprinting the content of the response.\nprinting the content of the response. Simple. Let's try to run this. Let's see\nSimple. Let's try to run this. Let's see\nSimple. Let's try to run this. Let's see what happens. Install requires IPI\nwhat happens. Install requires IPI\nwhat happens. Install requires IPI kernel. I told you yes, install it\nkernel. I told you yes, install it\nkernel. I told you yes, install it because it asks you to install IPI\nbecause it asks you to install IPI\nbecause it asks you to install IPI kernel in your virtual environment. So\nkernel in your virtual environment. So\nkernel in your virtual environment. So now let's see what will happen. Do you\nnow let's see what will happen. Do you\nnow let's see what will happen. Do you know what will happen? It will generate\nknow what will happen? It will generate\nknow what will happen? It will generate the response that will be created by\nthe response that will be created by\nthe response that will be created by LLM.\nLLM.\nLLM. LLM means this language model, this\nLLM means this language model, this\nLLM means this language model, this large language model GPD5 mini.\nOkay, let's wait.\nOkay, let's wait. Okay, see this is the response, bro. Fun\nOkay, see this is the response, bro. Fun\nOkay, see this is the response, bro. Fun fact. Wombats spin cube shaped piles.\nfact. Wombats spin cube shaped piles.\nfact. Wombats spin cube shaped piles. What? They used a stackable cubes to\nWhat? They used a stackable cubes to\nWhat? They used a stackable cubes to mark territory and communicate. Ooh.\nOh, bro.\nOh, bro. This is actually a fun fact. I still\nThis is actually a fun fact. I still\nThis is actually a fun fact. I still remember reading those fun facts written\nremember reading those fun facts written\nremember reading those fun facts written at the back or at the end of our\nat the back or at the end of our\nat the back or at the end of our classmates notebook which was the most\nclassmates notebook which was the most\nclassmates notebook which was the most premium notebooks. So yeah it's good. So\npremium notebooks. So yeah it's good. So\npremium notebooks. So yeah it's good. So see we have just got the response the\nsee we have just got the response the\nsee we have just got the response the same way we are getting from the chat\nsame way we are getting from the chat\nsame way we are getting from the chat GPD like from here. See same thing I'm\nGPD like from here. See same thing I'm\nGPD like from here. See same thing I'm just simply writing the message here and\njust simply writing the message here and\njust simply writing the message here and I'm getting the message back. That's\nI'm getting the message back. That's\nI'm getting the message back. That's exactly chat GPD has done. We are simply\nexactly chat GPD has done. We are simply\nexactly chat GPD has done. We are simply passing the message. They are simply\npassing the message. They are simply\npassing the message. They are simply generating the response and we have done\ngenerating the response and we have done\ngenerating the response and we have done the same thing. We are passing the\nthe same thing. We are passing the\nthe same thing. We are passing the thing. We are getting the response and\nthing. We are getting the response and\nthing. We are getting the response and we are simply building a kind of front\nwe are simply building a kind of front\nwe are simply building a kind of front end just to show the message. That's it.\nend just to show the message. That's it.\nend just to show the message. That's it. That's it.\nThat's it.\nThat's it. So that means you have built chat GPT\nSo that means you have built chat GPT\nSo that means you have built chat GPT kind of kind of kind of. Okay. Now, now\nkind of kind of kind of. Okay. Now, now\nkind of kind of kind of. Okay. Now, now you will say an Lamba what is the\nyou will say an Lamba what is the\nyou will say an Lamba what is the problem? Like we didn't even use lang\nproblem? Like we didn't even use lang\nproblem? Like we didn't even use lang chain. So now why do we need to use lang\nchain. So now why do we need to use lang\nchain. So now why do we need to use lang chain? Okay. Let me just ask you a\nchain? Okay. Let me just ask you a\nchain? Okay. Let me just ask you a simple question.\nsimple question.\nsimple question. And let's say you are\nLet's say why lang chain right this is\nLet's say why lang chain right this is your question right so let's say Rahul\nyour question right so let's say Rahul\nyour question right so let's say Rahul where is Rahul bring it bring him here\nwhere is Rahul bring it bring him here\nwhere is Rahul bring it bring him here Rahul\nRahul\nRahul so let's say this Rahul is using openAI\nso let's say this Rahul is using openAI\nso let's say this Rahul is using openAI model right let's say this Rahul is\nmodel right let's say this Rahul is\nmodel right let's say this Rahul is using openAI model let's say this is\nusing openAI model let's say this is\nusing openAI model let's say this is open AAI okay\nopen AAI okay\nopen AAI okay GPD 5.2 right 5.2 to GPD\nGPD 5.2 right 5.2 to GPD\nGPD 5.2 right 5.2 to GPD 5 let's say not 5.25 to five. Let's be\n5 let's say not 5.25 to five. Let's be\n5 let's say not 5.25 to five. Let's be honest because we are using GPD5. So we\nhonest because we are using GPD5. So we\nhonest because we are using GPD5. So we are using GPD 5, okay, to make the API\nare using GPD 5, okay, to make the API\nare using GPD 5, okay, to make the API calls. Okay. And we are able to do it\ncalls. Okay. And we are able to do it\ncalls. Okay. And we are able to do it very easily.\nvery easily.\nvery easily. Okay. And we have got the response.\nOkay. And we have got the response.\nOkay. And we have got the response. Okay. Perfect. And I'm just doing my\nOkay. Perfect. And I'm just doing my\nOkay. Perfect. And I'm just doing my work. Now everything is running fine.\nwork. Now everything is running fine.\nwork. Now everything is running fine. And to make these API calls, I'm not\nAnd to make these API calls, I'm not\nAnd to make these API calls, I'm not using lang chain. I'm using the official\nusing lang chain. I'm using the official\nusing lang chain. I'm using the official Python SDK software development kit\nPython SDK software development kit\nPython SDK software development kit provided by the OpenAI model. Perfect.\nprovided by the OpenAI model. Perfect.\nprovided by the OpenAI model. Perfect. So I'm using\nSo I'm using\nSo I'm using open\nopen\nopen SDK.\nSDK.\nSDK. Perfect.\nPerfect.\nPerfect. Okay. Just to make this API call. Now\nOkay. Just to make this API call. Now\nOkay. Just to make this API call. Now suddenly I realized that OpenAI models\nsuddenly I realized that OpenAI models\nsuddenly I realized that OpenAI models are very good to generate analysis to\nare very good to generate analysis to\nare very good to generate analysis to generate reports to generate research\ngenerate reports to generate research\ngenerate reports to generate research whatever but they are not very good in\nwhatever but they are not very good in\nwhatever but they are not very good in terms of by the way no offense to open\nterms of by the way no offense to open\nterms of by the way no offense to open AA models. I'm just talking it as like\nAA models. I'm just talking it as like\nAA models. I'm just talking it as like you say hypothetical situation and we\nyou say hypothetical situation and we\nyou say hypothetical situation and we all know that entropic\nall know that entropic\nall know that entropic clawed models are very very good in\nclawed models are very very good in\nclawed models are very very good in terms of coding everyone knows that\nterms of coding everyone knows that\nterms of coding everyone knows that right so let's say in my project I want\nright so let's say in my project I want\nright so let's say in my project I want to use openai yes but for specific tasks\nto use openai yes but for specific tasks\nto use openai yes but for specific tasks I also want to use let's say special\nI also want to use let's say special\nI also want to use let's say special model called\nmodel called\nmodel called entropic or let's say claude\nentropic or let's say claude\nentropic or let's say claude And if you do not know about claude, you\nAnd if you do not know about claude, you\nAnd if you do not know about claude, you should know about claude bro because\nshould know about claude bro because\nshould know about claude bro because claude is very very famous. Now if I\nclaude is very very famous. Now if I\nclaude is very very famous. Now if I want to use claude as well. I want to\nwant to use claude as well. I want to\nwant to use claude as well. I want to use claude as well. What I will do? Same\nuse claude as well. What I will do? Same\nuse claude as well. What I will do? Same thing. I will make the API call and I\nthing. I will make the API call and I\nthing. I will make the API call and I will use\nwill use\nwill use claude or basically entropic SDK. I\nclaude or basically entropic SDK. I\nclaude or basically entropic SDK. I think entropic is a parent company.\nthink entropic is a parent company.\nthink entropic is a parent company. Claude is the product.\nClaude is the product.\nClaude is the product. Okay, makes sense. So I'll be using\nOkay, makes sense. So I'll be using\nOkay, makes sense. So I'll be using claude SDK. Now let's say I want to\nclaude SDK. Now let's say I want to\nclaude SDK. Now let's say I want to include one more uh you can say model.\ninclude one more uh you can say model.\ninclude one more uh you can say model. Let's say I want to use um Gemini to\nLet's say I want to use um Gemini to\nLet's say I want to use um Gemini to generate images. Let's say I want to use\ngenerate images. Let's say I want to use\ngenerate images. Let's say I want to use nanobanana probe whatever right? Let's\nnanobanana probe whatever right? Let's\nnanobanana probe whatever right? Let's say I want to use this thing. Let's say\nsay I want to use this thing. Let's say\nsay I want to use this thing. Let's say this is Gemini\nthis is Gemini\nthis is Gemini Gemini. Right? Let's say I want to use\nGemini. Right? Let's say I want to use\nGemini. Right? Let's say I want to use this. What will happen? You will say um\nthis. What will happen? You will say um\nthis. What will happen? You will say um same thing. We will simply write\nsame thing. We will simply write\nsame thing. We will simply write um our code using\num our code using\num our code using SDK.\nSDK.\nSDK. Simple. Perfect. And which SDK you'll be\nSimple. Perfect. And which SDK you'll be\nSimple. Perfect. And which SDK you'll be using? Obviously Google.\nusing? Obviously Google.\nusing? Obviously Google. Google SDK. Perfect.\nGoogle SDK. Perfect.\nGoogle SDK. Perfect. Now just tell me one thing, bro. How\nNow just tell me one thing, bro. How\nNow just tell me one thing, bro. How many SDKs will you use? Let's say I want\nmany SDKs will you use? Let's say I want\nmany SDKs will you use? Let's say I want to use 10 more models.\nto use 10 more models.\nto use 10 more models. Oh, so what if I say what if I just tell\nOh, so what if I say what if I just tell\nOh, so what if I say what if I just tell you that you can use this model GP5 okay\nyou that you can use this model GP5 okay\nyou that you can use this model GP5 okay GP5\nGP5\nGP5 use this model okay you want to use\nuse this model okay you want to use\nuse this model okay you want to use claude okay use it\nclaude okay use it\nclaude okay use it okay you want to use Gemini use it\nokay you want to use Gemini use it\nokay you want to use Gemini use it and here\nyou have all the models right\nyou have all the models right all all the models let's say 20 plus\nall all the models let's say 20 plus\nall all the models let's say 20 plus models 30 plus models whatever\nmodels 30 plus models whatever\nmodels 30 plus models whatever let's say all the models are here\nlet's say all the models are here\nlet's say all the models are here okay now if I say Rahul come here\nokay now if I say Rahul come here\nokay now if I say Rahul come here instead of writing so many different\ninstead of writing so many different\ninstead of writing so many different different SDKs because every SDK has its\ndifferent SDKs because every SDK has its\ndifferent SDKs because every SDK has its own syntax every SDK has its you can say\nown syntax every SDK has its you can say\nown syntax every SDK has its you can say on code whatever right so now this Rahul\non code whatever right so now this Rahul\non code whatever right so now this Rahul can use\ncan use\ncan use something called as lanching okay\nsomething called as lanching okay\nsomething called as lanching okay now this Rahul\nnow this Rahul\nnow this Rahul can use let's say something called as\ncan use let's say something called as\ncan use let's say something called as lang chain.\nNow this lang chain what we'll do this\nNow this lang chain what we'll do this lang chain is a kind of wrapper\nlang chain is a kind of wrapper\nlang chain is a kind of wrapper this is connected to it this is\nthis is connected to it this is\nthis is connected to it this is connecting all the models are connected\nconnecting all the models are connected\nconnecting all the models are connected to it so lang chain will handle all of\nto it so lang chain will handle all of\nto it so lang chain will handle all of the SDKs and we just want to use or need\nthe SDKs and we just want to use or need\nthe SDKs and we just want to use or need to use lang chain SDK just tell me one\nto use lang chain SDK just tell me one\nto use lang chain SDK just tell me one thing is it better to use just one SDK\nthing is it better to use just one SDK\nthing is it better to use just one SDK to use all the other models or is it\nto use all the other models or is it\nto use all the other models or is it better to use different different SDK\nbetter to use different different SDK\nbetter to use different different SDK case obviously this is better obviously\ncase obviously this is better obviously\ncase obviously this is better obviously this is better so we do the same thing\nthis is better so we do the same thing\nthis is better so we do the same thing we just use\nwe just use\nwe just use this thing as lang chain we do not use\nthis thing as lang chain we do not use\nthis thing as lang chain we do not use different different SDKs when we didn't\ndifferent different SDKs when we didn't\ndifferent different SDKs when we didn't have agentic frameworks let's say lang\nhave agentic frameworks let's say lang\nhave agentic frameworks let's say lang chain lang graph um all those things we\nchain lang graph um all those things we\nchain lang graph um all those things we used to use different different SDKs and\nused to use different different SDKs and\nused to use different different SDKs and it was a chaos and then now we have only\nit was a chaos and then now we have only\nit was a chaos and then now we have only single SDK which is lang chain and life\nsingle SDK which is lang chain and life\nsingle SDK which is lang chain and life is sorted\nis sorted\nis sorted We do not need to use different SDKs. We\nWe do not need to use different SDKs. We\nWe do not need to use different SDKs. We can access all the models. Wow, that is\ncan access all the models. Wow, that is\ncan access all the models. Wow, that is really cool. Yes. So now instead of\nreally cool. Yes. So now instead of\nreally cool. Yes. So now instead of using this, I will use lang chain. So\nusing this, I will use lang chain. So\nusing this, I will use lang chain. So how we can just do this? For that I need\nhow we can just do this? For that I need\nhow we can just do this? For that I need to install one more library. It's called\nto install one more library. It's called\nto install one more library. It's called lang chain open. So I will say UV add.\nlang chain open. So I will say UV add.\nlang chain open. So I will say UV add. Let me just open this one.\nLet me just open this one.\nLet me just open this one. Okay, perfect. UV add. Make sure you\nOkay, perfect. UV add. Make sure you\nOkay, perfect. UV add. Make sure you have this on. You have this virtual\nhave this on. You have this virtual\nhave this on. You have this virtual envirment turned on. And I have already\nenvirment turned on. And I have already\nenvirment turned on. And I have already shown you how you can just turn on VNV\nshown you how you can just turn on VNV\nshown you how you can just turn on VNV scripts activate. That's it. Okay. So UV\nscripts activate. That's it. Okay. So UV\nscripts activate. That's it. Okay. So UV add\nadd\nadd lang chain openai add this. Now\nlang chain openai add this. Now\nlang chain openai add this. Now what it will do? It will\nwhat it will do? It will\nwhat it will do? It will simply call it. So I will simply say\nsimply call it. So I will simply say\nsimply call it. So I will simply say from lang chain openai and I want to\nfrom lang chain openai and I want to\nfrom lang chain openai and I want to import a function called chat openai.\nimport a function called chat openai.\nimport a function called chat openai. Okay. Chat openai. Okay. Now simply say\nOkay. Chat openai. Okay. Now simply say\nOkay. Chat openai. Okay. Now simply say LLM\nLLM\nLLM open equals\nopen equals\nopen equals chat open AI\nchat open AI\nchat open AI model equals\nmodel equals\nmodel equals GPT 5 mini.\nGPT 5 mini.\nGPT 5 mini. GPT5 mini that's it and temperature\nGPT5 mini that's it and temperature\nGPT5 mini that's it and temperature equals zero. Now what is this\nequals zero. Now what is this\nequals zero. Now what is this temperature? If you do not know\ntemperature? If you do not know\ntemperature? If you do not know temperature is basically the creativity\ntemperature is basically the creativity\ntemperature is basically the creativity of a model. If you set it to zero that\nof a model. If you set it to zero that\nof a model. If you set it to zero that means model will not be creative. model\nmeans model will not be creative. model\nmeans model will not be creative. model will simply generate the answer with the\nwill simply generate the answer with the\nwill simply generate the answer with the best possibility. But if you set let's\nbest possibility. But if you set let's\nbest possibility. But if you set let's say 0.9, 0.8, 0.7, model will just play\nsay 0.9, 0.8, 0.7, model will just play\nsay 0.9, 0.8, 0.7, model will just play around the output and it can just\naround the output and it can just\naround the output and it can just generate some random output as well. It\ngenerate some random output as well. It\ngenerate some random output as well. It depends like what is your use case. If\ndepends like what is your use case. If\ndepends like what is your use case. If you are just generating very very\nyou are just generating very very\nyou are just generating very very serious and you do not need to play\nserious and you do not need to play\nserious and you do not need to play around with output, you you should only\naround with output, you you should only\naround with output, you you should only say temperature equals to zero. So lm\nsay temperature equals to zero. So lm\nsay temperature equals to zero. So lm openi equals t open i. Okay. And then\nopeni equals t open i. Okay. And then\nopeni equals t open i. Okay. And then you can simply say if if now if you just\nyou can simply say if if now if you just\nyou can simply say if if now if you just want to ask something from this model\nwant to ask something from this model\nwant to ask something from this model you will simply say llm openi\nyou will simply say llm openi\nyou will simply say llm openi dot invoke and then I will say pro tell\ndot invoke and then I will say pro tell\ndot invoke and then I will say pro tell me a fun fact\nmake sense and this will generate me the\nmake sense and this will generate me the output and do you know what this time I\noutput and do you know what this time I\noutput and do you know what this time I will not curate the output I will just\nwill not curate the output I will just\nwill not curate the output I will just show the real and row output how it\nshow the real and row output how it\nshow the real and row output how it generates the output let me just show\ngenerates the output let me just show\ngenerates the output let me just show you.\nyou.\nyou. So let's wait.\nPerfect. This is the output. We always\nPerfect. This is the output. We always get the output in the form of AI\nget the output in the form of AI\nget the output in the form of AI message. There are three different types\nmessage. There are three different types\nmessage. There are three different types of messages. AI message, system message\nof messages. AI message, system message\nof messages. AI message, system message and human message. We will just talk\nand human message. We will just talk\nand human message. We will just talk about messages as well very very soon.\nabout messages as well very very soon.\nabout messages as well very very soon. But this is the raw output. But we can\nBut this is the raw output. But we can\nBut this is the raw output. But we can just obviously curate it. We can simply\njust obviously curate it. We can simply\njust obviously curate it. We can simply say dot content and that we do every\nsay dot content and that we do every\nsay dot content and that we do every time. But this is a raw message. AI\ntime. But this is a raw message. AI\ntime. But this is a raw message. AI message. That means it is sent by AI.\nmessage. That means it is sent by AI.\nmessage. That means it is sent by AI. And this is the fun fact. Let's read it.\nAnd this is the fun fact. Let's read it.\nAnd this is the fun fact. Let's read it. Bananas are technically berries, but\nBananas are technically berries, but\nBananas are technically berries, but strawberries aren't. Wow. What? A berry\nstrawberries aren't. Wow. What? A berry\nstrawberries aren't. Wow. What? A berry comes from a single O. Okay.\ncomes from a single O. Okay.\ncomes from a single O. Okay. While strawberries are an aggregated\nWhile strawberries are an aggregated\nWhile strawberries are an aggregated accessory fruit made of. Wow. This is\naccessory fruit made of. Wow. This is\naccessory fruit made of. Wow. This is actually a fun fact. Okay. Now, if I\nactually a fun fact. Okay. Now, if I\nactually a fun fact. Okay. Now, if I just run this now, I can simply get the\njust run this now, I can simply get the\njust run this now, I can simply get the content. Only the content, not the row\ncontent. Only the content, not the row\ncontent. Only the content, not the row output. So this is a very basic use case\noutput. So this is a very basic use case\noutput. So this is a very basic use case of langchain API. Now what is the\nof langchain API. Now what is the\nof langchain API. Now what is the advantage of it? Now let's say I want to\nadvantage of it? Now let's say I want to\nadvantage of it? Now let's say I want to use entropic model. I will simply say\nuse entropic model. I will simply say\nuse entropic model. I will simply say from langchain\nimport chat entropic and I can make llm\nimport chat entropic and I can make llm entropic as well. Simple like this. And\nentropic as well. Simple like this. And\nentropic as well. Simple like this. And I can use claude 3.35 sonet. By the way\nI can use claude 3.35 sonet. By the way\nI can use claude 3.35 sonet. By the way we use 4.5 set. Yeah. But yeah, but I\nwe use 4.5 set. Yeah. But yeah, but I\nwe use 4.5 set. Yeah. But yeah, but I will not use entropic because I uh I do\nwill not use entropic because I uh I do\nwill not use entropic because I uh I do not want you to create multiple API keys\nnot want you to create multiple API keys\nnot want you to create multiple API keys because for this you would also need to\nbecause for this you would also need to\nbecause for this you would also need to create an API key with in uh in the\ncreate an API key with in uh in the\ncreate an API key with in uh in the entropic platform. But I just want to\nentropic platform. But I just want to\nentropic platform. But I just want to show you the use case of it. Make sense?\nshow you the use case of it. Make sense?\nshow you the use case of it. Make sense? Okay, makes sense. There's another way\nOkay, makes sense. There's another way\nOkay, makes sense. There's another way to create this particular thing which is\nto create this particular thing which is\nto create this particular thing which is recently added. It is called init chat\nrecently added. It is called init chat\nrecently added. It is called init chat model. So you can simply say from\nmodel. So you can simply say from\nmodel. So you can simply say from langchen\nlangchen\nlangchen dot models import\ninit chat model or I think from just\ninit chat model or I think from just models. Yeah, I think just maybe from\nmodels. Yeah, I think just maybe from\nmodels. Yeah, I think just maybe from lang chain because they have recently\nlang chain because they have recently\nlang chain because they have recently added from lang chain. Let me just check\nadded from lang chain. Let me just check\nadded from lang chain. Let me just check the init chat model class.\nInit chat model langchain.\nInit chat model langchain. Oh chat models. Perfect. So we have chat\nOh chat models. Perfect. So we have chat\nOh chat models. Perfect. So we have chat models\nmodels\nmodels chat models\nchat models\nchat models and import\nand import\nand import init chat model. Perfect. So in this\ninit chat model. Perfect. So in this\ninit chat model. Perfect. So in this init chat model we do not even need to\ninit chat model we do not even need to\ninit chat model we do not even need to define like if we are just trying to\ndefine like if we are just trying to\ndefine like if we are just trying to import openi models and thropping\nimport openi models and thropping\nimport openi models and thropping models. We simply need to put the model\nmodels. We simply need to put the model\nmodels. We simply need to put the model name and that's it. I will simply say\nname and that's it. I will simply say\nname and that's it. I will simply say llm openi\nllm openi\nllm openi equals init chat model and then model\nequals init chat model and then model\nequals init chat model and then model name or let's say model equals GPD 5\nname or let's say model equals GPD 5\nname or let's say model equals GPD 5 mini. This is recently added. Okay. Even\nmini. This is recently added. Okay. Even\nmini. This is recently added. Okay. Even if you use any function, it is fine. So,\nif you use any function, it is fine. So,\nif you use any function, it is fine. So, not a big deal. And let me just invoke\nnot a big deal. And let me just invoke\nnot a big deal. And let me just invoke it. Let me say hello, how are you? And\nit. Let me say hello, how are you? And\nit. Let me say hello, how are you? And it will say, I am doing well, well,\nit will say, I am doing well, well,\nit will say, I am doing well, well, thanks. How are you? How can I help you\nthanks. How are you? How can I help you\nthanks. How are you? How can I help you today? So, it is just making all of\ntoday? So, it is just making all of\ntoday? So, it is just making all of these API calls using your API. Make\nthese API calls using your API. Make\nthese API calls using your API. Make sense? So, these are the ways to make\nsense? So, these are the ways to make\nsense? So, these are the ways to make API calls. Basically, LLM calls. Simple.\nAPI calls. Basically, LLM calls. Simple.\nAPI calls. Basically, LLM calls. Simple. Good.\nGood.\nGood. Now, let's talk about the messages\nNow, let's talk about the messages\nNow, let's talk about the messages because we just saw the type of\nbecause we just saw the type of\nbecause we just saw the type of messages. Okay. Now I want you to make\nmessages. Okay. Now I want you to make\nmessages. Okay. Now I want you to make aware about the messages that we can use\naware about the messages that we can use\naware about the messages that we can use within lang chain or basically in the\nwithin lang chain or basically in the\nwithin lang chain or basically in the agentic framework that we use because in\nagentic framework that we use because in\nagentic framework that we use because in the every agentic framework we use\nthe every agentic framework we use\nthe every agentic framework we use messages and messages has a great role\nmessages and messages has a great role\nmessages and messages has a great role to play. Okay. So let me just bring it\nto play. Okay. So let me just bring it\nto play. Okay. So let me just bring it here. Uh let me just explain you the\nhere. Uh let me just explain you the\nhere. Uh let me just explain you the messages like why messages is a big\nmessages like why messages is a big\nmessages like why messages is a big deal. Why I would say you will know the\ndeal. Why I would say you will know the\ndeal. Why I would say you will know the reason why. So\nreason why. So\nreason why. So messages\nmessages\nmessages if you want to communicate to anyone\nif you want to communicate to anyone\nif you want to communicate to anyone let's say Rahul where is Rahul yeah come\nlet's say Rahul where is Rahul yeah come\nlet's say Rahul where is Rahul yeah come here bro let's say this Rahul this Rahul\nhere bro let's say this Rahul this Rahul\nhere bro let's say this Rahul this Rahul needs to communicate to anyone right\nneeds to communicate to anyone right\nneeds to communicate to anyone right anyone let's say this Rahul need to\nanyone let's say this Rahul need to\nanyone let's say this Rahul need to communicate to any team any human anyone\ncommunicate to any team any human anyone\ncommunicate to any team any human anyone let's say this Rahul needs to talk to\nlet's say this Rahul needs to talk to\nlet's say this Rahul needs to talk to these users make sense\nthese users make sense\nthese users make sense so obviously In order to communicate to\nso obviously In order to communicate to\nso obviously In order to communicate to these people\nthese people\nthese people this\nthis\nthis this this you can say two-way thing is\nthis this you can say two-way thing is\nthis this you can say two-way thing is very important\nvery important\nvery important make sense because this message this\nmake sense because this message this\nmake sense because this message this Rahul's message is you can say\nRahul's message is you can say\nRahul's message is you can say Rahul's message\nRahul's message\nRahul's message right let's say you are just hearing to\nright let's say you are just hearing to\nright let's say you are just hearing to a transcript of a call by the way you\na transcript of a call by the way you\na transcript of a call by the way you should not hear to someone else call\nshould not hear to someone else call\nshould not hear to someone else call transcription but let's say you are\ntranscription but let's say you are\ntranscription but let's say you are hearing it Okay, let's let's say you are\nhearing it Okay, let's let's say you are\nhearing it Okay, let's let's say you are just hearing the call transcription of\njust hearing the call transcription of\njust hearing the call transcription of your\nyour\nyour best friend, okay, with their whatever.\nbest friend, okay, with their whatever.\nbest friend, okay, with their whatever. Okay, you know what I mean? So let's say\nOkay, you know what I mean? So let's say\nOkay, you know what I mean? So let's say you are hearing to that transcription.\nyou are hearing to that transcription.\nyou are hearing to that transcription. So what you will hear? You will hear\nSo what you will hear? You will hear\nSo what you will hear? You will hear Rahul's message and in return you will\nRahul's message and in return you will\nRahul's message and in return you will hear this person's message. Okay, let's\nhear this person's message. Okay, let's\nhear this person's message. Okay, let's make it as let's say purple. Let's say\nmake it as let's say purple. Let's say\nmake it as let's say purple. Let's say this person's message or let's say\nthis person's message or let's say\nthis person's message or let's say users. Let's say this Rahul is listening\nusers. Let's say this Rahul is listening\nusers. Let's say this Rahul is listening to or basically talking to group of\nto or basically talking to group of\nto or basically talking to group of people. Okay. So I will simply say\npeople. Okay. So I will simply say\npeople. Okay. So I will simply say let's let's create one person. It will\nlet's let's create one person. It will\nlet's let's create one person. It will be fun. So let's say one user. Perfect.\nbe fun. So let's say one user. Perfect.\nbe fun. So let's say one user. Perfect. Okay. This Rahul is\nOkay. This Rahul is\nOkay. This Rahul is talking to who?\ntalking to who?\ntalking to who? H let's say Siman. Who is Siman? I don't\nH let's say Siman. Who is Siman? I don't\nH let's say Siman. Who is Siman? I don't know. just in a hypothetical just a\nknow. just in a hypothetical just a\nknow. just in a hypothetical just a hypothetical situation and hypothetical\nhypothetical situation and hypothetical\nhypothetical situation and hypothetical person. So this Rahul's message and this\nperson. So this Rahul's message and this\nperson. So this Rahul's message and this message would be called as Simon's\nmessage would be called as Simon's\nmessage would be called as Simon's message\nmessage\nmessage right by the way why are you laughing\nright by the way why are you laughing\nright by the way why are you laughing two people are talking to each other\ntwo people are talking to each other\ntwo people are talking to each other what's the big deal so now let's say\nwhat's the big deal so now let's say\nwhat's the big deal so now let's say these two people are talking to each\nthese two people are talking to each\nthese two people are talking to each other so now if I want to hear to this\nother so now if I want to hear to this\nother so now if I want to hear to this person I will say Rahul's message and\nperson I will say Rahul's message and\nperson I will say Rahul's message and this is the content tuck tuck tuck tuck\nthis is the content tuck tuck tuck tuck\nthis is the content tuck tuck tuck tuck tuck this is Siman's message tuck tuck\ntuck this is Siman's message tuck tuck\ntuck this is Siman's message tuck tuck tuck tuck tuck so this is the\ntuck tuck tuck so this is the\ntuck tuck tuck so this is the communication the way of communication\ncommunication the way of communication\ncommunication the way of communication ation between two people, right? We have\nation between two people, right? We have\nation between two people, right? We have like identity of each message like who\nlike identity of each message like who\nlike identity of each message like who is delivering which message and who is\nis delivering which message and who is\nis delivering which message and who is receiving which which message. Simple,\nreceiving which which message. Simple,\nreceiving which which message. Simple, nothing fancy. Very good. Now let's talk\nnothing fancy. Very good. Now let's talk\nnothing fancy. Very good. Now let's talk about the real stuff. Let's say this\nabout the real stuff. Let's say this\nabout the real stuff. Let's say this Rahul is talking to an AI.\nRahul is talking to an AI.\nRahul is talking to an AI. Okay. What? So let's say this Rahul is\nOkay. What? So let's say this Rahul is\nOkay. What? So let's say this Rahul is actually talking to AI.\nactually talking to AI.\nactually talking to AI. Okay.\nOkay.\nOkay. Now like real AI. Okay. So now let's say\nNow like real AI. Okay. So now let's say\nNow like real AI. Okay. So now let's say this Rahul is talking to AI. Make sense?\nthis Rahul is talking to AI. Make sense?\nthis Rahul is talking to AI. Make sense? Now just tell me one thing. This Rahul's\nNow just tell me one thing. This Rahul's\nNow just tell me one thing. This Rahul's message\nmessage\nmessage is this. I know that. And\nis this. I know that. And\nis this. I know that. And this AI message will be this one.\nthis AI message will be this one.\nthis AI message will be this one. Simple.\nSimple.\nSimple. This is the AI message.\nOkay, that AI is generating basically\nOkay, that AI is generating basically LLM is generating.\nLLM is generating.\nLLM is generating. Okay, makes sense. Okay, simple. So this\nOkay, makes sense. Okay, simple. So this\nOkay, makes sense. Okay, simple. So this is also done. So this Rahul's message\nis also done. So this Rahul's message\nis also done. So this Rahul's message will be called as\nwill be called as\nwill be called as user message in terms of technical\nuser message in terms of technical\nuser message in terms of technical language because Rahul is a user.\nlanguage because Rahul is a user.\nlanguage because Rahul is a user. Okay, Rahul is a user.\nOkay, Rahul is a user.\nOkay, Rahul is a user. So this will be considered as user's\nSo this will be considered as user's\nSo this will be considered as user's message.\nmessage.\nmessage. simple like I know it is new but it is\nsimple like I know it is new but it is\nsimple like I know it is new but it is not very much complex this is user's\nnot very much complex this is user's\nnot very much complex this is user's message okay this AI message is\nmessage okay this AI message is\nmessage okay this AI message is obviously like AI message simple now\nobviously like AI message simple now\nobviously like AI message simple now there's one more term it is called\nthere's one more term it is called\nthere's one more term it is called system message now what is a system\nsystem message now what is a system\nsystem message now what is a system message this can be confusing okay let\nmessage this can be confusing okay let\nmessage this can be confusing okay let me just take you to the previous example\nme just take you to the previous example\nme just take you to the previous example now let's say siman\nnow let's say siman\nnow let's say siman we we need to set the tone for siman\nwe we need to set the tone for siman\nwe we need to set the tone for siman I know Siman is talking to Rahul.\nI know Siman is talking to Rahul.\nI know Siman is talking to Rahul. Okay. And Rahul is also your friend.\nOkay. And Rahul is also your friend.\nOkay. And Rahul is also your friend. Siman is also your friend. Okay. And now\nSiman is also your friend. Okay. And now\nSiman is also your friend. Okay. And now you need to tell Siman that you need to\nyou need to tell Siman that you need to\nyou need to tell Siman that you need to talk to Rahul but you need to talk you\ntalk to Rahul but you need to talk you\ntalk to Rahul but you need to talk you can say softly or you do not need to um\ncan say softly or you do not need to um\ncan say softly or you do not need to um say anything rude to him. So you are\nsay anything rude to him. So you are\nsay anything rude to him. So you are setting the tone, you are setting the\nsetting the tone, you are setting the\nsetting the tone, you are setting the environment like how they should just\nenvironment like how they should just\nenvironment like how they should just talk to each other, right? So you are\ntalk to each other, right? So you are\ntalk to each other, right? So you are guiding Siman like how you should talk\nguiding Siman like how you should talk\nguiding Siman like how you should talk to Rahul. I will say\num talk nicely\num talk nicely and be a little\nand be a little\nand be a little polite.\nOkay,\nOkay, this is the tone that I have set for the\nthis is the tone that I have set for the\nthis is the tone that I have set for the environment, for the conversation, for\nenvironment, for the conversation, for\nenvironment, for the conversation, for the responses. So now do you know what?\nthe responses. So now do you know what?\nthe responses. So now do you know what? All of Siman's responses will be polite\nAll of Siman's responses will be polite\nAll of Siman's responses will be polite and will be very nice towards Rahul.\nand will be very nice towards Rahul.\nand will be very nice towards Rahul. Same thing you can do it in LLM as well.\nSame thing you can do it in LLM as well.\nSame thing you can do it in LLM as well. You can tell LLM that hey talk nicely to\nYou can tell LLM that hey talk nicely to\nYou can tell LLM that hey talk nicely to me and be a little polite. Why does it\nme and be a little polite. Why does it\nme and be a little polite. Why does it matter? For example, let's say you are\nmatter? For example, let's say you are\nmatter? For example, let's say you are asking LLM to write an email to your\nasking LLM to write an email to your\nasking LLM to write an email to your friend. Email to a friend. Let's say\nfriend. Email to a friend. Let's say\nfriend. Email to a friend. Let's say let's say you're living in '90s. You're\nlet's say you're living in '90s. You're\nlet's say you're living in '90s. You're writing the letter. So you're asking LLM\nwriting the letter. So you're asking LLM\nwriting the letter. So you're asking LLM to write an email to your friend. So\nto write an email to your friend. So\nto write an email to your friend. So obviously it should be a little polite.\nobviously it should be a little polite.\nobviously it should be a little polite. If you are just writing an email to your\nIf you are just writing an email to your\nIf you are just writing an email to your manager, it should be formal. So you\nmanager, it should be formal. So you\nmanager, it should be formal. So you need to set the tone. That tone that\nneed to set the tone. That tone that\nneed to set the tone. That tone that environment is called system message\nenvironment is called system message\nenvironment is called system message nothing else.\nSystem message. So this system message\nSystem message. So this system message is your this thing\nis your this thing\nis your this thing simple.\nsimple.\nsimple. I hope you understood with the example.\nI hope you understood with the example.\nI hope you understood with the example. I am so sure. So this is the system\nI am so sure. So this is the system\nI am so sure. So this is the system message\nbecause see I need to take example so\nbecause see I need to take example so that you can understand it simple. So\nthat you can understand it simple. So\nthat you can understand it simple. So this is system message. So that means\nthis is system message. So that means\nthis is system message. So that means only user message is in your control. AI\nonly user message is in your control. AI\nonly user message is in your control. AI message is in AI AI control. System\nmessage is in AI AI control. System\nmessage is in AI AI control. System message is for LLM but you control this\nmessage is for LLM but you control this\nmessage is for LLM but you control this because you set the tone.\nbecause you set the tone.\nbecause you set the tone. Okay, make sense? So this is all about\nOkay, make sense? So this is all about\nOkay, make sense? So this is all about the messages. This is like real real\nthe messages. This is like real real\nthe messages. This is like real real world analogy analogy whatever you want\nworld analogy analogy whatever you want\nworld analogy analogy whatever you want to say and this is the real use case of\nto say and this is the real use case of\nto say and this is the real use case of it. Now I hope you understood it with\nit. Now I hope you understood it with\nit. Now I hope you understood it with the help of example between Rahul and\nthe help of example between Rahul and\nthe help of example between Rahul and Siman. So now let's try to code it. Now\nSiman. So now let's try to code it. Now\nSiman. So now let's try to code it. Now Rahul just focus on coding. Forget about\nRahul just focus on coding. Forget about\nRahul just focus on coding. Forget about Simon and now let's talk to LLM, right?\nSimon and now let's talk to LLM, right?\nSimon and now let's talk to LLM, right? Because see now let's create a new\nBecause see now let's create a new\nBecause see now let's create a new notebook. This is like very basic\nnotebook. This is like very basic\nnotebook. This is like very basic notebook because I want to provide this\nnotebook because I want to provide this\nnotebook because I want to provide this particular master class in the form of\nparticular master class in the form of\nparticular master class in the form of structured chapters so that anyone can\nstructured chapters so that anyone can\nstructured chapters so that anyone can go to chapter chapter 1 2 3 within that\ngo to chapter chapter 1 2 3 within that\ngo to chapter chapter 1 2 3 within that we have topics basics and all those\nwe have topics basics and all those\nwe have topics basics and all those things. So it will be very handy and let\nthings. So it will be very handy and let\nthings. So it will be very handy and let me just rename it. Let me just say\nme just rename it. Let me just say\nme just rename it. Let me just say one lm\none lm\none lm call simple. Now let me create a new\ncall simple. Now let me create a new\ncall simple. Now let me create a new notebook. I will say second\nnotebook. I will say second\nnotebook. I will say second messages messages do ipy andb. Perfect.\nmessages messages do ipy andb. Perfect.\nmessages messages do ipy andb. Perfect. So let me just talk about the messages\nSo let me just talk about the messages\nSo let me just talk about the messages and you know the messages like why do we\nand you know the messages like why do we\nand you know the messages like why do we need messages? Why do we use messages?\nneed messages? Why do we use messages?\nneed messages? Why do we use messages? So now let me just write the code. So in\nSo now let me just write the code. So in\nSo now let me just write the code. So in order to use the message we have like a\norder to use the message we have like a\norder to use the message we have like a specific library that we need to\nspecific library that we need to\nspecific library that we need to install. It's called lang chain core.\ninstall. It's called lang chain core.\ninstall. It's called lang chain core. Langchain core. And by the way, I will\nLangchain core. And by the way, I will\nLangchain core. And by the way, I will be referring to this particular\nbe referring to this particular\nbe referring to this particular lang chain migration guide as well\nlang chain migration guide as well\nlang chain migration guide as well because they have recently upgraded the\nbecause they have recently upgraded the\nbecause they have recently upgraded the lang chain to 1.0 and earlier we were\nlang chain to 1.0 and earlier we were\nlang chain to 1.0 and earlier we were using 0.3 0.6 they just changed almost\nusing 0.3 0.6 they just changed almost\nusing 0.3 0.6 they just changed almost all the syntaxes all the classes. I\nall the syntaxes all the classes. I\nall the syntaxes all the classes. I don't know why I like does that make any\ndon't know why I like does that make any\ndon't know why I like does that make any sense? If you want to add something\nsense? If you want to add something\nsense? If you want to add something make new functions in the existing\nmake new functions in the existing\nmake new functions in the existing classes why you need to change the\nclasses why you need to change the\nclasses why you need to change the classes names you like what what what\nclasses names you like what what what\nclasses names you like what what what what will you get to break the code\nwhat will you get to break the code\nwhat will you get to break the code basis of the people\nbasis of the people\nbasis of the people okay so this is basically the guide so\nokay so this is basically the guide so\nokay so this is basically the guide so the thing is I am also exploring this\nthe thing is I am also exploring this\nthe thing is I am also exploring this particular new you can say version of\nparticular new you can say version of\nparticular new you can say version of langin um this is the older version not\nlangin um this is the older version not\nlangin um this is the older version not older version this was the you can say\nolder version this was the you can say\nolder version this was the you can say traditional way to use those particular\ntraditional way to use those particular\ntraditional way to use those particular classes simple but this is you can say\nclasses simple but this is you can say\nclasses simple but this is you can say latest way to use the classes it doesn't\nlatest way to use the classes it doesn't\nlatest way to use the classes it doesn't mean that you cannot use this method but\nmean that you cannot use this method but\nmean that you cannot use this method but I am thinking like if I'm just teaching\nI am thinking like if I'm just teaching\nI am thinking like if I'm just teaching you something so I should just use the\nyou something so I should just use the\nyou something so I should just use the um you can say most recent and latest\num you can say most recent and latest\num you can say most recent and latest version that's it okay so currently I'm\nversion that's it okay so currently I'm\nversion that's it okay so currently I'm not seeing anything for messages so we\nnot seeing anything for messages so we\nnot seeing anything for messages so we can use the messages as is okay makes\ncan use the messages as is okay makes\ncan use the messages as is okay makes sense okay um okay we have a new thing\nsense okay um okay we have a new thing\nsense okay um okay we have a new thing uh langchain agents\nuh langchain agents\nuh langchain agents no this is same messages is the same.\nno this is same messages is the same.\nno this is same messages is the same. Okay, makes sense. Okay, so I'll be just\nOkay, makes sense. Okay, so I'll be just\nOkay, makes sense. Okay, so I'll be just um switching between these particular\num switching between these particular\num switching between these particular things so that I I will just provide you\nthings so that I I will just provide you\nthings so that I I will just provide you the latest code as well. By the way, you\nthe latest code as well. By the way, you\nthe latest code as well. By the way, you can use the older version as well. It is\ncan use the older version as well. It is\ncan use the older version as well. It is not old. It is just like it is the\nnot old. It is just like it is the\nnot old. It is just like it is the previous classes that we used to work we\nprevious classes that we used to work we\nprevious classes that we used to work we used to work with. Okay. Nothing is\nused to work with. Okay. Nothing is\nused to work with. Okay. Nothing is there to\nthere to\nthere to make you worried. Okay. So let's say\nmake you worried. Okay. So let's say\nmake you worried. Okay. So let's say messages. So what I will do? I will\nmessages. So what I will do? I will\nmessages. So what I will do? I will simply add the library\nsimply add the library\nsimply add the library and I will say\nand I will say\nand I will say UV add and it will be like lang chain\nUV add and it will be like lang chain\nUV add and it will be like lang chain core.\ncore.\ncore. Perfect. Lang chain core. Did it use\nPerfect. Lang chain core. Did it use\nPerfect. Lang chain core. Did it use lang chain or lang chain core?\nlang chain or lang chain core?\nlang chain or lang chain core? Uh langchain.\nUh langchain.\nUh langchain. Where are the messages?\nOkay. Lang chain do messages. We use\nOkay. Lang chain do messages. We use lang chain core. Okay. Now we can just\nlang chain core. Okay. Now we can just\nlang chain core. Okay. Now we can just use lang chain dot messages as well. But\nuse lang chain dot messages as well. But\nuse lang chain dot messages as well. But yeah that's fine because if it is\nyeah that's fine because if it is\nyeah that's fine because if it is outdated we will see the error. Don't\noutdated we will see the error. Don't\noutdated we will see the error. Don't worry. So I'll simply say from langchain\nworry. So I'll simply say from langchain\nworry. So I'll simply say from langchain code.\nOh we need to select the kernel.\nOh we need to select the kernel. Perfect.\nPerfect.\nPerfect. From lang chain code dot messages. We\nFrom lang chain code dot messages. We\nFrom lang chain code dot messages. We will import human message. And do you\nwill import human message. And do you\nwill import human message. And do you see all of these code completions? And\nsee all of these code completions? And\nsee all of these code completions? And do you know what this is not pilance?\ndo you know what this is not pilance?\ndo you know what this is not pilance? This is not something an extension. This\nThis is not something an extension. This\nThis is not something an extension. This is called anti-gravity's agent mode\nis called anti-gravity's agent mode\nis called anti-gravity's agent mode which is available for free. If you go\nwhich is available for free. If you go\nwhich is available for free. If you go to anti-gravity settings you will see um\nto anti-gravity settings you will see um\nto anti-gravity settings you will see um snooze button. This is for AI agent\nsnooze button. This is for AI agent\nsnooze button. This is for AI agent right? So they are running the free AI\nright? So they are running the free AI\nright? So they are running the free AI models. Okay the pro models for you for\nmodels. Okay the pro models for you for\nmodels. Okay the pro models for you for free because all these code completions\nfree because all these code completions\nfree because all these code completions are not free. If you use VS code you\nare not free. If you use VS code you\nare not free. If you use VS code you have to purchase it. It is not available\nhave to purchase it. It is not available\nhave to purchase it. It is not available for free. Anti-gravity has made it\nfor free. Anti-gravity has made it\nfor free. Anti-gravity has made it available for free maybe for shorter\navailable for free maybe for shorter\navailable for free maybe for shorter period of time because they have just\nperiod of time because they have just\nperiod of time because they have just recently launched anti-gravity but yeah\nrecently launched anti-gravity but yeah\nrecently launched anti-gravity but yeah if it is for free you should just take\nif it is for free you should just take\nif it is for free you should just take the advantage of it so it will be like\nthe advantage of it so it will be like\nthe advantage of it so it will be like human message and then it will be let's\nhuman message and then it will be let's\nhuman message and then it will be let's say system message\nsay system message\nsay system message and we want to import AI message perfect\nand we want to import AI message perfect\nand we want to import AI message perfect and once we have all of these messages I\nand once we have all of these messages I\nand once we have all of these messages I will start writing my messages I will\nwill start writing my messages I will\nwill start writing my messages I will say my messages\nsay my messages\nsay my messages simple\nsimple\nsimple My messages will be this. I want to say\nMy messages will be this. I want to say\nMy messages will be this. I want to say list. I will say human message. Okay,\nlist. I will say human message. Okay,\nlist. I will say human message. Okay, human messages. And then I will say\nhuman messages. And then I will say\nhuman messages. And then I will say content\ncontent\ncontent equals and whatever I want to say. I\nequals and whatever I want to say. I\nequals and whatever I want to say. I will say bro\nwill say bro\nwill say bro tell me a fun fact. Simple. But this\ntell me a fun fact. Simple. But this\ntell me a fun fact. Simple. But this time I will not just send this\ntime I will not just send this\ntime I will not just send this particular message to LLM. I will also\nparticular message to LLM. I will also\nparticular message to LLM. I will also set the tone. Remember system message.\nset the tone. Remember system message.\nset the tone. Remember system message. And it is saying you are a helpful\nAnd it is saying you are a helpful\nAnd it is saying you are a helpful assistant. Right? This is a tone. This\nassistant. Right? This is a tone. This\nassistant. Right? This is a tone. This is telling LLM that you are a helpful\nis telling LLM that you are a helpful\nis telling LLM that you are a helpful assistant. I will say you are a\nassistant. I will say you are a\nassistant. I will say you are a Gen Z assistant.\nGen Z assistant.\nGen Z assistant. Yeah. Gen Z assistant\nYeah. Gen Z assistant\nYeah. Gen Z assistant which\nwhich\nwhich or let's say who always answers in a fun\nor let's say who always answers in a fun\nor let's say who always answers in a fun way. Let's say we want to just set the\nway. Let's say we want to just set the\nway. Let's say we want to just set the tone set the tone for LLM, right? Wow.\ntone set the tone for LLM, right? Wow.\ntone set the tone for LLM, right? Wow. Can we do that? Yes, you can do that. So\nCan we do that? Yes, you can do that. So\nCan we do that? Yes, you can do that. So now what I will do? I will simply say\nnow what I will do? I will simply say\nnow what I will do? I will simply say llm. Oh, we need to import the model as\nllm. Oh, we need to import the model as\nllm. Oh, we need to import the model as well. This one remember we just create\nwell. This one remember we just create\nwell. This one remember we just create the model\nthe model\nthe model like this. So I can also create the\nlike this. So I can also create the\nlike this. So I can also create the model first of all. Perfect. So we have\nmodel first of all. Perfect. So we have\nmodel first of all. Perfect. So we have the model ready. Now I will simply say\nthe model ready. Now I will simply say\nthe model ready. Now I will simply say llm dot llm dot openi\nllm dot llm dot openi\nllm dot llm dot openi dot invoke. Now I will pass the entire\ndot invoke. Now I will pass the entire\ndot invoke. Now I will pass the entire list of messages. System message human\nlist of messages. System message human\nlist of messages. System message human message. Perfect. Now let's see what we\nmessage. Perfect. Now let's see what we\nmessage. Perfect. Now let's see what we get. I will say dot content. So now you\nget. I will say dot content. So now you\nget. I will say dot content. So now you will see that this LLM will try to\nwill see that this LLM will try to\nwill see that this LLM will try to answer in a fun way in a genz way which\nanswer in a fun way in a genz way which\nanswer in a fun way in a genz way which will actually\nwill actually\nwill actually you can say impact the response\nyou can say impact the response\nyou can say impact the response obviously because you are just setting\nobviously because you are just setting\nobviously because you are just setting the tone for that LLM and this way you\nthe tone for that LLM and this way you\nthe tone for that LLM and this way you can ask LLM to behave like anyone you\ncan ask LLM to behave like anyone you\ncan ask LLM to behave like anyone you can even say hey behave like let's say\ncan even say hey behave like let's say\ncan even say hey behave like let's say this person not like this person\nthis person not like this person\nthis person not like this person obviously system would should know about\nobviously system would should know about\nobviously system would should know about this thing and why it is taking so long\nthis thing and why it is taking so long\nthis thing and why it is taking so long connecting to kernel VNV. Yeah, like see\nconnecting to kernel VNV. Yeah, like see\nconnecting to kernel VNV. Yeah, like see this is anti-gravity. This is new thing.\nthis is anti-gravity. This is new thing.\nthis is anti-gravity. This is new thing. So there can be so many bugs. So do not\nSo there can be so many bugs. So do not\nSo there can be so many bugs. So do not need to worry at all. So you can simply\nneed to worry at all. So you can simply\nneed to worry at all. So you can simply re rerun it and you can just cancel it\nre rerun it and you can just cancel it\nre rerun it and you can just cancel it and\nand\nand you can\nyou can\nyou can maybe refresh it.\nmaybe refresh it.\nmaybe refresh it. So let me just reopen this anti-gravity.\nSo let me just reopen this anti-gravity.\nSo let me just reopen this anti-gravity. Okay, let's see because see that's why\nOkay, let's see because see that's why\nOkay, let's see because see that's why they just provide you these stuff for\nthey just provide you these stuff for\nthey just provide you these stuff for for free in the beginning so that you\nfor free in the beginning so that you\nfor free in the beginning so that you can just detect the you can say bugs and\ncan just detect the you can say bugs and\ncan just detect the you can say bugs and all. could not register service work\nall. could not register service work\nall. could not register service work blah blah blah open in text editor are\nblah blah blah open in text editor are\nblah blah blah open in text editor are you serious let me just open this open\nyou serious let me just open this open\nyou serious let me just open this open text editor so I think something is\ntext editor so I think something is\ntext editor so I think something is broken in anti-gravity that's why it is\nbroken in anti-gravity that's why it is\nbroken in anti-gravity that's why it is not able to open anything ipy and we\nnot able to open anything ipy and we\nnot able to open anything ipy and we could not initialize web view and blah\ncould not initialize web view and blah\ncould not initialize web view and blah blah blah so if it happens with you you\nblah blah so if it happens with you you\nblah blah so if it happens with you you do not need to worry at all you can\ndo not need to worry at all you can\ndo not need to worry at all you can simply close everything and just refresh\nsimply close everything and just refresh\nsimply close everything and just refresh your or basically terminate this\nyour or basically terminate this\nyour or basically terminate this particular application anti-gravity and\nparticular application anti-gravity and\nparticular application anti-gravity and just start it again it will work after\njust start it again it will work after\njust start it again it will work after some time because something is broken\nsome time because something is broken\nsome time because something is broken maybe on the IPI kernel Right? And\nmaybe on the IPI kernel Right? And\nmaybe on the IPI kernel Right? And nothing is broken on your in your code.\nnothing is broken on your in your code.\nnothing is broken on your in your code. Your code is running mine. And you can\nYour code is running mine. And you can\nYour code is running mine. And you can also switch to any other code editor,\nalso switch to any other code editor,\nalso switch to any other code editor, let's say VS code, which is more stable.\nlet's say VS code, which is more stable.\nlet's say VS code, which is more stable. But yeah,\nBut yeah,\nBut yeah, so I have just restarted my application\nso I have just restarted my application\nso I have just restarted my application like completely by ending it or\nlike completely by ending it or\nlike completely by ending it or terminating it through the task manager.\nterminating it through the task manager.\nterminating it through the task manager. So now let's try to see and it has\nSo now let's try to see and it has\nSo now let's try to see and it has detected our kernel this time. Very\ndetected our kernel this time. Very\ndetected our kernel this time. Very good. Slow claps. If anti gravity you\ngood. Slow claps. If anti gravity you\ngood. Slow claps. If anti gravity you are watching this, this is a bug for\nare watching this, this is a bug for\nare watching this, this is a bug for you. Fix it. Call your developer\nyou. Fix it. Call your developer\nyou. Fix it. Call your developer development team. Okay. So now let me\ndevelopment team. Okay. So now let me\ndevelopment team. Okay. So now let me just invoke it because we want to see\njust invoke it because we want to see\njust invoke it because we want to see the output.\nthe output.\nthe output. Perfect. Let's wait.\nPerfect. Let's wait.\nPerfect. Let's wait. And perfect. Now I got the response.\nAnd perfect. Now I got the response.\nAnd perfect. Now I got the response. Bro, fun fact. Same thing. Bananas are\nBro, fun fact. Same thing. Bananas are\nBro, fun fact. Same thing. Bananas are berries but strawberries aren't. But\nberries but strawberries aren't. But\nberries but strawberries aren't. But this time if you see the tone, it is\nthis time if you see the tone, it is\nthis time if you see the tone, it is also using let's say emojis or all those\nalso using let's say emojis or all those\nalso using let's say emojis or all those things that a jenzi will say.\nthings that a jenzi will say.\nthings that a jenzi will say. Make sense? See wild, right? So this is\nMake sense? See wild, right? So this is\nMake sense? See wild, right? So this is actually talking like a Gen Z. This is\nactually talking like a Gen Z. This is\nactually talking like a Gen Z. This is actually talking like a person who is a\nactually talking like a person who is a\nactually talking like a person who is a Gen Z because you have set the tone.\nGen Z because you have set the tone.\nGen Z because you have set the tone. That way you can\nThat way you can\nThat way you can modify not modify actually you can\nmodify not modify actually you can\nmodify not modify actually you can direct the responses in a certain\ndirect the responses in a certain\ndirect the responses in a certain direction. Let's say you want to build\ndirection. Let's say you want to build\ndirection. Let's say you want to build an LLM who is a kind of comedian who is\nan LLM who is a kind of comedian who is\nan LLM who is a kind of comedian who is a kind of you can say um write posts\na kind of you can say um write posts\na kind of you can say um write posts about some um laughter or who generates\nabout some um laughter or who generates\nabout some um laughter or who generates a content about you can say around\na content about you can say around\na content about you can say around laughter. So you need to set the tone\nlaughter. So you need to set the tone\nlaughter. So you need to set the tone for it. You can simply say you are a\nfor it. You can simply say you are a\nfor it. You can simply say you are a comedian who talks about this thing. So\ncomedian who talks about this thing. So\ncomedian who talks about this thing. So you need you can set the tone. So now do\nyou need you can set the tone. So now do\nyou need you can set the tone. So now do you know what when you send when you\nyou know what when you send when you\nyou know what when you send when you just set this tone whenever you will be\njust set this tone whenever you will be\njust set this tone whenever you will be just invoking this thing with this\njust invoking this thing with this\njust invoking this thing with this particular system message message\nparticular system message message\nparticular system message message with the system message it will remember\nwith the system message it will remember\nwith the system message it will remember this thing this is a very basic use case\nthis thing this is a very basic use case\nthis thing this is a very basic use case of system message. Okay. Now, we have\nof system message. Okay. Now, we have\nof system message. Okay. Now, we have some built-in messages as well. Okay.\nsome built-in messages as well. Okay.\nsome built-in messages as well. Okay. What is built-in messages? Built-in\nWhat is built-in messages? Built-in\nWhat is built-in messages? Built-in messages or I would say prompts because\nmessages or I would say prompts because\nmessages or I would say prompts because this is a very basic thing, right? I'm\nthis is a very basic thing, right? I'm\nthis is a very basic thing, right? I'm simply writing human message and system\nsimply writing human message and system\nsimply writing human message and system message. This is a very standard way.\nmessage. This is a very standard way.\nmessage. This is a very standard way. This is like open AI style messages.\nThis is like open AI style messages.\nThis is like open AI style messages. Okay? Because system message, human\nOkay? Because system message, human\nOkay? Because system message, human message is only available in OpenAI. If\nmessage is only available in OpenAI. If\nmessage is only available in OpenAI. If you want to use let's say any other\nyou want to use let's say any other\nyou want to use let's say any other model, it will not be like system\nmodel, it will not be like system\nmodel, it will not be like system message or human message. It will be\nmessage or human message. It will be\nmessage or human message. It will be something else. So for solving this\nsomething else. So for solving this\nsomething else. So for solving this problem, Langchin provides us something\nproblem, Langchin provides us something\nproblem, Langchin provides us something called as prompts. We have dedicated\ncalled as prompts. We have dedicated\ncalled as prompts. We have dedicated library for prompts.\nlibrary for prompts.\nlibrary for prompts. What does it mean? Basically, let's say\nWhat does it mean? Basically, let's say\nWhat does it mean? Basically, let's say I want to pass a prompt because see\nI want to pass a prompt because see\nI want to pass a prompt because see whatever I'm passing here is a prompt,\nwhatever I'm passing here is a prompt,\nwhatever I'm passing here is a prompt, right? This is a prompt.\nright? This is a prompt.\nright? This is a prompt. This is a prompt, right? So we have a\nThis is a prompt, right? So we have a\nThis is a prompt, right? So we have a dedicated library for prompt. Why?\ndedicated library for prompt. Why?\ndedicated library for prompt. Why? Why? By the way, very good. you can say\nWhy? By the way, very good. you can say\nWhy? By the way, very good. you can say um\num\num question like why.\nquestion like why.\nquestion like why. So in order to answer your why, let me\nSo in order to answer your why, let me\nSo in order to answer your why, let me just ask you simple question. Okay. And\njust ask you simple question. Okay. And\njust ask you simple question. Okay. And by the way, I have a quick question. We\nby the way, I have a quick question. We\nby the way, I have a quick question. We we are not importing our API key. How it\nwe are not importing our API key. How it\nwe are not importing our API key. How it is able to call it? How it is able to\nis able to call it? How it is able to\nis able to call it? How it is able to call it?\ncall it?\ncall it? Oh, makes sense because I think it has\nOh, makes sense because I think it has\nOh, makes sense because I think it has automatically\nautomatically\nautomatically uh loaded everything in our env. So let\nuh loaded everything in our env. So let\nuh loaded everything in our env. So let me just add that thing as well because\nme just add that thing as well because\nme just add that thing as well because we are not loading all the dot dot dot\nwe are not loading all the dot dot dot\nwe are not loading all the dot dot dot env functions but I obviously this is a\nenv functions but I obviously this is a\nenv functions but I obviously this is a smart code editor. If you face anything\nsmart code editor. If you face anything\nsmart code editor. If you face anything then you need to import this particular\nthen you need to import this particular\nthen you need to import this particular environment variable. Sometimes if the\nenvironment variable. Sometimes if the\nenvironment variable. Sometimes if the editor is smart they can automatically\neditor is smart they can automatically\neditor is smart they can automatically import everything in the environment\nimport everything in the environment\nimport everything in the environment because that makes sense. It is created\nbecause that makes sense. It is created\nbecause that makes sense. It is created in the root directory. But if you want\nin the root directory. But if you want\nin the root directory. But if you want to do it you can do it like this from\nto do it you can do it like this from\nto do it you can do it like this from not lang chain. This is not related to\nnot lang chain. This is not related to\nnot lang chain. This is not related to lang chain. This is like pure Python\nlang chain. This is like pure Python\nlang chain. This is like pure Python import OS and then we have a function\nimport OS and then we have a function\nimport OS and then we have a function called load env. And for that you need\ncalled load env. And for that you need\ncalled load env. And for that you need to import one library. It is called\nto import one library. It is called\nto import one library. It is called it is called let me open this terminal.\nit is called let me open this terminal.\nit is called let me open this terminal. Uh this terminal this terminal. No\nUh this terminal this terminal. No\nUh this terminal this terminal. No terminal has that activated. Oh yeah\nterminal has that activated. Oh yeah\nterminal has that activated. Oh yeah this one. Perfect. So now the library\nthis one. Perfect. So now the library\nthis one. Perfect. So now the library says\nsays\nsays UV add load\nenv. Perfect. So this is the particular\nenv. Perfect. So this is the particular library load load load say load load\nlibrary load load load say load load\nlibrary load load load say load load load env. So how we can just say we will\nload env. So how we can just say we will\nload env. So how we can just say we will simply say from\nsimply say from\nsimply say from load load or import\nload load or import\nload load or import load envy\nload envy\nload envy need to run the function called load\nneed to run the function called load\nneed to run the function called load dot n. What this function does this\ndot n. What this function does this\ndot n. What this function does this function\nfunction\nfunction um doesn't um do anything fancy. What\num doesn't um do anything fancy. What\num doesn't um do anything fancy. What this function will do it will simply\nthis function will do it will simply\nthis function will do it will simply load everything which is saved in this\nload everything which is saved in this\nload everything which is saved in this particular file. If you have API keys,\nparticular file. If you have API keys,\nparticular file. If you have API keys, secrets, variables, everything, it will\nsecrets, variables, everything, it will\nsecrets, variables, everything, it will load that thing into this particular\nload that thing into this particular\nload that thing into this particular notebook. Why we didn't need to write\nnotebook. Why we didn't need to write\nnotebook. Why we didn't need to write this? Because obviously this particular\nthis? Because obviously this particular\nthis? Because obviously this particular code editor is smart. It would have\ncode editor is smart. It would have\ncode editor is smart. It would have already loaded it in our system. So it\nalready loaded it in our system. So it\nalready loaded it in our system. So it is fine. But usually you should add\nis fine. But usually you should add\nis fine. But usually you should add this. So I'm simply adding it here so\nthis. So I'm simply adding it here so\nthis. So I'm simply adding it here so that if you see the error, you can\nthat if you see the error, you can\nthat if you see the error, you can simply refer it and you should do that.\nsimply refer it and you should do that.\nsimply refer it and you should do that. Let me just run it again. Pick the\nLet me just run it again. Pick the\nLet me just run it again. Pick the virtual environment and that's it.\nvirtual environment and that's it.\nvirtual environment and that's it. Make sense? So this is loaded. Now in\nMake sense? So this is loaded. Now in\nMake sense? So this is loaded. Now in order to set this particular environment\norder to set this particular environment\norder to set this particular environment variable, you can also write one more\nvariable, you can also write one more\nvariable, you can also write one more command. It says\ncommand. It says\ncommand. It says OS dot by the way there's no need to set\nOS dot by the way there's no need to set\nOS dot by the way there's no need to set it but I can just show you in order to\nit but I can just show you in order to\nit but I can just show you in order to test it. So I will simply shift this\ntest it. So I will simply shift this\ntest it. So I will simply shift this code cell to here. Now I want to make\ncode cell to here. Now I want to make\ncode cell to here. Now I want to make sure that my variable is loaded. How I\nsure that my variable is loaded. How I\nsure that my variable is loaded. How I can just make sure? I will simply say if\ncan just make sure? I will simply say if\ncan just make sure? I will simply say if OS.Eviron environ\nopen that means if it exist print\nopen that means if it exist print bro API key\nbro API key\nbro API key variable exists because obviously I do\nvariable exists because obviously I do\nvariable exists because obviously I do not want to show you but this is a\nnot want to show you but this is a\nnot want to show you but this is a confirmation if you see this message\nconfirmation if you see this message\nconfirmation if you see this message that means the variable name is created\nthat means the variable name is created\nthat means the variable name is created okay and it is taking a lot of time\nokay and it is taking a lot of time\nokay and it is taking a lot of time so as you can see it says bro API key\nso as you can see it says bro API key\nso as you can see it says bro API key variable exist that means this function\nvariable exist that means this function\nvariable exist that means this function function has loaded and created all of\nfunction has loaded and created all of\nfunction has loaded and created all of the environment variables. I can also\nthe environment variables. I can also\nthe environment variables. I can also say else value error open key gain not\nsay else value error open key gain not\nsay else value error open key gain not found. Perfect. So I hope you understood\nfound. Perfect. So I hope you understood\nfound. Perfect. So I hope you understood this concept. This is like a very small\nthis concept. This is like a very small\nthis concept. This is like a very small thing but it is very important. So again\nthing but it is very important. So again\nthing but it is very important. So again this function this function will let me\nthis function this function will let me\nthis function this function will let me just write it for you.\njust write it for you.\njust write it for you. See\nSee\nSee I'm doing so much for you. This function\nI'm doing so much for you. This function\nI'm doing so much for you. This function will load all the variables from the env\nwill load all the variables from the env\nwill load all the variables from the env file and will\nfile and will\nfile and will and will make them available in the OS.\nand will make them available in the OS.\nand will make them available in the OS. Environ dictionary that means\nEnviron dictionary that means\nEnviron dictionary that means environment variables. Perfect. So it\nenvironment variables. Perfect. So it\nenvironment variables. Perfect. So it will automatically create environment\nwill automatically create environment\nwill automatically create environment variables for us for all the variables\nvariables for us for all the variables\nvariables for us for all the variables which are there in the uni file.\nwhich are there in the uni file.\nwhich are there in the uni file. Perfect. And you should add this code in\nPerfect. And you should add this code in\nPerfect. And you should add this code in your all the notebooks. Yes, in all the\nyour all the notebooks. Yes, in all the\nyour all the notebooks. Yes, in all the notebooks. Let me just make it available\nnotebooks. Let me just make it available\nnotebooks. Let me just make it available here. And I know like why we didn't see\nhere. And I know like why we didn't see\nhere. And I know like why we didn't see any kind of error because we loaded\nany kind of error because we loaded\nany kind of error because we loaded everything in our\neverything in our\neverything in our env root directory. But it is a good\nenv root directory. But it is a good\nenv root directory. But it is a good thing to do this here. Perfect. Perfect.\nthing to do this here. Perfect. Perfect.\nthing to do this here. Perfect. Perfect. So now we have added this thing. So we\nSo now we have added this thing. So we\nSo now we have added this thing. So we are good because I was just looking at\nare good because I was just looking at\nare good because I was just looking at the code. I was like why we didn't\nthe code. I was like why we didn't\nthe code. I was like why we didn't create this thing and it is still\ncreate this thing and it is still\ncreate this thing and it is still working because thanks to integravity to\nworking because thanks to integravity to\nworking because thanks to integravity to some things and bro there are some bugs.\nsome things and bro there are some bugs.\nsome things and bro there are some bugs. So you need to fix it. So now I was just\nSo you need to fix it. So now I was just\nSo you need to fix it. So now I was just talking about prompts. So let me just\ntalking about prompts. So let me just\ntalking about prompts. So let me just write prompts.\nOo\nOo prompts. So whenever you just talk about\nprompts. So whenever you just talk about\nprompts. So whenever you just talk about prompts now basically what are prompts?\nprompts now basically what are prompts?\nprompts now basically what are prompts? Prompts are the messages that are sent\nPrompts are the messages that are sent\nPrompts are the messages that are sent to the LM. Simple thing. So that means\nto the LM. Simple thing. So that means\nto the LM. Simple thing. So that means messages and prompts are same in a\nmessages and prompts are same in a\nmessages and prompts are same in a nutshell. Yes. So why do we have two\nnutshell. Yes. So why do we have two\nnutshell. Yes. So why do we have two different categories messages and\ndifferent categories messages and\ndifferent categories messages and prompts?\nprompts?\nprompts? Messages are like more static way of\nMessages are like more static way of\nMessages are like more static way of sending the message to the LM. Prompts\nsending the message to the LM. Prompts\nsending the message to the LM. Prompts are more dynamic way and more you can\nare more dynamic way and more you can\nare more dynamic way and more you can say user friendly. Let me just show you\nsay user friendly. Let me just show you\nsay user friendly. Let me just show you one thing. So let's say\none thing. So let's say\none thing. So let's say uh let me just write it for you.\nuh let me just write it for you.\nuh let me just write it for you. Prompts are\nPrompts are\nPrompts are um more user friendly than messages.\num more user friendly than messages.\num more user friendly than messages. Perfect.\nPerfect.\nPerfect. So now let's say let me just share the\nSo now let's say let me just share the\nSo now let's say let me just share the use case. I want to send a message. I\nuse case. I want to send a message. I\nuse case. I want to send a message. I want to send a message to what to lm and\nwant to send a message to what to lm and\nwant to send a message to what to lm and I want to say llm dot open app\nI want to say llm dot open app\nI want to say llm dot open app dot invoke and let me just create the my\ndot invoke and let me just create the my\ndot invoke and let me just create the my messages first of all let me just copy\nmessages first of all let me just copy\nmessages first of all let me just copy it from here\nperfect this time I want to say tell me\nperfect this time I want to say tell me a fun fact about\na fun fact about\na fun fact about about let's say\nabout let's say\nabout let's say honey I want to know the fun fact about\nhoney I want to know the fun fact about\nhoney I want to know the fun fact about running. Okay, simple. Okay, I will\nrunning. Okay, simple. Okay, I will\nrunning. Okay, simple. Okay, I will simply say invoke content and I will get\nsimply say invoke content and I will get\nsimply say invoke content and I will get the message. System message is not\nthe message. System message is not\nthe message. System message is not defined. Very good. We need to run this\ndefined. Very good. We need to run this\ndefined. Very good. We need to run this as well. Okay, let me run this.\nLet me run this. Okay.\nLet me run this. Okay. Okay. Perfect. So, if I just run this, I\nOkay. Perfect. So, if I just run this, I\nOkay. Perfect. So, if I just run this, I will get the message about honey. Okay.\nwill get the message about honey. Okay.\nwill get the message about honey. Okay. Makes sense. Makes sense. Okay. Very\nMakes sense. Makes sense. Okay. Very\nMakes sense. Makes sense. Okay. Very good.\ngood.\ngood. Perfect, bro. Fun fact, honey basically\nPerfect, bro. Fun fact, honey basically\nPerfect, bro. Fun fact, honey basically never dies. Oh, really? Archaeologists\nnever dies. Oh, really? Archaeologists\nnever dies. Oh, really? Archaeologists have found jars of honey in ancient\nhave found jars of honey in ancient\nhave found jars of honey in ancient Egyptian toms thousands of years old\nEgyptian toms thousands of years old\nEgyptian toms thousands of years old that Oh, bro,\nthat Oh, bro,\nthat Oh, bro, that was still edible. That's because\nthat was still edible. That's because\nthat was still edible. That's because honey's low water content and high\nhoney's low water content and high\nhoney's low water content and high acidity keep bacteria and microbes from\nacidity keep bacteria and microbes from\nacidity keep bacteria and microbes from growing. Wow.\ngrowing. Wow.\ngrowing. Wow. Wow. It takes about 2 million flowers\nWow. It takes about 2 million flowers\nWow. It takes about 2 million flowers visit. Oh man, honey is mysterious. So,\nvisit. Oh man, honey is mysterious. So,\nvisit. Oh man, honey is mysterious. So, now we have got the fun fact about\nnow we have got the fun fact about\nnow we have got the fun fact about honey, right? And let's say we do not\nhoney, right? And let's say we do not\nhoney, right? And let's say we do not need to set this system message. We are\nneed to set this system message. We are\nneed to set this system message. We are just okay with that. But this is the\njust okay with that. But this is the\njust okay with that. But this is the thing. We want to set the human message\nthing. We want to set the human message\nthing. We want to set the human message dynamically. If let's say you are\ndynamically. If let's say you are\ndynamically. If let's say you are building something, okay, and you want\nbuilding something, okay, and you want\nbuilding something, okay, and you want to get this message from a user, right?\nto get this message from a user, right?\nto get this message from a user, right? So how will you get it? Every time will\nSo how will you get it? Every time will\nSo how will you get it? Every time will you hardcode it? Will you go to your\nyou hardcode it? Will you go to your\nyou hardcode it? Will you go to your back end and you will change the code?\nback end and you will change the code?\nback end and you will change the code? Obviously no, right? So what I can do? I\nObviously no, right? So what I can do? I\nObviously no, right? So what I can do? I can simply get the input from the user.\ncan simply get the input from the user.\ncan simply get the input from the user. Okay? I will get the input from the\nOkay? I will get the input from the\nOkay? I will get the input from the user. Let's say\nuser. Let's say\nuser. Let's say input equals input function\ninput equals input function\ninput equals input function or let's say\nor let's say\nor let's say user input equals to input.\nuser input equals to input.\nuser input equals to input. What you what do you want to ask?\nWhat you what do you want to ask?\nWhat you what do you want to ask? Simple. I can create this thing. Now\nSimple. I can create this thing. Now\nSimple. I can create this thing. Now user input will be the topic of the fun\nuser input will be the topic of the fun\nuser input will be the topic of the fun fact. Now I want to make it dynamic.\nfact. Now I want to make it dynamic.\nfact. Now I want to make it dynamic. How? I can just make it. Now I will use\nHow? I can just make it. Now I will use\nHow? I can just make it. Now I will use it something called as prompts. So this\nit something called as prompts. So this\nit something called as prompts. So this is this thing you already know right? I\nis this thing you already know right? I\nis this thing you already know right? I can just delete this right. So I will\ncan just delete this right. So I will\ncan just delete this right. So I will simply say from langchain\nsimply say from langchain\nsimply say from langchain core. Now again core because core is\ncore. Now again core because core is\ncore. Now again core because core is like very important. core.prompts\nlike very important. core.prompts\nlike very important. core.prompts import\nimport\nimport prompt template. We have something\nprompt template. We have something\nprompt template. We have something called as prompt template. Now what is\ncalled as prompt template. Now what is\ncalled as prompt template. Now what is this prompt template? Prompt template is\nthis prompt template? Prompt template is\nthis prompt template? Prompt template is a kind of special you can say module\na kind of special you can say module\na kind of special you can say module that we have in um langen core class\nthat we have in um langen core class\nthat we have in um langen core class which we can build which we can use to\nwhich we can build which we can use to\nwhich we can build which we can use to build our prompt dynamically\nbuild our prompt dynamically\nbuild our prompt dynamically really dynamically\nreally dynamically\nreally dynamically okay so now what I will do I will simply\nokay so now what I will do I will simply\nokay so now what I will do I will simply say um\nsay um\nsay um input\ninput\ninput dynamic input let's say\ndynamic input let's say\ndynamic input let's say dynamic input or let's say dynamic\ndynamic input or let's say dynamic\ndynamic input or let's say dynamic prompt because everything is prompt I\nprompt because everything is prompt I\nprompt because everything is prompt I will say prompt prompt template dot from\nwill say prompt prompt template dot from\nwill say prompt prompt template dot from template. prompt template dot from\ntemplate. prompt template dot from\ntemplate. prompt template dot from template. Okay, then I need to write the\ntemplate. Okay, then I need to write the\ntemplate. Okay, then I need to write the prompt. I will say write\nprompt. I will say write\nprompt. I will say write a fun fact about now I don't need I I do\na fun fact about now I don't need I I do\na fun fact about now I don't need I I do not know the topic. This is dynamic.\nnot know the topic. This is dynamic.\nnot know the topic. This is dynamic. Have you ever worked with frings in\nHave you ever worked with frings in\nHave you ever worked with frings in Python? It is very similar to that. It\nPython? It is very similar to that. It\nPython? It is very similar to that. It is very similar to that. But obviously\nis very similar to that. But obviously\nis very similar to that. But obviously we do not use fing directly because\nwe do not use fing directly because\nwe do not use fing directly because people can use this same class for so\npeople can use this same class for so\npeople can use this same class for so many other purposes as well. So we\nmany other purposes as well. So we\nmany other purposes as well. So we simply write write a function or you can\nsimply write write a function or you can\nsimply write write a function or you can say write a fun fact about topic. Now\nsay write a fun fact about topic. Now\nsay write a fun fact about topic. Now this topic will be coming from the user.\nthis topic will be coming from the user.\nthis topic will be coming from the user. Make sense? Okay. So I'll simply say\nMake sense? Okay. So I'll simply say\nMake sense? Okay. So I'll simply say dynamic prompt. Okay. And prompt\ndynamic prompt. Okay. And prompt\ndynamic prompt. Okay. And prompt template dot from template write a fun\ntemplate dot from template write a fun\ntemplate dot from template write a fun fact about topic.\nfact about topic.\nfact about topic. Now what is this topic? Topic is\nequals input enter a topic for fun fact.\nequals input enter a topic for fun fact. Perfect. Make sense? So this will bring\nPerfect. Make sense? So this will bring\nPerfect. Make sense? So this will bring the topic from the user in the runtime.\nthe topic from the user in the runtime.\nthe topic from the user in the runtime. Okay. This is the dynamic prompt. Prompt\nOkay. This is the dynamic prompt. Prompt\nOkay. This is the dynamic prompt. Prompt template dotprompt template. Write a fun\ntemplate dotprompt template. Write a fun\ntemplate dotprompt template. Write a fun fact about topic. Simple. Now I need to\nfact about topic. Simple. Now I need to\nfact about topic. Simple. Now I need to inject user input into this thing. User\ninject user input into this thing. User\ninject user input into this thing. User input into topic.\ninput into topic.\ninput into topic. Make sense? Okay. User input into topic.\nMake sense? Okay. User input into topic.\nMake sense? Okay. User input into topic. Okay. So now what will happen if I say\nOkay. So now what will happen if I say\nOkay. So now what will happen if I say dynamic prompt?\ndynamic prompt?\ndynamic prompt? That's it. If I just show you the\nThat's it. If I just show you the\nThat's it. If I just show you the dynamic prompt, do you know what will\ndynamic prompt, do you know what will\ndynamic prompt, do you know what will happen? Do you know what will happen\nif I write dynamic prompt and I simply\nif I write dynamic prompt and I simply write this? Let me just run it. And it\nwrite this? Let me just run it. And it\nwrite this? Let me just run it. And it is asking me to put the fun fact. I will\nis asking me to put the fun fact. I will\nis asking me to put the fun fact. I will simply say this time let's say flowers.\nsimply say this time let's say flowers.\nsimply say this time let's say flowers. Okay. Hit enter.\nOkay. Hit enter.\nOkay. Hit enter. Now it has created this function\nNow it has created this function\nNow it has created this function automatically. Prompt template input\nautomatically. Prompt template input\nautomatically. Prompt template input variable topic which is the topic\nvariable topic which is the topic\nvariable topic which is the topic variable. Input types. We do not have\nvariable. Input types. We do not have\nvariable. Input types. We do not have any kind of type. Partial variables. We\nany kind of type. Partial variables. We\nany kind of type. Partial variables. We do not have any kind of partial\ndo not have any kind of partial\ndo not have any kind of partial variables. And it has injected this\nvariables. And it has injected this\nvariables. And it has injected this particular thing automatically.\nparticular thing automatically.\nparticular thing automatically. See template write a fun fact about\nSee template write a fun fact about\nSee template write a fun fact about topic. This has injected this variable\ntopic. This has injected this variable\ntopic. This has injected this variable but not the value of the topic but topic\nbut not the value of the topic but topic\nbut not the value of the topic but topic variable only.\nvariable only.\nvariable only. Topic variable only. Now if we need to\nTopic variable only. Now if we need to\nTopic variable only. Now if we need to inject the value as well. I will simply\ninject the value as well. I will simply\ninject the value as well. I will simply say dynamic prompt dot invoke and then\nsay dynamic prompt dot invoke and then\nsay dynamic prompt dot invoke and then you can say dictionary topic\nyou can say dictionary topic\nyou can say dictionary topic is user input. What did I do? I simply\nis user input. What did I do? I simply\nis user input. What did I do? I simply invoked it. Okay. And I passed a\ninvoked it. Okay. And I passed a\ninvoked it. Okay. And I passed a dictionary. Key is topic which should be\ndictionary. Key is topic which should be\ndictionary. Key is topic which should be exactly same like this. And user input\nexactly same like this. And user input\nexactly same like this. And user input which is this one.\nwhich is this one.\nwhich is this one. User input is what value I I will be\nUser input is what value I I will be\nUser input is what value I I will be passing. Now let's try to run this\npassing. Now let's try to run this\npassing. Now let's try to run this again. You will see flowers. Hit enter.\nagain. You will see flowers. Hit enter.\nagain. You will see flowers. Hit enter. Now see it has created this prompt for\nNow see it has created this prompt for\nNow see it has created this prompt for me. Text equals write a fun fact about\nme. Text equals write a fun fact about\nme. Text equals write a fun fact about flowers. Wow. And now we can pass this\nflowers. Wow. And now we can pass this\nflowers. Wow. And now we can pass this value to my LLM. I will simply say ready\nvalue to my LLM. I will simply say ready\nvalue to my LLM. I will simply say ready prompt because this is a ready prompt\nprompt because this is a ready prompt\nprompt because this is a ready prompt which can be used in llm. I will simply\nwhich can be used in llm. I will simply\nwhich can be used in llm. I will simply say llm.invoke.content.\nsay llm.invoke.content.\nsay llm.invoke.content. Perfect. Now let's run this again. I\nPerfect. Now let's run this again. I\nPerfect. Now let's run this again. I will simply say this time let's say\nwill simply say this time let's say\nwill simply say this time let's say flowers. Okay. Hit enter. Now this time\nflowers. Okay. Hit enter. Now this time\nflowers. Okay. Hit enter. Now this time what will happen step by step. I have\nwhat will happen step by step. I have\nwhat will happen step by step. I have just provided the u message about\njust provided the u message about\njust provided the u message about flowers. Right? It will inject that\nflowers. Right? It will inject that\nflowers. Right? It will inject that thing using invoke method and it will\nthing using invoke method and it will\nthing using invoke method and it will create the ready prompt which is this\ncreate the ready prompt which is this\ncreate the ready prompt which is this one which doesn't have any kind of\none which doesn't have any kind of\none which doesn't have any kind of variables because it has injected it\nvariables because it has injected it\nvariables because it has injected it using invoke and this ready prompt will\nusing invoke and this ready prompt will\nusing invoke and this ready prompt will go inside the llm simple and now we have\ngo inside the llm simple and now we have\ngo inside the llm simple and now we have got the fun fact what looks like a\ngot the fun fact what looks like a\ngot the fun fact what looks like a single sunflower or daisy is actually\nsingle sunflower or daisy is actually\nsingle sunflower or daisy is actually cluster of hundreds sometimes thousands\ncluster of hundreds sometimes thousands\ncluster of hundreds sometimes thousands of tiny flowers what called fluids I\nof tiny flowers what called fluids I\nof tiny flowers what called fluids I think I read about fluids in\nthink I read about fluids in\nthink I read about fluids in 10th standard I think. So florate can\n10th standard I think. So florate can\n10th standard I think. So florate can produce a seed. So aa sunflower head is\nproduce a seed. So aa sunflower head is\nproduce a seed. So aa sunflower head is a whole bouquet. Oh wow. Okay. So now I\na whole bouquet. Oh wow. Okay. So now I\na whole bouquet. Oh wow. Okay. So now I hope that you have got the context like\nhope that you have got the context like\nhope that you have got the context like why do we use template? Now you will say\nwhy do we use template? Now you will say\nwhy do we use template? Now you will say anula do you know what everything is\nanula do you know what everything is\nanula do you know what everything is fine but that thing is missing. Which\nfine but that thing is missing. Which\nfine but that thing is missing. Which thing? This thing this this thing is\nthing? This thing this this thing is\nthing? This thing this this thing is missing. This this talk nicely and be a\nmissing. This this talk nicely and be a\nmissing. This this talk nicely and be a little polite. How we can just set the\nlittle polite. How we can just set the\nlittle polite. How we can just set the tone? How we can just set the tone for\ntone? How we can just set the tone for\ntone? How we can just set the tone for our conversation? Bro, this is this is\nour conversation? Bro, this is this is\nour conversation? Bro, this is this is required bro. We have to tell Simon like\nrequired bro. We have to tell Simon like\nrequired bro. We have to tell Simon like talk nicely to Rahul. We have to tell\ntalk nicely to Rahul. We have to tell\ntalk nicely to Rahul. We have to tell it. Okay. If you want to tell tell LLM.\nit. Okay. If you want to tell tell LLM.\nit. Okay. If you want to tell tell LLM. So what you can do? You can use\nSo what you can do? You can use\nSo what you can do? You can use something called as chat prompt\nsomething called as chat prompt\nsomething called as chat prompt template. There's another class. So if\ntemplate. There's another class. So if\ntemplate. There's another class. So if you do not want to set the tone, you\nyou do not want to set the tone, you\nyou do not want to set the tone, you will use prompt template which is very\nwill use prompt template which is very\nwill use prompt template which is very similar to your frings. If you want to\nsimilar to your frings. If you want to\nsimilar to your frings. If you want to set the tone the same way you set here\nset the tone the same way you set here\nset the tone the same way you set here in the messages we have a class as well.\nin the messages we have a class as well.\nin the messages we have a class as well. What we are you the part of langchain?\nWhat we are you the part of langchain?\nWhat we are you the part of langchain? No but yeah not now maybe\nNo but yeah not now maybe\nNo but yeah not now maybe soon. So yeah so let's say from\nsoon. So yeah so let's say from\nsoon. So yeah so let's say from langchain.core\nlangchain.core\nlangchain.core or core.prompts\nor core.prompts\nor core.prompts I will import chat prompt template. This\nI will import chat prompt template. This\nI will import chat prompt template. This is special in this you can just set the\nis special in this you can just set the\nis special in this you can just set the tone as well. So I'll simply say now my\ntone as well. So I'll simply say now my\ntone as well. So I'll simply say now my prompt look will look like this. prompt\nprompt look will look like this. prompt\nprompt look will look like this. prompt template.\ntemplate.\ntemplate. I will say chat prompt template. Now I\nI will say chat prompt template. Now I\nI will say chat prompt template. Now I will say from messages. Here we say\nwill say from messages. Here we say\nwill say from messages. Here we say prompt template dot from template. Here\nprompt template dot from template. Here\nprompt template dot from template. Here we say prompt chat prompt template dot\nwe say prompt chat prompt template dot\nwe say prompt chat prompt template dot from messages because we are creating a\nfrom messages because we are creating a\nfrom messages because we are creating a messages list. Right? Messages.\nmessages list. Right? Messages.\nmessages list. Right? Messages. Now this is a list. Now this is very\nNow this is a list. Now this is very\nNow this is a list. Now this is very important. Now we do not use system\nimportant. Now we do not use system\nimportant. Now we do not use system message, human message, uh AI message,\nmessage, human message, uh AI message,\nmessage, human message, uh AI message, whatever. We simply use a kind of tpple\nwhatever. We simply use a kind of tpple\nwhatever. We simply use a kind of tpple and we simply say user. User means me me\nand we simply say user. User means me me\nand we simply say user. User means me me Rahul. Rahul user means Rahul. Okay. And\nRahul. Rahul user means Rahul. Okay. And\nRahul. Rahul user means Rahul. Okay. And it is the\nit is the\nit is the request. Write a\nrequest. Write a\nrequest. Write a fun fact about\nfun fact about\nfun fact about topic. Right now the second thing will\ntopic. Right now the second thing will\ntopic. Right now the second thing will be\nbe\nbe system. This is tone that we are telling\nsystem. This is tone that we are telling\nsystem. This is tone that we are telling Siman. right system. You are a\nSiman. right system. You are a\nSiman. right system. You are a polite\npolite\npolite um assistant.\num assistant.\num assistant. Simple. Perfect.\nSimple. Perfect.\nSimple. Perfect. Makes sense. So this is the list that we\nMakes sense. So this is the list that we\nMakes sense. So this is the list that we need to create. But here we do not need\nneed to create. But here we do not need\nneed to create. But here we do not need to write system message um human message\nto write system message um human message\nto write system message um human message and all those things. No chat prompt\nand all those things. No chat prompt\nand all those things. No chat prompt template class will automatically do it\ntemplate class will automatically do it\ntemplate class will automatically do it for us. Do you want to see? Let me\nfor us. Do you want to see? Let me\nfor us. Do you want to see? Let me assure you. prompt template. Let me show\nassure you. prompt template. Let me show\nassure you. prompt template. Let me show you the ready ready prompt template.\nyou the ready ready prompt template.\nyou the ready ready prompt template. Ready prompt or let's say ready prompt\nReady prompt or let's say ready prompt\nReady prompt or let's say ready prompt dot invoke. Let's say we want to write a\ndot invoke. Let's say we want to write a\ndot invoke. Let's say we want to write a fun fact about AI. Do you know what it\nfun fact about AI. Do you know what it\nfun fact about AI. Do you know what it will create for us? What it will create?\nwill create for us? What it will create?\nwill create for us? What it will create? It will create the same thing like this.\nIt will create the same thing like this.\nIt will create the same thing like this. But we do not need to do hard work like\nBut we do not need to do hard work like\nBut we do not need to do hard work like this. We can simply say generate this\nthis. We can simply say generate this\nthis. We can simply say generate this prompt for us using chat prompt\nprompt for us using chat prompt\nprompt for us using chat prompt template. See chat prompt template value\ntemplate. See chat prompt template value\ntemplate. See chat prompt template value messages system message. See we didn't\nmessages system message. See we didn't\nmessages system message. See we didn't write system message. We simply write\nwrite system message. We simply write\nwrite system message. We simply write system and we simply write our message\nsystem and we simply write our message\nsystem and we simply write our message and it automatically passed or created\nand it automatically passed or created\nand it automatically passed or created this particular string system message\nthis particular string system message\nthis particular string system message content equals to this and blah blah\ncontent equals to this and blah blah\ncontent equals to this and blah blah blah blah blah. See I can even display\nblah blah blah. See I can even display\nblah blah blah. See I can even display you the better results. So I will simply\nyou the better results. So I will simply\nyou the better results. So I will simply say\nsay\nsay dot messages. See this is a list. This\ndot messages. See this is a list. This\ndot messages. See this is a list. This is the same list we passed here here but\nis the same list we passed here here but\nis the same list we passed here here but this time we have more control because\nthis time we have more control because\nthis time we have more control because we can just inject the variables in the\nwe can just inject the variables in the\nwe can just inject the variables in the runtime\nruntime\nruntime and we can get the message make sense\nand we can get the message make sense\nand we can get the message make sense okay make sense and obviously here I'm\nokay make sense and obviously here I'm\nokay make sense and obviously here I'm just hard coding it but we can just ask\njust hard coding it but we can just ask\njust hard coding it but we can just ask the user for it user input\nthe user for it user input\nthe user for it user input input\nenter\nenter a topic\na topic\na topic Perfect. Makes sense. And then we can\nPerfect. Makes sense. And then we can\nPerfect. Makes sense. And then we can pass this particular prompt to our LLM,\npass this particular prompt to our LLM,\npass this particular prompt to our LLM, right? I will simply say ready prompt.\nright? I will simply say ready prompt.\nright? I will simply say ready prompt. Yeah, you can just pass the ready\nYeah, you can just pass the ready\nYeah, you can just pass the ready prompt. You do not need to cut down the\nprompt. You do not need to cut down the\nprompt. You do not need to cut down the messages and all. It is fine because\nmessages and all. It is fine because\nmessages and all. It is fine because this LLM is uh has some good\nthis LLM is uh has some good\nthis LLM is uh has some good understanding of the context. It can\nunderstanding of the context. It can\nunderstanding of the context. It can just handle that thing. But if you just\njust handle that thing. But if you just\njust handle that thing. But if you just want to be pro, you can simply save\nwant to be pro, you can simply save\nwant to be pro, you can simply save messages. If you just want to save some\nmessages. If you just want to save some\nmessages. If you just want to save some tokens if you are a pro developer. Yeah.\ntokens if you are a pro developer. Yeah.\ntokens if you are a pro developer. Yeah. So let's try to run this. And this time\nSo let's try to run this. And this time\nSo let's try to run this. And this time I want to run a topic about let's say\nI want to run a topic about let's say\nI want to run a topic about let's say code editors let's say or let's say\ncode editors let's say or let's say\ncode editors let's say or let's say Google\nGoogle\nGoogle let's say\nlet's say\nlet's say let's read the fun fact about Google's\nlet's read the fun fact about Google's\nlet's read the fun fact about Google's name is play on the word Google one\nname is play on the word Google one\nname is play on the word Google one followed by zero 100 zeros chosen to\nfollowed by zero 100 zeros chosen to\nfollowed by zero 100 zeros chosen to reflect the founders's mission to\nreflect the founders's mission to\nreflect the founders's mission to organize a huge amount of information\norganize a huge amount of information\norganize a huge amount of information and before that the search engine was\nand before that the search engine was\nand before that the search engine was actually called as backup.\nactually called as backup.\nactually called as backup. Oh, this I didn't know this. The search\nOh, this I didn't know this. The search\nOh, this I didn't know this. The search engine was actually called backup. Okay.\nengine was actually called backup. Okay.\nengine was actually called backup. Okay. Okay. H\nOkay. H\nOkay. H okay. So, this was all about your\nokay. So, this was all about your\nokay. So, this was all about your templates, your messages, whatever we\ntemplates, your messages, whatever we\ntemplates, your messages, whatever we use.\nuse.\nuse. Make sense? And I hope that you have\nMake sense? And I hope that you have\nMake sense? And I hope that you have clear understanding. Now, the good thing\nclear understanding. Now, the good thing\nclear understanding. Now, the good thing is you can create as many variables as\nis you can create as many variables as\nis you can create as many variables as you want to in your messages.\nyou want to in your messages.\nyou want to in your messages. Make sense? What do I mean? Because\nMake sense? What do I mean? Because\nMake sense? What do I mean? Because let's say you want to define multiple.\nlet's say you want to define multiple.\nlet's say you want to define multiple. Let's say you want to\nLet's say you want to\nLet's say you want to um choose the system um tone as well on\num choose the system um tone as well on\num choose the system um tone as well on your own in the runtime. You are a\nyour own in the runtime. You are a\nyour own in the runtime. You are a tone assistant. That means I will set\ntone assistant. That means I will set\ntone assistant. That means I will set the tone as well. So if I just ask this\nthe tone as well. So if I just ask this\nthe tone as well. So if I just ask this now, I will simply say us a tone\nnow, I will simply say us a tone\nnow, I will simply say us a tone enter a tone. Right? So I will simply\nenter a tone. Right? So I will simply\nenter a tone. Right? So I will simply add this variable as well here. So I can\nadd this variable as well here. So I can\nadd this variable as well here. So I can add as many variables as as I can. It's\nadd as many variables as as I can. It's\nadd as many variables as as I can. It's not like I can only add one variable\nnot like I can only add one variable\nnot like I can only add one variable while just creating this prompt. So I\nwhile just creating this prompt. So I\nwhile just creating this prompt. So I can just run this now. It will ask me\ncan just run this now. It will ask me\ncan just run this now. It will ask me topic. Let's get the fun fact of\ntopic. Let's get the fun fact of\ntopic. Let's get the fun fact of Microsoft.\nOkay. Tone is let's say funny.\nOkay. Tone is let's say funny. Let's see what we get.\nLet's see. Let's see. Let's see.\nLet's see. Let's see. Let's see. Microsoft started in 1975. Microsoft.\nMicrosoft started in 1975. Microsoft.\nMicrosoft started in 1975. Microsoft. Yes. With hyphen. Oh, okay. And it was\nYes. With hyphen. Oh, okay. And it was\nYes. With hyphen. Oh, okay. And it was founded in Alber, New Mexico, not\nfounded in Alber, New Mexico, not\nfounded in Alber, New Mexico, not Seattle. Okay. Proof that one of world's\nSeattle. Okay. Proof that one of world's\nSeattle. Okay. Proof that one of world's biggest tech companies literally grew\nbiggest tech companies literally grew\nbiggest tech companies literally grew out of the desert. Oh, wow. And outgrew\nout of the desert. Oh, wow. And outgrew\nout of the desert. Oh, wow. And outgrew its punctuation.\nits punctuation.\nits punctuation. Yes, this was a fun fact. Okay. So this\nYes, this was a fun fact. Okay. So this\nYes, this was a fun fact. Okay. So this was all about your chat problem\nwas all about your chat problem\nwas all about your chat problem templates and templates that you use.\ntemplates and templates that you use.\ntemplates and templates that you use. Make sense? Make sense? Makes sense. I\nMake sense? Make sense? Makes sense. I\nMake sense? Make sense? Makes sense. I hope that you have a clear\nhope that you have a clear\nhope that you have a clear understanding. Very good. Now we are\nunderstanding. Very good. Now we are\nunderstanding. Very good. Now we are going to talk about a very very very\ngoing to talk about a very very very\ngoing to talk about a very very very important topic. It's called structured\nimportant topic. It's called structured\nimportant topic. It's called structured output with LLM.\noutput with LLM.\noutput with LLM. If you are aware of something called as\nIf you are aware of something called as\nIf you are aware of something called as pientic,\npientic,\npientic, then it will be a piece of cake for you.\nthen it will be a piece of cake for you.\nthen it will be a piece of cake for you. If you are not aware about pientic, let\nIf you are not aware about pientic, let\nIf you are not aware about pientic, let me just tell you what is that first of\nme just tell you what is that first of\nme just tell you what is that first of all because if you do not know about\nall because if you do not know about\nall because if you do not know about pientic, you are missing very important\npientic, you are missing very important\npientic, you are missing very important concept, okay, in python especially. So\nconcept, okay, in python especially. So\nconcept, okay, in python especially. So let me show you what exactly is pyek. So\nlet me show you what exactly is pyek. So\nlet me show you what exactly is pyek. So let me just open my browser and let me\nlet me just open my browser and let me\nlet me just open my browser and let me just write pantic. Basically pantic is\njust write pantic. Basically pantic is\njust write pantic. Basically pantic is nothing but a kind of um parsing plus\nnothing but a kind of um parsing plus\nnothing but a kind of um parsing plus data validation library. Both parsing\ndata validation library. Both parsing\ndata validation library. Both parsing plus data validation. See pantic\nplus data validation. See pantic\nplus data validation. See pantic validation observability. Let me click\nvalidation observability. Let me click\nvalidation observability. Let me click this. Click on this. Oh, they have\nthis. Click on this. Oh, they have\nthis. Click on this. Oh, they have changed their UI. Nice. Earlier it was\nchanged their UI. Nice. Earlier it was\nchanged their UI. Nice. Earlier it was like very pinkish. Now they have just\nlike very pinkish. Now they have just\nlike very pinkish. Now they have just changed it to purplish. Okay, that's\nchanged it to purplish. Okay, that's\nchanged it to purplish. Okay, that's cool. That's cool. That's cool. So now,\ncool. That's cool. That's cool. So now,\ncool. That's cool. That's cool. So now, by the way, where's the documentation?\nby the way, where's the documentation?\nby the way, where's the documentation? Mhm.\nMhm.\nMhm. Logifier enterprise blog documentations.\nLogifier enterprise blog documentations.\nLogifier enterprise blog documentations. Okay. Py techch validation. Yes. Okay.\nOkay. Py techch validation. Yes. Okay.\nOkay. Py techch validation. Yes. Okay. This is perfect. So this is basically\nThis is perfect. So this is basically\nThis is perfect. So this is basically their official documentation that you\ntheir official documentation that you\ntheir official documentation that you should read. And don't worry, I will\nshould read. And don't worry, I will\nshould read. And don't worry, I will also tell you about pyentic very quickly\nalso tell you about pyentic very quickly\nalso tell you about pyentic very quickly because you'll be just directly using\nbecause you'll be just directly using\nbecause you'll be just directly using pyntic with lms. Basically, as I said,\npyntic with lms. Basically, as I said,\npyntic with lms. Basically, as I said, pyic is nothing but a kind of library\npyic is nothing but a kind of library\npyic is nothing but a kind of library which helps us to validate our data,\nwhich helps us to validate our data,\nwhich helps us to validate our data, validate our output and parse it as\nvalidate our output and parse it as\nvalidate our output and parse it as well. It is both parsing and validation\nwell. It is both parsing and validation\nwell. It is both parsing and validation both. Okay. So, this is a code that we\nboth. Okay. So, this is a code that we\nboth. Okay. So, this is a code that we write from py techch import base model.\nwrite from py techch import base model.\nwrite from py techch import base model. base model is a class that we use or\nbase model is a class that we use or\nbase model is a class that we use or inherit into our class and then we just\ninherit into our class and then we just\ninherit into our class and then we just create our schema and we can do a lot of\ncreate our schema and we can do a lot of\ncreate our schema and we can do a lot of things and you can just read this as\nthings and you can just read this as\nthings and you can just read this as well like this is the kind of schema\nwell like this is the kind of schema\nwell like this is the kind of schema that they have created and they are\nthat they have created and they are\nthat they have created and they are saying why use pyche power by typends\nsaying why use pyche power by typends\nsaying why use pyche power by typends speed JSON schema strict and lax mode\nspeed JSON schema strict and lax mode\nspeed JSON schema strict and lax mode data classes type dicts so if you are\ndata classes type dicts so if you are\ndata classes type dicts so if you are familiar with data class as well if you\nfamiliar with data class as well if you\nfamiliar with data class as well if you are then it is like very very simple for\nare then it is like very very simple for\nare then it is like very very simple for you okay if you're familiar with type\nyou okay if you're familiar with type\nyou okay if you're familiar with type dict as well then it is also very\ndict as well then it is also very\ndict as well then it is also very familiar with you, familiar for you. If\nfamiliar with you, familiar for you. If\nfamiliar with you, familiar for you. If you are not familiar with anything,\nyou are not familiar with anything,\nyou are not familiar with anything, don't worry. I will just make you\ndon't worry. I will just make you\ndon't worry. I will just make you explain this.\nexplain this.\nexplain this. Make you explain what I'm saying, bro.\nMake you explain what I'm saying, bro.\nMake you explain what I'm saying, bro. Wait, it's really cold here. So, I don't\nWait, it's really cold here. So, I don't\nWait, it's really cold here. So, I don't know. So, let me just take you to my\nknow. So, let me just take you to my\nknow. So, let me just take you to my notepad. So, basically,\nnotepad. So, basically,\nnotepad. So, basically, let me just tell you\nlet me just tell you\nlet me just tell you about this thing because this is very\nabout this thing because this is very\nabout this thing because this is very important.\nSo now what is pyntic? What exactly is\nSo now what is pyntic? What exactly is pyic? Pyntic is nothing but a kind of\npyic? Pyntic is nothing but a kind of\npyic? Pyntic is nothing but a kind of data validation and and data parsing\ndata validation and and data parsing\ndata validation and and data parsing library. So let me bring Rahul. So Rahul\nlibrary. So let me bring Rahul. So Rahul\nlibrary. So let me bring Rahul. So Rahul let's say\nlet's say\nlet's say Rahul is building a let's say function a\nRahul is building a let's say function a\nRahul is building a let's say function a normal Python function right and that\nnormal Python function right and that\nnormal Python function right and that function okay that function should have\nfunction okay that function should have\nfunction okay that function should have some schema. Schema means key value\nsome schema. Schema means key value\nsome schema. Schema means key value pairs\npairs\npairs anything right let's say\nanything right let's say\nanything right let's say this Python function\nthis Python function\nthis Python function it can be like literally anything but\nit can be like literally anything but\nit can be like literally anything but let's say Python function so let's say\nlet's say Python function so let's say\nlet's say Python function so let's say he is writing this Python function and\nhe is writing this Python function and\nhe is writing this Python function and he expects the output\nhe expects the output\nhe expects the output specifically in the form of\nspecifically in the form of\nspecifically in the form of schema that he wants\nschema that he wants\nschema that he wants okay fixed schema\nfixed schema schema or I would say yeah\nfixed schema schema or I would say yeah fixed schema schema is a uh you can say\nfixed schema schema is a uh you can say\nfixed schema schema is a uh you can say right word so he wants fixed schema make\nright word so he wants fixed schema make\nright word so he wants fixed schema make sense okay now we know that whenever we\nsense okay now we know that whenever we\nsense okay now we know that whenever we create our own python functions okay we\ncreate our own python functions okay we\ncreate our own python functions okay we define return what we want to return but\ndefine return what we want to return but\ndefine return what we want to return but let's say you are using llm\nlet's say you are using llm\nlet's say you are using llm okay you are using llm to do that work\nokay you are using llm to do that work\nokay you are using llm to do that work now llm is what llm is llm lm can\nnow llm is what llm is llm lm can\nnow llm is what llm is llm lm can generate the output in that particular\ngenerate the output in that particular\ngenerate the output in that particular specific format and sometimes it will\nspecific format and sometimes it will\nspecific format and sometimes it will not sometimes it will not so in order to\nnot sometimes it will not so in order to\nnot sometimes it will not so in order to direct LLM hey LLM listen to me\ndirect LLM hey LLM listen to me\ndirect LLM hey LLM listen to me hey Siman listen you need to reply with\nhey Siman listen you need to reply with\nhey Siman listen you need to reply with this schema you need to say key this\nthis schema you need to say key this\nthis schema you need to say key this value this for example if I bring my\nvalue this for example if I bring my\nvalue this for example if I bring my anti-gravity\nanti-gravity\nanti-gravity here so if you observe what is that\nhere so if you observe what is that\nhere so if you observe what is that particular key it is using nothing\nparticular key it is using nothing\nparticular key it is using nothing literally nothing right so let's say I\nliterally nothing right so let's say I\nliterally nothing right so let's say I will say user message is this one write\nwill say user message is this one write\nwill say user message is this one write a fun fact about topic\na fun fact about topic\na fun fact about topic return the result\nreturn the result\nreturn the result with\nwith\nwith um in return the result in key value\num in return the result in key value\num in return the result in key value pair in a JSON or let's say in a key\npair in a JSON or let's say in a key\npair in a JSON or let's say in a key value pair\nvalue pair\nvalue pair with\nwith\nwith key equals to response with key as fact\nkey equals to response with key as fact\nkey equals to response with key as fact and value as fun fact. Perfect. Let's\nand value as fun fact. Perfect. Let's\nand value as fun fact. Perfect. Let's say I'm directing LLM. I'm providing\nsay I'm directing LLM. I'm providing\nsay I'm directing LLM. I'm providing some information to this LLM.\nsome information to this LLM.\nsome information to this LLM. Okay, let's try to run this. Let's see\nOkay, let's try to run this. Let's see\nOkay, let's try to run this. Let's see what happens. Enter topic. Let's say\nwhat happens. Enter topic. Let's say\nwhat happens. Enter topic. Let's say honey\nhoney\nhoney tone. Let's say funny.\ntone. Let's say funny.\ntone. Let's say funny. And let's try to run this.\nAnd let's try to run this.\nAnd let's try to run this. Perfect. Now, can you see the\nPerfect. Now, can you see the\nPerfect. Now, can you see the difference? Now I have got the response\ndifference? Now I have got the response\ndifference? Now I have got the response with fact as my key and value is the\nwith fact as my key and value is the\nwith fact as my key and value is the response. So what I did I directed LLM\nresponse. So what I did I directed LLM\nresponse. So what I did I directed LLM to generate the response with specific\nto generate the response with specific\nto generate the response with specific key and value pair. What is the benefit\nkey and value pair. What is the benefit\nkey and value pair. What is the benefit of it? Let's say my downstream. Now you\nof it? Let's say my downstream. Now you\nof it? Let's say my downstream. Now you say what is advantage of it? Let's say\nsay what is advantage of it? Let's say\nsay what is advantage of it? Let's say the downstream of this particular\nthe downstream of this particular\nthe downstream of this particular function is using that key. Make sense?\nfunction is using that key. Make sense?\nfunction is using that key. Make sense? Like maybe they are dependent on this\nLike maybe they are dependent on this\nLike maybe they are dependent on this thing.\nthing.\nthing. We have another Python functions.\nWe have another Python functions.\nWe have another Python functions. Okay. Dependent on this.\nSo we have dependency.\nSo we have dependency. Cool. So that means we need to make sure\nCool. So that means we need to make sure\nCool. So that means we need to make sure that this Python function basically\nthat this Python function basically\nthat this Python function basically actually LLM. Okay. Because we are using\nactually LLM. Okay. Because we are using\nactually LLM. Okay. Because we are using LLM. So this LLM will generate the fixed\nLLM. So this LLM will generate the fixed\nLLM. So this LLM will generate the fixed schema and it should not generate\nschema and it should not generate\nschema and it should not generate something on its own.\nsomething on its own.\nsomething on its own. Simple. This is our goal. This is our\nSimple. This is our goal. This is our\nSimple. This is our goal. This is our end goal. This is Rahul's end goal. Make\nend goal. This is Rahul's end goal. Make\nend goal. This is Rahul's end goal. Make sense? Fix schema dependent on\nsense? Fix schema dependent on\nsense? Fix schema dependent on downstream function. It can be anything.\ndownstream function. It can be anything.\ndownstream function. It can be anything. It can be anything. We do not know.\nIt can be anything. We do not know.\nIt can be anything. We do not know. Maybe data scientists are using it.\nMaybe data scientists are using it.\nMaybe data scientists are using it. Maybe data analysts are using it. Maybe\nMaybe data analysts are using it. Maybe\nMaybe data analysts are using it. Maybe someone else is using it. We need to\nsomeone else is using it. We need to\nsomeone else is using it. We need to make sure our schema is intact every\nmake sure our schema is intact every\nmake sure our schema is intact every time. So one way of doing that is you\ntime. So one way of doing that is you\ntime. So one way of doing that is you have already seen directing LLM in the\nhave already seen directing LLM in the\nhave already seen directing LLM in the prompt but obviously we cannot direct it\nprompt but obviously we cannot direct it\nprompt but obviously we cannot direct it in the prompt like we can as you have\nin the prompt like we can as you have\nin the prompt like we can as you have seen that it is it is successful here\nseen that it is it is successful here\nseen that it is it is successful here right let me just write it for you let\nright let me just write it for you let\nright let me just write it for you let me just remove it from here let's create\nme just remove it from here let's create\nme just remove it from here let's create a new notebook so that you will have a\na new notebook so that you will have a\na new notebook so that you will have a better understanding\nbetter understanding\nbetter understanding let's create\nlet's create\nlet's create three\nthree\nthree structured\nstructured\nstructured output\noutput\noutput do it by MB and this is my code or let's\ndo it by MB and this is my code or let's\ndo it by MB and this is my code or let's say let's create markdown. First of all\nsay let's create markdown. First of all\nsay let's create markdown. First of all we will look at um prompts basically\nwe will look at um prompts basically\nwe will look at um prompts basically guiding in prompts.\nGuiding in prompts\nGuiding in prompts make sense. This is like one way of\nmake sense. This is like one way of\nmake sense. This is like one way of doing it because we have so many ways.\ndoing it because we have so many ways.\ndoing it because we have so many ways. One is guiding in prompts and what is\nOne is guiding in prompts and what is\nOne is guiding in prompts and what is the way to do that? First of all, we\nthe way to do that? First of all, we\nthe way to do that? First of all, we need to load this thing\nneed to load this thing\nneed to load this thing here.\nhere.\nhere. Okay.\nOkay.\nOkay. Attach this environment variable. Then\nAttach this environment variable. Then\nAttach this environment variable. Then we can load this entire thing, right?\nwe can load this entire thing, right?\nwe can load this entire thing, right? Perfect. And I can attach it to this\nPerfect. And I can attach it to this\nPerfect. And I can attach it to this one.\none.\none. Okay, makes sense. And I can remove this\nOkay, makes sense. And I can remove this\nOkay, makes sense. And I can remove this thing because I don't want to apply this\nthing because I don't want to apply this\nthing because I don't want to apply this thing. And I will simply say topic\nthing. And I will simply say topic\nthing. And I will simply say topic simple\nsimple\nsimple about let's say honey\nabout let's say honey\nabout let's say honey make sense okay or I can even remove\nmake sense okay or I can even remove\nmake sense okay or I can even remove this thing because now we are focusing\nthis thing because now we are focusing\nthis thing because now we are focusing on llm so I do not want you to be\non llm so I do not want you to be\non llm so I do not want you to be confused on prompts and all okay so I'll\nconfused on prompts and all okay so I'll\nconfused on prompts and all okay so I'll simply say llm openai\nsimply say llm openai\nsimply say llm openai dot invoke\ntell me\ntell me a joke let's say tell me a book.\na joke let's say tell me a book.\na joke let's say tell me a book. Perfect.\nPerfect.\nPerfect. So, we have this. Let me just try to run\nSo, we have this. Let me just try to run\nSo, we have this. Let me just try to run this uh lang core. Okay.\nthis uh lang core. Okay.\nthis uh lang core. Okay. LLM open is not defined. Okay. We didn't\nLLM open is not defined. Okay. We didn't\nLLM open is not defined. Okay. We didn't define the model. Really?\ndefine the model. Really?\ndefine the model. Really? Okay. We can define it. Not a big deal.\nOkay. We can define it. Not a big deal.\nOkay. We can define it. Not a big deal. This is the model.\nOkay. Tell me a joke.\nOkay. Tell me a joke. Now I will guide this LLM model, okay,\nNow I will guide this LLM model, okay,\nNow I will guide this LLM model, okay, to generate the output in a specific,\nto generate the output in a specific,\nto generate the output in a specific, you can say schema. Make sense? I will\nyou can say schema. Make sense? I will\nyou can say schema. Make sense? I will say tell me a joke.\nsay tell me a joke.\nsay tell me a joke. um generate the output\num generate the output\num generate the output in key value pair format\nin key value pair format\nin key value pair format with\nwith\nwith the following keys\nthe following keys\nthe following keys setup and punch line. Make sense? So it\nsetup and punch line. Make sense? So it\nsetup and punch line. Make sense? So it will it will be having two keys setup\nwill it will be having two keys setup\nwill it will be having two keys setup and punch line. Okay, let's try to see\nand punch line. Okay, let's try to see\nand punch line. Okay, let's try to see the answer.\nthe answer.\nthe answer. It should return a dictionary\nIt should return a dictionary\nIt should return a dictionary and let me just say result\nand let me just say result\nand let me just say result and then I can say result dot content.\nLet me just run it.\nLet me just run it. Okay. So I have setup and I have punch\nOkay. So I have setup and I have punch\nOkay. So I have setup and I have punch line. See\nline. See\nline. See so it has actually generated the output\nso it has actually generated the output\nso it has actually generated the output in the key value pairs. That is fine and\nin the key value pairs. That is fine and\nin the key value pairs. That is fine and I loved it. Setup. Why did Scar\nI loved it. Setup. Why did Scar\nI loved it. Setup. Why did Scar Scarecrow win an award? Because he was\nScarecrow win an award? Because he was\nScarecrow win an award? Because he was outstanding in his field. Okay. Wow.\noutstanding in his field. Okay. Wow.\noutstanding in his field. Okay. Wow. Nice job. Very nice job. 10 out of 10.\nNice job. Very nice job. 10 out of 10.\nNice job. Very nice job. 10 out of 10. 10. Okay. Now, one thing is clear that\n10. Okay. Now, one thing is clear that\n10. Okay. Now, one thing is clear that we can guide LLM in the prompt. But it\nwe can guide LLM in the prompt. But it\nwe can guide LLM in the prompt. But it is very simple when we have like a very\nis very simple when we have like a very\nis very simple when we have like a very simple prompt. Tell me a joke, tell me\nsimple prompt. Tell me a joke, tell me\nsimple prompt. Tell me a joke, tell me this thing, tell me that. But when we\nthis thing, tell me that. But when we\nthis thing, tell me that. But when we have let's say a very detailed prompt\nhave let's say a very detailed prompt\nhave let's say a very detailed prompt very big prompt dynamic prompts when we\nvery big prompt dynamic prompts when we\nvery big prompt dynamic prompts when we are just running complex workflows we\nare just running complex workflows we\nare just running complex workflows we cannot guide the LLM to generate the\ncannot guide the LLM to generate the\ncannot guide the LLM to generate the output in a specific format right then\noutput in a specific format right then\noutput in a specific format right then we have to follow a structured approach\nwe have to follow a structured approach\nwe have to follow a structured approach that structured approach is called\nthat structured approach is called\nthat structured approach is called pyntic let's try to discuss that and you\npyntic let's try to discuss that and you\npyntic let's try to discuss that and you will understand pyic don't worry even if\nwill understand pyic don't worry even if\nwill understand pyic don't worry even if you do not have any knowledge with that\nyou do not have any knowledge with that\nyou do not have any knowledge with that I will just make you understand it\nI will just make you understand it\nI will just make you understand it structured I put using let's say pyic\nstructured I put using let's say pyic\nstructured I put using let's say pyic using pyentic models.\nusing pyentic models.\nusing pyentic models. Now what is pyic model okay and you will\nNow what is pyic model okay and you will\nNow what is pyic model okay and you will also see the difference between pyantic\nalso see the difference between pyantic\nalso see the difference between pyantic and typic just stay with me now using\nand typic just stay with me now using\nand typic just stay with me now using pyntic model. So now we need to make\npyntic model. So now we need to make\npyntic model. So now we need to make sure that we are guiding LLM in a more\nsure that we are guiding LLM in a more\nsure that we are guiding LLM in a more formal manner to generate the output in\nformal manner to generate the output in\nformal manner to generate the output in a specific structure. Right? So we use a\na specific structure. Right? So we use a\na specific structure. Right? So we use a library called pyic\nlibrary called pyic\nlibrary called pyic import base model. Now what is this base\nimport base model. Now what is this base\nimport base model. Now what is this base model? Base model is a class that we use\nmodel? Base model is a class that we use\nmodel? Base model is a class that we use to create our schema. Right? And how do\nto create our schema. Right? And how do\nto create our schema. Right? And how do we create our schema? We simply create a\nwe create our schema? We simply create a\nwe create our schema? We simply create a class. And I can say let's say llm\nclass. And I can say let's say llm\nclass. And I can say let's say llm schema simple and I will inherit this\nschema simple and I will inherit this\nschema simple and I will inherit this base model in my normal class simple now\nbase model in my normal class simple now\nbase model in my normal class simple now within that I need to define the key\nwithin that I need to define the key\nwithin that I need to define the key value pairs not key value basically the\nvalue pairs not key value basically the\nvalue pairs not key value basically the schema like all the columns that I need\nschema like all the columns that I need\nschema like all the columns that I need all the keys that I need everything I\nall the keys that I need everything I\nall the keys that I need everything I need just two things one is let's say um\nneed just two things one is let's say um\nneed just two things one is let's say um what was the schema here let's use the\nwhat was the schema here let's use the\nwhat was the schema here let's use the same one setup so that you can relate\nsame one setup so that you can relate\nsame one setup so that you can relate setup right and what is the data type I\nsetup right and what is the data type I\nsetup right and what is the data type I want? Oh, that means we can even control\nwant? Oh, that means we can even control\nwant? Oh, that means we can even control the data type of this thing as well.\nthe data type of this thing as well.\nthe data type of this thing as well. Exactly. I will say string I want my\nExactly. I will say string I want my\nExactly. I will say string I want my setup key is of string type. Then I will\nsetup key is of string type. Then I will\nsetup key is of string type. Then I will define punch line\ndefine punch line\ndefine punch line and this will also be of string type.\nand this will also be of string type.\nand this will also be of string type. Make sense? Very good. Now this is my\nMake sense? Very good. Now this is my\nMake sense? Very good. Now this is my class. Let me just run this model.\nclass. Let me just run this model.\nclass. Let me just run this model. That's it. So this class is created. Do\nThat's it. So this class is created. Do\nThat's it. So this class is created. Do you know what? Now if let's say I want\nyou know what? Now if let's say I want\nyou know what? Now if let's say I want to use this particular LLM basically use\nto use this particular LLM basically use\nto use this particular LLM basically use this class use this schema I cannot I\nthis class use this schema I cannot I\nthis class use this schema I cannot I cannot use any other thing. What do I\ncannot use any other thing. What do I\ncannot use any other thing. What do I mean? Let's say I want to create a\nmean? Let's say I want to create a\nmean? Let's say I want to create a class. So I will simply say object\nclass. So I will simply say object\nclass. So I will simply say object equals llm schema right and I want to\nequals llm schema right and I want to\nequals llm schema right and I want to just create an object of this pyente. So\njust create an object of this pyente. So\njust create an object of this pyente. So I'll simply pass a dictionary. I simply\nI'll simply pass a dictionary. I simply\nI'll simply pass a dictionary. I simply say let's say setup\nsay let's say setup\nsay let's say setup right setup will be setup and punch line\nright setup will be setup and punch line\nright setup will be setup and punch line will be let's say punch line\nwill be let's say punch line\nwill be let's say punch line punch line will be let's say punch line\npunch line will be let's say punch line\npunch line will be let's say punch line or let's say some setup right and punch\nor let's say some setup right and punch\nor let's say some setup right and punch line will be let's say some punch line\nline will be let's say some punch line\nline will be let's say some punch line okay perfect and now let's unpack this\nokay perfect and now let's unpack this\nokay perfect and now let's unpack this list like this dictionary and let me\nlist like this dictionary and let me\nlist like this dictionary and let me just try to show you the output obj\njust try to show you the output obj\njust try to show you the output obj so what did I get I got this pentic\nso what did I get I got this pentic\nso what did I get I got this pentic object object if I want to now use this\nobject object if I want to now use this\nobject object if I want to now use this object I will be using like this obj dot\nobject I will be using like this obj dot\nobject I will be using like this obj dot let's say setup I want to use a setup\nlet's say setup I want to use a setup\nlet's say setup I want to use a setup key within this I will write like this I\nkey within this I will write like this I\nkey within this I will write like this I will get this some setup got it what is\nwill get this some setup got it what is\nwill get this some setup got it what is the advantage of it I told you that this\nthe advantage of it I told you that this\nthe advantage of it I told you that this is a kind of data validation library\nis a kind of data validation library\nis a kind of data validation library if I pass some wrong thing let's say I\nif I pass some wrong thing let's say I\nif I pass some wrong thing let's say I do not pass setup I pass let's say\ndo not pass setup I pass let's say\ndo not pass setup I pass let's say ketchup wow what playing anala no\nketchup wow what playing anala no\nketchup wow what playing anala no nothing is so let's say catch up some\nnothing is so let's say catch up some\nnothing is so let's say catch up some setup now if I try to create this object\nsetup now if I try to create this object\nsetup now if I try to create this object it will throw the error why validation\nit will throw the error why validation\nit will throw the error why validation error it will say bro you cannot do this\nerror it will say bro you cannot do this\nerror it will say bro you cannot do this you cannot do this because I can only\nyou cannot do this because I can only\nyou cannot do this because I can only accept either not either basically both\naccept either not either basically both\naccept either not either basically both I can accept setup and punch line if you\nI can accept setup and punch line if you\nI can accept setup and punch line if you will not provide me this thing I will\nwill not provide me this thing I will\nwill not provide me this thing I will not allow you to create an object wow\nThat means if LLM generates an output\nThat means if LLM generates an output and LLM does not follow our schema, it\nand LLM does not follow our schema, it\nand LLM does not follow our schema, it will not allow LLM to generate the\nwill not allow LLM to generate the\nwill not allow LLM to generate the output and it will protect us to just\noutput and it will protect us to just\noutput and it will protect us to just deliver that pseudo schema or let's say\ndeliver that pseudo schema or let's say\ndeliver that pseudo schema or let's say broken schema to our downstream\nbroken schema to our downstream\nbroken schema to our downstream workflows. This is an amazing thing.\nworkflows. This is an amazing thing.\nworkflows. This is an amazing thing. This is a kind of data validation check.\nThis is a kind of data validation check.\nThis is a kind of data validation check. Very good. An lamba does it only check\nVery good. An lamba does it only check\nVery good. An lamba does it only check on the keys or it does something else as\non the keys or it does something else as\non the keys or it does something else as well. It does this data type check as\nwell. It does this data type check as\nwell. It does this data type check as well. For example, let's say I pass some\nwell. For example, let's say I pass some\nwell. For example, let's say I pass some integer value here by mistake 1 2 3 then\ninteger value here by mistake 1 2 3 then\ninteger value here by mistake 1 2 3 then also it will fail. See validation error\nalso it will fail. See validation error\nalso it will fail. See validation error again because I am expecting of string\nagain because I am expecting of string\nagain because I am expecting of string type. So this is called validation.\ntype. So this is called validation.\ntype. So this is called validation. Okay. So data validation is done. Now\nOkay. So data validation is done. Now\nOkay. So data validation is done. Now what about data parsing? So this step is\nwhat about data parsing? So this step is\nwhat about data parsing? So this step is called data parsing. when you are\ncalled data parsing. when you are\ncalled data parsing. when you are parsing the values and it is creating\nparsing the values and it is creating\nparsing the values and it is creating this pyic object. Let me just correct\nthis pyic object. Let me just correct\nthis pyic object. Let me just correct it. If I say sum setup.\nit. If I say sum setup.\nit. If I say sum setup. So what I did I simply passed this\nSo what I did I simply passed this\nSo what I did I simply passed this particular class and I passed my\nparticular class and I passed my\nparticular class and I passed my arguments setup some setup punch line\narguments setup some setup punch line\narguments setup some setup punch line means some punch line. Make sense? And\nmeans some punch line. Make sense? And\nmeans some punch line. Make sense? And then only I got my object. Wow.\nthen only I got my object. Wow.\nthen only I got my object. Wow. This is literally wow. And do you know\nThis is literally wow. And do you know\nThis is literally wow. And do you know what? Now let me just show you with LLM.\nwhat? Now let me just show you with LLM.\nwhat? Now let me just show you with LLM. So I will simply say LLM structured\nSo I will simply say LLM structured\nSo I will simply say LLM structured output because I'm creating a new LLM\noutput because I'm creating a new LLM\noutput because I'm creating a new LLM object, right? I will simply say my\nobject, right? I will simply say my\nobject, right? I will simply say my normal LLM which was OpenAI and then I\nnormal LLM which was OpenAI and then I\nnormal LLM which was OpenAI and then I will say with structured output and now\nwill say with structured output and now\nwill say with structured output and now I need to pass the schema. And what is\nI need to pass the schema. And what is\nI need to pass the schema. And what is the schema? This is the schema. LLM\nthe schema? This is the schema. LLM\nthe schema? This is the schema. LLM schema, right? Pass this LLM schema.\nschema, right? Pass this LLM schema.\nschema, right? Pass this LLM schema. Perfect. Now if you use this model,\nPerfect. Now if you use this model,\nPerfect. Now if you use this model, Now if you\nuse this model, this one,\nuse this model, this one, this one, this will generate every time\nthis one, this will generate every time\nthis one, this will generate every time the output in this particular structure\nthe output in this particular structure\nthe output in this particular structure and I will not even tell LLM that\nand I will not even tell LLM that\nand I will not even tell LLM that generate uh the final output in the\ngenerate uh the final output in the\ngenerate uh the final output in the structure. This is a power of it. What?\nstructure. This is a power of it. What?\nstructure. This is a power of it. What? One pro tip that whenever whenever you\nOne pro tip that whenever whenever you\nOne pro tip that whenever whenever you are defining your class, you should\nare defining your class, you should\nare defining your class, you should always define a kind of description.\nalways define a kind of description.\nalways define a kind of description. Why? Because LLM is not magic, LLM will\nWhy? Because LLM is not magic, LLM will\nWhy? Because LLM is not magic, LLM will use your information as context.\nuse your information as context.\nuse your information as context. Now just tell me one thing. If you are a\nNow just tell me one thing. If you are a\nNow just tell me one thing. If you are a human, if I will say you need to um\nhuman, if I will say you need to um\nhuman, if I will say you need to um create a joke, okay? And I know you are\ncreate a joke, okay? And I know you are\ncreate a joke, okay? And I know you are a great comedian. You will just create\na great comedian. You will just create\na great comedian. You will just create it like this. So let's say you create\nit like this. So let's say you create\nit like this. So let's say you create the joke and I say you need to generate\nthe joke and I say you need to generate\nthe joke and I say you need to generate the output in these two key value pairs.\nthe output in these two key value pairs.\nthe output in these two key value pairs. Do you know like what should be the\nDo you know like what should be the\nDo you know like what should be the setup and what should go in the punch\nsetup and what should go in the punch\nsetup and what should go in the punch line? Obviously no, right? So I will\nline? Obviously no, right? So I will\nline? Obviously no, right? So I will tell that what should be go in the\ntell that what should be go in the\ntell that what should be go in the setup. I will simply say field that I\nsetup. I will simply say field that I\nsetup. I will simply say field that I need to first of all import.\nneed to first of all import.\nneed to first of all import. Perfect. And then I will write field\nPerfect. And then I will write field\nPerfect. And then I will write field description setup for the joke and joke.\ndescription setup for the joke and joke.\ndescription setup for the joke and joke. setup for the joke and then punch line\nsetup for the joke and then punch line\nsetup for the joke and then punch line for the joke.\nfor the joke.\nfor the joke. Make sense? So these are description.\nMake sense? So these are description.\nMake sense? So these are description. These are very important because LLM are\nThese are very important because LLM are\nThese are very important because LLM are LLMs are all about context. Simple. More\nLLMs are all about context. Simple. More\nLLMs are all about context. Simple. More and precise context you will provide\nand precise context you will provide\nand precise context you will provide better applications you will be able to\nbetter applications you will be able to\nbetter applications you will be able to build. Simple formula. So our this thing\nbuild. Simple formula. So our this thing\nbuild. Simple formula. So our this thing is done and let's try to test it. If it\nis done and let's try to test it. If it\nis done and let's try to test it. If it is working fine, I will simply say\nis working fine, I will simply say\nis working fine, I will simply say tell me a joke. That's it. I will not\ntell me a joke. That's it. I will not\ntell me a joke. That's it. I will not tell LLM to generate the output in any\ntell LLM to generate the output in any\ntell LLM to generate the output in any structure. I will simply say generate a\nstructure. I will simply say generate a\nstructure. I will simply say generate a joke. Let's see what it generates. I\njoke. Let's see what it generates. I\njoke. Let's see what it generates. I will simply say dot content.\nwill simply say dot content.\nwill simply say dot content. Let's see.\nOkay. LM schema object has no attribute\nOkay. LM schema object has no attribute content. Makes sense. So I can remove\ncontent. Makes sense. So I can remove\ncontent. Makes sense. So I can remove this content.\nthis content.\nthis content. Makes sense because that is a pyantic\nMakes sense because that is a pyantic\nMakes sense because that is a pyantic object. That is a silly mistake.\nobject. That is a silly mistake.\nobject. That is a silly mistake. Perfect. Now what I got? I got the\nPerfect. Now what I got? I got the\nPerfect. Now what I got? I got the response in the form of LLM schema which\nresponse in the form of LLM schema which\nresponse in the form of LLM schema which is this class. Then setup\nis this class. Then setup\nis this class. Then setup one key its value one key its value.\none key its value one key its value.\none key its value one key its value. Perfect. This is not just a generic\nPerfect. This is not just a generic\nPerfect. This is not just a generic string. This is not just a generic\nstring. This is not just a generic\nstring. This is not just a generic string. This is a pidantic class object.\nstring. This is a pidantic class object.\nstring. This is a pidantic class object. I can even show you if I say result\nI can even show you if I say result\nI can even show you if I say result and if I say type of result.\nand if I say type of result.\nand if I say type of result. Let's see what we get.\nLet's see what we get.\nLet's see what we get. See this is an LLM schema object. If I\nSee this is an LLM schema object. If I\nSee this is an LLM schema object. If I want to use any key value pair now I can\nwant to use any key value pair now I can\nwant to use any key value pair now I can use it easily because now I'm sure that\nuse it easily because now I'm sure that\nuse it easily because now I'm sure that LLM will generate the output with those\nLLM will generate the output with those\nLLM will generate the output with those keys. I will simply say result dot let's\nkeys. I will simply say result dot let's\nkeys. I will simply say result dot let's say punch line.\nsay punch line.\nsay punch line. Perfect. Let's see.\nPerfect. That's how you guide LLM to\nPerfect. That's how you guide LLM to make a better AI agents because remember\nmake a better AI agents because remember\nmake a better AI agents because remember remember our discussion\nremember our discussion\nremember our discussion our main agenda is to build this kind of\nour main agenda is to build this kind of\nour main agenda is to build this kind of workflow. So let's say this output\nworkflow. So let's say this output\nworkflow. So let's say this output output of this node\noutput of this node\noutput of this node output of this node is actually like\noutput of this node is actually like\noutput of this node is actually like this this node is dependent on this one\nthis this node is dependent on this one\nthis this node is dependent on this one right this is the upstream for it. So if\nright this is the upstream for it. So if\nright this is the upstream for it. So if it generates something else this whole\nit generates something else this whole\nit generates something else this whole workflow will be broken. So we need to\nworkflow will be broken. So we need to\nworkflow will be broken. So we need to make sure that we pass the exact schema\nmake sure that we pass the exact schema\nmake sure that we pass the exact schema that we should pass. We should not tell\nthat we should pass. We should not tell\nthat we should pass. We should not tell everything LLM please generate this. LLM\neverything LLM please generate this. LLM\neverything LLM please generate this. LLM please generate that. Build LLM object\nplease generate that. Build LLM object\nplease generate that. Build LLM object smartly. This is the concept of Pentic.\nsmartly. This is the concept of Pentic.\nsmartly. This is the concept of Pentic. If you are learning Pyentric for the\nIf you are learning Pyentric for the\nIf you are learning Pyentric for the first time, take some time to digest\nfirst time, take some time to digest\nfirst time, take some time to digest this information because this is new. If\nthis information because this is new. If\nthis information because this is new. If you already know Pentic, then it will be\nyou already know Pentic, then it will be\nyou already know Pentic, then it will be like piece of cake for you. You are\nlike piece of cake for you. You are\nlike piece of cake for you. You are simply implementing its you can say\nsimply implementing its you can say\nsimply implementing its you can say feature. That's it. Make sense? I hope\nfeature. That's it. Make sense? I hope\nfeature. That's it. Make sense? I hope like it was easy. Now let's talk about\nlike it was easy. Now let's talk about\nlike it was easy. Now let's talk about the third type which is not very\nthe third type which is not very\nthe third type which is not very recommended but people still use it. Let\nrecommended but people still use it. Let\nrecommended but people still use it. Let me just tell you what's that.\nme just tell you what's that.\nme just tell you what's that. It is called as type.\nIt is called as type.\nIt is called as type. Okay.\nUsing\nUsing type dict.\nPerfect. Now what is typic? type dict is\nPerfect. Now what is typic? type dict is exactly same as pyic. Okay, exactly\nexactly same as pyic. Okay, exactly\nexactly same as pyic. Okay, exactly same. Then why are we using two things?\nsame. Then why are we using two things?\nsame. Then why are we using two things? Okay. Okay, let me tell you. So this is\nOkay. Okay, let me tell you. So this is\nOkay. Okay, let me tell you. So this is exactly same as pyantic. But piantic is\nexactly same as pyantic. But piantic is\nexactly same as pyantic. But piantic is like the principle of principal uh you\nlike the principle of principal uh you\nlike the principle of principal uh you can say principal of the school. Okay.\ncan say principal of the school. Okay.\ncan say principal of the school. Okay. And typic is like the class teacher.\nAnd typic is like the class teacher.\nAnd typic is like the class teacher. Wow. Oamba just solve this puzzle that\nWow. Oamba just solve this puzzle that\nWow. Oamba just solve this puzzle that you have just created. Okay.\nyou have just created. Okay.\nyou have just created. Okay. Principles are like very very strict\nPrinciples are like very very strict\nPrinciples are like very very strict right they will not listen to you they\nright they will not listen to you they\nright they will not listen to you they will simply do this like in my school\nwill simply do this like in my school\nwill simply do this like in my school like I got this so they will not listen\nlike I got this so they will not listen\nlike I got this so they will not listen to you they will not correct like they\nto you they will not correct like they\nto you they will not correct like they will not listen to you that why did you\nwill not listen to you that why did you\nwill not listen to you that why did you make any mistake if you made the mistake\nmake any mistake if you made the mistake\nmake any mistake if you made the mistake t\nt\nt so they are like this class teachers on\nso they are like this class teachers on\nso they are like this class teachers on the other hand if they are good they\nthe other hand if they are good they\nthe other hand if they are good they will listen to your mistake even if you\nwill listen to your mistake even if you\nwill listen to your mistake even if you make a mistake they will simply say okay\nmake a mistake they will simply say okay\nmake a mistake they will simply say okay do not do it next time exactly same\ndo not do it next time exactly same\ndo not do it next time exactly same thing happens here in pyic models and\nthing happens here in pyic models and\nthing happens here in pyic models and typic. So you define the schema for\ntypic. So you define the schema for\ntypic. So you define the schema for sure. Let me assure you. So I will\nsure. Let me assure you. So I will\nsure. Let me assure you. So I will import the typect from typing\nimport the typect from typing\nimport the typect from typing from typing import\ntype. Okay. And here I will create the\ntype. Okay. And here I will create the class same way I have created there\nclass same way I have created there\nclass same way I have created there class. Let's say llm\nclass. Let's say llm\nclass. Let's say llm schema\nschema\nschema TD that means type dict and I will add\nTD that means type dict and I will add\nTD that means type dict and I will add type dict class same way everything is\ntype dict class same way everything is\ntype dict class same way everything is same then again same things name age or\nsame then again same things name age or\nsame then again same things name age or no not name age let's say setup string\nno not name age let's say setup string\nno not name age let's say setup string and punch line string same right see\nand punch line string same right see\nand punch line string same right see exactly same now if I just show you\nexactly same now if I just show you\nexactly same now if I just show you let's create this object okay now what\nlet's create this object okay now what\nlet's create this object okay now what will happen if I just create an object\nwill happen if I just create an object\nwill happen if I just create an object of this object equals llm schema TD and\nllm\nllm schema\nschema\nschema TD\nTD\nTD and if I pass a dictionary with same\nand if I pass a dictionary with same\nand if I pass a dictionary with same thing\nthing\nthing setup let's say some setup\nsetup let's say some setup\nsetup let's say some setup why it is by again again writing tier\nwhy it is by again again writing tier\nwhy it is by again again writing tier down tier down let's say punch line and\ndown tier down let's say punch line and\ndown tier down let's say punch line and let's say\nlet's say\nlet's say some punch Right.\nsome punch Right.\nsome punch Right. Perfect. Let's just sum it up. If I do\nPerfect. Let's just sum it up. If I do\nPerfect. Let's just sum it up. If I do this, what I will get? I will get\nthis, what I will get? I will get\nthis, what I will get? I will get dictionary, not an object. Dictionary.\ndictionary, not an object. Dictionary.\ndictionary, not an object. Dictionary. Simple dictionary.\nSimple dictionary.\nSimple dictionary. Okay. If I want to use any object, I can\nOkay. If I want to use any object, I can\nOkay. If I want to use any object, I can use it like this\nuse it like this\nuse it like this setup or I can use dot get option as\nsetup or I can use dot get option as\nsetup or I can use dot get option as well. Simple dictionary. This is not an\nwell. Simple dictionary. This is not an\nwell. Simple dictionary. This is not an object. Simple dictionary.\nobject. Simple dictionary.\nobject. Simple dictionary. Okay. And do you know what is the\nOkay. And do you know what is the\nOkay. And do you know what is the advantage? Why I just said like it\nadvantage? Why I just said like it\nadvantage? Why I just said like it corrects your mistake.\ncorrects your mistake.\ncorrects your mistake. Let's say you wrote it here catchup.\nLet's say you wrote it here catchup.\nLet's say you wrote it here catchup. What happened with pyic? It failed,\nWhat happened with pyic? It failed,\nWhat happened with pyic? It failed, right? Let me show you. It won't fail.\nright? Let me show you. It won't fail.\nright? Let me show you. It won't fail. It will simply say catchup.\nIt will simply say catchup.\nIt will simply say catchup. What? What?\nWhat? What?\nWhat? What? That means you made a mistake. You wrote\nThat means you made a mistake. You wrote\nThat means you made a mistake. You wrote catchup. But still it will not fail.\ncatchup. But still it will not fail.\ncatchup. But still it will not fail. Why? It will create the output using\nWhy? It will create the output using\nWhy? It will create the output using your mistake and it will simply give you\nyour mistake and it will simply give you\nyour mistake and it will simply give you the error on the typing part. Basically\nthe error on the typing part. Basically\nthe error on the typing part. Basically these are for type hinting. Type\nthese are for type hinting. Type\nthese are for type hinting. Type hinting. So when you're let's say typing\nhinting. So when you're let's say typing\nhinting. So when you're let's say typing something wrong, you will get the error\nsomething wrong, you will get the error\nsomething wrong, you will get the error while typing it. It is not runtime\nwhile typing it. It is not runtime\nwhile typing it. It is not runtime error. It will not fail your code.\nerror. It will not fail your code.\nerror. It will not fail your code. Okay. So this is like a class teacher.\nOkay. So this is like a class teacher.\nOkay. So this is like a class teacher. She will say okay do not make a mistake\nShe will say okay do not make a mistake\nShe will say okay do not make a mistake next time. But if you made a mistake in\nnext time. But if you made a mistake in\nnext time. But if you made a mistake in front of your principal, he will say\nfront of your principal, he will say\nfront of your principal, he will say come here bro. Come here. Come to my\ncome here bro. Come here. Come to my\ncome here bro. Come here. Come to my office. And I used to just get those\noffice. And I used to just get those\noffice. And I used to just get those announcements like I was in I think\nannouncements like I was in I think\nannouncements like I was in I think class 8th or 9th or 10th and we we had a\nclass 8th or 9th or 10th and we we had a\nclass 8th or 9th or 10th and we we had a mic we had like uh cameras in all our\nmic we had like uh cameras in all our\nmic we had like uh cameras in all our classes. Okay. And if someone is like\nclasses. Okay. And if someone is like\nclasses. Okay. And if someone is like doing something crazy in the classroom a\ndoing something crazy in the classroom a\ndoing something crazy in the classroom a principal used to watch everything in\nprincipal used to watch everything in\nprincipal used to watch everything in the cameras and if you made a mistake\nthe cameras and if you made a mistake\nthe cameras and if you made a mistake bro you're gone. So on I would say every\nbro you're gone. So on I would say every\nbro you're gone. So on I would say every week there was an announcement used to\nweek there was an announcement used to\nweek there was an announcement used to happen in our whole auditorium whole\nhappen in our whole auditorium whole\nhappen in our whole auditorium whole floor I would say an Lamba come to the\nfloor I would say an Lamba come to the\nfloor I would say an Lamba come to the principal office. I was like oh bro.\nprincipal office. I was like oh bro.\nprincipal office. I was like oh bro. So yeah crazy stuff but I I like I used\nSo yeah crazy stuff but I I like I used\nSo yeah crazy stuff but I I like I used to go to the principal office with all\nto go to the principal office with all\nto go to the principal office with all of my friends. I never went to the\nof my friends. I never went to the\nof my friends. I never went to the principal office alone.\nprincipal office alone.\nprincipal office alone. Alone because whenever I go there like\nAlone because whenever I go there like\nAlone because whenever I go there like whenever I used to go there I simply\nwhenever I used to go there I simply\nwhenever I used to go there I simply take names. I used to take names. See I\ntake names. I used to take names. See I\ntake names. I used to take names. See I was not alone. I was with this this this\nwas not alone. I was with this this this\nwas not alone. I was with this this this this this this and then all of those\nthis this this and then all of those\nthis this this and then all of those people would also come down to the\npeople would also come down to the\npeople would also come down to the principal office and let then let's have\nprincipal office and let then let's have\nprincipal office and let then let's have a group discussion together.\na group discussion together.\na group discussion together. Okay. Okay. Crazy stuff. So now I was\nOkay. Okay. Crazy stuff. So now I was\nOkay. Okay. Crazy stuff. So now I was saying that we can create this object\nsaying that we can create this object\nsaying that we can create this object like this. Simple. Now you will say\nlike this. Simple. Now you will say\nlike this. Simple. Now you will say Anlama then what is the benefit of it?\nAnlama then what is the benefit of it?\nAnlama then what is the benefit of it? There's a benefit. We can still use this\nThere's a benefit. We can still use this\nThere's a benefit. We can still use this particular class to provide the context\nparticular class to provide the context\nparticular class to provide the context because we simply need to provide a\nbecause we simply need to provide a\nbecause we simply need to provide a context to LLM, right? We simply need to\ncontext to LLM, right? We simply need to\ncontext to LLM, right? We simply need to provide a context. I can simply say llm\nprovide a context. I can simply say llm\nprovide a context. I can simply say llm structured\nstructured\nstructured or let's say type dict\nor let's say type dict\nor let's say type dict right and I will simply say llm open aai\nright and I will simply say llm open aai\nright and I will simply say llm open aai dot\ndot\ndot bit with structure output\nbit with structure output\nbit with structure output perfect and this is my llm schema TD now\nperfect and this is my llm schema TD now\nperfect and this is my llm schema TD now if I ask the same question here\nif I ask the same question here\nif I ask the same question here this thing\nI will get it I will simply Please say\nI will get it I will simply Please say result.\nresult.\nresult. Perfect. Llm OP. What is LLM OP?\nPerfect. Llm OP. What is LLM OP?\nPerfect. Llm OP. What is LLM OP? Where is L? Oh,\nWhere is L? Oh,\nWhere is L? Oh, open AI.\nUh, I think we just need to use type\nUh, I think we just need to use type deck. Perfect.\ndeck. Perfect.\ndeck. Perfect. So here you will see that you will still\nSo here you will see that you will still\nSo here you will see that you will still get the answer in the form of dictionary\nget the answer in the form of dictionary\nget the answer in the form of dictionary but not in the form of pentic object.\nbut not in the form of pentic object.\nbut not in the form of pentic object. not in the form of pyic object. Wow.\nnot in the form of pyic object. Wow.\nnot in the form of pyic object. Wow. Very good.\nVery good.\nVery good. Make sense? Make sense? Very very very\nMake sense? Make sense? Very very very\nMake sense? Make sense? Very very very good. But what is the advantage? If\ngood. But what is the advantage? If\ngood. But what is the advantage? If let's say your workflows are not very\nlet's say your workflows are not very\nlet's say your workflows are not very very errorprone and you are not very\nvery errorprone and you are not very\nvery errorprone and you are not very much dependent on the same keys, then\nmuch dependent on the same keys, then\nmuch dependent on the same keys, then you can simply use typic for just as a\nyou can simply use typic for just as a\nyou can simply use typic for just as a context. But when your workflows are\ncontext. But when your workflows are\ncontext. But when your workflows are very very strict and you do not want\nvery very strict and you do not want\nvery very strict and you do not want them to create any random key on its\nthem to create any random key on its\nthem to create any random key on its own, you can use pyantic. So it's\nown, you can use pyantic. So it's\nown, you can use pyantic. So it's totally depends upon the pentic and do\ntotally depends upon the pentic and do\ntotally depends upon the pentic and do you know what's my philosophy if you're\nyou know what's my philosophy if you're\nyou know what's my philosophy if you're using a specific schema right and if you\nusing a specific schema right and if you\nusing a specific schema right and if you want to just use a specific you can say\nwant to just use a specific you can say\nwant to just use a specific you can say guide to your LLM just go with pyentic\nguide to your LLM just go with pyentic\nguide to your LLM just go with pyentic go with pyic for the final output and if\ngo with pyic for the final output and if\ngo with pyic for the final output and if you if you just want to provide some\nyou if you just want to provide some\nyou if you just want to provide some context to your LLM then simply go with\ncontext to your LLM then simply go with\ncontext to your LLM then simply go with typic simple\ntypic simple\ntypic simple I hope it makes sense like when should\nI hope it makes sense like when should\nI hope it makes sense like when should we use pide and when should we use um\nwe use pide and when should we use um\nwe use pide and when should we use um your typed it make sense okay\nso now let's try to cover this important\nso now let's try to cover this important topic and I would say this is one of the\ntopic and I would say this is one of the\ntopic and I would say this is one of the most important topics what is this topic\nmost important topics what is this topic\nmost important topics what is this topic chains and obviously we are studying\nchains and obviously we are studying\nchains and obviously we are studying learning lang chain so as you would have\nlearning lang chain so as you would have\nlearning lang chain so as you would have already guessed that this is really\nalready guessed that this is really\nalready guessed that this is really really important chains basically what\nreally important chains basically what\nreally important chains basically what are chains or basically what is a chain\nare chains or basically what is a chain\nare chains or basically what is a chain in lang chain if you are a data engineer\nin lang chain if you are a data engineer\nin lang chain if you are a data engineer you can better relate this concept with\nyou can better relate this concept with\nyou can better relate this concept with your something called as pipeline. So\nyour something called as pipeline. So\nyour something called as pipeline. So being a data engineer\nbeing a data engineer\nbeing a data engineer chains\nare very similar to the pipelines.\nare very similar to the pipelines. And what are pipelines? Pipelines are\nAnd what are pipelines? Pipelines are\nAnd what are pipelines? Pipelines are nothing but sequence of tasks. If you\nnothing but sequence of tasks. If you\nnothing but sequence of tasks. If you would just define it in one line,\nwould just define it in one line,\nwould just define it in one line, sequence of task. So let's say where is\nsequence of task. So let's say where is\nsequence of task. So let's say where is Rahul? Rahul will become so popular now.\nRahul? Rahul will become so popular now.\nRahul? Rahul will become so popular now. So let's say this is a data engineer.\nSo let's say this is a data engineer.\nSo let's say this is a data engineer. Okay. And now this data engineer needs\nOkay. And now this data engineer needs\nOkay. And now this data engineer needs to build the entire pipeline. So how\nto build the entire pipeline. So how\nto build the entire pipeline. So how this pipeline look like? Let's say this\nthis pipeline look like? Let's say this\nthis pipeline look like? Let's say this is the task number one maybe then task\nis the task number one maybe then task\nis the task number one maybe then task number two\nnumber two\nnumber two maybe like this maybe like this maybe\nmaybe like this maybe like this maybe\nmaybe like this maybe like this maybe like this like very simple pipeline\nlike this like very simple pipeline\nlike this like very simple pipeline right and if you're data engineer you\nright and if you're data engineer you\nright and if you're data engineer you would know like how to build this. So\nwould know like how to build this. So\nwould know like how to build this. So let's say this is one,\nthis is two\nthis is two and then this then this then this very\nand then this then this then this very\nand then this then this then this very simple pipeline right\nsimple pipeline right\nsimple pipeline right basic pipeline.\nbasic pipeline.\nbasic pipeline. So now\nSo now\nSo now now what we need to do being a data\nnow what we need to do being a data\nnow what we need to do being a data engineer let me first of all remove\nengineer let me first of all remove\nengineer let me first of all remove this.\nSo now let's say this you you have this\nSo now let's say this you you have this pipeline\npipeline\npipeline in data engineering language we just\nin data engineering language we just\nin data engineering language we just call it as a pipeline and in AI\ncall it as a pipeline and in AI\ncall it as a pipeline and in AI engineering language or basically in\nengineering language or basically in\nengineering language or basically in lang chain language we just call it as a\nlang chain language we just call it as a\nlang chain language we just call it as a chain okay\nchain okay\nchain okay chain\nchain\nchain remember one thing this is my personal\nremember one thing this is my personal\nremember one thing this is my personal tip\ntip\ntip um when we just create pipelines\num when we just create pipelines\num when we just create pipelines we create small small pipelines right\nwe create small small pipelines right\nwe create small small pipelines right listen to me carefully so let's Say this\nlisten to me carefully so let's Say this\nlisten to me carefully so let's Say this is only one pipeline and this is a very\nis only one pipeline and this is a very\nis only one pipeline and this is a very simple straightforward pipeline and if\nsimple straightforward pipeline and if\nsimple straightforward pipeline and if we want to create complex pipeline what\nwe want to create complex pipeline what\nwe want to create complex pipeline what do we create?\ndo we create?\ndo we create? We create dedicated nodes. So let me\nWe create dedicated nodes. So let me\nWe create dedicated nodes. So let me just make you understand this concept\njust make you understand this concept\njust make you understand this concept with our previous example. So let's say\nwith our previous example. So let's say\nwith our previous example. So let's say we have this particular lang chain\nwe have this particular lang chain\nwe have this particular lang chain right? So now I am talking about only\nright? So now I am talking about only\nright? So now I am talking about only this part. See so let's say you want to\nthis part. See so let's say you want to\nthis part. See so let's say you want to build something like this.\nbuild something like this.\nbuild something like this. What is this thing? This is a pipeline\nWhat is this thing? This is a pipeline\nWhat is this thing? This is a pipeline but smaller pipeline. Perfect. What is\nbut smaller pipeline. Perfect. What is\nbut smaller pipeline. Perfect. What is this pipeline in itself? Smaller\nthis pipeline in itself? Smaller\nthis pipeline in itself? Smaller pipeline. What is this smaller pipeline?\npipeline. What is this smaller pipeline?\npipeline. What is this smaller pipeline? And when we combine all of these things,\nAnd when we combine all of these things,\nAnd when we combine all of these things, this become our parent pipeline or\nthis become our parent pipeline or\nthis become our parent pipeline or basically the workflow.\nbasically the workflow.\nbasically the workflow. Make sense? So what we are learning\nMake sense? So what we are learning\nMake sense? So what we are learning currently? We are learning about chains.\ncurrently? We are learning about chains.\ncurrently? We are learning about chains. So chains can be complex but if you want\nSo chains can be complex but if you want\nSo chains can be complex but if you want to become um you can say efficient\nto become um you can say efficient\nto become um you can say efficient developer with lang chain do not create\ndeveloper with lang chain do not create\ndeveloper with lang chain do not create long and complex chains always create\nlong and complex chains always create\nlong and complex chains always create smaller smaller chains and then you can\nsmaller smaller chains and then you can\nsmaller smaller chains and then you can combine those chains. Are you getting my\ncombine those chains. Are you getting my\ncombine those chains. Are you getting my point? So let's say one chain like this\npoint? So let's say one chain like this\npoint? So let's say one chain like this okay one chain like this you can just\nokay one chain like this you can just\nokay one chain like this you can just create only one node for this chain only\ncreate only one node for this chain only\ncreate only one node for this chain only one and then you can just connect or\none and then you can just connect or\none and then you can just connect or let's say you have let's say similar\nlet's say you have let's say similar\nlet's say you have let's say similar chain like this here you can connect it\nchain like this here you can connect it\nchain like this here you can connect it make sense so instead of creating the\nmake sense so instead of creating the\nmake sense so instead of creating the whole thing in one go you can just have\nwhole thing in one go you can just have\nwhole thing in one go you can just have this one chain first of all second chain\nthis one chain first of all second chain\nthis one chain first of all second chain then you can just connect both the\nthen you can just connect both the\nthen you can just connect both the things do not make everything available\nthings do not make everything available\nthings do not make everything available within single chain it will be very hard\nwithin single chain it will be very hard\nwithin single chain it will be very hard to debug this This is my personal\nto debug this This is my personal\nto debug this This is my personal recommendation. It's not like hard and\nrecommendation. It's not like hard and\nrecommendation. It's not like hard and set rule that you have to have to break\nset rule that you have to have to break\nset rule that you have to have to break down the chains or you cannot build long\ndown the chains or you cannot build long\ndown the chains or you cannot build long chains. These are like best practices.\nchains. These are like best practices.\nchains. These are like best practices. Again, the choice is yours. If you are a\nAgain, the choice is yours. If you are a\nAgain, the choice is yours. If you are a developer, you can just do whatever you\ndeveloper, you can just do whatever you\ndeveloper, you can just do whatever you want. But this is the concept of chain.\nwant. But this is the concept of chain.\nwant. But this is the concept of chain. Make sense? So this sequence sequence of\nMake sense? So this sequence sequence of\nMake sense? So this sequence sequence of tasks is basically a chain. Make sense?\ntasks is basically a chain. Make sense?\ntasks is basically a chain. Make sense? Very very good. Now you will say an\nVery very good. Now you will say an\nVery very good. Now you will say an Lamba like in um data engineering world\nLamba like in um data engineering world\nLamba like in um data engineering world we just use maybe airflow to build the\nwe just use maybe airflow to build the\nwe just use maybe airflow to build the chain basically to build the pipeline we\nchain basically to build the pipeline we\nchain basically to build the pipeline we also use Azure data factory we also have\nalso use Azure data factory we also have\nalso use Azure data factory we also have let's say AWS glue and all those things\nlet's say AWS glue and all those things\nlet's say AWS glue and all those things what do we use in lang chain because we\nwhat do we use in lang chain because we\nwhat do we use in lang chain because we have just covered how we can just make\nhave just covered how we can just make\nhave just covered how we can just make wrappers on top of our agents prompts\nwrappers on top of our agents prompts\nwrappers on top of our agents prompts and everything the good answer is or you\nand everything the good answer is or you\nand everything the good answer is or you can say the good point is we can just\ncan say the good point is we can just\ncan say the good point is we can just use everything within the lang chain\nuse everything within the lang chain\nuse everything within the lang chain itself. You do not need to combine it\nitself. You do not need to combine it\nitself. You do not need to combine it with that third party tool or whatever.\nwith that third party tool or whatever.\nwith that third party tool or whatever. We have like inbuilt functions\nWe have like inbuilt functions\nWe have like inbuilt functions available. Let me just show you. Let me\navailable. Let me just show you. Let me\navailable. Let me just show you. Let me just create a new folder. Let's call it\njust create a new folder. Let's call it\njust create a new folder. Let's call it as chapter 2. And let's create our first\nas chapter 2. And let's create our first\nas chapter 2. And let's create our first notebook. And I'll simply say basic\nnotebook. And I'll simply say basic\nnotebook. And I'll simply say basic chain or basically first chain.\nchain or basically first chain.\nchain or basically first chain. First chain. IP by NB. Okay. Perfect.\nFirst chain. IP by NB. Okay. Perfect.\nFirst chain. IP by NB. Okay. Perfect. Now I have already talked about what are\nNow I have already talked about what are\nNow I have already talked about what are chains.\nchains.\nchains. Chains in lang chain. Okay. Perfect. So\nChains in lang chain. Okay. Perfect. So\nChains in lang chain. Okay. Perfect. So now let me first of all import that\nnow let me first of all import that\nnow let me first of all import that boilerplate code.\nboilerplate code.\nboilerplate code. So this is the code. Let me just bring\nSo this is the code. Let me just bring\nSo this is the code. Let me just bring it here. Perfect. And let me just select\nit here. Perfect. And let me just select\nit here. Perfect. And let me just select the markdown um this kernel\nthe markdown um this kernel\nthe markdown um this kernel and make it as Python.\nand make it as Python.\nand make it as Python. Perfect. Let me run this.\nSo this is our boiler plate code that we\nSo this is our boiler plate code that we have just copied.\nhave just copied.\nhave just copied. Now let's wait\nNow let's wait\nNow let's wait if our LLM openi is created because see\nif our LLM openi is created because see\nif our LLM openi is created because see this anti-gravity is really new and I\nthis anti-gravity is really new and I\nthis anti-gravity is really new and I have been using for I think almost a\nhave been using for I think almost a\nhave been using for I think almost a month now. Um\nmonth now. Um\nmonth now. Um I started like using this last year and\nI started like using this last year and\nI started like using this last year and I personally feel like it's a great\nI personally feel like it's a great\nI personally feel like it's a great tool. Obviously it is free. Second thing\ntool. Obviously it is free. Second thing\ntool. Obviously it is free. Second thing is it has like so many bugs like\nis it has like so many bugs like\nis it has like so many bugs like sometime it takes so long to just run\nsometime it takes so long to just run\nsometime it takes so long to just run some cells and we have to just close it\nsome cells and we have to just close it\nsome cells and we have to just close it and open it again. So there are some\nand open it again. So there are some\nand open it again. So there are some bugs and that's why they have just kept\nbugs and that's why they have just kept\nbugs and that's why they have just kept it for free. I don't know. Okay. See now\nit for free. I don't know. Okay. See now\nit for free. I don't know. Okay. See now it is also taking so much of time like\nit is also taking so much of time like\nit is also taking so much of time like it should not take much time. If I would\nit should not take much time. If I would\nit should not take much time. If I would be just running the cell in my VS code\nbe just running the cell in my VS code\nbe just running the cell in my VS code editor, it will be like this. VS code is\neditor, it will be like this. VS code is\neditor, it will be like this. VS code is like very stable and very good. So yeah,\nlike very stable and very good. So yeah,\nlike very stable and very good. So yeah, let me just close it and open it once\nlet me just close it and open it once\nlet me just close it and open it once again. Let's see.\nagain. Let's see.\nagain. Let's see. So I have closed the integravity and I\nSo I have closed the integravity and I\nSo I have closed the integravity and I have just ran this again and now it is\nhave just ran this again and now it is\nhave just ran this again and now it is done. Sometime it takes like this much\ndone. Sometime it takes like this much\ndone. Sometime it takes like this much of effort. Okay. That is our LLM open\nof effort. Okay. That is our LLM open\nof effort. Okay. That is our LLM open instance is created. That means we can\ninstance is created. That means we can\ninstance is created. That means we can now just call the models. Now let me\nnow just call the models. Now let me\nnow just call the models. Now let me just show you a basic um you can say\njust show you a basic um you can say\njust show you a basic um you can say chain that we'll be creating and this\nchain that we'll be creating and this\nchain that we'll be creating and this chain is very cool. So now you know that\nchain is very cool. So now you know that\nchain is very cool. So now you know that how do we um connect with LLM and how\nhow do we um connect with LLM and how\nhow do we um connect with LLM and how should we connect with LM? We should\nshould we connect with LM? We should\nshould we connect with LM? We should always create a prompt first of all\nalways create a prompt first of all\nalways create a prompt first of all right obviously. So we will be creating\nright obviously. So we will be creating\nright obviously. So we will be creating a prompt. So let me just write\num first chain right. So in this first\num first chain right. So in this first chain I will first of all create the\nchain I will first of all create the\nchain I will first of all create the prompt.\nprompt.\nprompt. And how do we create the prompt? You\nAnd how do we create the prompt? You\nAnd how do we create the prompt? You should know now.\nshould know now.\nshould know now. You should you should know now. You\nYou should you should know now. You\nYou should you should know now. You should know by now. So now I will simply\nshould know by now. So now I will simply\nshould know by now. So now I will simply say prompt template because first of all\nsay prompt template because first of all\nsay prompt template because first of all we create the template right?\nwe create the template right?\nwe create the template right? prop template equals chat prompt\nprop template equals chat prompt\nprop template equals chat prompt template and let me just import it if it\ntemplate and let me just import it if it\ntemplate and let me just import it if it is not imported.\nis not imported.\nis not imported. Uh no it is not imported. So let me just\nUh no it is not imported. So let me just\nUh no it is not imported. So let me just import it\nimport it\nimport it from lang chain\nfrom lang chain\nfrom lang chain core.prompts\ncore.prompts\ncore.prompts import chat prompt template. Okay\nimport chat prompt template. Okay\nimport chat prompt template. Okay perfect.\nperfect.\nperfect. So we have chat prompt template for\nSo we have chat prompt template for\nSo we have chat prompt template for messages and system message UR helpful\nmessages and system message UR helpful\nmessages and system message UR helpful assistant simple and human is input\nassistant simple and human is input\nassistant simple and human is input because we do not know like what will be\nbecause we do not know like what will be\nbecause we do not know like what will be the input okay and it we are creating a\nthe input okay and it we are creating a\nthe input okay and it we are creating a you can say\nyou can say\nyou can say chain which can take any value like\nchain which can take any value like\nchain which can take any value like basically anything it is not just a you\nbasically anything it is not just a you\nbasically anything it is not just a you can say joke generator or fun fact\ncan say joke generator or fun fact\ncan say joke generator or fun fact generator it can take anything so that\ngenerator it can take anything so that\ngenerator it can take anything so that is why I'm not guiding anything in the\nis why I'm not guiding anything in the\nis why I'm not guiding anything in the human message I'm simply saying input\nhuman message I'm simply saying input\nhuman message I'm simply saying input sorted good now this is my first task\nsorted good now this is my first task\nsorted good now this is my first task Okay, let me just write it task one.\nOkay, let me just write it task one.\nOkay, let me just write it task one. Task one prompt. I want to create task\nTask one prompt. I want to create task\nTask one prompt. I want to create task two. Let me just first of all run this.\ntwo. Let me just first of all run this.\ntwo. Let me just first of all run this. Then I will say task 2. And this task\nThen I will say task 2. And this task\nThen I will say task 2. And this task two will be lm, right? Because I want to\ntwo will be lm, right? Because I want to\ntwo will be lm, right? Because I want to send this message to the llm. So I'll\nsend this message to the llm. So I'll\nsend this message to the llm. So I'll simply say um\nsimply say um\nsimply say um llm open.\nllm open.\nllm open. Okay, let's let's actually automate this\nOkay, let's let's actually automate this\nOkay, let's let's actually automate this as well. So we already have llm open a\nas well. So we already have llm open a\nas well. So we already have llm open a here. So I will simply repeat this step.\nhere. So I will simply repeat this step.\nhere. So I will simply repeat this step. That's it.\nThat's it.\nThat's it. Perfect. So this is our LLM. Third step\nPerfect. So this is our LLM. Third step\nPerfect. So this is our LLM. Third step will be I want to use a parser. Now what\nwill be I want to use a parser. Now what\nwill be I want to use a parser. Now what is parser? If you remember if my\nis parser? If you remember if my\nis parser? If you remember if my previous notebook if you'll see you will\nprevious notebook if you'll see you will\nprevious notebook if you'll see you will see that every time I use something\nsee that every time I use something\nsee that every time I use something called as content. If I want to fetch\ncalled as content. If I want to fetch\ncalled as content. If I want to fetch only the content value see dot content\nonly the content value see dot content\nonly the content value see dot content dot content dot content like every time\ndot content dot content like every time\ndot content dot content like every time we see dot content because if we do not\nwe see dot content because if we do not\nwe see dot content because if we do not write dot content we will get the\nwrite dot content we will get the\nwrite dot content we will get the unnecessary things as well such as um\nunnecessary things as well such as um\nunnecessary things as well such as um parameters um those things let's say\nparameters um those things let's say\nparameters um those things let's say token size and all and we do not need to\ntoken size and all and we do not need to\ntoken size and all and we do not need to show that thing right so one option is\nshow that thing right so one option is\nshow that thing right so one option is we can create like we can use dot\nwe can create like we can use dot\nwe can create like we can use dot content right or we can use a built-in\ncontent right or we can use a built-in\ncontent right or we can use a built-in function in lang chain which is called\nfunction in lang chain which is called\nfunction in lang chain which is called string output parser So what that output\nstring output parser So what that output\nstring output parser So what that output part uh but what that output parcel will\npart uh but what that output parcel will\npart uh but what that output parcel will do it will take your generated response\ndo it will take your generated response\ndo it will take your generated response and it will only fetch the content out\nand it will only fetch the content out\nand it will only fetch the content out of it and behind the scenes it is doing\nof it and behind the scenes it is doing\nof it and behind the scenes it is doing the same thing it is writing dot content\nthe same thing it is writing dot content\nthe same thing it is writing dot content but it is automating that thing and you\nbut it is automating that thing and you\nbut it is automating that thing and you do not need to write again and again. So\ndo not need to write again and again. So\ndo not need to write again and again. So I can simply say\nI can simply say\nI can simply say from lang chain\nuh\nuh let me just check here what did we use\nlet me just check here what did we use\nlet me just check here what did we use output parser let me just check the\noutput parser let me just check the\noutput parser let me just check the class for it let's try using first this\nclass for it let's try using first this\nclass for it let's try using first this dot parsers import\ndot parsers import\ndot parsers import string string output parser. Yeah, let's\nstring string output parser. Yeah, let's\nstring string output parser. Yeah, let's try this. No, let me just check the\ntry this. No, let me just check the\ntry this. No, let me just check the library for this. Um, string output\nlibrary for this. Um, string output\nlibrary for this. Um, string output parser. This is our lang chain.\nparser. This is our lang chain.\nparser. This is our lang chain. String output parser lang chain. What is\nString output parser lang chain. What is\nString output parser lang chain. What is the library, bro?\nthe library, bro?\nthe library, bro? What is the library?\nYeah, it's called lang chain core dot\nYeah, it's called lang chain core dot output parser. Okay, makes sense.\noutput parser. Okay, makes sense.\noutput parser. Okay, makes sense. output parsers. Okay. Okay. Okay.\nPerfect. Yeah. So, we have a function\nPerfect. Yeah. So, we have a function called output parser, right? See like\ncalled output parser, right? See like\ncalled output parser, right? See like this. Now, I can just use this. How? Let\nthis. Now, I can just use this. How? Let\nthis. Now, I can just use this. How? Let me just show you\ntask three\ntask three string parser.\nstring parser.\nstring parser. So, I will create an instance of this\nSo, I will create an instance of this\nSo, I will create an instance of this function. I will simply say string\nfunction. I will simply say string\nfunction. I will simply say string parser\nparser\nparser equals\nequals\nequals string\nstring\nstring output parser. Simple. So how many tasks\noutput parser. Simple. So how many tasks\noutput parser. Simple. So how many tasks have I created? Three. Perfect. If I\nhave I created? Three. Perfect. If I\nhave I created? Three. Perfect. If I will be just manually running all these\nwill be just manually running all these\nwill be just manually running all these tasks. I know how to run this. First of\ntasks. I know how to run this. First of\ntasks. I know how to run this. First of all, I will invoke this template by\nall, I will invoke this template by\nall, I will invoke this template by feeding the value inside this. Then I\nfeeding the value inside this. Then I\nfeeding the value inside this. Then I will pass this prompt template into this\nwill pass this prompt template into this\nwill pass this prompt template into this into this lm openi. Let me show you. So\ninto this lm openi. Let me show you. So\ninto this lm openi. Let me show you. So first of all let me just say manual\nfirst of all let me just say manual\nfirst of all let me just say manual uh manual thing force chain let me just\nuh manual thing force chain let me just\nuh manual thing force chain let me just write it for you\nwrite it for you\nwrite it for you manual invocation.\nSo manual invocation will look like\nSo manual invocation will look like this.\nthis.\nthis. So first of all I will simply say prompt\nSo first of all I will simply say prompt\nSo first of all I will simply say prompt template. So I will say template\nequals prompt template dot invoke some\nequals prompt template dot invoke some value\nvalue\nvalue let's say and what is the key input okay\nwhat is the capital of France then I\nwhat is the capital of France then I will say llm dot invoke\nwill say llm dot invoke\nwill say llm dot invoke template and then I will say final\ntemplate and then I will say final\ntemplate and then I will say final output is\noutput is\noutput is final result equals uh this dot\nfinal result equals uh this dot\nfinal result equals uh this dot Let me just store it in the result\nLet me just store it in the result\nLet me just store it in the result and then I will say result dot content.\nand then I will say result dot content.\nand then I will say result dot content. Right? This will be my manual invocation\nRight? This will be my manual invocation\nRight? This will be my manual invocation of every single step. Just imagine that\nof every single step. Just imagine that\nof every single step. Just imagine that you want to run this particular chain\nyou want to run this particular chain\nyou want to run this particular chain and you are just manually invoking every\nand you are just manually invoking every\nand you are just manually invoking every step. That's how you build data\nstep. That's how you build data\nstep. That's how you build data pipelines. No. So how you can just\npipelines. No. So how you can just\npipelines. No. So how you can just create it? So I will simply comment it\ncreate it? So I will simply comment it\ncreate it? So I will simply comment it out\nout\nout because we are not manually running it.\nbecause we are not manually running it.\nbecause we are not manually running it. If you want to just run it, you can. But\nIf you want to just run it, you can. But\nIf you want to just run it, you can. But I am not running this at all. Okay. So I\nI am not running this at all. Okay. So I\nI am not running this at all. Okay. So I will say chain invocation.\nwill say chain invocation.\nwill say chain invocation. So I can invoke it through the chain\nSo I can invoke it through the chain\nSo I can invoke it through the chain and in order to invoke this in the form\nand in order to invoke this in the form\nand in order to invoke this in the form of chain I will be creating a chain\nof chain I will be creating a chain\nof chain I will be creating a chain obviously. So I will simply say chain\nobviously. So I will simply say chain\nobviously. So I will simply say chain equals whatever value you want to use\nequals whatever value you want to use\nequals whatever value you want to use and there there are basically two ways\nand there there are basically two ways\nand there there are basically two ways to create the chains. I like the I\nto create the chains. I like the I\nto create the chains. I like the I personally like the first one. So what\npersonally like the first one. So what\npersonally like the first one. So what do we need to write in this particular\ndo we need to write in this particular\ndo we need to write in this particular method? You simply need to use a pipe\nmethod? You simply need to use a pipe\nmethod? You simply need to use a pipe operator which is also called as LCEL\noperator which is also called as LCEL\noperator which is also called as LCEL lang chain expression language. So what\nlang chain expression language. So what\nlang chain expression language. So what do I need to write? I just need to\ndo I need to write? I just need to\ndo I need to write? I just need to display or basically write the function\ndisplay or basically write the function\ndisplay or basically write the function in the order I want to run it. So first\nin the order I want to run it. So first\nin the order I want to run it. So first of all I want to run prompt right prompt\nof all I want to run prompt right prompt\nof all I want to run prompt right prompt template. Then I will use pipe operator.\ntemplate. Then I will use pipe operator.\ntemplate. Then I will use pipe operator. Then I will simply say llm open my\nThen I will simply say llm open my\nThen I will simply say llm open my second function task basically. Then\nsecond function task basically. Then\nsecond function task basically. Then this then pipe operator. Then I will\nthis then pipe operator. Then I will\nthis then pipe operator. Then I will simply say string output parser\nsimply say string output parser\nsimply say string output parser basically string parser. Perfect.\nbasically string parser. Perfect.\nbasically string parser. Perfect. string parser.\nstring parser.\nstring parser. Perfect. So this is my chain now. And\nPerfect. So this is my chain now. And\nPerfect. So this is my chain now. And you know what? I just need to say\nyou know what? I just need to say\nyou know what? I just need to say chain.invoke\nchain do.invoke and I can pass the\nchain do.invoke and I can pass the dictionary\ndictionary\ndictionary input. Let's say what is the capital of\ninput. Let's say what is the capital of\ninput. Let's say what is the capital of France? Whatever I want to ask. Okay. So\nFrance? Whatever I want to ask. Okay. So\nFrance? Whatever I want to ask. Okay. So now do you know what will happen? It\nnow do you know what will happen? It\nnow do you know what will happen? It will first of all invoke\nwill first of all invoke\nwill first of all invoke this thing first function.\nthis thing first function.\nthis thing first function. Then it will invoke lm open and then it\nThen it will invoke lm open and then it\nThen it will invoke lm open and then it will invoke string parser automatically.\nwill invoke string parser automatically.\nwill invoke string parser automatically. Let me show you the result.\nLet me show you the result.\nLet me show you the result. Let me just run this. See you are just\nLet me just run this. See you are just\nLet me just run this. See you are just getting the final answer automatically.\ngetting the final answer automatically.\ngetting the final answer automatically. You didn't run this manually. You didn't\nYou didn't run this manually. You didn't\nYou didn't run this manually. You didn't run this manually. You didn't run this\nrun this manually. You didn't run this\nrun this manually. You didn't run this manually. You are simply getting the\nmanually. You are simply getting the\nmanually. You are simply getting the final response as is. The capital of\nfinal response as is. The capital of\nfinal response as is. The capital of France is Paris. Really? Yeah. Okay,\nFrance is Paris. Really? Yeah. Okay,\nFrance is Paris. Really? Yeah. Okay, makes sense. Now you will have so many\nmakes sense. Now you will have so many\nmakes sense. Now you will have so many questions in your mind right now. I'm so\nquestions in your mind right now. I'm so\nquestions in your mind right now. I'm so sure because I also had so many\nsure because I also had so many\nsure because I also had so many questions when I was learning about\nquestions when I was learning about\nquestions when I was learning about lang. How this flow is working, how\nlang. How this flow is working, how\nlang. How this flow is working, how input is going, how the data is flowing.\ninput is going, how the data is flowing.\ninput is going, how the data is flowing. I know I know I know wait I will just\nI know I know I know wait I will just\nI know I know I know wait I will just explain you everything. So how\nexplain you everything. So how\nexplain you everything. So how everything is going on? So let me just\neverything is going on? So let me just\neverything is going on? So let me just open my\nopen my\nopen my this thing. So let me show you how the\nthis thing. So let me show you how the\nthis thing. So let me show you how the things are happening. So first of all we\nthings are happening. So first of all we\nthings are happening. So first of all we have let's say three tasks, right?\nhave let's say three tasks, right?\nhave let's say three tasks, right? So first of all we have um template\nfirst of all we have template then we\nfirst of all we have template then we have lm openi let's say l lm\nllm just lm okay then we have string\nllm just lm okay then we have string output parser\nperfect so we have this chain\nperfect so we have this chain simple sorted and you know what will\nsimple sorted and you know what will\nsimple sorted and you know what will happen first of all you will\nhappen first of all you will\nhappen first of all you will Rahul will send the input now input can\nRahul will send the input now input can\nRahul will send the input now input can be anything right so input will go like\nbe anything right so input will go like\nbe anything right so input will go like this\nthis\nthis um input and he asked what is the\num input and he asked what is the\num input and he asked what is the capital of France\ncapital of France\ncapital of France just focus there because this is very\njust focus there because this is very\njust focus there because this is very important and this will cover are all\nimportant and this will cover are all\nimportant and this will cover are all the fundamentals and Rahul so sorry I'm\nthe fundamentals and Rahul so sorry I'm\nthe fundamentals and Rahul so sorry I'm just removing your name because I want\njust removing your name because I want\njust removing your name because I want to just put the input perfect so this\nto just put the input perfect so this\nto just put the input perfect so this will go here\nwill go here\nwill go here as the input simple now this template\nas the input simple now this template\nas the input simple now this template this template this template will receive\nthis template this template will receive\nthis template this template will receive this as input\nso this is the input.\nso this is the input. Okay, let me just change the color as\nOkay, let me just change the color as\nOkay, let me just change the color as well. Let's say red. Okay, perfect. So,\nwell. Let's say red. Okay, perfect. So,\nwell. Let's say red. Okay, perfect. So, this is the input. Okay, what is the\nthis is the input. Okay, what is the\nthis is the input. Okay, what is the capital of France? Because this requires\ncapital of France? Because this requires\ncapital of France? Because this requires this dictionary. If you just look at the\nthis dictionary. If you just look at the\nthis dictionary. If you just look at the template, how do we invoke it? We invoke\ntemplate, how do we invoke it? We invoke\ntemplate, how do we invoke it? We invoke it like this, right? Like this. Perfect.\nit like this, right? Like this. Perfect.\nit like this, right? Like this. Perfect. Now,\nNow,\nNow, we will\nwe will\nwe will we will generate what? How our output\nwe will generate what? How our output\nwe will generate what? How our output will look like?\nOutput will have obviously the template\nOutput will have obviously the template object\nobject\nobject human message system message. Remember\nhuman message system message. Remember\nhuman message system message. Remember we discussed it in the fundamentals here\nwe discussed it in the fundamentals here\nwe discussed it in the fundamentals here in the messages like how we just use the\nin the messages like how we just use the\nin the messages like how we just use the templates.\ntemplates.\ntemplates. Remember\nRemember\nRemember remember like this.\nremember like this.\nremember like this. So now\nSo now\nSo now I think we discussed here as well like\nI think we discussed here as well like\nI think we discussed here as well like how do we just create the prompts here?\nhow do we just create the prompts here?\nhow do we just create the prompts here? See when we invoke it what do we see? So\nSee when we invoke it what do we see? So\nSee when we invoke it what do we see? So now what will happen? This will generate\nnow what will happen? This will generate\nnow what will happen? This will generate the AI message, human message, something\nthe AI message, human message, something\nthe AI message, human message, something like that.\nlike that.\nlike that. Simple. Let's say it generated. Let me\nSimple. Let's say it generated. Let me\nSimple. Let's say it generated. Let me just invoke it. You will say like, bro,\njust invoke it. You will say like, bro,\njust invoke it. You will say like, bro, we forgot blah blah blah. So let me just\nwe forgot blah blah blah. So let me just\nwe forgot blah blah blah. So let me just run this particular thing.\nrun this particular thing.\nrun this particular thing. And perfect. This ran fine.\nAnd perfect. This ran fine.\nAnd perfect. This ran fine. And let me just run this. And let me\nAnd let me just run this. And let me\nAnd let me just run this. And let me just run this prompt thing. And I will\njust run this prompt thing. And I will\njust run this prompt thing. And I will not invoke this function. I will simply\nnot invoke this function. I will simply\nnot invoke this function. I will simply show you what ready prompt look like.\nshow you what ready prompt look like.\nshow you what ready prompt look like. Ready prompt content is very important.\nReady prompt content is very important.\nReady prompt content is very important. Now let's say what is the\nWhat is the\nWhat is the capital of France? Okay this is the\ncapital of France? Okay this is the\ncapital of France? Okay this is the thing. This is the thing. This is the\nthing. This is the thing. This is the\nthing. This is the thing. This is the ready prompt.\nready prompt.\nready prompt. Right? And if you are using let's say\nRight? And if you are using let's say\nRight? And if you are using let's say chat message let's say chat prompt\nchat message let's say chat prompt\nchat message let's say chat prompt template then obviously the message will\ntemplate then obviously the message will\ntemplate then obviously the message will look like something else like if you are\nlook like something else like if you are\nlook like something else like if you are using chat prompt template like here if\nusing chat prompt template like here if\nusing chat prompt template like here if I just invoke this as well just for you\nI just invoke this as well just for you\nI just invoke this as well just for you I can show you ready prompt\nI can show you ready prompt\nI can show you ready prompt and let me just comment it out here as\nand let me just comment it out here as\nand let me just comment it out here as well topic\nwell topic\nwell topic um let's say honey\num let's say honey\num let's say honey and tone let's say funny so this is the\nand tone let's say funny so this is the\nand tone let's say funny so this is the value that I will see see round value\nvalue that I will see see round value\nvalue that I will see see round value messages is equals to this and whatever\nmessages is equals to this and whatever\nmessages is equals to this and whatever values do we have here perfect so I'll\nvalues do we have here perfect so I'll\nvalues do we have here perfect so I'll be having values like this make sense so\nbe having values like this make sense so\nbe having values like this make sense so this will be my output\nthis will be my output\nthis will be my output yes this will be my output and obviously\nyes this will be my output and obviously\nyes this will be my output and obviously it is too big but yeah this will be my\nit is too big but yeah this will be my\nit is too big but yeah this will be my output make sense now I have this output\noutput make sense now I have this output\noutput make sense now I have this output this output this one this output will\nthis output this one this output will\nthis output this one this output will flow will actually flow from here and it\nflow will actually flow from here and it\nflow will actually flow from here and it will go to here as input.\nwill go to here as input.\nwill go to here as input. Okay. Okay. It will go here to as input\nOkay. Okay. It will go here to as input\nOkay. Okay. It will go here to as input or I can also move it here so that you\nor I can also move it here so that you\nor I can also move it here so that you can understand it better. So this will\ncan understand it better. So this will\ncan understand it better. So this will be my input and this will go here. This\nbe my input and this will go here. This\nbe my input and this will go here. This will be my input. So output of one task\nwill be my input. So output of one task\nwill be my input. So output of one task will exactly be the input. Why it is so\nwill exactly be the input. Why it is so\nwill exactly be the input. Why it is so important? Why I'm stressing so much\nimportant? Why I'm stressing so much\nimportant? Why I'm stressing so much here? Because the next topics are\nhere? Because the next topics are\nhere? Because the next topics are dependent strongly dependent on this\ndependent strongly dependent on this\ndependent strongly dependent on this concept. So this output will become my\nconcept. So this output will become my\nconcept. So this output will become my input. Whatever it is, we cannot weak it\ninput. Whatever it is, we cannot weak it\ninput. Whatever it is, we cannot weak it because it is automated. Whatever it\nbecause it is automated. Whatever it\nbecause it is automated. Whatever it will generate, we have to accept it. V\nwill generate, we have to accept it. V\nwill generate, we have to accept it. V means LLM node.\nmeans LLM node.\nmeans LLM node. Make sense? Now this LLM what it will\nMake sense? Now this LLM what it will\nMake sense? Now this LLM what it will generate? It will generate its own\ngenerate? It will generate its own\ngenerate? It will generate its own content like AI message and quarks and\ncontent like AI message and quarks and\ncontent like AI message and quarks and whatever. And this will become its\nwhatever. And this will become its\nwhatever. And this will become its output. This output will become now\noutput. This output will become now\noutput. This output will become now input of string\ninput of string\ninput of string input parser basically function. Make\ninput parser basically function. Make\ninput parser basically function. Make sense? This will become its input. Now\nsense? This will become its input. Now\nsense? This will become its input. Now this parser will generate what only\nthis parser will generate what only\nthis parser will generate what only content because it will simply say dot\ncontent because it will simply say dot\ncontent because it will simply say dot content and whatever we already know\ncontent and whatever we already know\ncontent and whatever we already know like what it will do. So this is the\nlike what it will do. So this is the\nlike what it will do. So this is the basically the flow of it like how does\nbasically the flow of it like how does\nbasically the flow of it like how does it work make sense. So I have just\nit work make sense. So I have just\nit work make sense. So I have just rearranged this with the values so that\nrearranged this with the values so that\nrearranged this with the values so that you can better relate with it whenever\nyou can better relate with it whenever\nyou can better relate with it whenever you will be referring to this particular\nyou will be referring to this particular\nyou will be referring to this particular nodes. So I hope now you know what is\nnodes. So I hope now you know what is\nnodes. So I hope now you know what is the intent. The intent is you need to be\nthe intent. The intent is you need to be\nthe intent. The intent is you need to be very sure while creating the chains\nvery sure while creating the chains\nvery sure while creating the chains because the output of one node basically\nbecause the output of one node basically\nbecause the output of one node basically task will be the input for the next node\ntask will be the input for the next node\ntask will be the input for the next node or basically task. If you are expecting\nor basically task. If you are expecting\nor basically task. If you are expecting here something else and your previous\nhere something else and your previous\nhere something else and your previous task is generating some something else\ntask is generating some something else\ntask is generating some something else then it will not work because it will\nthen it will not work because it will\nthen it will not work because it will simply mess up with the um you can say\nsimply mess up with the um you can say\nsimply mess up with the um you can say requirements because let's say you are\nrequirements because let's say you are\nrequirements because let's say you are here requesting a kind of um different\nhere requesting a kind of um different\nhere requesting a kind of um different um input variable and you are giving it\num input variable and you are giving it\num input variable and you are giving it something else. So it can actually break\nsomething else. So it can actually break\nsomething else. So it can actually break it. So this was a very simple example\nit. So this was a very simple example\nit. So this was a very simple example that's why it worked extremely fine and\nthat's why it worked extremely fine and\nthat's why it worked extremely fine and we didn't need to make any changes and\nwe didn't need to make any changes and\nwe didn't need to make any changes and by the way you were just talking about\nby the way you were just talking about\nby the way you were just talking about we have like two different ways to run\nwe have like two different ways to run\nwe have like two different ways to run it. Yes. So we can also run it with the\nit. Yes. So we can also run it with the\nit. Yes. So we can also run it with the help of runnable sequence. Now what is\nhelp of runnable sequence. Now what is\nhelp of runnable sequence. Now what is this runnable sequence? Um this is\nthis runnable sequence? Um this is\nthis runnable sequence? Um this is basically the kind of function that we\nbasically the kind of function that we\nbasically the kind of function that we can use in order to create the chains\ncan use in order to create the chains\ncan use in order to create the chains and basically run it. I personally like\nand basically run it. I personally like\nand basically run it. I personally like this pipe operator but if you want to\nthis pipe operator but if you want to\nthis pipe operator but if you want to know I can just show you as well. So\nknow I can just show you as well. So\nknow I can just show you as well. So we'll simply say\nwe'll simply say\nwe'll simply say optional\noptional\noptional um\num\num from lang chain core\nfrom lang chain core\nfrom lang chain core dot runnables I guess import runnable\ndot runnables I guess import runnable\ndot runnables I guess import runnable sequence so you can simply say chain\nsequence so you can simply say chain\nsequence so you can simply say chain two or let's say chain one equals\ntwo or let's say chain one equals\ntwo or let's say chain one equals runnable sequence and in this particular\nrunnable sequence and in this particular\nrunnable sequence and in this particular list we just need to pass all of the\nlist we just need to pass all of the\nlist we just need to pass all of the functions all of the functions\nfunctions all of the functions\nfunctions all of the functions functions means like all of the tasks\nfunctions means like all of the tasks\nfunctions means like all of the tasks that we have created\nthat we have created\nthat we have created Makes sense. Makes sense. Makes sense.\nMakes sense. Makes sense. Makes sense.\nMakes sense. Makes sense. Makes sense. Okay. So now I can simply pass like\nOkay. So now I can simply pass like\nOkay. So now I can simply pass like renable sequence and let me just say um\nrenable sequence and let me just say um\nrenable sequence and let me just say um prompt template\nprompt template\nprompt template and then lm open ai\nand then we have output parser. Perfect.\nand then we have output parser. Perfect. So we can also write it like this. L\nSo we can also write it like this. L\nSo we can also write it like this. L open ai is not defined. Wow.\nopen ai is not defined. Wow.\nopen ai is not defined. Wow. LM opi\noutput parser very good very good\noutput parser very good very good very good code awareness whatever model\nvery good code awareness whatever model\nvery good code awareness whatever model this anti-gravity is using so see we\nthis anti-gravity is using so see we\nthis anti-gravity is using so see we also got this value but I'm a big fan of\nalso got this value but I'm a big fan of\nalso got this value but I'm a big fan of using this because it makes um the chain\nusing this because it makes um the chain\nusing this because it makes um the chain more readable we can easily define the\nmore readable we can easily define the\nmore readable we can easily define the um you can say dependencies and it is\num you can say dependencies and it is\num you can say dependencies and it is very much similar to defining\nvery much similar to defining\nvery much similar to defining dependencies in airflow we use like this\ndependencies in airflow we use like this\ndependencies in airflow we use like this operator in airflow tuck tuck And here\noperator in airflow tuck tuck And here\noperator in airflow tuck tuck And here we are using piper operator. It's fine.\nwe are using piper operator. It's fine.\nwe are using piper operator. It's fine. So yeah, choice is yours if you want to\nSo yeah, choice is yours if you want to\nSo yeah, choice is yours if you want to use this one because some people would\nuse this one because some people would\nuse this one because some people would like this one. So I'm not uh like\nlike this one. So I'm not uh like\nlike this one. So I'm not uh like restricting you to just use one method\nrestricting you to just use one method\nrestricting you to just use one method but you have the options available.\nbut you have the options available.\nbut you have the options available. Okay, perfect. So this was a very basic\nOkay, perfect. So this was a very basic\nOkay, perfect. So this was a very basic example of chains. Now in real life we\nexample of chains. Now in real life we\nexample of chains. Now in real life we build chains which require some custom\nbuild chains which require some custom\nbuild chains which require some custom functions as well. What do I mean by\nfunctions as well. What do I mean by\nfunctions as well. What do I mean by custom functions? So basically let's say\ncustom functions? So basically let's say\ncustom functions? So basically let's say you want to build a chain where you want\nyou want to build a chain where you want\nyou want to build a chain where you want to basically add some functions between\nto basically add some functions between\nto basically add some functions between this. For example, we have this chain\nthis. For example, we have this chain\nthis. For example, we have this chain template llm parser. Parser means the\ntemplate llm parser. Parser means the\ntemplate llm parser. Parser means the final output.\nfinal output.\nfinal output. Now I want to add one more um you can\nNow I want to add one more um you can\nNow I want to add one more um you can say task, right? One more task and this\nsay task, right? One more task and this\nsay task, right? One more task and this task will pass this value to a different\ntask will pass this value to a different\ntask will pass this value to a different LLM. Okay. And what this LM will do,\nLLM. Okay. And what this LM will do,\nLLM. Okay. And what this LM will do, this will behave as a kind of\nthis will behave as a kind of\nthis will behave as a kind of post generator. Let's say you want to\npost generator. Let's say you want to\npost generator. Let's say you want to first of all write a joke, right? And\nfirst of all write a joke, right? And\nfirst of all write a joke, right? And then you will simply get the message and\nthen you will simply get the message and\nthen you will simply get the message and then you will just um get the content\nthen you will just um get the content\nthen you will just um get the content out of it. Now in the second task or\nout of it. Now in the second task or\nout of it. Now in the second task or basically in the second chain what you\nbasically in the second chain what you\nbasically in the second chain what you can do or basically second part of the\ncan do or basically second part of the\ncan do or basically second part of the chain what you can do you can now send\nchain what you can do you can now send\nchain what you can do you can now send this particular output to a different\nthis particular output to a different\nthis particular output to a different part where it will evaluate and it will\npart where it will evaluate and it will\npart where it will evaluate and it will make it in a form that you can just post\nmake it in a form that you can just post\nmake it in a form that you can just post it on your social media.\nit on your social media.\nit on your social media. Let's say you just want to create a post\nLet's say you just want to create a post\nLet's say you just want to create a post on LinkedIn for this particular purpose.\non LinkedIn for this particular purpose.\non LinkedIn for this particular purpose. So what you will do you will then add a\nSo what you will do you will then add a\nSo what you will do you will then add a function in between. Why? Why? Let me\nfunction in between. Why? Why? Let me\nfunction in between. Why? Why? Let me show you why. Because if you\nshow you why. Because if you\nshow you why. Because if you do not do that,\ndo not do that,\ndo not do that, if you let's say, let me just delete\nif you let's say, let me just delete\nif you let's say, let me just delete this. Let me just delete this because\nthis. Let me just delete this because\nthis. Let me just delete this because you already know what the output of\nyou already know what the output of\nyou already know what the output of this. So let's say you want to send this\nthis. So let's say you want to send this\nthis. So let's say you want to send this thing to the LLM and how do you send it?\nthing to the LLM and how do you send it?\nthing to the LLM and how do you send it? Obviously, we use this node, right? We\nObviously, we use this node, right? We\nObviously, we use this node, right? We those we we use this kind of node or\nthose we we use this kind of node or\nthose we we use this kind of node or basically task.\nbasically task.\nbasically task. Now just tell me one thing.\nNow just tell me one thing.\nNow just tell me one thing. If I want to send this message to LLM,\nIf I want to send this message to LLM,\nIf I want to send this message to LLM, what's the best practice? Obviously\nwhat's the best practice? Obviously\nwhat's the best practice? Obviously using template,\nusing template,\nusing template, right? So let me just bring template\nright? So let me just bring template\nright? So let me just bring template here first of all before even LLM. So\nhere first of all before even LLM. So\nhere first of all before even LLM. So when this output will go to the input,\nwhen this output will go to the input,\nwhen this output will go to the input, does that make any sense that I'm just\ndoes that make any sense that I'm just\ndoes that make any sense that I'm just sending this input like this? Yeah, it\nsending this input like this? Yeah, it\nsending this input like this? Yeah, it makes sense. Does it make sense? Think\nmakes sense. Does it make sense? Think\nmakes sense. Does it make sense? Think about it. Like why did I explain\nabout it. Like why did I explain\nabout it. Like why did I explain everything to you regarding the output?\neverything to you regarding the output?\neverything to you regarding the output? Because now here the output is just\nBecause now here the output is just\nBecause now here the output is just generating the text. And how do we pass\ngenerating the text. And how do we pass\ngenerating the text. And how do we pass the input to any template? We pass it in\nthe input to any template? We pass it in\nthe input to any template? We pass it in the form of dictionary. Oh,\nthe form of dictionary. Oh,\nthe form of dictionary. Oh, so where is the dictionary?\nso where is the dictionary?\nso where is the dictionary? Where's the dictionary?\nWhere's the dictionary?\nWhere's the dictionary? Makes sense. And where's the key for\nMakes sense. And where's the key for\nMakes sense. And where's the key for that dictionary? Because we just create\nthat dictionary? Because we just create\nthat dictionary? Because we just create key value pairs, right? So we need to\nkey value pairs, right? So we need to\nkey value pairs, right? So we need to create one node which will be called as\ncreate one node which will be called as\ncreate one node which will be called as let's say custom node or basically\nlet's say custom node or basically\nlet's say custom node or basically custom task.\ncustom task.\ncustom task. Okay, I will simply say custom.\nOkay, I will simply say custom.\nOkay, I will simply say custom. So this custom function what will do? It\nSo this custom function what will do? It\nSo this custom function what will do? It will take the output of the previous\nwill take the output of the previous\nwill take the output of the previous task. It will modify it in such a way so\ntask. It will modify it in such a way so\ntask. It will modify it in such a way so that we can pass this value to the\nthat we can pass this value to the\nthat we can pass this value to the template in the form of a dictionary. So\ntemplate in the form of a dictionary. So\ntemplate in the form of a dictionary. So this will be dictionary maker. This task\nthis will be dictionary maker. This task\nthis will be dictionary maker. This task will be dictionary maker. Now can we\nwill be dictionary maker. Now can we\nwill be dictionary maker. Now can we just write our custom Python function as\njust write our custom Python function as\njust write our custom Python function as well? Yes, you can even convert your\nwell? Yes, you can even convert your\nwell? Yes, you can even convert your custom Python functions into the form of\ncustom Python functions into the form of\ncustom Python functions into the form of tasks. Wow. And how do we do that? It's\ntasks. Wow. And how do we do that? It's\ntasks. Wow. And how do we do that? It's very simple. You just create a Python\nvery simple. You just create a Python\nvery simple. You just create a Python task. Basically, Python functions and\ntask. Basically, Python functions and\ntask. Basically, Python functions and then you use one for um use one\nthen you use one for um use one\nthen you use one for um use one function. It's called add the rate I\nfunction. It's called add the rate I\nfunction. It's called add the rate I think just forget about add the rate\nthink just forget about add the rate\nthink just forget about add the rate just use simple one runnable lambda. You\njust use simple one runnable lambda. You\njust use simple one runnable lambda. You can just use runnable lambda and you can\ncan just use runnable lambda and you can\ncan just use runnable lambda and you can convert any function into a runnable.\nconvert any function into a runnable.\nconvert any function into a runnable. Runnable means this task.\nRunnable means this task.\nRunnable means this task. Wow. Yeah. You can even convert this\nWow. Yeah. You can even convert this\nWow. Yeah. You can even convert this thing into a runnable lambda.\nthing into a runnable lambda.\nthing into a runnable lambda. Okay. And do you know what? You can even\nOkay. And do you know what? You can even\nOkay. And do you know what? You can even convert this whole chain as the lambda.\nconvert this whole chain as the lambda.\nconvert this whole chain as the lambda. I will just talk about it. Don't worry.\nI will just talk about it. Don't worry.\nI will just talk about it. Don't worry. Well, but but first of all, let's cover\nWell, but but first of all, let's cover\nWell, but but first of all, let's cover this custom functions. And I hope you\nthis custom functions. And I hope you\nthis custom functions. And I hope you got the requirement, right? So, let me\ngot the requirement, right? So, let me\ngot the requirement, right? So, let me just write it here. Now, we need to pass\njust write it here. Now, we need to pass\njust write it here. Now, we need to pass the input\nthe input\nthe input and input is here.\nand input is here.\nand input is here. here input is\nhere input is\nhere input is let's say here\nperfect input is here right now this\nperfect input is here right now this input will send this message to uh to\ninput will send this message to uh to\ninput will send this message to uh to the template and then to the lm\nthe template and then to the lm\nthe template and then to the lm right and then at the end we will use\nright and then at the end we will use\nright and then at the end we will use the parser and this parser will send\nthe parser and this parser will send\nthe parser and this parser will send back the result to the user make sense\nback the result to the user make sense\nback the result to the user make sense this is our workflow that we want to\nthis is our workflow that we want to\nthis is our workflow that we want to create basically our chain.\ncreate basically our chain.\ncreate basically our chain. It's a simple one. Don't worry. It looks\nIt's a simple one. Don't worry. It looks\nIt's a simple one. Don't worry. It looks a little bit like tricky but it's very\na little bit like tricky but it's very\na little bit like tricky but it's very simple.\nsimple.\nsimple. Make sense? Okay. Let's try to build\nMake sense? Okay. Let's try to build\nMake sense? Okay. Let's try to build this and you will learn like how can we\nthis and you will learn like how can we\nthis and you will learn like how can we just create this kind of task. Let me\njust create this kind of task. Let me\njust create this kind of task. Let me just show you. So let's try to create\njust show you. So let's try to create\njust show you. So let's try to create this particular\nthis particular\nthis particular pipeline chain whatever you want to say.\npipeline chain whatever you want to say.\npipeline chain whatever you want to say. And I have just mentioned this quick\nAnd I have just mentioned this quick\nAnd I have just mentioned this quick note so that you will understand like\nnote so that you will understand like\nnote so that you will understand like why are we creating this custom task and\nwhy are we creating this custom task and\nwhy are we creating this custom task and what's the need of that custom task.\nwhat's the need of that custom task.\nwhat's the need of that custom task. Make sense? Okay. Perfect. So now in\nMake sense? Okay. Perfect. So now in\nMake sense? Okay. Perfect. So now in order to do that I will simply create a\norder to do that I will simply create a\norder to do that I will simply create a new notebook and I will say\nnew notebook and I will say\nnew notebook and I will say um chain with\num chain with\num chain with custom runnable\ndot ipy and b. Okay perfect. So now I'll\ndot ipy and b. Okay perfect. So now I'll simply say\nsimply say\nsimply say let's try to import it. What do we need\nlet's try to import it. What do we need\nlet's try to import it. What do we need to put in the heading? It's fine.\nto put in the heading? It's fine.\nto put in the heading? It's fine. Perfect.\nPerfect.\nPerfect. So let me just run it\nand you got the requirement right. We\nand you got the requirement right. We need to just create that particular\nneed to just create that particular\nneed to just create that particular chain. So I will simply\nchain. So I will simply\nchain. So I will simply copy all the tasks of the chain first of\ncopy all the tasks of the chain first of\ncopy all the tasks of the chain first of all and I will say\nwith custom runnable.\nwith custom runnable. Okay. So here we have chain with custom\nOkay. So here we have chain with custom\nOkay. So here we have chain with custom runnable and this is our task number\nrunnable and this is our task number\nrunnable and this is our task number one.\none.\none. And what is this list? Okay,\nAnd what is this list? Okay,\nAnd what is this list? Okay, perfect. And this is our task number\nperfect. And this is our task number\nperfect. And this is our task number two.\nPerfect.\nPerfect. And\nAnd\nAnd perfect. Now we do not need to invoke it\nperfect. Now we do not need to invoke it\nperfect. Now we do not need to invoke it because we want to create more tasks. So\nbecause we want to create more tasks. So\nbecause we want to create more tasks. So here I will write task number four\nhere I will write task number four\nhere I will write task number four which will be my custom task. custom\nwhich will be my custom task. custom\nwhich will be my custom task. custom function or basically custom runnable.\nfunction or basically custom runnable.\nfunction or basically custom runnable. Runnable is just a fancy name. Runnable\nRunnable is just a fancy name. Runnable\nRunnable is just a fancy name. Runnable is something like which you can run in\nis something like which you can run in\nis something like which you can run in the sequence. Okay. So I'll simply say\nthe sequence. Okay. So I'll simply say\nthe sequence. Okay. So I'll simply say df my custom function that I write. I\ndf my custom function that I write. I\ndf my custom function that I write. I will simply say um dictionary maker.\nwill simply say um dictionary maker.\nwill simply say um dictionary maker. Perfect. Dictionary maker. Now input can\nPerfect. Dictionary maker. Now input can\nPerfect. Dictionary maker. Now input can be text of string format and it will\nbe text of string format and it will\nbe text of string format and it will generate dictionary.\ngenerate dictionary.\ngenerate dictionary. Okay. Now whatever I want, whatever I\nOkay. Now whatever I want, whatever I\nOkay. Now whatever I want, whatever I want, I simply want to return it like\nwant, I simply want to return it like\nwant, I simply want to return it like this. Return dictionary text key. Okay.\nthis. Return dictionary text key. Okay.\nthis. Return dictionary text key. Okay. And this can be customized. This will be\nAnd this can be customized. This will be\nAnd this can be customized. This will be as per your requirements. This will be\nas per your requirements. This will be\nas per your requirements. This will be as per your this requirement. Like what\nas per your this requirement. Like what\nas per your this requirement. Like what kind of key are you expecting in the\nkind of key are you expecting in the\nkind of key are you expecting in the template, right? We are simply um\ntemplate, right? We are simply um\ntemplate, right? We are simply um expecting the text key. That is why I am\nexpecting the text key. That is why I am\nexpecting the text key. That is why I am using text. If you want anything else,\nusing text. If you want anything else,\nusing text. If you want anything else, you can just customize it. I will simply\nyou can just customize it. I will simply\nyou can just customize it. I will simply say text. Simple. This is my dictionary.\nsay text. Simple. This is my dictionary.\nsay text. Simple. This is my dictionary. Perfect. Now in order to create this\nPerfect. Now in order to create this\nPerfect. Now in order to create this like or convert this into dictionary I\nlike or convert this into dictionary I\nlike or convert this into dictionary I will import something. I will say from\nwill import something. I will say from\nwill import something. I will say from lang chain\nlang chain\nlang chain core.trunnables import runnable lambda.\ncore.trunnables import runnable lambda.\ncore.trunnables import runnable lambda. Perfect. So now this is not converted\nPerfect. So now this is not converted\nPerfect. So now this is not converted into a runnable. Now this is just a\ninto a runnable. Now this is just a\ninto a runnable. Now this is just a function. If you want to run it in a\nfunction. If you want to run it in a\nfunction. If you want to run it in a sequence we need to convert it into a\nsequence we need to convert it into a\nsequence we need to convert it into a runnable lambda. So I'll simply say\nrunnable lambda. So I'll simply say\nrunnable lambda. So I'll simply say dictionary maker\ndictionary\ndictionary maker runnable\nmaker runnable\nmaker runnable equals runnable lambda and then I will\nequals runnable lambda and then I will\nequals runnable lambda and then I will pass my function. Now I can use this\npass my function. Now I can use this\npass my function. Now I can use this object to create a chain. Are you\nobject to create a chain. Are you\nobject to create a chain. Are you getting my point? Because we cannot\ngetting my point? Because we cannot\ngetting my point? Because we cannot directly use a Python function in the\ndirectly use a Python function in the\ndirectly use a Python function in the lang chain. Basically chain we need to\nlang chain. Basically chain we need to\nlang chain. Basically chain we need to convert it or basically apply a wrapper\nconvert it or basically apply a wrapper\nconvert it or basically apply a wrapper on top of our function. So that chain in\non top of our function. So that chain in\non top of our function. So that chain in lang chain will treat our function as a\nlang chain will treat our function as a\nlang chain will treat our function as a runnable. Simple make sense. Okay. This\nrunnable. Simple make sense. Okay. This\nrunnable. Simple make sense. Okay. This is our task number four. Let me just run\nis our task number four. Let me just run\nis our task number four. Let me just run this. Perfect. Now the task number five\nthis. Perfect. Now the task number five\nthis. Perfect. Now the task number five is very simple, very similar basically\nis very simple, very similar basically\nis very simple, very similar basically and simple obviously because you are on\nand simple obviously because you are on\nand simple obviously because you are on this channel. So this is like template\nthis channel. So this is like template\nthis channel. So this is like template for post that I want to post, right? So\nfor post that I want to post, right? So\nfor post that I want to post, right? So I'll simply say\nI'll simply say\nI'll simply say um prompt\num prompt\num prompt or you can say post equals prompt\nor you can say post equals prompt\nor you can say post equals prompt template dot chat prompt template\nand here I will say un message um\nand here I will say un message um helpful assistant okay or I can also say\nhelpful assistant okay or I can also say\nhelpful assistant okay or I can also say you are a\nsocial media post generator simple\nsocial media post generator simple okay and text.\nokay and text.\nokay and text. Create a post for the following text\nCreate a post for the following text\nCreate a post for the following text following text for your favorite social\nfollowing text for your favorite social\nfollowing text for your favorite social platform. Let's say for LinkedIn, right?\nplatform. Let's say for LinkedIn, right?\nplatform. Let's say for LinkedIn, right? So, system and human message. Make\nSo, system and human message. Make\nSo, system and human message. Make sense? Now, this is my uh prompt post.\nsense? Now, this is my uh prompt post.\nsense? Now, this is my uh prompt post. So, now if I want to invoke it, let's\nSo, now if I want to invoke it, let's\nSo, now if I want to invoke it, let's say I want to invoke it. How will how\nsay I want to invoke it. How will how\nsay I want to invoke it. How will how will I invoke it? Prom template. Let me\nwill I invoke it? Prom template. Let me\nwill I invoke it? Prom template. Let me just import it if it is not imported.\njust import it if it is not imported.\njust import it if it is not imported. Uh where is that prompt template?\nUh where is that prompt template?\nUh where is that prompt template? I think we have here prompt template.\nI think we have here prompt template.\nI think we have here prompt template. Oh, I see what is the issue. We using\nOh, I see what is the issue. We using\nOh, I see what is the issue. We using prompt template. We need to use like\nprompt template. We need to use like\nprompt template. We need to use like this\nthis\nthis chat prompt template dot from messages.\nPerfect. So now let's say this is my\nPerfect. So now let's say this is my prompt. If I want to invoke it,\nprompt. If I want to invoke it,\nprompt. If I want to invoke it, obviously I'll be writing like this. Um\nobviously I'll be writing like this. Um\nobviously I'll be writing like this. Um let's say prompt post\nthis auto suggest some sometimes\nthis auto suggest some sometimes irritates\nirritates\nirritates prompt post\ndot invoke and then I will pass a\ndot invoke and then I will pass a dictionary right dictionary will be\ndictionary right dictionary will be\ndictionary right dictionary will be looking like this text and let's say\nhello world simple so I can only invoke\nhello world simple so I can only invoke it with this dictionary If I try\nit with this dictionary If I try\nit with this dictionary If I try invoking it like this without this\ninvoking it like this without this\ninvoking it like this without this dictionary and if I simply pass this\ndictionary and if I simply pass this\ndictionary and if I simply pass this text, this will work. But we need to\ntext, this will work. But we need to\ntext, this will work. But we need to just make sure that text is replaceable\njust make sure that text is replaceable\njust make sure that text is replaceable here because if it it works because we\nhere because if it it works because we\nhere because if it it works because we just have one variable here. If we have\njust have one variable here. If we have\njust have one variable here. If we have multiple variable, how it will know like\nmultiple variable, how it will know like\nmultiple variable, how it will know like which variable will take which value. So\nwhich variable will take which value. So\nwhich variable will take which value. So in that particular scenario, we have to\nin that particular scenario, we have to\nin that particular scenario, we have to mention this dictionary otherwise it\nmention this dictionary otherwise it\nmention this dictionary otherwise it doesn't work. So that's why I'm creating\ndoesn't work. So that's why I'm creating\ndoesn't work. So that's why I'm creating that particular function called\nthat particular function called\nthat particular function called dictionary maker. And then once we have\ndictionary maker. And then once we have\ndictionary maker. And then once we have this prompt template ready, we can\nthis prompt template ready, we can\nthis prompt template ready, we can simply create another task which will be\nsimply create another task which will be\nsimply create another task which will be lm\ngenerate the post and this will be my lm\ngenerate the post and this will be my lm lm open as we all know right this one\nlm open as we all know right this one\nlm open as we all know right this one and I want to use the same model.\nand I want to use the same model.\nand I want to use the same model. Perfect.\nPerfect. This my task six. I can even um\nPerfect. This my task six. I can even um ignore this writing because we can just\nignore this writing because we can just\nignore this writing because we can just use the same object right lm open but\nuse the same object right lm open but\nuse the same object right lm open but I'm just simply writing it again so that\nI'm just simply writing it again so that\nI'm just simply writing it again so that you can understand it otherwise like see\nyou can understand it otherwise like see\nyou can understand it otherwise like see it is the same thing why I'm writing\nit is the same thing why I'm writing\nit is the same thing why I'm writing this here so that you can actually\nthis here so that you can actually\nthis here so that you can actually understand and relate it relate to it\nunderstand and relate it relate to it\nunderstand and relate it relate to it more so\nmore so\nmore so and our next or basically last task is\nand our next or basically last task is\nand our next or basically last task is task seven\noutput parser as you know string parser\noutput parser as you know string parser And we also do not need to define it\nAnd we also do not need to define it\nAnd we also do not need to define it again because we have already defined\nagain because we have already defined\nagain because we have already defined it. But I'm again defining it for you so\nit. But I'm again defining it for you so\nit. But I'm again defining it for you so that you can understand it. Perfect. So\nthat you can understand it. Perfect. So\nthat you can understand it. Perfect. So let me just run all the things now. And\nlet me just run all the things now. And\nlet me just run all the things now. And now let's create the chain. And our\nnow let's create the chain. And our\nnow let's create the chain. And our chain is here.\nSo now how we create the chain? Now you\nSo now how we create the chain? Now you already know LCEL lang chain expression\nalready know LCEL lang chain expression\nalready know LCEL lang chain expression language, right? So I'll simply say\nlanguage, right? So I'll simply say\nlanguage, right? So I'll simply say chain equals and all the tasks in the uh\nchain equals and all the tasks in the uh\nchain equals and all the tasks in the uh you can say sequence. So first task is\nyou can say sequence. So first task is\nyou can say sequence. So first task is prompt template\nand then we have llm open air.\nand then we have llm open air. Then we have\nThen we have\nThen we have string parser.\nThen we have custom runnable which is\nThen we have custom runnable which is dictionary maker and runnable.\ndictionary maker and runnable.\ndictionary maker and runnable. dictionary maker runnable.\ndictionary maker runnable.\ndictionary maker runnable. Oh man, this\nOh man, this\nOh man, this dictionary maker runnable which is the\ndictionary maker runnable which is the\ndictionary maker runnable which is the runnable instance of our function. Then\nrunnable instance of our function. Then\nrunnable instance of our function. Then we have again prompt post, right?\nwe have again prompt post, right?\nwe have again prompt post, right? Prompt post.\nPrompt post.\nPrompt post. Then we have\nThen we have\nThen we have lm open again because this prompt will\nlm open again because this prompt will\nlm open again because this prompt will go to llm openi\ngo to llm openi\ngo to llm openi and then string parser. Make sense? So\nand then string parser. Make sense? So\nand then string parser. Make sense? So this is our chain. Now let me just hit\nthis is our chain. Now let me just hit\nthis is our chain. Now let me just hit this what is the capital of France and\nthis what is the capital of France and\nthis what is the capital of France and let's see what is the output. The output\nlet's see what is the output. The output\nlet's see what is the output. The output should be a post that we can just post\nshould be a post that we can just post\nshould be a post that we can just post it on LinkedIn right and this whole\nit on LinkedIn right and this whole\nit on LinkedIn right and this whole chain is now running. See quickly\nchain is now running. See quickly\nchain is now running. See quickly geography refresher the capital of\ngeography refresher the capital of\ngeography refresher the capital of France is Paris and it's more than\nFrance is Paris and it's more than\nFrance is Paris and it's more than landmark field city. So this has created\nlandmark field city. So this has created\nlandmark field city. So this has created the entire post for me and it has done\nthe entire post for me and it has done\nthe entire post for me and it has done everything. So what it has done the same\neverything. So what it has done the same\neverything. So what it has done the same thing that I have just demonstrated\nthing that I have just demonstrated\nthing that I have just demonstrated here. So it will simply create this\nhere. So it will simply create this\nhere. So it will simply create this thing like let's say fun fact about\nthing like let's say fun fact about\nthing like let's say fun fact about honey or um it is just creating the\nhoney or um it is just creating the\nhoney or um it is just creating the capital of France. It will go to the\ncapital of France. It will go to the\ncapital of France. It will go to the input for this custom function. So what\ninput for this custom function. So what\ninput for this custom function. So what it will do it will create a kind of\nit will do it will create a kind of\nit will do it will create a kind of dictionary\ndictionary\ndictionary simple and then this dictionary will\nsimple and then this dictionary will\nsimple and then this dictionary will take this output and it will send to the\ntake this output and it will send to the\ntake this output and it will send to the template and the template will fill the\ntemplate and the template will fill the\ntemplate and the template will fill the variables simple. Then it will go to lm\nvariables simple. Then it will go to lm\nvariables simple. Then it will go to lm then it will go to parser and then it\nthen it will go to parser and then it\nthen it will go to parser and then it will go to us.\nwill go to us.\nwill go to us. This is the chain that we have just\nThis is the chain that we have just\nThis is the chain that we have just built and that's how you can also build\nbuilt and that's how you can also build\nbuilt and that's how you can also build your chains.\nyour chains.\nyour chains. Make sense? Okay. Very good.\nMake sense? Okay. Very good.\nMake sense? Okay. Very good. Now let's talk about this important\nNow let's talk about this important\nNow let's talk about this important concept called parallel chains. Because\nconcept called parallel chains. Because\nconcept called parallel chains. Because see we already know the concept. We\nsee we already know the concept. We\nsee we already know the concept. We already know the fundamental thing right\nalready know the fundamental thing right\nalready know the fundamental thing right linear chains as we have just discussed\nlinear chains as we have just discussed\nlinear chains as we have just discussed this like how we can just create the\nthis like how we can just create the\nthis like how we can just create the linear flow. But obviously in your\nlinear flow. But obviously in your\nlinear flow. But obviously in your complex workflow as you're building some\ncomplex workflow as you're building some\ncomplex workflow as you're building some more complex and more advanced level um\nmore complex and more advanced level um\nmore complex and more advanced level um you can say uh flows or chains you will\nyou can say uh flows or chains you will\nyou can say uh flows or chains you will need this thing which is called parallel\nneed this thing which is called parallel\nneed this thing which is called parallel chains. What do I mean? So let's say\nchains. What do I mean? So let's say\nchains. What do I mean? So let's say where is that Rahul? Okay. So let's say\nwhere is that Rahul? Okay. So let's say\nwhere is that Rahul? Okay. So let's say this is a developer. Okay this Rahul.\nthis is a developer. Okay this Rahul.\nthis is a developer. Okay this Rahul. Yeah same person. This developer now\nYeah same person. This developer now\nYeah same person. This developer now needs to create a flow which looks like\nneeds to create a flow which looks like\nneeds to create a flow which looks like this. Let's say it has a kind of\nthis. Let's say it has a kind of\nthis. Let's say it has a kind of template\ntemplate\ntemplate okay which is a starting point which is\nokay which is a starting point which is\nokay which is a starting point which is starting point for every chain I would\nstarting point for every chain I would\nstarting point for every chain I would say every chain and after this it has\nsay every chain and after this it has\nsay every chain and after this it has LLM for sure\nLLM for sure\nLLM for sure perfect now it has maybe parser like\nperfect now it has maybe parser like\nperfect now it has maybe parser like parser is like optional it's not a very\nparser is like optional it's not a very\nparser is like optional it's not a very big thing so let's say it has like two\nbig thing so let's say it has like two\nbig thing so let's say it has like two nodes right two tasks but now the thing\nnodes right two tasks but now the thing\nnodes right two tasks but now the thing is now going to be interesting okay how\nis now going to be interesting okay how\nis now going to be interesting okay how so now we do not have like linear step\nso now we do not have like linear step\nso now we do not have like linear step we have parallel step what do I mean so\nwe have parallel step what do I mean so\nwe have parallel step what do I mean so let's say now we need to\nlet's say now we need to\nlet's say now we need to we need to\nwe need to\nwe need to create this particular\ncreate this particular\ncreate this particular task this can be anything just to take\ntask this can be anything just to take\ntask this can be anything just to take an example let's say um you want to\nan example let's say um you want to\nan example let's say um you want to analyze the summary of your data that\nanalyze the summary of your data that\nanalyze the summary of your data that you're processing and it can be anything\nyou're processing and it can be anything\nyou're processing and it can be anything let's say you are getting the CSV file\nlet's say you are getting the CSV file\nlet's say you are getting the CSV file and you are just processing the data.\nand you are just processing the data.\nand you are just processing the data. You are summarizing data and that's it.\nYou are summarizing data and that's it.\nYou are summarizing data and that's it. Cool. Okay. Now on the basis of that you\nCool. Okay. Now on the basis of that you\nCool. Okay. Now on the basis of that you need to do two tasks. One you need to\nneed to do two tasks. One you need to\nneed to do two tasks. One you need to draft an email or basically send an\ndraft an email or basically send an\ndraft an email or basically send an email to your manager regarding the\nemail to your manager regarding the\nemail to your manager regarding the summarization that you have received\nsummarization that you have received\nsummarization that you have received because that is a kind of financial\nbecause that is a kind of financial\nbecause that is a kind of financial report, right? You need to do that\nreport, right? You need to do that\nreport, right? You need to do that thing. You need to send an email to your\nthing. You need to send an email to your\nthing. You need to send an email to your manager. And second thing, you also need\nmanager. And second thing, you also need\nmanager. And second thing, you also need to draft that particular thing in your\nto draft that particular thing in your\nto draft that particular thing in your documentation folder as well. or let's\ndocumentation folder as well. or let's\ndocumentation folder as well. or let's say you want to send a message on teams\nsay you want to send a message on teams\nsay you want to send a message on teams to your group chat make sense so in\nto your group chat make sense so in\nto your group chat make sense so in short I want to perform two parallel\nshort I want to perform two parallel\nshort I want to perform two parallel things okay and whenever we just perform\nthings okay and whenever we just perform\nthings okay and whenever we just perform anything in land chain we have series of\nanything in land chain we have series of\nanything in land chain we have series of steps right so\nsteps right so\nsteps right so that being said this will be a kind of\nthat being said this will be a kind of\nthat being said this will be a kind of chain in itself right\nchain in itself right\nchain in itself right let's say this is chain number one\nlet's say this is chain number one\nlet's say this is chain number one parallel chain and just for the\nparallel chain and just for the\nparallel chain and just for the simplicity let me just add an arrow So\nsimplicity let me just add an arrow So\nsimplicity let me just add an arrow So let's say this is a task.\nlet's say this is a task.\nlet's say this is a task. This is the task. Okay, perfect. So this\nThis is the task. Okay, perfect. So this\nThis is the task. Okay, perfect. So this is like chain tasks and this is\nis like chain tasks and this is\nis like chain tasks and this is obviously the initial chain that was\nobviously the initial chain that was\nobviously the initial chain that was started.\nstarted.\nstarted. Perfect. So now we have this chain\nPerfect. So now we have this chain\nPerfect. So now we have this chain number one. Perfect. And I can just\nnumber one. Perfect. And I can just\nnumber one. Perfect. And I can just create a box. Perfect. So this is my\ncreate a box. Perfect. So this is my\ncreate a box. Perfect. So this is my chain number one. And similarly I have\nchain number one. And similarly I have\nchain number one. And similarly I have one more chain for the teams messages as\none more chain for the teams messages as\none more chain for the teams messages as well. One is for um email, one is for\nwell. One is for um email, one is for\nwell. One is for um email, one is for teams message or you can say one is for\nteams message or you can say one is for\nteams message or you can say one is for email and second one is for personal\nemail and second one is for personal\nemail and second one is for personal use. You want to draft it, you want to\nuse. You want to draft it, you want to\nuse. You want to draft it, you want to save it or you want to send it to let's\nsave it or you want to send it to let's\nsave it or you want to send it to let's say your director or whatever. Basically\nsay your director or whatever. Basically\nsay your director or whatever. Basically you want to perform parallel task. So\nyou want to perform parallel task. So\nyou want to perform parallel task. So what will happen here? You will be\nwhat will happen here? You will be\nwhat will happen here? You will be creating these two parallel chains.\ncreating these two parallel chains.\ncreating these two parallel chains. Parallel chains. Wow. So this will be my\nParallel chains. Wow. So this will be my\nParallel chains. Wow. So this will be my chain number one.\nPerfect.\nPerfect. Now you will say anala\nNow you will say anala\nNow you will say anala is it a difficult thing to do? Please\nis it a difficult thing to do? Please\nis it a difficult thing to do? Please tell us before. Okay. What if I say it\ntell us before. Okay. What if I say it\ntell us before. Okay. What if I say it is a difficult thing? What will you do?\nis a difficult thing? What will you do?\nis a difficult thing? What will you do? Nothing. We will be just prepared. Okay.\nNothing. We will be just prepared. Okay.\nNothing. We will be just prepared. Okay. So the thing is it is very easy. You\nSo the thing is it is very easy. You\nSo the thing is it is very easy. You first need to understand the concept.\nfirst need to understand the concept.\nfirst need to understand the concept. You have understood the concept. Now it\nYou have understood the concept. Now it\nYou have understood the concept. Now it is easy. Concept is everything. Concept\nis easy. Concept is everything. Concept\nis easy. Concept is everything. Concept is everything. Perfect. So now let's try\nis everything. Perfect. So now let's try\nis everything. Perfect. So now let's try to see how we can just build this thing.\nto see how we can just build this thing.\nto see how we can just build this thing. And just to demonstrate this, we will\nAnd just to demonstrate this, we will\nAnd just to demonstrate this, we will use an example.\nuse an example.\nuse an example. And\nAnd\nAnd yeah,\nyeah,\nyeah, let's quickly create a clone of this.\nlet's quickly create a clone of this.\nlet's quickly create a clone of this. And let me just say\nAnd let me just say\nAnd let me just say three and\nthree and\nthree and parallel chains. Perfect. So this cell\nparallel chains. Perfect. So this cell\nparallel chains. Perfect. So this cell will remain the same. Let me just run\nwill remain the same. Let me just run\nwill remain the same. Let me just run this with this virtual environment.\nPerfect chain with custom runnable. So\nPerfect chain with custom runnable. So as you know that this is our first task\nas you know that this is our first task\nas you know that this is our first task if you look at the this thing template.\nif you look at the this thing template.\nif you look at the this thing template. So now you will say obviously we do not\nSo now you will say obviously we do not\nSo now you will say obviously we do not have any kind of data for now. We will\nhave any kind of data for now. We will\nhave any kind of data for now. We will just discuss those examples as well. But\njust discuss those examples as well. But\njust discuss those examples as well. But for for now let's say\nfor for now let's say\nfor for now let's say I want to just do this thing right. So\nI want to just do this thing right. So\nI want to just do this thing right. So first of all just for the understanding\nfirst of all just for the understanding\nfirst of all just for the understanding we will just use a hypothetical\nwe will just use a hypothetical\nwe will just use a hypothetical situation hypothetical example then we\nsituation hypothetical example then we\nsituation hypothetical example then we will just also looking at some real\nwill just also looking at some real\nwill just also looking at some real examples as well using data engineering\nexamples as well using data engineering\nexamples as well using data engineering but first I want you to understand the\nbut first I want you to understand the\nbut first I want you to understand the thing okay so that is why I'm not\nthing okay so that is why I'm not\nthing okay so that is why I'm not directly pulling the real world example\ndirectly pulling the real world example\ndirectly pulling the real world example you will be like totally off first let's\nyou will be like totally off first let's\nyou will be like totally off first let's start slow right and then we're going to\nstart slow right and then we're going to\nstart slow right and then we're going to just grow okay so now the thing is the\njust grow okay so now the thing is the\njust grow okay so now the thing is the requirement is very simple this template\nrequirement is very simple this template\nrequirement is very simple this template this template what it will do it will\nthis template what it will do it will\nthis template what it will do it will simply\nsimply\nsimply Ask LLM to summarize a movie or just ask\nAsk LLM to summarize a movie or just ask\nAsk LLM to summarize a movie or just ask LLM to write a quick summary about a\nLLM to write a quick summary about a\nLLM to write a quick summary about a movie. Right? So you are a\nmovie. Right? So you are a\nmovie. Right? So you are a movie summarizer.\nmovie summarizer.\nmovie summarizer. Okay? And here\nOkay? And here\nOkay? And here I will simply provide the input as\nI will simply provide the input as\nI will simply provide the input as please\nplease\nplease summarize the movie in brief because we\nsummarize the movie in brief because we\nsummarize the movie in brief because we do not want to spend much tokens and\ndo not want to spend much tokens and\ndo not want to spend much tokens and This is the input or let's say movie\nThis is the input or let's say movie\nThis is the input or let's say movie name whatever you want to do. Okay,\nname whatever you want to do. Okay,\nname whatever you want to do. Okay, input. So this is my prompt. Okay, this\ninput. So this is my prompt. Okay, this\ninput. So this is my prompt. Okay, this is task number one. Perfect. Let me run\nis task number one. Perfect. Let me run\nis task number one. Perfect. Let me run this. This is my task number two.\nthis. This is my task number two.\nthis. This is my task number two. Perfect. And this is my task number\nPerfect. And this is my task number\nPerfect. And this is my task number three which is like string output parser\nthree which is like string output parser\nthree which is like string output parser which is like very common. Perfect. Now\nwhich is like very common. Perfect. Now\nwhich is like very common. Perfect. Now here comes the thing where we need to\nhere comes the thing where we need to\nhere comes the thing where we need to create the parallel chain. So let me\ncreate the parallel chain. So let me\ncreate the parallel chain. So let me just remove all of these cells because\njust remove all of these cells because\njust remove all of these cells because these are not required at all.\nthese are not required at all.\nthese are not required at all. Perfect. So now\nPerfect. So now\nPerfect. So now now there are two options. Okay. Now\nnow there are two options. Okay. Now\nnow there are two options. Okay. Now there are two options. I will just show\nthere are two options. I will just show\nthere are two options. I will just show you both the options because it's my\nyou both the options because it's my\nyou both the options because it's my responsibility to give you to give you\nresponsibility to give you to give you\nresponsibility to give you to give you all of the knowledge.\nall of the knowledge.\nall of the knowledge. Option number one is\nOption number one is\nOption number one is to create a dedicated chain for this\nto create a dedicated chain for this\nto create a dedicated chain for this task. Dedicated chain means like\ntask. Dedicated chain means like\ntask. Dedicated chain means like dedicated chain like it will run 1 2 3\ndedicated chain like it will run 1 2 3\ndedicated chain like it will run 1 2 3 and all.\nand all.\nand all. Okay. like it will have it its own tasks\nOkay. like it will have it its own tasks\nOkay. like it will have it its own tasks and it will do everything on its own\nand it will do everything on its own\nand it will do everything on its own creating the chain dedicated chain makes\ncreating the chain dedicated chain makes\ncreating the chain dedicated chain makes sense. It is very handy not a big deal.\nsense. It is very handy not a big deal.\nsense. It is very handy not a big deal. The second option is very simple and it\nThe second option is very simple and it\nThe second option is very simple and it is much more scalable and it is easy to\nis much more scalable and it is easy to\nis much more scalable and it is easy to manage as well. What is that option?\nmanage as well. What is that option?\nmanage as well. What is that option? See if we have let's say just a few\nSee if we have let's say just a few\nSee if we have let's say just a few tasks okay it is easy if we just create\ntasks okay it is easy if we just create\ntasks okay it is easy if we just create a dedicated chain it's fine but if we\na dedicated chain it's fine but if we\na dedicated chain it's fine but if we have like multiple tasks we can even\nhave like multiple tasks we can even\nhave like multiple tasks we can even create a function\ncreate a function\ncreate a function okay remember I told you that we can\nokay remember I told you that we can\nokay remember I told you that we can also create a function for the entire\nalso create a function for the entire\nalso create a function for the entire chain as well right so we can even\nchain as well right so we can even\nchain as well right so we can even create the function for the entire chain\ncreate the function for the entire chain\ncreate the function for the entire chain okay so what I will do so that you can\nokay so what I will do so that you can\nokay so what I will do so that you can learn both the ways for chain number one\nlearn both the ways for chain number one\nlearn both the ways for chain number one I will use the normal chain method I\nI will use the normal chain method I\nI will use the normal chain method I will create all the tasks for the chain\nwill create all the tasks for the chain\nwill create all the tasks for the chain for chain number two I will not create\nfor chain number two I will not create\nfor chain number two I will not create the chain I will only create one\nthe chain I will only create one\nthe chain I will only create one function and I will hard not hard code\nfunction and I will hard not hard code\nfunction and I will hard not hard code basically I will just write the code for\nbasically I will just write the code for\nbasically I will just write the code for every task okay that way you will learn\nevery task okay that way you will learn\nevery task okay that way you will learn both the ways simple simple okay let's\nboth the ways simple simple okay let's\nboth the ways simple simple okay let's do it so first of all let me just use\ndo it so first of all let me just use\ndo it so first of all let me just use the method number one so I will say\nthe method number one so I will say\nthe method number one so I will say uh markdown\nPerfect\nPerfect chain with\nparallel\nparallel chains.\nchains.\nchains. Perfect. So, parallel chain one. What\nPerfect. So, parallel chain one. What\nPerfect. So, parallel chain one. What will be the uh you can say step number\nwill be the uh you can say step number\nwill be the uh you can say step number one? Basically, task number one. So I'll\none? Basically, task number one. So I'll\none? Basically, task number one. So I'll simply say\nsimply say\nsimply say task one which is obviously prompt\ntask one which is obviously prompt\ntask one which is obviously prompt right\nright\nright which is obviously prompt and I will say\nwhich is obviously prompt and I will say\nwhich is obviously prompt and I will say and what what type of prompt is this? So\nand what what type of prompt is this? So\nand what what type of prompt is this? So let's say we have the summary of the\nlet's say we have the summary of the\nlet's say we have the summary of the movie. What we going to do after that?\nmovie. What we going to do after that?\nmovie. What we going to do after that? Once we have the summary of the movie,\nOnce we have the summary of the movie,\nOnce we have the summary of the movie, we will ask LLM to create basically a\nwe will ask LLM to create basically a\nwe will ask LLM to create basically a LinkedIn post, right? And in the second\nLinkedIn post, right? And in the second\nLinkedIn post, right? And in the second chain, we will simply say create an\nchain, we will simply say create an\nchain, we will simply say create an Instagram post because both the\nInstagram post because both the\nInstagram post because both the platforms are different. Um the tone for\nplatforms are different. Um the tone for\nplatforms are different. Um the tone for both the platforms are different. So\nboth the platforms are different. So\nboth the platforms are different. So let's try to do this. Okay. And in the\nlet's try to do this. Okay. And in the\nlet's try to do this. Okay. And in the real world, what we what we will be\nreal world, what we what we will be\nreal world, what we what we will be doing? Sending an email to manager and\ndoing? Sending an email to manager and\ndoing? Sending an email to manager and sending a team's message to a director\nsending a team's message to a director\nsending a team's message to a director or whatever. Okay. Okay. But for the\nor whatever. Okay. Okay. But for the\nor whatever. Okay. Okay. But for the simplicity, I'm just taking an example\nsimplicity, I'm just taking an example\nsimplicity, I'm just taking an example like hypothetical example so that we can\nlike hypothetical example so that we can\nlike hypothetical example so that we can quickly learn the thing that we're\nquickly learn the thing that we're\nquickly learn the thing that we're trying to do because once you know how\ntrying to do because once you know how\ntrying to do because once you know how to do it, you can just implement that\nto do it, you can just implement that\nto do it, you can just implement that solution anywhere. Right. So let's say I\nsolution anywhere. Right. So let's say I\nsolution anywhere. Right. So let's say I will write prompt. Let's say LinkedIn\nwill write prompt. Let's say LinkedIn\nwill write prompt. Let's say LinkedIn prompt.\nprompt.\nprompt. Perfect. You are a helpful assistant.\nPerfect. You are a helpful assistant.\nPerfect. You are a helpful assistant. Not helpful. You are a\nNot helpful. You are a\nNot helpful. You are a movie summarizer or I will say you are a\nmovie summarizer or I will say you are a\nmovie summarizer or I will say you are a post generator. LinkedIn post generator.\npost generator. LinkedIn post generator.\npost generator. LinkedIn post generator. Perfect. And I will write this input.\nPerfect. And I will write this input.\nPerfect. And I will write this input. Create a post for following text for\nCreate a post for following text for\nCreate a post for following text for LinkedIn.\nLinkedIn.\nLinkedIn. Perfect. This is my task number one. Let\nPerfect. This is my task number one. Let\nPerfect. This is my task number one. Let me just create task number two which is\nme just create task number two which is\nme just create task number two which is LLM. And task number three is\nLLM. And task number three is\nLLM. And task number three is this one. Perfect. Make sense? Simple.\nthis one. Perfect. Make sense? Simple.\nthis one. Perfect. Make sense? Simple. This is my chain number one.\nThis is my chain number one.\nThis is my chain number one. Let me just run this. Perfect. Three\nLet me just run this. Perfect. Three\nLet me just run this. Perfect. Three things. Perfect. This is my parallel\nthings. Perfect. This is my parallel\nthings. Perfect. This is my parallel chain number one. Okay. Now if you will\nchain number one. Okay. Now if you will\nchain number one. Okay. Now if you will closely observe\nclosely observe\nclosely observe this is this is and we have just covered\nthis is this is and we have just covered\nthis is this is and we have just covered this thing that's why I covered that\nthis thing that's why I covered that\nthis thing that's why I covered that topic in detail. This is a kind of\ntopic in detail. This is a kind of\ntopic in detail. This is a kind of template right like not template like\ntemplate right like not template like\ntemplate right like not template like this is a kind of\nthis is a kind of\nthis is a kind of input that depends on template right.\ninput that depends on template right.\ninput that depends on template right. What it needs? It needs a dictionary,\nWhat it needs? It needs a dictionary,\nWhat it needs? It needs a dictionary, right? It needs a dictionary. So, what\nright? It needs a dictionary. So, what\nright? It needs a dictionary. So, what we need to do, we need to add one small\nwe need to do, we need to add one small\nwe need to do, we need to add one small step here. Custom um you can say task\nstep here. Custom um you can say task\nstep here. Custom um you can say task which will create a dictionary for us.\nwhich will create a dictionary for us.\nwhich will create a dictionary for us. Make sense? That's why I covered that\nMake sense? That's why I covered that\nMake sense? That's why I covered that topic in detail the very previous one.\ntopic in detail the very previous one.\ntopic in detail the very previous one. So, I will add one task here. I can add\nSo, I will add one task here. I can add\nSo, I will add one task here. I can add it in the pallet chain one, but it will\nit in the pallet chain one, but it will\nit in the pallet chain one, but it will be a redundant task because we would\nbe a redundant task because we would\nbe a redundant task because we would need we would need to do it in both the\nneed we would need to do it in both the\nneed we would need to do it in both the chains. It's better to just create one\nchains. It's better to just create one\nchains. It's better to just create one task before both the chains, right? So\ntask before both the chains, right? So\ntask before both the chains, right? So let me just say and let me just grab\nlet me just say and let me just grab\nlet me just say and let me just grab that function from here.\nthat function from here.\nthat function from here. Dictionary maker um yeah here perfect\nDictionary maker um yeah here perfect\nDictionary maker um yeah here perfect task 4 and let me just write it here.\ntask 4 and let me just write it here.\ntask 4 and let me just write it here. Perfect. Custom runnable dictionary\nPerfect. Custom runnable dictionary\nPerfect. Custom runnable dictionary generator. Perfect. Let me just run this\ngenerator. Perfect. Let me just run this\ngenerator. Perfect. Let me just run this as well. So now we are good. Perfect. So\nas well. So now we are good. Perfect. So\nas well. So now we are good. Perfect. So it will simply generate text and the\nit will simply generate text and the\nit will simply generate text and the other thing. And here we have this\nother thing. And here we have this\nother thing. And here we have this particular chain\nparticular chain\nparticular chain which will\nwhich will\nwhich will which will expect text right and we are\nwhich will expect text right and we are\nwhich will expect text right and we are simply returning the text dictionary. So\nsimply returning the text dictionary. So\nsimply returning the text dictionary. So it needs to be same exactly same. If it\nit needs to be same exactly same. If it\nit needs to be same exactly same. If it is not same it will not run fine because\nis not same it will not run fine because\nis not same it will not run fine because you know the output of one is dependent\nyou know the output of one is dependent\nyou know the output of one is dependent on the previous one right like the\non the previous one right like the\non the previous one right like the output of one will be the input for the\noutput of one will be the input for the\noutput of one will be the input for the next one. So next one is dependent on\nnext one. So next one is dependent on\nnext one. So next one is dependent on the previous output. Okay I hope it is\nthe previous output. Okay I hope it is\nthe previous output. Okay I hope it is clear. Perfect. So now it is done.\nclear. Perfect. So now it is done.\nclear. Perfect. So now it is done. Simple. Now I will be creating parallel\nSimple. Now I will be creating parallel\nSimple. Now I will be creating parallel chain two\nchain two\nchain two and just see how I will create that.\nSo for parallel chain two so if you just\nSo for parallel chain two so if you just look at this parallel chain one I think\nlook at this parallel chain one I think\nlook at this parallel chain one I think it is incomplete right? How just just\nit is incomplete right? How just just\nit is incomplete right? How just just let me know in the comments how because\nlet me know in the comments how because\nlet me know in the comments how because these are just task we have not created\nthese are just task we have not created\nthese are just task we have not created the chain for it. So I will simply say\nthe chain for it. So I will simply say\nthe chain for it. So I will simply say chain LinkedIn\nchain LinkedIn\nchain LinkedIn equals LinkedIn prompt and llm openi and\nequals LinkedIn prompt and llm openi and\nequals LinkedIn prompt and llm openi and string parser. Perfect. So let's run\nstring parser. Perfect. So let's run\nstring parser. Perfect. So let's run this. Perfect. Now it is a chain. Now\nthis. Perfect. Now it is a chain. Now\nthis. Perfect. Now it is a chain. Now instead of creating chain like this,\ninstead of creating chain like this,\ninstead of creating chain like this, what I will do? Do you know I will\nwhat I will do? Do you know I will\nwhat I will do? Do you know I will simply create a function.\nsimply create a function.\nsimply create a function. I will say def um insta chain. Okay. And\nI will say def um insta chain. Okay. And\nI will say def um insta chain. Okay. And here in this function I will write all\nhere in this function I will write all\nhere in this function I will write all of these three things\nlike usually what we do in the function\nlike usually what we do in the function right like normal function.\nright like normal function.\nright like normal function. Exactly. That's exactly you need to do\nExactly. That's exactly you need to do\nExactly. That's exactly you need to do task one prompt. Right.\nPerfect. And here I will simply say\nPerfect. And here I will simply say insta\ninsta\ninsta prompt. And here I will say insta\nprompt. And here I will say insta\nprompt. And here I will say insta Instagram. And here I will say insta\nInstagram. And here I will say insta\nInstagram. And here I will say insta post.\npost.\npost. And here I will say insta. By the way\nAnd here I will say insta. By the way\nAnd here I will say insta. By the way this is not required. So we can just\nthis is not required. So we can just\nthis is not required. So we can just leave it as is. Perfect. So now as you\nleave it as is. Perfect. So now as you\nleave it as is. Perfect. So now as you can see that we can define everything in\ncan see that we can define everything in\ncan see that we can define everything in the function. Wow.\nthe function. Wow.\nthe function. Wow. Literally. Wow. Because whenever we\nLiterally. Wow. Because whenever we\nLiterally. Wow. Because whenever we create function we have more control.\ncreate function we have more control.\ncreate function we have more control. Why? Because I know the output of this\nWhy? Because I know the output of this\nWhy? Because I know the output of this the output of this will go inside this.\nthe output of this will go inside this.\nthe output of this will go inside this. If I need to make any changes I can do\nIf I need to make any changes I can do\nIf I need to make any changes I can do it in the function itself. I don't need\nit in the function itself. I don't need\nit in the function itself. I don't need to depend on like hey this will generate\nto depend on like hey this will generate\nto depend on like hey this will generate the output like this and hey this will\nthe output like this and hey this will\nthe output like this and hey this will generate the output like this. No no I'm\ngenerate the output like this. No no I'm\ngenerate the output like this. No no I'm open to do anything.\nopen to do anything.\nopen to do anything. Make sense? So I will get the text like\nMake sense? So I will get the text like\nMake sense? So I will get the text like this. Um text is of string type and I\nthis. Um text is of string type and I\nthis. Um text is of string type and I think this function will receive a kind\nthink this function will receive a kind\nthink this function will receive a kind of dictionary because we pass a\nof dictionary because we pass a\nof dictionary because we pass a dictionary and this will generate\ndictionary and this will generate\ndictionary and this will generate yeah perfect. So text is of dictionary\nyeah perfect. So text is of dictionary\nyeah perfect. So text is of dictionary type. Perfect. So now I will simply say\ntype. Perfect. So now I will simply say\ntype. Perfect. So now I will simply say text equals text of text right symbol\ntext equals text of text right symbol\ntext equals text of text right symbol dictionary. We are getting this key\ndictionary. We are getting this key\ndictionary. We are getting this key value. And now we are simply passing it\nvalue. And now we are simply passing it\nvalue. And now we are simply passing it here in the text. And now I can invoke\nhere in the text. And now I can invoke\nhere in the text. And now I can invoke it invoke it within the function itself.\nit invoke it within the function itself.\nit invoke it within the function itself. I don't need to go to like wait for it\nI don't need to go to like wait for it\nI don't need to go to like wait for it to execute. So I will simply say um\nto execute. So I will simply say um\nto execute. So I will simply say um insta\ninsta\ninsta um prompt final basically\num prompt final basically\num prompt final basically or like this is one way of doing it or\nor like this is one way of doing it or\nor like this is one way of doing it or you can even call the entire chain in\nyou can even call the entire chain in\nyou can even call the entire chain in the end like at the end like choice is\nthe end like at the end like choice is\nthe end like at the end like choice is yours. Choice is yours. You can even\nyours. Choice is yours. You can even\nyours. Choice is yours. You can even call the entire chain at the end because\ncall the entire chain at the end because\ncall the entire chain at the end because you know like how to do that. So I'll\nyou know like how to do that. So I'll\nyou know like how to do that. So I'll simply say chain insta and this thing\nsimply say chain insta and this thing\nsimply say chain insta and this thing and you will simply say chain do.invoke\nand you will simply say chain do.invoke\nand you will simply say chain do.invoke chain insta dot invoke and it will\nchain insta dot invoke and it will\nchain insta dot invoke and it will simply say text and you will simply\nsimply say text and you will simply\nsimply say text and you will simply return chain install\nreturn chain install\nreturn chain install not this one I will say result\nnot this one I will say result\nnot this one I will say result because you just need the result. So so\nbecause you just need the result. So so\nbecause you just need the result. So so this is another way to doing it. So what\nthis is another way to doing it. So what\nthis is another way to doing it. So what it will do? It will go to this function.\nit will do? It will go to this function.\nit will do? It will go to this function. It will create the chain. Everything\nIt will create the chain. Everything\nIt will create the chain. Everything like everything is exactly same. If you\nlike everything is exactly same. If you\nlike everything is exactly same. If you just compare the code, everything is\njust compare the code, everything is\njust compare the code, everything is same. We are simply creating a function\nsame. We are simply creating a function\nsame. We are simply creating a function on top of it. It is easy to manage\non top of it. It is easy to manage\non top of it. It is easy to manage because let's say you want to import\nbecause let's say you want to import\nbecause let's say you want to import this function as a as a utility, you can\nthis function as a as a utility, you can\nthis function as a as a utility, you can do it.\ndo it.\ndo it. It's just like that. And second benefit\nIt's just like that. And second benefit\nIt's just like that. And second benefit will be you can make the changes in\nwill be you can make the changes in\nwill be you can make the changes in between because if you do not like uh if\nbetween because if you do not like uh if\nbetween because if you do not like uh if you do not write like this you can\nyou do not write like this you can\nyou do not write like this you can simply make some changes here and you\nsimply make some changes here and you\nsimply make some changes here and you are not actually dependent on this\nare not actually dependent on this\nare not actually dependent on this particular thing. Make sense? So let's\nparticular thing. Make sense? So let's\nparticular thing. Make sense? So let's say even if I comment it out you can run\nsay even if I comment it out you can run\nsay even if I comment it out you can run all of these things in the steps right\nall of these things in the steps right\nall of these things in the steps right and you can just make those changes and\nand you can just make those changes and\nand you can just make those changes and if you create functions like this you do\nif you create functions like this you do\nif you create functions like this you do not need those functions like dictionary\nnot need those functions like dictionary\nnot need those functions like dictionary maker and all because you can make the\nmaker and all because you can make the\nmaker and all because you can make the changes in line. Again, it's personal\nchanges in line. Again, it's personal\nchanges in line. Again, it's personal choice, but my agenda is to make you\nchoice, but my agenda is to make you\nchoice, but my agenda is to make you aware of all the possible things. You do\naware of all the possible things. You do\naware of all the possible things. You do not need to stick to everything written\nnot need to stick to everything written\nnot need to stick to everything written in the documentation. You need to be\nin the documentation. You need to be\nin the documentation. You need to be creative as well. You need to know like\ncreative as well. You need to know like\ncreative as well. You need to know like what are the things we can who is this?\nwhat are the things we can who is this?\nwhat are the things we can who is this? We you need to know like how are the\nWe you need to know like how are the\nWe you need to know like how are the things we can do, right?\nthings we can do, right?\nthings we can do, right? Make sense? So now what are we doing? We\nMake sense? So now what are we doing? We\nMake sense? So now what are we doing? We are simply invoking it from within the\nare simply invoking it from within the\nare simply invoking it from within the function.\nfunction.\nfunction. Okay? And then we are simply returning\nOkay? And then we are simply returning\nOkay? And then we are simply returning the result. And what is that? The\nthe result. And what is that? The\nthe result. And what is that? The string. That's it. Okay. Okay. Makes\nstring. That's it. Okay. Okay. Makes\nstring. That's it. Okay. Okay. Makes sense. Now we will simply create a\nsense. Now we will simply create a\nsense. Now we will simply create a lambda function for it. I will say\nlambda function for it. I will say\nlambda function for it. I will say uh insta\nuh insta\nuh insta chain runnable equals runnable lambda.\nchain runnable equals runnable lambda.\nchain runnable equals runnable lambda. Insta chain. Right. Let me just run\nInsta chain. Right. Let me just run\nInsta chain. Right. Let me just run this. Perfect. So these two are done.\nthis. Perfect. So these two are done.\nthis. Perfect. So these two are done. Very good. So now finally we need to\nVery good. So now finally we need to\nVery good. So now finally we need to finalize the final orchestration. That\nfinalize the final orchestration. That\nfinalize the final orchestration. That means we need to now arrange task one,\nmeans we need to now arrange task one,\nmeans we need to now arrange task one, task two, task three, task four and then\ntask two, task three, task four and then\ntask two, task three, task four and then these two parallel chains. Right? Let's\nthese two parallel chains. Right? Let's\nthese two parallel chains. Right? Let's try to do it. Let me just write\ntry to do it. Let me just write\ntry to do it. Let me just write parallel chain one, parallel chain two.\nparallel chain one, parallel chain two.\nparallel chain one, parallel chain two. And now it will be\nAnd now it will be\nAnd now it will be final orchestration.\nPerfect. Now final orchestration is very\nPerfect. Now final orchestration is very very important. I will simply say final\nvery important. I will simply say final\nvery important. I will simply say final chain equals first of all we have I\nchain equals first of all we have I\nchain equals first of all we have I think prompt template\nthink prompt template\nthink prompt template then we have lm openi\nthen we have lm openi\nthen we have lm openi lm openi\nlm openi\nlm openi string parser dictionary maker runnable\nstring parser dictionary maker runnable\nstring parser dictionary maker runnable and then we have let me just use that\nand then we have let me just use that\nand then we have let me just use that braces so that I can just show you\nbraces so that I can just show you\nbraces so that I can just show you better.\nbetter.\nbetter. Okay, we have this, we have this,\nOkay, we have this, we have this,\nOkay, we have this, we have this, we have this and then we have this and\nwe have this and then we have this and\nwe have this and then we have this and then we have parallel chain. Now is the\nthen we have parallel chain. Now is the\nthen we have parallel chain. Now is the thing because now we need to create two\nthing because now we need to create two\nthing because now we need to create two chains, right? So we have something\nchains, right? So we have something\nchains, right? So we have something called as runnable parallel and let me\ncalled as runnable parallel and let me\ncalled as runnable parallel and let me just first of all import it and I would\njust first of all import it and I would\njust first of all import it and I would need to import it here\nOkay.\nOkay. Runnable lambda runnable. So here we\nRunnable lambda runnable. So here we\nRunnable lambda runnable. So here we will be using a special function called\nwill be using a special function called\nwill be using a special function called runnable\nrunnable\nrunnable parallel. Make sense? Runnable parallel.\nparallel. Make sense? Runnable parallel.\nparallel. Make sense? Runnable parallel. Perfect. Now what is this function? This\nPerfect. Now what is this function? This\nPerfect. Now what is this function? This function says now after this particular\nfunction says now after this particular\nfunction says now after this particular task called dictionary maker you need to\ntask called dictionary maker you need to\ntask called dictionary maker you need to create two parallel things. You need to\ncreate two parallel things. You need to\ncreate two parallel things. You need to create two parallel things. So what are\ncreate two parallel things. So what are\ncreate two parallel things. So what are the things? What are the task within\nthe things? What are the task within\nthe things? What are the task within that? Now I will define a chains that I\nthat? Now I will define a chains that I\nthat? Now I will define a chains that I have created. One chain is this one.\nhave created. One chain is this one.\nhave created. One chain is this one. Very simple chain linked. Very good. I\nVery simple chain linked. Very good. I\nVery simple chain linked. Very good. I will simply say I I just need to pass a\nwill simply say I I just need to pass a\nwill simply say I I just need to pass a list basically not list. Um I can also\nlist basically not list. Um I can also\nlist basically not list. Um I can also do like one more thing. It's called\ndo like one more thing. It's called\ndo like one more thing. It's called branches because there are like two\nbranches because there are like two\nbranches because there are like two branches. Let me just show you. So these\nbranches. Let me just show you. So these\nbranches. Let me just show you. So these are the two branches one two right so we\nare the two branches one two right so we\nare the two branches one two right so we have a built-in function as well like\nhave a built-in function as well like\nhave a built-in function as well like branches we can just use it and I think\nbranches we can just use it and I think\nbranches we can just use it and I think this is a better way to manage it manage\nthis is a better way to manage it manage\nthis is a better way to manage it manage it as well because I will just show you\nit as well because I will just show you\nit as well because I will just show you the output as well so when we say\nthe output as well so when we say\nthe output as well so when we say branches then we can define the branches\nbranches then we can define the branches\nbranches then we can define the branches like what are the branches I will simply\nlike what are the branches I will simply\nlike what are the branches I will simply say hey branch one you can name it\nsay hey branch one you can name it\nsay hey branch one you can name it anything okay branch one is let's say\nanything okay branch one is let's say\nanything okay branch one is let's say LinkedIn and second branch is Instagram\nLinkedIn and second branch is Instagram\nLinkedIn and second branch is Instagram okay so LinkedIn branch is chain\nokay so LinkedIn branch is chain\nokay so LinkedIn branch is chain LinkedIn yes but For Instagram, we have\nLinkedIn yes but For Instagram, we have\nLinkedIn yes but For Instagram, we have a function, right? We have a runnable.\na function, right? We have a runnable.\na function, right? We have a runnable. So, I will do this. Make sense? So, this\nSo, I will do this. Make sense? So, this\nSo, I will do this. Make sense? So, this is my final chain.\nis my final chain.\nis my final chain. Okay. And why it is red?\nOkay. And why it is red?\nOkay. And why it is red? Uh, okay. Make sense? I know why it is\nUh, okay. Make sense? I know why it is\nUh, okay. Make sense? I know why it is red\nred\nred because\nbecause\nbecause this is missing. Yeah, now it's fine.\nthis is missing. Yeah, now it's fine.\nthis is missing. Yeah, now it's fine. So, let me just invoke it now. Final\nSo, let me just invoke it now. Final\nSo, let me just invoke it now. Final chain\nchain\nchain dot invoke. Now, just tell me your\ndot invoke. Now, just tell me your\ndot invoke. Now, just tell me your favorite movie.\nfavorite movie.\nfavorite movie. Um because we just need to write the\nUm because we just need to write the\nUm because we just need to write the movie name, right? In the first\nmovie name, right? In the first\nmovie name, right? In the first template, we just need to write the\ntemplate, we just need to write the\ntemplate, we just need to write the movie name. Yes. So let's say I want to\nmovie name. Yes. So let's say I want to\nmovie name. Yes. So let's say I want to write a quick brief about KGF.\nwrite a quick brief about KGF.\nwrite a quick brief about KGF. I love that movie. Who is saying pushpa?\nI love that movie. Who is saying pushpa?\nI love that movie. Who is saying pushpa? That movie is also good. But KGF is like\nThat movie is also good. But KGF is like\nThat movie is also good. But KGF is like KGF.\nBy the way, I love Push by as well. But\nBy the way, I love Push by as well. But KGF is like KGF, right? KGF is KGF. So\nKGF is like KGF, right? KGF is KGF. So\nKGF is like KGF, right? KGF is KGF. So let me run this. Let's see\nlet me run this. Let's see\nlet me run this. Let's see what we get because I want to show you\nwhat we get because I want to show you\nwhat we get because I want to show you the final output. It will be like very\nthe final output. It will be like very\nthe final output. It will be like very interesting.\nIt is taking time because we have so\nIt is taking time because we have so many steps involved.\nString output parser this this. Okay,\nString output parser this this. Okay, perfect. So it took 25 seconds. Wow. So\nperfect. So it took 25 seconds. Wow. So\nperfect. So it took 25 seconds. Wow. So first of all what happened?\nfirst of all what happened?\nfirst of all what happened? Let me just show you what happened. So\nLet me just show you what happened. So\nLet me just show you what happened. So if you just look at this thing first of\nif you just look at this thing first of\nif you just look at this thing first of all template we created a template\nall template we created a template\nall template we created a template prompt template for movie KGF. Then we\nprompt template for movie KGF. Then we\nprompt template for movie KGF. Then we asked LLM, hey write a brief um summary\nasked LLM, hey write a brief um summary\nasked LLM, hey write a brief um summary about KGF. Simple. Then we use string\nabout KGF. Simple. Then we use string\nabout KGF. Simple. Then we use string output parser to just fetched out to\noutput parser to just fetched out to\noutput parser to just fetched out to just fetch out the string and then we\njust fetch out the string and then we\njust fetch out the string and then we used string to create the dictionary\nused string to create the dictionary\nused string to create the dictionary because I know we have these templates\nbecause I know we have these templates\nbecause I know we have these templates right. So we use those things. Now we\nright. So we use those things. Now we\nright. So we use those things. Now we need to pass that template to create the\nneed to pass that template to create the\nneed to pass that template to create the Instagram post and LinkedIn post. Make\nInstagram post and LinkedIn post. Make\nInstagram post and LinkedIn post. Make sense? That's what you want to do. So\nsense? That's what you want to do. So\nsense? That's what you want to do. So here we can just do this thing with with\nhere we can just do this thing with with\nhere we can just do this thing with with the help of these two chains. This is\nthe help of these two chains. This is\nthe help of these two chains. This is chain number one. That is a traditional\nchain number one. That is a traditional\nchain number one. That is a traditional way to do it. This is chain number two\nway to do it. This is chain number two\nway to do it. This is chain number two with function. Like now you have both\nwith function. Like now you have both\nwith function. Like now you have both the ways. Now you can just do anything.\nthe ways. Now you can just do anything.\nthe ways. Now you can just do anything. And I have also given you the option\nAnd I have also given you the option\nAnd I have also given you the option that you can even ignore this chain\nthat you can even ignore this chain\nthat you can even ignore this chain definition because you can call\ndefinition because you can call\ndefinition because you can call everything within this function with the\neverything within this function with the\neverything within this function with the changes that you want. That is another\nchanges that you want. That is another\nchanges that you want. That is another way to do it. Make sense? Okay. Now,\nway to do it. Make sense? Okay. Now,\nway to do it. Make sense? Okay. Now, now this is the final chain that we have\nnow this is the final chain that we have\nnow this is the final chain that we have created. Make sense? Now we invoked it.\ncreated. Make sense? Now we invoked it.\ncreated. Make sense? Now we invoked it. And if you just looked at the output,\nAnd if you just looked at the output,\nAnd if you just looked at the output, output is not very straightforward.\noutput is not very straightforward.\noutput is not very straightforward. output is saying that you have branches\noutput is saying that you have branches\noutput is saying that you have branches key and within that branches you have a\nkey and within that branches you have a\nkey and within that branches you have a dictionary with two values LinkedIn and\ndictionary with two values LinkedIn and\ndictionary with two values LinkedIn and Instagram and you know from where I just\nInstagram and you know from where I just\nInstagram and you know from where I just got these two values from here because I\ngot these two values from here because I\ngot these two values from here because I defined that I want these two branches\ndefined that I want these two branches\ndefined that I want these two branches like this\nlike this\nlike this H and this is my LinkedIn post and this\nH and this is my LinkedIn post and this\nH and this is my LinkedIn post and this is my Instagram post like content of it\nis my Instagram post like content of it\nis my Instagram post like content of it that LLM has generated\nthat LLM has generated\nthat LLM has generated makes sense\nmakes sense\nmakes sense and see both are\nand see both are\nand see both are both are different.\nboth are different.\nboth are different. Okay. So that's how you build the\nOkay. So that's how you build the\nOkay. So that's how you build the parallel orchestration. The just a bonus\nparallel orchestration. The just a bonus\nparallel orchestration. The just a bonus tip I want to also show you one more\ntip I want to also show you one more\ntip I want to also show you one more thing. It's a very quick one. Let's say\nthing. It's a very quick one. Let's say\nthing. It's a very quick one. Let's say you want to highlight or basically you\nyou want to highlight or basically you\nyou want to highlight or basically you want to um display this thing in a\nwant to um display this thing in a\nwant to um display this thing in a better way. Right? So you can even like\nbetter way. Right? So you can even like\nbetter way. Right? So you can even like this is a final chain. This is a chain\nthis is a final chain. This is a chain\nthis is a final chain. This is a chain in itself. You can also\nin itself. You can also\nin itself. You can also treat this chain as a runnable.\ntreat this chain as a runnable.\ntreat this chain as a runnable. Is it possible? Yeah, it is possible.\nIs it possible? Yeah, it is possible.\nIs it possible? Yeah, it is possible. For example, let's say you want to\nFor example, let's say you want to\nFor example, let's say you want to create a chain like this. Let me just\ncreate a chain like this. Let me just\ncreate a chain like this. Let me just write it here.\nChain as a runnable.\nChain as a runnable. Okay. Chain as a runnable.\nSo let's say once you have this chain,\nSo let's say once you have this chain, this final chain, now you want to extend\nthis final chain, now you want to extend\nthis final chain, now you want to extend this chain, right? Now you want to\nthis chain, right? Now you want to\nthis chain, right? Now you want to extend this chain and you want to now\nextend this chain and you want to now\nextend this chain and you want to now connect let's say one more function and\nconnect let's say one more function and\nconnect let's say one more function and what this function returns it beautifies\nwhat this function returns it beautifies\nwhat this function returns it beautifies the output\nthe output\nthe output because we have the dictionary here and\nbecause we have the dictionary here and\nbecause we have the dictionary here and we do not want to present it like this\nwe do not want to present it like this\nwe do not want to present it like this we just want to beautify it. So what I\nwe just want to beautify it. So what I\nwe just want to beautify it. So what I will do? I will create a function. I\nwill do? I will create a function. I\nwill do? I will create a function. I will say task one,\nwill say task one,\nwill say task one, right? Task one will be my\nright? Task one will be my\nright? Task one will be my beauty\nbeauty\nbeauty fire function.\nfire function.\nfire function. Okay, I will say df beautify\nOkay, I will say df beautify\nOkay, I will say df beautify and I will simply get the output and\nand I will simply get the output and\nand I will simply get the output and output can be let's say it it is a\noutput can be let's say it it is a\noutput can be let's say it it is a dictionary. We know that because it will\ndictionary. We know that because it will\ndictionary. We know that because it will return the dictionary. Again,\nreturn the dictionary. Again,\nreturn the dictionary. Again, fundamentals. This output will go as the\nfundamentals. This output will go as the\nfundamentals. This output will go as the input to the next function, next task,\ninput to the next function, next task,\ninput to the next function, next task, right? Fundamental thing. So now this\nright? Fundamental thing. So now this\nright? Fundamental thing. So now this this is a dictionary. So now we will\nthis is a dictionary. So now we will\nthis is a dictionary. So now we will just create our function accordingly. We\njust create our function accordingly. We\njust create our function accordingly. We cannot just expect anything any any\ncannot just expect anything any any\ncannot just expect anything any any string format. No, we are just treating\nstring format. No, we are just treating\nstring format. No, we are just treating it as a dictionary. So I'll simply say\nit as a dictionary. So I'll simply say\nit as a dictionary. So I'll simply say let's say final response, right? And it\nlet's say final response, right? And it\nlet's say final response, right? And it is of a dictionary type, right? And I\nis of a dictionary type, right? And I\nis of a dictionary type, right? And I also want to return dictionary but not a\nalso want to return dictionary but not a\nalso want to return dictionary but not a dictionary I would say um yeah\ndictionary I would say um yeah\ndictionary I would say um yeah dictionary but in a better way. Yeah we\ndictionary but in a better way. Yeah we\ndictionary but in a better way. Yeah we can say that or let's say text. Let's\ncan say that or let's say text. Let's\ncan say that or let's say text. Let's say I want to return string let's say or\nsay I want to return string let's say or\nsay I want to return string let's say or let's say dictionary\nlet's say dictionary\nlet's say dictionary because these are two different things.\nbecause these are two different things.\nbecause these are two different things. So usually we should just return\nSo usually we should just return\nSo usually we should just return dictionary best practices. Okay. So I\ndictionary best practices. Okay. So I\ndictionary best practices. Okay. So I will simply say final response equals to\nwill simply say final response equals to\nwill simply say final response equals to this.\nthis.\nthis. Um I will say LinkedIn\nUm I will say LinkedIn\nUm I will say LinkedIn response equals to this. Yeah, perfect.\nresponse equals to this. Yeah, perfect.\nresponse equals to this. Yeah, perfect. So I have created this function LinkedIn\nSo I have created this function LinkedIn\nSo I have created this function LinkedIn response Instagram response and I'm\nresponse Instagram response and I'm\nresponse Instagram response and I'm returning this dictionary with a\nreturning this dictionary with a\nreturning this dictionary with a beautified version. Make sense? And now\nbeautified version. Make sense? And now\nbeautified version. Make sense? And now obviously I need to create a runnable\nobviously I need to create a runnable\nobviously I need to create a runnable for it and you already know how to\nfor it and you already know how to\nfor it and you already know how to create that. So I'll simply say beautify\ncreate that. So I'll simply say beautify\ncreate that. So I'll simply say beautify runnable equals to runnable lambda.\nrunnable equals to runnable lambda.\nrunnable equals to runnable lambda. Perfect. Now this is done. This is like\nPerfect. Now this is done. This is like\nPerfect. Now this is done. This is like task number one. Now if for the task\ntask number one. Now if for the task\ntask number one. Now if for the task number two\nnumber two\nnumber two final chain you already know that I have\nfinal chain you already know that I have\nfinal chain you already know that I have a final chain so I'll simply say final\na final chain so I'll simply say final\na final chain so I'll simply say final chain equals\nchain equals\nchain equals final chain like whatever we have here\nfinal chain like whatever we have here\nfinal chain like whatever we have here perfect like this this is the final\nperfect like this this is the final\nperfect like this this is the final chain right yeah so what I will do I\nchain right yeah so what I will do I\nchain right yeah so what I will do I will create the runnable lambda for this\nwill create the runnable lambda for this\nwill create the runnable lambda for this like I have two options I can even\nlike I have two options I can even\nlike I have two options I can even simply write final chain like this like\nsimply write final chain like this like\nsimply write final chain like this like I if I write like because chains are\nI if I write like because chains are\nI if I write like because chains are also runnables by default if you do not\nalso runnables by default if you do not\nalso runnables by default if you do not know this I know this is confusing but\nknow this I know this is confusing but\nknow this I know this is confusing but this is important and see I have two\nthis is important and see I have two\nthis is important and see I have two options. I can only tell you just\noptions. I can only tell you just\noptions. I can only tell you just scratching the surface level things or I\nscratching the surface level things or I\nscratching the surface level things or I can just make you understand the things\ncan just make you understand the things\ncan just make you understand the things while going deeper. So I will just\nwhile going deeper. So I will just\nwhile going deeper. So I will just prefer the second option. So I know it\nprefer the second option. So I know it\nprefer the second option. So I know it is like confusing but it is good for\nis like confusing but it is good for\nis like confusing but it is good for your growth and I know you are\nyour growth and I know you are\nyour growth and I know you are uncomfortable. That's where you grow\nuncomfortable. That's where you grow\nuncomfortable. That's where you grow right? Okay. So I I'm with you. Don't\nright? Okay. So I I'm with you. Don't\nright? Okay. So I I'm with you. Don't worry. Don't worry. I know it is\nworry. Don't worry. I know it is\nworry. Don't worry. I know it is confusing but it is fine. So let me just\nconfusing but it is fine. So let me just\nconfusing but it is fine. So let me just tell you so let's say let me just take\ntell you so let's say let me just take\ntell you so let's say let me just take you to the diagram. So let's say this is\nyou to the diagram. So let's say this is\nyou to the diagram. So let's say this is the final chain, right? This is the\nthe final chain, right? This is the\nthe final chain, right? This is the final chain that we have built. Uh let\nfinal chain that we have built. Uh let\nfinal chain that we have built. Uh let me just move it a little bit up.\nPerfect. So let's say this is a chain\nPerfect. So let's say this is a chain that we have built. Perfect. Now this\nthat we have built. Perfect. Now this\nthat we have built. Perfect. Now this entire chain,\nentire chain,\nentire chain, this entire chain is\nthis entire chain is\nthis entire chain is irrunnable.\nirrunnable.\nirrunnable. Is a runnable.\nIs a runnable.\nIs a runnable. Okay. Okay.\nOkay. Okay.\nOkay. Okay. It is runnable.\nThis is the nature of it.\nThis is the nature of it. Make sense? So that means you can just\nMake sense? So that means you can just\nMake sense? So that means you can just use this entire chain as your runnable\nuse this entire chain as your runnable\nuse this entire chain as your runnable and you can connect it to the next task.\nand you can connect it to the next task.\nand you can connect it to the next task. So let's say you have this task, right?\nSo let's say you have this task, right?\nSo let's say you have this task, right? and you want to now connect it using a\nand you want to now connect it using a\nand you want to now connect it using a normal edges you can do it it's not a\nnormal edges you can do it it's not a\nnormal edges you can do it it's not a big deal which is let's say beauty file\nI hope it is clear so it is the property\nI hope it is clear so it is the property of it\nof it\nof it whatever we create in the chain it is a\nwhatever we create in the chain it is a\nwhatever we create in the chain it is a runnable by default so I can literally\nrunnable by default so I can literally\nrunnable by default so I can literally write now my final chain is like final\nwrite now my final chain is like final\nwrite now my final chain is like final chain as you all Okay. So now I will\nchain as you all Okay. So now I will\nchain as you all Okay. So now I will simply say h final chain.\nsimply say h final chain.\nsimply say h final chain. Okay. And I can comment it out because\nOkay. And I can comment it out because\nOkay. And I can comment it out because we do not need to recreate it because it\nwe do not need to recreate it because it\nwe do not need to recreate it because it is created here. Now I will simply say\nis created here. Now I will simply say\nis created here. Now I will simply say beautify chain\nbeautified chain. And now if I simply\nbeautified chain. And now if I simply create let's say beautified chain equals\ncreate let's say beautified chain equals\ncreate let's say beautified chain equals to final chain. and then beautify\nto final chain. and then beautify\nto final chain. and then beautify runnable. See, now I can literally use\nrunnable. See, now I can literally use\nrunnable. See, now I can literally use the entire chain as my runnable.\nthe entire chain as my runnable.\nthe entire chain as my runnable. So it is a by default behavior. You need\nSo it is a by default behavior. You need\nSo it is a by default behavior. You need to understand this.\nto understand this.\nto understand this. Yes, you need to explicitly convert your\nYes, you need to explicitly convert your\nYes, you need to explicitly convert your Python function as runnable. But if\nPython function as runnable. But if\nPython function as runnable. But if you're creating a chain, you don't need\nyou're creating a chain, you don't need\nyou're creating a chain, you don't need to convert into runnable. That is a\nto convert into runnable. That is a\nto convert into runnable. That is a runnable by default. Perfect. Now let's\nrunnable by default. Perfect. Now let's\nrunnable by default. Perfect. Now let's try to run this\ntry to run this\ntry to run this beautiful beautified chain. V and this\nbeautiful beautified chain. V and this\nbeautiful beautified chain. V and this time let's say pushp\nand I hope now this will give you a\nand I hope now this will give you a richer understanding about managing the\nricher understanding about managing the\nricher understanding about managing the chains because it is very easy to just\nchains because it is very easy to just\nchains because it is very easy to just show you wow we have an error wow wow\nshow you wow we have an error wow wow\nshow you wow we have an error wow wow What? What is error? LinkedIn output is\nWhat? What is error? LinkedIn output is\nWhat? What is error? LinkedIn output is truncated LinkedIn. Okay.\ntruncated LinkedIn. Okay.\ntruncated LinkedIn. Okay. So it is saying error\nSo it is saying error\nSo it is saying error is at LinkedIn. Here we have LinkedIn.\nis at LinkedIn. Here we have LinkedIn.\nis at LinkedIn. Here we have LinkedIn. Oh, I see. I see. Basing mistake. So now\nOh, I see. I see. Basing mistake. So now\nOh, I see. I see. Basing mistake. So now we have final response in the form of\nwe have final response in the form of\nwe have final response in the form of dictionary. Right? Do we have LinkedIn\ndictionary. Right? Do we have LinkedIn\ndictionary. Right? Do we have LinkedIn key available? No. The parent key is\nkey available? No. The parent key is\nkey available? No. The parent key is branches. Inside the branches we have\nbranches. Inside the branches we have\nbranches. Inside the branches we have LinkedIn. So I will simply first of all\nLinkedIn. So I will simply first of all\nLinkedIn. So I will simply first of all add the parent key which is branches and\nadd the parent key which is branches and\nadd the parent key which is branches and then we have Instagram or LinkedIn\nthen we have Instagram or LinkedIn\nthen we have Instagram or LinkedIn whatever. Make sense? Makes sense. Makes\nwhatever. Make sense? Makes sense. Makes\nwhatever. Make sense? Makes sense. Makes sense. Makes sense. Let's use single\nsense. Makes sense. Let's use single\nsense. Makes sense. Let's use single quotes. Perfect.\nquotes. Perfect.\nquotes. Perfect. Perfect. Now let's run this.\nYeah. So I was saying that this will\nYeah. So I was saying that this will give you a richer understanding because\ngive you a richer understanding because\ngive you a richer understanding because it is very easy to build those simple\nit is very easy to build those simple\nit is very easy to build those simple chains okay where I will simply say show\nchains okay where I will simply say show\nchains okay where I will simply say show you hey this is prom this is llm this is\nyou hey this is prom this is llm this is\nyou hey this is prom this is llm this is string out parser who wa that's it\nstring out parser who wa that's it\nstring out parser who wa that's it that's it that's it no no no you are\nthat's it that's it no no no you are\nthat's it that's it no no no you are becoming a good developer here and you\nbecoming a good developer here and you\nbecoming a good developer here and you need to understand all of those things\nneed to understand all of those things\nneed to understand all of those things which are complex\nwhich are complex\nwhich are complex makes sense push is taking more time why\nmakes sense push is taking more time why\nmakes sense push is taking more time why kgf took just 25 seconds wow we have\nkgf took just 25 seconds wow we have\nkgf took just 25 seconds wow we have fire\nSo as you know now we have the\nSo as you know now we have the beautified version right now I can\nbeautified version right now I can\nbeautified version right now I can literally use these two key value pairs\nliterally use these two key value pairs\nliterally use these two key value pairs to perform my thing let's say email and\nto perform my thing let's say email and\nto perform my thing let's say email and um teams message whatever but we took\num teams message whatever but we took\num teams message whatever but we took just hypothetical situation just for fun\njust hypothetical situation just for fun\njust hypothetical situation just for fun so that you can just enjoy while\nso that you can just enjoy while\nso that you can just enjoy while learning it but you know like you can\nlearning it but you know like you can\nlearning it but you know like you can literally apply this thing anywhere and\nliterally apply this thing anywhere and\nliterally apply this thing anywhere and don't worry I'll just show you one quick\ndon't worry I'll just show you one quick\ndon't worry I'll just show you one quick example as well even if you do not have\nexample as well even if you do not have\nexample as well even if you do not have any kind of real data but we can just\nany kind of real data but we can just\nany kind of real data but we can just show you make sense okay let's see\nshow you make sense okay let's see\nshow you make sense okay let's see so now let's talk about the conditional\nso now let's talk about the conditional\nso now let's talk about the conditional chains. Now what are these conditional\nchains. Now what are these conditional\nchains. Now what are these conditional chains? So now we know like how parallel\nchains? So now we know like how parallel\nchains? So now we know like how parallel chains run in parallel. But now every\nchains run in parallel. But now every\nchains run in parallel. But now every time we do not want to just run\ntime we do not want to just run\ntime we do not want to just run everything in parallel. Now if you just\neverything in parallel. Now if you just\neverything in parallel. Now if you just look at the example above it's it is\nlook at the example above it's it is\nlook at the example above it's it is exactly the same. Right.\nexactly the same. Right.\nexactly the same. Right. Right. It is it is exactly the same. But\nRight. It is it is exactly the same. But\nRight. It is it is exactly the same. But now the difference is this is a\nnow the difference is this is a\nnow the difference is this is a conditional chain. That means both of\nconditional chain. That means both of\nconditional chain. That means both of these chains will not run.\nthese chains will not run.\nthese chains will not run. This chain will run if the condition is\nThis chain will run if the condition is\nThis chain will run if the condition is yes. this chain will run if the\nyes. this chain will run if the\nyes. this chain will run if the condition is no.\ncondition is no.\ncondition is no. So that means we can even create\nSo that means we can even create\nSo that means we can even create conditional chains as well. And again in\nconditional chains as well. And again in\nconditional chains as well. And again in the real world examples or in the real\nthe real world examples or in the real\nthe real world examples or in the real world scenarios we use conditional\nworld scenarios we use conditional\nworld scenarios we use conditional chains as well because you do not need\nchains as well because you do not need\nchains as well because you do not need to run everything. You need to keep so\nto run everything. You need to keep so\nto run everything. You need to keep so many things autonomous so that you can\nmany things autonomous so that you can\nmany things autonomous so that you can decide and that is the entire I would\ndecide and that is the entire I would\ndecide and that is the entire I would say u backbone of autonomous agents\nsay u backbone of autonomous agents\nsay u backbone of autonomous agents autonomous workflows. Autonomous are\nautonomous workflows. Autonomous are\nautonomous workflows. Autonomous are nothing but just fancy if else\nnothing but just fancy if else\nnothing but just fancy if else statements and that's it\nstatements and that's it\nstatements and that's it because there is something which is\nbecause there is something which is\nbecause there is something which is actually evaluating the decision.\nactually evaluating the decision.\nactually evaluating the decision. In simple terms it is simple if else\nIn simple terms it is simple if else\nIn simple terms it is simple if else condition.\ncondition.\ncondition. Okay, great. So now let's see how we can\nOkay, great. So now let's see how we can\nOkay, great. So now let's see how we can just implement this thing and\njust implement this thing and\njust implement this thing and architecture wise it is very clear. So\narchitecture wise it is very clear. So\narchitecture wise it is very clear. So now I'm not just explaining this from\nnow I'm not just explaining this from\nnow I'm not just explaining this from scratch because you know all of these\nscratch because you know all of these\nscratch because you know all of these things. Now only thing is how we can\nthings. Now only thing is how we can\nthings. Now only thing is how we can just create these conditional branches.\njust create these conditional branches.\njust create these conditional branches. Right. So let's go to our\nRight. So let's go to our\nRight. So let's go to our anti-gravity and now let's create\nanti-gravity and now let's create\nanti-gravity and now let's create conditional chains\nparallel chains and we can just call it\nparallel chains and we can just call it as\nas\nas conditional chains. Very good. Now the\nconditional chains. Very good. Now the\nconditional chains. Very good. Now the flow will be very simple and very\nflow will be very simple and very\nflow will be very simple and very similar as well. So what we going to do?\nsimilar as well. So what we going to do?\nsimilar as well. So what we going to do? We will simply ask for the um movie\nWe will simply ask for the um movie\nWe will simply ask for the um movie review movie summary and before that\nreview movie summary and before that\nreview movie summary and before that what I will do I will write the review.\nwhat I will do I will write the review.\nwhat I will do I will write the review. What does it mean? Let me show you. So\nWhat does it mean? Let me show you. So\nWhat does it mean? Let me show you. So first of all this is the prompt template\nfirst of all this is the prompt template\nfirst of all this is the prompt template right? We do not need to now write the\nright? We do not need to now write the\nright? We do not need to now write the summary. Now because we want to\nsummary. Now because we want to\nsummary. Now because we want to just evaluate the response. So let's say\njust evaluate the response. So let's say\njust evaluate the response. So let's say you are actually building a pipeline.\nyou are actually building a pipeline.\nyou are actually building a pipeline. Okay. And you need to automate this\nOkay. And you need to automate this\nOkay. And you need to automate this particular AI workflow as a data\nparticular AI workflow as a data\nparticular AI workflow as a data engineer. So let's say you're a product\nengineer. So let's say you're a product\nengineer. So let's say you're a product based company. If you're not working in\nbased company. If you're not working in\nbased company. If you're not working in a product based company, don't worry.\na product based company, don't worry.\na product based company, don't worry. You will be working in a product based\nYou will be working in a product based\nYou will be working in a product based company very soon. Don't worry at all.\ncompany very soon. Don't worry at all.\ncompany very soon. Don't worry at all. So let's say you're working in a product\nSo let's say you're working in a product\nSo let's say you're working in a product based company and your company is\nbased company and your company is\nbased company and your company is selling some products, right? And you\nselling some products, right? And you\nselling some products, right? And you are assigned a task where you need to\nare assigned a task where you need to\nare assigned a task where you need to automate their customer reviews. What\nautomate their customer reviews. What\nautomate their customer reviews. What does it mean? So basically let's say\ndoes it mean? So basically let's say\ndoes it mean? So basically let's say there's a product\nthere's a product\nthere's a product and there's a customer XYZ wrote\nand there's a customer XYZ wrote\nand there's a customer XYZ wrote something. Hey, what is this product?\nsomething. Hey, what is this product?\nsomething. Hey, what is this product? product blah blah blah blah blah.\nproduct blah blah blah blah blah.\nproduct blah blah blah blah blah. So in that particular scenario,\nSo in that particular scenario,\nSo in that particular scenario, you do not need to build your NLP or\nyou do not need to build your NLP or\nyou do not need to build your NLP or basically natural language processing u\nbasically natural language processing u\nbasically natural language processing u machine learning model to categorize it.\nmachine learning model to categorize it.\nmachine learning model to categorize it. You can leverage LLM. That's why you are\nYou can leverage LLM. That's why you are\nYou can leverage LLM. That's why you are an AI data engineer, right? You do not\nan AI data engineer, right? You do not\nan AI data engineer, right? You do not need to involve an ML engineer. You as a\nneed to involve an ML engineer. You as a\nneed to involve an ML engineer. You as a data engineer can do it. So now what you\ndata engineer can do it. So now what you\ndata engineer can do it. So now what you will do? You will take the review,\nwill do? You will take the review,\nwill do? You will take the review, right? And you will pass it to the LLM.\nright? And you will pass it to the LLM.\nright? And you will pass it to the LLM. Make sense? And that LLM will actually\nMake sense? And that LLM will actually\nMake sense? And that LLM will actually categorize it whether the review is\ncategorize it whether the review is\ncategorize it whether the review is positive or negative. Okay. How LLM can\npositive or negative. Okay. How LLM can\npositive or negative. Okay. How LLM can do that? LLMs are actually the NLP\ndo that? LLMs are actually the NLP\ndo that? LLMs are actually the NLP models. Okay. Behind the scenes built on\nmodels. Okay. Behind the scenes built on\nmodels. Okay. Behind the scenes built on top of the transformer model. So they\ntop of the transformer model. So they\ntop of the transformer model. So they are well trained on these kind of tasks.\nare well trained on these kind of tasks.\nare well trained on these kind of tasks. Really? Yeah. Yeah. They are very very\nReally? Yeah. Yeah. They are very very\nReally? Yeah. Yeah. They are very very well organiz very very well like trained\nwell organiz very very well like trained\nwell organiz very very well like trained on these types of tasks like\non these types of tasks like\non these types of tasks like categorization and you can say if you\ncategorization and you can say if you\ncategorization and you can say if you want to evaluate something LLM can do\nwant to evaluate something LLM can do\nwant to evaluate something LLM can do that very well because it is trained on\nthat very well because it is trained on\nthat very well because it is trained on such a large corpus of data I cannot\nsuch a large corpus of data I cannot\nsuch a large corpus of data I cannot even tell like such a large corpus of\neven tell like such a large corpus of\neven tell like such a large corpus of data so it can evaluate these things\ndata so it can evaluate these things\ndata so it can evaluate these things very nicely usually we do it with the\nvery nicely usually we do it with the\nvery nicely usually we do it with the help of bird models as well basically\nhelp of bird models as well basically\nhelp of bird models as well basically encoder only models but it is not very\nencoder only models but it is not very\nencoder only models but it is not very performant that's why we use LLM that\nperformant that's why we use LLM that\nperformant that's why we use LLM that means like um transformer based models\nmeans like um transformer based models\nmeans like um transformer based models so LLM can do that very\nso LLM can do that very\nso LLM can do that very Okay. But yes, we need to guide it. We\nOkay. But yes, we need to guide it. We\nOkay. But yes, we need to guide it. We need to also use a structured output\nneed to also use a structured output\nneed to also use a structured output that you learned in the beginning\nthat you learned in the beginning\nthat you learned in the beginning pyantic because LLM yes can do that\npyantic because LLM yes can do that\npyantic because LLM yes can do that work. But LLM will not just say yes or\nwork. But LLM will not just say yes or\nwork. But LLM will not just say yes or no. You need to guide LLM bro. Only\nno. You need to guide LLM bro. Only\nno. You need to guide LLM bro. Only respond as yes or no. Do not say this\nrespond as yes or no. Do not say this\nrespond as yes or no. Do not say this person has said da da da da da. You need\nperson has said da da da da da. You need\nperson has said da da da da da. You need to just say yes or no. And you will be\nto just say yes or no. And you will be\nto just say yes or no. And you will be doing it with the help of identic. Make\ndoing it with the help of identic. Make\ndoing it with the help of identic. Make sense? Okay. So now your fundamentals\nsense? Okay. So now your fundamentals\nsense? Okay. So now your fundamentals are strong enough to build those things.\nare strong enough to build those things.\nare strong enough to build those things. So let's try to write our this thing.\nSo let's try to write our this thing.\nSo let's try to write our this thing. You are a movie reviewer or review um\nYou are a movie reviewer or review um\nYou are a movie reviewer or review um you are a movie review evaluator.\nyou are a movie review evaluator.\nyou are a movie review evaluator. Make sense?\nMake sense?\nMake sense? And then I will say please\nAnd then I will say please\nAnd then I will say please categorize the movie review as positive\ncategorize the movie review as positive\ncategorize the movie review as positive or negative. Simple. So this is my\nor negative. Simple. So this is my\nor negative. Simple. So this is my prompt. But before building this prompt\nprompt. But before building this prompt\nprompt. But before building this prompt I will build my pente class. if you\nI will build my pente class. if you\nI will build my pente class. if you remember. So I will simply say from paid\nremember. So I will simply say from paid\nremember. So I will simply say from paid import base model and I will simply say\nimport base model and I will simply say\nimport base model and I will simply say class and let's say llm schema\nschema base model and let's say movie\nschema base model and let's say movie summary flag\nsummary flag\nsummary flag movie summary flag okay and I will use\nmovie summary flag okay and I will use\nmovie summary flag okay and I will use something called as literal yes or no\nsomething called as literal yes or no\nsomething called as literal yes or no okay or let's say positive or negative\nokay or let's say positive or negative\nokay or let's say positive or negative positive and negative And let me just\npositive and negative And let me just\npositive and negative And let me just import literal as well from typing\nimport literal as well from typing\nimport literal as well from typing import literal. So this is my schema\nimport literal. So this is my schema\nimport literal. So this is my schema because I just want literal means like\nbecause I just want literal means like\nbecause I just want literal means like you just have these two options either\nyou just have these two options either\nyou just have these two options either positive or negative. You cannot answer\npositive or negative. You cannot answer\npositive or negative. You cannot answer anything else. So I will just use this\nanything else. So I will just use this\nanything else. So I will just use this particular schema to build my LLM. So\nparticular schema to build my LLM. So\nparticular schema to build my LLM. So how I can just do that? I will simply\nhow I can just do that? I will simply\nhow I can just do that? I will simply say llm structured\nstructured output equals llm openai dot\nstructured output equals llm openai dot with structured output and lm schema.\nwith structured output and lm schema.\nwith structured output and lm schema. Perfect. So now this LLM knows that it\nPerfect. So now this LLM knows that it\nPerfect. So now this LLM knows that it needs to generate the response in the\nneeds to generate the response in the\nneeds to generate the response in the form of\nform of\nform of this thing positive or negative and\nthis thing positive or negative and\nthis thing positive or negative and that's it. Make sense?\nthat's it. Make sense?\nthat's it. Make sense? Make sense? Okay.\nOkay. Makes sense. And obviously um I\nOkay. Makes sense. And obviously um I can just test it as well. Let's see what\ncan just test it as well. Let's see what\ncan just test it as well. Let's see what it returns. Dict must be a prompt value.\nit returns. Dict must be a prompt value.\nit returns. Dict must be a prompt value. What is this? Um okay, makes sense\nWhat is this? Um okay, makes sense\nWhat is this? Um okay, makes sense because we do not have like any kind of\nbecause we do not have like any kind of\nbecause we do not have like any kind of input. But okay, makes sense. So I can\ninput. But okay, makes sense. So I can\ninput. But okay, makes sense. So I can simply say like this.\nOkay, see now it has done just this\nOkay, see now it has done just this movie summary flag equals to positive.\nmovie summary flag equals to positive.\nmovie summary flag equals to positive. Very good. Now do you know what? Now you\nVery good. Now do you know what? Now you\nVery good. Now do you know what? Now you will say an llama we need to use this\nwill say an llama we need to use this\nwill say an llama we need to use this particular thing in our downstream\nparticular thing in our downstream\nparticular thing in our downstream models as well. But this is a pyic\nmodels as well. But this is a pyic\nmodels as well. But this is a pyic object. How we can just parse it? You\nobject. How we can just parse it? You\nobject. How we can just parse it? You have two options. You can either create\nhave two options. You can either create\nhave two options. You can either create your own um custom function custom\nyour own um custom function custom\nyour own um custom function custom runnable to use it or there's um one\nrunnable to use it or there's um one\nrunnable to use it or there's um one thing as well provided by\nthing as well provided by\nthing as well provided by langchen. It's called pyntic output\nlangchen. It's called pyntic output\nlangchen. It's called pyntic output parser. what pyic output parser will do.\nparser. what pyic output parser will do.\nparser. what pyic output parser will do. Pentic output parser will simply parse\nPentic output parser will simply parse\nPentic output parser will simply parse it. Okay, let me assure you. So if I say\nit. Okay, let me assure you. So if I say\nit. Okay, let me assure you. So if I say demo chain, okay, this is a demo chain\ndemo chain, okay, this is a demo chain\ndemo chain, okay, this is a demo chain and I will simply say llm structured\nand I will simply say llm structured\nand I will simply say llm structured output and I will say pentic output\noutput and I will say pentic output\noutput and I will say pentic output parser.\nparser.\nparser. Pentic output parser and I need to\nPentic output parser and I need to\nPentic output parser and I need to define the schema and schema is llm\ndefine the schema and schema is llm\ndefine the schema and schema is llm schema. Right? And now if I say this\nschema. Right? And now if I say this\nschema. Right? And now if I say this thing, now let's see what we get.\nPyic output parser is not defined\nPyic output parser is not defined because we didn't run this. Now let's\nbecause we didn't run this. Now let's\nbecause we didn't run this. Now let's see. Um, okay. What's wrong with this?\nsee. Um, okay. What's wrong with this?\nsee. Um, okay. What's wrong with this? Okay.\nOkay.\nOkay. Okay.\nOkay.\nOkay. I think\nI think\nI think this is fine because by default it\nthis is fine because by default it\nthis is fine because by default it should take the battery. Let me just\nshould take the battery. Let me just\nshould take the battery. Let me just specify this pyic object. I think so\nspecify this pyic object. I think so\nspecify this pyic object. I think so because this is the code.\nbecause this is the code.\nbecause this is the code. Oh. Oh. Oh. This time is the error is\nOh. Oh. Oh. This time is the error is\nOh. Oh. Oh. This time is the error is different. Okay. This time it says\ndifferent. Okay. This time it says\ndifferent. Okay. This time it says validation. Okay. Validation error makes\nvalidation. Okay. Validation error makes\nvalidation. Okay. Validation error makes sense. That means it is running fine.\nsense. That means it is running fine.\nsense. That means it is running fine. Okay. This is running fine. So now this\nOkay. This is running fine. So now this\nOkay. This is running fine. So now this is a good thing. So now do you know what\nis a good thing. So now do you know what\nis a good thing. So now do you know what happened? This has actually thrown one\nhappened? This has actually thrown one\nhappened? This has actually thrown one error which is called validation error.\nerror which is called validation error.\nerror which is called validation error. That means they're saying hey validate\nThat means they're saying hey validate\nThat means they're saying hey validate see if I am just running it without\nsee if I am just running it without\nsee if I am just running it without this. Okay, if I'm just running it\nthis. Okay, if I'm just running it\nthis. Okay, if I'm just running it without this. So what do we get? If I\nwithout this. So what do we get? If I\nwithout this. So what do we get? If I just comment it out let's say\njust comment it out let's say\njust comment it out let's say let's say like this. What do we get? We\nlet's say like this. What do we get? We\nlet's say like this. What do we get? We simply get llm schema movie summary flag\nsimply get llm schema movie summary flag\nsimply get llm schema movie summary flag equals to positive. Okay, make sense? So\nequals to positive. Okay, make sense? So\nequals to positive. Okay, make sense? So it is just returning like this. Okay. So\nit is just returning like this. Okay. So\nit is just returning like this. Okay. So now if I'm just adding this particular\nnow if I'm just adding this particular\nnow if I'm just adding this particular thing which is this one, it is throwing\nthing which is this one, it is throwing\nthing which is this one, it is throwing the validation error. That means it is\nthe validation error. That means it is\nthe validation error. That means it is saying the validation is not right. And\nsaying the validation is not right. And\nsaying the validation is not right. And what is the error? Let's read it.\nwhat is the error? Let's read it.\nwhat is the error? Let's read it. Input should be a valid string. Type\nInput should be a valid string. Type\nInput should be a valid string. Type equals string type. Input valid equals\nequals string type. Input valid equals\nequals string type. Input valid equals to lm schema. This thing input type\nto lm schema. This thing input type\nto lm schema. This thing input type equals to LM schema. For further\nequals to LM schema. For further\nequals to LM schema. For further information visit this this is this. So\ninformation visit this this is this. So\ninformation visit this this is this. So it is saying like it is a kind of\nit is saying like it is a kind of\nit is saying like it is a kind of validation issue and you cannot do it\nvalidation issue and you cannot do it\nvalidation issue and you cannot do it like this. So in order to solve this,\nlike this. So in order to solve this,\nlike this. So in order to solve this, you actually just need to remove this.\nyou actually just need to remove this.\nyou actually just need to remove this. Why? Because you do not need to do this.\nWhy? Because you do not need to do this.\nWhy? Because you do not need to do this. Why? Why? Because you are already\nWhy? Why? Because you are already\nWhy? Why? Because you are already converting the text into a pyic model.\nconverting the text into a pyic model.\nconverting the text into a pyic model. If you so if you see the response, it is\nIf you so if you see the response, it is\nIf you so if you see the response, it is already there. If your LLM would be just\nalready there. If your LLM would be just\nalready there. If your LLM would be just generating the dictionary but not a\ngenerating the dictionary but not a\ngenerating the dictionary but not a pyic, then you would have used a pyic\npyic, then you would have used a pyic\npyic, then you would have used a pyic output parser. That's the advantage you\noutput parser. That's the advantage you\noutput parser. That's the advantage you get when you use the premium models\nget when you use the premium models\nget when you use the premium models which are hosted by reputed tech\nwhich are hosted by reputed tech\nwhich are hosted by reputed tech companies because they understand\ncompanies because they understand\ncompanies because they understand everything and you do not need to do\neverything and you do not need to do\neverything and you do not need to do anything. So that makes sense. So our\nanything. So that makes sense. So our\nanything. So that makes sense. So our this thing is ready and we know that\nthis thing is ready and we know that\nthis thing is ready and we know that this will generate a kind of um you can\nthis will generate a kind of um you can\nthis will generate a kind of um you can say pilenting object right and that's\nsay pilenting object right and that's\nsay pilenting object right and that's it. That's what you want. So now what we\nit. That's what you want. So now what we\nit. That's what you want. So now what we need to do we will simply use it\nneed to do we will simply use it\nneed to do we will simply use it and how we can just use it. So let's say\nand how we can just use it. So let's say\nand how we can just use it. So let's say this is our lm structured output. Let me\nthis is our lm structured output. Let me\nthis is our lm structured output. Let me just rerun this. So let's make sure\njust rerun this. So let's make sure\njust rerun this. So let's make sure everything is fine. So this is done.\neverything is fine. So this is done.\neverything is fine. So this is done. Okay. So now we have this particular LLM\nOkay. So now we have this particular LLM\nOkay. So now we have this particular LLM created. And now we can simply say chain\ncreated. And now we can simply say chain\ncreated. And now we can simply say chain with conditional chains.\nwith conditional chains.\nwith conditional chains. Perfect. So this is the kind of prompt\nPerfect. So this is the kind of prompt\nPerfect. So this is the kind of prompt that we are using. You are a review\nthat we are using. You are a review\nthat we are using. You are a review evaluator. Please categorize a movie\nevaluator. Please categorize a movie\nevaluator. Please categorize a movie review as positive or negative. And this\nreview as positive or negative. And this\nreview as positive or negative. And this is the input. I will just pass the input\nis the input. I will just pass the input\nis the input. I will just pass the input in the runtime. Perfect. And this is the\nin the runtime. Perfect. And this is the\nin the runtime. Perfect. And this is the task two which is this one.\nPerfect.\nPerfect. And I can run this as well. Perfect. Now\nAnd I can run this as well. Perfect. Now\nAnd I can run this as well. Perfect. Now I don't need to create the string out\nI don't need to create the string out\nI don't need to create the string out parser because we have pyic. So that is\nparser because we have pyic. So that is\nparser because we have pyic. So that is fine. We need to use this to convert the\nfine. We need to use this to convert the\nfine. We need to use this to convert the pyantic object into a kind of dictionary\npyantic object into a kind of dictionary\npyantic object into a kind of dictionary you can say because we will be receiving\nyou can say because we will be receiving\nyou can say because we will be receiving the object pyic object right and in\nthe object pyic object right and in\nthe object pyic object right and in order to create the further conditionals\norder to create the further conditionals\norder to create the further conditionals we need to just use a kind of custom\nwe need to just use a kind of custom\nwe need to just use a kind of custom function. So I'll simply say\nfunction. So I'll simply say\nfunction. So I'll simply say pentic\nJSON because I want JSON. So this will\nJSON because I want JSON. So this will give me a kind of you can say pyentic\ngive me a kind of you can say pyentic\ngive me a kind of you can say pyentic object\nobject\nobject right. This will give me the pyantic\nright. This will give me the pyantic\nright. This will give me the pyantic object. So now how I can just convert\nobject. So now how I can just convert\nobject. So now how I can just convert this pyentic object into the form of\nthis pyentic object into the form of\nthis pyentic object into the form of this thing. So what I will do I will\nthis thing. So what I will do I will\nthis thing. So what I will do I will simply say input\nsimply say input\nsimply say input right and this will be of llm schema\nright and this will be of llm schema\nright and this will be of llm schema type make sense because this is a schema\ntype make sense because this is a schema\ntype make sense because this is a schema this is not string this is not integer\nthis is not string this is not integer\nthis is not string this is not integer this is not list this is a different\nthis is not list this is a different\nthis is not list this is a different schema and I will return a string\nschema and I will return a string\nschema and I will return a string perfect\nperfect\nperfect now let's try to do it how we can just\nnow let's try to do it how we can just\nnow let's try to do it how we can just do that so I simply say input domodel\ndo that so I simply say input domodel\ndo that so I simply say input domodel dump that is the you can say modern way\ndump that is the you can say modern way\ndump that is the you can say modern way to do that because if If you will see\nto do that because if If you will see\nto do that because if If you will see the result of this, let's say I want to\nthe result of this, let's say I want to\nthe result of this, let's say I want to invoke this model. I will say result\ninvoke this model. I will say result\ninvoke this model. I will say result equals llm\ndot invoke.\ndot invoke. Let's say\nLet's say\nLet's say this\nthis\nthis movie is good. Okay. And if I write\nmovie is good. Okay. And if I write\nmovie is good. Okay. And if I write result,\nso this is a schema. If I say model\nso this is a schema. If I say model dump,\ndump,\ndump, then do you know what? You will see the\nthen do you know what? You will see the\nthen do you know what? You will see the dictionary. This is a function that we\ndictionary. This is a function that we\ndictionary. This is a function that we use especially with the pyic objects. So\nuse especially with the pyic objects. So\nuse especially with the pyic objects. So I simply need to say model dump. And if\nI simply need to say model dump. And if\nI simply need to say model dump. And if I just want to get the movie summary\nI just want to get the movie summary\nI just want to get the movie summary flag, I will use it like this. Movie\nflag, I will use it like this. Movie\nflag, I will use it like this. Movie summary flag.\nPerfect. So these are the best practices\nPerfect. So these are the best practices that you use. Okay. So you just need to\nthat you use. Okay. So you just need to\nthat you use. Okay. So you just need to get this thing model dump JSON and I can\nget this thing model dump JSON and I can\nget this thing model dump JSON and I can delete this cell. This is not required\ndelete this cell. This is not required\ndelete this cell. This is not required and we can just do it here. Perfect. I\nand we can just do it here. Perfect. I\nand we can just do it here. Perfect. I will simply say input dot model dump and\nwill simply say input dot model dump and\nwill simply say input dot model dump and we can return this thing.\nPerfect. Nothing fancy. Yeah. And then\nPerfect. Nothing fancy. Yeah. And then we can just simply create the runnable\nwe can just simply create the runnable\nwe can just simply create the runnable lambda and that's it. Panteek\nJSON lambda\nJSON lambda to runnable lambda. Perfect. So this\nto runnable lambda. Perfect. So this\nto runnable lambda. Perfect. So this these are my tasks that I have created 1\nthese are my tasks that I have created 1\nthese are my tasks that I have created 1 2 3. And once we have this thing then we\n2 3. And once we have this thing then we\n2 3. And once we have this thing then we will be creating two chains like\nwill be creating two chains like\nwill be creating two chains like whatever you want to do. We will simply\nwhatever you want to do. We will simply\nwhatever you want to do. We will simply say um parall chain one and you say you\nsay um parall chain one and you say you\nsay um parall chain one and you say you are a LinkedIn post generator. Create a\nare a LinkedIn post generator. Create a\nare a LinkedIn post generator. Create a post for the following text for let's\npost for the following text for let's\npost for the following text for let's say you want to just post it on LinkedIn\nsay you want to just post it on LinkedIn\nsay you want to just post it on LinkedIn like whether the comment was positive or\nlike whether the comment was positive or\nlike whether the comment was positive or negative. It's just a hypothetical\nnegative. It's just a hypothetical\nnegative. It's just a hypothetical situation. But in the real world, what\nsituation. But in the real world, what\nsituation. But in the real world, what you will do, you will simply simply\nyou will do, you will simply simply\nyou will do, you will simply simply categorize it and you will simply revert\ncategorize it and you will simply revert\ncategorize it and you will simply revert to that email back saying okay this\nto that email back saying okay this\nto that email back saying okay this movie was good. Thanks for your\nmovie was good. Thanks for your\nmovie was good. Thanks for your feedback. If the person says movie is\nfeedback. If the person says movie is\nfeedback. If the person says movie is bad, you will simply say apologies for\nbad, you will simply say apologies for\nbad, you will simply say apologies for that and I hope blah blah blah whatever\nthat and I hope blah blah blah whatever\nthat and I hope blah blah blah whatever you want to say. But here we already\nyou want to say. But here we already\nyou want to say. But here we already have the code written. So I don't want\nhave the code written. So I don't want\nhave the code written. So I don't want to write the code again and we can just\nto write the code again and we can just\nto write the code again and we can just reuse it. Make sense? So let's say you\nreuse it. Make sense? So let's say you\nreuse it. Make sense? So let's say you want to use this parallel chain one\nwant to use this parallel chain one\nwant to use this parallel chain one which is chain linked and we want to use\nwhich is chain linked and we want to use\nwhich is chain linked and we want to use this because we it will just simply turn\nthis because we it will just simply turn\nthis because we it will just simply turn positive or negative. So it will simply\npositive or negative. So it will simply\npositive or negative. So it will simply generate the code you can say prompt for\ngenerate the code you can say prompt for\ngenerate the code you can say prompt for positive. Okay. And it will generate the\npositive. Okay. And it will generate the\npositive. Okay. And it will generate the post for just positive word. That's\npost for just positive word. That's\npost for just positive word. That's fine. Makes sense. And it will do it\nfine. Makes sense. And it will do it\nfine. Makes sense. And it will do it here as well. Perfect. So now let's come\nhere as well. Perfect. So now let's come\nhere as well. Perfect. So now let's come to the final orchestration. Remember\nto the final orchestration. Remember\nto the final orchestration. Remember that we created this runnable parallel.\nthat we created this runnable parallel.\nthat we created this runnable parallel. Yeah we created this. And do you know\nYeah we created this. And do you know\nYeah we created this. And do you know what here we have something called as\nwhat here we have something called as\nwhat here we have something called as runnable branch.\nrunnable branch.\nrunnable branch. Yes, runnable branch. So what we do here\nYes, runnable branch. So what we do here\nYes, runnable branch. So what we do here we simply need to create a kind of\nwe simply need to create a kind of\nwe simply need to create a kind of conditional chain. Okay conditional\nconditional chain. Okay conditional\nconditional chain. Okay conditional chain like we will simply start with the\nchain like we will simply start with the\nchain like we will simply start with the normal orchestra. Let me just show you.\nnormal orchestra. Let me just show you.\nnormal orchestra. Let me just show you. So first of all let me just create a\nSo first of all let me just create a\nSo first of all let me just create a conditional chain. Now this conditional\nconditional chain. Now this conditional\nconditional chain. Now this conditional chain will be runnable\nchain will be runnable\nchain will be runnable branch and obviously we need to import\nbranch and obviously we need to import\nbranch and obviously we need to import it.\nit.\nit. Runnable\nRunnable\nRunnable branch.\nbranch.\nbranch. Perfect.\nPerfect.\nPerfect. So now in this runnable branch I will\nSo now in this runnable branch I will\nSo now in this runnable branch I will define all the things. All the things.\ndefine all the things. All the things.\ndefine all the things. All the things. All the conditional branches. All the.\nAll the conditional branches. All the.\nAll the conditional branches. All the. So first condition is what? First\nSo first condition is what? First\nSo first condition is what? First condition is this one. So I will simply\ncondition is this one. So I will simply\ncondition is this one. So I will simply write condition. Okay. You can also name\nwrite condition. Okay. You can also name\nwrite condition. Okay. You can also name it. It's your choice. But usually we\nit. It's your choice. But usually we\nit. It's your choice. But usually we simply write a function directly.\nsimply write a function directly.\nsimply write a function directly. Because just tell me one thing. We want\nBecause just tell me one thing. We want\nBecause just tell me one thing. We want to go to this chain. If the answer is\nto go to this chain. If the answer is\nto go to this chain. If the answer is yes. So we obviously need to define that\nyes. So we obviously need to define that\nyes. So we obviously need to define that particular function. Okay. So we'll be\nparticular function. Okay. So we'll be\nparticular function. Okay. So we'll be using the lambda function. So I will say\nusing the lambda function. So I will say\nusing the lambda function. So I will say lambda x\nlambda x\nlambda x okay lambda x and then I will say\nokay lambda x and then I will say\nokay lambda x and then I will say positive in\npositive in\npositive in or let's say positive in x\nor let's say positive in x\nor let's say positive in x make sense if that is the scenario if\nmake sense if that is the scenario if\nmake sense if that is the scenario if lambda is there then simply\nlambda is there then simply\nlambda is there then simply you need to\nyou need to\nyou need to run my function which is the chain link\nrun my function which is the chain link\nrun my function which is the chain link let's say if positive we want to run\nlet's say if positive we want to run\nlet's say if positive we want to run LinkedIn chain otherwise um you can say\nLinkedIn chain otherwise um you can say\nLinkedIn chain otherwise um you can say chain LinkedIn\nchain LinkedIn\nchain LinkedIn and we can also run chain install\ninstall Instagram not insta chain\ninstall Instagram not insta chain runnable make sense so these are the two\nrunnable make sense so these are the two\nrunnable make sense so these are the two conditions that I have created if this\nconditions that I have created if this\nconditions that I have created if this is true then this and otherwise it will\nis true then this and otherwise it will\nis true then this and otherwise it will be like this make sense and then default\nbe like this make sense and then default\nbe like this make sense and then default we can simply say chain default we do\nwe can simply say chain default we do\nwe can simply say chain default we do not have any kind of chain default so we\nnot have any kind of chain default so we\nnot have any kind of chain default so we will simply leave it as is this so this\nwill simply leave it as is this so this\nwill simply leave it as is this so this is my conditional chain default but much\nis my conditional chain default but much\nis my conditional chain default but much oh man what is this so we can simply say\noh man what is this so we can simply say\noh man what is this so we can simply say default in this one because we just have\ndefault in this one because we just have\ndefault in this one because we just have to so I will simply say default\nto so I will simply say default\nto so I will simply say default equals Equals like you can just say\nequals Equals like you can just say\nequals Equals like you can just say default equals or you can just simply\ndefault equals or you can just simply\ndefault equals or you can just simply write it like normal chain let's say\nwrite it like normal chain let's say\nwrite it like normal chain let's say chain\nchain\nchain Instagram\ninstall chain runnable perfect\ninstall chain runnable perfect now what do you want got an unexpected\nnow what do you want got an unexpected\nnow what do you want got an unexpected yeah I didn't pass this thing this auto\nyeah I didn't pass this thing this auto\nyeah I didn't pass this thing this auto suggestion pass this thing okay so now\nsuggestion pass this thing okay so now\nsuggestion pass this thing okay so now this is my conditional chain so now we\nthis is my conditional chain so now we\nthis is my conditional chain so now we can simply create our final\ncan simply create our final\ncan simply create our final orchestration and it will look like\nFinal\nFinal orchestrator\norchestrator\norchestrator equals first of all we need to write\nequals first of all we need to write\nequals first of all we need to write this all of these things right this\nthis all of these things right this\nthis all of these things right this insta chain runnable and blah blah blah\ninsta chain runnable and blah blah blah\ninsta chain runnable and blah blah blah blah blah right and this is not parallel\nblah blah right and this is not parallel\nblah blah right and this is not parallel chain we will simply say conditional\nchain we will simply say conditional\nchain we will simply say conditional chain\nperfect\nperfect and here it will become\nand here it will become\nand here it will become conditional chain two so from where we\nconditional chain two so from where we\nconditional chain two so from where we are starting our chain from Here\nare starting our chain from Here\nare starting our chain from Here obviously\nobviously\nobviously we started our ch with this prompt\nwe started our ch with this prompt\nwe started our ch with this prompt template then lmm structure output right\ntemplate then lmm structure output right\ntemplate then lmm structure output right then pyntic JSON so let's write it and I\nthen pyntic JSON so let's write it and I\nthen pyntic JSON so let's write it and I don't know why this model this\ndon't know why this model this\ndon't know why this model this anti-gravity model is not picking right\nanti-gravity model is not picking right\nanti-gravity model is not picking right context super super high premium coders\ncontext super super high premium coders\ncontext super super high premium coders see in front of you so final\nsee in front of you so final\nsee in front of you so final orchestrator equals and we can simply\norchestrator equals and we can simply\norchestrator equals and we can simply define it prompt template and then\ndefine it prompt template and then\ndefine it prompt template and then conditional\nconditional\nconditional idiot lm structured output\nidiot lm structured output\nidiot lm structured output and then we have that condition that um\nand then we have that condition that um\nand then we have that condition that um pyantic JSON function\npyantic JSON function\npyantic JSON function pentic JSON lambda and then we have\npentic JSON lambda and then we have\npentic JSON lambda and then we have conditional chain make sense let's run\nconditional chain make sense let's run\nconditional chain make sense let's run this and let's try to\nrun this\nrun this and what do I need to do I simply need\nand what do I need to do I simply need\nand what do I need to do I simply need to pass the input and input how does it\nto pass the input and input how does it\nto pass the input and input how does it look like I think we have the input as\nlook like I think we have the input as\nlook like I think we have the input as input. Okay. So let's try to give the\ninput. Okay. So let's try to give the\ninput. Okay. So let's try to give the input and input will be I love this\ninput and input will be I love this\ninput and input will be I love this movie or let's say this KGF movie. Wow.\nmovie or let's say this KGF movie. Wow.\nmovie or let's say this KGF movie. Wow. Now let's see what it does.\nWe should see like two things. No, not\nWe should see like two things. No, not two things like just one thing.\ntwo things like just one thing.\ntwo things like just one thing. Positive. Perfect. So it has written the\nPositive. Perfect. So it has written the\nPositive. Perfect. So it has written the LinkedIn post for positive. Why? Because\nLinkedIn post for positive. Why? Because\nLinkedIn post for positive. Why? Because it actually evaluated our you can say\nit actually evaluated our you can say\nit actually evaluated our you can say prompt whatever we have given as\nprompt whatever we have given as\nprompt whatever we have given as positive make sense and you know how I\npositive make sense and you know how I\npositive make sense and you know how I know this was like a little bit complex\nknow this was like a little bit complex\nknow this was like a little bit complex but that's how you grow that's how you\nbut that's how you grow that's how you\nbut that's how you grow that's how you learn and I hope now you have a clear\nlearn and I hope now you have a clear\nlearn and I hope now you have a clear understanding of all the branches all\nunderstanding of all the branches all\nunderstanding of all the branches all the things and everything now let's jump\nthe things and everything now let's jump\nthe things and everything now let's jump on to the next chapter and this chapter\non to the next chapter and this chapter\non to the next chapter and this chapter is my personal favorite as well because\nis my personal favorite as well because\nis my personal favorite as well because this is very very special in this we\nthis is very very special in this we\nthis is very very special in this we will be covering\nwill be covering\nwill be covering First of all,\nFirst of all,\nFirst of all, yes, finally react\nyes, finally react\nyes, finally react agent\nagent\nagent intro.\nintro.\nintro. Perfect. So now you would have heard\nPerfect. So now you would have heard\nPerfect. So now you would have heard about this word, this keyword, whatever\nabout this word, this keyword, whatever\nabout this word, this keyword, whatever you want to say, it's called React.\nyou want to say, it's called React.\nyou want to say, it's called React. Let's talk about it because it is one of\nLet's talk about it because it is one of\nLet's talk about it because it is one of the most popular keyword, one of the\nthe most popular keyword, one of the\nthe most popular keyword, one of the most popular jargon we use in the\nmost popular jargon we use in the\nmost popular jargon we use in the agentic AI and AI data engineering\nagentic AI and AI data engineering\nagentic AI and AI data engineering world. It is called\nworld. It is called\nworld. It is called react.\nNow this word is not react\nNow this word is not react bro. This is react. See\nbro. This is react. See\nbro. This is react. See this is not like react. This is react.\nthis is not like react. This is react.\nthis is not like react. This is react. What does it mean? React is actually\nWhat does it mean? React is actually\nWhat does it mean? React is actually made up of two words.\nmade up of two words.\nmade up of two words. Reasoning\nacting. Basically, it is reasoning plus\nacting. Basically, it is reasoning plus evaluation plus acting or reasoning plus\nevaluation plus acting or reasoning plus\nevaluation plus acting or reasoning plus observation plus acting. But this is the\nobservation plus acting. But this is the\nobservation plus acting. But this is the highle definition. So let me show you\nhighle definition. So let me show you\nhighle definition. So let me show you react agent.\nreact agent.\nreact agent. uh react agent full form.\nuh react agent full form.\nuh react agent full form. Yeah, reasoning plus acting. Yeah,\nYeah, reasoning plus acting. Yeah,\nYeah, reasoning plus acting. Yeah, that's it. Like that is the you can say\nthat's it. Like that is the you can say\nthat's it. Like that is the you can say core of everything. Reasoning plus\ncore of everything. Reasoning plus\ncore of everything. Reasoning plus acting. There's a middle thing as well\nacting. There's a middle thing as well\nacting. There's a middle thing as well which is called evaluation or\nwhich is called evaluation or\nwhich is called evaluation or observation. But I will just explain you\nobservation. But I will just explain you\nobservation. But I will just explain you everything step by step. Now just sit\neverything step by step. Now just sit\neverything step by step. Now just sit back and relax because you need to enjoy\nback and relax because you need to enjoy\nback and relax because you need to enjoy this thing because this is very\nthis thing because this is very\nthis thing because this is very important because once you understand\nimportant because once you understand\nimportant because once you understand the React architecture, boom, you\nthe React architecture, boom, you\nthe React architecture, boom, you understood almost everything. Really?\nunderstood almost everything. Really?\nunderstood almost everything. Really? Yes. you you would have already\nYes. you you would have already\nYes. you you would have already understood so many things but this is\nunderstood so many things but this is\nunderstood so many things but this is amazing. This is the hot hottest part of\namazing. This is the hot hottest part of\namazing. This is the hot hottest part of the entire thing react. So let's say\nthe entire thing react. So let's say\nthe entire thing react. So let's say you want to build an agent.\nyou want to build an agent.\nyou want to build an agent. Okay, let's say you want to build an\nOkay, let's say you want to build an\nOkay, let's say you want to build an agent.\nagent.\nagent. Okay, let's\nOkay, let's\nOkay, let's build an agent.\nbuild an agent.\nbuild an agent. Let's say you want to build a react\nLet's say you want to build a react\nLet's say you want to build a react agent\nbecause everyone is just trying to say I\nbecause everyone is just trying to say I want to build react agent. I want to\nwant to build react agent. I want to\nwant to build react agent. I want to build react agent. I want to build react\nbuild react agent. I want to build react\nbuild react agent. I want to build react agent. Now how to build react agent? Why\nagent. Now how to build react agent? Why\nagent. Now how to build react agent? Why to build react agent? So we know that\nto build react agent? So we know that\nto build react agent? So we know that react means reasoning plus acting. Okay.\nreact means reasoning plus acting. Okay.\nreact means reasoning plus acting. Okay. Now do you know what what does it mean?\nNow do you know what what does it mean?\nNow do you know what what does it mean? Let's say there is Rahul. Bro, come\nLet's say there is Rahul. Bro, come\nLet's say there is Rahul. Bro, come here.\nhere.\nhere. So let's say this is Rahul. This Rahul\nSo let's say this is Rahul. This Rahul\nSo let's say this is Rahul. This Rahul wants to build an agent.\nwants to build an agent.\nwants to build an agent. Okay. So that whenever there is a user\nOkay. So that whenever there is a user\nOkay. So that whenever there is a user who sends the input,\nwho sends the input,\nwho sends the input, that's what users do, right? Input.\nthat's what users do, right? Input.\nthat's what users do, right? Input. Okay,\nOkay,\nOkay, make sense?\nmake sense?\nmake sense? This user will send the input.\nThis user will send the input.\nThis user will send the input. Okay. Yes. Now do you know what?\nOkay. Yes. Now do you know what?\nOkay. Yes. Now do you know what? Now currently we are using um really\nNow currently we are using um really\nNow currently we are using um really really high level and advanced agents\nreally high level and advanced agents\nreally high level and advanced agents LLMs provided by OpenAI which are\nLLMs provided by OpenAI which are\nLLMs provided by OpenAI which are amazing\namazing\namazing which are actually answering all of\nwhich are actually answering all of\nwhich are actually answering all of those things that it knows right but\nthose things that it knows right but\nthose things that it knows right but there are some questions that LM does\nthere are some questions that LM does\nthere are some questions that LM does not know. Oh really really what are\nnot know. Oh really really what are\nnot know. Oh really really what are those question? LLM knows everything.\nthose question? LLM knows everything.\nthose question? LLM knows everything. Let me show you. If I ask LLM this\nLet me show you. If I ask LLM this\nLet me show you. If I ask LLM this question,\nquestion,\nquestion, if I ask LLM this question, let me just\nif I ask LLM this question, let me just\nif I ask LLM this question, let me just bring the code\nbring the code\nbring the code conditional chains and this is my code.\nconditional chains and this is my code.\nconditional chains and this is my code. So let me just ask this simple question\nlm open a which is the best model one of\nlm open a which is the best model one of the best model that we have in the\nthe best model that we have in the\nthe best model that we have in the market. Okay. dot invoke I will say hey\nmarket. Okay. dot invoke I will say hey\nmarket. Okay. dot invoke I will say hey what's my name\nwhat's my name\nwhat's my name simple simple question let me just run\nsimple simple question let me just run\nsimple simple question let me just run this\nthis\nthis let's see what it says\nlet's see what it says\nlet's see what it says I do not know your name would you like\nI do not know your name would you like\nI do not know your name would you like me to call you if you want tell me now I\nme to call you if you want tell me now I\nme to call you if you want tell me now I will use it for this chat simple so it\nwill use it for this chat simple so it\nwill use it for this chat simple so it does not know my name I don't know why\ndoes not know my name I don't know why\ndoes not know my name I don't know why but yeah it doesn't know my name so what\nbut yeah it doesn't know my name so what\nbut yeah it doesn't know my name so what does it mean that means LLMs are not\ndoes it mean that means LLMs are not\ndoes it mean that means LLMs are not trained on your personal data. LLMs are\ntrained on your personal data. LLMs are\ntrained on your personal data. LLMs are not trained on your organization's data.\nnot trained on your organization's data.\nnot trained on your organization's data. LLMs are not trained on data that it\nLLMs are not trained on data that it\nLLMs are not trained on data that it doesn't have access to. Only you have\ndoesn't have access to. Only you have\ndoesn't have access to. Only you have access. So what to do in that scenario?\naccess. So what to do in that scenario?\naccess. So what to do in that scenario? For example, let's say you have a\nFor example, let's say you have a\nFor example, let's say you have a database. You have a data warehouse in\ndatabase. You have a data warehouse in\ndatabase. You have a data warehouse in your organization and you want to talk\nyour organization and you want to talk\nyour organization and you want to talk to that database. You want to just let's\nto that database. You want to just let's\nto that database. You want to just let's say make some API calls, right? Let's\nsay make some API calls, right? Let's\nsay make some API calls, right? Let's say you want to just you you have a um\nsay you want to just you you have a um\nsay you want to just you you have a um posgress database and you can just use\nposgress database and you can just use\nposgress database and you can just use psychopg um library to connect to it and\npsychopg um library to connect to it and\npsychopg um library to connect to it and you just want to grab the data as a\nyou just want to grab the data as a\nyou just want to grab the data as a result or let's say you have data\nresult or let's say you have data\nresult or let's say you have data warehouse and AWS and Azure you can use\nwarehouse and AWS and Azure you can use\nwarehouse and AWS and Azure you can use the API calls to get the data make sense\nthe API calls to get the data make sense\nthe API calls to get the data make sense like you can just do that and you know\nlike you can just do that and you know\nlike you can just do that and you know how to build those Python functions yes\nhow to build those Python functions yes\nhow to build those Python functions yes but how NLM will do that because LLM\nbut how NLM will do that because LLM\nbut how NLM will do that because LLM doesn't have access to do to do to to do\ndoesn't have access to do to do to to do\ndoesn't have access to do to do to to do that and how What we'll do then in that\nthat and how What we'll do then in that\nthat and how What we'll do then in that scenario we simply create something\nscenario we simply create something\nscenario we simply create something called as tools. What do we create\ncalled as tools. What do we create\ncalled as tools. What do we create tools? So let's say we have some tools.\ntools? So let's say we have some tools.\ntools? So let's say we have some tools. Let's say we have a tool for our\nLet's say we have a tool for our\nLet's say we have a tool for our postgress. Let's say right or let me\npostgress. Let's say right or let me\npostgress. Let's say right or let me just use postgrace.\njust use postgrace.\njust use postgrace. Perfect. So I have this function postgra\nPerfect. So I have this function postgra\nPerfect. So I have this function postgra database that can access this postgrace.\ndatabase that can access this postgrace.\ndatabase that can access this postgrace. Okay. Then let's say I also have access\nOkay. Then let's say I also have access\nOkay. Then let's say I also have access to um email\nto um email\nto um email right\nright\nright because obviously LLM cannot send an\nbecause obviously LLM cannot send an\nbecause obviously LLM cannot send an email to uh me. Let me show you. If I\nemail to uh me. Let me show you. If I\nemail to uh me. Let me show you. If I say\nsay\nsay uh\nuh\nuh send an email to\nsend an email to\nsend an email to anal lamb blah blah.com.\nYeah. at the rate gmail.\nYeah. at the rate gmail. Right? Let me just write this.\nIs it actually sending an email?\nIs it actually sending an email? See,\nSee,\nSee, I can't send email directly, but I can\nI can't send email directly, but I can\nI can't send email directly, but I can draft you one copy. See, it cannot send\ndraft you one copy. See, it cannot send\ndraft you one copy. See, it cannot send the actual email. It cannot. So, what\nthe actual email. It cannot. So, what\nthe actual email. It cannot. So, what does it mean? That means it doesn't have\ndoes it mean? That means it doesn't have\ndoes it mean? That means it doesn't have the capability to send an email. Let's\nthe capability to send an email. Let's\nthe capability to send an email. Let's say it also wants to um do something\nsay it also wants to um do something\nsay it also wants to um do something with your let's say code. It doesn't\nwith your let's say code. It doesn't\nwith your let's say code. It doesn't have access to access to do that. It can\nhave access to access to do that. It can\nhave access to access to do that. It can answer your questions but it doesn't\nanswer your questions but it doesn't\nanswer your questions but it doesn't have the capability to perform these\nhave the capability to perform these\nhave the capability to perform these things. Then who can perform these\nthings. Then who can perform these\nthings. Then who can perform these things? Who has the access and who has\nthings? Who has the access and who has\nthings? Who has the access and who has the capability to do all of these\nthe capability to do all of these\nthe capability to do all of these things? You. Okay. So, should we become\nthings? You. Okay. So, should we become\nthings? You. Okay. So, should we become LLM now? No, not really. I didn't say\nLLM now? No, not really. I didn't say\nLLM now? No, not really. I didn't say that. No, we do not need to become an\nthat. No, we do not need to become an\nthat. No, we do not need to become an LLM. Now you need to create the tools.\nLLM. Now you need to create the tools.\nLLM. Now you need to create the tools. You need to create the functions for\nYou need to create the functions for\nYou need to create the functions for these things. One function for this,\nthese things. One function for this,\nthese things. One function for this, one function for this, one function for\none function for this, one function for\none function for this, one function for this. Okay.\nthis. Okay.\nthis. Okay. Okay. These are our functions.\nOkay. These are our functions.\nOkay. These are our functions. Okay. These are our functions. Perfect.\nOkay. These are our functions. Perfect.\nOkay. These are our functions. Perfect. Perfect. And you will create something\nPerfect. And you will create something\nPerfect. And you will create something called as tools or basically toolkit.\ncalled as tools or basically toolkit.\ncalled as tools or basically toolkit. These are tools. Tools are what?\nThese are tools. Tools are what?\nThese are tools. Tools are what? Functions, right? So when when I combine\nFunctions, right? So when when I combine\nFunctions, right? So when when I combine all of these things when I combine all\nall of these things when I combine all\nall of these things when I combine all of these things what it will what it\nof these things what it will what it\nof these things what it will what it will become toolkit right this is my\nwill become toolkit right this is my\nwill become toolkit right this is my toolkit\ntoolkit\ntoolkit where we have all the tools written this\nwhere we have all the tools written this\nwhere we have all the tools written this is my toolkit\nis my toolkit\nis my toolkit make sense anything anything fancy we\nmake sense anything anything fancy we\nmake sense anything anything fancy we have discussed nothing so simple\nhave discussed nothing so simple\nhave discussed nothing so simple so so so simple\nso so so simple\nso so so simple perfect so now I have this toolkit do\nperfect so now I have this toolkit do\nperfect so now I have this toolkit do you know what will happen\nyou know what will happen\nyou know what will happen this thing,\nthis thing,\nthis thing, this LLM will connect to to this\nthis LLM will connect to to this\nthis LLM will connect to to this toolkit,\ntoolkit,\ntoolkit, okay, to this toolkit and it can make\nokay, to this toolkit and it can make\nokay, to this toolkit and it can make use of these functions.\nuse of these functions.\nuse of these functions. Really? Yes. It can talk to these tools\nReally? Yes. It can talk to these tools\nReally? Yes. It can talk to these tools in the back end. Wow. Now, what we did,\nin the back end. Wow. Now, what we did,\nin the back end. Wow. Now, what we did, we actually increase the power of our\nwe actually increase the power of our\nwe actually increase the power of our LLM.\nLLM.\nLLM. And this step is called tool binding.\nAnd this step is called tool binding.\nAnd this step is called tool binding. This is what\nThis is what\nThis is what tool binding. We basically bind our LLM\ntool binding. We basically bind our LLM\ntool binding. We basically bind our LLM with the tools. That's why it is called\nwith the tools. That's why it is called\nwith the tools. That's why it is called tool binding. Okay. Okay. Makes sense.\ntool binding. Okay. Okay. Makes sense.\ntool binding. Okay. Okay. Makes sense. Tool binding. Right? Do you know what\nTool binding. Right? Do you know what\nTool binding. Right? Do you know what will happen now?\nwill happen now?\nwill happen now? Now my input\nNow my input\nNow my input my input if this input\ncan be answered directly by LLM directly\ncan be answered directly by LLM directly by LLM. It will simply give the response\nby LLM. It will simply give the response\nby LLM. It will simply give the response take it like this. It will simply say\ntake it like this. It will simply say\ntake it like this. It will simply say hey you want the response okay let me\nhey you want the response okay let me\nhey you want the response okay let me just give you the response and this is\njust give you the response and this is\njust give you the response and this is the response take it like this.\nthe response take it like this.\nthe response take it like this. Simple right? But\nSimple right? But\nSimple right? But when this LLM doesn't have this\nwhen this LLM doesn't have this\nwhen this LLM doesn't have this capability to do that work, it will look\ncapability to do that work, it will look\ncapability to do that work, it will look for its tools. It will look for its\nfor its tools. It will look for its\nfor its tools. It will look for its toolkit. It will search for the relevant\ntoolkit. It will search for the relevant\ntoolkit. It will search for the relevant tool. Hey, let me just check. Do I have\ntool. Hey, let me just check. Do I have\ntool. Hey, let me just check. Do I have that tool to perform that action? This\nthat tool to perform that action? This\nthat tool to perform that action? This user has asked me to send an email. Do I\nuser has asked me to send an email. Do I\nuser has asked me to send an email. Do I have the tool? Let me just first of all\nhave the tool? Let me just first of all\nhave the tool? Let me just first of all check it. It will simply search for that\ncheck it. It will simply search for that\ncheck it. It will simply search for that particular toolkit,\nparticular toolkit,\nparticular toolkit, right? It will search for that toolkit.\nright? It will search for that toolkit.\nright? It will search for that toolkit. Now\nNow\nNow that tool from that toolkit will be used\nthat tool from that toolkit will be used\nthat tool from that toolkit will be used if that can do the work. Let's say this\nif that can do the work. Let's say this\nif that can do the work. Let's say this email tool is there. So what will\nemail tool is there. So what will\nemail tool is there. So what will happen? It will\nhappen? It will\nhappen? It will it will\nit will\nit will simply\nsimply\nsimply go to its toolkit. Okay, go to its\ngo to its toolkit. Okay, go to its\ngo to its toolkit. Okay, go to its toolkit.\ntoolkit.\ntoolkit. to search for the tool. Let's say this\nto search for the tool. Let's say this\nto search for the tool. Let's say this toolkit has got the response. Now, do\ntoolkit has got the response. Now, do\ntoolkit has got the response. Now, do you know what will happen? This tool\nyou know what will happen? This tool\nyou know what will happen? This tool will not send the answer directly back\nwill not send the answer directly back\nwill not send the answer directly back to the user. No, no, no, no, no, no, no,\nto the user. No, no, no, no, no, no, no,\nto the user. No, no, no, no, no, no, no, no, no. This will give the response back\nno, no. This will give the response back\nno, no. This will give the response back to the React agent.\nto the React agent.\nto the React agent. Okay?\nOkay?\nOkay? Like this. This will be a loop. So, what\nLike this. This will be a loop. So, what\nLike this. This will be a loop. So, what did I say? This React agent will search\ndid I say? This React agent will search\ndid I say? This React agent will search for the tool. It will make the use of\nfor the tool. It will make the use of\nfor the tool. It will make the use of the tool. The output of that tool will\nthe tool. The output of that tool will\nthe tool. The output of that tool will go back to the LLM. Basically, the React\ngo back to the LLM. Basically, the React\ngo back to the LLM. Basically, the React engine that we are building. Under the\nengine that we are building. Under the\nengine that we are building. Under the word, it is just a just an LLM, right?\nword, it is just a just an LLM, right?\nword, it is just a just an LLM, right? So, this will go back to the LLM. It\nSo, this will go back to the LLM. It\nSo, this will go back to the LLM. It will observe it. Hey, this tool has\nwill observe it. Hey, this tool has\nwill observe it. Hey, this tool has generated this output.\ngenerated this output.\ngenerated this output. Is it um good enough to go ahead or\nIs it um good enough to go ahead or\nIs it um good enough to go ahead or should I just make another tool tool\nshould I just make another tool tool\nshould I just make another tool tool call? Let's say this email tool has done\ncall? Let's say this email tool has done\ncall? Let's say this email tool has done the work and has not actually completed\nthe work and has not actually completed\nthe work and has not actually completed the work. It needs to make another tool\nthe work. It needs to make another tool\nthe work. It needs to make another tool call. It will again go back to the\ncall. It will again go back to the\ncall. It will again go back to the toolkit and it will search for the task\ntoolkit and it will search for the task\ntoolkit and it will search for the task that it's spending.\nthat it's spending.\nthat it's spending. Then it will bring that tool and then it\nThen it will bring that tool and then it\nThen it will bring that tool and then it will just give the response back and\nwill just give the response back and\nwill just give the response back and then it will just observe it. This step\nthen it will just observe it. This step\nthen it will just observe it. This step is called evaluation. This step is\nis called evaluation. This step is\nis called evaluation. This step is called basically um you can say\ncalled basically um you can say\ncalled basically um you can say reasoning or basically observation. So\nreasoning or basically observation. So\nreasoning or basically observation. So that is why it is reasoning plus acting.\nthat is why it is reasoning plus acting.\nthat is why it is reasoning plus acting. That means it will first of all call it.\nThat means it will first of all call it.\nThat means it will first of all call it. It will take action. Then it will just\nIt will take action. Then it will just\nIt will take action. Then it will just do the reasoning part as well. It will\ndo the reasoning part as well. It will\ndo the reasoning part as well. It will also observe the output. Once LLM is\nalso observe the output. Once LLM is\nalso observe the output. Once LLM is satisfied that I have the final answer\nsatisfied that I have the final answer\nsatisfied that I have the final answer only then it will generate the output\nonly then it will generate the output\nonly then it will generate the output only then\nonly then\nonly then and if it is not satisfied with the\nand if it is not satisfied with the\nand if it is not satisfied with the output it will not generate the output\noutput it will not generate the output\noutput it will not generate the output and it will keep on searching for the\nand it will keep on searching for the\nand it will keep on searching for the tools.\ntools.\ntools. Yes, that is the entire concept of React\nYes, that is the entire concept of React\nYes, that is the entire concept of React and this is a kind of agent that\nand this is a kind of agent that\nand this is a kind of agent that everyone tries to build nowadays because\neveryone tries to build nowadays because\neveryone tries to build nowadays because this is a kind of autonomous agent\nthis is a kind of autonomous agent\nthis is a kind of autonomous agent because you have the tools, you just\nbecause you have the tools, you just\nbecause you have the tools, you just create a wide range of tools or let's\ncreate a wide range of tools or let's\ncreate a wide range of tools or let's say long list of tools and your React\nsay long list of tools and your React\nsay long list of tools and your React agent can actually do everything\nagent can actually do everything\nagent can actually do everything but you need to understand all the ins\nbut you need to understand all the ins\nbut you need to understand all the ins and outs of this thing because it is not\nand outs of this thing because it is not\nand outs of this thing because it is not very easy to understand it. Let me be\nvery easy to understand it. Let me be\nvery easy to understand it. Let me be very honest. Yes, I will show you the\nvery honest. Yes, I will show you the\nvery honest. Yes, I will show you the code to implement this thing first of\ncode to implement this thing first of\ncode to implement this thing first of all. so that you can actually see what\nall. so that you can actually see what\nall. so that you can actually see what is going on, how the things are going.\nis going on, how the things are going.\nis going on, how the things are going. Then I will be breaking down each and\nThen I will be breaking down each and\nThen I will be breaking down each and everything like how it is doing\neverything like how it is doing\neverything like how it is doing everything behind the scenes because in\neverything behind the scenes because in\neverything behind the scenes because in lang chain you do not see behind the\nlang chain you do not see behind the\nlang chain you do not see behind the scenes. No, you do not see it. If you\nscenes. No, you do not see it. If you\nscenes. No, you do not see it. If you like if you know like the things in\nlike if you know like the things in\nlike if you know like the things in detail in in depth then obviously you\ndetail in in depth then obviously you\ndetail in in depth then obviously you can just see it by default it will not\ncan just see it by default it will not\ncan just see it by default it will not tell you how it is doing that. I will\ntell you how it is doing that. I will\ntell you how it is doing that. I will just show you how it does that behind\njust show you how it does that behind\njust show you how it does that behind the scenes and why it is very useful\nthe scenes and why it is very useful\nthe scenes and why it is very useful because then you can just customize the\nbecause then you can just customize the\nbecause then you can just customize the this agent as well for your own use case\nthis agent as well for your own use case\nthis agent as well for your own use case and again for the interviews it is like\nand again for the interviews it is like\nand again for the interviews it is like very very very handy very super very\nvery very very handy very super very\nvery very very handy very super very superful very superful very helpful\nsuperful very superful very helpful\nsuperful very superful very helpful I hope it makes sense\nI hope it makes sense\nI hope it makes sense I will also show you tool binding I will\nI will also show you tool binding I will\nI will also show you tool binding I will also show you how it is working make\nalso show you how it is working make\nalso show you how it is working make sense okay let's see first of all the\nsense okay let's see first of all the\nsense okay let's see first of all the code implementation which is given in\ncode implementation which is given in\ncode implementation which is given in the documentation as well which is the\nthe documentation as well which is the\nthe documentation as well which is the first step then we going to deep Okay.\nfirst step then we going to deep Okay.\nfirst step then we going to deep Okay. Okay.\nOkay.\nOkay. So now let's just try to write the code\nSo now let's just try to write the code\nSo now let's just try to write the code and I can also show you the\nand I can also show you the\nand I can also show you the documentation.\ndocumentation.\ndocumentation. Let's see\nLet's see\nLet's see react and lang. And you know what there\nreact and lang. And you know what there\nreact and lang. And you know what there are so many tools available in the lang\nare so many tools available in the lang\nare so many tools available in the lang as well. So you do not need to create\nas well. So you do not need to create\nas well. So you do not need to create tools from scratch. There are tools such\ntools from scratch. There are tools such\ntools from scratch. There are tools such as Wikipedia search. There are tools\nas Wikipedia search. There are tools\nas Wikipedia search. There are tools such as called um duck the go search. If\nsuch as called um duck the go search. If\nsuch as called um duck the go search. If you just want to search for news, latest\nyou just want to search for news, latest\nyou just want to search for news, latest news. There are so many search. There is\nnews. There are so many search. There is\nnews. There are so many search. There is like tely search which for for from\nlike tely search which for for from\nlike tely search which for for from which you can just make some um search\nwhich you can just make some um search\nwhich you can just make some um search like you can just use tei as a search\nlike you can just use tei as a search\nlike you can just use tei as a search engine. So there are so many so many\nengine. So there are so many so many\nengine. So there are so many so many integrated tools as well. I will be\nintegrated tools as well. I will be\nintegrated tools as well. I will be using two to three and then you can just\nusing two to three and then you can just\nusing two to three and then you can just explore and learn. Okay. Um react agent\nexplore and learn. Okay. Um react agent\nexplore and learn. Okay. Um react agent lang chain. Okay. Where where is that\nlang chain. Okay. Where where is that\nlang chain. Okay. Where where is that documentation? I think documentation.\ndocumentation? I think documentation.\ndocumentation? I think documentation. Let's try to explore on our own lang\nLet's try to explore on our own lang\nLet's try to explore on our own lang chain and agents maybe here. Okay. So\nchain and agents maybe here. Okay. So\nchain and agents maybe here. Okay. So here they have just updated this thing.\nhere they have just updated this thing.\nhere they have just updated this thing. Earlier it was called as create react\nEarlier it was called as create react\nEarlier it was called as create react agent. But now they have just created a\nagent. But now they have just created a\nagent. But now they have just created a new function, new class whatever from\nnew function, new class whatever from\nnew function, new class whatever from langchain.gent import create react\nlangchain.gent import create react\nlangchain.gent import create react agent. So now you just create like this\nagent. So now you just create like this\nagent. So now you just create like this create agent and tools. See simple\ncreate agent and tools. See simple\ncreate agent and tools. See simple simple.\nsimple.\nsimple. Did you understand that? You simply need\nDid you understand that? You simply need\nDid you understand that? You simply need to define the model that you want to use\nto define the model that you want to use\nto define the model that you want to use and you want to just add the tools that\nand you want to just add the tools that\nand you want to just add the tools that you want to attach to the LM. That's it.\nyou want to attach to the LM. That's it.\nyou want to attach to the LM. That's it. And everything will be done on its own.\nAnd everything will be done on its own.\nAnd everything will be done on its own. See simple. But yes, you need to create\nSee simple. But yes, you need to create\nSee simple. But yes, you need to create the tools on your own and obviously you\nthe tools on your own and obviously you\nthe tools on your own and obviously you you need to define the tools how LLM\nyou need to define the tools how LLM\nyou need to define the tools how LLM would know like what tools it needs to\nwould know like what tools it needs to\nwould know like what tools it needs to just use or create right and there are\njust use or create right and there are\njust use or create right and there are some dynamic models as well which I'm\nsome dynamic models as well which I'm\nsome dynamic models as well which I'm not a big fan of it because if you just\nnot a big fan of it because if you just\nnot a big fan of it because if you just want to make a tweak I would rather\nwant to make a tweak I would rather\nwant to make a tweak I would rather stick with my own code to make some\nstick with my own code to make some\nstick with my own code to make some tweaks instead of using their decorators\ntweaks instead of using their decorators\ntweaks instead of using their decorators because they can just make changes to\nbecause they can just make changes to\nbecause they can just make changes to their code base anytime and they will\ntheir code base anytime and they will\ntheir code base anytime and they will simply say hey we have a new update\nsimply say hey we have a new update\nsimply say hey we have a new update available we have this v1 available\navailable we have this v1 available\navailable we have this v1 available And yes, your code base will be broken.\nAnd yes, your code base will be broken.\nAnd yes, your code base will be broken. I cannot just make my own decorators,\nI cannot just make my own decorators,\nI cannot just make my own decorators, bro. You do not need your decorators. So\nbro. You do not need your decorators. So\nbro. You do not need your decorators. So that's why I do not use um these\nthat's why I do not use um these\nthat's why I do not use um these decorators and all. If I want to make\ndecorators and all. If I want to make\ndecorators and all. If I want to make any tweaks, if I just want to make my\nany tweaks, if I just want to make my\nany tweaks, if I just want to make my agent dynamic, I will just write my own\nagent dynamic, I will just write my own\nagent dynamic, I will just write my own code and I will just make it. And yeah,\ncode and I will just make it. And yeah,\ncode and I will just make it. And yeah, that's me. Okay, so let's try to see\nthat's me. Okay, so let's try to see\nthat's me. Okay, so let's try to see what are the tools. Let me just click on\nwhat are the tools. Let me just click on\nwhat are the tools. Let me just click on tools. And here are the tools that we\ntools. And here are the tools that we\ntools. And here are the tools that we have. And yes, we can use the tool\nhave. And yes, we can use the tool\nhave. And yes, we can use the tool decorator. This is something which is\ndecorator. This is something which is\ndecorator. This is something which is stable. We can just use this. Okay. And\nstable. We can just use this. Okay. And\nstable. We can just use this. Okay. And yeah, it's I I like like using this tool\nyeah, it's I I like like using this tool\nyeah, it's I I like like using this tool definition. And there are so many tools.\ndefinition. And there are so many tools.\ndefinition. And there are so many tools. First of all, I just told you that you\nFirst of all, I just told you that you\nFirst of all, I just told you that you can create your own customized tool. So\ncan create your own customized tool. So\ncan create your own customized tool. So this is a definition for that. If let's\nthis is a definition for that. If let's\nthis is a definition for that. If let's say you want to sort something within a\nsay you want to sort something within a\nsay you want to sort something within a database, right? And you can just simply\ndatabase, right? And you can just simply\ndatabase, right? And you can just simply do it. If you want to use integrated\ndo it. If you want to use integrated\ndo it. If you want to use integrated tools, let me just see if they can just\ntools, let me just see if they can just\ntools, let me just see if they can just show you the integrated tools. Um\nshow you the integrated tools. Um\nshow you the integrated tools. Um integrated tools. No, they have not\nintegrated tools. No, they have not\nintegrated tools. No, they have not shown here. I can just search. Let me\nshown here. I can just search. Let me\nshown here. I can just search. Let me first of all close all these tabs. So I\nfirst of all close all these tabs. So I\nfirst of all close all these tabs. So I will simply say duck go search\nwill simply say duck go search\nwill simply say duck go search LinkedIn because that is like one of the\nLinkedIn because that is like one of the\nLinkedIn because that is like one of the tools.\ntools.\ntools. See so here we have oh yeah I think they\nSee so here we have oh yeah I think they\nSee so here we have oh yeah I think they have just moved the thing to\nhave just moved the thing to\nhave just moved the thing to integrations. Perfect. So if I just go\nintegrations. Perfect. So if I just go\nintegrations. Perfect. So if I just go to integrations\nto integrations\nto integrations I will see the\nI will see the\nI will see the all of these tools.\nall of these tools.\nall of these tools. popular providers I have tool for\npopular providers I have tool for\npopular providers I have tool for OpenAI, Google, Enthropic, AWS, Google,\nOpenAI, Google, Enthropic, AWS, Google,\nOpenAI, Google, Enthropic, AWS, Google, HuggingFace, Chroma, Pinecon. These are\nHuggingFace, Chroma, Pinecon. These are\nHuggingFace, Chroma, Pinecon. These are by by the vector databases and data\nby by the vector databases and data\nby by the vector databases and data bricks and Mistral. All of these things\nbricks and Mistral. All of these things\nbricks and Mistral. All of these things are here. Red is elastic search and all\nare here. Red is elastic search and all\nare here. Red is elastic search and all providers like we have everything here.\nproviders like we have everything here.\nproviders like we have everything here. These are the you can say the ones which\nThese are the you can say the ones which\nThese are the you can say the ones which are validated and there are some\nare validated and there are some\nare validated and there are some community providers as well. If I just\ncommunity providers as well. If I just\ncommunity providers as well. If I just open this one, I will see a long list of\nopen this one, I will see a long list of\nopen this one, I will see a long list of all the things. See all providers. Click\nall the things. See all providers. Click\nall the things. See all providers. Click here and then you will see all of the\nhere and then you will see all of the\nhere and then you will see all of the providers here. See? Wow. Such a big\nproviders here. See? Wow. Such a big\nproviders here. See? Wow. Such a big list of tools that you can just create.\nlist of tools that you can just create.\nlist of tools that you can just create. These are all of the you can say\nThese are all of the you can say\nThese are all of the you can say tool integrations whatever they have\ntool integrations whatever they have\ntool integrations whatever they have created duck go search duck db like so\ncreated duck go search duck db like so\ncreated duck go search duck db like so many things 11 labs. Wow. So you have so\nmany things 11 labs. Wow. So you have so\nmany things 11 labs. Wow. So you have so many things. So let's try to start with\nmany things. So let's try to start with\nmany things. So let's try to start with anything. We can also use IBM\nanything. We can also use IBM\nanything. We can also use IBM Jaguar. Wow,\nJaguar. Wow,\nJaguar. Wow, so many things. Wow, man. They have\nso many things. Wow, man. They have\nso many things. Wow, man. They have actually added so many things.\nactually added so many things.\nactually added so many things. You can also explore on your own. See,\nYou can also explore on your own. See,\nYou can also explore on your own. See, te is also there. Tele is basically the\nte is also there. Tele is basically the\nte is also there. Tele is basically the search engine that is very popular. And\nsearch engine that is very popular. And\nsearch engine that is very popular. And then we have wow so many things. We have\nthen we have wow so many things. We have\nthen we have wow so many things. We have Twitter as well. Wow. So now let's try\nTwitter as well. Wow. So now let's try\nTwitter as well. Wow. So now let's try to uh apply this thing react intro.\nto uh apply this thing react intro.\nto uh apply this thing react intro. Okay.\nOkay.\nOkay. So this is our\nSo this is our\nSo this is our import thing. Perfect. So now what we\nimport thing. Perfect. So now what we\nimport thing. Perfect. So now what we going to do? First of all, we will\ngoing to do? First of all, we will\ngoing to do? First of all, we will create our tools, right? Let's create\ncreate our tools, right? Let's create\ncreate our tools, right? Let's create our tools because that is the base.\nour tools because that is the base.\nour tools because that is the base. Okay. Let's create our tools. And don't\nOkay. Let's create our tools. And don't\nOkay. Let's create our tools. And don't worry, we will be just creating some\nworry, we will be just creating some\nworry, we will be just creating some amazing tools. Okay. So first of all,\namazing tools. Okay. So first of all,\namazing tools. Okay. So first of all, let's create the tool number one.\nlet's create the tool number one.\nlet's create the tool number one. And this will be let's say uh news\nAnd this will be let's say uh news\nAnd this will be let's say uh news search because let's say your user wants\nsearch because let's say your user wants\nsearch because let's say your user wants to get some news right so we can just\nto get some news right so we can just\nto get some news right so we can just have that tool and what is the basis of\nhave that tool and what is the basis of\nhave that tool and what is the basis of the new search tool it is called duck go\nthe new search tool it is called duck go\nthe new search tool it is called duck go search and I can just show you duck go\nsearch and I can just show you duck go\nsearch and I can just show you duck go search it's here so if you want to use\nsearch it's here so if you want to use\nsearch it's here so if you want to use duck go search it is free that's the\nduck go search it is free that's the\nduck go search it is free that's the best thing you do not even need any kind\nbest thing you do not even need any kind\nbest thing you do not even need any kind of API key because with some other\nof API key because with some other\nof API key because with some other search or new search tools you need some\nsearch or new search tools you need some\nsearch or new search tools you need some API keys but it is completely completely\nAPI keys but it is completely completely\nAPI keys but it is completely completely completely So how we can just use this\ncompletely So how we can just use this\ncompletely So how we can just use this tool? This is first of all very well\ntool? This is first of all very well\ntool? This is first of all very well integrated. Duck go search is actually a\nintegrated. Duck go search is actually a\nintegrated. Duck go search is actually a standalone um you can say service that\nstandalone um you can say service that\nstandalone um you can say service that you can just go to their website and you\nyou can just go to their website and you\nyou can just go to their website and you can just see how you can just use it um\ncan just see how you can just use it um\ncan just see how you can just use it um as an API as a as a you can say as a\nas an API as a as a you can say as a\nas an API as a as a you can say as a python function as a python API but you\npython function as a python API but you\npython function as a python API but you do not need to do this because even if\ndo not need to do this because even if\ndo not need to do this because even if you write this let's say python so you\nyou write this let's say python so you\nyou write this let's say python so you will get the code like how to make an\nwill get the code like how to make an\nwill get the code like how to make an API call using pi and then you can just\nAPI call using pi and then you can just\nAPI call using pi and then you can just see this but with lang chain you do not\nsee this but with lang chain you do not\nsee this but with lang chain you do not actually need to you do not need to even\nactually need to you do not need to even\nactually need to you do not need to even install this you actually do not even\ninstall this you actually do not even\ninstall this you actually do not even need to install this And you do not need\nneed to install this And you do not need\nneed to install this And you do not need to even write this Python function like\nto even write this Python function like\nto even write this Python function like this because that's how you just make\nthis because that's how you just make\nthis because that's how you just make API calls, right? Because we are simply\nAPI calls, right? Because we are simply\nAPI calls, right? Because we are simply writing the making the API calls to the\nwriting the making the API calls to the\nwriting the making the API calls to the function. But here we have lang chain\nfunction. But here we have lang chain\nfunction. But here we have lang chain integration uh and we can simply use\nintegration uh and we can simply use\nintegration uh and we can simply use this function tuck search run which is\nthis function tuck search run which is\nthis function tuck search run which is created by langchain. Okay. So what I\ncreated by langchain. Okay. So what I\ncreated by langchain. Okay. So what I will do I will simply copy this\nwill do I will simply copy this\nwill do I will simply copy this and\nand\nand I can simply go here from langchain\nI can simply go here from langchain\nI can simply go here from langchain community.tools tools import duck duck\ncommunity.tools tools import duck duck\ncommunity.tools tools import duck duck go search run and this is an instance\ngo search run and this is an instance\ngo search run and this is an instance for that okay let's say search tool it\nfor that okay let's say search tool it\nfor that okay let's say search tool it makes more sense and I'll simply say\nmakes more sense and I'll simply say\nmakes more sense and I'll simply say Obama's first name so what it will do it\nObama's first name so what it will do it\nObama's first name so what it will do it will make that tool call or let's say I\nwill make that tool call or let's say I\nwill make that tool call or let's say I want to search the news who is an lamba\nwant to search the news who is an lamba\nwant to search the news who is an lamba let's see if it knows\nlet's see if it knows\nlet's see if it knows search no module name langchain\nsearch no module name langchain\nsearch no module name langchain community yes we need to install this\ncommunity yes we need to install this\ncommunity yes we need to install this module which is called langchain\nmodule which is called langchain\nmodule which is called langchain community so I will Here\ncommunity so I will Here\ncommunity so I will Here this one\nthis one\nthis one do we have virtual enabled? I think they\ndo we have virtual enabled? I think they\ndo we have virtual enabled? I think they should add this feature where you can\nshould add this feature where you can\nshould add this feature where you can automatically enable the virtual\nautomatically enable the virtual\nautomatically enable the virtual environment.\nenvironment.\nenvironment. Why I'm writing this again and again?\nWhy I'm writing this again and again?\nWhy I'm writing this again and again? Activate activate activate\nActivate activate activate\nActivate activate activate UV add and then\nUV add and then\nUV add and then line chain community.\nline chain community.\nline chain community. Perfect.\nPerfect.\nPerfect. Now let's wait. Perfect. So now let me\nNow let's wait. Perfect. So now let me\nNow let's wait. Perfect. So now let me just rerun this.\njust rerun this.\njust rerun this. DDAX Python please install the pip\nDDAX Python please install the pip\nDDAX Python please install the pip install UDAX. What do you mean could not\ninstall UDAX. What do you mean could not\ninstall UDAX. What do you mean could not import DDGS Python package?\nimport DDGS Python package?\nimport DDGS Python package? Uh please install it with okay there\nUh please install it with okay there\nUh please install it with okay there would be something maybe dug search. So\nwould be something maybe dug search. So\nwould be something maybe dug search. So I will simply say UV pip install\nI will simply say UV pip install\nI will simply say UV pip install DDGS. Let's add it as well\nDDGS. Let's add it as well\nDDGS. Let's add it as well because it is saying that. So let's do\nbecause it is saying that. So let's do\nbecause it is saying that. So let's do that.\nthat.\nthat. Ooh,\nOoh,\nOoh, it knows man. Okay. Okay. Okay. That's\nit knows man. Okay. Okay. Okay. That's\nit knows man. Okay. Okay. Okay. That's good. H good. Okay.\ngood. H good. Okay.\ngood. H good. Okay. So what it has already like what it has\nSo what it has already like what it has\nSo what it has already like what it has done this is not generated by AI. This\ndone this is not generated by AI. This\ndone this is not generated by AI. This is a tool call. That means I made a tool\nis a tool call. That means I made a tool\nis a tool call. That means I made a tool call to duck go search run. Okay. And I\ncall to duck go search run. Okay. And I\ncall to duck go search run. Okay. And I simply wrote this thing.\nsimply wrote this thing.\nsimply wrote this thing. this thing who is Anchal Lamba basically\nthis thing who is Anchal Lamba basically\nthis thing who is Anchal Lamba basically a news I want to get some news some\na news I want to get some news some\na news I want to get some news some information regarding this question who\ninformation regarding this question who\ninformation regarding this question who is Anala so this answer is the response\nis Anala so this answer is the response\nis Anala so this answer is the response of this particular function called duck\nof this particular function called duck\nof this particular function called duck dug go search on this is not generated\ndug go search on this is not generated\ndug go search on this is not generated by llm\nby llm\nby llm this is not generated by lm yes we will\nthis is not generated by lm yes we will\nthis is not generated by lm yes we will bind this tool to lm so that it can make\nbind this tool to lm so that it can make\nbind this tool to lm so that it can make the use of this function but currently\nthe use of this function but currently\nthe use of this function but currently it is not connected to lm this is simply\nit is not connected to lm this is simply\nit is not connected to lm this is simply the response getting we are getting from\nthe response getting we are getting from\nthe response getting we are getting from the duck the go search make sense\nthe duck the go search make sense\nthe duck the go search make sense Perfect. Makes sense. Simple, simple,\nPerfect. Makes sense. Simple, simple,\nPerfect. Makes sense. Simple, simple, simple. Okay. So, this is our tool. Let\nsimple. Okay. So, this is our tool. Let\nsimple. Okay. So, this is our tool. Let me just hide it. Um, it's fine because\nme just hide it. Um, it's fine because\nme just hide it. Um, it's fine because why show up? So, search tool one tool is\nwhy show up? So, search tool one tool is\nwhy show up? So, search tool one tool is done. Let's say I want to add one more\ndone. Let's say I want to add one more\ndone. Let's say I want to add one more tool and it will be um which tool we\ntool and it will be um which tool we\ntool and it will be um which tool we should use because we have very long\nshould use because we have very long\nshould use because we have very long list and I'm a big fan of this tools\nlist and I'm a big fan of this tools\nlist and I'm a big fan of this tools list. If you can even go to integration\nlist. If you can even go to integration\nlist. If you can even go to integration and then tools and toolkits, you will\nand then tools and toolkits, you will\nand then tools and toolkits, you will get so much so much so much so much. So,\nget so much so much so much so much. So,\nget so much so much so much so much. So, you have Google Serper. See these are\nyou have Google Serper. See these are\nyou have Google Serper. See these are the paid ones, these are the free ones.\nthe paid ones, these are the free ones.\nthe paid ones, these are the free ones. So you can actually compare what you\nSo you can actually compare what you\nSo you can actually compare what you want to use for search, for code, for\nwant to use for search, for code, for\nwant to use for search, for code, for productivity. You have like so many\nproductivity. You have like so many\nproductivity. You have like so many things database, finance integration.\nthings database, finance integration.\nthings database, finance integration. And let's try to use one tool for um\nlet's say oh we have Gmail toolkit as\nlet's say oh we have Gmail toolkit as well. Wow, nice. Let's use for\nwell. Wow, nice. Let's use for\nwell. Wow, nice. Let's use for Wikipedia. Let's use Wikipedia.\nWikipedia. Let's use Wikipedia.\nWikipedia. Let's use Wikipedia. Wikipedia. Wikipedia. Wikipedia.\nWikipedia. Wikipedia. Wikipedia.\nWikipedia. Wikipedia. Wikipedia. Um\nUm\nUm where is Wikipedia? Yeah, here. Let's\nwhere is Wikipedia? Yeah, here. Let's\nwhere is Wikipedia? Yeah, here. Let's use Wikipedia. Okay. So, what I will do?\nuse Wikipedia. Okay. So, what I will do?\nuse Wikipedia. Okay. So, what I will do? I will copy this\nI will copy this\nI will copy this and I will\nand I will\nand I will Wikipedia search tool. I will write this\nWikipedia search tool. I will write this\nWikipedia search tool. I will write this and then I can simply go to\nand then I can simply go to\nand then I can simply go to this wrapper because we want to run this\nthis wrapper because we want to run this\nthis wrapper because we want to run this and let's write this. So, this is like\nand let's write this. So, this is like\nand let's write this. So, this is like Wikipedia tool. So, I can say Wikipedia\nWikipedia tool. So, I can say Wikipedia\nWikipedia tool. So, I can say Wikipedia tool. This is source tool. Make sense?\ntool. This is source tool. Make sense?\ntool. This is source tool. Make sense? Okay. So now if you just want to test it\nOkay. So now if you just want to test it\nOkay. So now if you just want to test it uh the same way we tested is uh pip\nuh the same way we tested is uh pip\nuh the same way we tested is uh pip install Wikipedia. Okay. Let's install\ninstall Wikipedia. Okay. Let's install\ninstall Wikipedia. Okay. Let's install Wikipedia. Not a big deal.\nWikipedia. Not a big deal.\nWikipedia. Not a big deal. Perfect. Now let me just run this.\nPerfect. Now let me just run this.\nPerfect. Now let me just run this. Now let me just test it.\nNow let me just test it.\nNow let me just test it. Wikipedia tool do invoke.\nWikipedia tool do invoke.\nWikipedia tool do invoke. Let's say what is the capital of France?\nLet's say what is the capital of France?\nLet's say what is the capital of France? It should know right because this is\nIt should know right because this is\nIt should know right because this is Wikipedia. Wikipedia knows everything.\nWikipedia. Wikipedia knows everything.\nWikipedia. Wikipedia knows everything. page closed. See what is this thing? You\npage closed. See what is this thing? You\npage closed. See what is this thing? You are actually getting everything because\nare actually getting everything because\nare actually getting everything because when you just make a call to Wikipedia\nwhen you just make a call to Wikipedia\nwhen you just make a call to Wikipedia tool, you are actually getting the\ntool, you are actually getting the\ntool, you are actually getting the response of the entire Wikipedia, right?\nresponse of the entire Wikipedia, right?\nresponse of the entire Wikipedia, right? So this is that particular thing. That's\nSo this is that particular thing. That's\nSo this is that particular thing. That's why you're simply seeing everything. So\nwhy you're simply seeing everything. So\nwhy you're simply seeing everything. So you are saying France but before France,\nyou are saying France but before France,\nyou are saying France but before France, you are also seeing this garbage stuff\nyou are also seeing this garbage stuff\nyou are also seeing this garbage stuff as well. Closeed question contrast with\nas well. Closeed question contrast with\nas well. Closeed question contrast with open-minded question. So this is like\nopen-minded question. So this is like\nopen-minded question. So this is like the response that you get in the in\nthe response that you get in the in\nthe response that you get in the in whenever you're visiting the page the\nwhenever you're visiting the page the\nwhenever you're visiting the page the Wikipedia web page, you will see all of\nWikipedia web page, you will see all of\nWikipedia web page, you will see all of these things, right? So that's exactly\nthese things, right? So that's exactly\nthese things, right? So that's exactly it is returning. So do not feel bad.\nit is returning. So do not feel bad.\nit is returning. So do not feel bad. That's how it works. That's why we need\nThat's how it works. That's why we need\nThat's how it works. That's why we need to connect these things to LLM so that\nto connect these things to LLM so that\nto connect these things to LLM so that LLM can prune these things. Right? Okay.\nLLM can prune these things. Right? Okay.\nLLM can prune these things. Right? Okay. Makes sense. So these are the two tools\nMakes sense. So these are the two tools\nMakes sense. So these are the two tools that we have created. Let me also create\nthat we have created. Let me also create\nthat we have created. Let me also create one custom tool so that you will also\none custom tool so that you will also\none custom tool so that you will also learn how to create custom tools. So\nlearn how to create custom tools. So\nlearn how to create custom tools. So let's say you want to create one tool uh\nlet's say you want to create one tool uh\nlet's say you want to create one tool uh which will do something that it can also\nwhich will do something that it can also\nwhich will do something that it can also not do like we do not have any kind of\nnot do like we do not have any kind of\nnot do like we do not have any kind of integration with that particular\nintegration with that particular\nintegration with that particular function that you want to build right.\nfunction that you want to build right.\nfunction that you want to build right. So you can build your custom function as\nSo you can build your custom function as\nSo you can build your custom function as well.\nwell.\nwell. tool three and I want to make it as um\ntool three and I want to make it as um\ntool three and I want to make it as um custom\ncustom\ncustom enterprise tool. Let's say that tool is\nenterprise tool. Let's say that tool is\nenterprise tool. Let's say that tool is just for my enterprise like where let's\njust for my enterprise like where let's\njust for my enterprise like where let's say you are working right. So I will\nsay you are working right. So I will\nsay you are working right. So I will simply say from lang chain\nsimply say from lang chain\nsimply say from lang chain dot tools import\ndot tools import\ndot tools import tool perfect now I will create a\ntool perfect now I will create a\ntool perfect now I will create a decorator tool and then I will write\ndecorator tool and then I will write\ndecorator tool and then I will write let's say enterprise tool\nlet's say enterprise tool\nlet's say enterprise tool make sense make sense so now what I will\nmake sense make sense so now what I will\nmake sense make sense so now what I will do I will simply create the function and\ndo I will simply create the function and\ndo I will simply create the function and what this function does this simply\nwhat this function does this simply\nwhat this function does this simply sends an email\nsends an email\nsends an email we have integrations as well for email.\nwe have integrations as well for email.\nwe have integrations as well for email. I'm just like demonstrating it with a\nI'm just like demonstrating it with a\nI'm just like demonstrating it with a hypothetical situation where we do not\nhypothetical situation where we do not\nhypothetical situation where we do not have the let's say integration with\nhave the let's say integration with\nhave the let's say integration with email. Right? So what I will do I will\nemail. Right? So what I will do I will\nemail. Right? So what I will do I will simply say return\nsimply say return\nsimply say return and I will say email sent. Let's say\nand I will say email sent. Let's say\nand I will say email sent. Let's say email sent. Obviously I need to write\nemail sent. Obviously I need to write\nemail sent. Obviously I need to write the code here to send the email and for\nthe code here to send the email and for\nthe code here to send the email and for that you would need to create these APIs\nthat you would need to create these APIs\nthat you would need to create these APIs as well from mail gun and maybe Gmail\nas well from mail gun and maybe Gmail\nas well from mail gun and maybe Gmail API. We are not getting the those APIs\nAPI. We are not getting the those APIs\nAPI. We are not getting the those APIs right now but you know how to create all\nright now but you know how to create all\nright now but you know how to create all those basic stuff like API and then just\nthose basic stuff like API and then just\nthose basic stuff like API and then just making the calls. You can just write\nmaking the calls. You can just write\nmaking the calls. You can just write your code here. Now very important thing\nyour code here. Now very important thing\nyour code here. Now very important thing whenever you are creating your custom\nwhenever you are creating your custom\nwhenever you are creating your custom function you need to add a dock string.\nfunction you need to add a dock string.\nfunction you need to add a dock string. Why\nWhy\nWhy legends are asking what is a dock\nlegends are asking what is a dock\nlegends are asking what is a dock string? Okay good. Do string is nothing\nstring? Okay good. Do string is nothing\nstring? Okay good. Do string is nothing but a kind of way to add comments in\nbut a kind of way to add comments in\nbut a kind of way to add comments in your function. You write it like this.\nyour function. You write it like this.\nyour function. You write it like this. three um double quotes and then you\nthree um double quotes and then you\nthree um double quotes and then you simply provide a description. You simply\nsimply provide a description. You simply\nsimply provide a description. You simply add a kind of message so that LLM can\nadd a kind of message so that LLM can\nadd a kind of message so that LLM can read because just tell me one thing you\nread because just tell me one thing you\nread because just tell me one thing you have created this tool right you have\nhave created this tool right you have\nhave created this tool right you have created this tool you are creating this\ncreated this tool you are creating this\ncreated this tool you are creating this tool LLM will pick one tool two tool it\ntool LLM will pick one tool two tool it\ntool LLM will pick one tool two tool it needs to pick that tool on the basis of\nneeds to pick that tool on the basis of\nneeds to pick that tool on the basis of a you can say a kind of message right\na you can say a kind of message right\na you can say a kind of message right how LLM will decide how LM will decide\nhow LLM will decide how LM will decide\nhow LLM will decide how LM will decide it so in that particular thing you have\nit so in that particular thing you have\nit so in that particular thing you have to you have to just tell okay this is\nto you have to just tell okay this is\nto you have to just tell okay this is the information this tool is built for\nthe information this tool is built for\nthe information this tool is built for this purpose. This tool is built for\nthis purpose. This tool is built for\nthis purpose. This tool is built for this purpose like this.\nthis purpose like this.\nthis purpose like this. Okay. Okay. So, we have to write the\nOkay. Okay. So, we have to write the\nOkay. Okay. So, we have to write the description. So, that's why I think we\ndescription. So, that's why I think we\ndescription. So, that's why I think we can even write description here as well\ncan even write description here as well\ncan even write description here as well because that makes sense. Description. I\nbecause that makes sense. Description. I\nbecause that makes sense. Description. I will simply say um this is\nwill simply say um this is\nwill simply say um this is a tool to search Wikipedia.\na tool to search Wikipedia.\na tool to search Wikipedia. And I can also write maybe duck go\nAnd I can also write maybe duck go\nAnd I can also write maybe duck go search here description. This is a tool\nsearch here description. This is a tool\nsearch here description. This is a tool to search the web for news. Let's say\nto search the web for news. Let's say\nto search the web for news. Let's say and I can just complete this. This is a\nand I can just complete this. This is a\nand I can just complete this. This is a tool to send emails to employees. Very\ntool to send emails to employees. Very\ntool to send emails to employees. Very good. Email sent. Let me just run this.\ngood. Email sent. Let me just run this.\ngood. Email sent. Let me just run this. So that's how we can add the\nSo that's how we can add the\nSo that's how we can add the description. Obviously these are the\ndescription. Obviously these are the\ndescription. Obviously these are the integrated functions. So we do not have\nintegrated functions. So we do not have\nintegrated functions. So we do not have like power to add the dock string within\nlike power to add the dock string within\nlike power to add the dock string within this. But I'm so sure when lang chain\nthis. But I'm so sure when lang chain\nthis. But I'm so sure when lang chain would have integrated these functions,\nwould have integrated these functions,\nwould have integrated these functions, it would have already added the dock\nit would have already added the dock\nit would have already added the dock string because they know like you are\nstring because they know like you are\nstring because they know like you are very lazy. You will not write dock\nvery lazy. You will not write dock\nvery lazy. You will not write dock strings. But you should always try to\nstrings. But you should always try to\nstrings. But you should always try to add it. Right. So now this is the\nadd it. Right. So now this is the\nadd it. Right. So now this is the information. Now these are the tools\ninformation. Now these are the tools\ninformation. Now these are the tools that we have created. Simple. Okay. Now\nthat we have created. Simple. Okay. Now\nthat we have created. Simple. Okay. Now let me just add a toolkit. Basically\nlet me just add a toolkit. Basically\nlet me just add a toolkit. Basically let's create a toolkit. Now this is our\nlet's create a toolkit. Now this is our\nlet's create a toolkit. Now this is our toolkit. And in this toolkit we will\ntoolkit. And in this toolkit we will\ntoolkit. And in this toolkit we will simply write all the function names that\nsimply write all the function names that\nsimply write all the function names that we have. So we have first of all um\nwe have. So we have first of all um\nwe have. So we have first of all um search tool.\nsearch tool.\nsearch tool. Search tool. And we have then Wikipedia\nSearch tool. And we have then Wikipedia\nSearch tool. And we have then Wikipedia tool. Perfect.\ntool. Perfect.\ntool. Perfect. And then we have enterprise tool. See\nAnd then we have enterprise tool. See\nAnd then we have enterprise tool. See how well this particular IDE is picking\nhow well this particular IDE is picking\nhow well this particular IDE is picking the context. It is written here.\nthe context. It is written here.\nthe context. It is written here. Enterprise tool. It is suggesting me\nEnterprise tool. It is suggesting me\nEnterprise tool. It is suggesting me email sender tool. Bro, bro,\nemail sender tool. Bro, bro,\nemail sender tool. Bro, bro, which model are you using in the in the\nwhich model are you using in the in the\nwhich model are you using in the in the back end? I want to know. Okay, now\nback end? I want to know. Okay, now\nback end? I want to know. Okay, now enterprise tool. Let me just run this\nenterprise tool. Let me just run this\nenterprise tool. Let me just run this toolkit. So now I have the toolkit of\ntoolkit. So now I have the toolkit of\ntoolkit. So now I have the toolkit of these three tools. If I want to see the\nthese three tools. If I want to see the\nthese three tools. If I want to see the names of these toolkits, I can also\nnames of these toolkits, I can also\nnames of these toolkits, I can also write like this\nwrite like this\nwrite like this toolkit\ntoolkit\ntoolkit dot get tools. I think this was a\ndot get tools. I think this was a\ndot get tools. I think this was a function. I don't know if it is\nfunction. I don't know if it is\nfunction. I don't know if it is available now. List has no attribute.\navailable now. List has no attribute.\navailable now. List has no attribute. Get tools. Um\nGet tools. Um\nGet tools. Um makes sense. I think because this is a\nmakes sense. I think because this is a\nmakes sense. I think because this is a list. So actually we can actually write\nlist. So actually we can actually write\nlist. So actually we can actually write like this toolkit directly.\nlike this toolkit directly.\nlike this toolkit directly. Yeah. So now we have these two like\nYeah. So now we have these two like\nYeah. So now we have these two like these tools. Let's say DGO search\nthese tools. Let's say DGO search\nthese tools. Let's say DGO search Wikipedia query run structured tool\nWikipedia query run structured tool\nWikipedia query run structured tool basically name equals to enterprise tool\nbasically name equals to enterprise tool\nbasically name equals to enterprise tool because this is the kind of tool that we\nbecause this is the kind of tool that we\nbecause this is the kind of tool that we have created. So that is why it is\nhave created. So that is why it is\nhave created. So that is why it is coming like this. Make sense? So now we\ncoming like this. Make sense? So now we\ncoming like this. Make sense? So now we have created these toolkits and it looks\nhave created these toolkits and it looks\nhave created these toolkits and it looks good. It looks good. Let me just add\ngood. It looks good. Let me just add\ngood. It looks good. Let me just add this code here. So now let's create our\nthis code here. So now let's create our\nthis code here. So now let's create our agent because now we're good to go.\nagent because now we're good to go.\nagent because now we're good to go. Perfect.\nPerfect.\nPerfect. Agent basically react agent.\nAgent basically react agent.\nAgent basically react agent. React agent.\nPerfect. In order to create the react\nPerfect. In order to create the react agent, you already know the code that we\nagent, you already know the code that we\nagent, you already know the code that we have just seen in the documentation. It\nhave just seen in the documentation. It\nhave just seen in the documentation. It is very simple, straightforward code.\nis very simple, straightforward code.\nis very simple, straightforward code. Just few lines of code. Earlier it used\nJust few lines of code. Earlier it used\nJust few lines of code. Earlier it used to be a little bit bigger but now they\nto be a little bit bigger but now they\nto be a little bit bigger but now they have just simplified it. So I can just\nhave just simplified it. So I can just\nhave just simplified it. So I can just make some changes. I want to use GPT 5\nmake some changes. I want to use GPT 5\nmake some changes. I want to use GPT 5 mini and tools equals to toolkit.\nPerfect. Now let's try to show you the\nPerfect. Now let's try to show you the agent. Now let's run this. Perfect. So\nagent. Now let's run this. Perfect. So\nagent. Now let's run this. Perfect. So this is my agent. Wow. Do you know what?\nthis is my agent. Wow. Do you know what?\nthis is my agent. Wow. Do you know what? They have just started visualizing this\nThey have just started visualizing this\nThey have just started visualizing this in the V1. It was not available before.\nin the V1. It was not available before.\nin the V1. It was not available before. We had zero visibility. We have we had\nWe had zero visibility. We have we had\nWe had zero visibility. We have we had zero visibility of our agent that we\nzero visibility of our agent that we\nzero visibility of our agent that we were creating before. So now do you know\nwere creating before. So now do you know\nwere creating before. So now do you know what is this? This is a kind of mermaid\nwhat is this? This is a kind of mermaid\nwhat is this? This is a kind of mermaid mermaid it is called mermaid code. So\nmermaid it is called mermaid code. So\nmermaid it is called mermaid code. So whatever the it is like doing this thing\nwhatever the it is like doing this thing\nwhatever the it is like doing this thing we can actually um see the back end code\nwe can actually um see the back end code\nwe can actually um see the back end code as well but I don't want to show you\nas well but I don't want to show you\nas well but I don't want to show you that code because you will not\nthat code because you will not\nthat code because you will not understand anything in that back end\nunderstand anything in that back end\nunderstand anything in that back end code. No no no one can understand it.\ncode. No no no one can understand it.\ncode. No no no one can understand it. It's basically a way of designing all of\nIt's basically a way of designing all of\nIt's basically a way of designing all of these you can say DAGs and in Langraph\nthese you can say DAGs and in Langraph\nthese you can say DAGs and in Langraph we just can show you as well and then we\nwe just can show you as well and then we\nwe just can show you as well and then we can just visualize it as well because in\ncan just visualize it as well because in\ncan just visualize it as well because in the back end what I have um realized\nthe back end what I have um realized\nthe back end what I have um realized after exploring v1 they have literally\nafter exploring v1 they have literally\nafter exploring v1 they have literally rewritten everything in langraph I'm so\nrewritten everything in langraph I'm so\nrewritten everything in langraph I'm so sure like under the hood um lang graph\nsure like under the hood um lang graph\nsure like under the hood um lang graph is running I'm so sure because this kind\nis running I'm so sure because this kind\nis running I'm so sure because this kind of thing we built in lang graph but yeah\nof thing we built in lang graph but yeah\nof thing we built in lang graph but yeah it's fine\nit's fine\nit's fine so this is our agent now you can see\nso this is our agent now you can see\nso this is our agent now you can see that this is the model right because\nthat this is the model right because\nthat this is the model right because this is the model\nthis is the model\nthis is the model And what is the model? Model is this\nAnd what is the model? Model is this\nAnd what is the model? Model is this this one. This is the one. We can also\nthis one. This is the one. We can also\nthis one. This is the one. We can also say llm. It's not a big deal. I can also\nsay llm. It's not a big deal. I can also\nsay llm. It's not a big deal. I can also say model equals to lm\nsay model equals to lm\nsay model equals to lm lm openi that we have right. I can even\nlm openi that we have right. I can even\nlm openi that we have right. I can even run this um chat open object has no\nrun this um chat open object has no\nrun this um chat open object has no attribute lower. What do you mean? Did\nattribute lower. What do you mean? Did\nattribute lower. What do you mean? Did we create lm openi? Yeah, we have this\nwe create lm openi? Yeah, we have this\nwe create lm openi? Yeah, we have this llm open. Oh, I see. I see. I see. Okay.\nllm open. Oh, I see. I see. I see. Okay.\nllm open. Oh, I see. I see. I see. Okay. Okay. Okay. Makes sense. We have to use\nOkay. Okay. Makes sense. We have to use\nOkay. Okay. Makes sense. We have to use this name as is because remember our\nthis name as is because remember our\nthis name as is because remember our init chat model thing where we have to\ninit chat model thing where we have to\ninit chat model thing where we have to define the model itself. So we cannot\ndefine the model itself. So we cannot\ndefine the model itself. So we cannot say lm openi like this because this is\nsay lm openi like this because this is\nsay lm openi like this because this is not the model variable. This is the\nnot the model variable. This is the\nnot the model variable. This is the model itself that they are calling.\nmodel itself that they are calling.\nmodel itself that they are calling. Okay, that's fine. That's fine. So now\nOkay, that's fine. That's fine. So now\nOkay, that's fine. That's fine. So now this is the model. Okay. And the these\nthis is the model. Okay. And the these\nthis is the model. Okay. And the these are the tools. Is it similar to this\nare the tools. Is it similar to this\nare the tools. Is it similar to this one? Yes. Obviously, obviously this is\none? Yes. Obviously, obviously this is\none? Yes. Obviously, obviously this is exactly I know like this looks better\nexactly I know like this looks better\nexactly I know like this looks better but yeah this is exactly the same thing\nbut yeah this is exactly the same thing\nbut yeah this is exactly the same thing because this is the model and these are\nbecause this is the model and these are\nbecause this is the model and these are tool calls the this start and end is\ntool calls the this start and end is\ntool calls the this start and end is just a way to tell that this graph is\njust a way to tell that this graph is\njust a way to tell that this graph is starting and ending\nstarting and ending\nstarting and ending make sense this graph is starting and\nmake sense this graph is starting and\nmake sense this graph is starting and ending okay now I will just show you\nending okay now I will just show you\nending okay now I will just show you very good thing so now let's test this\nvery good thing so now let's test this\nvery good thing so now let's test this agent that we have built now I want to\nagent that we have built now I want to\nagent that we have built now I want to invoke the agent but I will invoke it\ninvoke the agent but I will invoke it\ninvoke the agent but I will invoke it using streaming now what is streaming So\nusing streaming now what is streaming So\nusing streaming now what is streaming So you are a data engineer you should know\nyou are a data engineer you should know\nyou are a data engineer you should know about streaming. Streaming is streaming\nabout streaming. Streaming is streaming\nabout streaming. Streaming is streaming means when we just display or basically\nmeans when we just display or basically\nmeans when we just display or basically use a data as soon as it arrives right.\nuse a data as soon as it arrives right.\nuse a data as soon as it arrives right. So when I will be just making this um\nSo when I will be just making this um\nSo when I will be just making this um model call there will be so many things\nmodel call there will be so many things\nmodel call there will be so many things running in the back end right in the\nrunning in the back end right in the\nrunning in the back end right in the background and we want to also visualize\nbackground and we want to also visualize\nbackground and we want to also visualize this. We also want to see what are the\nthis. We also want to see what are the\nthis. We also want to see what are the things going on in the background. So\nthings going on in the background. So\nthings going on in the background. So we'll be using streaming so that we can\nwe'll be using streaming so that we can\nwe'll be using streaming so that we can see everything what's going on and it's\nsee everything what's going on and it's\nsee everything what's going on and it's a good way to actually understand the\na good way to actually understand the\na good way to actually understand the thing as well. So let me just show you\nthing as well. So let me just show you\nthing as well. So let me just show you React agent invoke\nReact agent invoke\nReact agent invoke with streams.\nOkay, very good.\nOkay, very good. Perfect. So now I can bring that\nPerfect. So now I can bring that\nPerfect. So now I can bring that streaming code. It is also written in\nstreaming code. It is also written in\nstreaming code. It is also written in the documentation I guess. Uh where is\nthe documentation I guess. Uh where is\nthe documentation I guess. Uh where is stream? Stre stream\nstream? Stre stream\nstream? Stre stream uh wav tool call. Okay, maybe in the\nuh wav tool call. Okay, maybe in the\nuh wav tool call. Okay, maybe in the quick start\nquick start\nquick start system prompt\nsystem prompt\nsystem prompt because I use streaming with langraph. I\nbecause I use streaming with langraph. I\nbecause I use streaming with langraph. I do not use it with lang chain. Uh, okay.\ndo not use it with lang chain. Uh, okay.\ndo not use it with lang chain. Uh, okay. Where is that? I I saw it somewhere.\nWhere is that? I I saw it somewhere.\nWhere is that? I I saw it somewhere. I saw it. I don't know where, but I saw\nI saw it. I don't know where, but I saw\nI saw it. I don't know where, but I saw it.\nOh, yeah. Here it is. It was under SQL\nOh, yeah. Here it is. It was under SQL database toolkit because I know like I\ndatabase toolkit because I know like I\ndatabase toolkit because I know like I saw it. So let me just copy this uh let\nsaw it. So let me just copy this uh let\nsaw it. So let me just copy this uh let me just paste it here. So what is this\nme just paste it here. So what is this\nme just paste it here. So what is this thing? I am simply trying to make an\nthing? I am simply trying to make an\nthing? I am simply trying to make an make a model call. Okay. And I want to\nmake a model call. Okay. And I want to\nmake a model call. Okay. And I want to write my question. So what I will do? I\nwrite my question. So what I will do? I\nwrite my question. So what I will do? I will simply say um who or let's say give\nwill simply say um who or let's say give\nwill simply say um who or let's say give me the latest news\nme the latest news\nme the latest news about let's say stock market. That's a\nabout let's say stock market. That's a\nabout let's say stock market. That's a good question. So do you know what will\ngood question. So do you know what will\ngood question. So do you know what will happen? Let me just first of all run\nhappen? Let me just first of all run\nhappen? Let me just first of all run this. So human message this this is my\nthis. So human message this this is my\nthis. So human message this this is my message give me the latest news that I\nmessage give me the latest news that I\nmessage give me the latest news that I have sent to the AI. Now it will go to\nhave sent to the AI. Now it will go to\nhave sent to the AI. Now it will go to the tool call. Just wait just wait just\nthe tool call. Just wait just wait just\nthe tool call. Just wait just wait just wait you will see everything. Just wait.\nwait you will see everything. Just wait.\nwait you will see everything. Just wait. It is running in the real time. See see\nIt is running in the real time. See see\nIt is running in the real time. See see now again. Again\nlet's wait. Let's wait because it is\nlet's wait. Let's wait because it is generating more messages. Okay. Okay.\ngenerating more messages. Okay. Okay.\ngenerating more messages. Okay. Okay. Tool message. Okay.\nTool message. Okay.\nTool message. Okay. Okay. More tool messages. Wow.\nOkay. More tool messages. Wow.\nOkay. More tool messages. Wow. Okay,\nlet's wait because we just simply posted\nlet's wait because we just simply posted a very generic message and it is just\na very generic message and it is just\na very generic message and it is just making more and more more and more tool\nmaking more and more more and more tool\nmaking more and more more and more tool calls and that now that is done. Now let\ncalls and that now that is done. Now let\ncalls and that now that is done. Now let me just show you what happened. First of\nme just show you what happened. First of\nme just show you what happened. First of all, let me click here scorable element\nall, let me click here scorable element\nall, let me click here scorable element so that you can see all of the output.\nso that you can see all of the output.\nso that you can see all of the output. So first of all, first of all, we made a\nSo first of all, first of all, we made a\nSo first of all, first of all, we made a first of all we made a\nfirst of all we made a\nfirst of all we made a query like we asked a query. Give me the\nquery like we asked a query. Give me the\nquery like we asked a query. Give me the latest news about stock market. Perfect.\nlatest news about stock market. Perfect.\nlatest news about stock market. Perfect. This is my message. It went to the\nThis is my message. It went to the\nThis is my message. It went to the model. Yes, simple remember the flow. It\nmodel. Yes, simple remember the flow. It\nmodel. Yes, simple remember the flow. It went to the model react agent. Okay, our\nwent to the model react agent. Okay, our\nwent to the model react agent. Okay, our react agent that we have created. So now\nreact agent that we have created. So now\nreact agent that we have created. So now then what happened? React agent\nthen what happened? React agent\nthen what happened? React agent basically LLM didn't have the built-in\nbasically LLM didn't have the built-in\nbasically LLM didn't have the built-in function to search for the news because\nfunction to search for the news because\nfunction to search for the news because LLM is not trained on news right or it\nLLM is not trained on news right or it\nLLM is not trained on news right or it cannot be because you need news like\ncannot be because you need news like\ncannot be because you need news like latest news maybe today's news so now it\nlatest news maybe today's news so now it\nlatest news maybe today's news so now it looked for any tool in its inventory in\nlooked for any tool in its inventory in\nlooked for any tool in its inventory in its toolkit and it found a tool how\nits toolkit and it found a tool how\nits toolkit and it found a tool how because you provide the description\nbecause you provide the description\nbecause you provide the description right so now we found a tool called\nright so now we found a tool called\nright so now we found a tool called duckgo search simple it used that tool\nduckgo search simple it used that tool\nduckgo search simple it used that tool called the go search and it called that\ncalled the go search and it called that\ncalled the go search and it called that tool and this is the call ID and these\ntool and this is the call ID and these\ntool and this is the call ID and these are the arguments stock market news\nare the arguments stock market news\nare the arguments stock market news January 826 because today is January 8 C\nJanuary 826 because today is January 8 C\nJanuary 826 because today is January 8 C I'm literally getting today's news US\nI'm literally getting today's news US\nI'm literally getting today's news US S&amp;P 500 do NASDAQ latest blah blah blah\nS&amp;P 500 do NASDAQ latest blah blah blah\nS&amp;P 500 do NASDAQ latest blah blah blah blah blah then this tool this AI message\nblah blah then this tool this AI message\nblah blah then this tool this AI message didn't generate anything didn't generate\ndidn't generate anything didn't generate\ndidn't generate anything didn't generate anything this tool basically AI message\nanything this tool basically AI message\nanything this tool basically AI message generated a tool call and that's it with\ngenerated a tool call and that's it with\ngenerated a tool call and that's it with arguments ments with arguments.\narguments ments with arguments.\narguments ments with arguments. Okay, with arguments and remember this\nOkay, with arguments and remember this\nOkay, with arguments and remember this thing because this concept is really\nthing because this concept is really\nthing because this concept is really important. You need to understand\nimportant. You need to understand\nimportant. You need to understand everything. So AI message that means AI\neverything. So AI message that means AI\neverything. So AI message that means AI message means message is coming from\nmessage means message is coming from\nmessage means message is coming from LLM. So LLM did not generate a response.\nLLM. So LLM did not generate a response.\nLLM. So LLM did not generate a response. It generated a tool call. It generated\nIt generated a tool call. It generated\nIt generated a tool call. It generated an indication that I want to make a tool\nan indication that I want to make a tool\nan indication that I want to make a tool call bro. Please allow me to make a tool\ncall bro. Please allow me to make a tool\ncall bro. Please allow me to make a tool call. Make sense? Then it made a tool\ncall. Make sense? Then it made a tool\ncall. Make sense? Then it made a tool call because we have provided those\ncall because we have provided those\ncall because we have provided those tools.\ntools.\ntools. This is the answer that my tool has\nThis is the answer that my tool has\nThis is the answer that my tool has returned. This is not generated by LLM.\nreturned. This is not generated by LLM.\nreturned. This is not generated by LLM. This is the message that my tool has\nThis is the message that my tool has\nThis is the message that my tool has returned. Basically, duct go search has\nreturned. Basically, duct go search has\nreturned. Basically, duct go search has returned. Perfect. Now it went to AI\nreturned. Perfect. Now it went to AI\nreturned. Perfect. Now it went to AI message again. It got the data. AI\nmessage again. It got the data. AI\nmessage again. It got the data. AI message was not satisfied. It made\nmessage was not satisfied. It made\nmessage was not satisfied. It made another tool call. It said give me more\nanother tool call. It said give me more\nanother tool call. It said give me more news. Now it got this message. Then the\nnews. Now it got this message. Then the\nnews. Now it got this message. Then the AI message said like give me more data.\nAI message said like give me more data.\nAI message said like give me more data. It gave its query with obviously some\nIt gave its query with obviously some\nIt gave its query with obviously some changes because it wanted to make some\nchanges because it wanted to make some\nchanges because it wanted to make some more calls. Then it got the value back\nmore calls. Then it got the value back\nmore calls. Then it got the value back from the function. Then AI message again\nfrom the function. Then AI message again\nfrom the function. Then AI message again made a tool call and then again tool\nmade a tool call and then again tool\nmade a tool call and then again tool message gave its a data. Make sense? And\nmessage gave its a data. Make sense? And\nmessage gave its a data. Make sense? And then finally AI message has returned the\nthen finally AI message has returned the\nthen finally AI message has returned the data like returned the answer at the\ndata like returned the answer at the\ndata like returned the answer at the end. This is the flow. The point why I'm\nend. This is the flow. The point why I'm\nend. This is the flow. The point why I'm explaining this thing because I will\nexplaining this thing because I will\nexplaining this thing because I will just show you the back end as well like\njust show you the back end as well like\njust show you the back end as well like how it is running in the background. LLM\nhow it is running in the background. LLM\nhow it is running in the background. LLM never generates the answer. It simply\nnever generates the answer. It simply\nnever generates the answer. It simply indicates that I want to make a tool\nindicates that I want to make a tool\nindicates that I want to make a tool call. If it needs to make a tool call,\ncall. If it needs to make a tool call,\ncall. If it needs to make a tool call, it will make the tool call. It will get\nit will make the tool call. It will get\nit will make the tool call. It will get the response back and then it will\nthe response back and then it will\nthe response back and then it will generate the response. If it does not\ngenerate the response. If it does not\ngenerate the response. If it does not need to make a tool call, it will not do\nneed to make a tool call, it will not do\nneed to make a tool call, it will not do anything. So all of these things are\nanything. So all of these things are\nanything. So all of these things are actually managed by lang chain. Actually\nactually managed by lang chain. Actually\nactually managed by lang chain. Actually everything is managed by lang chain.\neverything is managed by lang chain.\neverything is managed by lang chain. Under the hood\nUnder the hood\nUnder the hood how it is possible to do it. So\nhow it is possible to do it. So\nhow it is possible to do it. So basically under the hood what do we do?\nbasically under the hood what do we do?\nbasically under the hood what do we do? We simply first of all create an LLM. We\nWe simply first of all create an LLM. We\nWe simply first of all create an LLM. We bind the tools manually. Here Langchen\nbind the tools manually. Here Langchen\nbind the tools manually. Here Langchen has done that part for us. First step\nhas done that part for us. First step\nhas done that part for us. First step tool binding. Very good. After binding\ntool binding. Very good. After binding\ntool binding. Very good. After binding the tools we make the API call. We\nthe tools we make the API call. We\nthe tools we make the API call. We basically make the um call to model.\nbasically make the um call to model.\nbasically make the um call to model. Right. Then we need to define this\nRight. Then we need to define this\nRight. Then we need to define this logic. These two arrows these are not\nlogic. These two arrows these are not\nlogic. These two arrows these are not very simple. We need to build this\nvery simple. We need to build this\nvery simple. We need to build this logic. We will simply say if in the\nlogic. We will simply say if in the\nlogic. We will simply say if in the response\nresponse\nresponse if in the response we have something\nif in the response we have something\nif in the response we have something called as let's say tool call I will\ncalled as let's say tool call I will\ncalled as let's say tool call I will just show you if it is written make a\njust show you if it is written make a\njust show you if it is written make a tool call then at that time I will call\ntool call then at that time I will call\ntool call then at that time I will call that function. How? because I have the\nthat function. How? because I have the\nthat function. How? because I have the information for the tool call name\ninformation for the tool call name\ninformation for the tool call name basically call ID plus its argument\nbasically call ID plus its argument\nbasically call ID plus its argument because LLM is smart enough to generate\nbecause LLM is smart enough to generate\nbecause LLM is smart enough to generate the arguments. It will say bro you can\nthe arguments. It will say bro you can\nthe arguments. It will say bro you can make this tool call and the tool call\nmake this tool call and the tool call\nmake this tool call and the tool call will require this argument. I can do\nwill require this argument. I can do\nwill require this argument. I can do that for you but I cannot make a tool\nthat for you but I cannot make a tool\nthat for you but I cannot make a tool call. You will make the tool call and\ncall. You will make the tool call and\ncall. You will make the tool call and you will just run the function and you\nyou will just run the function and you\nyou will just run the function and you will give me the response back. Who is\nwill give me the response back. Who is\nwill give me the response back. Who is you? Who llm is you? it is talking to\nyou? Who llm is you? it is talking to\nyou? Who llm is you? it is talking to himself or herself or itself.\nhimself or herself or itself.\nhimself or herself or itself. Make sense?\nMake sense?\nMake sense? That's how it works. That's how it works\nThat's how it works. That's how it works\nThat's how it works. That's how it works under the hood.\nunder the hood.\nunder the hood. Okay. Now, if I just show you um very\nOkay. Now, if I just show you um very\nOkay. Now, if I just show you um very good example of like tool calling like\ngood example of like tool calling like\ngood example of like tool calling like under the hood. Let me just show you\nunder the hood. Let me just show you\nunder the hood. Let me just show you that.\nthat.\nthat. Uh if I want to bind my own model\nUh if I want to bind my own model\nUh if I want to bind my own model because currently we didn't bind the\nbecause currently we didn't bind the\nbecause currently we didn't bind the model, right? Bind the elem. didn't bind\nmodel, right? Bind the elem. didn't bind\nmodel, right? Bind the elem. didn't bind the uh LLM. Let me show you\nthe uh LLM. Let me show you\nthe uh LLM. Let me show you manually\nmanually\nmanually binding the LLM. So currently\nbinding the LLM. So currently\nbinding the LLM. So currently LLM with tools and once you grab this\nLLM with tools and once you grab this\nLLM with tools and once you grab this concept bro I would say just go in the\nconcept bro I would say just go in the\nconcept bro I would say just go in the industry and build something simple. So\nindustry and build something simple. So\nindustry and build something simple. So manually binding the LMIT tools. So\nmanually binding the LMIT tools. So\nmanually binding the LMIT tools. So currently if I use my traditional llm\ncurrently if I use my traditional llm\ncurrently if I use my traditional llm openi right if I say llm.t invoke and if\nopeni right if I say llm.t invoke and if\nopeni right if I say llm.t invoke and if I just ask the same thing what's the\nI just ask the same thing what's the\nI just ask the same thing what's the latest news\nwhat's the latest news about the stock\nwhat's the latest news about the stock market maybe it will just give me the\nmarket maybe it will just give me the\nmarket maybe it will just give me the news which is like outdated because it\nnews which is like outdated because it\nnews which is like outdated because it was trained like long way back but it\nwas trained like long way back but it\nwas trained like long way back but it will not do anything but let's see the\nwill not do anything but let's see the\nwill not do anything but let's see the result and then I can just continue with\nresult and then I can just continue with\nresult and then I can just continue with that because I can uh otherwise give the\nthat because I can uh otherwise give the\nthat because I can uh otherwise give the other prompt for which like it will not\nother prompt for which like it will not\nother prompt for which like it will not have access to do that at all maybe send\nhave access to do that at all maybe send\nhave access to do that at all maybe send an email or whatever\nSo let's wait. Let's wait. Let's wait.\nSo let's wait. Let's wait. Let's wait. See, I don't have internet access.\nSee, I don't have internet access.\nSee, I don't have internet access. Perfect. So it cannot do that. Right?\nPerfect. So it cannot do that. Right?\nPerfect. So it cannot do that. Right? Now let me show you the magic. So this\nNow let me show you the magic. So this\nNow let me show you the magic. So this is the uh without binding.\nPerfect. Now let me just write with\nPerfect. Now let me just write with binding.\nPerfect. Now with binding what will\nPerfect. Now with binding what will happen? I will simply say llm bind it.\nhappen? I will simply say llm bind it.\nhappen? I will simply say llm bind it. And I will bind my LLM with tools.\nAnd I will bind my LLM with tools.\nAnd I will bind my LLM with tools. llm openi\nllm openi\nllm openi dot\ndot\ndot bind tools.\nPerfect. And what are the tools?\nPerfect. And what are the tools? Basically toolkit.\nBasically toolkit.\nBasically toolkit. Make sense? Now what will happen?\nMake sense? Now what will happen?\nMake sense? Now what will happen? First of all, let me just run this.\nFirst of all, let me just run this.\nFirst of all, let me just run this. Perfect. Now if I ask the same question\nPerfect. Now if I ask the same question\nPerfect. Now if I ask the same question from it\nbind it let me just ask this question\nbind it let me just ask this question let's see what will happen\nperfect now do you know what will happen\nperfect now do you know what will happen like what actually happened AI message\nlike what actually happened AI message\nlike what actually happened AI message didn't say I can't fetch live news or\ndidn't say I can't fetch live news or\ndidn't say I can't fetch live news or blah blah blah it has returned empty\nblah blah blah it has returned empty\nblah blah blah it has returned empty content because it didn't generate\ncontent because it didn't generate\ncontent because it didn't generate anything but just scroll towards\nanything but just scroll towards\nanything but just scroll towards Just scroll towards right and you will\nJust scroll towards right and you will\nJust scroll towards right and you will see\nsee\nsee something called as\nright see tool calls\nright see tool calls llm is saying yes I cannot generate\nllm is saying yes I cannot generate\nllm is saying yes I cannot generate anything but I can make a tool call. So\nanything but I can make a tool call. So\nanything but I can make a tool call. So it has listed the tool it wants to use\nit has listed the tool it wants to use\nit has listed the tool it wants to use for the tool call. That is called\nfor the tool call. That is called\nfor the tool call. That is called binding the model with the tools. Langen\nbinding the model with the tools. Langen\nbinding the model with the tools. Langen does it automatically for us. And you\ndoes it automatically for us. And you\ndoes it automatically for us. And you will say like how it is doing this. This\nwill say like how it is doing this. This\nwill say like how it is doing this. This is the code. This is writing this code\nis the code. This is writing this code\nis the code. This is writing this code for you.\nfor you.\nfor you. Okay. So it uses this thing and you can\nOkay. So it uses this thing and you can\nOkay. So it uses this thing and you can also see it has generated the arguments.\nalso see it has generated the arguments.\nalso see it has generated the arguments. See arguments query because we are using\nSee arguments query because we are using\nSee arguments query because we are using query um you can say input. I can also\nquery um you can say input. I can also\nquery um you can say input. I can also show you um if I go here in my duck go\nshow you um if I go here in my duck go\nshow you um if I go here in my duck go search\nsearch\nsearch where is that duck go search duck go\nwhere is that duck go search duck go\nwhere is that duck go search duck go search uh okay so this is like general\nsearch uh okay so this is like general\nsearch uh okay so this is like general function so it knows like it will be a\nfunction so it knows like it will be a\nfunction so it knows like it will be a query so it's fine and then it is saying\nquery so it's fine and then it is saying\nquery so it's fine and then it is saying stock market news today it has generated\nstock market news today it has generated\nstock market news today it has generated a phrase that it will use perfect\nmake sense and see it has created\nmake sense and see it has created multiple instances of it now this time\nmultiple instances of it now this time\nmultiple instances of it now this time again same tool but with different\nagain same tool but with different\nagain same tool but with different argument because it can have like\nargument because it can have like\nargument because it can have like multiple um you can say those um\nmultiple um you can say those um\nmultiple um you can say those um phrases. So earlier it said like latest\nphrases. So earlier it said like latest\nphrases. So earlier it said like latest news today then S&amp;P 500 today then it\nnews today then S&amp;P 500 today then it\nnews today then S&amp;P 500 today then it will be like one more see do Jones\nwill be like one more see do Jones\nwill be like one more see do Jones industrial whatever. So it has actually\nindustrial whatever. So it has actually\nindustrial whatever. So it has actually created three different phrases. So it\ncreated three different phrases. So it\ncreated three different phrases. So it called the model three times and do you\ncalled the model three times and do you\ncalled the model three times and do you know how it happened? You know it knows\nknow how it happened? You know it knows\nknow how it happened? You know it knows this thing. So LLM returned this thing.\nthis thing. So LLM returned this thing.\nthis thing. So LLM returned this thing. So let me show you. So LLM returned\nSo let me show you. So LLM returned\nSo let me show you. So LLM returned LLM returned this tool call. Right now,\nLLM returned this tool call. Right now,\nLLM returned this tool call. Right now, now what we do, what we do manually like\nnow what we do, what we do manually like\nnow what we do, what we do manually like lang is doing everything for you. But\nlang is doing everything for you. But\nlang is doing everything for you. But what we do manually, we take this list\nwhat we do manually, we take this list\nwhat we do manually, we take this list of tools because it can have only one\nof tools because it can have only one\nof tools because it can have only one tool or it can have like a list of tools\ntool or it can have like a list of tools\ntool or it can have like a list of tools as well. We take this list of tools and\nas well. We take this list of tools and\nas well. We take this list of tools and we make the function call one by one and\nwe make the function call one by one and\nwe make the function call one by one and we keep on updating that particular\nwe keep on updating that particular\nwe keep on updating that particular response with the message type called\nresponse with the message type called\nresponse with the message type called tool message. That's why you are seeing\ntool message. That's why you are seeing\ntool message. That's why you are seeing a new type of message called tool\na new type of message called tool\na new type of message called tool message. See tool message we do it\nmessage. See tool message we do it\nmessage. See tool message we do it manually. We do it in langraph.\nmanually. We do it in langraph.\nmanually. We do it in langraph. But in lang chain that's why like lang\nBut in lang chain that's why like lang\nBut in lang chain that's why like lang chain is a lightweight you can say AI\nchain is a lightweight you can say AI\nchain is a lightweight you can say AI agent builder because you do not manage\nagent builder because you do not manage\nagent builder because you do not manage it anything. you everything is like done\nit anything. you everything is like done\nit anything. you everything is like done for you from the agentic point of view\nfor you from the agentic point of view\nfor you from the agentic point of view not from the branching and all branching\nnot from the branching and all branching\nnot from the branching and all branching and is like fine with language but for\nand is like fine with language but for\nand is like fine with language but for agentic standpoint\nagentic standpoint\nagentic standpoint I hope it makes sense why I'm putting\nI hope it makes sense why I'm putting\nI hope it makes sense why I'm putting this this much of effort for this topic\nthis this much of effort for this topic\nthis this much of effort for this topic because when I was also learning I was\nbecause when I was also learning I was\nbecause when I was also learning I was so confused and I wish I had this kind\nso confused and I wish I had this kind\nso confused and I wish I had this kind of explanation in front of me and in\nof explanation in front of me and in\nof explanation in front of me and in just one video I would have felt so so\njust one video I would have felt so so\njust one video I would have felt so so so good but I'm still so grateful for\nso good but I'm still so grateful for\nso good but I'm still so grateful for all the resources that I also got. So\nall the resources that I also got. So\nall the resources that I also got. So it's fine. It's fine. It's fine. It's\nit's fine. It's fine. It's fine. It's\nit's fine. It's fine. It's fine. It's fine. Right? Because every resource is\nfine. Right? Because every resource is\nfine. Right? Because every resource is good because\ngood because\ngood because I would say each resource or every\nI would say each resource or every\nI would say each resource or every resource will teach you something. It\nresource will teach you something. It\nresource will teach you something. It will not like give you something which\nwill not like give you something which\nwill not like give you something which is not useful. It will still give you\nis not useful. It will still give you\nis not useful. It will still give you something useful. So still grateful for\nsomething useful. So still grateful for\nsomething useful. So still grateful for all the resources that I have that I\nall the resources that I have that I\nall the resources that I have that I have used. So amazing. So now I hope it\nhave used. So amazing. So now I hope it\nhave used. So amazing. So now I hope it makes sense. Right? Now let's try to\nmakes sense. Right? Now let's try to\nmakes sense. Right? Now let's try to build a quick one more example and it\nbuild a quick one more example and it\nbuild a quick one more example and it will be like very handy and very useful\nwill be like very handy and very useful\nwill be like very handy and very useful for data engineers because I want to\nfor data engineers because I want to\nfor data engineers because I want to build that it will be fun. So let's end\nbuild that it will be fun. So let's end\nbuild that it will be fun. So let's end this video with that example and then I\nthis video with that example and then I\nthis video with that example and then I will at the end give you a very\nwill at the end give you a very\nwill at the end give you a very important thing to just cover and it's\nimportant thing to just cover and it's\nimportant thing to just cover and it's called rag. I know you are waiting for\ncalled rag. I know you are waiting for\ncalled rag. I know you are waiting for that thing. Do not worry. I will also\nthat thing. Do not worry. I will also\nthat thing. Do not worry. I will also tell you what to cover in rag and how to\ntell you what to cover in rag and how to\ntell you what to cover in rag and how to cover in rag because rag is actually a\ncover in rag because rag is actually a\ncover in rag because rag is actually a totally different area and you need to\ntotally different area and you need to\ntotally different area and you need to go much much much deeper in rag as an\ngo much much much deeper in rag as an\ngo much much much deeper in rag as an independent topic but don't worry I will\nindependent topic but don't worry I will\nindependent topic but don't worry I will just tell you how to cover that as well\njust tell you how to cover that as well\njust tell you how to cover that as well because there's a good news I have a\nbecause there's a good news I have a\nbecause there's a good news I have a dedicated course on rag as well but\ndedicated course on rag as well but\ndedicated course on rag as well but let's cover this let's complete this\nlet's cover this let's complete this\nlet's cover this let's complete this example first of all it will hardly take\nexample first of all it will hardly take\nexample first of all it will hardly take 10 to 15 more minutes and then we're\n10 to 15 more minutes and then we're\n10 to 15 more minutes and then we're going to sum up and then we can just\ngoing to sum up and then we can just\ngoing to sum up and then we can just talk about rag as well quickly and then\ntalk about rag as well quickly and then\ntalk about rag as well quickly and then I will just guide you to the best\nI will just guide you to the best\nI will just guide you to the best resource for rag\nresource for rag\nresource for rag simple if you just want to learn from me\nsimple if you just want to learn from me\nsimple if you just want to learn from me obviously make sense okay because rag is\nobviously make sense okay because rag is\nobviously make sense okay because rag is actually different like totally\nactually different like totally\nactually different like totally different as an independent topic it has\ndifferent as an independent topic it has\ndifferent as an independent topic it has nothing to do with any framework rag is\nnothing to do with any framework rag is\nnothing to do with any framework rag is rag like rag is totally different okay\nrag like rag is totally different okay\nrag like rag is totally different okay so let's talk let's create second and\nso let's talk let's create second and\nso let's talk let's create second and react\nreact\nreact db agent\ndb agent\ndb agent let's try to create an agent for our SQL\nlet's try to create an agent for our SQL\nlet's try to create an agent for our SQL database let's try to do this it will be\ndatabase let's try to do this it will be\ndatabase let's try to do this it will be fun so now you'll be saying an Lamba\nfun so now you'll be saying an Lamba\nfun so now you'll be saying an Lamba from where we can just get the database\nfrom where we can just get the database\nfrom where we can just get the database bro\nbro\nbro this is this a question is this a\nthis is this a question is this a\nthis is this a question is this a question you don't have a database\nI know you would have a database but\nI know you would have a database but let's say the database is in the other\nlet's say the database is in the other\nlet's say the database is in the other machine and you do not want to download\nmachine and you do not want to download\nmachine and you do not want to download any server again and you do not even\nany server again and you do not even\nany server again and you do not even have a docker installed so let's quickly\nhave a docker installed so let's quickly\nhave a docker installed so let's quickly create a lightweight database it's\ncreate a lightweight database it's\ncreate a lightweight database it's called SQLite if you are not familiar\ncalled SQLite if you are not familiar\ncalled SQLite if you are not familiar with SQLite it's very simple SQLite is a\nwith SQLite it's very simple SQLite is a\nwith SQLite it's very simple SQLite is a kind of serverless database that you can\nkind of serverless database that you can\nkind of serverless database that you can run and it stores everything within the\nrun and it stores everything within the\nrun and it stores everything within the file files within the storage and you do\nfile files within the storage and you do\nfile files within the storage and you do not need any kind of server. It is a\nnot need any kind of server. It is a\nnot need any kind of server. It is a lightweight database not like it should\nlightweight database not like it should\nlightweight database not like it should not be good for scaling and it's just\nnot be good for scaling and it's just\nnot be good for scaling and it's just for P and we are good with that. So\nfor P and we are good with that. So\nfor P and we are good with that. So let's create a file to init database\nlet's create a file to init database\nlet's create a file to init database right and for that I would need to\nright and for that I would need to\nright and for that I would need to install SQLite if it is not installed\ninstall SQLite if it is not installed\ninstall SQLite if it is not installed already.\nSQLite\nSQLite SQLite was not found. What do you mean\nSQLite was not found. What do you mean\nSQLite was not found. What do you mean in package registry depends on SQLite?\nin package registry depends on SQLite?\nin package registry depends on SQLite? What is this Python version? The\nWhat is this Python version? The\nWhat is this Python version? The resolution failed for other Python\nresolution failed for other Python\nresolution failed for other Python versions. What project while the active\nversions. What project while the active\nversions. What project while the active Python version 3.12? Okay. The\nPython version 3.12? Okay. The\nPython version 3.12? Okay. The resolution failed for other Python\nresolution failed for other Python\nresolution failed for other Python version supported by this\nversion supported by this\nversion supported by this uh project supported Python. If you want\nuh project supported Python. If you want\nuh project supported Python. If you want to add Python regardless failed\nto add Python regardless failed\nto add Python regardless failed resolution, provide the frozen flag. Why\nresolution, provide the frozen flag. Why\nresolution, provide the frozen flag. Why do I need to provide frozen flag? Let me\ndo I need to provide frozen flag? Let me\ndo I need to provide frozen flag? Let me just import it.\njust import it.\njust import it. SQLite 3. It has given me the whole\nSQLite 3. It has given me the whole\nSQLite 3. It has given me the whole code.\ncode.\ncode. Let me just comment it out everything\nLet me just comment it out everything\nLet me just comment it out everything and let me just\nand let me just\nand let me just run only this thing import thing. Let's\nrun only this thing import thing. Let's\nrun only this thing import thing. Let's see if it is working fine or not. Yeah,\nsee if it is working fine or not. Yeah,\nsee if it is working fine or not. Yeah, it is working fine. Okay, we do not need\nit is working fine. Okay, we do not need\nit is working fine. Okay, we do not need your import. I think it would have\nyour import. I think it would have\nyour import. I think it would have already imported it. So, okay, perfect.\nalready imported it. So, okay, perfect.\nalready imported it. So, okay, perfect. That's good. That is good. So now what\nThat's good. That is good. So now what\nThat's good. That is good. So now what we need to do, we will create a\nwe need to do, we will create a\nwe need to do, we will create a database.\ndatabase.\ndatabase. So in order to create a SQLite database,\nSo in order to create a SQLite database,\nSo in order to create a SQLite database, it is very very very simple to create a\nit is very very very simple to create a\nit is very very very simple to create a database. If you have the database here\ndatabase. If you have the database here\ndatabase. If you have the database here in the folder, it can connect to it and\nin the folder, it can connect to it and\nin the folder, it can connect to it and currently we do not have. So it will\ncurrently we do not have. So it will\ncurrently we do not have. So it will create the database for us as well. So\ncreate the database for us as well. So\ncreate the database for us as well. So let's create it. And yeah, so I'll\nlet's create it. And yeah, so I'll\nlet's create it. And yeah, so I'll simply say connection equals SQLite 3\nsimply say connection equals SQLite 3\nsimply say connection equals SQLite 3 dot connect. And if you are familiar\ndot connect. And if you are familiar\ndot connect. And if you are familiar with pi myql or psychopg2, it is exactly\nwith pi myql or psychopg2, it is exactly\nwith pi myql or psychopg2, it is exactly the same. So in that thing, we simply\nthe same. So in that thing, we simply\nthe same. So in that thing, we simply write psychopg2.connect and then we just\nwrite psychopg2.connect and then we just\nwrite psychopg2.connect and then we just pass all the thing host db whatever\npass all the thing host db whatever\npass all the thing host db whatever password and all. Same with pyql. It is\npassword and all. Same with pyql. It is\npassword and all. Same with pyql. It is sqlite3.connect. So now you'll say how\nsqlite3.connect. So now you'll say how\nsqlite3.connect. So now you'll say how to connect with this because we do not\nto connect with this because we do not\nto connect with this because we do not have any database. We do not have any\nhave any database. We do not have any\nhave any database. We do not have any password. We don't have anything. So it\npassword. We don't have anything. So it\npassword. We don't have anything. So it is called the location of database where\nis called the location of database where\nis called the location of database where our database is located. It is located\nour database is located. It is located\nour database is located. It is located in the current directory. So I'll simply\nin the current directory. So I'll simply\nin the current directory. So I'll simply say um um I will say um\nsay um um I will say um\nsay um um I will say um my DB. Okay. My DB\nmy DB. Okay. My DB\nmy DB. Okay. My DB or let's say sales DB just to make it\nor let's say sales DB just to make it\nor let's say sales DB just to make it more professional. Sales DB. And then I\nmore professional. Sales DB. And then I\nmore professional. Sales DB. And then I want to store it inside\nwant to store it inside\nwant to store it inside um\num\num let's say\nlet's say\nlet's say orders\norders\norders orders dot or let's say sales do\norders dot or let's say sales do\norders dot or let's say sales do database\ndatabase\ndatabase sales db perfect and let's create a\nsales db perfect and let's create a\nsales db perfect and let's create a folder called sales db\nfolder called sales db\nfolder called sales db in the chapter 3\nin the chapter 3\nin the chapter 3 sales db\nsales db\nsales db bd\nbd\nbd okay perfect so let me run\nokay perfect so let me run\nokay perfect so let me run And so what it will do, let me show you.\nAnd so what it will do, let me show you.\nAnd so what it will do, let me show you. So it will throw the error. Wow. What?\nSo it will throw the error. Wow. What?\nSo it will throw the error. Wow. What? What's wrong with you, bro? Uh unable to\nWhat's wrong with you, bro? Uh unable to\nWhat's wrong with you, bro? Uh unable to open database file. Are you serious? So\nopen database file. Are you serious? So\nopen database file. Are you serious? So you need to create this, right?\nyou need to create this, right?\nyou need to create this, right? SQL3.Connect.\nSales DB. Is the spelling right? Yes.\nSales DB. Is the spelling right? Yes. Okay. Sales DP.\nOkay. Sales DP.\nOkay. Sales DP. I think I know the issue. Like there's\nI think I know the issue. Like there's\nI think I know the issue. Like there's no issue. So currently this file is\nno issue. So currently this file is\nno issue. So currently this file is running from this folder langchen\nrunning from this folder langchen\nrunning from this folder langchen tutorial and we want to run this from\ntutorial and we want to run this from\ntutorial and we want to run this from this folder. So I will simply say cd and\nthis folder. So I will simply say cd and\nthis folder. So I will simply say cd and I can also add the system path from\nI can also add the system path from\nI can also add the system path from here. It's not a big deal. So I can also\nhere. It's not a big deal. So I can also\nhere. It's not a big deal. So I can also say import os and then os.make directory\nsay import os and then os.make directory\nsay import os and then os.make directory if it is there. But we already have the\nif it is there. But we already have the\nif it is there. But we already have the folder. So we can also say system dot\nfolder. So we can also say system dot\nfolder. So we can also say system dot os. Cis or let me just import sis as\nos. Cis or let me just import sis as\nos. Cis or let me just import sis as well. import\nSIS and once we have the SIS\nSIS and once we have the SIS we can simply add it to the system path.\nwe can simply add it to the system path.\nwe can simply add it to the system path. So now what will happen? Now it will add\nSo now what will happen? Now it will add\nSo now what will happen? Now it will add our you can say system path to this\nour you can say system path to this\nour you can say system path to this directory. So now let's try to run it.\ndirectory. So now let's try to run it.\ndirectory. So now let's try to run it. If not we can just run it from here.\nIf not we can just run it from here.\nIf not we can just run it from here. Unable to it should add it. Um OS\nUnable to it should add it. Um OS\nUnable to it should add it. Um OS system.path. Let me just print it first\nsystem.path. Let me just print it first\nsystem.path. Let me just print it first of all. Like what is the directory?\nof all. Like what is the directory?\nof all. Like what is the directory? Print\nthis thing.\nthis thing. Hm. Okay. Lang tutorial chapter 3 unit.\nHm. Okay. Lang tutorial chapter 3 unit.\nHm. Okay. Lang tutorial chapter 3 unit. py\npy\npy that makes sense. Okay. So it is just\nthat makes sense. Okay. So it is just\nthat makes sense. Okay. So it is just returning the same thing.\nreturning the same thing.\nreturning the same thing. Let me just try running it from the CD\nLet me just try running it from the CD\nLet me just try running it from the CD because I know it happens whenever we\nbecause I know it happens whenever we\nbecause I know it happens whenever we just use or run this thing via this\njust use or run this thing via this\njust use or run this thing via this debug button because this is not that\ndebug button because this is not that\ndebug button because this is not that good way to run the Python file. We\ngood way to run the Python file. We\ngood way to run the Python file. We should always run like run Python\nshould always run like run Python\nshould always run like run Python something like that. So let me just\nsomething like that. So let me just\nsomething like that. So let me just remove it or keep it. It doesn't make\nremove it or keep it. It doesn't make\nremove it or keep it. It doesn't make any sense. So let me say cd\nand then chapter 3 and then we want to\nand then chapter 3 and then we want to go inside sales DB. Not sales DB. I\ngo inside sales DB. Not sales DB. I\ngo inside sales DB. Not sales DB. I think chapter 3 is fine. Now if I say\nthink chapter 3 is fine. Now if I say\nthink chapter 3 is fine. Now if I say python\npython\npython init db. py.\ninit db. py.\ninit db. py. Perfect. Now if I check the sales db it\nPerfect. Now if I check the sales db it\nPerfect. Now if I check the sales db it would have created the sales.bc.\nwould have created the sales.bc.\nwould have created the sales.bc. Perfect. Because it creates a sales. Db\nPerfect. Because it creates a sales. Db\nPerfect. Because it creates a sales. Db automatically. Make sense? If I write\nautomatically. Make sense? If I write\nautomatically. Make sense? If I write con equals to this. And if I just run\ncon equals to this. And if I just run\ncon equals to this. And if I just run python it will create this. See sales.\npython it will create this. See sales.\npython it will create this. See sales. Db. I know this database is empty for\nDb. I know this database is empty for\nDb. I know this database is empty for now. But database is created. Simple.\nnow. But database is created. Simple.\nnow. But database is created. Simple. And now I can just rerun the cell as\nAnd now I can just rerun the cell as\nAnd now I can just rerun the cell as many times as we can. Because this will\nmany times as we can. Because this will\nmany times as we can. Because this will not throw any error. This will not. Why?\nnot throw any error. This will not. Why?\nnot throw any error. This will not. Why? Because file is there. Oh, okay. Makes\nBecause file is there. Oh, okay. Makes\nBecause file is there. Oh, okay. Makes sense. Because now\nsense. Because now\nsense. Because now like it usually either creates the\nlike it usually either creates the\nlike it usually either creates the database if it doesn't exist and if it\ndatabase if it doesn't exist and if it\ndatabase if it doesn't exist and if it exist, it simply connects to it. So\nexist, it simply connects to it. So\nexist, it simply connects to it. So let's create a cursor quickly. Uh cursor\nlet's create a cursor quickly. Uh cursor\nlet's create a cursor quickly. Uh cursor equals con.cursor.\nequals con.cursor.\nequals con.cursor. And here I can simply say quant cursor\nAnd here I can simply say quant cursor\nAnd here I can simply say quant cursor which is this one connection is this\nwhich is this one connection is this\nwhich is this one connection is this one. And now I can create a table\none. And now I can create a table\none. And now I can create a table quickly.\nquickly.\nquickly. And I will simply say cursor.execute\nAnd I will simply say cursor.execute\nAnd I will simply say cursor.execute create table ID customer name product\ncreate table ID customer name product\ncreate table ID customer name product name quantity price total. Perfect. I\nname quantity price total. Perfect. I\nname quantity price total. Perfect. I want to create this table. And I also\nwant to create this table. And I also\nwant to create this table. And I also want to say cursor do.execute\nwant to say cursor do.execute\nwant to say cursor do.execute and let's insert some data. Insert into\nand let's insert some data. Insert into\nand let's insert some data. Insert into orders and values will be this. This\norders and values will be this. This\norders and values will be this. This just write the values as well. Bro,\nbro,\nbro, you can just do this thing as well or\nyou can just do this thing as well or\nyou can just do this thing as well or you can just run a loop. But for now, we\nyou can just run a loop. But for now, we\nyou can just run a loop. But for now, we are simply running it\nare simply running it\nare simply running it for the demo purpose. So let's run it\nfor the demo purpose. So let's run it\nfor the demo purpose. So let's run it like this. Okay, this is one way of\nlike this. Okay, this is one way of\nlike this. Okay, this is one way of doing it. You can also say with\ndoing it. You can also say with\ndoing it. You can also say with um not open basically cursor\num not open basically cursor\num not open basically cursor as\nas\nas like cursor and then you can just simply\nlike cursor and then you can just simply\nlike cursor and then you can just simply use the context manager to do it like\nuse the context manager to do it like\nuse the context manager to do it like there's multiple ways to do that. I\nthere's multiple ways to do that. I\nthere's multiple ways to do that. I personally like this thing\npersonally like this thing\npersonally like this thing cursor.execute. If you just want to use\ncursor.execute. If you just want to use\ncursor.execute. If you just want to use context manager you can just do that as\ncontext manager you can just do that as\ncontext manager you can just do that as well like this with cursor as cursor and\nwell like this with cursor as cursor and\nwell like this with cursor as cursor and cursor.execute. What is the benefit of\ncursor.execute. What is the benefit of\ncursor.execute. What is the benefit of it? All the things will be done within\nit? All the things will be done within\nit? All the things will be done within this context. It is called context\nthis context. It is called context\nthis context. It is called context manager. Okay. I can even run this as\nmanager. Okay. I can even run this as\nmanager. Okay. I can even run this as well like this. So the moment we are\nwell like this. So the moment we are\nwell like this. So the moment we are writing anything outside this the\nwriting anything outside this the\nwriting anything outside this the context is over. We do not need to worry\ncontext is over. We do not need to worry\ncontext is over. We do not need to worry about anything. Make sense? Okay. So now\nabout anything. Make sense? Okay. So now\nabout anything. Make sense? Okay. So now let's say let me just make it here.\nOkay. Width is here. And I can say\nOkay. Width is here. And I can say cursor dot commit and connection.\ncursor dot commit and connection.\ncursor dot commit and connection. Okay. So now let me just run this again.\nCursor object does not support the\nCursor object does not support the context manager pro. Oh man, this\ncontext manager pro. Oh man, this\ncontext manager pro. Oh man, this doesn't support because this is SQLite\ndoesn't support because this is SQLite\ndoesn't support because this is SQLite because I have used it with um postgra\nbecause I have used it with um postgra\nbecause I have used it with um postgra and my SQL it works fine but SQLite\nand my SQL it works fine but SQLite\nand my SQL it works fine but SQLite doesn't support it because SQLite is\ndoesn't support it because SQLite is\ndoesn't support it because SQLite is like very lightweight lightweight light\nlike very lightweight lightweight light\nlike very lightweight lightweight light lightweight lightweight. So let me just\nlightweight lightweight. So let me just\nlightweight lightweight. So let me just remove it and just follow that\nremove it and just follow that\nremove it and just follow that traditional approach. So nothing to do\ntraditional approach. So nothing to do\ntraditional approach. So nothing to do with it. So what again what we can do it\nwith it. So what again what we can do it\nwith it. So what again what we can do it do with it if it is not allowed.\ndo with it if it is not allowed.\ndo with it if it is not allowed. So let me just run it now.\nUh okay it has\nUh okay it has run it successfully. Now this sdb has\nrun it successfully. Now this sdb has\nrun it successfully. Now this sdb has some data. Now we'll say how do we know?\nsome data. Now we'll say how do we know?\nsome data. Now we'll say how do we know? I can just show you. Um I can just\nI can just show you. Um I can just\nI can just show you. Um I can just simply create a demo file just for you.\nsimply create a demo file just for you.\nsimply create a demo file just for you. I will simply say demo. py and I will\nI will simply say demo. py and I will\nI will simply say demo. py and I will simply say import SQLite 3. I will\nsimply say import SQLite 3. I will\nsimply say import SQLite 3. I will create a connection with this one and I\ncreate a connection with this one and I\ncreate a connection with this one and I will say select a from orders and let's\nwill say select a from orders and let's\nwill say select a from orders and let's see what does it return. Okay,\nsee what does it return. Okay,\nsee what does it return. Okay, let me just show you bro.\nlet me just show you bro.\nlet me just show you bro. Perfect.\nPerfect.\nPerfect. And this one do we have any kind of\nAnd this one do we have any kind of\nAnd this one do we have any kind of opened that thing? No.\nopened that thing? No.\nopened that thing? No. Oh man. Let me write cd.\nOh man. Let me write cd.\nOh man. Let me write cd. Wow, it is saying no this thing is\nWow, it is saying no this thing is\nWow, it is saying no this thing is existed.\nexisted.\nexisted. Oh man, let me just open the new\nOh man, let me just open the new\nOh man, let me just open the new terminal. Amazing product man. Amazing\nterminal. Amazing product man. Amazing\nterminal. Amazing product man. Amazing product. Amazing product.\nPerfect. Object does not support the\nPerfect. Object does not support the context manager pro. Okay, we forgot,\ncontext manager pro. Okay, we forgot,\ncontext manager pro. Okay, we forgot, bro.\nPerfect. See, I can see the data. That\nPerfect. See, I can see the data. That means data is there. Wow. That's how we\nmeans data is there. Wow. That's how we\nmeans data is there. Wow. That's how we retrieve the data. So now we have\nretrieve the data. So now we have\nretrieve the data. So now we have database ready. Your tension, your\ndatabase ready. Your tension, your\ndatabase ready. Your tension, your stress is over now because I have\nstress is over now because I have\nstress is over now because I have created the database for you. You do not\ncreated the database for you. You do not\ncreated the database for you. You do not need to download any server. So we have\nneed to download any server. So we have\nneed to download any server. So we have this database ready. Now what we are\nthis database ready. Now what we are\nthis database ready. Now what we are trying to do here, we want to create an\ntrying to do here, we want to create an\ntrying to do here, we want to create an agent which will take care of our SQL\nagent which will take care of our SQL\nagent which will take care of our SQL things. Can we do that? Yes. it will\nthings. Can we do that? Yes. it will\nthings. Can we do that? Yes. it will automatically run everything and it will\nautomatically run everything and it will\nautomatically run everything and it will give the um you can say data to us. So\ngive the um you can say data to us. So\ngive the um you can say data to us. So as you can see we have three records 1 2\nas you can see we have three records 1 2\nas you can see we have three records 1 2 3 because we ran it I think two times so\n3 because we ran it I think two times so\n3 because we ran it I think two times so that's why it is returning like this. Uh\nthat's why it is returning like this. Uh\nthat's why it is returning like this. Uh let me just clear it and let me just run\nlet me just clear it and let me just run\nlet me just clear it and let me just run it just to see the response. We have\nit just to see the response. We have\nit just to see the response. We have three response. Okay so we have laptop,\nthree response. Okay so we have laptop,\nthree response. Okay so we have laptop, tablet and smartphone, right? So maybe I\ntablet and smartphone, right? So maybe I\ntablet and smartphone, right? So maybe I just want to check the order for tablet,\njust want to check the order for tablet,\njust want to check the order for tablet, right? Maybe I want to check it or maybe\nright? Maybe I want to check it or maybe\nright? Maybe I want to check it or maybe I want to just check how many sales we\nI want to just check how many sales we\nI want to just check how many sales we made for smartphone. I want to just see\nmade for smartphone. I want to just see\nmade for smartphone. I want to just see you are creating an agent for any\nyou are creating an agent for any\nyou are creating an agent for any manager and you are a data engineer. You\nmanager and you are a data engineer. You\nmanager and you are a data engineer. You have created this particular data\nhave created this particular data\nhave created this particular data warehouse. Now you need to build an AI\nwarehouse. Now you need to build an AI\nwarehouse. Now you need to build an AI agent as well for your database, right?\nagent as well for your database, right?\nagent as well for your database, right? So what now what you will do? You will\nSo what now what you will do? You will\nSo what now what you will do? You will create a SQL agent and you can build SQL\ncreate a SQL agent and you can build SQL\ncreate a SQL agent and you can build SQL agent so easily with lang.\nagent so easily with lang.\nagent so easily with lang. Literally man you do not need to run any\nLiterally man you do not need to run any\nLiterally man you do not need to run any kind of those things. Um it will already\nkind of those things. Um it will already\nkind of those things. Um it will already like run everything on your sandbox and\nlike run everything on your sandbox and\nlike run everything on your sandbox and it it is very good. Let me show you if I\nit it is very good. Let me show you if I\nit it is very good. Let me show you if I can just find that documentation. This\ncan just find that documentation. This\ncan just find that documentation. This is the one SQL database toolkit. So they\nis the one SQL database toolkit. So they\nis the one SQL database toolkit. So they have created a special tool for SQL\nhave created a special tool for SQL\nhave created a special tool for SQL workloads. But yes, you need to be very\nworkloads. But yes, you need to be very\nworkloads. But yes, you need to be very very cautious to implement it in\nvery cautious to implement it in\nvery cautious to implement it in production because you need to add some\nproduction because you need to add some\nproduction because you need to add some guardrail so that no one can inject some\nguardrail so that no one can inject some\nguardrail so that no one can inject some queries which can harm your database and\nqueries which can harm your database and\nqueries which can harm your database and all. So you need to take care of those\nall. So you need to take care of those\nall. So you need to take care of those things and it is a kind of common sense\nthings and it is a kind of common sense\nthings and it is a kind of common sense you need to do that. But let's say we\nyou need to do that. But let's say we\nyou need to do that. But let's say we are good with that and let's actually\nare good with that and let's actually\nare good with that and let's actually try to build this. Okay. So our DB is\ntry to build this. Okay. So our DB is\ntry to build this. Okay. So our DB is done. So first of all I can just create\ndone. So first of all I can just create\ndone. So first of all I can just create this DB. What is this DB? This DB is\nthis DB. What is this DB? This DB is\nthis DB. What is this DB? This DB is nothing but a kind of you can say\nnothing but a kind of you can say\nnothing but a kind of you can say connection that you need to create. So\nconnection that you need to create. So\nconnection that you need to create. So first of all I will write this thing\nfirst of all I will write this thing\nfirst of all I will write this thing SQL\nSQL\nSQL database agent. Perfect. And I can just\ndatabase agent. Perfect. And I can just\ndatabase agent. Perfect. And I can just close this and let me just write it\nclose this and let me just write it\nclose this and let me just write it here.\nOkay. So this is done. So now now what\nOkay. So this is done. So now now what we will do? We'll simply create an agent\nwe will do? We'll simply create an agent\nwe will do? We'll simply create an agent which will run the query as well because\nwhich will run the query as well because\nwhich will run the query as well because there are two types of agents. One is\nthere are two types of agents. One is\nthere are two types of agents. One is like who creates the query. The second\nlike who creates the query. The second\nlike who creates the query. The second one is who\none is who\none is who just runs the query as well because we\njust runs the query as well because we\njust runs the query as well because we want to create the runs the query as\nwant to create the runs the query as\nwant to create the runs the query as well because we do not need to just\nwell because we do not need to just\nwell because we do not need to just generate the query. Okay. So let's first\ngenerate the query. Okay. So let's first\ngenerate the query. Okay. So let's first of all create the database and we have a\nof all create the database and we have a\nof all create the database and we have a special function called SQL database\nspecial function called SQL database\nspecial function called SQL database which is this one. And we do not need to\nwhich is this one. And we do not need to\nwhich is this one. And we do not need to even create cursor or basically the\neven create cursor or basically the\neven create cursor or basically the connection. So everything will be done\nconnection. So everything will be done\nconnection. So everything will be done by lang chain. So how we can just create\nby lang chain. So how we can just create\nby lang chain. So how we can just create the database connection? If you read the\nthe database connection? If you read the\nthe database connection? If you read the documentation, you will feel a little\ndocumentation, you will feel a little\ndocumentation, you will feel a little bit confused. But do not worry. You do\nbit confused. But do not worry. You do\nbit confused. But do not worry. You do not need to create this connection at\nnot need to create this connection at\nnot need to create this connection at all. You simply need to use this\nall. You simply need to use this\nall. You simply need to use this particular thing SQL database and that's\nparticular thing SQL database and that's\nparticular thing SQL database and that's it because you already have the engine.\nit because you already have the engine.\nit because you already have the engine. Okay, make sense? So you simply need to\nOkay, make sense? So you simply need to\nOkay, make sense? So you simply need to say SQL database\nsay SQL database\nsay SQL database uh like this\nuh like this\nuh like this SQL database or let's say DB whatever\nSQL database or let's say DB whatever\nSQL database or let's say DB whatever you want to say let's say SQL DB equals\nyou want to say let's say SQL DB equals\nyou want to say let's say SQL DB equals SQL database from from URI and then you\nSQL database from from URI and then you\nSQL database from from URI and then you simply need to pass the URL and the URL\nsimply need to pass the URL and the URL\nsimply need to pass the URL and the URL is like this SQLite\nand we do not have this thing dot and we\nand we do not have this thing dot and we have I think sales DB\nhave I think sales DB\nhave I think sales DB Perfect. And within that we have sales\nPerfect. And within that we have sales\nPerfect. And within that we have sales db dot sales.db basically I think so yes\ndb dot sales.db basically I think so yes\ndb dot sales.db basically I think so yes sales db and sales dodb I think it makes\nsales db and sales dodb I think it makes\nsales db and sales dodb I think it makes sense because we have like same folder\nsense because we have like same folder\nsense because we have like same folder within the same you can say folder and\nwithin the same you can say folder and\nwithin the same you can say folder and then we have the database within that.\nthen we have the database within that.\nthen we have the database within that. Okay so this is our DB that we have\nOkay so this is our DB that we have\nOkay so this is our DB that we have created. So so far we are good. This\ncreated. So so far we are good. This\ncreated. So so far we are good. This thing is done. Now we need to create\nthing is done. Now we need to create\nthing is done. Now we need to create LLM. Let's quickly create the LLM.\nLLM. Let's quickly create the LLM.\nLLM. Let's quickly create the LLM. Now let's quickly create the LLM and let\nNow let's quickly create the LLM and let\nNow let's quickly create the LLM and let me just bring that particular thing\nme just bring that particular thing\nme just bring that particular thing that code\nthat code\nthat code this one.\nthis one.\nthis one. Let me just write it here. So it will\nLet me just write it here. So it will\nLet me just write it here. So it will create the LLM for us. LLM open. So now\ncreate the LLM for us. LLM open. So now\ncreate the LLM for us. LLM open. So now we have both the things both the things.\nwe have both the things both the things.\nwe have both the things both the things. Yes. LLM and DB. And now we need to\nYes. LLM and DB. And now we need to\nYes. LLM and DB. And now we need to create a toolkit like this.\ncreate a toolkit like this.\ncreate a toolkit like this. Okay. Make sense? Now we just need to\nOkay. Make sense? Now we just need to\nOkay. Make sense? Now we just need to create the toolkit like this. Let me\ncreate the toolkit like this. Let me\ncreate the toolkit like this. Let me just copy it. Uh let me just paste it\njust copy it. Uh let me just paste it\njust copy it. Uh let me just paste it here. So now what are we trying to do?\nhere. So now what are we trying to do?\nhere. So now what are we trying to do? We are creating a toolkit and in this\nWe are creating a toolkit and in this\nWe are creating a toolkit and in this toolkit we have so many tools related to\ntoolkit we have so many tools related to\ntoolkit we have so many tools related to SQL.\nSQL.\nSQL. Really? Yeah. They have built-in toolkit\nReally? Yeah. They have built-in toolkit\nReally? Yeah. They have built-in toolkit where they have integrated so many tools\nwhere they have integrated so many tools\nwhere they have integrated so many tools within the same toolkit. Let me just\nwithin the same toolkit. Let me just\nwithin the same toolkit. Let me just show you. If you scroll down, if you\nshow you. If you scroll down, if you\nshow you. If you scroll down, if you just write get tools, you will see the\njust write get tools, you will see the\njust write get tools, you will see the entire list of tools. Wow. We have query\nentire list of tools. Wow. We have query\nentire list of tools. Wow. We have query SQL database. We have infosql database.\nSQL database. We have infosql database.\nSQL database. We have infosql database. We have list SQL database. We have query\nWe have list SQL database. We have query\nWe have list SQL database. We have query SQL checker. So these are the built-in\nSQL checker. So these are the built-in\nSQL checker. So these are the built-in functions that are created by Langchain.\nfunctions that are created by Langchain.\nfunctions that are created by Langchain. You can even create your own function.\nYou can even create your own function.\nYou can even create your own function. It's not a big deal. What will happen?\nIt's not a big deal. What will happen?\nIt's not a big deal. What will happen? You will simply get the uh you can say\nYou will simply get the uh you can say\nYou will simply get the uh you can say input. Then you will pass the database\ninput. Then you will pass the database\ninput. Then you will pass the database as context and then you will simply get\nas context and then you will simply get\nas context and then you will simply get the query in return and then you will\nthe query in return and then you will\nthe query in return and then you will just run it on sandbox. So it's not very\njust run it on sandbox. So it's not very\njust run it on sandbox. So it's not very something magical. It's just like\nsomething magical. It's just like\nsomething magical. It's just like building a wrapper and making your life\nbuilding a wrapper and making your life\nbuilding a wrapper and making your life easy. You can simply use the functions.\neasy. You can simply use the functions.\neasy. You can simply use the functions. Make sense? So toolkit is like toolkit\nMake sense? So toolkit is like toolkit\nMake sense? So toolkit is like toolkit SQL database and DB equals to DB which\nSQL database and DB equals to DB which\nSQL database and DB equals to DB which is not DB SQL DB and LLM is lm openi\nis not DB SQL DB and LLM is lm openi\nis not DB SQL DB and LLM is lm openi perfect now it makes sense perfect so\nperfect now it makes sense perfect so\nperfect now it makes sense perfect so now what it will do it will use this\nnow what it will do it will use this\nnow what it will do it will use this database as the context and it will use\ndatabase as the context and it will use\ndatabase as the context and it will use this model obviously because just tell\nthis model obviously because just tell\nthis model obviously because just tell me one thing is anything magical\nme one thing is anything magical\nme one thing is anything magical happening here no so what is happening\nhappening here no so what is happening\nhappening here no so what is happening here I will go to lm with my NLP with my\nhere I will go to lm with my NLP with my\nhere I will go to lm with my NLP with my natural query natural language query,\nnatural query natural language query,\nnatural query natural language query, right? I will go to LLM. I will say,\nright? I will go to LLM. I will say,\nright? I will go to LLM. I will say, \"Hey, generate this SQL query.\" LLM will\n\"Hey, generate this SQL query.\" LLM will\n\"Hey, generate this SQL query.\" LLM will say, \"Okay, bro. I will generate the\nsay, \"Okay, bro. I will generate the\nsay, \"Okay, bro. I will generate the query, but at least tell me the database\nquery, but at least tell me the database\nquery, but at least tell me the database name. At least tell me the table name.\nname. At least tell me the table name.\nname. At least tell me the table name. At least tell me the column names. Only\nAt least tell me the column names. Only\nAt least tell me the column names. Only then I can create the query.\" Simple.\nthen I can create the query.\" Simple.\nthen I can create the query.\" Simple. So, you are going to LLM with this\nSo, you are going to LLM with this\nSo, you are going to LLM with this database as the context. So, when you\ndatabase as the context. So, when you\ndatabase as the context. So, when you pass a database as the context, now LLM\npass a database as the context, now LLM\npass a database as the context, now LLM has the you can say\nhas the you can say\nhas the you can say context of the entire database. It knows\ncontext of the entire database. It knows\ncontext of the entire database. It knows the database name. It knows the columns.\nthe database name. It knows the columns.\nthe database name. It knows the columns. It knows the table name. Everything.\nIt knows the table name. Everything.\nIt knows the table name. Everything. Oh, okay. So, that's how it generates a\nOh, okay. So, that's how it generates a\nOh, okay. So, that's how it generates a query. You're simply passing the context\nquery. You're simply passing the context\nquery. You're simply passing the context and that's it. Okay. So, let's run this.\nand that's it. Okay. So, let's run this.\nand that's it. Okay. So, let's run this. Toolkit is ready. And if I just show you\nToolkit is ready. And if I just show you\nToolkit is ready. And if I just show you toolkit dot\ntoolkit dot\ntoolkit dot get tools, you will see all of the four\nget tools, you will see all of the four\nget tools, you will see all of the four tools that you have. Perfect. Now, we\ntools that you have. Perfect. Now, we\ntools that you have. Perfect. Now, we have toolkit as well. We have everything\nhave toolkit as well. We have everything\nhave toolkit as well. We have everything as well. So, now how we can just create\nas well. So, now how we can just create\nas well. So, now how we can just create the agent. So, now just skip everything.\nthe agent. So, now just skip everything.\nthe agent. So, now just skip everything. You simply need to copy this and just\nYou simply need to copy this and just\nYou simply need to copy this and just paste it here. So we are using langchain\npaste it here. So we are using langchain\npaste it here. So we are using langchain aagents import create agent. Okay. So\naagents import create agent. Okay. So\naagents import create agent. Okay. So now this is the function that we use and\nnow this is the function that we use and\nnow this is the function that we use and we use\nwe use\nwe use only and only\nLLM and toolkit and do we have toolkit?\nLLM and toolkit and do we have toolkit? Yeah, we have toolkit variable and I\nYeah, we have toolkit variable and I\nYeah, we have toolkit variable and I need to use lm open. Perfect. So now\nneed to use lm open. Perfect. So now\nneed to use lm open. Perfect. So now this is my agent.\nthis is my agent.\nthis is my agent. This is my agent. See, now it has\nThis is my agent. See, now it has\nThis is my agent. See, now it has created this agent model tools and blah\ncreated this agent model tools and blah\ncreated this agent model tools and blah blah blah. And this tools has all the\nblah blah. And this tools has all the\nblah blah. And this tools has all the SQL tools. Make sense? Make sense? Now,\nSQL tools. Make sense? Make sense? Now,\nSQL tools. Make sense? Make sense? Now, do you know what?\ndo you know what?\ndo you know what? Now, you can literally invoke this agent\nNow, you can literally invoke this agent\nNow, you can literally invoke this agent and you will get the answer. Okay, let's\nand you will get the answer. Okay, let's\nand you will get the answer. Okay, let's see. On Lamb, let's see. Okay, I will\nsee. On Lamb, let's see. Okay, I will\nsee. On Lamb, let's see. Okay, I will say agent dot invoke. Okay. And I want\nsay agent dot invoke. Okay. And I want\nsay agent dot invoke. Okay. And I want to say how much\nto say how much\nto say how much sales we made for\nsales we made for\nsales we made for smartphone.\nsmartphone.\nsmartphone. I know the answer. The answer is if I\nI know the answer. The answer is if I\nI know the answer. The answer is if I see the terminal, if I have the real\nsee the terminal, if I have the real\nsee the terminal, if I have the real records for smartphone, we made 500,\nrecords for smartphone, we made 500,\nrecords for smartphone, we made 500, right? But is this 500 and,000?\nright? But is this 500 and,000?\nright? But is this 500 and,000? What are these two columns? Let me just\nWhat are these two columns? Let me just\nWhat are these two columns? Let me just check the init. py and let me just\ncheck the init. py and let me just\ncheck the init. py and let me just remove this. So we have price and total.\nremove this. So we have price and total.\nremove this. So we have price and total. Okay. Oh, okay. Price and total. Oh,\nOkay. Oh, okay. Price and total. Oh,\nOkay. Oh, okay. Price and total. Oh, because we have quantity as well. So\nbecause we have quantity as well. So\nbecause we have quantity as well. So total price is,000. Okay. 1,000,000. Let\ntotal price is,000. Okay. 1,000,000. Let\ntotal price is,000. Okay. 1,000,000. Let let's try to use tablet because it is\nlet's try to use tablet because it is\nlet's try to use tablet because it is very straightforward. 600. Let's use\nvery straightforward. 600. Let's use\nvery straightforward. 600. Let's use this tablet. How much\nthis tablet. How much\nthis tablet. How much total sales we made for tablet? It\ntotal sales we made for tablet? It\ntotal sales we made for tablet? It should be 600. If it is using everything\nshould be 600. If it is using everything\nshould be 600. If it is using everything right, if it is using right SQL and\nright, if it is using right SQL and\nright, if it is using right SQL and everything. Wow, we have an error.\neverything. Wow, we have an error.\neverything. Wow, we have an error. Expected dict how much total sales you\nExpected dict how much total sales you\nExpected dict how much total sales you made for tablet? Okay. For\nmade for tablet? Okay. For\nmade for tablet? Okay. For troubleshooting this this expected\ntroubleshooting this this expected\ntroubleshooting this this expected dictionary. Okay. So do we need to use\ndictionary. Okay. So do we need to use\ndictionary. Okay. So do we need to use any kind of um input while calling it?\nany kind of um input while calling it?\nany kind of um input while calling it? Let me see.\nLet me see.\nLet me see. Yes. Okay. Okay. We need user and me.\nYes. Okay. Okay. We need user and me.\nYes. Okay. Okay. We need user and me. Oh, so messages is the key. See this is\nOh, so messages is the key. See this is\nOh, so messages is the key. See this is this is the downside when we use the\nthis is the downside when we use the\nthis is the downside when we use the automated creator autom automated\nautomated creator autom automated\nautomated creator autom automated created functions. But no not a big\ncreated functions. But no not a big\ncreated functions. But no not a big deal. We can just use this as this one\ndeal. We can just use this as this one\ndeal. We can just use this as this one as well. So if you have got the idea\nas well. So if you have got the idea\nas well. So if you have got the idea like what I'm trying to say\nlike what I'm trying to say\nlike what I'm trying to say how much\nhow much\nhow much total sales\ntotal sales\ntotal sales we made for\nwe made for\nwe made for tablet.\ntablet.\ntablet. So we need to pass the messages key as\nSo we need to pass the messages key as\nSo we need to pass the messages key as well.\nwell.\nwell. Okay. Let's now try to run it. Let's see\nOkay. Let's now try to run it. Let's see\nOkay. Let's now try to run it. Let's see what happens.\nwhat happens.\nwhat happens. Okay. Okay. Very good. Very good. Very\nOkay. Okay. Very good. Very good. Very\nOkay. Okay. Very good. Very good. Very good. Yes. Yes. Yes,\ngood. Yes. Yes. Yes,\ngood. Yes. Yes. Yes, it is generating the output. Let me just\nit is generating the output. Let me just\nit is generating the output. Let me just zoom out a little bit. Perfect. Total\nzoom out a little bit. Perfect. Total\nzoom out a little bit. Perfect. Total sales for tablet equals to 600. And it\nsales for tablet equals to 600. And it\nsales for tablet equals to 600. And it has already given you the reason as\nhas already given you the reason as\nhas already given you the reason as well. Three units into 200 each. Wow,\nwell. Three units into 200 each. Wow,\nwell. Three units into 200 each. Wow, man. Literally amazing. Let me show you\nman. Literally amazing. Let me show you\nman. Literally amazing. Let me show you what actually happened. So,\nwhat actually happened. So,\nwhat actually happened. So, first of all, I asked this question. How\nfirst of all, I asked this question. How\nfirst of all, I asked this question. How much total sales we made for tablet?\nmuch total sales we made for tablet?\nmuch total sales we made for tablet? Perfect. Okay. Now, it made a tool call\nPerfect. Okay. Now, it made a tool call\nPerfect. Okay. Now, it made a tool call to this particular\nto this particular\nto this particular function. It's called SQL DB list\nfunction. It's called SQL DB list\nfunction. It's called SQL DB list tables. We didn't create this function.\ntables. We didn't create this function.\ntables. We didn't create this function. We didn't create this tool. Lang can\nWe didn't create this tool. Lang can\nWe didn't create this tool. Lang can create this tool. So what it does, it\ncreate this tool. So what it does, it\ncreate this tool. So what it does, it simply lists down all the tables that we\nsimply lists down all the tables that we\nsimply lists down all the tables that we have because currently I just asked\nhave because currently I just asked\nhave because currently I just asked query related to single query, single\nquery related to single query, single\nquery related to single query, single table. Maybe I'm asking a query which\ntable. Maybe I'm asking a query which\ntable. Maybe I'm asking a query which needs to join multiple tables. So first\nneeds to join multiple tables. So first\nneeds to join multiple tables. So first of all, it will list all the tables and\nof all, it will list all the tables and\nof all, it will list all the tables and it will give to LLM. Simple. See list\nit will give to LLM. Simple. See list\nit will give to LLM. Simple. See list tables are done. Then it figured out\ntables are done. Then it figured out\ntables are done. Then it figured out that I want to use orders table. LM is\nthat I want to use orders table. LM is\nthat I want to use orders table. LM is smart enough to identify. Perfect. Now\nsmart enough to identify. Perfect. Now\nsmart enough to identify. Perfect. Now this table information see SQL DB schema\nthis table information see SQL DB schema\nthis table information see SQL DB schema that means what is the schema of the\nthat means what is the schema of the\nthat means what is the schema of the orders like what are the column names?\norders like what are the column names?\norders like what are the column names? Oh so this function actually acted and\nOh so this function actually acted and\nOh so this function actually acted and it gave it the schema name like all the\nit gave it the schema name like all the\nit gave it the schema name like all the column name. Then this is the table\ncolumn name. Then this is the table\ncolumn name. Then this is the table create table orders ID integer and blah\ncreate table orders ID integer and blah\ncreate table orders ID integer and blah blah blah. This is the tool message and\nblah blah. This is the tool message and\nblah blah. This is the tool message and this is the schema that we pass to LLM.\nthis is the schema that we pass to LLM.\nthis is the schema that we pass to LLM. Now AI is smart enough to just generate\nNow AI is smart enough to just generate\nNow AI is smart enough to just generate a query and run the\na query and run the\na query and run the generated query against it based on the\ngenerated query against it based on the\ngenerated query against it based on the schema\nschema\nschema based on the schema.\nbased on the schema.\nbased on the schema. That is why in the modern applications\nThat is why in the modern applications\nThat is why in the modern applications we always say that whenever you're\nwe always say that whenever you're\nwe always say that whenever you're creating a table whenever you're\ncreating a table whenever you're\ncreating a table whenever you're generating the tables you need to\ngenerating the tables you need to\ngenerating the tables you need to provide some description to each column\nprovide some description to each column\nprovide some description to each column so that LLM can easily pick the right\nso that LLM can easily pick the right\nso that LLM can easily pick the right columns. People are saying LLMs are\ncolumns. People are saying LLMs are\ncolumns. People are saying LLMs are hallucinating. LLMs are not generating\nhallucinating. LLMs are not generating\nhallucinating. LLMs are not generating right results because data governance is\nright results because data governance is\nright results because data governance is poor. Data quality is poor. You need to\npoor. Data quality is poor. You need to\npoor. Data quality is poor. You need to create tables with right descriptions.\ncreate tables with right descriptions.\ncreate tables with right descriptions. In the modern tools and technologies,\nIn the modern tools and technologies,\nIn the modern tools and technologies, there are some options where we can just\nthere are some options where we can just\nthere are some options where we can just generate the descriptions like this. And\ngenerate the descriptions like this. And\ngenerate the descriptions like this. And it is important because see I know that\nit is important because see I know that\nit is important because see I know that there will be some column let's say\nthere will be some column let's say\nthere will be some column let's say total. So it aligned that particular\ntotal. So it aligned that particular\ntotal. So it aligned that particular question with that particular column\nquestion with that particular column\nquestion with that particular column name. But sometimes we are asking a\nname. But sometimes we are asking a\nname. But sometimes we are asking a question which is not very much aligned\nquestion which is not very much aligned\nquestion which is not very much aligned with the column name. So how it will\nwith the column name. So how it will\nwith the column name. So how it will find it? Using description\nfind it? Using description\nfind it? Using description h\nh\nh make sense\nmake sense\nmake sense that's how it works. So see you have\nthat's how it works. So see you have\nthat's how it works. So see you have actually built a kind of agent that can\nactually built a kind of agent that can\nactually built a kind of agent that can create or basically and I can also\ncreate or basically and I can also\ncreate or basically and I can also remove it that can actually\nwrite or b not just write you have\nwrite or b not just write you have actually built an agent which can just\nactually built an agent which can just\nactually built an agent which can just do anything on your database anything.\ndo anything on your database anything.\ndo anything on your database anything. Make sense? Makes sense. Makes sense. So\nMake sense? Makes sense. Makes sense. So\nMake sense? Makes sense. Makes sense. So that's how you just do all of these\nthat's how you just do all of these\nthat's how you just do all of these things. Wow.\nthings. Wow.\nthings. Wow. Literally. Wow. So, it is a kind of\nLiterally. Wow. So, it is a kind of\nLiterally. Wow. So, it is a kind of agent in itself.\nagent in itself.\nagent in itself. H okay.\nH okay.\nH okay. So now let's try to understand the\nSo now let's try to understand the\nSo now let's try to understand the concept of rag and then I will just\nconcept of rag and then I will just\nconcept of rag and then I will just provide you the best resource for that\nprovide you the best resource for that\nprovide you the best resource for that as well. So first of all let me just\nas well. So first of all let me just\nas well. So first of all let me just provide you that then you can just jump\nprovide you that then you can just jump\nprovide you that then you can just jump on that topic. Let me just say rack anal\non that topic. Let me just say rack anal\non that topic. Let me just say rack anal lamba just search it and this is the\nlamba just search it and this is the\nlamba just search it and this is the rack full course that you can just watch\nrack full course that you can just watch\nrack full course that you can just watch and this is an amazing beginner friendly\nand this is an amazing beginner friendly\nand this is an amazing beginner friendly like this will not expect any any\nlike this will not expect any any\nlike this will not expect any any anything from you in terms of rag and\nanything from you in terms of rag and\nanything from you in terms of rag and this is I think 3.5 hours course you can\nthis is I think 3.5 hours course you can\nthis is I think 3.5 hours course you can simply go there and you will love this\nsimply go there and you will love this\nsimply go there and you will love this course this is like from zero to pro and\ncourse this is like from zero to pro and\ncourse this is like from zero to pro and actually mean it this is like amazing\nactually mean it this is like amazing\nactually mean it this is like amazing course okay but yes let's cover like\ncourse okay but yes let's cover like\ncourse okay but yes let's cover like what exactly the rag is just\nwhat exactly the rag is just\nwhat exactly the rag is just fundamentals and then you can simply go\nfundamentals and then you can simply go\nfundamentals and then you can simply go to this course make sense because see I\nto this course make sense because see I\nto this course make sense because see I could have just added rag in this\ncould have just added rag in this\ncould have just added rag in this particular course as well not a big deal\nparticular course as well not a big deal\nparticular course as well not a big deal for me but I know the depth the\nfor me but I know the depth the\nfor me but I know the depth the fundamental knowledge the conceptual\nfundamental knowledge the conceptual\nfundamental knowledge the conceptual knowledge the coding knowledge the\nknowledge the coding knowledge the\nknowledge the coding knowledge the practical knowledge the implementation\npractical knowledge the implementation\npractical knowledge the implementation knowledge that you will get in this\nknowledge that you will get in this\nknowledge that you will get in this particular 3.5 hours course like\nparticular 3.5 hours course like\nparticular 3.5 hours course like obviously it is worth watching this\nobviously it is worth watching this\nobviously it is worth watching this course so I was like let's guide this\ncourse so I was like let's guide this\ncourse so I was like let's guide this particular thing and let's provide this\nparticular thing and let's provide this\nparticular thing and let's provide this amazing masterpiece that we have created\namazing masterpiece that we have created\namazing masterpiece that we have created to my lovely data fam okay but still let\nto my lovely data fam okay but still let\nto my lovely data fam okay but still let me just tell you what exactly The rag is\nme just tell you what exactly The rag is\nme just tell you what exactly The rag is why rag is so popular.\nwhy rag is so popular.\nwhy rag is so popular. Let's try to cover rag.\nLet's try to cover rag.\nLet's try to cover rag. Now\nNow\nNow rag actually solves a problem. Okay,\nrag actually solves a problem. Okay,\nrag actually solves a problem. Okay, what problem? So let's say\nwhat problem? So let's say\nwhat problem? So let's say you have\nyou have\nyou have this thing.\nOkay.\nOkay. Well, let's even remove this thing.\nWell, let's even remove this thing.\nWell, let's even remove this thing. Let's say you just have that LLM\nOkay. Perfect. So, let's say you just\nOkay. Perfect. So, let's say you just have this thing LLM\nhave this thing LLM\nhave this thing LLM and you are here. Perfect. Now, this LLM\nand you are here. Perfect. Now, this LLM\nand you are here. Perfect. Now, this LLM doesn't have the access to your data\ndoesn't have the access to your data\ndoesn't have the access to your data that we already know and that we have\nthat we already know and that we have\nthat we already know and that we have validated as well. Let's say you want to\nvalidated as well. Let's say you want to\nvalidated as well. Let's say you want to um ask some questions ask some questions\num ask some questions ask some questions\num ask some questions ask some questions from a PDF and that PDF is actually the\nfrom a PDF and that PDF is actually the\nfrom a PDF and that PDF is actually the sales file for your organization.\nsales file for your organization.\nsales file for your organization. Okay, simple. Now just telling one thing\nOkay, simple. Now just telling one thing\nOkay, simple. Now just telling one thing if I want to ask some questions related\nif I want to ask some questions related\nif I want to ask some questions related to that PDF,\nto that PDF,\nto that PDF, it can be a PDF, it can be a text file,\nit can be a PDF, it can be a text file,\nit can be a PDF, it can be a text file, it can be literally anything. It can be\nit can be literally anything. It can be\nit can be literally anything. It can be a database as well. Okay, because rag is\na database as well. Okay, because rag is\na database as well. Okay, because rag is not just a tool or just a kind of\nnot just a tool or just a kind of\nnot just a tool or just a kind of function. And rag is a concept. Rag\nfunction. And rag is a concept. Rag\nfunction. And rag is a concept. Rag means first of all retrieval\naugmented\naugmented generation\nretrieval augmented generation. This is\nretrieval augmented generation. This is rag. Okay. Now what happens when LLM\nrag. Okay. Now what happens when LLM\nrag. Okay. Now what happens when LLM doesn't have the access to your data?\ndoesn't have the access to your data?\ndoesn't have the access to your data? How it will access your data? The answer\nHow it will access your data? The answer\nHow it will access your data? The answer is cannot access your data. That is why\nis cannot access your data. That is why\nis cannot access your data. That is why what we do we provide a kind of\nwhat we do we provide a kind of\nwhat we do we provide a kind of inventory\ninventory\ninventory inventory inventory we provide a kind of\ninventory inventory we provide a kind of\ninventory inventory we provide a kind of folder we provide a kind of repository\nfolder we provide a kind of repository\nfolder we provide a kind of repository from where LLM can access the data\nfrom where LLM can access the data\nfrom where LLM can access the data okay so let's suppose I wrote this\nokay so let's suppose I wrote this\nokay so let's suppose I wrote this question\nquestion\nquestion how much sales\nhow much sales\nhow much sales we made\nwe made\nwe made doesn't know this\ndoesn't know this\ndoesn't know this let's say you are writing this\nlet's say you are writing this\nlet's say you are writing this from your organization right but LLM\nfrom your organization right but LLM\nfrom your organization right but LLM doesn't know this thing at all\ndoesn't know this thing at all\ndoesn't know this thing at all at all\nat all\nat all okay so now how LLM would know this so\nokay so now how LLM would know this so\nokay so now how LLM would know this so what I will do I will attach something\nwhat I will do I will attach something\nwhat I will do I will attach something called as let's say a tool let's say\ncalled as let's say a tool let's say\ncalled as let's say a tool let's say this thing let's say I provide this\nthis thing let's say I provide this\nthis thing let's say I provide this particular archive\nparticular archive\nparticular archive or archive or let's say my data\nor archive or let's say my data\nor archive or let's say my data my data\nmy data\nmy data Okay, I have provided this. So what I\nOkay, I have provided this. So what I\nOkay, I have provided this. So what I will do, I will simply link these both\nwill do, I will simply link these both\nwill do, I will simply link these both two things like this.\ntwo things like this.\ntwo things like this. So now what will happen whenever I will\nSo now what will happen whenever I will\nSo now what will happen whenever I will be just asking this question\nbe just asking this question\nbe just asking this question how much sales you made or any question\nhow much sales you made or any question\nhow much sales you made or any question and I know that information is available\nand I know that information is available\nand I know that information is available within this data lm can do that. H okay\nwithin this data lm can do that. H okay\nwithin this data lm can do that. H okay is this magic? No, there's a you can say\nis this magic? No, there's a you can say\nis this magic? No, there's a you can say logic plus formulas behind it. So what\nlogic plus formulas behind it. So what\nlogic plus formulas behind it. So what it does rag is basically like\nit does rag is basically like\nit does rag is basically like multi-steps process. That's why I'm just\nmulti-steps process. That's why I'm just\nmulti-steps process. That's why I'm just suggesting you just watch that video.\nsuggesting you just watch that video.\nsuggesting you just watch that video. Okay, but still I'm just simply giving\nOkay, but still I'm just simply giving\nOkay, but still I'm just simply giving you very high level knowledge, very high\nyou very high level knowledge, very high\nyou very high level knowledge, very high level. So what it does, it stores your\nlevel. So what it does, it stores your\nlevel. So what it does, it stores your data in the form of vectors. Okay, in\ndata in the form of vectors. Okay, in\ndata in the form of vectors. Okay, in the form of vectors. Okay, let's see if\nthe form of vectors. Okay, let's see if\nthe form of vectors. Okay, let's see if we have anything for vectors.\nwe have anything for vectors.\nwe have anything for vectors. Oh, let's use this one.\nOh, let's use this one.\nOh, let's use this one. Yeah, let that makes sense.\nYeah, let that makes sense.\nYeah, let that makes sense. Perfect.\nPerfect.\nPerfect. This will store your data in the form of\nThis will store your data in the form of\nThis will store your data in the form of vectors.\nOkay, let's say vector 1, vector 2,\nOkay, let's say vector 1, vector 2, vector 3, vector 1,\nvector 3, vector 1,\nvector 3, vector 1, vector 2,\nvector 2,\nvector 2, and vector 3. Let's say this has three\nand vector 3. Let's say this has three\nand vector 3. Let's say this has three vectors because it totally depends upon\nvectors because it totally depends upon\nvectors because it totally depends upon the context size that you want to pick,\nthe context size that you want to pick,\nthe context size that you want to pick, right? So it has these three things okay\nright? So it has these three things okay\nright? So it has these three things okay under the hood. So now what will happen?\nunder the hood. So now what will happen?\nunder the hood. So now what will happen? This LLM will go here.\nThis LLM will go here.\nThis LLM will go here. LLM will actually go to this thing. So\nLLM will actually go to this thing. So\nLLM will actually go to this thing. So when it is going to my data actually it\nwhen it is going to my data actually it\nwhen it is going to my data actually it is going to this particular thing. Make\nis going to this particular thing. Make\nis going to this particular thing. Make sense? So I can\nsense? So I can\nsense? So I can change the angle now. So this is the\nchange the angle now. So this is the\nchange the angle now. So this is the reality of it.\nreality of it.\nreality of it. Okay. So it is actually talking to this.\nOkay. So it is actually talking to this.\nOkay. So it is actually talking to this. So what it does it will convert your\nSo what it does it will convert your\nSo what it does it will convert your question as well in the form of vector.\nquestion as well in the form of vector.\nquestion as well in the form of vector. Okay. Hm interesting and it is called\nOkay. Hm interesting and it is called\nOkay. Hm interesting and it is called query vector because this is the query\nquery vector because this is the query\nquery vector because this is the query right now this query will go now this\nright now this query will go now this\nright now this query will go now this query will go inside this particular\nquery will go inside this particular\nquery will go inside this particular area and it will find the relevant\narea and it will find the relevant\narea and it will find the relevant relevant vector. So this goes inside\nrelevant vector. So this goes inside\nrelevant vector. So this goes inside this let's say this and it found that\nthis let's say this and it found that\nthis let's say this and it found that this query is similar or let's say it\nthis query is similar or let's say it\nthis query is similar or let's say it stays out but it can just go and check\nstays out but it can just go and check\nstays out but it can just go and check that this particular thing is similar.\nthat this particular thing is similar.\nthat this particular thing is similar. How does it know this? Because LLM knows\nHow does it know this? Because LLM knows\nHow does it know this? Because LLM knows it through the formula that it uses and\nit through the formula that it uses and\nit through the formula that it uses and what kind of formula it uses it uses\nwhat kind of formula it uses it uses\nwhat kind of formula it uses it uses something called as\nsomething called as\nsomething called as cosine similarity. It can be Ukraidian\ncosine similarity. It can be Ukraidian\ncosine similarity. It can be Ukraidian distance as well but do not worry we are\ndistance as well but do not worry we are\ndistance as well but do not worry we are not teaching you statistics statistics\nnot teaching you statistics statistics\nnot teaching you statistics statistics okay so don't worry at all so we can\nokay so don't worry at all so we can\nokay so don't worry at all so we can just use this thing so it is not\njust use this thing so it is not\njust use this thing so it is not allowing me to just push this let me\nallowing me to just push this let me\nallowing me to just push this let me just push it wait because I want to go\njust push it wait because I want to go\njust push it wait because I want to go to this vector at anyhow perfect\nokay let's do it like this. Okay,\nokay let's do it like this. Okay, perfect. So let's say this query is\nperfect. So let's say this query is\nperfect. So let's say this query is actually similar to this vector one.\nactually similar to this vector one.\nactually similar to this vector one. Okay, do you know what will what it will\nOkay, do you know what will what it will\nOkay, do you know what will what it will do? It will return this vector one like\ndo? It will return this vector one like\ndo? It will return this vector one like this\nthis\nthis like this back to the LLM. Okay, now\nlike this back to the LLM. Okay, now\nlike this back to the LLM. Okay, now what will happen?\nwhat will happen?\nwhat will happen? Now what will happen? This LLM has this\nNow what will happen? This LLM has this\nNow what will happen? This LLM has this information in the form of vector.\ninformation in the form of vector.\ninformation in the form of vector. Okay. And then obviously the original\nOkay. And then obviously the original\nOkay. And then obviously the original text as well. Let's say it has the file\ntext as well. Let's say it has the file\ntext as well. Let's say it has the file associated with it. Um let's say any\nassociated with it. Um let's say any\nassociated with it. Um let's say any file. Let's say this is the file\nfile. Let's say this is the file\nfile. Let's say this is the file associated. Let's say the CSV or this\nassociated. Let's say the CSV or this\nassociated. Let's say the CSV or this txt any file. Okay. This file basically\ntxt any file. Okay. This file basically\ntxt any file. Okay. This file basically data textual data because actual data is\ndata textual data because actual data is\ndata textual data because actual data is in the form of text right. So this will\nin the form of text right. So this will\nin the form of text right. So this will go to this particular LLM. How? On the\ngo to this particular LLM. How? On the\ngo to this particular LLM. How? On the basis of similarity. And what formula\nbasis of similarity. And what formula\nbasis of similarity. And what formula are we using for similarity? You do not\nare we using for similarity? You do not\nare we using for similarity? You do not need to know this for now. Right? Like\nneed to know this for now. Right? Like\nneed to know this for now. Right? Like how it is just comparing this query with\nhow it is just comparing this query with\nhow it is just comparing this query with this vector. How it knows like it is\nthis vector. How it knows like it is\nthis vector. How it knows like it is similar to this and not similar to this\nsimilar to this and not similar to this\nsimilar to this and not similar to this and this. Basically we achieve this\nand this. Basically we achieve this\nand this. Basically we achieve this using cosine similarity or ukidian\nusing cosine similarity or ukidian\nusing cosine similarity or ukidian distance. But you don't need to worry\ndistance. But you don't need to worry\ndistance. But you don't need to worry about that. Just imagine that LLM is\nabout that. Just imagine that LLM is\nabout that. Just imagine that LLM is doing that all of that stuff for you\ndoing that all of that stuff for you\ndoing that all of that stuff for you like all the similarity search\nlike all the similarity search\nlike all the similarity search right. Okay. And in lang as well we have\nright. Okay. And in lang as well we have\nright. Okay. And in lang as well we have like dedicated function called\nlike dedicated function called\nlike dedicated function called similarity search. So we do not need to\nsimilarity search. So we do not need to\nsimilarity search. So we do not need to actually create that function because\nactually create that function because\nactually create that function because every um you can say vector database has\nevery um you can say vector database has\nevery um you can say vector database has its own particular way to retrieve or\nits own particular way to retrieve or\nits own particular way to retrieve or basically compare the data. So we will\nbasically compare the data. So we will\nbasically compare the data. So we will just we can just use lang chain's um\njust we can just use lang chain's um\njust we can just use lang chain's um package simple. So when we have this\npackage simple. So when we have this\npackage simple. So when we have this information available now just tell me\ninformation available now just tell me\ninformation available now just tell me one thing can lang chain answer or\none thing can lang chain answer or\none thing can lang chain answer or basically can lm answer this question\nbasically can lm answer this question\nbasically can lm answer this question obviously yes because it will use this\nobviously yes because it will use this\nobviously yes because it will use this data right now it will answer you back\ndata right now it will answer you back\ndata right now it will answer you back with the\nwith the\nwith the with the right answer basically\nwith the right answer basically\nwith the right answer basically okay\nokay\nokay perfect let's say right answer\nperfect let's say right answer\nperfect let's say right answer make sense that's the concept of rag\nmake sense that's the concept of rag\nmake sense that's the concept of rag Now why do we call it as retrieval\nNow why do we call it as retrieval\nNow why do we call it as retrieval augmented generation? First of all\naugmented generation? First of all\naugmented generation? First of all retrieval it retrieves the similar\nretrieval it retrieves the similar\nretrieval it retrieves the similar vector. Then augmented. Augmented means\nvector. Then augmented. Augmented means\nvector. Then augmented. Augmented means making something onried making something\nmaking something onried making something\nmaking something onried making something curated. So we augmented our model with\ncurated. So we augmented our model with\ncurated. So we augmented our model with this particular text which was not a\nthis particular text which was not a\nthis particular text which was not a available to LM before and at the end we\navailable to LM before and at the end we\navailable to LM before and at the end we generated the data with this context.\ngenerated the data with this context.\ngenerated the data with this context. This is called retrieval augmented\nThis is called retrieval augmented\nThis is called retrieval augmented generation.\ngeneration.\ngeneration. This is the highlevel overview or you\nThis is the highlevel overview or you\nThis is the highlevel overview or you can say highlevel um process of rack\ncan say highlevel um process of rack\ncan say highlevel um process of rack complete process. Now how to store this\ncomplete process. Now how to store this\ncomplete process. Now how to store this data, how these vectors are created, how\ndata, how these vectors are created, how\ndata, how these vectors are created, how the similarity search is happening, how\nthe similarity search is happening, how\nthe similarity search is happening, how this augmentation is happening, how\nthis augmentation is happening, how\nthis augmentation is happening, how generation is happening, you can watch\ngeneration is happening, you can watch\ngeneration is happening, you can watch that video that I have created on this\nthat video that I have created on this\nthat video that I have created on this channel and you can even search it on\nchannel and you can even search it on\nchannel and you can even search it on YouTube um rag anlamba okay or I will\nYouTube um rag anlamba okay or I will\nYouTube um rag anlamba okay or I will also attach the video in the\nalso attach the video in the\nalso attach the video in the description. If not, you can simply\ndescription. If not, you can simply\ndescription. If not, you can simply search on YouTube anchar rag and you\nsearch on YouTube anchar rag and you\nsearch on YouTube anchar rag and you will find that video. I would say that's\nwill find that video. I would say that's\nwill find that video. I would say that's all for this particular video and I know\nall for this particular video and I know\nall for this particular video and I know you learned a lot of things and you\nyou learned a lot of things and you\nyou learned a lot of things and you loved this video. So just just just hit\nloved this video. So just just just hit\nloved this video. So just just just hit the subscribe button right now and just\nthe subscribe button right now and just\nthe subscribe button right now and just click on the video coming on the screen\nclick on the video coming on the screen\nclick on the video coming on the screen and I will see you there. Bye-bye."
  },
  {
    "id": 60494624,
    "timestamp": "2026-02-25T08:15:31.539Z",
    "title": "What is LangChain?",
    "url": "https://www.youtube.com/watch?v=1bUy-1hGZpI",
    "text": "now stop me if you've heard this one\nnow stop me if you've heard this one before but there are a lot of large\nbefore but there are a lot of large\nbefore but there are a lot of large language models available today and they\nlanguage models available today and they\nlanguage models available today and they have their own capabilities and\nhave their own capabilities and\nhave their own capabilities and specialities what if I prefer to use one\nspecialities what if I prefer to use one\nspecialities what if I prefer to use one llm to interpret some user queries in my\nllm to interpret some user queries in my\nllm to interpret some user queries in my business application but a whole other\nbusiness application but a whole other\nbusiness application but a whole other llm to author a response to those\nllm to author a response to those\nllm to author a response to those queries well that scenario is exactly\nqueries well that scenario is exactly\nqueries well that scenario is exactly what Lang chain caters to Lang chain is\nwhat Lang chain caters to Lang chain is\nwhat Lang chain caters to Lang chain is an open-source orchestration framework\nan open-source orchestration framework\nan open-source orchestration framework for the development of applications that\nfor the development of applications that\nfor the development of applications that use large language models and it comes\nuse large language models and it comes\nuse large language models and it comes in both Python and JavaScript libraries\nin both Python and JavaScript libraries\nin both Python and JavaScript libraries it's it's essentially a generic\nit's it's essentially a generic\nit's it's essentially a generic interface for nearly any llm so you have\ninterface for nearly any llm so you have\ninterface for nearly any llm so you have a centralized development environment to\na centralized development environment to\na centralized development environment to build your large language model\nbuild your large language model\nbuild your large language model applications and then integrate them\napplications and then integrate them\napplications and then integrate them with stuff like data sources and\nwith stuff like data sources and\nwith stuff like data sources and software workflows now when it was\nsoftware workflows now when it was\nsoftware workflows now when it was launched by Harrison Chase in October\nlaunched by Harrison Chase in October\nlaunched by Harrison Chase in October 2022 Lang chain enjoyed a meteoric rise\n2022 Lang chain enjoyed a meteoric rise\n2022 Lang chain enjoyed a meteoric rise and by June of the following year it was\nand by June of the following year it was\nand by June of the following year it was the single fastest growing open- source\nthe single fastest growing open- source\nthe single fastest growing open- source project on GitHub and while the Lang\nproject on GitHub and while the Lang\nproject on GitHub and while the Lang chain hype\nchain hype\nchain hype train has uh slightly cooled a little\ntrain has uh slightly cooled a little\ntrain has uh slightly cooled a little bit there's plenty of utility here so\nbit there's plenty of utility here so\nbit there's plenty of utility here so let's take a look at its components so\nlet's take a look at its components so\nlet's take a look at its components so what makes up Lang\nwhat makes up Lang\nwhat makes up Lang chain well Lang chain streamlines the\nchain well Lang chain streamlines the\nchain well Lang chain streamlines the programming of llm applications through\nprogramming of llm applications through\nprogramming of llm applications through something called abstractions now what\nsomething called abstractions now what\nsomething called abstractions now what do I mean by that well your thermostat\ndo I mean by that well your thermostat\ndo I mean by that well your thermostat that allows you to control the\nthat allows you to control the\nthat allows you to control the temperature in your home with without\ntemperature in your home with without\ntemperature in your home with without needing to understand all the complex\nneeding to understand all the complex\nneeding to understand all the complex circuitary that this entails we just set\ncircuitary that this entails we just set\ncircuitary that this entails we just set the temperature that's an abstraction so\nthe temperature that's an abstraction so\nthe temperature that's an abstraction so Lang chains abstractions represent\nLang chains abstractions represent\nLang chains abstractions represent common steps and Concepts necessary to\ncommon steps and Concepts necessary to\ncommon steps and Concepts necessary to work with language models and they can\nwork with language models and they can\nwork with language models and they can be chained together to create\nbe chained together to create\nbe chained together to create applications minimizing the amount of\napplications minimizing the amount of\napplications minimizing the amount of code required to execute complex NLP\ncode required to execute complex NLP\ncode required to execute complex NLP tasks so let's start with the\ntasks so let's start with the\ntasks so let's start with the llm module now nearly any LM LM can be\nllm module now nearly any LM LM can be\nllm module now nearly any LM LM can be used in Lang chain you just need an API\nused in Lang chain you just need an API\nused in Lang chain you just need an API key the llm class is designed to provide\nkey the llm class is designed to provide\nkey the llm class is designed to provide a standard interface for all models so\na standard interface for all models so\na standard interface for all models so pick an llm of your choice be that a\npick an llm of your choice be that a\npick an llm of your choice be that a closed Source One like gp4 or an Open\nclosed Source One like gp4 or an Open\nclosed Source One like gp4 or an Open Source One like llama 2 or this being\nSource One like llama 2 or this being\nSource One like llama 2 or this being Lang chain pick both okay what else we\nLang chain pick both okay what else we\nLang chain pick both okay what else we got we have\ngot we have\ngot we have prompts now prompts are the instructions\nprompts now prompts are the instructions\nprompts now prompts are the instructions given to a large language model and the\ngiven to a large language model and the\ngiven to a large language model and the prompt template class in Lang chain\nprompt template class in Lang chain\nprompt template class in Lang chain formalizes the composition of prompts\nformalizes the composition of prompts\nformalizes the composition of prompts without the need to manually hardcode\nwithout the need to manually hardcode\nwithout the need to manually hardcode context and queries a prompt template\ncontext and queries a prompt template\ncontext and queries a prompt template can contain instructions like uh do not\ncan contain instructions like uh do not\ncan contain instructions like uh do not use technical terms in your response\nuse technical terms in your response\nuse technical terms in your response that would be a good one or it could be\nthat would be a good one or it could be\nthat would be a good one or it could be a set of examples to guide its responses\na set of examples to guide its responses\na set of examples to guide its responses that's called f shot prompting or it\nthat's called f shot prompting or it\nthat's called f shot prompting or it could specify an output\ncould specify an output\ncould specify an output format now\nformat now\nformat now chains as the name implies are the core\nchains as the name implies are the core\nchains as the name implies are the core of Lang chain workflows they combine\nof Lang chain workflows they combine\nof Lang chain workflows they combine llms with other components creating\nllms with other components creating\nllms with other components creating applications by executing a sequence of\napplications by executing a sequence of\napplications by executing a sequence of functions so let's say our application\nfunctions so let's say our application\nfunctions so let's say our application that needs to first of all retrieve data\nthat needs to first of all retrieve data\nthat needs to first of all retrieve data from a website then it needs to\nfrom a website then it needs to\nfrom a website then it needs to summarize the text it gets back and then\nsummarize the text it gets back and then\nsummarize the text it gets back and then finally it needs to use that summary to\nfinally it needs to use that summary to\nfinally it needs to use that summary to answer User submitted questions that's a\nanswer User submitted questions that's a\nanswer User submitted questions that's a sequential chain where the output of one\nsequential chain where the output of one\nsequential chain where the output of one function access the input to the next\nfunction access the input to the next\nfunction access the input to the next and each function in the chain could use\nand each function in the chain could use\nand each function in the chain could use different prompts different parameters\ndifferent prompts different parameters\ndifferent prompts different parameters and even different\nand even different\nand even different models now to achieve certain tasks llms\nmodels now to achieve certain tasks llms\nmodels now to achieve certain tasks llms might need to access specific external\nmight need to access specific external\nmight need to access specific external data sources that are not included in\ndata sources that are not included in\ndata sources that are not included in the training data set of the llm itself\nthe training data set of the llm itself\nthe training data set of the llm itself so things like internal documents or\nso things like internal documents or\nso things like internal documents or emails that sort of thing now Lang chain\nemails that sort of thing now Lang chain\nemails that sort of thing now Lang chain collectively refers to this sort of\ncollectively refers to this sort of\ncollectively refers to this sort of documentation as\ndocumentation as\ndocumentation as indexes and there are a number of them\nindexes and there are a number of them\nindexes and there are a number of them so let's take a look at a few now one of\nso let's take a look at a few now one of\nso let's take a look at a few now one of them\nthem\nthem is called a\nis called a\nis called a document\ndocument\ndocument loader now document loaders they work\nloader now document loaders they work\nloader now document loaders they work with thirdparty applications for\nwith thirdparty applications for\nwith thirdparty applications for importing data sources from sources like\nimporting data sources from sources like\nimporting data sources from sources like file storage services so think Dropbox\nfile storage services so think Dropbox\nfile storage services so think Dropbox or Google drive or web content from like\nor Google drive or web content from like\nor Google drive or web content from like YouTube transcripts or collaboration\nYouTube transcripts or collaboration\nYouTube transcripts or collaboration tools like air table or databases like\ntools like air table or databases like\ntools like air table or databases like pandas and mongod\npandas and mongod\npandas and mongod DB there's also support for vector\nDB there's also support for vector\nDB there's also support for vector databases as well now unlike traditional\ndatabases as well now unlike traditional\ndatabases as well now unlike traditional structured databases Vector databases\nstructured databases Vector databases\nstructured databases Vector databases represent data points by converting them\nrepresent data points by converting them\nrepresent data points by converting them into something called Vector embeddings\ninto something called Vector embeddings\ninto something called Vector embeddings which are numerical representations in\nwhich are numerical representations in\nwhich are numerical representations in the form of vectors with a fixed number\nthe form of vectors with a fixed number\nthe form of vectors with a fixed number of dimensions and you can store a lot of\nof dimensions and you can store a lot of\nof dimensions and you can store a lot of information in this format as as it's a\ninformation in this format as as it's a\ninformation in this format as as it's a very efficient means of retrieval there\nvery efficient means of retrieval there\nvery efficient means of retrieval there are also something called text\nare also something called text\nare also something called text Splitters which can be very useful as\nSplitters which can be very useful as\nSplitters which can be very useful as well because they can split text up into\nwell because they can split text up into\nwell because they can split text up into small semantically meaningful chunks\nsmall semantically meaningful chunks\nsmall semantically meaningful chunks that can then be combined using the\nthat can then be combined using the\nthat can then be combined using the methods and parameters of your\nmethods and parameters of your\nmethods and parameters of your choosing Now llms by default don't\nchoosing Now llms by default don't\nchoosing Now llms by default don't really have any long-term memory of\nreally have any long-term memory of\nreally have any long-term memory of Prior conversations unless you happen to\nPrior conversations unless you happen to\nPrior conversations unless you happen to pass the chat history in as an input to\npass the chat history in as an input to\npass the chat history in as an input to your query but Lang chain solves this\nyour query but Lang chain solves this\nyour query but Lang chain solves this problem with simple utilities for adding\nproblem with simple utilities for adding\nproblem with simple utilities for adding in memory into your\nin memory into your\nin memory into your application and you have options retain\napplication and you have options retain\napplication and you have options retain for retaining like the entire High\nfor retaining like the entire High\nfor retaining like the entire High conversations through two options to\nconversations through two options to\nconversations through two options to just retain a summarization of the\njust retain a summarization of the\njust retain a summarization of the conversation that we've had so far and\nconversation that we've had so far and\nconversation that we've had so far and then finally the last one we'll look at\nthen finally the last one we'll look at\nthen finally the last one we'll look at are agents now agents can use a given\nare agents now agents can use a given\nare agents now agents can use a given language model as a reasoning engine to\nlanguage model as a reasoning engine to\nlanguage model as a reasoning engine to determine which actions to take and when\ndetermine which actions to take and when\ndetermine which actions to take and when building a chain for an agent you'll\nbuilding a chain for an agent you'll\nbuilding a chain for an agent you'll want to include inputs like a list of\nwant to include inputs like a list of\nwant to include inputs like a list of the available tools that it should use\nthe available tools that it should use\nthe available tools that it should use uh the user input like the prompts and\nuh the user input like the prompts and\nuh the user input like the prompts and the queries and then any other relevant\nthe queries and then any other relevant\nthe queries and then any other relevant previously executed steps so how can we\npreviously executed steps so how can we\npreviously executed steps so how can we put all of this to work for our\nput all of this to work for our\nput all of this to work for our applications well let's talk about a few\napplications well let's talk about a few\napplications well let's talk about a few Lang chain use cases now obviously we\nLang chain use cases now obviously we\nLang chain use cases now obviously we have chatbots Lang chain can be used to\nhave chatbots Lang chain can be used to\nhave chatbots Lang chain can be used to provide proper context for the specific\nprovide proper context for the specific\nprovide proper context for the specific use of a chatbot and to integrate\nuse of a chatbot and to integrate\nuse of a chatbot and to integrate chatbots into existing communication\nchatbots into existing communication\nchatbots into existing communication channels and workflows with their own\nchannels and workflows with their own\nchannels and workflows with their own apis we also have summarization language\napis we also have summarization language\napis we also have summarization language model can be tasked with summarizing\nmodel can be tasked with summarizing\nmodel can be tasked with summarizing many types of text from breaking down\nmany types of text from breaking down\nmany types of text from breaking down complex academic papers and transcripts\ncomplex academic papers and transcripts\ncomplex academic papers and transcripts to providing just a digest of incoming\nto providing just a digest of incoming\nto providing just a digest of incoming emails we've also seen lots of examples\nemails we've also seen lots of examples\nemails we've also seen lots of examples where this is used for question\nwhere this is used for question\nwhere this is used for question answering so using specific documents or\nanswering so using specific documents or\nanswering so using specific documents or specialized knowledge basis llms can\nspecialized knowledge basis llms can\nspecialized knowledge basis llms can retrieve the relevant information from\nretrieve the relevant information from\nretrieve the relevant information from the storage and then articulate helpful\nthe storage and then articulate helpful\nthe storage and then articulate helpful answers using the information that would\nanswers using the information that would\nanswers using the information that would otherwise not have been in their\notherwise not have been in their\notherwise not have been in their training data set and uh yeah this is a\ntraining data set and uh yeah this is a\ntraining data set and uh yeah this is a good one data augmentation llms can be\ngood one data augmentation llms can be\ngood one data augmentation llms can be used to generate synthetic data for use\nused to generate synthetic data for use\nused to generate synthetic data for use of machine learning so for example llm\nof machine learning so for example llm\nof machine learning so for example llm can be trained to generate additional\ncan be trained to generate additional\ncan be trained to generate additional samples that closely resemble the real\nsamples that closely resemble the real\nsamples that closely resemble the real data points in a training data set and\ndata points in a training data set and\ndata points in a training data set and there are of course virtual agents as we\nthere are of course virtual agents as we\nthere are of course virtual agents as we already started to discuss integrated\nalready started to discuss integrated\nalready started to discuss integrated with the the right workflows Lang chains\nwith the the right workflows Lang chains\nwith the the right workflows Lang chains agent modules can use an llm to\nagent modules can use an llm to\nagent modules can use an llm to autonomously determine the next steps\nautonomously determine the next steps\nautonomously determine the next steps and then take the action that it needs\nand then take the action that it needs\nand then take the action that it needs to complete that step using something\nto complete that step using something\nto complete that step using something called RPA or robotic process\ncalled RPA or robotic process\ncalled RPA or robotic process automation Lang chain is open source and\nautomation Lang chain is open source and\nautomation Lang chain is open source and free to use there are also related\nfree to use there are also related\nfree to use there are also related Frameworks like Lang serve for creating\nFrameworks like Lang serve for creating\nFrameworks like Lang serve for creating chains as rest apis and Lang Smith which\nchains as rest apis and Lang Smith which\nchains as rest apis and Lang Smith which provides tools to monitor evaluate and\nprovides tools to monitor evaluate and\nprovides tools to monitor evaluate and debug applications essentially Lang\ndebug applications essentially Lang\ndebug applications essentially Lang Chain's tools and apis simplify the\nChain's tools and apis simplify the\nChain's tools and apis simplify the process of building applications that\nprocess of building applications that\nprocess of building applications that make use of large language\nmake use of large language\nmake use of large language models if you have any questions please\nmodels if you have any questions please\nmodels if you have any questions please drop us a line below and if you want to\ndrop us a line below and if you want to\ndrop us a line below and if you want to see more videos like this in the future\nsee more videos like this in the future\nsee more videos like this in the future please like And subscribe thanks for\nplease like And subscribe thanks for\nplease like And subscribe thanks for watching"
  }
]
